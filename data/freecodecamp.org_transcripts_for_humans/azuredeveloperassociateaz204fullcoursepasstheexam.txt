With timestamps:

00:00 - hey this is andrew brown your cloud
00:02 - instructor exam pro bringing you another
00:04 - complete study course and this time it's
00:06 - the microsoft azure developer associate
00:07 - also known as the az204 made available
00:10 - to you on freecodecamp this course is
00:12 - designed to help you pass and achieve
00:14 - microsoft issued certification and the
00:16 - way we're going to do that is by doing
00:18 - lecture content follow alongs a practice
00:21 - exam and have cheat sheets for the day
00:23 - of the exam so you can prove on your
00:25 - resume your linkedin you have that as
00:26 - your knowledge you can go get that cloud
00:28 - job or get that promotion and tell you a
00:30 - bit about me i was previously the cto of
00:32 - multiple ed tech companies 15 years
00:34 - industry experience five years
00:35 - specializing in cloud i'm a nativist
00:37 - community hero and i've published many
00:39 - many free courses just like this one i
00:42 - love star trek and coconut water
00:44 - and i just want to take a moment here to
00:46 - thank viewers like you because it's you
00:47 - that make these courses possible and the
00:50 - best way to support more courses just
00:53 - like this one is to purchase the
00:55 - additional study materials at az204 so
00:58 - you can get access to study notes flash
01:00 - cards quizlets downloadable lecture
01:01 - slides downloadable cheat sheets
01:03 - practice exams ask questions get
01:05 - learning support but if you sign up
01:07 - right now for free you'll get a practice
01:09 - exam and cheat sheet there's no credit
01:11 - card required the free stuff does not
01:13 - have a trial limit so you do not have to
01:15 - worry about it vanishing it's not a demo
01:18 - uh if you have if there are updates to
01:19 - this course they'll be in the youtube
01:21 - descriptions on free code camp so watch
01:24 - out if there are corrections additions
01:26 - modifications to make sure you're using
01:28 - the latest version of this course and if
01:30 - you want to keep up to date on upcoming
01:32 - courses the best way to find that is on
01:35 - twitter i'm at andrew brown on twitter
01:38 - and if you do pass i love to hear uh you
01:41 - know your success story um or what you'd
01:44 - like to see next so there you go
01:51 - hey this is andrew brown from exam pro
01:53 - and we're at the start of our journey
01:54 - asking the most important questions
01:56 - first which is what is the azure
01:58 - developer associate so it's a microsoft
02:00 - certification about the azure platform
02:02 - focused on multiple ways to deploy web
02:04 - apps to azure a deep dive into azure
02:06 - functions i mean seriously deep a broad
02:09 - look into application integration
02:11 - services which azure has a lot of
02:13 - to the point where they have ones that
02:15 - cover the same purpose and lots of
02:17 - hands-on with azure cli sdks or
02:20 - programmatic interactions with azure
02:22 - services so anything to do with the
02:23 - azure api the course code for the azure
02:26 - developer is the ac204 not to be
02:28 - confused with the 104 but it is very
02:30 - complementary to the other course which
02:32 - we'll talk about when we talk about the
02:33 - roadmap and i do want to point out that
02:35 - microsoft azure is a very code and
02:37 - script driven uh platform compared to
02:40 - aws and gcp so it's better having
02:42 - developer knowledge and i actually
02:44 - consider it a must when we're working
02:45 - with microsoft and i'll explain why here
02:47 - in a bit so who's the certification for
02:49 - we'll consider the 204 if you are a web
02:52 - app developer looking to pick up cloud
02:53 - skills or transition to a cloud
02:55 - developer role you are a cloud developer
02:57 - cloud engineer who needs to integrate
02:58 - azure services into your app or deploy
03:01 - your app to azure or you need to learn a
03:03 - lot about azure functions to build
03:04 - service workloads but as a note i kind
03:07 - of feel that this is an essential
03:08 - certification at the associate level and
03:11 - so looking at a roadmap i always suggest
03:13 - people to start with the az900 which is
03:15 - the fundamentals because it's going to
03:16 - really
03:17 - help you get not just introduced to
03:19 - azure but also the test taking
03:22 - experience because it is a much
03:24 - difficult and different
03:26 - experience than the other two providers
03:28 - and generally from that we usually
03:29 - recommend to go with the administrator
03:30 - the 104 because that is a broad amount
03:33 - of services and that is the most common
03:35 - use case why people are on microsoft
03:37 - azure because they're usually i.t dev
03:39 - shops
03:40 - but as a complimentary usually after
03:42 - that i would recommend to go to the
03:43 - developer the az204 because the nature
03:46 - of azure is that a lot of the ui it like
03:50 - is script driven so when you are in the
03:52 - portal uh you have to touch script more
03:54 - often than not or you might run into
03:56 - features that simply do not exist in the
03:58 - portal so you have to use scripting so
04:01 - really to be a proficient pers person
04:03 - working on azure you need developer
04:05 - programming experience and so i always
04:07 - pair these two together now where you
04:09 - want to go after that is up to you a lot
04:11 - of people like to go the solutions
04:12 - architect or the devops engineer expert
04:14 - there's a bunch of other
04:16 - associates and i consider these kind of
04:18 - like uh like mid-level specialties where
04:21 - you can do those there's of course more
04:23 - certifications than we're looking at
04:25 - that is actually here
04:27 - but you know these are the most general
04:28 - ones here so you know hopefully that
04:30 - gives you an idea that you should
04:32 - probably take the developer after your
04:33 - associate
04:35 - and then after that you go wherever you
04:36 - want in terms of difficulty the azure
04:39 - developer associate is i would say two
04:41 - times harder than the aws developer
04:43 - associate and three times more difficult
04:45 - than the gcp associate and i'm talking
04:47 - about the exams not necessarily the
04:49 - application
04:51 - of being a developer in the platform but
04:52 - just the fact that the way azure makes
04:55 - their exams is they really want to test
04:57 - for practical knowledge so it's not
04:59 - about conceptual or strategic
05:00 - information not to say that there isn't
05:02 - those kind of questions on exam but it's
05:04 - a huge focus on do you know how to
05:06 - actually set things up and do you know
05:08 - the nuances of them so you're going to
05:10 - see a lot more labs than normal in my
05:12 - courses for this particular study course
05:15 - how long does it take to pass well it
05:17 - depends on you but to give you a general
05:19 - idea if you're a beginner so let's say
05:21 - you had the az 900 but not much hands-on
05:23 - experience you've never written code or
05:25 - attack role then you're looking 50 hours
05:26 - even if you've taken the easy to 104
05:29 - it's still going to take this long
05:30 - because it's a different uh different
05:32 - beast than the 104. if you're
05:33 - experienced so you already have
05:35 - practical knowledge working on azure uh
05:37 - you've deployed apps to azure you have a
05:39 - strong background in programming you're
05:40 - looking at 20 hours and so i like to uh
05:43 - set a goal of somewhere in between so 30
05:46 - hours average study time so for lectures
05:48 - in lab that's going to uh take up 50 of
05:50 - your time so you know we're looking at
05:52 - it's probably more uh labs and lecture
05:55 - and then the other part of it is 50
05:57 - practice exams so i recommend you study
05:59 - one to two hours for 20 days and really
06:02 - do spread that out because if you do too
06:03 - much of it together uh you have a a good
06:06 - chance of
06:08 - forgetting information so you know make
06:10 - sure you spread that out and make sure
06:11 - that it's it becomes part of your
06:13 - long-term memory so where do you take
06:15 - the example you can do it in person or
06:16 - test center or online from the
06:18 - convenience of your own home so
06:19 - microsoft delivers exams via two
06:21 - different uh providers psi online
06:24 - pearson vue and those are online proctor
06:26 - exam systems both of these providers
06:28 - have their own test center networks so
06:30 - whether you want the online experience
06:31 - or in person it's just to be up to you i
06:34 - strongly recommend if you can to go in
06:35 - person it's less stressful if you have
06:37 - the opportunity if you can't you have to
06:38 - do your home it's up to you which one
06:40 - you want to choose they're more or less
06:41 - the same
06:43 - i like
06:45 - pearson vue but some people like psi so
06:47 - it just depends if you take a couple
06:49 - you'll decide which one you like and if
06:50 - you're wondering what the word proctor
06:51 - means it means there's a supervisor
06:52 - somebody is monitoring you as you taste
06:55 - taste uh take the exam to make sure
06:56 - you're not cheating that's the whole
06:58 - idea behind it to make sure these are
06:59 - legitimate uh scores so what does it
07:02 - take to pass example you're going to
07:04 - have to watch video lecture memorize key
07:05 - information you have to do hands-on labs
07:08 - and follow along within your own account
07:11 - you i would recommend paid exams to
07:13 - simulate the real exam i'm going to help
07:15 - you out by giving your first exam for
07:17 - free no credit card required just sign
07:19 - up on exam pro and um you know you can
07:22 - go get access to that and other free
07:24 - additional content that i strongly
07:26 - recommend that you do
07:27 - for the content outline we got five
07:29 - domains each domain has its own
07:30 - weighting that determines how many
07:32 - questions in a domain will show up and
07:33 - azure is interesting because they do a
07:35 - range of questions so it's not a
07:37 - guaranteed of a certain amount of
07:38 - questions on exam it's going to be
07:39 - between ranges so the first developer or
07:41 - sorry develop azure compute solutions
07:44 - develop azure storage implement azure
07:46 - security monitor troubleshoot and
07:48 - optimize azure solutions connect and
07:50 - consume to azure services and
07:51 - third-party services in terms of the
07:53 - grading you've got to get about 70
07:54 - percent to pass
07:56 - and they use scaled scoring so
07:58 - uh you know it's not always exactly on
08:00 - the dot but you for the most part if you
08:02 - get 70 percent you should pass in terms
08:04 - of um amount of questions is between 40
08:06 - to 60 you probably see 55 questions so
08:08 - you can afford to get about 12 to 18
08:10 - wrong some questions are worth more than
08:12 - one point there's no penalty for wrong
08:14 - questions some questions cannot be
08:16 - skipped and for the formatted question
08:17 - you've got multiple choice multiple
08:19 - answer drag and drop hot area case
08:21 - studies all sorts of kinds
08:23 - of questions that you'll encounter much
08:26 - more difficult than the az 900 for sure
08:28 - in terms of the duration you get three
08:29 - hours so that's about one minute per
08:31 - question but of course different
08:32 - question formats are going to be
08:34 - different uh you have 180 minutes for
08:36 - the exam time your seat time is 210. so
08:38 - you have about um 30 minutes um in terms
08:41 - of the whole time that you said schedule
08:43 - so even if you have 180 minutes you have
08:45 - to consider the entire time to get
08:47 - logged in and all that other stuff so
08:49 - time to review instructions reading
08:50 - accept the nda complete the exam provide
08:52 - feedback at the end and i'm telling you
08:54 - if you're taking this online
08:55 - show up early because so often you're
08:57 - fiddling around with your license to try
08:59 - to get to scan properly and then they
09:01 - don't like it so you have to scan it
09:02 - twice so you know if you can show up an
09:05 - hour early and make sure you block that
09:07 - time
09:08 - this exam is valid for i believe two
09:10 - years before recertification so you know
09:13 - there you go
09:15 - [Music]
09:20 - hey this is andrew brown from exam pro
09:22 - and before we dive into azure functions
09:25 - we need to understand what is serverless
09:28 - so serverless architecture generally
09:30 - describes fully managed cloud services
09:33 - and i say generally because um you know
09:36 - that definition of serverless can be
09:37 - highly contested about what can be
09:40 - serverless and what can not be
09:41 - serverless so it's not a boolean answer
09:43 - of yes or no but some services are more
09:47 - uh serverless than others to a degree
09:50 - and so you know the way to help you
09:52 - understand serverless is the way i
09:53 - define it is i look at the following
09:55 - characteristics one thing for a
09:58 - cloud service to be serverless is that
10:00 - it should be highly elastic and scalable
10:02 - highly available highly durable and
10:05 - secure by default another thing is that
10:08 - it abstracts away the underlying
10:09 - infrastructure and are built based on
10:11 - the execution of your business task so
10:14 - you're not really worried about how many
10:16 - cpus and things like that maybe they are
10:18 - abstracted into like
10:20 - virtual cpus or
10:22 - away from what the original
10:25 - hardware software is using the idea is
10:27 - that it's a simpler value then the idea
10:30 - is that serverless can scale to zero
10:32 - meaning that when it's not in use you're
10:34 - not paying for anything and the most
10:35 - important thing is you're paying for
10:37 - value so you do not pay for idle servers
10:41 - and just to kind of reinforce that idea
10:43 - that um
10:44 - that uh it's a degree uh my friend
10:47 - daniel who's really big in the service
10:49 - loves to describe it as like the energy
10:51 - rating system so the idea is that uh
10:54 - when you go to buy an appliance such as
10:56 - um you know a washer or dryer they'll
11:00 - tell you how energy efficient it is and
11:02 - that's kind of the idea behind
11:04 - serverless services and uh you know
11:07 - we're going to be talking about function
11:08 - as a service but function of service
11:10 - does not necessarily guarantee that it's
11:11 - a serverless service but we'll talk
11:13 - about that in the next slide okay
11:15 - [Music]
11:19 - all right let's talk about function as a
11:21 - service also known as fast so here's
11:23 - kind of a diagram that i like to use to
11:26 - visualize it but we'll get to it in a
11:28 - moment here so what is function as a
11:30 - service it allows developers to focus on
11:32 - just writing pieces of codes also known
11:34 - as functions and it has event driven
11:36 - integration trigger functions based on
11:38 - event data or to emit event data so it's
11:41 - not just a matter of having a piece of
11:43 - function or code that you write in a box
11:45 - but the fact that it has to be event
11:47 - driven
11:48 - generally multiple functions are
11:49 - orchestrated together to create a
11:50 - serverless application sometimes also
11:52 - known as microservices functions
11:55 - generally only only run when needed and
11:57 - so function as a service is not
11:59 - serverless on its own
12:01 - so fast function as a service it's only
12:03 - serverless if it's fully managed and it
12:05 - scales to
12:07 - zero just to take a closer look at this
12:09 - graphic here so the idea is that if
12:12 - we're talking about the underlying
12:13 - infrastructure it's of course running on
12:15 - some kind of physical server
12:17 - and from there you'll have a host
12:19 - operating system and then you it could
12:21 - be a hypervisor
12:23 - and then from there you could have a
12:25 - virtual machine running a container
12:26 - runtime like the docker daemon um or you
12:30 - know it could be
12:31 - there are some
12:32 - os's that are optimized to run a
12:34 - container runtime without a hypervisor
12:37 - but the important part is the idea is
12:39 - that you're just deploying these little
12:40 - pieces of functions and they generally
12:42 - will go into a container so you might
12:44 - have a
12:45 - container runtime
12:48 - that is specifically configured for
12:50 - ruby or it might be configured for java
12:53 - or python or net and then the idea is
12:56 - that you're dropping that code and that
12:57 - code's getting
12:58 - scheduled into that container
13:01 - but hopefully that gives you kind of the
13:02 - general idea of what function as a
13:04 - service is
13:08 - [Music]
13:09 - hey this is andrew brown from exam pro
13:11 - and we are taking a look at azure
13:12 - functions and this is a function as a
13:14 - service fast offering that allows
13:17 - developers to focus on writing code and
13:19 - not worry about maintaining the
13:20 - underlying computing infrastructure and
13:23 - so here is our
13:24 - visualization so we can kind of break
13:26 - down the anatomy of how azure functions
13:29 - work so the first thing you'll need is
13:30 - an function app and this defines the
13:32 - underlying compute for a collection of
13:34 - functions so a function act defines the
13:36 - hosting runtime and global
13:37 - configurations then you have the
13:39 - functions themselves these represent
13:41 - code along with application runtime
13:44 - configuration so the idea is you can say
13:46 - i want this to be a python function a
13:49 - net function
13:50 - etc etc there's always going to be a
13:52 - trigger so a trigger is the chosen event
13:54 - data that will cause
13:56 - the function to execute and you can only
13:58 - have one trigger
14:00 - you have input bindings these are one or
14:02 - more multiple data sources that will be
14:04 - passed to the function when a trigger
14:06 - occurs so the idea is you can pull in
14:07 - data from a variety of different azure
14:09 - services at the time of trigger which is
14:11 - uh quite nice
14:13 - and then you have the output bindings
14:14 - these are one or more data syncs that
14:16 - will receive outputted data from the
14:19 - function on successful execution they
14:21 - say syncs you could say consumers if you
14:23 - like
14:24 - and also azure functions at least as of
14:27 - today has four different versions you
14:29 - really do want to just use the latest
14:30 - version
14:31 - there could be a version 5 out by now i
14:33 - don't know
14:34 - but you know just be aware that you're
14:36 - always using the latest version but
14:38 - in practicality you probably won't be
14:39 - able to tell the difference between the
14:41 - versions but just make sure you're using
14:43 - the latest okay
14:44 - [Music]
14:49 - all right let's take a look at storage
14:50 - considerations for azure functions
14:52 - because every function app requires a
14:55 - storage account to operate and if that
14:56 - account is deleted your functions are
14:58 - just not going to work so kind of a
15:00 - small visualization of your function app
15:02 - linked to a storage account so azure
15:04 - functions uses the following storage
15:05 - types in the storage account it's going
15:07 - to vary based on use case so for blob
15:09 - storage it maintains binding state and
15:11 - function keys for azure files file share
15:14 - used to store and run function app code
15:17 - in a consumption plan premium plan as
15:19 - your files is set up by default but you
15:21 - can create an app without azure files
15:23 - under certain conditions for cue storage
15:25 - this is used by task tubs in durable
15:27 - functions we have a little section on
15:29 - durable functions and then for table
15:31 - storage used by task hubs and durable
15:32 - functions as well so there you go
15:38 - [Music]
15:40 - okay let's take a look here at the
15:42 - anatomy of a single azure function so
15:45 - here is a screenshot of visual studio
15:47 - code because this is where you're going
15:48 - to be writing your functions you can
15:50 - write them somewhere else i sure don't
15:51 - know where but
15:52 - they have really strong integrations
15:54 - with visual studio code but the idea
15:56 - here is that we have a
15:58 - visual studio code plug-in installed
15:59 - that allows us to
16:01 - manage our remote functions and we have
16:03 - local projects but let's take a look at
16:04 - some of the files that are in here so we
16:06 - can understand what kind of files matter
16:08 - to our functions the first is
16:10 - functions.json this is configuration of
16:12 - a single function defining bindings and
16:15 - we talk a lot about bindings uh in the
16:17 - azure functions section of this course
16:19 - there's the code itself in this case
16:21 - it's a javascript file we have the func
16:25 - ignore that's just like a git ignore
16:27 - file but it's to ignore files like files
16:29 - to be not packaged right so like
16:32 - when it actually deploys the function
16:33 - you don't want those files included but
16:35 - you might use them in local development
16:37 - we have host.json this is global
16:39 - configuration of all functions at the
16:42 - function app level
16:44 - then there's the local project itself
16:45 - this is just where the function is
16:47 - stored locally and a lot of times you're
16:49 - moving that code to uh to the remote
16:54 - storage on azure and if we were just to
16:56 - open that up you can kind of see the
16:58 - same files being mirrored so we have the
17:00 - name of the folder so host json function
17:02 - json index etc local settings json
17:06 - package json
17:07 - things like that
17:09 - so i'll just kind of erase a little bit
17:11 - of this out of here but you get the idea
17:14 - and it'll make a lot more sense when we
17:15 - start
17:16 - making functions which we absolutely do
17:18 - in this course critical for the az204 so
17:21 - but we'll do that
17:22 - soon
17:23 - [Music]
17:27 - all right let's take a look at
17:28 - authorization levels this determines
17:30 - what key if any needs to be present when
17:33 - making a request when a function is
17:35 - getting invoked and the authorization
17:37 - levels can be one of the following we
17:38 - have nominus function and admin so
17:41 - anonymous means that anybody can invoke
17:43 - the function so if it's an
17:44 - https request
17:46 - in order to trigger it anybody can
17:48 - trigger it
17:49 - for function you'll need a specified api
17:51 - key this is the default value when
17:53 - you're creating your functions generally
17:55 - recommended as the means to use or you
17:57 - can have it so that it has to use a main
17:59 - key
18:00 - to be required yes it's called master
18:02 - key i'm just going to use the word binky
18:05 - and for integration if you're trying to
18:07 - figure out how to
18:09 - change the authorization levels after
18:10 - you've deployed a function you're just
18:11 - going to click on your https trigger and
18:14 - from there you can switch the
18:16 - authorization level now that's not
18:17 - always the case there's some cases where
18:19 - you're not able to change it via the
18:21 - portal
18:22 - um and that just has to do with you know
18:24 - the type of runtime you're running
18:26 - whether it's a custom run time
18:28 - but yeah that's where you're going to do
18:30 - it there and you can change it after the
18:31 - fact after creation okay
18:33 - [Music]
18:38 - let's talk about how we would debug our
18:39 - azure function well you have this
18:41 - ability to turn on uh streaming logs and
18:45 - this allows you to see near real-time
18:47 - logging when errors occur and it's not
18:50 - as simple as that because there are two
18:52 - options for streaming a file log the
18:55 - first is built-in logging stream
18:57 - and so the idea is that you have the app
18:59 - service platform lets you view a stream
19:01 - of your application log files and then
19:03 - there's live metrics stream so when your
19:05 - function app is connected to application
19:08 - insights you can view log data and other
19:10 - metrics in near real time in the azure
19:12 - portal using live metrics stream and log
19:15 - streams can be viewed both in the portal
19:17 - and in most
19:18 - cases the local developer environments
19:20 - such as visual studio code now when i
19:22 - was doing this i was trying to turn it
19:24 - on and i did not get any logs in near
19:26 - real time but i think the reason why was
19:28 - because the hosting that i had it backed
19:30 - on wasn't on app service platform so i
19:32 - think it really does matter where you
19:34 - host it um and so you know maybe we'll
19:37 - explore that in the follow along to see
19:38 - if we can see your real-time logs
19:42 - but yeah there you go
19:43 - [Music]
19:48 - all right let's take a look at the key
19:49 - concepts for azure functions these are
19:51 - not exactly my words but i will uh
19:54 - provide exceptions here as we describe
19:57 - this stuff but the idea here is that we
19:58 - want to get broader knowledge about
20:00 - azure functions so azure functions are
20:02 - lightweight and can be serverless and
20:04 - they can be and this is only going to be
20:05 - dependent on how you host them and we'll
20:08 - talk about that in a moment azure
20:09 - functions are easier to write and deploy
20:11 - usually write deploy than
20:13 - full web applications uh that can be
20:15 - true the idea though is that you are
20:18 - uh you might have to
20:20 - do less
20:22 - coding in some regards but then you
20:23 - might have to do more in other places
20:25 - like application integration so
20:27 - i would say that it shifts the
20:29 - responsibilities of what you have to
20:31 - work on but you it is better i think
20:34 - azure functions are fast execute because
20:36 - there is no at large application startup
20:37 - time initialization or other events
20:39 - fired before the code is executed now
20:42 - true that large applications there are
20:44 - those downsides to them but the the
20:46 - trade-off here is that if you're using
20:49 - serverless functions then you will be
20:51 - dealing with cold starts
20:53 - um and so
20:55 - cold starts just means that uh you know
20:57 - when you're when you have a function
20:59 - that hasn't been used in a while that's
21:01 - going to have to boot up an environment
21:03 - so
21:04 - you know there is that trade-off
21:06 - now in many cases you can
21:09 - pre-warm or have continuously running
21:11 - compute
21:12 - but if you do that then you're not
21:14 - necessarily leveraging serverless like
21:16 - to be able to scale to zero but you know
21:18 - you do get trade-offs azure functions
21:20 - execution is triggered when an event is
21:22 - fired that is true azure functions do
21:24 - not need any infrastructure and it has
21:26 - zero maintenance well they do have
21:28 - infrastructure it's just that you don't
21:30 - generally have to worry about the
21:31 - infrastructure
21:32 - in terms of azure's implementation of
21:35 - functions you do have to think a little
21:37 - bit more in general about the
21:38 - infrastructure underneath as opposed to
21:41 - google cloud or aws but at least you
21:43 - have a lot of options on
21:45 - azure and it has zero maintenance well
21:49 - the infrastructure that's true but um
21:51 - the your code itself you know if you're
21:54 - using python 2 and microsoft says hey
21:56 - we're going to expire or uh deprecate
22:01 - the
22:02 - python 2 runtime you have to use python
22:04 - 3 well then you're going to have to
22:05 - upgrade that stuff so that is your
22:06 - maintenance so there is management code
22:08 - but
22:10 - that's no different than any other
22:11 - provider azure functions can be built
22:13 - tested deployed in the azure portal
22:14 - using a browser only if you're using
22:16 - windows so uh if you are hosting on
22:19 - linux there are definitely
22:22 - lots of limitations in the azure portal
22:25 - if you're using windows
22:26 - then there's no problem in most cases
22:28 - you should just use windows because
22:30 - you're not going to be able to tell the
22:31 - difference
22:32 - in most cases
22:34 - but if you want to take the full
22:35 - advantage of being able to work in the
22:37 - portal
22:38 - absolutely use windows azure functions
22:39 - are easy to upgrade and doesn't affect
22:41 - other parts of the website again this is
22:43 - subjective based on uh the code that you
22:46 - have to upgrade right so
22:48 - or functionality that they change so
22:51 - you know generally it's true it can be
22:53 - easier in isolate to update parts of
22:55 - your application because they're all
22:57 - little functions but there are
22:59 - exceptions to that azure functions use
23:00 - industry standard protocols probably
23:02 - https here and communicate with other
23:04 - apis databases and libraries that's true
23:06 - you only pay while your functions are
23:09 - running that is true if you use the
23:11 - serverless model if you're continuously
23:12 - running virtual machines behind the
23:14 - scenes then you are going to be paying
23:17 - all the time azure functions
23:18 - automatically scale to meet the demand
23:19 - of traffic that is true underneath azure
23:21 - will provision more service containers
23:23 - that is true azure function scale
23:25 - scale to zero cost can depend on your
23:27 - hosting azure function has built-in
23:29 - monitoring via azure monitor which it
23:31 - absolutely does and it can integrate or
23:33 - has built-in ci cd via azure devops
23:35 - which is very nice azure functions are
23:37 - event driven and will be triggered based
23:39 - on event data and emit event data
23:42 - absolutely and if they didn't then it
23:44 - wouldn't be functioned as a service but
23:46 - there you go
23:50 - [Music]
23:51 - all right let's take a look here at use
23:52 - cases both business and technical for
23:54 - azure functions so for business use
23:56 - cases you can use them for scheduled
23:58 - tasks reminders and notifications
24:00 - lightweight web apis sending background
24:03 - emails running background backup tasks
24:06 - doing back-end calculations there are
24:08 - technical use cases such as sending
24:10 - emails starting uh
24:12 - starting a backup ordering processing
24:14 - task scheduling database cleanup send
24:17 - notifications messages iot data
24:19 - processing so azure functions are best
24:21 - suited for smaller apps that have events
24:24 - that can work independently of other
24:25 - websites originally i had some text in
24:27 - here that said like azure functions were
24:29 - not suited for large applications but
24:31 - more of these menial
24:34 - jobs around a larger application which
24:37 - is not true you can build a whole
24:39 - application out of functions
24:41 - now whether that's easy to do with azure
24:43 - functions is another story because it
24:45 - has to do with cold starts and things
24:46 - like that but
24:47 - i i definitely think you can but this is
24:49 - just kind of give you creative ideas of
24:50 - how you can use azure functions outside
24:52 - of the normal use case of just building
24:54 - a web application so there you go
24:59 - [Music]
25:00 - all right let's take a look at vs code
25:02 - extension because this is going to be
25:04 - the primary way that you are going to be
25:05 - working with azure functions so in order
25:07 - to productively work with azure
25:08 - functions you'll need the visual studio
25:10 - code
25:12 - azure functions extension and so what
25:14 - you'll do there is install that once
25:16 - it's installed and you have an azure
25:18 - icon
25:19 - in your activity bar with the side bar
25:22 - drawer you can look for functions i'm
25:24 - going to get my pen tool out here so the
25:25 - idea is you install this
25:27 - and then now you have at the top here
25:28 - the functions
25:31 - tab so you can manage azure functions
25:32 - it's possible to use azure portal to
25:35 - create and update windows host hosted
25:37 - azure functions but in practice it's not
25:39 - an easy experience with linux it's
25:41 - impossible
25:42 - this is a very kind of different
25:44 - experience compared to something like
25:46 - aws or google cloud where you can do
25:48 - basically
25:50 - everything in the cloud
25:51 - but with azure functions you really
25:53 - really really rely on visual studio code
25:56 - you can use visual studio for net and
25:59 - c-sharp app functions i've never done it
26:02 - myself but it is an option out there so
26:04 - there you go
26:05 - [Music]
26:09 - let's talk about runtime so what is an
26:11 - application runtime an application
26:13 - runtime is a compute environment
26:14 - configured with the necessary software
26:16 - and system configuration to run a
26:18 - specific type of application code so
26:21 - here
26:21 - uh if we were to create an azure
26:23 - function we have some options and you
26:25 - can see runtime stack so azure provides
26:27 - multiple applications and runtimes for
26:30 - popular programming languages just going
26:32 - to highlight that out there because i
26:34 - don't see my
26:36 - my arrow but uh it has things for uh net
26:39 - so that's c sharp java node.js that's
26:41 - both javascript typescript
26:43 - powershell core python there's my uh
26:47 - there's my nice arrow there um
26:50 - unfortunately does not have ruby at the
26:52 - time of recording this i really would
26:54 - like them to have it but if you want
26:56 - something that's not there what you can
26:57 - do is implement your own custom handler
27:00 - i believe they have an example either
27:01 - for rust or go in the documentation or
27:03 - let's say you want to do ruby
27:05 - that is something that's on the 204 exam
27:07 - but in practice it's really really
27:10 - really hard to get a working custom
27:11 - handler and i say that by
27:14 - i tried to make a ruby one it didn't
27:16 - work i tried their tutorial it didn't
27:18 - work i reached out to the person that
27:20 - wrote the tutorial they couldn't get to
27:22 - work i went to support they couldn't get
27:23 - it to work so in theory custom handler
27:26 - sounds great if you can figure it out um
27:29 - so you know if there isn't a custom
27:31 - handler uh follow along you'll know why
27:34 - um but uh yeah that is an option on the
27:37 - table the runtimes provided by microsoft
27:39 - are just docker containers you can see
27:40 - them on the docker hub um and you know a
27:44 - lot of times you know containers and
27:46 - functions are
27:48 - strongly related because they're
27:49 - generally almost always running on
27:50 - containers okay
27:52 - [Music]
27:56 - all right let's take a look at windows
27:58 - versus linux hosting so when you create
28:00 - an azure function app you can choose
28:02 - either windows or linux as your host and
28:05 - this actually it makes a big difference
28:08 - based on the capabilities that are
28:09 - available to you
28:10 - and we're talking about things where
28:12 - there could be performance differences
28:14 - functionality limitations feature
28:16 - differences but generally you know when
28:19 - we're using microsoft it's always good
28:21 - to attempt to use windows when possible
28:24 - because their whole ecosystem is built
28:26 - around windows and not so much linux
28:29 - one example here
28:30 - is that if you are using
28:32 - a linux hosting for azure function apps
28:35 - you cannot edit the function after it's
28:38 - been deployed and that is a very major
28:42 - inconvenience so just consider that
28:44 - when deploying functions i usually can't
28:46 - tell
28:47 - much the difference between linux and
28:49 - windows so i'll just choose windows so i
28:51 - get that additional functionality if you
28:53 - need to determine the exact os that is
28:56 - being used what you can do is go to
28:58 - docker hub repository because again all
29:00 - these
29:01 - uh all these images or
29:04 - runtimes are stored on docker hub and
29:06 - there we can see the windows images and
29:08 - the linux images
29:10 - and notice for linux there is a lot more
29:13 - variation where windows is just the nano
29:15 - server 1809 so that's the windows server
29:18 - 1809 whereas linux there's a little bit
29:21 - more options and mostly debian but you
29:23 - do have one down here that's ubuntu but
29:25 - there you go
29:26 - [Music]
29:31 - all right let's take a look at templates
29:33 - for azure functions so azure provides
29:35 - function templates to get you started
29:36 - with common functionality scenarios this
29:38 - is going to happen when you first create
29:39 - your function now you don't have to use
29:41 - one but definitely this is the route
29:44 - that you should want to go especially if
29:46 - you're choosing http trigger which is
29:48 - very common so
29:50 - to choose the template you're going to
29:51 - be doing that in visual studio code so
29:52 - you don't do it in the portal but let's
29:55 - just kind of walk through the templates
29:57 - we have available so we have http so
29:58 - triggered by http request and returns an
30:01 - http request you have a timer so
30:03 - triggered based on a schedule
30:05 - blob storage triggered when files are
30:06 - uploaded updated in a blob storage
30:09 - container
30:10 - cosmos db triggered when processing a
30:12 - new modified cosmodb document
30:14 - cue storage triggered by azure storage
30:17 - queue messages
30:18 - event grid so triggered by
30:21 - event from event grid
30:23 - so many azure services can be triggered
30:25 - as a function through event grid
30:27 - okay as we cover event grid in this
30:29 - course
30:30 - a lot of sources can be ingested into
30:32 - event grid event grid is serverless
30:34 - event bus integrated with azure service
30:35 - as we do cover
30:36 - at other parts intercourse uh event hub
30:39 - so triggered by an event hub event so
30:41 - this is for streaming service bus queues
30:44 - are triggered by a bus queue this is
30:46 - messaging service bus topics the
30:48 - triggered by uh an event from a bus
30:51 - topic so this is a pub sub model then
30:54 - you have send grid so triggered by an
30:55 - email event in a third party service
30:57 - center if you ever send grid it's just
30:59 - for sending out emails
31:01 - and so there's a lot there does it set
31:03 - up the bindings for you probably i never
31:05 - really noticed or checked but i would
31:07 - think that if you were to set one up for
31:09 - blob storage you would get
31:11 - the binding set up for you in your
31:12 - function
31:14 - json but we will
31:15 - look actually next at function json
31:17 - configuration because that is what is
31:19 - next in the course
31:20 - [Music]
31:24 - so for every single function there is a
31:26 - function configuration file called
31:29 - function json and so there's the
31:31 - structure there it defines the functions
31:33 - triggers bindings and other
31:34 - configuration settings when you choose a
31:36 - template you're going to get some
31:38 - default stuff there let's talk about
31:40 - three main things under bindings so you
31:42 - have the type of binding
31:44 - which is just the name of the binding
31:45 - which will affect the other attributes
31:47 - that are underneath often that you'll
31:49 - have a direction like in and out uh and
31:51 - then there's the name and the name is
31:53 - going to vary so for c sharp it's an
31:54 - argument name for javascript it's the
31:56 - key valued list but it's what is used to
31:58 - bound data in the function
32:01 - and we will definitely cover a lot about
32:04 - bindings because bindings is super
32:05 - important
32:07 - but just so you know function
32:08 - configuration file again is for trigger
32:10 - bindings and other configuration
32:11 - settings
32:12 - [Music]
32:16 - another file that you'll find in your
32:18 - function is a hosts dot json and uh this
32:22 - is for configuring global configuration
32:24 - options for all functions within the
32:26 - function app here is an example one uh
32:30 - so
32:30 - as extension for http you can see it has
32:33 - a rope prefix things like that it's
32:35 - setting a custom header and there is a
32:38 - lot of stuff you can configure here for
32:40 - options so we can do aggregators
32:42 - application insights blob console
32:44 - cosmodb extensions
32:46 - and then you know http cues retry send
32:49 - grid a whole host of things
32:52 - and you know not super important for the
32:54 - exam but definitely in practice you will
32:56 - be
32:57 - going this fall uh and configuring
32:59 - things need be but there you go
33:01 - [Music]
33:05 - let's take a look at plan services for
33:08 - azure functions and we have three to
33:09 - choose from consumption function premium
33:12 - and app service plan
33:14 - very different very interesting uh
33:17 - options for you to decide where you want
33:20 - your compute to run and how your compute
33:22 - to run for functions you do not see a
33:25 - bus or google cloud doing that but the
33:27 - flexibility here is very interesting so
33:30 - the first is consumption plan and this
33:31 - would be considered serverless and
33:33 - because it's serverless
33:35 - it can scale to zero and because it can
33:37 - scale to zero that means that uh
33:40 - generally we will have cold starts so
33:42 - you only pay for the time your code or
33:44 - application is running billing is based
33:46 - on the number of executions and the
33:48 - duration of each execution and the
33:49 - amount of memory used just pay while you
33:51 - have functions running and scale it
33:53 - automatically even through
33:55 - long loading times then you have premium
33:58 - plan these are functions premium this is
34:00 - where we have pre-warmed uh pre-warmed
34:03 - compute underneath so the user has
34:05 - designated a set of pre-warmed cases
34:08 - which are already online and ready to
34:11 - react instantly azure provides any
34:14 - additional computing services that are
34:16 - required when your function is running
34:20 - you pay for the constantly pre-warmed
34:22 - instances including any additional
34:24 - instances needed to scale the azure app
34:27 - in and out azure functions host
34:29 - instances are added and removed based on
34:31 - the number of incoming events so
34:34 - um you know the idea there is that
34:36 - you're not waiting for the container
34:38 - runtime to spin up it's already running
34:41 - but it's not going to scale to zero
34:44 - and you know i mean you can still say
34:45 - that it's serverless in some regard i
34:47 - would consider that now being a fully
34:49 - managed service but not necessarily
34:51 - serverless
34:53 - but you know that's debatable the last
34:54 - here we have dedicated plan this is with
34:56 - app service plan and this is where we're
34:58 - doing vm sharing and this one is
35:01 - extremely unique uh you do not see this
35:03 - in any other cloud provider but
35:06 - this is where when you use app service
35:08 - for other apps your functions will run
35:10 - on the same plan virtual machine at no
35:12 - extra cost you may scale it out manually
35:14 - by adding more virtual machine instances
35:17 - for an app service plan you may also
35:19 - have
35:20 - auto scale enabled optional when you
35:23 - have existing underutilized virtual
35:25 - machines which also operate other
35:27 - instances of the app service so i think
35:29 - what happened was
35:30 - um
35:31 - people were using apps app service right
35:34 - platform as a service and they just had
35:36 - un under they weren't utilizing all of
35:39 - the virtual machine compute and so a
35:40 - customer must have said hey
35:43 - can i just
35:44 - take my functions and utilize or utilize
35:48 - the unutilized
35:50 - compute on my plan and that's azure's
35:51 - like
35:52 - sure and they made that a thing
35:54 - so it's just like an interesting way to
35:56 - save money but it is one of those things
35:58 - where it is a little bit hard to
35:59 - understand because it's not uh as
36:02 - isolate or modular in terms of
36:04 - functionality where we see other cloud
36:05 - service providers but those are your
36:06 - three plans so there you go
36:12 - all right let's take a look at triggers
36:14 - and bindings for azure functions we
36:16 - talked about them before but let's give
36:17 - them a little bit more attention so
36:18 - triggers and bindings let you avoid
36:20 - hard-coding access to other services and
36:22 - abstracting away boilerplate code
36:24 - keeping your functions lean
36:26 - because the idea is that you won't have
36:27 - to
36:28 - add that code into your functions so
36:31 - here is our graphic which represents a
36:34 - function we have input bindings output
36:36 - bindings we have a trigger there so what
36:38 - is a trigger a trigger is a specific
36:40 - type of event which causes the function
36:42 - to run
36:43 - it defines how a function is invoked and
36:45 - a function must only have one trigger uh
36:49 - triggers can have associated data which
36:51 - is often provided as the payload of the
36:53 - function and you know as we saw in the
36:56 - introduction that we can have multiple
36:58 - sources of inputs that that get added
37:00 - there in this example here we have http
37:04 - but we're bringing in data from
37:06 - blob storage as well
37:08 - and obviously the main trigger uh which
37:11 - http is an input as well so you can have
37:13 - additional inputs at the same time then
37:16 - we'll take a look here is what is a
37:17 - binding
37:18 - um and so
37:20 - my arrow's a little bit messed up but
37:21 - the idea is binding is defined if your
37:23 - function
37:24 - is connected to another service so we
37:26 - have input bindings and output bindings
37:28 - so the data from bindings is provided to
37:31 - functions as parameters bindings are
37:33 - optional and a function can have
37:34 - multiple input and output bindings
37:37 - optional but often used
37:39 - which we'll find out but let's take a
37:40 - look at what supported bindings are in
37:42 - the next video
37:44 - [Music]
37:48 - all right now let's take a look at what
37:49 - supported bindings are available for
37:52 - azure functions there's quite a few here
37:54 - and we'll quickly go through them just
37:56 - so you have a scope of idea so we have
37:58 - blob storage azure cosmos db
38:01 - um or cosmodb i always want to add that
38:03 - s in there azure sql dapper which is
38:07 - a
38:09 - run time we cover it in the course it's
38:11 - a runtime for
38:12 - distributed applications for
38:14 - microservices event grid event hub uh
38:17 - http web hook that's gonna be the most
38:19 - common one you'll use iot hub kafka
38:22 - mobile apps notification hub that's for
38:23 - push notifications queue storage
38:25 - messaging rabbitmq that is an open
38:27 - source um
38:29 - viewing system sendgrid which is for
38:31 - sending emails service bus which is both
38:33 - queuing and pub sub signal r which we
38:36 - don't really talk about much in this
38:37 - course but it's an open source.net
38:39 - library to send asynchronous
38:41 - notifications to client-side web
38:43 - applications table storage timer uh
38:47 - twilio all right and so what's also
38:50 - important to note and sorry i could not
38:51 - make this graphic better i really really
38:53 - did try
38:54 - to present this better but it's just
38:56 - there's so many here but um the idea is
38:59 - that some don't support triggers so like
39:01 - azure sql does not support triggers
39:03 - mobile apps notifications don't table
39:05 - storage and twilio does not then you can
39:07 - see we have a big gap of inputs for
39:10 - a bunch here and a bunch there
39:12 - it's not super important to know for the
39:14 - exam but the fact that that
39:17 - you don't get triggers inputs and
39:18 - outputs across all services and there
39:20 - are some cases where the api version
39:22 - matters so like if you are using
39:25 - version one some things are not going to
39:27 - be supported and even some cases version
39:29 - two like mobile apps notifications don't
39:31 - have
39:32 - um uh don't have support so in some
39:35 - cases you do have to use uh version one
39:38 - functions
39:39 - but most cases you'll be using version
39:41 - four so hopefully that gives you kind of
39:42 - a broad idea of supported bindings but
39:45 - we're not done with bindings we'll be
39:46 - looking at more of it okay
39:47 - [Music]
39:51 - let us talk about binding directions so
39:53 - all triggers and bindings have a
39:55 - direction property in the function json
39:57 - file the direction of the triggers is
39:59 - always in and the input output binds
40:01 - either is in and or out and some
40:04 - bindings support a special direction
40:06 - called in out
40:08 - which a little bit confusing but it's
40:10 - nice to have a extra option this idea is
40:13 - we have a direction and triggers again
40:15 - are always in
40:16 - and then you have in out and in and out
40:19 - um the trigger is defined alongside the
40:21 - input and output binding so trigger will
40:23 - have the same as the input type but with
40:26 - trigger appended so for example if we
40:29 - have blob that would be our input
40:31 - binding then we'd have blob trigger as
40:33 - the trigger and that's how you would
40:35 - know the difference right so again just
40:37 - iterating see where it says
40:39 - uh a type here if it was just http
40:42 - then it would just be a
40:44 - a binding but if it's a http trigger
40:46 - then it's a trigger with a with also an
40:49 - in binding
40:50 - if you use the in and out
40:52 - only the advanced editor is available
40:54 - via the integrate tab in the portal just
40:57 - so you know because there's a
40:58 - visualization of integrations that we
41:00 - see in the azure portal and so they're
41:03 - talking about that in and out option
41:04 - there okay
41:06 - [Music]
41:10 - okay so now what i want to do is just go
41:12 - through a few scenarios of triggers and
41:14 - bindings to help cement the idea of
41:16 - their utility so we'll take a look at
41:19 - our first scenario so every hour you
41:21 - want to read a new log file delivered by
41:23 - your application and you'll need to
41:25 - transform the data to be ingested
41:28 - in your nosql or sql database that
41:30 - resides in cosmodb
41:33 - so here is our visualization we have our
41:36 - function which is python for fun
41:39 - it's going to output the cosmodb so we
41:40 - need a binding out to there
41:42 - it has a
41:43 - trigger um here that's just triggered by
41:46 - an acp request to the timer and we got
41:48 - the blob storage
41:50 - so you use a trigger timer i guess
41:52 - actually the http shouldn't even be
41:54 - there i think that's just a mistake
41:56 - but you use a trigger type timer because
41:58 - it's a scheduled job that will run at a
42:00 - specified time the trigger will be in
42:02 - for the blob storage and out for cosmodb
42:05 - because whenever function runs it'll be
42:07 - on a timer and when it executes it'll
42:09 - read data from blob storage process the
42:12 - data from blob storage and then write
42:14 - some data in cosmo db so the key thing
42:16 - here is that this runs on a schedule
42:18 - you're not you're not
42:20 - even though it's there you're not uh
42:22 - invoking it like with a an api request
42:24 - this is invoked on schedule let's take a
42:26 - look at scenario two every time someone
42:29 - signs up to your application you want to
42:31 - trigger an email so here there is an
42:33 - http request coming in and it's going to
42:35 - send out to sendgrid to send out that
42:36 - email so you want to develop an api that
42:38 - allows you to send an email after
42:40 - request is received
42:43 - and you'll use http trigger because it's
42:45 - an api that will be triggered based on
42:46 - this request for the bindings you uh you
42:49 - won't be accessing any data well the
42:50 - function starts so the in direction is
42:53 - none
42:54 - and you'll be uh you'll use send grid
42:55 - for the out direction which allows you
42:57 - to send messages via email looking at
42:59 - our third scenario consider a scenario
43:01 - in which you're using a cue service and
43:03 - you want a function to process a storage
43:06 - message in one queue and enter a new
43:08 - message in another so the idea is you
43:10 - have a queue again it's fun it's
43:12 - triggering something and it's going to
43:14 - output to something so in this case
43:17 - you'll want to use um
43:19 - into it says consider uh scenario q
43:22 - service where you want to function the
43:23 - storage message in one queue and enter
43:26 - in another so probably
43:28 - this this icon's old but we probably
43:30 - should have this icon repeated twice
43:32 - because this is the sendgrid logo so
43:34 - just imagine that we have we took this
43:36 - icon and we put it over here very sorry
43:38 - for that uh graphical error but um the
43:41 - idea here in this case you want a
43:43 - trigger type of queue and a binding type
43:44 - of cue for the direction out because
43:46 - you're not accessing the queue in the in
43:48 - direction you read the data from one
43:50 - queue and process it to create a new
43:51 - message and then to write to a different
43:53 - queue which may or may not be connected
43:55 - to another service or function so there
43:56 - you go
43:57 - [Music]
44:01 - all right let's take a look here at
44:02 - bindingexpressions so in function json
44:05 - file and the function parameters code
44:07 - you can use binding expressions that
44:08 - resolve to values from various sources
44:11 - so most expressions are identified by
44:13 - wrapping them in curly braces i say most
44:15 - because there's an exception where you
44:16 - do not use curly braces but the idea is
44:18 - that
44:19 - it allows you to have kind of dynamic
44:22 - content within your functioned
44:24 - json file and there are a variety of
44:27 - different binding expressions we have
44:29 - app settings trigger file name trigger
44:30 - metadata json payloads
44:33 - guid current date and time
44:35 - let's go take a look at what those look
44:37 - like the first is app settings so when
44:39 - you want to change configuration based
44:40 - on the environment you're going to use
44:41 - percentage science this is the only case
44:43 - where you use percentage signs
44:45 - uh it's confusing because it's the first
44:46 - example instead of curly so notice that
44:48 - there's curly sorry percentage signs
44:50 - there
44:51 - um
44:52 - and then for trigger file name this can
44:54 - be used to change the path of a file
44:55 - name works for both in and out
44:57 - directions so here we have
44:59 - the curly's here to say for the file
45:01 - name
45:02 - for trigger metadata many triggers
45:04 - provide additional metadata values these
45:06 - values can be used as input parameters
45:08 - for c sharp f sharp or properties for
45:11 - context bindings and objects in
45:13 - javascript so for example if we're doing
45:15 - azure queue storage sugar supports the
45:17 - following properties q trigger dq
45:20 - id etc etc
45:22 - uh and then you'll notice that they are
45:23 - available
45:25 - here's the cq trigger is here q trigger
45:27 - that's typed in camel case mode
45:30 - for json payloads when a trigger payload
45:32 - is json that means the the the data
45:35 - being passes json you can refer to its
45:37 - properties and configuration of other
45:39 - bindings and the same function in the
45:40 - code
45:41 - so you know if the payload was blob name
45:44 - and then hello helloworld.txt
45:47 - do blob name and that would put that
45:49 - here
45:50 - okay um and
45:53 - if some of your properties and your json
45:55 - payloads are objects you can use the
45:57 - dot notation so that's just a common
45:59 - thing for json or
46:01 - yeah json or javascript um for for if
46:04 - you want a globally unique identifier
46:07 - you can just do rand guid and you'll get
46:10 - something that looks like that if you
46:11 - want to have the current date and time
46:13 - you do date time you're going to get the
46:15 - current date and time in this format so
46:17 - there you go
46:17 - [Music]
46:22 - let's take a look at local settings file
46:24 - this is a file we saw uh when we were
46:26 - looking at the anatomy
46:28 - of the local files
46:30 - and so the local settings file stores
46:32 - app settings and settings used by your
46:34 - local development tools this file is
46:36 - called
46:37 - local.settings.json is expected to be at
46:39 - the root of your project folder because
46:41 - the local settings json may contain
46:43 - secrets such as connection strings you
46:44 - should never store this in a remote
46:46 - repository so make sure you add it to
46:48 - your dot git ignore file here's an
46:50 - example of one so first we have is
46:52 - encrypted so when the setting is true
46:54 - all values encrypted with the local
46:56 - machine key
46:58 - for values these are a collection of
47:00 - application settings used when running a
47:02 - project so notice we have function
47:04 - worker runtime azure web storage uh or
47:07 - job storage mybinding connections things
47:09 - like that
47:10 - the host customize the functions host
47:12 - process when you run it locally
47:15 - and then connection strings used only by
47:17 - frameworks that typically get connected
47:18 - strings from the connection
47:20 - strings area there okay
47:26 - okay so as your function core tools lets
47:28 - you develop and test your functions on
47:30 - your local computer from your command
47:32 - prompt or terminal so it's a cli
47:34 - uh and you type in func and allows you
47:37 - to do a whole host of things so let's
47:39 - take a look at the commands we have for
47:41 - top level commands we have init so
47:43 - create a new function project in a
47:44 - specific language logs get the logs for
47:46 - functions running in kubernetes cluster
47:49 - new creates a new function in the
47:51 - current project based on a template run
47:54 - enables you to invoke functions directly
47:55 - which is similar to running a function
47:57 - using the test tab in the azure portal
48:00 - start starts the local runtime host and
48:03 - loads the function project in the
48:04 - current folder deploy which was replaced
48:06 - with function kubernetes deploy
48:09 - then we have command groups
48:12 - that contain their own set of sub
48:13 - commands so we have
48:15 - func azure so this is when you're
48:16 - working with azure resources durable for
48:18 - durable functions extensions for
48:20 - managing extensions if you're doing
48:22 - working with kubernetes
48:24 - and azure functions you can do that if
48:26 - you are messing with settings that you
48:28 - do settings if it's with templates for
48:30 - this available templates you can do that
48:32 - so there you go
48:36 - [Music]
48:37 - all right let's talk about custom
48:38 - handlers these are lightweight web
48:40 - servers that receive events from
48:41 - function hosts
48:43 - and they can be written as long as it
48:45 - supports http primitives and these are
48:48 - really great in
48:49 - situations where you want to have a run
48:51 - time that is not supported for like a
48:53 - language like go rust ruby
48:56 - uh or a runtime environment for a
48:58 - specific technology where you need a
49:00 - bunch of libraries preloaded like ruby
49:02 - on rails or deno
49:04 - and so within your function
49:06 - function json the idea here is you'll
49:08 - have a custom handler section and this
49:10 - is going to just basically define the
49:12 - executional execution
49:14 - path because it basically is running a
49:16 - program custom handlers you can use
49:17 - triggers and inputs and output bindings
49:19 - via the extension
49:21 - bundles
49:22 - okay so let's kind of visualize that so
49:24 - we really understand that this is like a
49:25 - separate thing that's running
49:27 - so the function will pass along the
49:28 - payload to the custom handler the
49:31 - communication between the function and
49:32 - custom handler is via https requests and
49:35 - response the custom handler runs on
49:38 - white lightweight web servers
49:40 - the underlying compute will likely vary
49:42 - based on what service plan is chosen so
49:46 - it's going to be other places i couldn't
49:47 - really figure out where they were
49:49 - the custom executable for the custom
49:52 - handler is bundled along with your
49:54 - function code
49:56 - so there's that there uh some some
49:58 - things about the application structure
50:00 - to implement a custom handler your
50:01 - application must have the following a
50:03 - host json file a local settings json
50:05 - file a function json file
50:08 - uh a command script or executable which
50:11 - will run on the web server
50:13 - uh and then the following diagram shows
50:15 - how these files look on the file system
50:17 - for a function named myq and a custom
50:19 - handler
50:20 - so here you can just kind of see
50:22 - it's not the prettiest but i mean it
50:24 - gets the point across the idea is you
50:26 - have a folder here's your function json
50:28 - and this one happens to use an
50:29 - executable in practice custom handlers
50:32 - are super super hard to do
50:34 - uh and i think i mentioned this before
50:36 - but you know i was trying to set up a
50:37 - ruby one
50:39 - and it didn't work and i try to set up
50:41 - the the tutorial one that's in the docs
50:43 - for rust or go that didn't work i
50:45 - reached out the person that made it that
50:47 - didn't work i reset the azure support
50:49 - they didn't know how to do it so um if
50:51 - people running custom handlers if anyone
50:53 - figures it out please tell me uh
50:54 - hopefully i do figure it out and we get
50:56 - included in this course if we don't just
50:58 - understand if i couldn't do it then i
51:00 - don't it's not a big deal if you don't
51:02 - know how to do it okay but for me i
51:04 - personally would have loved to
51:06 - run ruby because that's my favorite
51:08 - language to use
51:09 - and that's what i would use on azure
51:11 - functions
51:13 - [Music]
51:16 - okay so let's say you are making a
51:19 - custom handler you deploy it and you end
51:21 - up having an error that says runtime is
51:23 - unreachable what do you do well not a
51:25 - whole lot but i can i can tell you
51:27 - that there are some things that you can
51:29 - narrow down because a runtime could not
51:30 - be running for multiple reasons
51:32 - um so
51:34 - uh you know the idea is that the runtime
51:36 - couldn't start and so there are some
51:38 - common reasons that azure suggests that
51:40 - it could be so it could be storage
51:41 - account was deleted storage account
51:43 - application setting was deleted storage
51:44 - account credentials are invalid storage
51:46 - account is accessible daily execution
51:49 - quota is full
51:50 - app is behind a firewall so there are
51:53 - your possible reasons for things to go
51:55 - wrong uh i just wanted to share that you
51:57 - know i ran into this and i i didn't
52:00 - necessarily get through it but i i
52:01 - wanted to share it to you possible
52:03 - solutions okay
52:05 - [Music]
52:09 - hey this is andrew brown from exam pro
52:10 - and we are taking a look at durable
52:12 - functions
52:13 - so durable functions is a serverless
52:15 - compute extension of azure functions
52:17 - that allow you to write stateful
52:19 - functions the extension introduces two
52:21 - types of functions so we have
52:22 - orchestrator functions this defines
52:24 - stateful workflows implicitly
52:26 - representing state via control flow and
52:28 - entity function which manage the state
52:30 - of entity explicitly representing state
52:33 - they define workflows in code so no json
52:35 - schema or designers are needed but
52:37 - honestly i would like one because other
52:40 - providers like aws have like no code
52:44 - solutions so it's kind of weird they try
52:46 - to make it like sound like an advantage
52:48 - but it isn't really an advantage
52:50 - they can call out functions
52:51 - synchronously and asynchronously uh the
52:53 - output from uh
52:55 - from the called functions can be saved
52:57 - to local variables they automatically
53:00 - checkpoint their progress whenever the
53:01 - function awaits local state is never
53:03 - lost if the process
53:05 - if the process recycles or the vm
53:08 - reboots
53:09 - so durable functions currently support
53:10 - the following languages so we have for
53:12 - c-sharp javascript python
53:15 - f-sharp
53:16 - powershell and powershell seven to use
53:18 - durable functions you need to install a
53:20 - library specific to your language in the
53:21 - root of your function app project such
53:24 - as npm install durable function so there
53:26 - you go
53:30 - [Music]
53:31 - all right so if we are going to be
53:33 - having stateful functions we are going
53:34 - to need a variety of different patterns
53:36 - for
53:37 - setting up
53:38 - different kinds of serverless workloads
53:40 - let's take a look at them the first is
53:42 - function chaining so this is the pattern
53:44 - of executing a sequence of functions in
53:45 - a specific order often the output of one
53:47 - function needs to be applied to the
53:49 - input for another function
53:52 - durable functions allow us to implement
53:54 - this pattern concisely in code then you
53:56 - have fan out fan in so this is this is a
53:58 - pattern of executing multiple functions
54:00 - in parallel and then waiting for them
54:02 - all to finish fanning out can be
54:04 - completed with normal functions by
54:06 - having the function send multiple
54:07 - messages to a queue fanning in is much
54:10 - more difficult because we have to write
54:12 - code to track when the q triggered
54:13 - functions end
54:15 - and store function's output the durable
54:17 - function extension handles this pattern
54:18 - with relatively simple code
54:21 - we have async http api so this pattern
54:23 - addresses the problem of coordinating
54:25 - the state of long-running operations
54:26 - with external clients a common way to
54:29 - implement this pattern is to have an
54:30 - http call trigger the long-running
54:32 - action then redirect the client to
54:34 - status endpoint then they can pull to
54:36 - learn when operation
54:38 - when the operation is complete durable
54:40 - functions provide built-in apis that
54:42 - simplify the code we write for
54:43 - interacting with long-running function
54:45 - executions the monitor pattern refers to
54:48 - a flexible recurring process in a
54:50 - workflow such as polling until certain
54:52 - conditions are met a simple scenario
54:55 - such as a periodic cleanup job can be
54:57 - addressed with a regular time trigger
54:59 - but it is interval but its interval is
55:01 - fixed making managing instance lifetime
55:03 - difficult durable functions enables
55:06 - flexible recurrence intervals task
55:08 - lifetime management and the ability to
55:10 - create multiple monitor process from a
55:13 - single orchestration a little bit more
55:15 - here so we have human interaction so
55:16 - many automated processes involve some
55:18 - form of human interaction humans are not
55:20 - always available and responsive
55:23 - as cloud services which makes invoking
55:25 - humans an automated process tricky so
55:28 - automated process must
55:30 - must allow for this and they often do so
55:32 - by using timeouts and compensation logic
55:35 - then we have aggregators so stateful
55:37 - entities this pattern is about
55:39 - aggregating event data over a period of
55:41 - time into a single addressable entity
55:43 - the data being aggregated may come from
55:45 - multiple sources be delivered in batches
55:47 - or may be scattered
55:49 - over long periods of time the aggregator
55:51 - may need to act on event data as it
55:53 - arrives external clients may need to
55:55 - query the aggregator data so there you
55:57 - go
56:02 - let us take a look at azure functions on
56:04 - kubernetes now i don't think this is on
56:06 - the exam but i just thought it was very
56:08 - interesting a thing that azure functions
56:10 - can do uh and so i just kind of wanted
56:13 - to run quickly through it so you can
56:14 - deploy any azure function app to a
56:16 - kubernetes cluster running on k running
56:19 - k-e-d-a if you don't know kita it is
56:22 - kubernetes event-driven auto-scaling uh
56:25 - service and allows you to set up
56:26 - auto-scaling based on events from
56:28 - various cloud native
56:29 - services and third-party services so
56:31 - in this case
56:33 - it could be for azure azure function
56:35 - apps
56:36 - and so when we were looking at the core
56:39 - uh core tools azure power tools whatever
56:41 - you want to call it um
56:42 - they had an opera they had the option to
56:44 - deploy to uh kubernetes so i'm gonna
56:47 - assume that you'd probably be running an
56:48 - aks azure kubernetes service uh but
56:51 - let's just read through some of the
56:52 - things here so kubernetes-based
56:53 - functions provide the functions runtime
56:55 - in a docker container with event drive
56:57 - driven scaling through
56:59 - kita
57:00 - kita can scale into zero instances with
57:03 - no events occurring and out to n
57:05 - instances so it scales to zero it does
57:07 - this by exposing custom metrics for
57:08 - kubernetes auto scaler so horizontal
57:11 - autoscaler using function containers
57:13 - with kita make it possible to replicate
57:15 - serverless function capabilities in
57:17 - kubernetes cluster these functions can
57:19 - be deployed using aks virtual node
57:22 - features for serverless infrastructure
57:24 - i'm really curious like how they're
57:26 - running and like other things there but
57:28 - that's outside of the scope of this
57:29 - course but you know if you're into
57:30 - kubernetes and i do have a kubernetes
57:32 - course um you know it's just interesting
57:34 - to see that that is a functionality
57:36 - there and so i just wanted to call that
57:37 - out for our kubernetes fans okay
57:40 - [Music]
57:48 - hey this is andrew brown from exam pro
57:50 - and we are looking at azure virtual
57:51 - machines which makes it easy for you to
57:53 - choose your own os compute memory and
57:54 - storage and launch a server within
57:57 - minutes
57:58 - all right so let's take a quick look
57:59 - here at vms for azure and this is a
58:02 - highly configurable server that relies
58:04 - on virtualization meaning that you're
58:06 - running a server without having to
58:07 - actually buy and maintain physical
58:10 - hardware that runs it you will still
58:12 - have to manage things at the software
58:14 - layer so you would have to apply your
58:16 - own os system patches and installing the
58:18 - configuration packages but the nice
58:20 - thing is that you're not dealing with
58:22 - that hardware it's just going to work
58:23 - for you and some things i want you to
58:25 - know about azure virtual machines is
58:27 - that the size of the virtual machine is
58:29 - determined by the azure image uh and so
58:32 - the image defines the combination of
58:34 - vcpus memory and storage capacity the
58:36 - current limit on a per subscription
58:38 - basis is 20 vms per region i would think
58:40 - that if you wanted more you could just
58:42 - use another subscription i don't know if
58:44 - they have service limit increase for
58:45 - that but that might be possible as well
58:47 - azure vms are billed at an hourly rate
58:49 - an instance uh or a single instance that
58:52 - you launch is going to have availability
58:54 - of 99.9 percent when you're using
58:56 - premium disk if you have standard disk i
58:59 - have no idea what it is um but it's
59:01 - going to be less than that if you want
59:03 - to get 99.95
59:05 - availability you're going to have to
59:07 - launch two instances within an
59:08 - availability set and you can attach
59:10 - multiple manage this to an azure virtual
59:13 - machine
59:14 - now just to give you uh like a quick
59:17 - visual of what's actually happening when
59:19 - you launch an instance because when you
59:20 - do launch a virtual machine there's
59:22 - going to be other networking components
59:23 - that are going to be created or
59:24 - associated with you and you definitely
59:26 - need to know all these components here
59:28 - so i have this nice diagram we're going
59:30 - to do a quick run through with it and i
59:32 - want you to just know that when you do
59:34 - launch a virtual machine it actually
59:35 - does give you a list of all the
59:37 - components that it creates this is
59:38 - actually very common with most azure
59:40 - services and sometimes what they'll do
59:42 - is they'll put it within a resource
59:44 - group for you so that they're all
59:45 - grouped together which is very nice
59:48 - but let's just run through these common
59:50 - components that you need to know such as
59:52 - the network security group and this is
59:53 - going to be attached to your network
59:55 - interface it acts as a virtual firewall
59:57 - with rules around ports and protocols so
59:59 - that is what's protecting our virtual
60:01 - machine you have the network interface
60:03 - that handles ip protocol so that's how
60:05 - the virtual machine
60:07 - talks to the internet or other things on
60:08 - the network you have the virtual machine
60:10 - itself or the instance that's a the
60:12 - version or the
60:14 - uh instance that is currently running
60:16 - it's going to get a public ip address
60:19 - assigned to it so that's how um people
60:21 - from the internet can actually access
60:22 - the virtual machine
60:24 - and then you have to launch it within a
60:26 - virtual network uh and so that's you're
60:29 - going to have that v-net so you'll
60:30 - either you'll either choose one or
60:32 - you'll be creating one uh during that
60:34 - wizard process so there you go
60:37 - [Music]
60:41 - so now let's take a quick look at the
60:42 - options for operating systems on azure
60:44 - vm and let's just define what an os is
60:47 - uh so the os is the program that manages
60:50 - all other programs in a computer and the
60:52 - most commonly known operating systems
60:53 - are windows mac os and linux and when
60:57 - you want to
60:58 - launch a virtual machine the way you
61:00 - determine what operating system you use
61:02 - is based on the image that you choose
61:04 - okay
61:05 - and
61:06 - azure has a marketplace so they have
61:09 - so many different kinds of operating
61:10 - systems you're going to have every
61:12 - possible option you want and microsoft
61:14 - has also partnered with specific
61:16 - companies to make sure that they're
61:18 - providing you images that are updated
61:20 - and optimized for the azure runtime so
61:22 - let's do a quick run through of these
61:24 - supported or partnered
61:26 - os's so we have seuss we have red hat
61:30 - ubuntu debian freebsd
61:32 - then you have flat card container linux
61:35 - rancher os which is for containerization
61:37 - but nami binami is more like images that
61:40 - have preloaded software on them they're
61:42 - very popular for their bitnami wordpress
61:45 - you have mesosphere
61:46 - and then you have images that have
61:48 - docker with it so you have a lot of
61:50 - options open to you there and i always
61:52 - forget about jenkins and jenkins is on
61:54 - the end there now if you want to bring
61:56 - your own linux version you can totally
61:58 - do so all you have to do is package it
62:00 - into a virtual hard disk or a vhd
62:04 - if you've never heard of vhds uh these
62:06 - are these are just uh virtual hard disk
62:08 - formats that you can create using
62:10 - hyper-v software which would be on your
62:13 - windows machine
62:15 - and just be aware that azure doesn't
62:16 - support vhdx which is a newer format it
62:20 - only uses vhd okay so there you go
62:23 - [Music]
62:27 - so let's take a quick look here at cloud
62:29 - and knit and this is something that's
62:30 - not going to be on your exam but it's
62:31 - something you absolutely should know and
62:34 - need to know because it's an industry
62:36 - standard and it's just something that uh
62:38 - you might like if i didn't show it to
62:40 - you no one's gonna show it to you so
62:41 - let's just get to it and so cloud init
62:43 - is this multi-distribution method for
62:45 - cross-platform cloud instance
62:47 - initialization and it's supported across
62:49 - all major uh public cloud providers so
62:53 - azure aws gcp it's provisioning systems
62:56 - for private cloud infrastructure
62:58 - and bare metal installations so what is
63:01 - cloud instance initialization well this
63:03 - is the process of preparing an instance
63:06 - with configuration data for the
63:07 - operating system and the runtime
63:09 - environment and so uh the idea is that
63:12 - you're going to have cloud instances
63:13 - that are initialized from a disk image
63:15 - and instance data right so the image is
63:18 - whatever the the the vm image is but
63:20 - you're going to have data such as
63:22 - metadata user data
63:23 - and vendor data and i'm not going to get
63:25 - into all the data i just want you to
63:27 - know the one which is the one you'll be
63:28 - working with is user data
63:31 - and so user data is a script that you
63:33 - want to run when an instance first boots
63:35 - uh and so um
63:37 - if you've ever used aws and you launch
63:39 - an instance you'll see a box in the the
63:41 - wizard that says user data and that's
63:43 - what this is uh in azure i don't think
63:46 - they make it as clear i don't think they
63:47 - call it user data but if you're
63:48 - pragmatically doing it with um arm
63:51 - templates when you're doing
63:52 - infrastructure as code
63:53 - that is what you're doing you're using
63:55 - cloud in it underneath i just wanted to
63:57 - make that association to you so when you
64:00 - see that word user data you think cloud
64:01 - init
64:02 - and so cloud in it really only works
64:04 - with linux distributions so um
64:08 - it should work with all the linux
64:09 - distributions on
64:12 - azure i'm pretty sure it's not being
64:14 - used for windows machines but there you
64:16 - go
64:17 - [Music]
64:21 - azure virtual machines come in a variety
64:24 - of different sizes that are optimized
64:25 - for specific use cases and the way azure
64:29 - groups these are into types so that
64:31 - would be like general purpose compute
64:32 - optimized and sizes
64:35 - and sizes is kind of a weird word
64:37 - because i've seen in other places in
64:38 - their documentation called series or sku
64:41 - family so you'll see me using those
64:43 - three terms interchangeably they
64:45 - definitely confused me when i was
64:46 - looking at it at first
64:48 - but let's just go quickly through some
64:50 - of them there's a lot of them so they're
64:52 - not going to be all of them on the list
64:53 - here
64:54 - but we'll just quickly go through it so
64:55 - the first is general purpose these are
64:57 - balanced cpu to memory ratio testing
65:00 - development small to medium databases
65:01 - and low to medium traffic web servers
65:04 - this is pretty much what you're going to
65:05 - be using unless you work for a company
65:07 - that has a lot of money and the one
65:08 - you're going to see the most is called
65:10 - b1 because it's super super cost
65:12 - effective
65:14 - and that's the one i'm always using when
65:15 - i'm launching linux machines you'll see
65:16 - me using it through
65:19 - follow alongs here in this course the
65:20 - next one here is compute optimize so
65:22 - that's high cpu uh
65:24 - uh to memory or ratio good for medium
65:27 - traffic web servers network appliances
65:29 - batch processes and app servers so this
65:32 - just has more cpu here uh the most
65:34 - common one is the fsv2 or the f series
65:39 - then you have memory optimized so high
65:41 - memory to cpu ratio great for relational
65:43 - databases being able to hide large
65:45 - caches and in memory analytics
65:48 - and we have a variety of skus there
65:51 - then you have storage optimized so these
65:53 - are high a disk throughput and if and
65:57 - ideal for big data sql no school
66:00 - databases database warehousing and large
66:02 - transactional databases and we just have
66:04 - lsv2 for those
66:06 - then you have the specialized ones for
66:08 - gpu for heavy graphic rendering video
66:10 - editing model training and
66:12 - interferencing
66:13 - with deep learning so available with
66:15 - single or multi multiple gpus and so we
66:18 - have a variety of skus there and then
66:20 - you have your hpcs your high performance
66:23 - compute and these are the the most fast
66:25 - and powerful cpu virtual machines with
66:28 - uh with optional high throughput network
66:30 - interfaces all right and i just want to
66:33 - point out that there are more here um
66:37 - but uh i'm just showing you ones that
66:39 - are occurring so there are previous
66:40 - series um here like we're not showing
66:43 - basic a all right like the a series is
66:46 - not being shown here now when you want
66:48 - to actually go see this full list here
66:49 - it's very easy you just launch up a
66:51 - virtual machine and uh you're going to
66:53 - be able to choose two things you're
66:54 - going to choose the first the image so
66:56 - that's going to be
66:57 - what do you want to run right so it's
66:58 - going to be ubuntu or
67:00 - windows machine and then you're going to
67:02 - choose your standard size image i
67:04 - believe that if you choose certain
67:06 - images not all sizes will be available
67:08 - to you
67:09 - because some images have to be optimized
67:11 - for
67:13 - those uh
67:16 - series sorry i'm just drinking some tea
67:18 - here uh and so there's that b1 series
67:20 - and you can see in canadian dollars it's
67:22 - 9.72 which is a pretty good price for me
67:25 - and you can explore all the costs in
67:26 - azure which is really nice you don't
67:28 - have to go to a separate marketing
67:29 - website you can just open it up and you
67:31 - can just sort by the lowest cost which
67:33 - is what i always do
67:35 - and they have a lot of filters there so
67:36 - there you
67:40 - go let's quickly talk about azure
67:43 - compute units so azure compute units
67:45 - also known as acus provides a way of
67:47 - computing comparing compute so cpu
67:50 - performance across azure skus remember
67:52 - we said skus is the same thing as sizes
67:54 - or series
67:56 - so you'll just see us varying the terms
67:58 - on that
67:59 - acu is currently standardized on a small
68:02 - so standard a1 and you saw prior that a1
68:05 - is a previous series so it's not
68:07 - something that you generally be
68:08 - launching but everything's based off of
68:10 - that one and
68:12 - those are given a value of 100 it's just
68:15 - an arbitrary number to um give it a
68:18 - point against other machines and all
68:20 - other skus then represent approximately
68:22 - how much faster that sku can run a
68:25 - standard benchmark i don't fully
68:27 - understand the math behind uh their
68:30 - their stuff here but i generally get the
68:31 - idea here and i'll just show you quickly
68:33 - a quick comparison so here we have the
68:36 - a1 to a4
68:38 - series
68:40 - against the d1 d14 series a1 is our
68:43 - baseline at at 100 acus
68:46 - and it has a ratio one to one from vcpus
68:49 - to core and so you can see that for d1
68:52 - to d14 it says 160 points to 250 points
68:56 - and so if i did the math here right i
68:58 - think i did uh it's going to be
69:00 - basically 60 to 150 percent more
69:04 - performant than choosing an a1 to a4 so
69:08 - that's how that works so there you go
69:10 - [Music]
69:14 - so here's a neat feature that azure
69:16 - virtual machines allows you to do and
69:17 - you can actually do this with a lot of
69:19 - different services with azure and it's
69:21 - the azure mobile app and so what you'll
69:23 - notice is that when you're using virtual
69:26 - machines they might have this little qr
69:29 - code and if you have the app installed
69:30 - what you can do is scan that code but
69:32 - honestly you don't even need to scan the
69:33 - code if you just have the app installed
69:35 - connect your account you can
69:36 - log in on your phone and check out the
69:38 - virtual machine how it's performing and
69:40 - you can take basic actions there i
69:42 - believe as well you can access the cloud
69:45 - shell so it's pretty darn cool um and so
69:48 - i just wanted you to know about that and
69:49 - if you haven't just go ahead and install
69:50 - it on your phone i think it's available
69:53 - for
69:54 - um android and ios i have an android
69:57 - phone so i didn't check ios but um yeah
69:59 - go check it out
70:04 - so sometimes when you're launching a
70:06 - virtual machine you'll have the option
70:07 - between generation one versus generation
70:09 - two when you're choosing images and i
70:11 - want you to know about uh this stuff
70:13 - here and to understand it we need to
70:14 - know a little bit about hyper-v so
70:16 - hyper-v's is microsoft's hardware
70:18 - virtualization product and it lets you
70:20 - create and run a software version of a
70:22 - computer called a virtual machine and
70:23 - each virtual machine acts like a
70:25 - complete computer running an operating
70:27 - system and
70:29 - programs and so if you're still kind of
70:31 - wondering what hyper-v is if you've ever
70:33 - heard of virtualbox which is used for
70:35 - running vms on mac and linux
70:39 - this is the this is basically the exact
70:41 - same offering it's just windows offering
70:43 - okay and it already comes pre-installed
70:45 - on windows 10. so you might have to
70:47 - toggle something on i can't remember it
70:49 - because it's been a long time since i've
70:51 - had to configure it
70:52 - but the point is you have a windows
70:54 - machine you're already
70:55 - ready to start using hyper-v and hyper-v
70:59 - has two generations of virtual machines
71:01 - so the first one was generation one and
71:03 - this uh this will support most guest
71:06 - operating systems and then generation
71:07 - two supports most 64-bit versions of
71:11 - windows and more current versions of
71:13 - linux and freebsd operating systems and
71:16 - there are these two here but
71:19 - neither one or two is bad so they both
71:22 - have their own use case so it's not like
71:24 - you should always use two
71:25 - and there's a big list online but we're
71:28 - not going to get into that full list
71:30 - but anyway so we see that hyper-v has
71:32 - generation one and two vms but azure
71:34 - also has them as well but they're not
71:36 - one-to-one the same as hyper-v
71:38 - generation so if you look up hyper v uh
71:41 - like the hyper v generation um feature
71:44 - set you might not have that in azure so
71:46 - i wouldn't rely on that list but
71:49 - what i definitely want you to know
71:50 - between azure gen 1 and gen 2 that's a
71:54 - key difference is that gen 1 is
71:56 - bios-based architecture and gen 2 is
72:00 - u-e-f-i
72:02 - based boot architecture and if you've
72:04 - ever built a video gaming machine you
72:06 - definitely know the difference between
72:07 - these two if you don't it's just the way
72:09 - the machine boots up it's the first
72:11 - screen you see
72:12 - um and it's going to change what kind of
72:14 - configuration options you have and you
72:16 - you uh uefi has a lot of great features
72:20 - in it so for instance it has secure boot
72:22 - which verifies the bootloader is signed
72:24 - by a trustee
72:26 - authority and you can have larger boot
72:28 - volumes up to 64
72:30 - terabytes
72:31 - and if you want to know more you'll just
72:33 - have to look up uefi because i'm not
72:35 - covering it in this course i just want
72:36 - you to know that's the the key
72:37 - difference there and hyper vvms are
72:40 - packaged into virtual hard disk formats
72:43 - so that's going to be either vhd or vhdx
72:47 - files okay
72:52 - one thing you might want to be able to
72:54 - do is actually get into your virtual
72:56 - machine and we have a lot of different
72:58 - options available to us in fact we have
73:00 - three ways to connect to our virtual
73:02 - machines we have rdp ssh and bastion and
73:05 - so if you don't know these things we're
73:06 - going to walk through them right now the
73:08 - first is secure shell and this is a
73:10 - protocol to establish a secure
73:12 - connection between a client and server
73:14 - this is how you can remotely connect to
73:16 - a
73:17 - azure virtual machine via a terminal
73:20 - ssh happens on port 22 via tcp port 22
73:24 - is something you should absolutely
73:25 - remember as ssh
73:27 - and you will generally use an rsa key
73:29 - pair
73:30 - to gain access you can also use username
73:32 - and password but most people don't do
73:34 - that anymore
73:35 - another way of getting access to your
73:37 - machine is by using rdp which is remote
73:39 - desktop protocol this is a proprietary
73:42 - protocol developed by microsoft which
73:43 - provides a user with a graphical
73:46 - interface to connect to another computer
73:47 - over a network connection basically what
73:49 - it means is that it's going to open up a
73:51 - window and you're going to be able to
73:53 - see like another windows desktop in it
73:54 - and control it remotely
73:56 - so this is how you can remotely connect
73:58 - windows server to a virtual desktop rdp
74:01 - happens on port 3389 via tcp and udp i
74:05 - think there might be some other ports
74:06 - but this is the port that you need to
74:07 - know
74:09 - i just remember from aws because there
74:10 - were some extra ports i had to configure
74:12 - but
74:13 - that's a different story
74:15 - then you have uh your
74:17 - bastion which is your third option and
74:19 - so azure bastion which is actually an
74:22 - azure service is a service that you can
74:24 - deploy lets you connect to a virtual
74:26 - machine using your browser and azure
74:27 - portal
74:29 - and so the reason why this is so useful
74:31 - is that
74:32 - if you have
74:34 - let's say a chrome desktop
74:36 - which doesn't actually
74:38 - it can't actually use ssh
74:41 - client or can't use the rdb client this
74:43 - just gives you another way of using the
74:45 - browser
74:46 - so you don't have to install those
74:47 - clients but we'll get into that shortly
74:49 - here okay
74:51 - [Music]
74:55 - so let's take a closer look at ssh and
74:58 - the most common way to access your vm or
75:00 - to authenticate yourself uh so that
75:02 - you're allowed to get into your vm with
75:05 - ssh is by using ssh key pair
75:08 - and so the idea is that you're going to
75:10 - have to generate out two keys that's why
75:12 - it's called a key pair there's two keys
75:14 - and so you'd use that command ssh hyphen
75:16 - keygen hyphen t rsa and that is
75:19 - something that is definitely etched in
75:21 - my memory it's definitely something you
75:23 - want to memorize because it'll be with
75:24 - you throughout your entire career and so
75:26 - you're going to have a private key and a
75:28 - public key and so the way these keys
75:30 - work is that the private key
75:33 - should remain on your local system and
75:34 - not be shared with others and the public
75:36 - key is going to be stored on the vm and
75:39 - when you go ahead and ssh you're going
75:41 - to provide your private key
75:43 - as part of the ssh
75:45 - command and it's going to try to match
75:47 - it against the public key to
75:48 - authenticate you and so that is what you
75:51 - normally do ssh hyphen i provide the
75:54 - public key see where it says dot pub
75:55 - that's how you know it's a public key
75:57 - and then there's the actual address of
75:59 - the server um and that's how you're
76:01 - gonna gain access okay
76:07 - so let's take a look at the process of
76:09 - using rdp
76:10 - so the first thing you'll want to do is
76:12 - download the rdb file so when you go and
76:14 - try to connect to that virtual machine
76:16 - you're going to go ahead and download
76:17 - the rdp file that's going to download
76:19 - this rdp uh
76:21 - this file that has rdp extension you're
76:24 - going to double click that file and
76:26 - you're going to be prompted to fill in
76:27 - your username and password this uh this
76:29 - username and password is what you
76:31 - specified when you created your virtual
76:33 - machine and i just want you to know that
76:35 - if you are on a mac
76:37 - you can install the remote desktop
76:38 - client from the apple store
76:40 - but if you're on windows you already
76:42 - have rdp client installed there's
76:44 - nothing extra that you have to do
76:46 - [Music]
76:51 - so let's take a closer look here at
76:52 - azure bastion and this is an
76:54 - intermediate uh hardened instance that
76:57 - allows you to connect to your target
76:58 - server via ssh or rdp and it will
77:00 - provision a web-based rdp client or ssh
77:03 - terminal i'm going to tell you honestly
77:05 - i never really thought of this use case
77:07 - for bastions prior to this
77:09 - but the reason
77:11 - like bastions are good or have still a
77:13 - utility these days is because some
77:15 - people are using a google chromebook and
77:17 - they're not going to have
77:18 - a terminal so they can't use ssh or if
77:21 - they're on windows they can't sell the
77:22 - putty client or if they're on windows
77:24 - they can't
77:25 - they won't have the
77:27 - remote desktop client and so the only
77:30 - way is through the web because a google
77:32 - chromebook's all a browser and so this
77:34 - is one of those utilities for a bastion
77:37 - now bastions don't necessarily are just
77:39 - for this use case they're definitely a
77:41 - very good and secure way to connect to
77:44 - your virtual machines and having a way
77:46 - of auditing who has access to what
77:49 - but this is the use case that azure is
77:50 - putting forward with bastion here and so
77:52 - the idea is that when you want to create
77:54 - a bastion you're going to have to add it
77:55 - to a subnet
77:57 - uh in your v-net or you're gonna have to
77:59 - make a new subnet and it has to be
78:01 - called azure bastion subnet and it has
78:03 - to have at least a size of four slash 27
78:06 - so 32 addresses so that's what you're
78:08 - gonna have to do
78:09 - so um you'll just have to go in and add
78:12 - in that subnet there
78:14 - and then once you do
78:15 - uh you can go ahead and launch your
78:17 - bastion so let's first take a look at
78:18 - rdp uh and so rdp uh we saw how to do it
78:22 - but let's see what it looks like with
78:24 - with azure bastion so what you do is you
78:26 - say connect with azure bastion for your
78:28 - windows machine that's going to assume
78:30 - that you want rdp you're going to just
78:31 - put in your username and password as you
78:33 - normally would and boom you're in your
78:35 - machine that's all it takes let's take a
78:37 - look at how you deal with ssh so
78:41 - you know if you're using a linux server
78:42 - it's going to obviously want to use ssh
78:44 - because you don't have rdp for that
78:46 - and so
78:47 - you can actually use either the ssh
78:49 - private key or the the username and
78:51 - password
78:52 - i
78:53 - recommend using always ssh private key
78:56 - let's take a look at that process so
78:58 - you'll enter your username in and then
79:00 - you can switch over to ssh private key
79:03 - and then
79:04 - the dot pem file which is downloaded
79:06 - locally to your machine
79:07 - which is your private key you can just
79:09 - uh select it and then that will upload
79:12 - it and use it in the comparison and boom
79:14 - you're in your machine so
79:16 - that's just kind of a cool service that
79:18 - they have there
79:23 - let's just do a quick comparison between
79:24 - windows and linux servers this is going
79:26 - to be
79:27 - very obvious for people that are used to
79:29 - running windows workloads but for those
79:30 - who uh
79:32 - grew up on linux i just want to make
79:33 - sure
79:34 - you understand some of the caveats of
79:35 - windows that you might not be used to so
79:38 - obviously azure allows you to run both
79:39 - windows and linux
79:42 - instances or vms
79:44 - and so let's just talk about windows
79:45 - first so when you launch a windows
79:47 - server just like your desktop computer
79:49 - it needs a windows license to run
79:51 - whether it's windows 10 or
79:54 - whatever windows server you're using but
79:57 - i just want you to know that
80:00 - when you launch your your server you
80:02 - don't necessarily have to have the
80:03 - license on hand and so if you launch a
80:06 - server without a license
80:08 - um all it's going to do is say windows
80:10 - is unactivated and you're going to take
80:12 - some manual step to activate that
80:14 - license and i just want to tell you that
80:15 - so that you're not afraid of launching a
80:18 - windows machine because you think you're
80:19 - going to get charged a license fee uh
80:21 - it's not going to happen you have to
80:22 - take some manual intervention to do that
80:24 - so do not worry about launching that
80:27 - windows machine
80:28 - you can bring your own license via a
80:30 - hybrid license so some people like
80:33 - enterprises already own their licenses
80:35 - and they just want to uh they already
80:37 - made a deal with microsoft and so they
80:38 - just want to reuse those instead of
80:40 - using whatever azure provides because it
80:42 - can be cheaper
80:44 - you're going to set a username and
80:45 - password because you're using rdp to
80:46 - access the machine so you're not using
80:48 - your ssh key pairs
80:50 - um
80:51 - windows machines require
80:53 - larger instances okay so
80:56 - if you're running windows 10 or whatever
80:57 - it needs a lot more memory and so you're
81:00 - gonna have to run it on a b2 which is a
81:01 - lot more expensive than a b1 and that's
81:03 - because it runs a full desktop
81:05 - environment all the windows servers do
81:06 - this
81:07 - and so this is generally why i like
81:09 - running linux so
81:10 - just be aware not to keep your windows
81:12 - server running for very long if you
81:14 - don't have a lot of money and you're
81:15 - just learning to pass the certification
81:17 - let's talk about linux really quickly
81:19 - here most versions of linux require no
81:21 - type of license
81:23 - so i think like you can get red hat you
81:25 - might need a license for that if you
81:26 - want support you set either a username
81:29 - password or ssh key pair you can utilize
81:32 - smaller vm sizes because you're not
81:33 - running a full desktop experience
81:36 - could you run
81:38 - like a full desktop experience possibly
81:40 - i haven't ever tried to do it on cloud
81:42 - myself
81:43 - uh unix and like space uh are unix and
81:46 - linux-based systems traditionally are
81:48 - terminal based environments right so
81:50 - you're going to be sshinging into them
81:52 - and you're not going to have something
81:53 - visual okay so there you go
81:55 - [Music]
81:59 - so let's talk about update management at
82:01 - the start of this section for virtual
82:03 - machines i had said that there was some
82:06 - uh
82:07 - requirements ongoing software patches
82:09 - and things that you had to perform on
82:10 - your virtual machines
82:12 - and so that is something that you have
82:14 - to take care of but if you wanted to
82:16 - automate that process this is where you
82:18 - can use update management so that's what
82:20 - it'll do it'll install operating system
82:22 - updates and patches for both windows and
82:24 - linux
82:25 - and not only can it do it for your
82:27 - virtual machines on azure but it can do
82:28 - it on premise and even in other cloud
82:30 - providers
82:32 - okay
82:33 - and the way this is going to work is
82:35 - you're going to go to your operations
82:36 - tab where it says guest plus host
82:38 - updates and you're going to click on
82:40 - update management
82:42 - now update management looks like it's
82:43 - its own service but it's actually using
82:45 - azure automations under underneath
82:48 - and so that's what is
82:50 - installing the agent and the agent is
82:52 - the
82:53 - microsoft monitoring agent mma so that's
82:55 - how it knows what to do you have to get
82:57 - that installed on your machine so update
82:59 - management will perform a scan for
83:01 - update compliance a compliance scan is
83:03 - by default
83:04 - performed every 12 hours on a windows
83:07 - machine and every three hours on a linux
83:09 - machine
83:10 - it can take between 30 minutes and six
83:12 - hours for the dashboard to display
83:14 - updated data from managed computers
83:16 - and i just wanted to point out a little
83:17 - bit about azure automation so because it
83:20 - does more than just obviously update
83:22 - management but azure automation you can
83:24 - enable update management change tracking
83:27 - inventory start stop vms during off
83:30 - hours features for your servers and
83:31 - virtual machines and these features have
83:33 - a
83:34 - dependency on
83:35 - on a log analytics workspace and
83:38 - therefore required linking the workspace
83:40 - with an automation account so there is a
83:42 - little bit extra that we have to have
83:45 - but there you go
83:46 - [Music]
83:50 - hey this is andrew brown from exam pro
83:52 - and we're going to be launching our own
83:54 - bastion using azure bastion services so
83:56 - there's two ways to set this up we can
83:58 - go to bastions over here and create a
84:01 - bastion this way or we can create one
84:03 - after we've created a virtual machine i
84:05 - prefer the latter so let's go ahead and
84:07 - do that and launch ourselves a new
84:10 - virtual machine and so we could either
84:12 - use launch a windows server or a linux
84:15 - server today i'm going to be launching a
84:16 - windows server
84:19 - and what we'll do is go down here i'll
84:21 - make a new group we'll call it the
84:23 - enterprise
84:26 - and as we do that we'll just name this
84:29 - uh enterprise
84:31 - d
84:32 - and we'll launch that in canada centra
84:34 - or us or central us that's fine with me
84:37 - uh 2019 data center gen 2 is totally
84:40 - fine if you go here
84:42 - you're trying to find it you go hit
84:43 - select
84:44 - and we'll choose gen 2 data center it is
84:47 - expensive but uh we're not going to be
84:49 - using this for very long
84:51 - for the username i'm going to put
84:53 - data
84:54 - and for the i'll just make it azure user
84:56 - to make our lives a bit easier and then
84:57 - we'll put testing capital t one two
85:00 - three four five six
85:01 - testing one two three four five six
85:04 - and we will go down below we're fine
85:06 - with the settings here we're gonna go
85:07 - next to disk we're gonna leave the disk
85:08 - to premium that's fine uh we'll let it
85:10 - create a new network that's totally fine
85:14 - management is okay
85:15 - and we'll just actually go hit review
85:17 - and create
85:24 - and now we'll just hit create so that it
85:25 - will go ahead and do that
85:27 - it'll tell us that it's in progress
85:31 - and we'll just wait a little bit here
85:32 - i'll see you back in a moment
85:34 - all right so our instance is ready so
85:36 - let's go ahead and go to this resource
85:37 - here and then on the left hand side
85:40 - you'll have connect and so i'm going to
85:42 - show you and you don't it's not
85:43 - necessary for you to do uh this step
85:45 - because you're going to i'm going to
85:46 - show you how to connect be the bastion
85:47 - but i'm going to go ahead and download
85:49 - this file the rdp file this will only
85:52 - work if you're on windows by the way
85:54 - well i guess it'll work on mac but you'd
85:55 - have to install the um
85:57 - uh the
85:59 - rdp service for the users with windows
86:02 - there and so here this is azure user
86:05 - we're going to type in testing with a
86:06 - capital t just double check that there
86:08 - i'm going to log in make sure that this
86:10 - works
86:11 - uh do we'll do that one more time oh you
86:14 - know it's testing one two three four
86:16 - five six
86:19 - there we go
86:20 - we'll say yes
86:25 - and we'll just make sure that we can uh
86:27 - remote desktop into this just before
86:28 - anything else and there we go so that's
86:30 - all good to me i don't need to see any
86:32 - more
86:33 - we'll go over to bastian we'll say use
86:34 - bastion and this is going to set up a
86:36 - bastion service in order to use bastion
86:39 - you need to have another
86:41 - address space
86:42 - defined for it it makes it really easy
86:44 - to
86:45 - make it here so i'm just going to go
86:46 - 10.0.1.04
86:52 - and we'll go ahead and hit ok
86:58 - and so down below it's going to choose
86:59 - an address space
87:02 - we have a security group um i'm just
87:05 - going to put it for none
87:07 - i don't think i want one on that
87:13 - and if we scroll on down here we have
87:15 - the resource group so we're going to put
87:17 - in the same resource group
87:18 - and we'll go ahead and create that
87:22 - so now before this they didn't have this
87:23 - really nice wizard used to have to go
87:25 - and and create all those things
87:27 - individually in your virtual network but
87:28 - this is really nice it does take a bit
87:30 - of time for this to provision so i'll
87:32 - see you back here in a bit that took a
87:35 - bit of time for that to create that
87:36 - bastion but it is ready to go and so now
87:38 - that we have it we can go ahead and
87:40 - utilize uh this connection here and so
87:43 - right away it i think it's setting up
87:45 - for rdp here so what we'll do
87:48 - is type in azure user
87:50 - and then capital t testing one two three
87:52 - four five six we'll go ahead and hit
87:53 - connect
87:57 - and so notice that i didn't have to use
87:59 - an external application i could just
88:02 - run it in right here
88:04 - it's all in the web browser so that's
88:05 - pretty much how the bastion works i
88:08 - can't remember the pricing on bastion i
88:09 - think it's a little bit of money so i
88:11 - don't want to keep this laying around
88:12 - here but this is great if you let's say
88:14 - you're on a chromebook which are
88:15 - becoming really popular where you can't
88:17 - install native applications uh or you're
88:20 - just having issues because you're on
88:22 - like linux or something like that so
88:23 - there you go that's all there is to it
88:24 - we'll go ahead and clean this up
88:27 - and so i'm just going to go here find
88:29 - the resource group
88:30 - and we'll go ahead and delete i'm just
88:32 - making sure that bastion's within there
88:34 - so it is good
88:40 - and there we go
88:44 - [Music]
88:48 - so we just launched a virtual machine
88:50 - for linux now let's go ahead and launch
88:52 - one for windows i'm going to go to the
88:54 - top here and type in virtual machines
88:56 - we'll go to the first link i'm going to
88:58 - hit add add virtual machine
89:01 - and
89:02 - what we'll do is we'll create a new
89:04 - group the last one i had was called
89:05 - bajor i'm going to call this one
89:07 - cardassia
89:09 - cardassia
89:14 - and i'm going to name this machine also
89:16 - named cardassia
89:20 - and this time what we want to do is is
89:21 - we want to move over to a windows server
89:24 - i find the easiest one to learn with is
89:26 - the windows 10 pro server just because i
89:29 - find these ones a little bit daunting so
89:30 - i'm going to go windows 10 pro
89:33 - and then what we're going to do is go
89:34 - choose a larger size this is not going
89:36 - to work we cannot run a windows server
89:38 - on a b1 ls so we're going to have to go
89:41 - a little bit larger
89:42 - and we don't have to go too much larger
89:45 - here but the idea here is that there's
89:47 - going to be a more expensive spend here
89:49 - so we're not going to be running to keep
89:51 - this running for long but here we have
89:52 - the b2s that is the appropriate size to
89:55 - run this anything smaller i don't think
89:56 - is going to work
89:58 - and we are going to put in a password
90:00 - here so i'm just going to put in
90:02 - cardassia
90:06 - and we'll do cardassio123
90:12 - put a capital on it i guess
90:14 - we'll just do this here
90:18 - and i'm just going to go back and lower
90:20 - case this one
90:24 - and we're going to allow the inbound
90:26 - port of 3389 because that is what rdp
90:28 - needs i'm going to confirm that i have a
90:30 - windows license i actually don't but the
90:32 - thing is you can still launch one for
90:34 - your test purposes it'll just complain
90:37 - saying you're not activated so there are
90:39 - some limitations but it's good enough
90:41 - for us to learn okay and so now that
90:43 - that is all great we'll go next to disks
90:46 - we're going to go with premium or
90:48 - standard ssd this time
90:50 - uh we're going to go ahead and hit next
90:52 - and go to networking it's going to
90:53 - create us a new v-net which is a great
90:55 - idea we're going to let it create a
90:57 - a network security group on the nick
90:58 - just like before we'll go ahead and hit
91:01 - next
91:02 - uh we'll leave all these options alone
91:04 - this all seems fine to me
91:06 - uh and we'll hit next
91:08 - review and create
91:13 - and we'll go ahead and create this
91:14 - server
91:22 - all right and so that's gonna go ahead
91:24 - and create it so i'll just see you back
91:25 - here in a moment when that's done
91:26 - deploying all right so after a short
91:28 - little while here it looks like our
91:30 - windows server is now deployed so what
91:32 - we can do is go to that resource if you
91:34 - wanted to see what it's deployed it's
91:36 - the same stuff as always you have your
91:38 - network interface card your virtual
91:39 - networks
91:40 - nsg the ip address but let's actually go
91:43 - to that resource now
91:45 - and so let's see how we can gain access
91:47 - to this virtual machine and so what we
91:49 - can do is use rdp luckily i am on a
91:52 - windows machine and so
91:54 - i already have the rdb client that i can
91:56 - use so all you got to do is download the
91:58 - rdb file
92:00 - and then once we have that file i can
92:01 - just double click it
92:03 - and i can open this up if you're on a
92:05 - mac you can download the app in the app
92:07 - store and so i'll go ahead and type in
92:09 - my password so my username was cardassia
92:13 - and then my password was capital c
92:15 - a-r-d-a-s-s-i-a
92:17 - one two three we'll hit okay
92:20 - and then it'll give us another one we'll
92:22 - say yes
92:23 - and now we are in our virtual machine so
92:26 - there you go how cool is that
92:28 - i'll just give it a moment to load up
92:30 - but this is a full uh windows 10 pro
92:33 - uh and as i said before you know we
92:34 - don't actually have a license so if
92:36 - you're afraid of spinning it up because
92:38 - you're gonna get charged a license fee
92:40 - uh for windows you do not have to worry
92:42 - that's not gonna happen you have to do
92:44 - some manual intervention uh for that to
92:46 - happen so we'll just wait a little while
92:48 - here for this to load um it is not we're
92:50 - not using the most powerful machine so
92:52 - it does take a little bit of time and so
92:54 - we just hit accept here
92:57 - and here we are so we are on uh
93:00 - we have our nice windows machine here
93:01 - whoops
93:03 - i don't know if it has any games let's
93:05 - go take a look
93:08 - maybe play minesweeper
93:14 - um
93:15 - no maybe maybe you have to download the
93:17 - store i'm not that familiar with windows
93:19 - machines but um so there you go so we'll
93:21 - go ahead and close that and you know if
93:23 - we were using the bastion it's the same
93:24 - process you saw how we used it with ssh
93:26 - but if we had the bastion and it's so
93:28 - much work to set one up we already did
93:29 - that before uh but all you do is enter
93:31 - your credentials in on the page just as
93:33 - we did and it and it's just a lot easier
93:35 - that way uh so let's go ahead and just
93:37 - tear down this machine we're all done
93:39 - with it so i'm just going to hit um
93:41 - uh
93:42 - delete
93:44 - and uh if we find that resource group we
93:46 - should be able to easily delete them all
93:48 - i find the easiest ways to go up here go
93:49 - to all resources
93:51 - and then there's the resource group
93:53 - there and then hit delete resource group
93:55 - and then i'll type in the name of it
93:57 - which is cardassia and i'll delete all
93:59 - those resources but after that's done
94:01 - always just take a double check
94:03 - on your all resources tab
94:05 - and just make sure that those resources
94:07 - are gone just because sometimes they
94:08 - stick around but there you go that's as
94:10 - simple as it was to launch a windows
94:12 - machine
94:14 - [Music]
94:19 - hey this is andrew brown from exam pro
94:20 - and let's take a look here at virtual
94:22 - machines which i consider the backbone
94:24 - of most cloud service providers and
94:26 - azure keeps it really simple by calling
94:28 - it
94:29 - virtual machine so we can go up here and
94:31 - type in virtual machine and make our way
94:33 - over here but right now i don't have any
94:35 - options because i'm using a tenant that
94:37 - doesn't have a subscription applied to
94:38 - it so what i'm going to do is go switch
94:40 - back to my original tenant and this one
94:42 - has this subscription applied to it
94:46 - and so what i'll do is just click back
94:48 - up here and now we'll just type in
94:49 - virtual machines
94:51 - and i can now see uh i have options of
94:53 - creating virtual machines let's go ahead
94:55 - and go create a linux one first and then
94:58 - we'll go ahead and create a windows one
94:59 - and then we'll see how we can connect to
95:00 - it all right so first we'll go to the
95:02 - top here hit add we'll click on virtual
95:05 - machine
95:07 - and we're gonna be present with a lot of
95:08 - options so we'll have to choose a
95:09 - subscription and so there is mine i want
95:12 - to probably create a new resource group
95:14 - here i'm going to call this one bajor
95:18 - and i will name this
95:21 - uh bajor again
95:22 - and i'm going to launch this in us east
95:25 - i'll just set it to one availability
95:27 - zone for the time being
95:28 - then here we have what we can choose as
95:30 - an image i can click on see all images
95:34 - and choose from a variety of them so if
95:36 - i didn't want to use ubuntu i could
95:37 - launch something else like debian or
95:39 - something like that
95:40 - but really i just want to stick with uh
95:43 - ubuntu because i'm fine with that
95:44 - version with 18. uh then here's what
95:47 - what really matters is the size because
95:48 - that's going to affect our cost so if we
95:50 - click on see all sizes we have this
95:52 - nifty
95:54 - table where we can sort the cost it's
95:55 - just loading the cost here it's dynamic
95:57 - this is going to be based on uh what
95:59 - your base subscription is so if you're
96:02 - in canada you're going to see canadian
96:03 - prices in the us you can see us prices
96:05 - etc etc
96:07 - and i care a lot about cost here so i'm
96:10 - just going to sort this by cost and here
96:12 - we have the b1 ls which is very cost
96:15 - effective
96:17 - we have a ram of 0.5 gigabytes and some
96:20 - other options there
96:21 - so we'll go ahead and select that there
96:24 - and we have a couple options we can use
96:26 - ssh public key or we can uh utilize a
96:29 - password and so i think what we'll do is
96:31 - use an ssh public key because that's
96:33 - pretty much the standard there we're
96:35 - going to name the username beige or if
96:37 - it lets us
96:39 - probably won't probably want some
96:40 - additional options there nope it's okay
96:42 - oh great and we'll go ahead and generate
96:45 - a new key pair
96:47 - and i'm just going to name that one
96:48 - bajor
96:50 - and uh we have some options here for
96:53 - inbound rules uh so you could set to
96:55 - none this is just setting up the nsg for
96:58 - you
96:58 - but we're probably going to want to have
97:00 - that port open for ssh because that's
97:02 - how we're going to make our way back in
97:04 - here if we're running a um
97:07 - like an apache server we'd want to have
97:09 - port 80 open so we can go ahead and do
97:10 - that
97:12 - we'll take a look at now disks
97:14 - so here we have options between premium
97:17 - standard and standard hhd
97:20 - i just want this to be cost effective so
97:22 - i'm just going to go with standard hdd
97:24 - but generally you you want to have
97:26 - at least a standard or premium ssd when
97:29 - you're running real web development
97:31 - workloads
97:32 - then there's encryption here and so it's
97:34 - always turned on by default which is
97:35 - great they also have this option of
97:37 - double encryption with the platform
97:38 - managing customer manage key we're just
97:40 - going to leave that as default enable
97:42 - alter discompatibility that's not
97:43 - something we need to do here because we
97:45 - are not using ultradisc
97:47 - and here you can see that you can attach
97:48 - multiple uh disks here
97:50 - so i can go and do that but that's not
97:52 - something i need to do today
97:54 - and some other advanced options which we
97:56 - do not care about we'll go over to
97:58 - networking and so it's going to end up
97:59 - creating us a new v-net for us and we'll
98:02 - create a new subnet for us and assign it
98:04 - a ip address
98:06 - uh it will also set up a nics
98:09 - network security group so the next
98:11 - security group is not going to be
98:12 - applied at the subnet level it's going
98:14 - to be applied at the nic which is
98:16 - attached to the um
98:18 - dc2 there and so we'll just leave it to
98:20 - basic we're going to allow inboard
98:22 - inbound ports for port 80
98:24 - and 22 that was carried over from
98:26 - earlier
98:27 - we can put this behind a load balancer
98:29 - but i don't think we're going to do that
98:30 - right now
98:32 - we'll go over to management uh we have
98:34 - some additional options here for
98:35 - monitoring uh this is all okay here we
98:38 - can set it to auto shutdown actually
98:40 - i'll leave that alone
98:42 - you can also enable backups here
98:45 - we'll go advanced
98:46 - and now we have this option here for
98:48 - custom data
98:50 - i covered a section on cloud init and
98:53 - they don't call this user data but most
98:55 - other providers will call this user data
98:57 - so we could provide
98:58 - a bash script or additional information
99:00 - here if we wanted to
99:02 - then down below there's some host group
99:05 - options we're not going to worry about
99:06 - that in proximity placement group this
99:07 - is really important if you need to have
99:11 - instances nearby uh i think this is
99:13 - pretty common with um
99:15 - what's it called high capacity workloads
99:17 - hfc i can't remember the initialism
99:20 - right now but we covered in the core
99:21 - content then we can tag our resource
99:24 - here uh we'll just leave that alone i
99:26 - don't care about tagging too much but
99:27 - generally it's good to tag in practice
99:30 - and then we will get to review and
99:32 - create our server here we'll go ahead
99:34 - and hit create
99:36 - and then we'll have to download our
99:37 - private key so we can utilize it later
99:41 - and so that's downloaded there
99:43 - and now we're just waiting for it to
99:45 - deploy this and i'll see you back here
99:46 - in a moment
99:48 - so we had to wait a little bit there and
99:49 - finally our deployment is complete
99:52 - and we can go ahead and just review all
99:54 - the things that it created
99:56 - so notice that it created the virtual
99:57 - machine it created a network interface a
100:00 - nic for us
100:01 - the nsg the network security group
100:04 - the virtual network and also a public ip
100:07 - address
100:08 - when i do clean up a lot of things i
100:09 - always miss are these ip addresses and i
100:11 - know that azure gave me a warning that
100:14 - said hey you're about to spend
100:15 - uh 700 yearly on ip addresses because
100:19 - you weren't releasing them
100:20 - so when we do the cleanup step i'll
100:22 - definitely emphasize about deleting
100:23 - those ip addresses and how to go about
100:25 - that
100:26 - let's go take a look at the actual
100:27 - resource now
100:29 - so here we are and you can see we have a
100:32 - lot of options the left hand side such
100:33 - as the disk
100:35 - so we can see the disk options there and
100:37 - there's other additional security
100:39 - options
100:40 - let's go take a look at how we can go
100:42 - ahead and connect to the server
100:44 - and so there's different options here so
100:46 - we have rdp ssh and bastion since we are
100:50 - using a linux machine we're not going to
100:51 - be using rdp that's really for windows
100:55 - um but the trick here is that um
100:58 - i would need to have a client on my
100:59 - computer to connect like um
101:02 - i think it's called putty if you're on a
101:03 - linux based machine it's a lot easier
101:06 - uh and certainly i have the linux
101:08 - subsystem installed so i could probably
101:10 - um connect that way
101:12 - but i figured let's just go ahead and
101:14 - connect via bastion because i think this
101:16 - is a pretty darn cool feature so let's
101:18 - go ahead and create ourselves a bastion
101:22 - and this will take a little bit of time
101:23 - here
101:24 - but we'll go ahead here and just set up
101:26 - subnet so to associate a virtual network
101:28 - to a bastion they must contain a subnet
101:30 - with the azure bastion subnet so they
101:32 - actually have a special subnet for it
101:34 - so what we'll do
101:35 - is we'll just go back to our um our
101:38 - server here
101:39 - which we call bajor
101:41 - and i think we can find the subnet
101:43 - through here
101:44 - so on the left-hand side if we go to
101:45 - networking we probably could find it
101:47 - that way um
101:49 - so i'm just looking for that security
101:51 - group
101:52 - um in there
101:54 - it should be
101:56 - um maybe it's not there if it's not
101:58 - there
102:00 - um well you know we could just go over
102:02 - make our way over to subnets it's not a
102:04 - big deal
102:06 - because it's called bajor it's pretty
102:07 - darn easy to find to begin with and so
102:10 - under subnets here what we need to do is
102:12 - add a special one here
102:15 - and just gonna remember how this works
102:18 - um so you need to create a subnet called
102:20 - azure bastion subnet with a prefix of at
102:22 - least 27.
102:24 - so we'll go ahead and add a new subnet
102:26 - and we'll call it that
102:28 - uh we'll take out the space there
102:31 - and the range is 10 0 1 0 etc so we'll
102:35 - just do it on two
102:37 - zero dot um two dot zero four slash
102:41 - twenty seven
102:44 - uh
102:45 - ten zero zero two is not contained
102:50 - yeah it overlaps so we'll do two
102:53 - let's not contain the virtual network
102:54 - address space oh right so we have to add
102:56 - the address space first
102:58 - oops we'll just hit cancel here
103:02 - that's okay we'll discard that we'll
103:03 - make our way over to address space and
103:04 - we'll go ahead and add 10.0.2.0424
103:10 - they'll give us a pretty darn
103:12 - large range there
103:14 - and so now what we'll do is go back to
103:16 - our subnet
103:19 - and we'll go ahead and create that there
103:24 - and it said it only needed 27 so we'll
103:25 - just give it only 27. we don't need to
103:27 - go bigger than we need
103:29 - and that should be okay we'll go ahead
103:31 - and hit save
103:35 - and it shouldn't take too long so now
103:36 - that we have that we can go back here
103:38 - and we'll give give this another go here
103:47 - uh it's there so it shouldn't be
103:49 - complaining
103:50 - maybe what we'll do is just start from
103:51 - the the start here again
103:59 - yeah there you go the azure portal is
104:01 - like that a lot where you'll have
104:03 - something set up and it has the old
104:05 - state of it and so you just have to
104:07 - trust yourself that you know what you're
104:08 - doing and you have to go back if you
104:10 - don't have a lot of confidence a lot of
104:11 - times you'll get stuck and you'll think
104:13 - okay i don't have it right but i always
104:14 - just try again and hit refresh
104:16 - because the azure portal is very
104:18 - inconsistent so we're just going to wait
104:20 - for this to create this does take a
104:22 - little while to create so i'll see you
104:23 - back in a moment
104:25 - so after waiting five minutes our
104:26 - bastion is now created and so what we
104:28 - can do is without even using a putty
104:30 - client or having to use linux directly
104:34 - we can just connect through via the
104:36 - bastion
104:37 - so here we'll see we have some options
104:39 - here so we want to do ssh
104:41 - private key from local file
104:44 - okay
104:45 - and what we can do is go ahead and
104:46 - select our bajor key
104:48 - and then i'll just scroll down here and
104:49 - hit connect
104:51 - oh um and i think we made the username
104:54 - bajor
104:56 - and we'll go ahead and connect now
104:59 - and it's complaining about a pop-up here
105:01 - so we'll go up here and say
105:04 - always allow
105:06 - and we'll try that again
105:11 - and then we'll say allow again
105:15 - and so now that we're into our server
105:16 - here let's go ahead and try to install
105:18 - apache and see if we can get at least
105:19 - the default page running um so this is
105:22 - using ubuntu if my memory serves me
105:24 - correctly it should be app get install
105:26 - apache 2
105:28 - and we'll just hit y for continue
105:33 - and we'll just wait for this install
105:35 - doesn't take too darn long
105:41 - and after a short little wait there it
105:43 - finally did install also if you notice
105:45 - this little icon here we have a little
105:46 - clipboard here
105:48 - i don't seem to ever use that there so
105:50 - that's fine
105:51 - now when you install apache we might
105:54 - have to go ahead and start it up
105:56 - so let's just take a look to see if it
105:59 - actually is in the running directory
106:00 - here so we'll go to cd var www and so
106:03 - that's where the default directory is
106:06 - right
106:07 - but we can just check to see if it's
106:09 - running by doing a ps aux i think it's
106:12 - httpd
106:14 - or we can say apache here
106:17 - and so it looks like it's already
106:18 - running so that's pretty great for us
106:21 - and since it's running on port 80 and
106:23 - we've opened up port 80 we should
106:24 - probably be able to access that
106:27 - here so let's go back to our actual
106:29 - virtual machine
106:30 - so we'll go to virtual machines
106:36 - and we have that virtual machine running
106:38 - i'm just going to click into it because
106:39 - i just want to find out its public ip
106:41 - address
106:43 - so here it is there and for lucky this
106:46 - will just work just copy the clipboard
106:47 - button right there
106:50 - and look at that we have the default
106:51 - page isn't that cool
106:53 - so that's all there really is to it and
106:55 - i could even update this page you don't
106:56 - have to do it but i'm just going to
106:57 - update it for fun
106:59 - actually i probably have to restart the
107:00 - server so maybe i won't do that
107:02 - but yeah so we connected through the
107:04 - bastion so that was pretty darn easy
107:06 - we probably could have also used um the
107:08 - cloud shell to connect um
107:11 - but maybe we should we could give that a
107:13 - go as well
107:16 - since we're all done here let's go ahead
107:18 - and do some cleanup the first thing i
107:20 - want to do is the easiest way is
107:22 - actually to go to all resources here at
107:23 - the left-hand side
107:24 - and this really gives you an idea of
107:26 - everything that's running your account
107:27 - so i actually have other stuff in here
107:29 - that's not relevant
107:30 - but the idea is that all of our stuff is
107:32 - running within a resource group
107:35 - and so i'm just taking a look there i'm
107:37 - not saying this is all resources here
107:39 - um see if you see resource group here
107:42 - yeah they're all there right there so i
107:44 - can go ahead and click that
107:46 - and so everything more or less should be
107:47 - self-contained within here see all that
107:49 - stuff
107:50 - you can even see the v-net is part of it
107:52 - as well and so if i go ahead and delete
107:54 - this resource group
107:56 - it should delete all this stuff so i'm
107:58 - just going to type bajor to confirm
108:01 - and we'll go ahead and delete and that
108:04 - should do a good job of cleaning up all
108:06 - those files
108:08 - i'm not sure if it'll delete the ip it
108:10 - should right there but if it doesn't
108:13 - what i recommend is after everything is
108:15 - deleted
108:16 - just go back here to all resources and
108:18 - just double check to make sure they all
108:19 - vanish
108:20 - because when this is done they're all
108:22 - going to start to vanish from this list
108:24 - and if there's anything remaining you'll
108:25 - know because it's still here right
108:27 - so just be careful about that that's all
108:29 - i want you to know
108:30 - and so that's the linux part and so
108:32 - let's go ahead and actually now set up a
108:34 - windows server
108:36 - [Music]
108:40 - hey this is andrew brown from exam pro
108:42 - and it's really important that we know
108:43 - how to monitor our virtual machines so
108:46 - we can kind of check their performance
108:48 - or do some diagnostics on them and so a
108:50 - couple things we're going to look into
108:52 - we're going to set up some virtual
108:53 - machines and we're going to look into
108:54 - automation accounts uh log analytics and
108:58 - metrics and alerts okay
109:00 - so the first thing we'll need to do
109:02 - before we do anything else is make sure
109:03 - that we have particular resource
109:05 - providers enabled so go to the top here
109:07 - and as always go to your subscriptions
109:10 - and we will click into our subscription
109:12 - go all the way down the bottom click on
109:14 - resource providers and there's two in
109:16 - particular we're looking for uh one is
109:18 - for alerts so it's alerts management so
109:20 - make sure that is registered if it's not
109:22 - press that button
109:23 - and then we want insights so make sure
109:26 - insights is enabled as well they should
109:28 - be by default but just in case they're
109:30 - not uh you should go ahead and do that
109:32 - so we're going to need some virtual
109:34 - machines i'm going to set up three and
109:36 - the reason i want to do that is just to
109:37 - show you that there's many different
109:39 - ways in azure to do the exact same thing
109:41 - and sometimes when you do something it
109:43 - just works and sometimes it just doesn't
109:46 - and it can work again and not again so i
109:48 - want to give you a few different options
109:50 - available to you so the first thing
109:52 - we're going to spin up is a virtual
109:53 - machine
109:55 - and this one in particular
109:57 - we'll make a new group we're going to
109:58 - call it dax and so we're going to choose
110:01 - for this one an ubuntu linux i'm going
110:04 - to switch it over i'm going to choose a
110:05 - minimal install so we'll choose this one
110:07 - over here which is the ubuntu server
110:09 - minimal
110:10 - and uh what i'll do is name this and
110:13 - there's a bunch of different daxes in
110:15 - star trek so we're going to go with
110:16 - kurzon dax here
110:18 - and we'll put all this in east us so
110:21 - wherever it defaults here so if that's
110:23 - where it wants to go that's where it's
110:24 - going to be but let's pay attention to
110:26 - where that is okay
110:27 - let's make sure that we're using a very
110:29 - inexpensive instance
110:31 - i'll give it a second to load if you do
110:32 - not see the cost just drag these over
110:34 - until you do it just depends on your
110:36 - screen right
110:37 - um so
110:38 - once this loads it will tell us that
110:40 - this is probably the cheapest i think
110:42 - that is the cheapest because something's
110:44 - going to go lower than at 500 megabytes
110:46 - of ram so i think we're already set to
110:48 - it i'm not going to wait for the load to
110:49 - take forever notice the red line there
110:51 - is no error is that's just what it shows
110:54 - and so i'm going to go for password
110:56 - authentication here and what i'm going
110:57 - to do for this account is i'm going to
111:00 - type in um dax as the username and then
111:03 - testing with a capital t e s t i g one
111:05 - two three four five six because it wants
111:08 - an upper case a lower case some numbers
111:10 - and a certain length and that's what
111:12 - works for me
111:14 - so now that i have those filled in ssh
111:16 - we're gonna keep that open we're gonna
111:17 - go to disk you can leave it on premium
111:20 - if you want i'm going to go to standard
111:21 - i want to save some money just in case i
111:22 - forget to turn this machine off it's
111:24 - going to set up some networking for us
111:26 - which is totally fine we're going to go
111:28 - to management
111:30 - and i believe so there should be guest
111:33 - os install here
111:35 - it is not here it's the next step i
111:38 - believe
111:41 - where is it i know you're in here
111:44 - let's go back one step here i have a
111:46 - feeling it's on this page because you
111:47 - can install the um
111:49 - guess os metrics right away
111:53 - so i'm just seeing here i cannot seem to
111:55 - remember where it is
111:57 - i might have skipped over it because i'm
111:58 - going so quick here
112:02 - um
112:04 - well maybe it's not set here so when we
112:06 - go to do the other virtual machine we'll
112:08 - see if it's actually there but for this
112:09 - one we'll just launch it without it
112:11 - which is totally fine i was going to do
112:13 - one that way anyway
112:16 - and so we'll go ahead there and create
112:18 - that virtual machine okay now while that
112:21 - is going
112:23 - and by the way if you think you call
112:24 - that number that's not me so
112:27 - uh you know you can call if you want but
112:28 - it won't go to me
112:30 - so that's going to go ahead and deploy
112:32 - that what i'm going to do now is go and
112:34 - launch another virtual machine in the
112:36 - same
112:37 - uh in the same environment so this one
112:39 - this time around is going to be again
112:41 - it's going to be linux we're going to
112:42 - choose dax here
112:44 - and this one is going to be
112:46 - [Music]
112:47 - jadziadax
112:50 - and we this time around are going to
112:52 - choose
112:55 - ubuntu server lts so this one's a
112:57 - minimal install and this one here has a
113:00 - bit more to it we're going to choose
113:02 - gen2
113:03 - uh this is now 4.86 which is strange it
113:06 - looks like it was the same one before i
113:08 - don't know why the other one was still
113:08 - more expensive because where it said 40
113:10 - dollars but maybe i was mistaking the
113:12 - red means nothing as per usual it's just
113:14 - as you're uh tricking us we're going to
113:16 - call the username here dax testing with
113:18 - a capital t-e-s-t-i-n-g
113:21 - you know what i'm trying to say here
113:23 - and i'm going to do uh testing one two
113:26 - three four five six
113:28 - and we will go next to disks we'll stick
113:31 - with premium for this time it doesn't
113:32 - really matter if we go to advanced here
113:35 - there's no uh interesting options there
113:38 - this is all fine this looks good
113:43 - we go to management so now notice here
113:45 - so we didn't have it for the minimal
113:47 - install but for this one we do we can
113:48 - actually put in the guest os
113:50 - diagnostics and what it will do is it
113:53 - will have to create itself a storage
113:55 - account so that will be in azure storage
113:57 - to store
113:59 - our diagnostic data
114:01 - and so that seems like a good idea to me
114:04 - so what we'll do is we're going to go to
114:06 - next but notice we didn't have it for
114:08 - the minimal install probably because it
114:10 - doesn't have the um the agent what's
114:12 - called the wag agent installed
114:14 - for it to do that by default so just
114:16 - notice that it varies on some machines
114:18 - we're going to go to next we're going to
114:19 - hit review and create
114:24 - and we're not done yet we're going to
114:25 - create one more virtual machine and this
114:27 - time it's going to be a windows once i
114:28 - hit the create button here
114:36 - uh we're going to go back to virtual
114:37 - machines and this time i'm going to
114:38 - launch a windows machine and i'm going
114:40 - to use we're going to put in the same
114:43 - group here i'm going to call this one
114:45 - uh tobin or let's call it
114:47 - little dax here
114:50 - and this time i'm going to choose a
114:52 - windows server so i'm going to go to
114:53 - windows server here we have a lot of
114:54 - options i'm not that great with windows
114:56 - servers i'm going to choose a gen
114:59 - 2
115:01 - windows server here it's not going to
115:02 - let us get away with this b1 l it's not
115:04 - big enough you need a lot more compute
115:06 - and memory i think it's like two and
115:08 - four uh but i just remember it always
115:10 - choose ds ds2 v2
115:13 - um which is pretty hefty it's pretty
115:15 - expensive maybe i could choose something
115:17 - cheaper like the ds1
115:20 - no i need to i need two cpus so ds2 v3
115:24 - that's when you're launching a windows
115:25 - server if you don't do that they'll
115:27 - complain
115:28 - and that'll probably be pretty darn
115:29 - expensive but we got to make sure that
115:30 - we just shut this off and get through
115:32 - this as quick as possible i'm going to
115:34 - choose the same thing as always so i'm
115:35 - going to i'm going to put in dax and
115:37 - then for the password i'm going to do
115:39 - testing 1 2 3 capital on that t
115:43 - 4 5 6
115:44 - testing 1 2 3 4 5 6 because it wants 12
115:47 - characters we're going to let it do on
115:49 - the rdp that's totally fine
115:51 - uh sure premium ssd we're not going to
115:53 - keep this around for very long
115:55 - we're going to scroll on down go to
115:56 - management
115:58 - and notice that you can enable os
116:00 - diagnostics here i'm not going to do
116:01 - that here just so i can show you show
116:03 - you it in the other variation
116:06 - we're going to go to next we're going to
116:07 - go hit review and create
116:09 - and we're going to go ahead and create
116:11 - this instance whenever it lets us do it
116:14 - there we go
116:15 - okay we'll hit create
116:18 - and now it's deploying so
116:21 - we have these three virtual machines
116:23 - we're creating and so what i'm going to
116:25 - do is make my way back to virtual
116:26 - machines here and so i have the two
116:28 - there i'm going to give it a refresh
116:29 - make sure i can see the third one
116:30 - creating
116:32 - we don't see it just yet but these ones
116:33 - are running
116:34 - so
116:35 - what i'm going to do is open up kurzone
116:37 - i'm going to open up jedzia and one has
116:40 - the guest um the the uh the guests ins
116:44 - or the
116:45 - what's it called diagnostic settings
116:46 - it's the guest metrics installed and the
116:48 - other one does not so if i scroll down
116:50 - here so i just want to show you there's
116:52 - a lot of variation and sometimes you
116:53 - know you do it one way it works another
116:55 - way it doesn't so for jetsea it was
116:57 - installed on default so that's the one
116:59 - that is using ubuntu standard
117:01 - and then this is the minimal install
117:04 - where it's saying hey do you want to
117:05 - install the metrics and you have to pick
117:07 - a storage account if you didn't then you
117:09 - would have the option to create one here
117:11 - it's called dax
117:12 - diag so i just want to show you
117:15 - we're going to go ahead and enable those
117:17 - guest
117:18 - level monitoring metrics okay
117:21 - so
117:23 - we have a storage account and that's
117:24 - where the data is going to go
117:25 - um
117:26 - and this will probably fail because i
117:28 - don't think it has the necessary things
117:30 - installed but i just wanted to show you
117:31 - you could do it that way
117:33 - but what another thing we're going to
117:34 - need is we're going to need
117:36 - um a log analytics workspace
117:41 - so all a log analytics workspace is is a
117:44 - place to store your logs it is
117:46 - essentially a data lake for a lot of
117:47 - different resources so you can put all
117:49 - sorts of resources under here and that
117:51 - way you can search across them see how
117:53 - this failed
117:54 - if you ever have a failure like this the
117:56 - way you investigate this as you go in
117:58 - here even though it says assass token
117:59 - for the storage account couldn't
118:00 - generate it's kind of a
118:03 - misleading thing here because if i
118:04 - actually click in up into my
118:06 - notifications we go to our activity log
118:09 - we can see kind of what something has
118:11 - happened here so i know this failed
118:16 - it might not show up here yet sometimes
118:17 - it takes time to show up so we'll
118:20 - revisit this later but it would show
118:22 - red errors here and if we looked in
118:25 - there it would tell us that python's not
118:26 - installed on that machine because it's a
118:27 - minimal linux install so if you run into
118:29 - those problems that's the reason why
118:31 - some vms are a bit easier to work with
118:32 - than others but we're looking at log
118:34 - analytics and again it's just a place to
118:36 - collect your logs but i want to see
118:38 - because i think jitsi is the one that
118:41 - actually is the one that is working
118:43 - properly but if we go down to
118:45 - metrics or logs
118:49 - it's going to say hey you if you want to
118:51 - use logs you're going to have to go
118:52 - ahead and create
118:54 - you have to enable it but what it's
118:56 - really doing is just creating a log
118:57 - analytics workspace so if i click this
118:59 - here
119:00 - what's going to tell me is say hey you
119:01 - don't have one we'll create a default
119:03 - one for you
119:04 - um i don't want to do it that way what i
119:06 - want to do is create one myself
119:08 - and then link it through there or what i
119:10 - could do is make it through automation
119:12 - so what i'm going to do
119:14 - is i think what i'm going to do is i'm
119:16 - going to do through automation
119:17 - so there's another service called
119:19 - automation accounts and this particular
119:21 - service is for configuration management
119:23 - update management and it's a way for us
119:25 - to keep our vms up to date
119:28 - and so this one's going to be called dax
119:30 - oh it's too short it doesn't like that
119:31 - dax automation
119:34 - and i'm going to choose the resource
119:35 - group dax it's
119:38 - e-s-e-s
119:39 - and here it's going to say azure run as
119:41 - account so this is going to give it more
119:43 - permissions so that it can run as a
119:44 - contributor role
119:46 - and that way it will have access to
119:47 - resources to do what it wants to do so
119:49 - we'll go ahead and deploy that and then
119:51 - once this is deployed we'll create our
119:53 - our workspace through this because i
119:56 - found an issue where if i made this and
119:57 - tried to link it just didn't link and i
119:59 - don't want to have those problems
120:02 - so i'm just going to make my life a
120:03 - little bit easier by making it through
120:04 - here so under here if we go to update
120:07 - management
120:09 - it's going to say hey you need a
120:10 - workspace in order to use update
120:11 - management if you're wondering what
120:12 - update management is it keeps your vms
120:14 - up to date by applying patches
120:16 - so if we drop down here we're just going
120:18 - to go ahead and create a new workspace
120:20 - and i'm going to do it that way because
120:21 - it'll be automatically associated with
120:23 - that workspace on creation
120:28 - and uh we'll let that go there for a bit
120:31 - but while that's setting up a new
120:33 - workspace for us if you go over to
120:36 - run books this is the power of
120:40 - well i'm not i'm not
120:42 - it wants to make that thing there so i'm
120:43 - just going to click ok there don't worry
120:45 - about that discard but the whole power
120:47 - of
120:48 - this is that it can apply patches it can
120:50 - it can manage the state of your vms
120:53 - but if you need to do something
120:55 - like run something like a a series of
120:58 - run commands
120:59 - they have a bunch of different run books
121:01 - here
121:02 - uh so there's a variety of different
121:04 - kinds and i think we can click into them
121:06 - if there's something about sql that
121:07 - sound kind of how to use an escrow
121:08 - command on a automation runbook
121:12 - so
121:13 - that's just a runbook so runbook just
121:15 - means like a series of run commands
121:16 - something that you're automating on your
121:17 - server and you do it through here um so
121:20 - update management looks like it is now
121:22 - working uh we didn't press that enable
121:24 - button uh initially but it worked there
121:27 - but if we wanted to have vms under here
121:29 - we'd have to go ahead and add them
121:31 - manually
121:32 - um and so we're just gonna wait for
121:34 - those to show up
121:36 - alternatively if we go back to our
121:38 - server or sorry one of our vms
121:41 - we should be able to add it this way it
121:43 - will just tell us to go that way anyway
121:45 - um so there should be like update
121:46 - management or something in here
121:48 - could be configuration management
121:52 - um
121:54 - yeah that still would be through
121:56 - something right so it'd probably be
121:57 - still through automation
121:59 - but i could have swore there was maybe
122:01 - guest and host updates
122:03 - yeah so here it's just saying go to
122:05 - azure automation so you don't really set
122:07 - it through here you have to associate it
122:10 - through azure automation and so these
122:12 - are ready to enable so i'm going to say
122:14 - go ahead and enable those i don't know
122:16 - where our windows server is we
122:17 - definitely spun one up it's not a big
122:19 - deal if we don't have it it's not really
122:20 - crucial to this tutorial but it would
122:22 - have been nice to show um the windows 10
122:24 - there
122:26 - i'm gonna go see what happened to our
122:27 - windows server oh it is there okay
122:28 - leela's there
122:30 - so we'll take a look at that in a second
122:32 - uh but we're just adding those there
122:33 - we're not going to really apply any
122:34 - patches i just want to show you how to
122:36 - associate them there um and just to
122:38 - highlight the fact that there are run
122:39 - books which are pretty cool and you can
122:41 - also do other things like if you need to
122:43 - like install python packages or stuff
122:45 - like that uh to to do stuff you could do
122:48 - that as well
122:50 - um so there's a bunch of interesting
122:51 - things in there but it's kind of like a
122:53 - side dish to the
122:55 - follow along here so this would have
122:57 - created our workspace so now let's go
122:58 - over to our workspaces
123:02 - and we have one here and if you're just
123:04 - curious what it looks like if we go here
123:06 - something super exciting you just put
123:07 - the name and region in and it has the
123:08 - default page you go
123:10 - so you're not really missing out on
123:12 - creating one uh by hand there
123:15 - but underneath here
123:16 - what we can do is see if our virtual
123:18 - machines are actually associated uh with
123:20 - this here so if we go under virtual
123:22 - machine you're gonna see that these ones
123:23 - are connecting
123:25 - which is interesting because i didn't do
123:27 - anything to connect them i'm not sure
123:29 - why that's happening but i'm going to
123:30 - hit connect here
123:32 - and that will connect that a lot of
123:33 - times i'll come in here i have to
123:34 - manually connect them i don't know if
123:35 - it's because we put them in the uh
123:38 - what do you call it within the uh the
123:40 - automation there if we go back to
123:41 - automation
123:44 - which ones did i choose i already forgot
123:46 - update management
123:49 - um
123:52 - they're not showing up here now but i
123:53 - know they're there
123:56 - maybe we triggered it when we were doing
123:57 - there but anyway the point is is that
123:59 - you have to make sure that those are
124:01 - um configured so that it's actually
124:03 - sending data over to the workspaces so
124:05 - that we can then use the logs and things
124:07 - like that
124:09 - so if we go back to our virtual machines
124:11 - over here
124:13 - you can see they're connecting and this
124:14 - takes forever
124:16 - while this is going let's go take a look
124:18 - at storage accounts
124:20 - and actually a better way would just be
124:22 - to go look at the virtual machine
124:23 - individually here
124:25 - so we'll just go back to my virtual
124:26 - machines and i go to the resource group
124:29 - we scroll on down here we should see a
124:31 - storage account within here
124:33 - let's just type in storage
124:36 - there it is
124:37 - whoops
124:39 - and uh the thing is is that we can
124:41 - actually view some of the data in here
124:43 - so if you go to open explorer you'd have
124:45 - to install this it's for windows i don't
124:47 - i think it's for mac as well but once
124:49 - you have the storage explorer uh open
124:51 - you can just click open here
124:54 - and it'll open up once it's installed on
124:56 - your computer of course
124:58 - and what i can do
125:00 - if we have any data it might not be
125:01 - collecting any data yet but if we go
125:03 - down to our storage accounts here
125:07 - oh here it is down below
125:08 - and expand it under tables see how this
125:11 - wad that is the name of the um
125:15 - uh the agent that is collecting data and
125:17 - here you can see that it's already
125:18 - collecting stuff
125:19 - all right
125:21 - and it's collecting for jet zia dax
125:24 - and so i'm just kind of looking at the
125:25 - data that's collecting nothing super
125:27 - interesting
125:28 - but the thing is is that
125:30 - we're gonna make our way over to logs
125:32 - because by default
125:35 - azure
125:36 - collects host metrics so things about
125:39 - the instance like cpu usage but there's
125:41 - things that it doesn't collect like
125:42 - memory
125:44 - and other things like that
125:46 - so
125:47 - that's why we need to turn on this
125:49 - setting here the diagnostic settings
125:52 - so we can collect those metrics these
125:53 - are called guest metrics so we have
125:55 - memory network file disk disk and
125:58 - storage so if you wanted to know
126:00 - how much storage was left on the on the
126:02 - actual uh disk on with the with the
126:05 - virtual machine or how much memory was
126:07 - being used you'd need that uh set up
126:09 - here
126:09 - uh once this is enabled uh you can send
126:12 - data over with uh into azure monitor and
126:15 - other places um it's not showing app
126:17 - analytics i think that's for
126:20 - or log insight whatever the name of the
126:22 - service is it's for uh
126:24 - that one i think is for the windows one
126:25 - so we'll see that when we go over there
126:26 - but syncs just means we're gonna send
126:28 - data uh from here to to um
126:32 - azure monitor right
126:34 - but we'd have to have preview to see
126:36 - that
126:37 - uh nothing that interesting here uh but
126:39 - if we go over to logs now um this should
126:42 - not be showing up here anymore so if
126:44 - this is still showing an enable screen
126:46 - then we got to go back to
126:48 - our log analytics we'll click into here
126:51 - we'll go down to virtual machines and
126:52 - we'll see if it's connected yet so it
126:54 - still says it's connecting right
126:56 - this one's ready though and i haven't
126:58 - even done anything with that one yet so
127:00 - what we'll do while we're waiting for
127:01 - these linux ones to uh to spin up we'll
127:03 - go over back over here to our virtual
127:06 - machines and we'll go find that uh
127:08 - windows machine which i don't know why
127:09 - it's not showing up here
127:11 - there it is
127:12 - and what we'll do is we'll go ahead and
127:14 - connect to it
127:16 - so i am on a
127:18 - windows machine so i'm going to shape
127:19 - here
127:20 - and i'm going to click on rdp
127:23 - and i'm going to download that file i'm
127:26 - going to double click this file on my
127:27 - windows machine
127:29 - and i'll be able to open this up
127:30 - instantly it was called dax testing one
127:33 - two three oops
127:35 - testing one two three four five six
127:39 - looks good to me we'll hit okay
127:41 - we'll say yes
127:43 - because if you want to collect
127:46 - um
127:48 - uh particular metrics like guest metrics
127:50 - you have to enable like when you're on
127:51 - windows server you have to enable it to
127:52 - uh to send performance counters
127:56 - so i'll give it a moment here it doesn't
127:58 - take too long
128:05 - while that's going we should look to see
128:06 - if we actually have um
128:09 - the uh
128:11 - the settings set up here the diagnostic
128:12 - settings and so this is what i'm talking
128:14 - about the enable guest level monitoring
128:17 - so we want cpu utilization disk and
128:18 - network so we'll go ahead and click that
128:20 - there
128:23 - and while that's going to go back here
128:25 - and so this has now started up
128:28 - i don't know much much about windows
128:30 - servers but i do know how to turn on
128:33 - at least last time i did i know how to
128:35 - turn on the performance counter so see
128:36 - here it says online performance counters
128:38 - are not started so i can right click
128:40 - this and then just say
128:42 - start performance counters and that will
128:44 - have it start sending data over to our
128:46 - storage accounts assuming that this is
128:49 - enabled here so this is going to go
128:50 - ahead and install
128:52 - wag into
128:53 - the windows server so it can send back
128:55 - metrics
128:58 - uh and so we got a lot of things going
129:00 - on here but let's go back to jadzia
129:03 - and um i want to see now if i have any
129:07 - if i can use logs now
129:10 - there we go so it's now connected to the
129:12 - workspace so it knows where to send that
129:13 - data
129:14 - and you can pull up when you go to logs
129:16 - it'll show different types of default
129:18 - queries if you're in the actual group
129:20 - here you'll have the same screen but
129:21 - it'll show you across all possible
129:24 - resources so it's a little bit easier to
129:27 - figure out like useful queries for
129:29 - virtual machines when you do it through
129:30 - here and there's some ones there's some
129:33 - that we can run right away so if we go
129:35 - over to
129:36 - um
129:39 - the diagnostics
129:40 - we could or actually availability is a
129:42 - good example so heartbeats will tell you
129:45 - if the server is live so a server will
129:47 - have heartbeat so every whatever second
129:49 - or minute it's going to
129:51 - say hey i'm still alive that's what a
129:52 - heartbeat is
129:53 - so we'll go ahead and run that
129:57 - and we'll give it a moment here i'm not
129:59 - sure why it's crying
130:04 - that doesn't work i'll just click off
130:06 - and try that again
130:08 - so go back to availability we'll hit run
130:17 - and it has the right scope if you're
130:18 - wondering how it knows what to check
130:20 - that's all based on scope there
130:22 - and for whatever reason it just decides
130:24 - that it doesn't really want to run right
130:25 - now
130:27 - that's a bit frustrating but if it's not
130:29 - working for this one we can always check
130:30 - our other one which is curizon
130:33 - and we can go see if this one works
130:34 - that's why i made multiple servers
130:36 - because i knew one of these might not
130:37 - work right away
130:39 - so i'll go ahead and run this one here
130:47 - it says no logs are configured so that
130:48 - makes me think that
130:50 - our workspace here if we go back over to
130:52 - here and we go down to virtual machines
130:54 - that they're actually not linked oh they
130:56 - still are connecting there you go so
130:57 - it's a bit misleading it looked like we
130:59 - could query but we still can't
131:02 - um let's go take a look at this virtual
131:04 - machine here
131:05 - our little one still
131:08 - and let's see if we can do anything with
131:09 - it yet
131:12 - so we go down to
131:16 - diagnostic settings
131:19 - it looks like it's installed and so if
131:20 - we go to performance counters these are
131:22 - the counters that it should be
131:23 - collecting data on you could do custom
131:25 - if you want and change those values
131:27 - but we're just going to leave it basic
131:28 - for the time being
131:29 - go ahead and save that
131:31 - and i believe we can do logs over here
131:33 - on this one so we'll go over to
131:35 - availability i'm going to load that into
131:36 - the editor this time instead of hitting
131:38 - run i can hit run and it will return and
131:40 - says that it has a count to seven so
131:43 - seven heartbeats that's good
131:45 - if you wanted to create uh rules you
131:47 - could easily do that there's actually uh
131:49 - some nice ones here if we go into here
131:52 - um
131:53 - this won't work unless our
131:55 - our our metrics are working our guest
131:58 - metrics
131:59 - but there's a bunch down below here
132:03 - i'm just trying to see one that might be
132:04 - interesting for us to set
132:07 - so you might go here
132:09 - and
132:12 - i don't think i like that one
132:16 - say this one here i'm just going to
132:17 - clear this all the way out
132:20 - oops
132:22 - so here they even say to create this
132:24 - alert just hit the plus new button here
132:25 - so we go here
132:27 - and
132:28 - sometimes it does this it doesn't
132:30 - actually bring it over which is weird so
132:32 - you have to go back
132:35 - i told you like this the the ui for
132:37 - azure is super super buggy
132:39 - so if you don't have any confidence uh
132:41 - don't worry it's it happens to all of us
132:45 - um so we'll go back to our alerts
132:49 - and i'm going to load that into the
132:50 - editor and what i'm going to do is just
132:52 - copy it
132:56 - and um
133:02 - we're going to go ahead and hit run
133:05 - and so here we're getting some data back
133:08 - i think it's going to complain about the
133:09 - time generated but if we go and create a
133:11 - new alert
133:13 - oh man it just does not want to uh copy
133:16 - anything over a
133:17 - and so
133:20 - it's like i can't even paste
133:22 - we'll go back to here
133:25 - i'm going to copy this i noticed this
133:27 - because i had it double clicked maybe
133:28 - just just
133:29 - creates one what you have selected
133:32 - and so it should complain about this but
133:34 - it's not so that's great and so we can
133:36 - go here and we can put a threshold to
133:38 - five
133:39 - i'm just putting any value here and that
133:41 - gives you a monthly cost for that alert
133:43 - so you hit review and create
133:46 - please correct the following details
133:49 - where
133:54 - oh the alert name okay
133:56 - my alert
133:59 - and we will hit review and create
134:01 - we'll hit create
134:04 - and so there we can create an alert so
134:05 - we could create alert like our memory is
134:07 - running out of stuff and those are kind
134:09 - of useful things to do
134:11 - so
134:12 - talking about metrics
134:14 - we're going to make our way over here
134:16 - and notice we have host metrics and
134:18 - guest metrics so host metrics are
134:20 - general ones that are available to us
134:21 - and guest metrics are
134:23 - memory and stuff like that
134:25 - so if we have that you're going to
134:26 - notice now we have like disk and memory
134:28 - and things like that if we didn't have
134:30 - that installed we wouldn't be able to
134:31 - access this information
134:33 - so what i can do is go ahead here and
134:34 - try disk time i don't know if it'll show
134:36 - us anything right now because it might
134:38 - have not collected enough information go
134:40 - over to virtual host
134:42 - we'll have different metrics here so a
134:44 - standard one would be like cp usage
134:47 - okay so that one is
134:50 - displaying there the poor performance
134:51 - metrics always take a little bit more
134:53 - time to collect so i'm not sure if i'll
134:55 - stick around to show you that but
134:58 - if we go over to availability sometimes
134:59 - we need to go debug
135:01 - performance like you can hit run and
135:04 - this will tell you if it's actually
135:05 - collecting information so see right now
135:06 - it shows me nothing so it hasn't
135:08 - collected anything for process memory or
135:10 - processor but if it did then we could go
135:13 - and use these other ones down below here
135:15 - so we have
135:16 - free disk space so that's a really
135:18 - useful one to have which we can't get
135:19 - metrics on that right now
135:21 - but that's what we've been trying to do
135:22 - is collect that information so let's go
135:24 - back to our workspace
135:28 - and what i'm going to do
135:30 - is take a look
135:32 - at our virtual machines that are
135:33 - connected
135:34 - and this one has an error um and that's
135:37 - kurzon and i'm not surprised because
135:39 - again it's a minimal windows
135:42 - linux instance so probably couldn't
135:44 - install the agent so that's expected but
135:46 - i just wanted to show you that edge case
135:48 - if we go into the jet zeo one which is
135:51 - the one we were waiting for this entire
135:53 - time
135:54 - we should
135:57 - hopefully be able to see some metrics so
136:00 - if we go down here
136:02 - and we choose yes metrics
136:05 - i want something with memory here
136:09 - uh like percentage
136:12 - okay so it's still not showing me
136:13 - anything and that's fine if we're not
136:15 - getting any metrics what we can do is we
136:17 - can go log into this instance and give
136:19 - it some data so that it does something
136:21 - all right
136:22 - so what i'll do is go all the way to up
136:25 - to the top here into my overview i'm
136:27 - going to copy this public ip address
136:29 - we're going to open up our cloud shell
136:31 - and make sure that it's in bash all
136:33 - right
136:35 - and we're we're going to type in ssh
136:38 - um
136:40 - dax
136:41 - and then we're going to paste
136:43 - the ip address in we'll hit enter we'll
136:45 - say yes
136:46 - and the password is
136:48 - testing123456 with a capital t
136:54 - and what we can do here is we can
136:55 - install a tool called stressng what that
136:58 - tool does is it will
137:01 - help us do stress testing and so that
137:03 - allows us to
137:04 - make some metrics
137:08 - so we'll go ahead and install that via
137:10 - snap
137:11 - snap is the new package manager for
137:14 - ubuntu used to be apt-get or apt
137:17 - it doesn't take too long to install
137:29 - and while it's installing i actually
137:30 - have a pre-made uh line here
137:33 - to check the memory
137:35 - or to fill up the memory
137:38 - i'm just going to go ahead and paste
137:39 - this line in so this is stress hyphen ng
137:42 - vm bytes and it uses awk it's going to
137:44 - do memory availability and it's going to
137:45 - fill it up to 90 percent so we want to
137:48 - have 90 memory usage i'm going to go hit
137:50 - enter i'm just going to let that run now
137:52 - it's not outputting anything but it
137:54 - definitely is running there we go and so
137:56 - we're just going to leave it there and
137:57 - so soon enough we should have some
137:59 - information
138:01 - and i think if i go there yep it's still
138:02 - there so it's fine
138:05 - so now if we go over to
138:09 - um down to our
138:13 - logs
138:14 - and we go back to
138:16 - performance
138:19 - and we just choose this one here
138:23 - we might see something now
138:26 - again it takes time so you know it looks
138:27 - in the 15 minute uh window here
138:30 - but i just want to see that it's
138:31 - collecting anything for me it doesn't
138:33 - matter what it collects just collects
138:34 - something for me you know
138:36 - so i think it'll just take a little bit
138:38 - of time while we're doing that we should
138:39 - make our way back over to our workspace
138:41 - because there's some other settings that
138:42 - are kind of interesting that i noticed
138:45 - if you go over to agent configuration
138:47 - um you can collect additional
138:49 - information over here so we have
138:50 - performance counters for
138:52 - windows and linux i think we have to set
138:54 - the memory one here if we wanted to
138:56 - actually appear here
138:57 - so
138:59 - i don't know if there's a way to set all
139:00 - of them
139:02 - but i'm gonna just choose some here so
139:03 - i'm gonna do memory used
139:10 - we'll do logical use space here
139:14 - we'll go here and do free space i think
139:16 - that might be the reason why it's not
139:17 - showing up for us
139:18 - maybe process time here
139:23 - we did
139:24 - use memory used percentage of use space
139:28 - we will do um
139:34 - do reads and writes
139:36 - so here we have some for our linux
139:40 - so i think if i we set that then we
139:42 - should be able to see some information
139:44 - uh syslogs is just a type of logs that
139:46 - you can collect we're not going to look
139:48 - into those today
139:50 - but yeah that's the
139:52 - management there
139:55 - we could also try setting some here
139:57 - maybe one for memory i can't remember
139:58 - what the one for memory is called here
140:00 - because they do all of this kind of
140:01 - stuff
140:02 - like windows has so much stuff i can't
140:04 - make sense of it
140:09 - i think we might want paged owls
140:14 - and
140:16 - page
140:18 - so that went called there
140:26 - bytes we'll give that a go
140:29 - and we'll see if that starts collecting
140:31 - information
140:33 - i'm going to go back over to
140:36 - uh here
140:37 - and oh this is kurzon that's the windows
140:40 - machine we're going to go to the linux
140:42 - machine here and we'll give it another
140:43 - go
140:45 - see if we get anything this time around
140:49 - so we'll go back to
140:51 - performance
140:53 - we'll say what is being collected run
140:57 - okay
140:59 - and so i think we're just going to have
141:00 - to wait a little bit so i'm just going
141:01 - to stop the video here and i'll see you
141:03 - back in maybe 15-20 minutes okay
141:05 - all right so i'm still waiting here and
141:07 - i don't seem to be seeing any data but
141:09 - that's okay because there's another way
141:10 - we can kind of approximate this and we
141:12 - don't have to wait
141:13 - forever especially if you don't have a
141:15 - lot of time like me
141:17 - the thing is is that azure has a log
141:19 - analytics tutorial and within here they
141:22 - have a demo environment that i'm very
141:24 - familiar with and so this can
141:26 - approximate exactly what i'm trying to
141:28 - show you here if you notice at the top
141:29 - here
141:30 - this is microsoft azure monitoring logs
141:33 - demo
141:33 - log blade
141:35 - so if we go to queries
141:37 - and we go down to virtual machines
141:39 - and we'd look at what data is being
141:40 - collected we can hit run here
141:43 - and this is the kind of information we
141:44 - are trying to see right this is what we
141:47 - want to see what it's actually
141:47 - collecting
141:49 - that's okay we're not doing remote
141:51 - desktop anymore here and if we go to
141:53 - virtual memory available we'll just
141:54 - clear that out there
141:56 - and run that
141:59 - this is kind of the information we're
142:01 - trying to find all right so i just
142:03 - wanted to show you that when you collect
142:04 - those metrics that you can run these
142:06 - queries and this is that language up
142:08 - there with cousteau
142:10 - and that's pretty much it that's all i
142:11 - really wanted to show you so let's go
142:13 - ahead and tear down all our stuff it's
142:16 - all self-contained within a resource
142:17 - group so what we can do
142:19 - is go to all resources in the top left
142:21 - corner there
142:22 - and we called it dax so we can
142:26 - search for
142:27 - dax here whoops
142:30 - actually it's all over the place we just
142:32 - click here into the name
142:34 - and we'll hit delete resource group
142:38 - type in dax
142:39 - we have a lot of resources created for
142:41 - this one
142:42 - and there you go so yeah hopefully that
142:44 - helps you understand uh those
142:46 - relationships between automation
142:47 - accounts where we can run run books do
142:49 - patching uh you know stuff like that
142:52 - install python stuff um you know the
142:54 - fact that
142:56 - log log analytics is basically a data
142:59 - lake for all your data that you can
143:01 - create alerts uh that you have metrics
143:03 - you have guest os metrics which collect
143:05 - those three values the
143:07 - memory the network and the additional
143:09 - cpu information or disk space
143:12 - and just the variance there so you don't
143:14 - get stuck so there you go
143:16 - [Music]
143:20 - hey this is andrew brown from exam pro
143:22 - and this follow along we're going to
143:23 - learn about managed identities or system
143:25 - managed identities so that we don't have
143:26 - to pass credentials or hard code them
143:28 - into our virtual machines or other
143:30 - environments so what we're going to do
143:32 - is launch ourselves a new virtual
143:34 - machine this is going to be a windows
143:36 - server so go ahead and hit create we'll
143:38 - create a new virtual machine
143:41 - and from here we'll say create new and i
143:43 - say my
143:45 - managed identity probably say id so i'd
143:48 - have to spell the whole word and we'll
143:49 - just say my managed id
143:52 - and from here
143:54 - central us is fine the zones are fine
143:56 - but we're not going to use ubuntu we're
143:57 - going to hit see all images
144:01 - and from here we'll choose windows 10
144:03 - and there's a lot of options here so i
144:04 - want windows 10 pro
144:06 - um
144:07 - it's a bit hard to decide so i'll go gen
144:09 - 2 2 1 h2 hopefully you'll be okay doing
144:12 - this as well um it's using standard b2s
144:15 - 46 so just make sure you're not
144:17 - overspending so that should be fine i
144:19 - think we need at least a b2s to run this
144:22 - um but we're not going to keep it around
144:24 - for very long so username i'm going to
144:26 - go admin user and for the password
144:28 - capital t testing one two three four
144:30 - five six
144:31 - i can do that again capital t testing
144:33 - one two three four five six exclamation
144:35 - mark just because of the requirements
144:36 - such a pain that's what i always do in
144:38 - these follow alongs
144:41 - and we're going to use rdp to connect to
144:43 - it because we do have to uh remote into
144:46 - it we'll just say i confirm we'll hit
144:47 - review create of course we don't
144:49 - actually have a license but we'll be
144:51 - okay because we're not doing anything
144:52 - too serious
144:53 - and we'll give it a moment here
144:56 - and we'll hit create
145:00 - so that is going to create while that is
145:02 - going i'm going to
145:03 - uh we'll actually have to wait a little
145:05 - bit but we'll make our way over to the
145:06 - resource group because this is where
145:08 - we're going to be adding an im policy at
145:10 - some point
145:11 - so if we go over to
145:13 - i am over here
145:15 - and we'll wait we'll actually wait for
145:17 - this to provision so this takes
145:19 - quite a few minutes so i'll see you back
145:21 - in maybe 5
145:22 - 10 minutes because we are launching
145:24 - windows server they just take longer
145:26 - so after waiting you know five to ten
145:28 - minutes the environment is ready so
145:30 - we'll go ahead and go to that resource
145:31 - and we're going to want to go connect
145:32 - and choose rdp this is going to be
145:34 - remote desktop protocol i believe that's
145:36 - what it stands for if it doesn't not a
145:38 - big deal just know it's called rdp and
145:40 - so we'll have an rdp file we'll double
145:42 - click it we'll hit connect this is going
145:44 - to open up the room remote desktop
145:46 - client and so we will type in admin user
145:50 - and then capital t testing one two three
145:51 - four five six if you're on a mac you're
145:53 - like how do i do this well it is in the
145:56 - mac store just type in remote desktop
145:58 - it can open here i'll just show you what
146:00 - i mean
146:01 - over here we do rdp mac
146:04 - you can see that it's free to download
146:06 - so it's just you download that and you
146:08 - can open those rdp files
146:10 - so we are just waiting for this to spin
146:12 - up this takes a little bit of time so
146:14 - just give it a few minutes
146:19 - all right so after waiting a little
146:20 - while the environment is ready so
146:22 - hopefully you can see what is going on
146:23 - here and so what we want to do is open
146:26 - up powershell so we'll type in
146:27 - powershell here
146:29 - and we will right click and run as
146:31 - administrator this does have an outbound
146:33 - connection the internet so i know that
146:34 - uh we are okay for our connection and
146:36 - then this is where
146:38 - you will need to install azure modules
146:40 - because it's not
146:42 - pre-installed and this part
146:44 - really sucks because we have to wait
146:46 - a lot more again so i hope you like
146:48 - waiting a lot but anyway what we'll do
146:50 - is right click this properties here and
146:51 - we will bump this up because this font
146:53 - is super super small
146:55 - and uh we'll type in install
146:58 - module
147:00 - a z and i'll install all the commandlets
147:03 - for azure or azure and we'll wait for
147:06 - some prompts the prompts are a bit
147:07 - finicky
147:08 - hopefully we can just do a for all but
147:10 - you might have to hit yes a couple times
147:14 - but this part is just so so so slow
147:18 - there we go so notice it doesn't let me
147:20 - say all we'll just say y
147:23 - and i know that it's going to prompt us
147:24 - again so we'll just wait a little bit
147:26 - here the progress will show up at the
147:28 - top it's kind of glitchy um yeah so now
147:31 - we'll do a for all
147:34 - and so now we just wait a long time now
147:36 - it seems like it's not doing anything
147:37 - but you will eventually see something
147:39 - appear up at the top here that shows you
147:41 - that there's progress but you just have
147:42 - to be very patient
147:45 - yeah so it'll look like that i'm just
147:47 - showing you what it looks like and i'm
147:48 - gonna go back and pause till this is all
147:50 - done because it takes forever
147:53 - all right so i think it's done it's
147:54 - sometimes hard to tell because it'll
147:56 - just kind of like blank out here
147:59 - and
148:00 - it's not
148:02 - doing anything so i'm not 100 sure but
148:04 - what i'm going to do i'm going to see if
148:06 - i can just open it twice
148:09 - this is just the powershell experience
148:12 - right and um
148:14 - to test this we'll just do a connect and
148:16 - see if it works so i'm going to do
148:17 - connect i just want to close the old one
148:19 - if it's actually still doing something
148:20 - connect account
148:22 - identity
148:25 - and so if this prompts us with something
148:27 - then we know that it's working correctly
148:32 - so
148:33 - identity not found and that's fine so
148:35 - the thing is if we just just did this it
148:36 - would actually prompt us to log in
148:39 - so see it's prompting us to log with
148:40 - microsoft so this this worked
148:42 - um but we need to actually connect an
148:44 - identity to this virtual machine so
148:46 - that's what we're gonna do so we'll
148:47 - minimize this we'll go back over here so
148:50 - go into your
148:51 - uh the resource group that we had before
148:53 - under iem and we're going to add
148:56 - a new role assignment and from here we
148:59 - will uh choose contributor because that
149:01 - will give us enough access for stuff
149:02 - we'll choose manage identity
149:04 - and we will
149:06 - choose system identity
149:10 - no no no no that's not what i want
149:12 - because it should show the virtual
149:13 - machine here
149:15 - maybe it's because um we opened this
149:17 - before it was done creating so that's
149:18 - why it's not showing up so we'll try
149:20 - this one more time we'll go to
149:21 - contributor because it should show the
149:22 - virtual machine manage identity and
149:24 - select
149:26 - and
149:26 - [Music]
149:28 - i want to see
149:30 - the virtual machine
149:33 - weird so i'll click way out i'll go back
149:36 - in i'll go to i am again we'll go to oh
149:40 - you know what
149:42 - we launched this virtual machine we
149:43 - forgot to check box something
149:45 - so when you launch the virtual machine
149:47 - you're supposed to checkbox manage
149:49 - system identities here
149:51 - so this would be
149:52 - [Music]
149:54 - where would it be
149:56 - i only ever do it through the setup i
149:58 - never ever have to retroactively do it
150:02 - oh what a pain identity maybe
150:05 - system managed so we'll just say on and
150:06 - we'll save it
150:08 - uh grant permissions do you want to
150:10 - enable a system energetic yes
150:13 - but normally when you you do the virtual
150:15 - machine hold on here
150:18 - we'll just go pretend like we're setting
150:20 - up a new one we're not actually going to
150:21 - set one up but when you are setting it
150:22 - up
150:25 - and we go over to advanced
150:28 - no no management
150:31 - we would check box check box it on uh
150:34 - here
150:35 - that's not what we did
150:36 - so we forgot to do that but we turned it
150:38 - on here so hopefully it works without us
150:40 - having to restart the image if we go to
150:42 - role assignments over here
150:44 - i mean that's just another way of
150:45 - getting to this page so it doesn't
150:46 - really matter for the resource group
150:48 - what we'll do is go back to our resource
150:50 - group
150:50 - we will find it here
150:53 - and we will go to
150:55 - [Music]
150:58 - i am and we will add a role assignment
151:02 - and we will add contributor we will go
151:05 - next we will say manage identity we will
151:07 - select we will go to virtual machine we
151:09 - will select this one we'll hit select
151:12 - we'll review and assign we'll review and
151:14 - assign and then it will sign us
151:15 - contributor access so now we should be
151:17 - able to connect our identity and then
151:19 - also get access to some stuff
151:21 - um
151:23 - because what we really want is to do get
151:25 - a z like to just show that we can get
151:26 - anything because right now we do this
151:28 - get ac subscription
151:30 - we're not
151:31 - we're not um
151:33 - connected in any way so we'll go here
151:35 - and we'll do identity
151:37 - fingers cross it just works yes it does
151:39 - that was nice if we didn't have to
151:40 - restart the machine
151:41 - and so now if we do
151:43 - get
151:44 - ac subscription it should work
151:51 - so i don't see it working do i type it
151:54 - wrong the term is not recognized
151:57 - oh i must have typed it wrong
151:59 - subscription here
152:01 - let's get this description so if we were
152:03 - not authenticated that would not work
152:05 - and so it does work now
152:07 - so we are pretty much
152:10 - all done here well we could probably set
152:12 - some resources just to make sure this is
152:13 - working properly so maybe go get
152:15 - a z resource like that
152:21 - there so list some resources and it's
152:22 - only listing what's in the um
152:24 - the resource group because that's all it
152:26 - has permissions too it's not going to
152:27 - show things outside of that
152:30 - because that's the scope when we did it
152:31 - at the resource group and that's why we
152:33 - went at the im level there so we are all
152:35 - done we will go ahead and close this out
152:36 - okay we'll make our way back to our
152:38 - resource group we're already on it we'll
152:41 - go ahead and delete this resource group
152:46 - say delete
152:49 - and there you go
152:52 - [Music]
152:56 - hey this is andrew brown from exam pro
152:57 - and we are looking at virtual machines
152:59 - for azure cheat sheets so let's jump
153:00 - into it so azure virtual machines allows
153:03 - you to create linux and windows virtual
153:05 - machines the size of the virtual machine
153:06 - is determined by the image the image
153:08 - defines the combination of vpcus memory
153:10 - storage capacity the current limit on a
153:12 - prescription is 20 vms per region vms
153:15 - are built at an hourly rate a single
153:17 - instance vm has an availability of 99.9
153:19 - percent when all storage disks are
153:21 - premium two instances deployed into
153:23 - availability set will give you 99.5 95
153:26 - availability
153:28 - you can attach multiple managed disks to
153:29 - an azure vm when you launch an azure
153:32 - virtual machine other networking
153:33 - components will be either created or
153:35 - associated to your virtual machine
153:37 - such as nsgs interface cards
153:40 - ips and v-net you can bring your own
153:42 - linux
153:43 - by creating your own virtual hard disk
153:46 - azure vm comes with a variety of sizes
153:47 - that are also uh optimized for specific
153:50 - use cases and there's a lot of different
153:51 - ones here uh so it's not super important
153:54 - but i mean you should know the broad
153:55 - categories here uh azure compute unit
153:58 - acu provides a way of comparing uh cpu
154:01 - performance across azure skus and the
154:04 - standard one that the one they're all
154:05 - benchmarked against is standard a1 all
154:08 - other skus then represent approxim
154:11 - approximately how fast faster
154:13 - the sku can run on the standard
154:15 - benchmark you can install the azure
154:17 - mobile app so you can monitor your vms
154:19 - on the go
154:20 - hyper-v is microsoft's hardware
154:22 - virtualization product it lets you
154:24 - create and run software versions of a
154:26 - computer called virtual machines that's
154:27 - how it all works there are two types or
154:29 - two generations of hyper vm generation
154:32 - one which works with most operating
154:34 - systems and generation two that supports
154:36 - most 64-bit versions of windows and
154:38 - current
154:39 - versions of linux and freebsc uh
154:41 - operating systems
154:43 - hyper v hyper vvms are packaged into
154:45 - virtual hard disk formats such as vhd
154:48 - vhdx files not on the exam but just
154:50 - understand that uh hyper vm generations
154:53 - are different from the azure hyper vm
154:55 - generation so it doesn't have all the
154:57 - features of hyper vm okay
155:00 - there are three ways to connect to
155:01 - virtual machines the first is via secure
155:03 - shell it's not sure shell it's secure
155:07 - shell
155:08 - but yeah so that'll have it get fixed at
155:10 - some point ssh happens on port 22
155:13 - and rsa key pairs are commonly used for
155:16 - authorized access then you have rdp
155:18 - which is a graphical interface to
155:20 - connect to another computer over the
155:22 - network connection this is how you can
155:23 - remotely connect to the windows server
155:25 - via virtual desktop this happens on port
155:28 - 3389 on tcp and udp now i mark these red
155:32 - because i want you to remember these two
155:33 - so make sure you know them okay
155:36 - azure bastions and we'll clear that
155:37 - because now it's getting a bit of a mess
155:39 - here
155:40 - there we go so ask your
155:42 - azure bastion a service you can deploy
155:45 - lets you connect to a virtual machine
155:47 - using your browser and azure portal
155:49 - supports both ssh and rdp useful when
155:51 - you only have a browser like a
155:53 - chromebook or do not have permission to
155:54 - configure or install software then you
155:56 - have update management this allows you
155:58 - to manage and install operating system
156:00 - updates and patches for both windows and
156:01 - linux virtual machines that are deployed
156:03 - in azure on-premise or in other cloud
156:05 - providers update management will perform
156:07 - a scan or update compliance a compliance
156:09 - scan by default is performed every 12
156:11 - hours on a windows and three hours on a
156:13 - linux machine that can take between 30
156:15 - minutes and six hours for the dashboard
156:17 - to display updated data from the managed
156:19 - computer so there you go that is the
156:21 - virtual machines chi chi
156:22 - [Music]
156:26 - so let's take a look at arm templates uh
156:28 - and before we jump into it let's just
156:30 - make sure we're familiar with
156:31 - infrastructure as code because that's
156:33 - what arm templates are all about and
156:34 - this is the process of managing and
156:36 - provisioning computer center
156:38 - data centers such as azure through
156:40 - machine readable definition files such
156:42 - as a json file and in this case an arm
156:44 - template because that's all it is a json
156:46 - file and rather than physical hardware
156:48 - configuration or interactive
156:49 - configuration tools and just to simplify
156:52 - that even further basically you're
156:54 - writing a script that's going to set up
156:55 - your cloud services for you and with
156:58 - iacs they usually come in two different
157:00 - flavors we have declarative that's where
157:03 - exactly what you write is what you get
157:04 - and imperative so you're going to
157:06 - generally define what you want and then
157:07 - the service will guess what you want
157:09 - it's going to fill in the blanks to save
157:11 - you a lot of time writing those scripts
157:13 - and so
157:14 - arm templates are json files that are
157:17 - that define azure resources that you
157:19 - want to provision uh and azure services
157:22 - that you want to configure and with arm
157:24 - templates they are going to be
157:25 - declarative so you get exactly what you
157:27 - define uh they're gonna stand up tear
157:30 - down or share entire architectures in
157:32 - minutes and they're going to reduce
157:33 - configuration mistakes and you know
157:35 - exactly what you have defined for a
157:37 - stack to establish an architectural
157:39 - baseline for compliance so you
157:41 - definitely want to use arm templates or
157:44 - iac whenever you can and try to avoid
157:47 - using the console unless you're doing
157:48 - development or test workloads but let's
157:51 - just run through the big list of what
157:53 - you can do with an arm template i know
157:54 - this is a big boring list but let's just
157:56 - get it get through as quickly as
157:57 - possible just so we know what we can do
157:59 - with arm templates
158:01 - so with arm templates that are
158:02 - declarative so what you see is what you
158:04 - get you can stand up tear down share
158:06 - entire environments minutes you can
158:08 - reduce configuration mistakes you can
158:10 - establish an architectural baseline for
158:12 - compliance they're modular so you can
158:14 - break up your architecture multiple
158:16 - files and reuse them they're extendable
158:18 - so you can add powershell and bash
158:19 - scripts to your templates uh it has
158:22 - testing so you can use the arm ttk
158:25 - to test make sure exactly what what
158:27 - you've deployed is what you wanted
158:30 - you have preview changes so before you
158:32 - create infrastructure via templates you
158:34 - see what it will create built-in
158:36 - validation so it will only deploy your
158:38 - template if it passes track deployments
158:40 - keep track of changes to your
158:41 - architecture over time policy is code so
158:43 - apply azure policies to ensure you you
158:45 - remain compliant uh and then you have
158:47 - microsoft blueprints which we we did
158:49 - cover here uh which establishes
158:51 - relationships between resources and
158:53 - templates so it's just one step further
158:55 - where arm templates don't have any
158:57 - relationship with the resources where
158:58 - blueprints is like a better version of
159:00 - our templates and then you have ci cd
159:02 - integration exportable code so exporting
159:05 - the current state of the resource groups
159:07 - and resources
159:08 - authoring tools i believe is the last on
159:10 - our list here so you can use visual
159:11 - studio code that has advanced features
159:13 - for authoring arm templates makes it a
159:15 - lot easier to write them so there you go
159:17 - [Music]
159:21 - so now we were talking about what arm
159:23 - templates can do let's actually take a
159:25 - look at what one looks like and so just
159:27 - to get a good snapshot or overview let's
159:30 - just define what the skeleton is the
159:31 - general structure of an arm template so
159:34 - here on the right hand side we have json
159:36 - and you can see that we have a structure
159:38 - there so we have schema content version
159:40 - api profile parameters variables
159:41 - functions resources outputs and let's go
159:44 - down the list here and see what all
159:45 - these things are so the schema describes
159:48 - the properties that are available within
159:49 - a template
159:50 - and so the idea is that you have that
159:52 - json link there and it's going to say we
159:54 - expect the schema the actual structure
159:56 - to have these parameters then you have
159:59 - the content version this is the version
160:00 - of the template and you can provide any
160:02 - value for this element it's totally
160:04 - abstract it's just your way of keeping
160:06 - track of the version of your current
160:08 - template then you have the api profile
160:10 - you use this value to avoid having to
160:12 - specify api versions for each resource
160:14 - in the template you have parameters
160:16 - these are values you can pass along to
160:18 - your template you have variables this is
160:19 - where you transform your parameters or
160:21 - resource properties using function
160:23 - function expressions you have functions
160:25 - these are user-defined functions
160:26 - available within the template you have
160:28 - resources these are the azure resources
160:30 - you want to deploy or update and then
160:32 - you have outputs and these are the
160:33 - values that are returned after
160:35 - deployment so let's go
160:37 - more into deep into some of these things
160:39 - [Music]
160:43 - so let's take a deeper look here at the
160:45 - a resource section within your arm
160:47 - template and so a resource is an azure
160:49 - resource you want to provision could be
160:51 - a virtual machine could be a database
160:53 - and so here on the right hand side look
160:55 - at where it says resources and we're
160:57 - going to take a a breakdown of the
160:59 - actual columns or tributes that we can
161:02 - set so the first thing is the type and
161:04 - this is going to follow the format of
161:05 - the resource provider and resource type
161:07 - so there you can see that we are setting
161:09 - up a storage account then we have the
161:11 - api version and this is the version of
161:13 - rest api that we're going to use for
161:14 - that resource and each resource provider
161:16 - publishes its own api version so you've
161:18 - got to go look up each one because they
161:20 - all could be different then you have the
161:22 - name of the resource and so i believe
161:25 - there that is using a variable so that
161:27 - is going to be dynamic then you have the
161:29 - location so most resources have a
161:31 - location property and that's the region
161:33 - that you wanted to be deployed in
161:36 - then you have other properties and other
161:38 - properties can
161:40 - will be
161:41 - based on different kinds of resources so
161:45 - for a storage account you want to be
161:46 - able to set the kind and within the
161:48 - properties you're going to have a bunch
161:50 - of other options there and it's just
161:51 - going to vary okay
161:53 - but anyway let's take a closer look next
161:55 - at parameters
162:00 - so let's take a closer look here at
162:01 - parameters and these allow you to pass
162:03 - variables to your arm templates so on
162:05 - the right hand side
162:07 - we have
162:08 - our arm template and we saw before
162:10 - resources but if you look above there we
162:12 - have a thing called parameters and so
162:14 - there we are setting a parameter and
162:15 - then down below in the resource we are
162:17 - accessing um a parameter
162:20 - and so
162:21 - let's just actually talk about some of
162:23 - the parameter options we have so the
162:25 - first thing you'll want to do is set a
162:26 - type and on the right hand side you can
162:28 - see that we're saying as a string and so
162:30 - we could define that as a string secure
162:31 - string in blue object secure object and
162:34 - array
162:35 - and there are some other things that we
162:36 - can provide when we when we are defining
162:38 - your parameters that we're not showing
162:39 - in that diagram but we have default
162:41 - values so this is if you do not provide
162:44 - if there's no value being passed in this
162:45 - is what it should use then you have
162:47 - allowed value so this is where it's an
162:49 - array and you're restricting it to only
162:51 - the values that are there then you have
162:52 - the min value so if you let's say it's
162:55 - an integer you want to say this is the
162:56 - minimum value so it can only like
162:58 - nothing lower than 10 then you the max
163:00 - value so nothing above 10 you have the
163:02 - minimum length so if you're working with
163:03 - strings you might say it has to be a
163:05 - minimum of five characters
163:07 - then you have the maximum so let's say
163:09 - you have a maximum of five characters uh
163:11 - then you have the description so this is
163:13 - just going to be something that when you
163:15 - are using the arm template within the
163:16 - portal you know what it's for so if you
163:18 - don't have a great
163:21 - description for that parameter or name
163:23 - and you want to provide more information
163:24 - that's what you can do so there you go
163:26 - [Music]
163:30 - so let's take a look here at functions
163:31 - for arm templates and these allow you to
163:34 - apply transformations to your arm
163:36 - variables and they come in two flavors
163:38 - we have template functions which are
163:39 - built in functions and then user defined
163:42 - functions which are custom functions
163:43 - that we're creating to allow what other
163:45 - kind of functionality that we want so
163:48 - the way functions work is you give the
163:51 - function name so here it's called equals
163:53 - and then you have parentheses and you
163:54 - pass in uh what you want to transform so
163:57 - if you see parentheses that's how you
163:59 - know that is a function
164:01 - so let's talk about some of the built-in
164:02 - functions and so there are a lot
164:04 - available for us and they're generally
164:06 - self-explanatory so i'm not going to be
164:08 - showing examples but let's just go
164:10 - quickly to the list so you have array
164:11 - functions like array concat contains
164:14 - create array empty first etc then you
164:17 - have comparisons like less equals less
164:20 - lesser equals greater greater or equals
164:22 - you have date functions uh you have
164:25 - deployment functions
164:27 - and for example parameters and variables
164:29 - are actually functions
164:30 - so that's kind of interesting then you
164:32 - have a logical operator so end or if not
164:35 - or
164:36 - numerical um functions like add div
164:39 - float int min max object so contains
164:43 - empty intersection and then you have
164:45 - resources so
164:47 - extension resource id
164:49 - providers references etc like that uh
164:51 - and there's one more i think we have
164:53 - string so we have base64
164:55 - concat contains etc i'm not going to
164:57 - show you how to do user defined
164:59 - functions i don't think that's that
165:00 - important
165:02 - but i just want to show you that you
165:03 - have a lot of functions available to you
165:05 - okay
165:06 - [Music]
165:10 - so let's take a look at variables and
165:11 - variables are used to simplify your arm
165:14 - templates and the idea is that you're
165:15 - transforming parameter and resource
165:17 - properties using functions and then
165:18 - assigning them to reusable variables so
165:21 - under your variables section notice that
165:23 - we have storage name and on the right
165:24 - hand side we're just using a bunch of
165:26 - functions and assign it to storage name
165:28 - so the idea is that when we have to go
165:30 - call them in our resources it's going to
165:31 - be a lot shorter than if we had to
165:33 - write that whole thing in there and
165:35 - what's more interesting is that you can
165:36 - nest your variables within a json object
165:39 - so on the left-hand side you have this
165:42 - variables environment settings test
165:43 - instance sizes etc and then on the
165:46 - right-hand side we have a parameter
165:48 - that we're defined here and then down
165:50 - below you can see that we're calling
165:52 - that parameter and then we're calling
165:54 - variables environment settings and then
165:55 - using the square braces to then call
165:58 - within that nested object i know that
166:00 - seems really confusing but just take a
166:01 - moment to look at it
166:03 - and it will make sense and you'll just
166:05 - understand that there's a lot of power
166:06 - there so there you go
166:12 - so last on our list here for uh arm
166:14 - templates is outputs and outputs return
166:16 - values from deployed resources so you
166:18 - can use them programmatically uh this is
166:20 - very simple the idea here is that you
166:22 - have outputs and let's say you want uh
166:25 - to output the resource id so you can say
166:27 - that it's a string and in the value you
166:29 - could use a variable or a bunch of
166:30 - functions and the idea is that when you
166:33 - use the cli
166:34 - so imagine here you are using the cli
166:36 - and you're saying show me this
166:37 - deployment group and
166:39 - i want to
166:41 - see the output of value so see where it
166:42 - says query says properties output
166:44 - resource id value the idea is that now
166:46 - i'm going to be able to pragmatically
166:48 - access it after i've created the
166:49 - resource and this is a great way of
166:51 - chaining things so you might have a
166:53 - bunch of arm templates you want to run
166:54 - in sequence and you're going to need the
166:56 - outputs to go into the next one and that
166:58 - is what you're going to use outputs for
166:59 - so there you go
167:00 - [Music]
167:04 - hey this is andrew brown from exam pro
167:06 - and we are looking at azure resource
167:08 - manager templates also known as arm
167:09 - templates and this helps you deliver
167:12 - infrastructure as code meaning that when
167:14 - you have a resource such as a virtual
167:15 - machine or a storage account instead of
167:17 - manually configuring it every single
167:19 - time through
167:21 - the portal what you can do is provide a
167:23 - configuration file that defines all the
167:25 - properties that you want it to be
167:26 - configured with and the idea is that you
167:28 - can
167:29 - keep this file and share with other
167:31 - other people so they can easily create
167:33 - the same resources as you
167:35 - and then you know exactly how your stuff
167:37 - is is configured so what we're going to
167:39 - do is uh launch a new template now you
167:42 - can't go up here and just type in arm
167:44 - because
167:46 - these arm templates are managed at
167:48 - different levels so at one level is a
167:49 - script subscription or they're resource
167:51 - groups so when you have a resource group
167:53 - you have deployments within them and
167:54 - that's where
167:55 - these templates are deployed
167:57 - but just to deploy one from here what
167:59 - we're going to do is type in deploy why
168:01 - they didn't make it so you can type in
168:02 - arm i do not know but if you go down
168:04 - here we have deploy a custom template
168:07 - and so from here we have some common
168:09 - templates so if i click into web app and
168:11 - i go edit a template we already have
168:12 - some stuff pre-filled in i'm just going
168:14 - to go back and discard that go back to
168:16 - select a template and we're going to
168:17 - build our own and by default we'll have
168:20 - that schema that content version which
168:21 - is 1.0.0.0
168:23 - our parameters and our resources so
168:26 - today i want to launch a virtual machine
168:28 - and what you normally would have to do
168:30 - is go here uh and look up what is that
168:32 - you want to create so if it's this uh
168:34 - microsoft compute virtual machine you'd
168:36 - go through here and you'd have to make
168:38 - sure you have all these uh properties so
168:40 - you define the resource here right the
168:43 - type
168:45 - and then you define the properties that
168:46 - you want and down below you can go
168:48 - through here and see them all that's a
168:50 - lot of work i don't want to do that so
168:52 - i'll go to add resource here drop this
168:54 - down and click
168:56 - uh where's it virtual machine where are
168:58 - you there you are
168:59 - and i'm going to call this one wharf
169:01 - and then wharf and wharf because it's
169:03 - not just going to create a virtual
169:04 - machine it's going to create the other
169:06 - things that i need with it as well such
169:08 - as the storage account the network
169:10 - interface and the virtual network
169:12 - so you can see that we have a bunch of
169:14 - parameters here so the name
169:17 - the type the name the admin username the
169:19 - password and the os version
169:22 - oh you know what i think i chose a
169:24 - windows one i do not want a windows one
169:26 - i want a linux one
169:28 - because that is easier for me to work
169:29 - with here so we choose ubuntu so i'll
169:31 - just fill this in again
169:35 - all right and so um back up here you
169:38 - know we have the ubuntu version between
169:40 - some versions here and then there's the
169:42 - type so that's for uh replication then
169:45 - we have variables here so if we go to vm
169:47 - size this is the vm it will it will set
169:49 - here
169:50 - variables are either you can have string
169:53 - values or you can use functions to
169:54 - transform other parameters into
169:57 - other stuff that you'll reference
169:58 - throughout your template then down below
170:00 - we have those resources here so what
170:03 - we'll do
170:04 - is actually i'm going to copy this
170:06 - because it's very highly likely we're
170:08 - going to want to make some kind of
170:09 - change and so i have vs code over here
170:11 - on the on the left or right hand side
170:13 - i'm just going to paste that on in there
170:16 - and what we will do this is a json file
170:18 - make things a little bit easier here
170:21 - great and what i'll do is just move that
170:23 - off screen and we'll go ahead
170:25 - and we will save this and we'll see if
170:28 - we can deploy this
170:30 - so i'm going to type in wharf here and
170:33 - we'll launch in canada east i'll name
170:34 - this wharf we will name the username
170:37 - warf but lowercase and then we'll do
170:39 - testing one two three four five six
170:41 - capital on the t
170:43 - notice that it is hidden there
170:45 - and then we will choose 14 which is
170:47 - defaulted here and lrs we'll go ahead
170:50 - and do review and create
170:52 - and we'll hit create here
170:55 - so this is going to fail i already know
170:57 - because it has a misconfiguration it'll
170:59 - tell us how but while that's going we'll
171:01 - take a look at our input so this is the
171:03 - values that were inputted these are the
171:05 - outputs if we had defined any which we
171:07 - have not
171:08 - um and if we go back to our template i
171:10 - just wanted to show you that we have
171:12 - that secure string so when we were
171:13 - typing our password that's why we didn't
171:15 - see it
171:16 - so just things like
171:18 - that so i'll go back up here and our
171:21 - deploy failed why what happened
171:23 - so we open it up here the requested vm
171:25 - size standard d1 is not available in the
171:27 - current region so the template we have
171:29 - is not that great
171:31 - it needs some configuration because we
171:33 - can't use d1 i think that doesn't exist
171:36 - anymore and so what we really want to
171:38 - use is the standard b1 ls
171:40 - all right
171:41 - standard b1 ls so i'm going to cut that
171:46 - and for the time being i'm going to go
171:48 - back to our original template
171:51 - and this is one big template
171:55 - i'm going to look for those variables oh
171:56 - they're all the way at the bottom here
171:57 - nice and so i'm going to just go ahead
171:59 - and paste that in
172:01 - b1 ls just double check making sure i
172:03 - spelled that right standard
172:06 - standard b1 ls looks good to me
172:10 - so i'm going to move that off screen and
172:12 - the question is what do we do
172:14 - what do we do when a deploy fails so
172:17 - let's go take a look at what has
172:19 - happened here so this all got deployed
172:20 - into a resource group and under here
172:22 - this is where our deployments are
172:24 - so when we look at this
172:26 - template it we can see that it failed we
172:29 - could click into here get the same
172:30 - information
172:32 - and if we click into here it just brings
172:33 - us back to where we just were but if we
172:36 - go look at what was actually deployed
172:39 - under our resource group
172:41 - under the overview we'll notice that it
172:43 - created the virtual network the storage
172:44 - account and the network interface so
172:46 - when it fails it creates what it can but
172:48 - it doesn't roll back
172:50 - okay
172:50 - so the question is is then how do you do
172:53 - cleanup so you might think i'll go to
172:54 - deployments and what i'll do is go ahead
172:57 - and delete that template and we can go
172:59 - ahead and do that which by the way you
173:01 - can't edit this template all you can do
173:03 - is
173:05 - all we can do here see i just want to
173:07 - show you that you cannot edit it
173:09 - we can download it and stuff like that
173:11 - but so you might think well if i go
173:14 - ahead and delete that template
173:17 - just making sure we're in the right
173:18 - place here
173:20 - you might think that might roll back
173:22 - those resources but it doesn't it just
173:23 - deletes the template so if you really
173:26 - want to get rid of the stuff what you
173:27 - got to do
173:28 - is go ahead and delete all these
173:30 - resources manually
173:34 - so um
173:35 - i wish it kind of had a rollback feature
173:37 - but that's just how it is
173:38 - but there are some nice things that uh
173:40 - azure does here which we'll talk about
173:42 - in a moment so i think we have adjusted
173:44 - it to the correct value now so hopefully
173:46 - this is going to be all we need to make
173:48 - it work
173:49 - so what we'll do
173:52 - is go to our deployments here
173:54 - and we can't do it here but so we'll go
173:56 - back to the top here and type in deploy
173:58 - and we'll go to custom template
174:01 - and what we'll do is build our own
174:03 - template in our editor and i'm just
174:05 - going to
174:06 - copy the contents here
174:12 - okay we'll go copy
174:15 - and i will go
174:17 - paste
174:18 - and we'll make sure that this is all
174:19 - good looks fine to me we'll go ahead and
174:21 - hit save
174:23 - and we will choose wharf so we don't
174:25 - have to make a new one and we will fill
174:27 - in uh the name as worf
174:30 - the username is warf i'll call warf2
174:31 - just in case
174:33 - helps us keep track of what we're doing
174:34 - here
174:35 - testing one two three four five six with
174:37 - a capital on the t
174:39 - 14 l rls lrs and we'll go ahead oh we
174:44 - have one issue here
174:46 - cannot deploy a resource group
174:48 - worth deleting
174:51 - um we'll go back
174:56 - and we will hit create here
174:59 - i don't think i deleted the resource
175:00 - group let me just go double check
175:03 - i almost i'm almost certain i deleted
175:05 - all the contents of it right
175:08 - oh so there's already one here so we're
175:10 - just waiting for that to delete
175:13 - just gonna go delete for us please thank
175:15 - you
175:23 - it failed to delete we'll go take a look
175:25 - as to why
175:28 - resource is not found
175:36 - uh we'll go back to our resource groups
175:40 - give us a refresh here
175:43 - okay so you know what i must have
175:44 - deleted the resource group which is
175:45 - totally fine
175:47 - i i could have saw i only deleted the
175:48 - contents of it
175:50 - but we'll just call this worth regular
175:52 - then
175:53 - we'll go ahead and hit
175:54 - create here
175:56 - and so this time
175:58 - i have better feeling about this
176:08 - and so we will just have to wait a
176:10 - little bit it won't take too long
176:15 - i'll see you back in a moment
176:17 - okay so after waiting a little bit here
176:18 - our our thing seems to be deployed so if
176:20 - we go to resource groups we can see that
176:23 - our virtual machine is deployed so
176:25 - that's pretty much all there is to it
176:27 - one other thing i'd like to show you is
176:29 - that whatever you have whatever is in
176:31 - your resource group you can actually
176:32 - export the template so
176:34 - if you did configure something manually
176:36 - all you'd have to do is find the
176:37 - resource
176:38 - go up here to up
176:40 - it is export template
176:43 - and there's your template
176:44 - so it just has that single resource in
176:46 - there i can't remember if
176:48 - if i go into here if i select multiples
176:51 - um and i go x exper where's it
176:54 - export template
176:57 - look it's gonna include all that stuff
176:58 - so if you already have existing
177:01 - resources that you provisioned and you
177:03 - want to have them that's what you can do
177:05 - notice that some things won't be
177:06 - included in the template when you do
177:07 - that but you can just go ahead and
177:09 - download them and then you have them for
177:11 - later so yeah that's all there really is
177:13 - to
177:14 - arm other than learning the uh the nitty
177:17 - gritties of the actual language that's
177:19 - just how you work with it there so what
177:21 - i'm going to do is make my way over to
177:23 - my resource group here
177:24 - and i'm just going to go ahead and
177:26 - delete this here
177:32 - and we're all good to go
177:36 - [Music]
177:40 - hey this is andrew brown from exam pro
177:42 - and we are looking at azure resource
177:43 - manager templates also known as arm
177:45 - template so the thing we need to know is
177:47 - infrastructure as code this is the
177:48 - process of managing and provisioning
177:50 - computer data centers through machine
177:52 - readable definition files such as json
177:54 - files rather than physical hardware
177:56 - configuration or interactive
177:57 - configuration tools iec comes in two
177:59 - flavors we've got declarative which is
178:02 - what you define is what you get and
178:04 - imperative so you say generally what you
178:06 - want and it will guess or it'll fill in
178:08 - the blanks for you uh for arm templates
178:11 - uh they are json files as we're
178:13 - suggesting above there that want to
178:15 - provision azure services you want to
178:17 - configure they are declarative so you
178:19 - get exactly what you define
178:21 - an arm template is made of the following
178:23 - json structure you should know the
178:25 - structure pretty well because you might
178:26 - see questions where they're showing you
178:28 - that code so be sure you know all these
178:29 - parts schema describes the properties
178:31 - that are available within a template
178:33 - content version the version of the
178:34 - template you can provide any value for
178:36 - this element api profile use this value
178:38 - to avoid having to specify api versions
178:40 - for each resource in the template
178:41 - parameters values you can pass along to
178:43 - your template variables uh things you
178:46 - can transform parameters or resource
178:48 - properties using functions and
178:49 - expressions uh and i did include them in
178:52 - this the cheat sheet but there is a
178:53 - bunch of variations on variables and
178:55 - functions but i think that's practical
178:57 - skills you should just grab we don't
178:59 - want this in a cheat sheet functions
179:01 - used to find functions available within
179:02 - the template there are so many functions
179:04 - it wasn't even worth pulling them out
179:06 - and putting some here
179:07 - resources the azure resources you want
179:09 - to deploy or update and under resources
179:12 - you've got type of resource api version
179:14 - the name the location other properties
179:16 - which can be a bunch of different stuff
179:18 - so there's no consistency there and then
179:20 - outputs which are values that are
179:22 - returned after deployment so you can do
179:23 - things with them pragmatically so there
179:25 - you go that is the arm templates cheat
179:27 - sheet
179:27 - [Music]
179:31 - hey this is andrew brown from exam pro
179:33 - and we're looking at azure container
179:34 - instances also known as aci and this
179:36 - allows you to package deploy and manage
179:38 - cloud applications using containers or
179:40 - the way i like to think of it as fully
179:42 - managed docker as a service
179:45 - azure container instances allow you to
179:47 - launch containers without the need to
179:48 - worry about configuring or managing the
179:50 - underlying virtual machines and you're
179:52 - going to be able to design isolate
179:53 - containers for simple applications task
179:55 - automations and build jobs let's talk
179:57 - about some of the uh reasons why you'd
179:59 - want to use containers over vms so
180:01 - containers can be provisioned within
180:03 - seconds where vms will take several
180:04 - minutes containers are built per second
180:06 - where vms are built per hour so you'll
180:08 - save a lot more money containers have
180:10 - granular
180:11 - and custom sizing vpc
180:13 - vpcus memory and gpus where vm sizes are
180:16 - predetermined uh so those are the
180:19 - benefits between containers and vms
180:21 - aci can be utilized for both windows and
180:23 - linux containers you can persist storage
180:26 - with azure files using aci containers
180:28 - and honestly if you have containers or
180:30 - functions you have to have an external
180:32 - storage mounted to persist
180:34 - that's just the way you do it acis are
180:37 - accessed via fully qualified domain
180:39 - names which is one of the things i
180:40 - really appreciate about azure services
180:42 - because mostly services are like that
180:44 - azure provides quick start images to
180:46 - help you start uh launching example apps
180:48 - but you can also source containers from
180:50 - azure container registry docker hub and
180:53 - privately hosted container registries it
180:54 - looks like that the choice option there
180:57 - let's just talk about container groups
180:59 - because this is pretty much the only
181:00 - major component you have to worry about
181:02 - these are a collection of containers
181:03 - that get that get scheduled on the same
181:05 - host and the containers uh the
181:07 - containers in a container group share
181:10 - life cycle resources local network
181:12 - storage volumes so the idea is that you
181:14 - have these tightly coupled containers so
181:16 - all of them act as a service within that
181:18 - container group so here you can see an
181:20 - example of a couple containers that are
181:22 - mounting azure files on different
181:24 - directories there and underneath all of
181:26 - that is running on an azure virtual
181:28 - machine
181:30 - container groups are similar to
181:31 - kubernetes pods
181:33 - yeah and it says similar to but not
181:35 - really the same thing multi-container
181:37 - groups can currently support only linux
181:39 - containers which is kind of a bummer but
181:41 - that's just what it is and there are two
181:43 - ways to deploy a multi-container group
181:45 - you can use arm templates when you need
181:47 - to deploy additional azure service
181:49 - resources or just a yaml file when you
181:52 - want to deploy
181:53 - when your deployment only includes
181:54 - container instances
181:56 - [Music]
182:00 - let's take a look at container restart
182:01 - policies and what these do is allow you
182:04 - to change how the policies uh restart
182:07 - and there's three different ways we have
182:09 - always uh never and on failure so the
182:12 - first one here always means always
182:14 - restart the container and the idea is to
182:17 - keep your container running as long as
182:19 - possible and the reason why you'd want
182:20 - that is if you're running a web server
182:22 - some other providers would call that a
182:24 - service then we have never so run only
182:27 - one time this is great for background
182:29 - jobs and so other providers would just
182:30 - call this a task
182:32 - then you have on failures so containers
182:33 - that encounter an error that's when it
182:35 - should
182:36 - restart
182:38 - and so it's as simple as just choosing
182:39 - that option uh when you uh are creating
182:42 - that container
182:44 - [Music]
182:48 - let's take a look at container
182:49 - environment variables also known as
182:51 - nvars that allow you to pass
182:53 - configuration details to your containers
182:55 - and you can do this through the portal
182:56 - the cli powershell which that's always
182:59 - the case with azure and so it's as
183:00 - simple as just putting in your uh key
183:03 - and your value uh and one other thing i
183:05 - want to point out is that you can also
183:06 - pass in secured environment variables so
183:08 - the idea is that sometimes you don't
183:09 - want them plain text so if you have like
183:11 - so you have that stripe secret key you
183:13 - don't want anyone ever seeing that in
183:15 - production so the idea is that through
183:17 - and i don't think you can do this
183:18 - through the portal but you can do this
183:20 - through the cli or powershell is provide
183:22 - the secure environment variables over
183:24 - the regular one which isn't environment
183:25 - variables and that way you can pass it
183:27 - securely so it's never
183:29 - exposed to human eyes
183:31 - [Music]
183:35 - let's talk about persisting storage and
183:37 - we talked about that a little while
183:39 - there when we were looking at azure
183:40 - files but uh containers are stateless by
183:42 - default so when a container crashes or
183:45 - stops all i'll state is lost to persist
183:48 - a state you need to mount an external
183:50 - volume and there's quite a few different
183:51 - things we can mount as azure files
183:53 - secret volumes empty directory a cloud
183:56 - git repo
183:57 - so you got a few options there and to
183:59 - mount a file volume you need to do this
184:01 - via powershell or cli you're going to
184:03 - give the following detail so when you
184:04 - launch the container there's going to be
184:06 - nothing in the portal so you got to do
184:07 - it this way all right
184:10 - [Music]
184:14 - let's talk about some of the cli options
184:15 - that we can use to troubleshoot our
184:17 - containers a lot of this stuff is
184:19 - probably uh accessible of the portal but
184:22 - it's great to know how to do this via
184:23 - the cli so the first thing is getting
184:26 - the log so you can use azcontainer logs
184:28 - and that's going to bring you logs back
184:31 - then you can use az container attach
184:33 - and this is going to give you diagnostic
184:35 - information about when the container is
184:36 - starting up probably with a cloud in it
184:38 - information then you have a z container
184:40 - exact and this allows you to execute uh
184:43 - remote commands but what you can do is
184:45 - use bin sh which actually will allow you
184:47 - to have an interactive container it's
184:49 - like having the terminal right to the
184:51 - container which is very very very useful
184:54 - and the last is just going grabbing
184:56 - metrics
184:57 - from azure monitor so azmonitor metrics
185:00 - list
185:01 - you could use the portal for that but i
185:02 - just wanted to show you all those things
185:04 - because they might show up on the exam
185:06 - [Music]
185:10 - hey this is andrew brown from exam pro
185:12 - and we're going to take a look at azure
185:14 - container instances so here it is so all
185:16 - we got to do is go to container
185:18 - instances we'll hit add and the nice
185:20 - thing is that azure provides us with a
185:22 - hello world one so it's very easy for us
185:24 - to get started
185:25 - it's a linux machine and it looks like
185:27 - it's pretty inexpensive there so we'll
185:29 - stick with that i'm going to create a
185:31 - new group here we're going to call it
185:33 - banana
185:35 - and we'll name the container instance
185:37 - banana
185:38 - and east u.s 2 seems fine to me you'll
185:41 - notice we're on a quick start image if
185:43 - we wanted we could use something from
185:45 - the docker hub and provide our own link
185:47 - but we'll just stick with the quick
185:49 - start image for today we're going to go
185:51 - ahead and hit next to networking just to
185:54 - see what we have as options you can make
185:56 - it public or private we'll go to
185:58 - advanced hold on here
186:00 - yep those are just the ports you can
186:01 - expose we'll go to advanced and for the
186:04 - restart policy we can set on failure
186:06 - always or never we can pass in
186:08 - environment variables and i've covered
186:10 - this a lot more in detail in the lecture
186:12 - content so we don't need to really dive
186:14 - deep into this
186:16 - and we'll go ahead and create this
186:17 - instance
186:20 - and so we'll have to wait a little while
186:21 - here and i'll see you back in a moment
186:24 - okay and so after a short wait our
186:25 - container instance is ready we'll go to
186:27 - that resource there and take a look
186:30 - around so on the left hand side we can
186:32 - go to our containers and there we can
186:33 - see it running we can see the events
186:35 - down below of what's going on so you can
186:38 - see that it's pulled the image
186:40 - it successfully pulled it and it started
186:42 - the container
186:44 - some properties nothing interesting
186:45 - there the logs if we wanted to see stuff
186:47 - and if we wanted to connect to the
186:48 - instance we could also go here and hit
186:50 - connect which is kind of nice
186:52 - i don't have any purpose to do that
186:54 - right now so and it's also not going to
186:56 - work the way we're doing it but i just
186:57 - wanted to show you you had those
186:59 - opportunities
187:00 - uh you can do identity so that means
187:02 - manage it with role based access
187:04 - controls but what i want to see is
187:06 - actually this hello world working i'm
187:09 - assuming there must be a
187:10 - hello page i've never looked at it
187:12 - before so we're going to go here grab
187:14 - the public ip address and paste it on in
187:16 - the top and there we go so we have
187:18 - deployed a instance onto azure container
187:22 - instances or a container i should say
187:24 - so nothing super exciting to talk about
187:27 - here but we do need to know the basics
187:29 - uh there
187:31 - if we wanted to deploy other containers
187:34 - it's just the one there so that's all
187:35 - you really need to do
187:37 - but yeah
187:38 - so yeah hopefully that gives you an idea
187:40 - there i'll just go back to the list here
187:42 - so we can see it and we'll go ahead and
187:44 - just uh delete that probably do it for
187:46 - the view the resources on the left-hand
187:48 - side like i always like to do
187:50 - and we will go into banana here
187:53 - and we will delete banana
187:56 - and there you go
188:00 - [Music]
188:04 - hey this is andrew brown from exam pro
188:05 - and we're looking at azure container
188:07 - images cheat sheet and this is a two
188:08 - pager so let's jump into it so aci
188:11 - allows you to launch containers without
188:13 - the need to worry about configuring or
188:14 - managing the underlying virtual machine
188:16 - acis is designed for isolate containers
188:18 - for simple applications task automation
188:20 - build jobs containers can be provisioned
188:22 - within seconds where the vms can take
188:24 - several minutes containers are built per
188:26 - second where vms are built per hour so
188:28 - you get greater savings when you're
188:30 - using containers containers have
188:32 - granular and custom sizing vpc memory
188:34 - and gpu when vm sizes are predetermined
188:36 - aci can deploy both on windows and linux
188:39 - you can persist storage with azure files
188:41 - acis
188:43 - are accessed via fully qualified domains
188:45 - so it's just like a domain name
188:47 - then we have container groups these are
188:49 - a collection of containers that get
188:50 - scheduled on the same host machine
188:52 - the container is a container group share
188:54 - so you got lifecycle resources local
188:56 - network storage volumes container groups
188:58 - are similar to kubernetes pods
189:00 - multi-container groups currently support
189:02 - only linux containers this is one of the
189:04 - cases where uh it's not windows it's
189:06 - linux okay there are two ways to deploy
189:09 - multi-container groups you have uh that
189:11 - via an arm template and yaml file i
189:13 - imagine you can do them via blueprints
189:14 - as well a container restart policy
189:17 - specifies what a container should do
189:19 - when their process has been completed
189:20 - aci has three restart policies always
189:23 - never and on
189:24 - failure okay
189:26 - we'll go on to the second one here so
189:28 - azure containers are stateless by
189:29 - default when a container crashes or
189:31 - stops
189:32 - all state is lost to persist state you
189:34 - need to mount an external volume you can
189:36 - mount the following external volumes so
189:38 - either azure file secret volume empty
189:40 - directory or cloud git repo
189:42 - for a container tribute shooting these
189:44 - are just azure cli commands you should
189:46 - really really know so i'm just going to
189:47 - list them out here you've got ac
189:49 - container logs ac container attached az
189:51 - container exact
189:53 - which is for execute at az monitor
189:55 - metrics list so there you go that is the
189:56 - aci images cheat sheet
189:58 - [Music]
190:02 - hey this is andrew brown from exam pro
190:04 - and we're looking at azure container
190:05 - registry also known as acr which create
190:07 - and maintain azure canadian registries
190:09 - to store and manage your private docker
190:11 - container images and related artifacts
190:14 - so azure container registry is a managed
190:16 - private docker registry service based on
190:18 - the open source docker registry 2.0 and
190:22 - the idea here is that you can use
190:24 - container registries with your existing
190:25 - container development deployment
190:27 - pipelines and it also has this thing
190:29 - called registry tasks to build container
190:32 - images in azure and the idea here is
190:34 - that you can pull images uh from your
190:37 - container registry to various deployment
190:39 - targets like kubernetes
190:41 - dc os which i forget what it stands for
190:44 - but it's basically an operating system
190:45 - for containers docker swarm uh and
190:48 - there's also many azure services that
190:50 - directly support acr so
190:52 - aks azure app service azure batch azure
190:56 - service fabric and a lot more so it's
190:59 - just a matter of saying hey use this
191:01 - from acr and it works right away
191:03 - uh developers can also push to container
191:05 - registry as part of a container
191:06 - development workflow with delivery tools
191:08 - such as azure pipelines and jenkins and
191:11 - it's so easy to push
191:13 - to container registry
191:15 - there are many ways to work with acr via
191:17 - the cli the power azure powershell azure
191:21 - portal azure sdk and there's also this
191:23 - really good extension called docker
191:25 - extension for visual studio code
191:27 - something you absolutely want to install
191:29 - and maybe i'll make a full slide on it
191:30 - because i like it so much
191:32 - [Music]
191:36 - let's take a look here at acr tasks
191:38 - which allow you to automate os and
191:40 - framework patching for your docker
191:41 - containers and uh we have a few
191:43 - different options here so the first is
191:45 - quick tasks these allow you to push a
191:47 - single container image to a container
191:49 - registry on demand and you don't need to
191:51 - be running a local docker engine
191:53 - installation to do it so that's a really
191:55 - good benefit
191:56 - uh you can also trigger some automated
191:59 - build actions so maybe you push some
192:01 - source code updates you update the
192:03 - container base image you want to do it
192:04 - on a a timer that's on a schedule you
192:08 - can also have multi-step tasks so you
192:11 - have to go for the various steps to
192:13 - complete that task
192:14 - for each acr task it has an associated
192:17 - source code context this is the location
192:19 - of the set source files used to build a
192:21 - container image or other artifact tasks
192:23 - can take advantage of run variables so
192:25 - reuse task definitions and standardized
192:27 - tags for images and artifacts so there
192:29 - you go
192:30 - hey this is andrew brown from exam pro
192:32 - and this along we are going to be
192:33 - working with acr or also known as
192:34 - container registries so type in acr at
192:37 - the top actually that doesn't work type
192:38 - in container and we will go to the
192:39 - registries and we'll create ourselves a
192:41 - new registry now list to have multiple
192:43 - container repositories or images hosted
192:46 - here on azure so i'm going to just type
192:48 - in my acr and we'll say ok and the
192:52 - register name is going to be my acr i'm
192:54 - just going to put a bunch of numbers
192:55 - here so maybe 8080
192:57 - [Music]
192:59 - it probably doesn't like the hyphens
193:01 - there we go
193:02 - all the names are crazy you just have to
193:04 - flip around to figure it out we're going
193:06 - to choose basic because we can do
193:07 - everything we want on basic
193:09 - we'll go ahead and hit review
193:11 - and create
193:12 - and we'll give it a moment to allow it
193:14 - to start deploying
193:19 - and we'll just wait till this is done
193:20 - deploying our registry
193:23 - all right so that took less than five
193:24 - seconds and we'll go ahead and go to
193:26 - that resource
193:27 - and so what we need to do is go to
193:29 - access keys we're going to enable admin
193:31 - users so that we can actually log in via
193:33 - docker to our image so now what we need
193:36 - to do is get some repositories going and
193:39 - if we go over to repositories tab i
193:41 - don't think there's much we can do here
193:43 - i don't know if you can create
193:43 - repositories directly here i always just
193:45 - push
193:46 - yeah so you don't you don't have a
193:47 - button to say create a repository like
193:49 - github you actually have to push to it
193:50 - so i'm going to go to github and i'm
193:52 - going to create myself a new repository
193:55 - i'm going to call under the other
193:57 - example we'll say my acr it's already
194:00 - there so i'll say new
194:02 - and
194:03 - we'll go down below actually doesn't
194:05 - matter we'll just go ahead and hit
194:06 - create
194:09 - and from there i'm going to open this up
194:11 - in git pod all you have to do to open
194:13 - something up in gitpod
194:15 - is to attach this end here you can also
194:17 - do this in your local environment but
194:19 - you do have to have docker
194:20 - set up and so that's why it's easier to
194:22 - use git pod now cloud shell in azure
194:26 - actually has docker installed but
194:27 - doesn't run a docker instance so it's
194:29 - not that easy to work with so it really
194:31 - is easier to just work with git pod here
194:32 - or locally if you have docker installed
194:35 - so we're going to say docker login and
194:36 - then we're going to provide the
194:40 - login server
194:42 - we'll hit enter it's going to ask for
194:44 - our username so that is our username
194:46 - there so i'm going to paste it in it's
194:48 - going to ask for our password so i'm
194:49 - going to copy that paste that on in
194:51 - there hit enter and so now we can push
194:54 - to that repository i'm going to pull
194:58 - a image called hello world that's just a
195:00 - standard
195:02 - application to say hello world
195:05 - if we do like docker run i t
195:08 - d
195:09 - um
195:10 - hello world i think that's what it is to
195:13 - type it whoops i can actually spell it
195:14 - right
195:16 - then we could probably run it over here
195:18 - there's like a docker tab so we can see
195:20 - if it's running notice that it exited if
195:21 - we view the logs
195:23 - it probably just says hello world
195:25 - yeah hello from docker so there you go
195:27 - that's a simple way of running a docker
195:29 - container
195:31 - and
195:32 - terminal will be reused for other tasks
195:34 - press any key to close it so i just hit
195:36 - space there
195:37 - i t d or t i think is for the tag name
195:40 - so it's called hello world so we type
195:41 - docker images it shows up there in i's
195:45 - for i don't know interactive d is for
195:47 - daemon so it runs in the background but
195:49 - anyway we want to push this to
195:51 - [Music]
195:52 - azure so the way we're going to do that
195:54 - is we're going to type in docker
195:56 - tag because we have to tag this image
195:58 - and we're going to choose the image id
195:59 - here and then we need to provide the
196:01 - repository name so i'm going to assume
196:03 - it's the login
196:05 - server
196:07 - and then we just say example
196:09 - hello world
196:11 - hit enter and we can say docker push and
196:14 - then copy
196:15 - this here
196:18 - and that should push it over to
196:20 - azure
196:22 - we'll go back over here give it a
196:23 - refresh so repository is there
196:27 - so
196:28 - that's all it takes to push so this is
196:30 - one part but we'll figure the other part
196:32 - next here soon okay
196:35 - hey this is andrew brown from exam pro
196:37 - and what we're going to do is use the
196:39 - last um
196:41 - follow along in order to reference an
196:43 - image in our container here because we
196:46 - pushed an image but what if we want to
196:47 - reference one as an application in an
196:50 - actual docker file because that is a
196:51 - very common use case so in our gitpod
196:53 - environment which we still have i'm
196:54 - going to type in dockerfile
196:56 - and in here what we can do is reference
196:58 - our image i always forget what the top
197:01 - line is export run import something so
197:03 - i'm going to just look for example i'm
197:05 - going to go to um
197:07 - docker ruby
197:08 - because i know how to use this image
197:10 - pretty pretty uh pretty good so we'll
197:12 - just go with it
197:14 - and it's from i always forget that so
197:16 - we'll type in from and the idea is that
197:18 - we want to
197:19 - reference something like this but i know
197:22 - if we use the hello world one it's a bit
197:24 - finicky to build so i don't want to use
197:25 - this one but i'm going to say ruby
197:28 - or um yeah ruby's fine here and so we're
197:31 - going to pull the the ruby image so we
197:33 - say docker
197:34 - pull ruby
197:37 - and we'll have to push this one
197:39 - so we'll pull it
197:45 - give it a moment
197:55 - and now we'll type in docker
197:58 - um
197:59 - images
198:00 - and we need to grab the ruby
198:02 - image id and we'll say docker
198:06 - tag
198:08 - this one
198:10 - and we gotta go up and grab this part
198:12 - here
198:14 - paste that in i could just say ruby here
198:16 - and then we can do docker push
198:19 - ruby
198:26 - and while that's going i'm just going to
198:27 - go here and type in run
198:29 - echo
198:31 - hello ruby because the idea is we just
198:33 - want to build a docker image that's from
198:35 - a remote location
198:38 - so what i'm going to do is just save
198:40 - this so we'll say uh save my docker file
198:44 - it's like the only file we saved here
198:48 - just waiting for that to finish pushing
198:51 - and then what we're going to do is we're
198:53 - going to uh you know close this
198:55 - environment and then launch it again
198:56 - because i don't want to have any of
198:57 - those images there i don't have to
198:58 - figure out how to delete them
199:00 - it's like docker rmi it can be really
199:02 - finicky about deleting images but the
199:04 - idea is i just want to make sure they're
199:05 - not there so that when we pull we know
199:07 - that it actually is pulling from there
199:08 - not locally
199:12 - we will have to do a docker login here
199:14 - in a moment
199:17 - so type in
199:20 - we'll type in docker login
199:22 - and we'll go back over here and we will
199:25 - log in
199:27 - say allow
199:28 - we'll go back
199:29 - to here we'll grab the username
199:32 - we'll go back over to here
199:34 - we'll grab the password
199:36 - so that's good and um
199:39 - hey where's our dockerfile
199:41 - our dockerfile seems to be
199:44 - gone i guess we forgot to sync it and
199:46 - that's totally fine we'll just type it
199:48 - in again it's not a big deal from
199:53 - here we'll say ruby and i'll say run
199:55 - echo
199:57 - hello ruby
200:02 - and so now what i want to do is i want
200:03 - to do docker build period
200:06 - and so if we are logged in and we can
200:07 - pull it so it's in great shape
200:14 - so it's the fact that we are pulling
200:15 - that image and that was the other part i
200:18 - wanted to show you there so it doesn't
200:19 - matter if this finishes because it
200:20 - obviously is working but we'll give it a
200:22 - second here to uh finish downloading
200:30 - there we go
200:32 - um and so that's all so we'll go ahead
200:34 - and close this and we'll make our way
200:35 - back over to azure
200:37 - and we will go and type in resource
200:39 - groups
200:45 - and we will choose my acr
200:48 - and we'll go delete resource group
200:52 - and we'll delete that
200:56 - we're done so there you go
200:57 - [Music]
201:02 - hey this is andrew brown from exam pro
201:04 - and we're taking a look here at azure
201:05 - container registries cheat sheet so acr
201:08 - is a managed private docker registry
201:10 - service based on the open source docker
201:12 - registry 2.0
201:13 - it uses azure container registries with
201:16 - your existing container development or
201:18 - sorry yeah development and deployment
201:19 - pipelines use azure
201:21 - or use acr tasks to build containers
201:24 - images in azure pull images from an
201:26 - azure container registry to various
201:28 - targets uh and targets could be
201:30 - kubernetes uh dc os which is i can't
201:33 - remember the name of the full name of
201:35 - that operating system docker swarm uh
201:37 - for acr tasks this allows you to
201:39 - automate os and framework patching for
201:41 - your docker containers you can trigger
201:43 - them
201:44 - you can trigger automated builds by
201:46 - source code updates updates to container
201:48 - based images triggers on a schedule you
201:50 - can create multi-step tasks each acr
201:53 - task has an associated code context and
201:56 - tasks can take
201:58 - advantage of run variables that's the
202:01 - end of it there you go see you in the
202:03 - next section
202:08 - hey this is andrew brown from exam pro
202:10 - and we're looking at azure app services
202:11 - which allows you to quickly deploy and
202:13 - manage web apps on azure without
202:14 - worrying about the underlying
202:16 - infrastructure and specifically this is
202:17 - known as a platform as a service
202:20 - so azure app service is an http based
202:23 - service for hosting web applications
202:25 - rest apis and mobile backends and you
202:28 - can choose your programming language and
202:29 - it can either be windows or linux
202:31 - environment and it's a platform as a
202:33 - service so if you've ever used heroku is
202:36 - basically the heroku of azure and so
202:39 - azure app service takes care of a lot of
202:40 - the underlying infrastructure for you so
202:42 - it can do security patches of the os and
202:44 - languages load balancing auto scaling
202:47 - automated manager
202:49 - and then um there's a lot of things that
202:51 - you can implement
202:53 - and so for integration such as azure
202:55 - devops github integrations docker hub
202:59 - package management easy to set up
203:01 - staging environments custom domains
203:04 - attaching uh ssl certificates so you can
203:07 - see that it just basically takes care of
203:08 - everything for you
203:10 - if it's really hard for you or time
203:12 - consuming for you to do all this on your
203:13 - own in azure and the way it works is you
203:16 - pay based on an azure app service plan
203:19 - honestly i find these really confusing
203:21 - um i really like how aws does elastic
203:24 - bean stock because you're just paying
203:25 - for the underlying services but azure
203:27 - has all these crazy uh tiers here but
203:30 - you have shared tier which is free and
203:32 - shared doesn't support linux you have
203:34 - the dedicated tier
203:36 - which is basic standard premium premium
203:38 - version two and three and the isolated
203:40 - tier and so uh another thing i need to
203:43 - note is that azure app services is not
203:45 - just for traditional
203:46 - vms for monoliths you can also run
203:49 - docker single or multi containers
203:52 - and when you set up a project you're
203:53 - going to choose your domain name on the
203:55 - azure websites.net obviously you can
203:57 - override that with your custom domain
203:58 - name but there you go
204:00 - [Music]
204:04 - let's talk about azure app services
204:06 - runtimes and so let's define what
204:08 - runtime is a runtime is software
204:10 - instructions that are executed while
204:12 - your program is running and so runtime
204:14 - generally means what programming
204:16 - language libraries and frameworks you're
204:17 - going to be using and so runtimes for
204:20 - azure app services are predefined
204:22 - containers that has your programming
204:24 - languages and probably some commonly
204:26 - used libraries for those languages
204:28 - installed and probably commonly used
204:29 - languages for its web frameworks
204:32 - and so the idea is that you're going to
204:34 - choose your runtime
204:36 - it could be net
204:38 - net core java ruby
204:41 - which i'm very disappointed in azure
204:42 - because uh as of shooting this video
204:44 - they do not support uh ruby for
204:47 - application insights but yet they have
204:49 - it here in azure app services node.js
204:52 - php python and there's all the logos if
204:55 - you like seeing the logos and so azure
204:57 - app services generally has multiple
204:59 - versions so they have like ruby 2.6 2.7
205:03 - for php they have a lot of versions for
205:05 - node.js they have a lot of versions
205:07 - but i just want to point out uh that
205:08 - it's pretty common for cloud providers
205:10 - to stop or to retire the old ones at
205:12 - some point to stop supporting them uh
205:15 - you know that's just because they want
205:16 - to keep things modern
205:18 - and the other thing is like it also
205:19 - helps you keep with your best security
205:21 - practices because really you should
205:23 - always be trying to upgrade to the
205:24 - latest version uh for those security
205:26 - patches and and things like that so
205:29 - there you go
205:30 - [Music]
205:34 - now let's say uh you wanted to use a
205:36 - language that wasn't supported on um
205:40 - azure app services like you wanted to
205:42 - use uh elixir what you could do is
205:44 - create your own custom container either
205:46 - for windows or linux uh and so uh you
205:49 - just go ahead and create your own docker
205:51 - container in your local environment you
205:53 - can push it to an azure container
205:55 - registry and then the idea is you can
205:57 - deploy your container image to your app
205:58 - service so i just wanted you to know
206:00 - that you could do that um if there's
206:02 - some languages or other things or maybe
206:04 - you're using a language but you need uh
206:06 - some
206:07 - bundle or packages that are uh they're
206:09 - just baked into the uh into the
206:11 - container so there you go
206:13 - [Music]
206:16 - let's talk about deployment slots so
206:18 - deployment slots allow you to create
206:19 - different environments for your web
206:20 - application and associate a different
206:22 - host name this is useful when you need a
206:24 - staging or a quality assurance
206:26 - environment or maybe you need to like a
206:28 - developer environment any kind of
206:29 - environment you want so think of it as a
206:30 - way to quickly clone your production
206:32 - environment for other uses and so down
206:34 - below here you'd have your deployment
206:36 - slots and there's your slots to maybe
206:37 - you have apps staging beta so that's the
206:39 - different
206:40 - host names there
206:42 - and so the idea is that not only do you
206:43 - have other environments but there's also
206:45 - this thing called swapping and the idea
206:46 - is like imagine
206:48 - uh you decide like you make a clone of
206:50 - your production environment and then you
206:52 - deploy the latest version to it and then
206:55 - when you decide that it's good it's in
206:56 - good shape then what you can do is swap
206:59 - it out with your current production
207:00 - environment and then just retire your
207:02 - old one this is called blue green
207:03 - deployment
207:05 - and so that is a great way to do
207:07 - deployments and i just wanted you to be
207:09 - aware of
207:12 - that so i wanted to talk about app
207:15 - service environment because i just
207:16 - wanted to show you how
207:17 - azure app service is not just for a
207:20 - little toy apps or your small startup
207:22 - but they can be really useful for larger
207:24 - enterprises so
207:26 - app service environment is an azure app
207:28 - service feature that provides fully
207:30 - isolated and dedicated environment for
207:31 - securely running
207:33 - app service uh at high scale and this is
207:36 - going to allow you to host web or
207:38 - windows web apps linux web apps docker
207:40 - containers mobile apps and functions
207:43 - and app service environments are
207:44 - appropriate for application workloads
207:46 - that require very high scale isolation
207:49 - and secure network access
207:51 - high memory utilization
207:53 - and again you know when you think of
207:54 - platform as a service you don't think at
207:56 - this kind of scale so it's really
207:57 - impressive that azure does this
207:58 - customers can create multiple ases
208:00 - within a single azure region or across
208:03 - multiple azure regions making ases ideal
208:05 - for horizontal scaling stateless
208:07 - application tiers in support of high
208:09 - requests per second rp rps workloads and
208:12 - ase comes with its own pricing tier and
208:14 - that's the isolated tier and ac can be
208:17 - used to configure security architecture
208:19 - apps running ase can have their access
208:23 - gated by upstream devices such as a web
208:26 - application firewall also known as a
208:27 - waff
208:28 - app or ssases
208:31 - can be deployed into availability zones
208:34 - using zone pinning i don't know what
208:36 - that is a zone pinning but that sounds
208:37 - good to me
208:38 - and there are two deployment types for
208:41 - ase we have external ase and ilb ase
208:45 - let's go take a quick look at what those
208:46 - look like so here's a big diagram
208:49 - and look in the middle there that's
208:50 - where our azure
208:53 - service environment is it's in our own
208:55 - v-net and a subnet and the idea is that
208:57 - you if the reason it's called an
208:59 - external ase is because it exposes the
209:01 - esc hosted app on an internet accessible
209:04 - ip address and then down below
209:07 - if you wanted to connect this this is
209:09 - generally what people are probably using
209:10 - for but what you want to do is you can
209:13 - connect it to your on-premise via a site
209:15 - to site or express route vpn
209:19 - so that's something you can do with it
209:20 - and the other part is like you can
209:22 - because the esc is within the same v-net
209:25 - it also has access to resources within
209:27 - the v-net without any additional
209:28 - configuration so that's really nice and
209:30 - then the second uh a second one here it
209:33 - looks it's exactly identical but there's
209:36 - one key difference and it has this ilb
209:38 - there and an ilb stands for internal
209:40 - load balancer uh and that is basically
209:43 - the only difference
209:44 - so uh there you go
209:46 - [Music]
209:50 - all right let's take a look here at uh
209:52 - deployment for azure app services and
209:54 - let's just first define deployment so
209:55 - that's the action of pushing changes or
209:57 - updates from a local environment or
209:58 - repository into a remote environment and
210:01 - azure app services has a lot of options
210:03 - for us and that's to me is the most
210:05 - powerful thing about azure app services
210:07 - because
210:08 - it's easy to deploy a web app to a
210:10 - virtual machine backed by a database but
210:12 - figuring out deployment is a very time
210:14 - consuming and tricky thing and azure app
210:16 - services gives us so many options
210:18 - unbelievable amount of options it is so
210:21 - so great of them to do that for us so we
210:23 - can run from a package deploy zip or or
210:25 - rar deploy via ftp deploy via cloud sync
210:28 - deploy continuously from like github or
210:31 - use azure pipelines
210:33 - deploy using custom containers
210:36 - using docker hub azure container
210:37 - registry deploy from a local git repo
210:40 - deploy using github actions deploy using
210:42 - github action containers or deploy with
210:44 - an arm template so we're not going to
210:46 - look at all of these how they work but
210:48 - let's look at a few of them so we just
210:49 - have an idea how robust of these options
210:52 - are the first one we're going to look at
210:53 - is running from a package and this is
210:55 - when files in the package are not copied
210:57 - to the www uh root directory instead the
211:00 - zip package itself gets mounted directly
211:02 - as a read only
211:04 - www route directory so basically all
211:07 - other deployment methods and app service
211:09 - have to deploy to the following
211:11 - directory if it's if you're on a windows
211:12 - machine it's going to be home site wwe
211:14 - root if you're on linux it's going to be
211:15 - home site www root and so since the same
211:18 - directory is used by your app at runtime
211:21 - it's possible for deployment to fail
211:23 - because of file lock
211:25 - conflicts and for that and for that
211:27 - reason uh the app might behave
211:29 - unpredictably because some of the files
211:31 - are not
211:32 - yet updated so this is the reason why
211:33 - you might want to create a package
211:35 - um it's just because it it circumvents
211:38 - the issue of just replacing files in the
211:40 - folder
211:42 - another method is the zip and rar
211:44 - deployment this uses the kudu servers
211:46 - service that powers continuous
211:48 - integration based deployments and qd is
211:50 - used for a lot of things but it's an
211:51 - engine behind git deployments and azure
211:54 - app services and it's open source
211:55 - project that can also run outside of
211:57 - azure
211:58 - and so cuda supports the following
212:00 - functionality for zip file deployments
212:02 - and it supports a lot more than just
212:04 - zips but it does deletion of files left
212:06 - over from a previous deployment option
212:08 - to turn on the default build process
212:10 - which includes package restore
212:12 - deployment customization including run
212:14 - deployment scripts deployment logs a
212:16 - file size of uh
212:19 - was it two gigs two gigs and uh you can
212:21 - use it with the cli the api uh vrs with
212:24 - curl or the azure portal
212:26 - uh so
212:28 - by the way that's that's you uploading a
212:29 - zip uh but you're essentially using kudu
212:32 - underneath okay
212:34 - uh and so let's go take a look at
212:36 - another deployment method which is file
212:38 - transfer protocol uh ftp has been around
212:41 - for forever and this is pretty much how
212:43 - people thought you were supposed to
212:44 - deploy your apps in the late 90s and
212:45 - early 2000s i don't think it's a really
212:48 - great way to deploy but the point is is
212:50 - that if you want to do it this way or
212:51 - you have a use case that makes sense you
212:53 - can do it so the idea is that the
212:55 - deployment center you would uh say i
212:57 - want to use ftp and you just you get an
213:00 - ftp endpoint your username and password
213:02 - and that's your credentials you use your
213:04 - ftp client to connect
213:05 - uh very old school but it's an option
213:07 - for you which is really nice another way
213:09 - which to me is a bit bizarre but it's
213:11 - cool that you can do it is you can use
213:13 - dropbox or onedrive to deploy using
213:15 - cloud sync so the idea is that you have
213:17 - dropbox it's a third party cloud storage
213:20 - service onedrive's the same thing it's
213:22 - just microsoft's uh thing
213:24 - and so you go to deployment center
213:25 - configure dropbox or onedrive and when
213:27 - you turn it on it will just sync or
213:29 - it'll just create a folder in your
213:31 - dropbox drive and that will get synced
213:33 - so that's the one for onedrive that's
213:35 - the one for dropbox and this is going to
213:36 - sync to that www route there
213:39 - i would have loved to take in some
213:40 - screenshots but i couldn't find how to
213:42 - turn the service on but i know it exists
213:45 - i just was i thought it was so bizarre
213:47 - but yeah there you go that is the
213:49 - deployment methods there
213:50 - [Music]
213:54 - so the way you pay
213:56 - when you use azure app service is you
213:58 - need an azure app service plan and
214:00 - that's going to determine you know how
214:02 - much you pay and what's going to be
214:03 - available to you they got three tiers
214:05 - which we're going to go uh through here
214:07 - shortly and we did mention them earlier
214:09 - i honestly do not like this whatsoever
214:11 - this tells you this is a microsoft
214:12 - product because it has these uh wonky
214:15 - pricing tiers i hope in the future
214:17 - they'll change it but that's just what
214:18 - it is and so let's go learn it so
214:21 - basically what you do is you have like
214:22 - this big wizard uh that tells you all
214:25 - the stuff that you can have it tells you
214:26 - what's included and stuff like that
214:28 - but let's work our way through it so the
214:30 - first thing is the shared tier and
214:32 - there's two types here we got free and
214:33 - shared and so there's the free tier that
214:35 - red one there it's called f1 it gives
214:37 - you one gigabyte of disk space up to 10
214:39 - apps on a single shared instance no sla
214:42 - for availability and each app app has a
214:45 - compute quota of 60 minutes per day so
214:47 - there you go you get some free tier
214:49 - there then there's the shared tier this
214:51 - provides up to 100 apps on a single
214:52 - shared instance no sla availability each
214:54 - app has a compute quota of 240 minutes
214:56 - per day
214:57 - uh and the thing is is that i didn't
214:59 - know where the button is for that so
215:00 - like the next thing right beside it is
215:02 - the dedicated tier and i thought that's
215:04 - what it would be because it says 100
215:06 - total acus
215:08 - so
215:09 - is it i'm not sure but anyway the point
215:11 - is there is a shared tier where it is i
215:13 - don't know uh and you can't use the
215:14 - shared tier on uh on the on linux based
215:17 - system so you're using windows i'm using
215:19 - windows you gotta use bigger instances
215:20 - anyway which i don't know i don't like
215:21 - that too much but that's what it is
215:24 - uh moving on uh over we're now into the
215:27 - dedicated tiers and look it's right
215:28 - beside the free tier that's the green
215:31 - one it says b1
215:32 - and if you expand it actually has a
215:34 - couple other tiers there so i just
215:35 - wanted to show you that there was uh
215:36 - three uh there
215:38 - and so uh for dedicated tiers we've got
215:40 - basic standard premium premium version
215:42 - two premium version three and we're
215:43 - looking at basically that's what that is
215:45 - more disk space unlimited apps three
215:47 - levels in this tier that offer very
215:49 - amount of compute power memory disk
215:50 - usage it must be b1 b2 b3
215:53 - uh and then the next thing over is uh
215:56 - the standard and i we had we had to
215:57 - switch tabs there onto the production
215:59 - tab notice the terminologies don't
216:00 - really match the tiers
216:02 - uh and so uh with standard you can scale
216:04 - out to three dedicated instances has an
216:06 - sla of 99.95
216:08 - availability and three levels in this
216:11 - tier that are offering varying amounts
216:12 - of compute power memory and storage disk
216:16 - and so then
216:17 - uh that's that tier there and we're on
216:19 - to our last here which is the premium
216:21 - tier
216:22 - and this scales to uh 10 dedicated
216:24 - instances it has availability sla of
216:27 - 99.95 percent and multiple levels of
216:30 - hardware so that's the dedicated tiers
216:33 - then we're on to the last thing which is
216:34 - isolated and this is really only going
216:36 - to be used i think for ase so the uh the
216:39 - isolated tier has dedicated azure
216:40 - virtual networks uh full network compute
216:43 - isolation scales out to 100 instances
216:46 - availability sla of 99.95 and again i
216:49 - think it's just for those ases um but
216:52 - there you go that is all the tiers and
216:54 - hopefully it makes sense to you but it
216:56 - is a little bit tricky to figure out
216:57 - what to choose but you don't really get
216:58 - to like pick at a granular level that's
217:00 - what i don't like
217:02 - but
217:03 - it is a really great service azure app
217:05 - service it does figure everything out
217:07 - for you so maybe it's okay for you
217:09 - [Music]
217:13 - so let's say you're using azure app
217:14 - services to host um one of your
217:17 - applications you need to run a random
217:18 - script how are you going to do that
217:20 - that's where web jobs comes into play so
217:23 - it comes at no additional cost and web
217:25 - jobs is not yet supported for linux
217:27 - which is of no surprise here because
217:30 - microsoft is all about windows but i
217:31 - hope one day they will support it the
217:33 - following types they support as of today
217:36 - are command files bat files executables
217:38 - we've got powershell files bash files
217:40 - php files python files javascript files
217:43 - and java files when you go ahead and
217:45 - create your job you're going to have
217:46 - between two types you've got continuous
217:48 - and triggered a continuous means it's
217:50 - going to run continually until stopped
217:51 - which is pretty clear
217:53 - and this particular mode supports
217:55 - debugging so if you need to bug in
217:56 - that's the mode you're going to be using
217:58 - for trigger this only runs when a
218:00 - trigger occurs and you have different
218:02 - kinds of triggered here so you notice we
218:04 - have scheduled and we can enter a quran
218:05 - job in and this will also expose a web
218:07 - hook that can be called to enable
218:09 - scenarios like scheduling i think that
218:11 - would be like manual triggers
218:13 - it doesn't support debugging though so
218:15 - looking at web job scales and this is
218:17 - for continuous only
218:19 - uh we have a multi-instance and single
218:21 - instance so multi is will scale your web
218:24 - jobs across all instances for your app
218:26 - service plan and single instance will
218:29 - only keep a single copy of your web job
218:30 - running regardless of the azure app plan
218:33 - instance count so there you go
218:35 - [Music]
218:39 - hey this is andrew brown from exam pro
218:41 - and we are going to be learning about
218:42 - azure app services in this follow along
218:45 - uh and it's a service that's supposed to
218:46 - make it easy for you to deploy web
218:48 - applications i say supposed to because
218:50 - it really depends on your stack azure
218:52 - has more synergies with other
218:54 - stacks than others so like if you're
218:56 - like me and you like ruby on rails
218:57 - you're going to find a lot of friction
218:59 - with rails and linux but if you're using
219:01 - something like windows servers or python
219:04 - or net you're going to have a much
219:05 - easier time still really great service
219:07 - just wish they'd make it a bit more
219:08 - broad there but let's hop into it so
219:11 - before we can go use that service let's
219:13 - make sure that it's activated and so
219:15 - we'll go over here
219:17 - and we'll go to azure subscription and
219:19 - then down below we're going to go to
219:21 - resource provider now you'd think what
219:23 - you could do is just type in app
219:24 - services
219:26 - and you'd be wrong because the the
219:28 - service is under a particular provider
219:30 - so if you want to figure out what
219:31 - provider it is we can go um azure
219:34 - resource providers
219:37 - and they have a page on documentation
219:39 - here that lists them all
219:41 - so if i search for azure app services
219:45 - it's under web and domain registration
219:47 - so we're going to make sure this is
219:48 - registered if we're using a custom
219:50 - domain which we are not today we need
219:52 - this one activated so going back here i
219:56 - will type in web and you can see it's
219:58 - registered so if yours is not registered
220:00 - go ahead and hit that i believe this by
220:02 - default is generally registered with new
220:05 - azure accounts so i don't think that is
220:07 - an issue for you but we'll go back up
220:09 - here close these additional tabs and we
220:11 - will type in azure app services
220:15 - and we will look for that service so
220:17 - there it is and we'll go ahead and hit
220:18 - add
220:21 - and so i'm going to give it a new name i
220:23 - just made it a moment ago but i'm going
220:25 - to try again and try to use the same
220:26 - name so we're going to call this voyager
220:31 - great and then i'm going to go ahead and
220:32 - name this voyager and i already know
220:34 - that that is taken so i'm going to type
220:36 - in delta flyer
220:38 - and these are fully qualified domains so
220:40 - they are unique with azure app services
220:42 - you can run a docker container we're
220:44 - doing code this time around and what i
220:47 - like to use is ruby
220:49 - but again you know if i want to use the
220:50 - ci cd i'm not going to be able to use
220:52 - the deployment center with ruby so that
220:54 - is not possible
220:56 - and so we're going to go with python and
220:57 - run either a flask or a django app i
220:59 - haven't decided yet i am in canada so
221:02 - let's go to canada east
221:04 - and
221:05 - down below here we have the plans
221:07 - generally the plans will tell you the
221:09 - cost underneath look you'll notice that
221:10 - it's loading but i just want to show you
221:12 - that there are some discrepancies in
221:14 - terms of pricing so if i was to go to
221:16 - azure app services pricing
221:20 - and we were to pull this up here we can
221:22 - kind of see the pricing here
221:25 - okay and if we scroll on down right now
221:28 - we're looking at a premium v2
221:30 - uh and oh no i don't need help i'm okay
221:33 - you'll notice that it's 20 cents per
221:34 - hour so if i go here and do
221:37 - that times 730 because there's 730 hours
221:40 - in the year that's 146 dollars i believe
221:42 - this is showing me in usd dollars yeah
221:45 - and and here it's showing me 103
221:49 - canadian which is lower um so it could
221:52 - be that because i'm running in a canada
221:54 - east region it's the price is different
221:57 - but you can imagine that if i had this
221:59 - at this cost at uh what did we say here
222:03 - at 146 usd
222:06 - to cad i'd actually be paying
222:08 - 182 dollars so you got to watch out for
222:11 - that kind of stuff but i'm pretty sure
222:13 - this is what the cost is so just be
222:15 - aware that if you look stuff up in here
222:17 - it's not necessarily reflective so you
222:18 - got to do a little bit more work to
222:20 - figure that out
222:21 - if we wanted to go here we cannot choose
222:23 - the free tier when we're using linux if
222:25 - we're using windows i believe we can use
222:27 - it we're working with linux today so
222:29 - that's just how it's going to be
222:31 - for the b1 this is totally fine but we
222:33 - want to utilize deployment slots
222:35 - deployment slots is an advanced feature
222:37 - of the production version and that's the
222:40 - only way we're going to be able to use
222:41 - it here this is 20 cents per hour again
222:43 - so i don't want to be doing this for too
222:45 - long but i think what we'll do is before
222:47 - we do that we can just do an upgrade to
222:50 - dev to prod so we can experience that
222:52 - i'm going to go and just choose b1 okay
222:55 - so go next
222:57 - we do not need any application insights
222:59 - for the time being and it will not let
223:01 - us so it's okay
223:03 - we'll go next review and create
223:06 - and we'll go ahead and create this
223:07 - resource here
223:09 - and i will see you back when this is
223:11 - done
223:12 - so um our resource is now set up we'll
223:15 - go to resource and now that we're in
223:17 - here you'll notice if we hit browse
223:19 - we're not going to see anything because
223:20 - we do not have anything deployed which
223:23 - makes sense right
223:24 - so we're going to actually have to go
223:25 - ahead and deploy something so we're
223:27 - going to make our way over to the
223:28 - deployment center
223:30 - and it's just going to tell us that we
223:32 - have yet to configure anything and
223:34 - that's totally fine we're going to go to
223:35 - settings
223:37 - it'll give it a moment and so the thing
223:39 - is is that we're going to need something
223:41 - to deploy
223:43 - i did not create an app but the great
223:44 - thing
223:45 - is in the azure documentation they have
223:48 - a bunch of quick starts here all right
223:50 - and apparently they have one for ruby as
223:51 - well but today we are looking at python
223:54 - uh and so they actually have an example
223:57 - repository for us here
223:59 - which is github.com azure samples python
224:02 - docs hello world and i mean i can go
224:05 - make a repo for you but we might as well
224:06 - just use the one that is already
224:08 - provided to us
224:09 - so i'm just going to pull this up to
224:11 - show you what's in it it's a very very
224:12 - simple application even if you don't
224:14 - know anything about building web apps
224:16 - i'm going to walk you through it really
224:17 - easily here okay so we're going to open
224:19 - up app.pi so we are using flask if
224:22 - you've never heard of flask it is a very
224:24 - minimal python framework for creating
224:26 - web apps uh very uninspiring uh home
224:30 - page here but it gets the job done it's
224:32 - going to create a default route for us
224:34 - which uh we have there we're going to
224:37 - call hello here and we're going to have
224:39 - hello world so that's all that's going
224:41 - on here
224:42 - very very simple and we have a
224:44 - requirements this is our package manager
224:46 - i don't know why python uses txt files
224:48 - it's very outdated to me but that's what
224:50 - they use and here we have flask
224:53 - all right
224:54 - so we're going to use that repo it's a
224:56 - public repo so it should be very easy
224:59 - for us to connect so we'll drop down
225:01 - go to github
225:03 - and uh the next thing we need to do is
225:05 - authorize github
225:07 - all right so i ran into a bit of trouble
225:08 - there because i could not uh
225:09 - authenticate my uh github account but
225:12 - you know what i just made another github
225:14 - account so that made it a lot easier i'm
225:16 - going to go ahead here hit github and
225:18 - we're going to try to authorize it and
225:20 - so now i'm logged into this new one
225:21 - called exampro dev and we'll go ahead
225:24 - and authorize this application and we're
225:26 - now in good shape
225:28 - this repository doesn't have anything in
225:30 - it so
225:31 - if i want to clone something i guess
225:33 - i'll probably have to fork that repo
225:36 - so we'll give it a moment to authorize
225:38 - and while that's going i think that's
225:39 - what i'm going to do i'm going to go and
225:42 - fork the example repo if i can find the
225:44 - link again here
225:46 - myself
225:50 - i believe
225:53 - it is
225:55 - that's still authorizing over there i'm
225:56 - still looking for it so
225:58 - it was like examples or something
226:01 - samples or examples
226:04 - all right so i found a way around the
226:05 - problem i just made a new github account
226:08 - so that's all i had to do
226:09 - um and i just won't be using my primary
226:11 - account until i get my phone back but so
226:14 - what we'll do is go hit connect i'll hit
226:15 - authorize
226:17 - and it didn't prompt me because it
226:19 - already connected to this new one called
226:20 - exam pro dev you might have to put your
226:22 - credentials in here and it's going to
226:23 - ask me to select some things it's a new
226:25 - account so there are no organizations
226:27 - there are no repositories there are no
226:29 - branches totally brand new so what i'm
226:32 - going to need to do is get a repo in
226:34 - there so we'll just go ahead and fork
226:35 - the azure samples one
226:37 - so that is azure samples
226:40 - python
226:42 - docs hello world
226:45 - if i type that right we're in good shape
226:47 - i'm going to go ahead and fork this
226:48 - repository
226:51 - i'll say got it
226:53 - and then i'll move this off screen here
226:55 - this is now cloned you should see it
226:56 - cloned here
226:58 - and we'll go back here and this probably
227:01 - isn't live so there's no refresh button
227:04 - here so we'll have to hit discard
227:06 - and we will give this another go here
227:09 - and we will select our organization
227:11 - which is our name there is the
227:12 - repository
227:14 - uh should be main branch is kind of
227:16 - outdated i'm sorry but it's called
227:17 - master that's what it is
227:19 - not my fault that's azure's fault okay
227:22 - um and i think that's it
227:26 - i don't know if we need a workflow
227:27 - configuration file i don't think so
227:30 - [Music]
227:31 - i'm just going to double check here no i
227:33 - don't think so and what we'll do is
227:35 - we'll just go ahead and save that
227:42 - and so now we are set up for deployment
227:50 - [Music]
227:54 - all right so now that that's all hooked
227:56 - up if we were to go to browse we're
227:57 - actually still seeing the default page a
227:59 - deployment hasn't been triggered just
228:01 - yet
228:02 - so the way it works is it's using github
228:04 - actions so if we click into our we call
228:06 - it main branch i know they got the wrong
228:08 - name but we're going to click into our
228:10 - github workflows and then below here we
228:12 - can see we have a yaml file
228:14 - and this is for github actions
228:16 - integration here and so what it's doing
228:17 - is it's specifying the branch
228:20 - what how it's going to build it's going
228:22 - to run on ubuntu latest the steps it's
228:24 - going to do is going to check it out
228:26 - it's going to set up the python version
228:28 - it's going to build it it's going to do
228:29 - that stuff and so in order for this to
228:33 - um take action we'd actually have to go
228:35 - ahead and make some kind of manual
228:37 - change which we have yet to do so eh
228:40 - so what we'll do is we'll go back to our
228:42 - main here
228:45 - and uh it should be as simple as uh just
228:48 - changing something here
228:50 - so it's not i'm not sure how it's
228:52 - supposed to know that it's supposed to
228:53 - be doing the hello oh i guess yeah sorry
228:55 - so this means it's going to route over
228:57 - to here
228:58 - so i'm just going to make any kind of
228:59 - change here doesn't matter what it is
229:01 - just one space
229:03 - we'll go ahead and give it a commit
229:05 - and
229:07 - if i go back to my latest commits we
229:09 - should see that i made that change there
229:11 - it is
229:12 - we'll go back over here
229:13 - and this should be deploying
229:18 - so if we go over to logs
229:20 - here you can see one's in progress right
229:22 - now okay and so that's what we're
229:24 - weighing we're just going to see that
229:25 - finish there we could probably open the
229:26 - logs and get some more information there
229:28 - and so it just brings you back over to
229:30 - github actions and so here's github
229:32 - actions and it's performing the stuff
229:34 - here so we're just going to give it time
229:36 - here and i'll see you back in a moment
229:38 - so we didn't have to wait too long it
229:39 - only took one minute and 29 seconds if
229:41 - we go back over here
229:43 - um we might need to do a refresh and so
229:45 - we can see this is reflected over here
229:48 - and so if we go back to it doesn't
229:51 - really matter if we go to settings or
229:52 - logs here but i'm going to hit browse
229:53 - and see if my page is deployed it still
229:55 - is not so we do have a small little
229:58 - problem here and it's really going to
229:59 - just have to do with how the app is
230:00 - served so that's what we need to figure
230:01 - out next
230:03 - all right so our app is not currently
230:04 - working and there's a few approaches we
230:06 - can take and the thing i can think right
230:08 - away is we should go an ssh into that
230:10 - instance if you scroll on down here from
230:12 - developer tools you can go to ssh and
230:14 - click this button and that's going to
230:16 - ssh you right into that machine right
230:18 - away you can also
230:20 - access ssh via the um
230:23 - cli command so i believe it's like
230:27 - it's like a z web app
230:31 - um
230:32 - ssh it'll do the exact same thing you do
230:35 - that from the cloud shell but that's not
230:36 - what we're doing today
230:38 - if i give this an ls in here and we're
230:39 - in linux we can see we have our app here
230:42 - and what i would do is i would see
230:44 - what's running so i i would do a puma
230:47 - uh or sorry not puma
230:49 - ps ox grep uh python and you can notice
230:52 - that we have uh g unicorn that's running
230:55 - so that is where our python instances
230:57 - are running so you're not looking for
230:58 - flash you're looking for python here
231:01 - and if we wanted to make sure that was
231:02 - working we just type in curl
231:05 - local host
231:08 - um and so that is going to return a port
231:11 - 80. so that tells me that because like
231:13 - curl just means like let's go look at
231:15 - that page it should return some html
231:17 - like print out the html to us so that
231:19 - means the app is not running
231:21 - so what you could do is run flask run
231:23 - and it's going to start on
231:26 - port 5000
231:28 - right so what i can do
231:32 - is i can go up uh back to my deployment
231:34 - center here
231:36 - and i'm going to go get that link here
231:40 - and just ignore the fact that it's
231:41 - working it's it's not working right now
231:43 - i know for certain it's not
231:45 - but if we do 5000 that won't resolve
231:47 - because port 5000 isn't open so we can't
231:50 - really just
231:51 - put 5000 in there and the default server
231:53 - here would be 5000 so if i stop this and
231:57 - i specify port 80
232:00 - right
232:02 - then this will start up the app on port
232:03 - 80 and so now when you go here
232:07 - okay it will work this is not a great
232:09 - way because of course as soon as you
232:11 - kill it here uh technically the site
232:13 - should stop running um and so you'll run
232:15 - into that step uh so what we need to do
232:18 - is provide a configuration to g unicorn
232:21 - which is a python thing again it's not
232:23 - so important that you know how like what
232:24 - these things are but the idea is that
232:26 - you understand as administrator you want
232:28 - to make sure you have an app that runs
232:29 - after you do a deploy and so in this
232:31 - particular one we need a startup.txt
232:34 - and interestingly enough there is a
232:37 - example code by the same author of the
232:38 - other one we were looking at here i
232:40 - believe it's the same person
232:43 - or it might not be but they have a
232:45 - startup txt right
232:47 - and so in here you can see that it binds
232:48 - on port zero zero zero it starts up four
232:50 - workers starts up the app
232:52 - all right
232:53 - and so that's something that we can go
232:55 - ahead and do
232:57 - so
232:58 - what i will do is i will go back to my
233:00 - github repository
233:04 - that we have here
233:06 - and i can just go ahead and add a new
233:08 - file so i'm going to say
233:12 - add a file create a new file here
233:15 - we'll call it startup.txt
233:18 - i'm going to copy this command here and
233:21 - paste it in there so g unicorn will bind
233:24 - the workers and start up on the app
233:26 - um startup app is being ran by
233:29 - something here so if i go back here i
233:31 - think they have a startup pi here
233:33 - and that's all that it is doing um
233:36 - i think i want to
233:39 - i could do it this way i suppose let me
233:41 - just see here
233:43 - there's just a slightly different egg so
233:45 - they actually have like a full app going
233:47 - on here
233:48 - and i just want a very simple flask app
233:51 - so i think what i can do
233:54 - is put flask run here
233:59 - port 80
234:01 - and that should start up the app there
234:03 - i'm going to go ahead and commit that
234:04 - file
234:06 - okay and as soon as i commit that if i
234:08 - go back to my actions
234:10 - it created that startup file there so it
234:12 - should trigger a build
234:15 - it's queued up um and i'll just put this
234:19 - tab up here so we'll be back here in two
234:21 - seconds and if i give this a nice
234:23 - refresh
234:24 - yeah you can see it deploys in progress
234:26 - so
234:27 - this doesn't take too long we'll just
234:28 - wait close that there we'll just wait a
234:30 - few minutes we click logs it just opens
234:32 - it back up here and we'll see how that
234:34 - goes
234:35 - all right so your deploy may have
234:36 - finished there but the thing is is that
234:38 - we're not going to really know if a
234:40 - change has taken effect unless we
234:41 - actually go ahead and update our code so
234:44 - what i want you to do is go to your code
234:45 - tab go to your app.pi we'll hit edit and
234:48 - i'm going to go ahead and change this
234:50 - to vulkan
234:53 - and then we'll scroll on down hit commit
234:55 - changes and we'll make our way back over
234:57 - to our deployment center
234:59 - and we'll give it a refresh here and
235:00 - we're just going to wait until this one
235:02 - is complete and we will double check to
235:04 - make sure that that has changed if it's
235:05 - not we will take action to fix that okay
235:09 - all right so we just waited a little
235:11 - while there for that deploy to happen
235:12 - and if we go to our website here it is
235:16 - taking effect so that's all we had to do
235:17 - to get it working so that's pretty good
235:19 - um so that is deployment
235:22 - [Music]
235:25 - so let's talk about deployment slots in
235:28 - order to utilize this feature we're
235:30 - going to actually have to upgrade our
235:31 - account because we cannot utilize them
235:33 - at this uh the basic plan here we got to
235:35 - go to standard or premium so let's go
235:37 - ahead and give that an upgrade
235:39 - so here's the b1 we're gonna go to
235:41 - production here um
235:44 - and i think
235:46 - yeah we're gonna have to choose this one
235:48 - here uh very expensive so the thing is
235:51 - we're gonna just upgrade it temporarily
235:53 - unless there's more options down below
235:55 - that are cheaper
235:57 - yeah these are the standard tiers
235:59 - let's go with this one here because it's
236:01 - only eighty dollars again we're not
236:03 - going to be doing this for long but i
236:04 - want to show you how to do staging slots
236:06 - and auto scaling okay so we'll go ahead
236:08 - and apply that there
236:11 - and now it says that it's applied so if
236:13 - i go back to our app here and we click
236:16 - on deployment slots sometimes it doesn't
236:17 - show up right away if it doesn't that's
236:19 - not a big deal you just wait a bit but
236:21 - today it's super fast so we're going to
236:24 - go ahead and add a new slot we're going
236:25 - to call it
236:26 - staging we're going to deploy from our
236:28 - production branch here and i'm going to
236:30 - go ahead and create that there
236:36 - and we'll just wait until that's done
236:38 - okay
236:40 - great so we waited a little bit there
236:42 - and our slot is created so i'm going to
236:44 - just hit close there
236:46 - and so now let's go take a look and see
236:48 - if we can actually see the application
236:50 - here so i just clicked into it i click
236:52 - browse and we're getting the default
236:53 - page so nothing is actually really
236:55 - deployed to it
236:56 - so how are we going to do that that's
236:58 - the the main question here
237:00 - so what i'm going to do
237:03 - is i'm going to make my way over to the
237:04 - deployment center
237:08 - and you can see that it's not configured
237:10 - for this slot so
237:12 - we're going to have to set it up all
237:14 - over again even though it copied over
237:15 - configuration settings it didn't copy
237:17 - over the code so we go to github we'll
237:20 - choose our organization again i'm going
237:22 - to choose the repository we're going to
237:23 - choose
237:24 - that main branch again there we're going
237:26 - to let it add a workflow and notice that
237:28 - this time it's going to call it
237:29 - staging.yaml so there'll be a separate
237:31 - workflow that gets created we're going
237:33 - to go ahead and save that there
237:40 - and what we can do is again click onto
237:42 - our branch name there
237:44 - and if we click into our workflows we'll
237:46 - note now notice that we have a staging
237:48 - example it's the same thing
237:50 - but it should be able to now deploy so
237:53 - the whole purpose of
237:55 - these deployment branches is that it
237:57 - helps us
237:58 - we can deploy different versions of our
238:00 - apps but also
238:01 - it's just a place where we can
238:04 - view things before we actually roll them
238:06 - out so we want to make sure 100 that
238:07 - they are working correctly
238:09 - i don't think this will automatically
238:11 - push out let me just go to my actions to
238:12 - see if this is deploying notice that we
238:14 - have two workflows now we have staging
238:16 - here
238:17 - uh and yeah it looks like it's going to
238:19 - deploy here so we'll just wait a little
238:20 - bit
238:21 - but maybe what we can do is try to have
238:23 - a slightly different version uh for each
238:26 - one here okay
238:27 - but we'll just let that finish and i'll
238:28 - see you back in a moment
238:34 - all right so our deploy finished there
238:35 - so now if we go back to our website here
238:37 - we go browse
238:38 - we should see that application it says
238:40 - hello vulcan and if we go and take out
238:43 - this we still have hello vulcan so how
238:44 - can we have a
238:46 - a variant of this so that we can push
238:48 - out to that so what i'm going to do is
238:50 - i'm going to go back to my application
238:51 - here i'm going to go to code and i'm
238:53 - just going to make a minor change um i
238:56 - don't say also
238:57 - does that spell right startup
239:00 - doesn't look correct to me um
239:02 - so maybe i'll go and adjust that file
239:04 - but it doesn't seem to be affecting
239:06 - anything which is i'm a bit surprised
239:07 - there so what i'll do
239:09 - is i'm going to go and edit that file
239:13 - and give it the proper name can i rename
239:15 - this file
239:16 - yes i can so we'll call that startup
239:18 - file
239:20 - i thought we needed that for deploying i
239:21 - guess it just works without it which is
239:22 - nice
239:23 - if we go back here i'm going to go and
239:26 - actually just want to edit my
239:28 - app here again
239:30 - and i'm going to go and edit this
239:32 - and we'll say
239:34 - hello
239:35 - pandoria
239:38 - or hello and dorians maybe
239:43 - and so if i go back to my actions the
239:44 - question what is it deploying is it
239:46 - going to deploy the production or the
239:47 - staging
239:49 - and it looks like it's going to do
239:54 - both
239:56 - looks like it's doing both here but one
239:57 - way we could tell is we can go to our
239:59 - logs here
240:00 - and we can see that um so we did two
240:02 - deploy so there's one change here
240:05 - if we go back to our main application
240:07 - our deployment center here
240:12 - we go over to our logs
240:15 - you can see that they're both deploying
240:16 - so it doesn't seem like it's a great
240:18 - thing that that's how it works so the
240:20 - question is then how would we
240:22 - um facilitate that deploy right how
240:25 - could we do that i suppose what we could
240:27 - do is just make a separate staging
240:28 - branch
240:30 - so if i go over to code here
240:34 - i don't think we can just make branches
240:36 - through here so what i'm going to have
240:37 - to do is go ahead and
240:39 - oh i can create a branch right here so
240:40 - we'll just type in staging
240:42 - and we'll go create ourselves a new
240:44 - branch
240:47 - and now we're in this branch and what
240:48 - i'm going to do is go ahead and modify
240:50 - this and we're just going to call this
240:56 - hello klingons
240:59 - okay
241:00 - we'll go ahead and update that and so
241:02 - this should be a separate branch so you
241:03 - think what we could do is go in and just
241:05 - change our settings so that it deploys
241:07 - from that one
241:08 - we'll go back to our deployment slots
241:11 - we'll click into staging here
241:14 - and we need to change our configuration
241:17 - settings um
241:19 - i think we could just do it from
241:23 - here hold on here i could have swore it
241:25 - specified the branch if we go to
241:26 - deployment center here
241:28 - i think it's set up on that other branch
241:30 - there
241:31 - i think we just adjusted here so yeah i
241:33 - think we could just
241:35 - adjust these settings
241:39 - we can't discard them
241:41 - but maybe what we can do is just go in
241:45 - and modify that file
241:47 - so we will go into our code here
241:51 - and
241:52 - we will go ahead and click into here go
241:55 - into staging and we'll just change what
241:56 - the branch is called
241:59 - so we'll just say staging
242:05 - and we'll hit start commit and we will
242:07 - save that
242:09 - and we'll see if it actually reflects
242:10 - those changes there so we will go here
242:13 - and hit refresh
242:18 - we'll see if it picks up staging now if
242:19 - we go to settings
242:22 - it's not picking it up so um i'm not
242:25 - sure i don't think perform a redeploy
242:27 - operation we don't want to redeploy so
242:29 - maybe what we'll do is just we'll have
242:30 - to do a disconnect here because it's
242:32 - collect it has the wrong one here so
242:34 - save workflow file
242:35 - um
242:38 - okay we'll just go ahead and delete it
242:40 - it's not a big deal we'll just have to
242:41 - make a new one here
242:43 - we'll go to github
242:45 - we'll choose our
242:46 - organization again or repository our
242:49 - staging branch this time around
242:51 - we'll let it add one see it says we
242:53 - could use an available workflow so we
242:54 - could have kept it there and added it
242:55 - there
242:56 - um and we'll go ahead and save that
242:59 - so now we'll have two separate branches
243:01 - there and we'll give that some time to
243:04 - deploy because that will now trigger a
243:06 - deploy off the bat and so i'll see you
243:07 - back here in a moment
243:09 - all right so after a short little wait
243:10 - here it looks like our app is done
243:12 - deploying so we'll go over here we'll
243:13 - make sure that this is our staging
243:15 - server is good and we want to see that
243:17 - our production is different perfect so
243:19 - we now have a way to deploy to each one
243:21 - but imagine that we want to swap our
243:23 - traffic so we're happy with our staging
243:26 - server and we want to roll that out to
243:27 - production and that's where we can do
243:29 - some swapping so what we'll do is click
243:31 - the swap button and we're going to say
243:33 - the source is the staging and this is
243:35 - our target production we're going to
243:36 - perform that swap
243:38 - right now we can't do a preview because
243:41 - we don't have a particular setting set
243:42 - that's okay
243:44 - and it's kind of showing if there are
243:45 - any changes so set of configuration
243:47 - changes we don't have any so that's
243:49 - totally fine as well we'll go ahead and
243:50 - hit swap
243:53 - and that's going to swap those two i
243:55 - believe it has has zero downtime so
243:57 - we'll be in good shape if that happens
243:58 - there
244:02 - and we'll just give it a moment to do
244:03 - that
244:07 - great so after a short little wait there
244:08 - the swap is complete and so if you
244:11 - remember clearly
244:13 - this was our production right and so if
244:15 - i was to hit refresh
244:17 - it so now say klingons and if i go to my
244:19 - staging server it should be the other
244:20 - way around right
244:23 - good so now imagine that i want to just
244:25 - split the traffic that's something else
244:27 - that we can do um so notice over here we
244:29 - have these percentages here
244:31 - i'm not sure why it won't let me change
244:33 - those
244:35 - so maybe i'll have to look into that so
244:37 - i'll be back
244:38 - so i'm not sure why it's not showing us
244:39 - that traffic slot there but what i'm
244:40 - going to do is just maybe try to trigger
244:42 - a deploy back into our staging and maybe
244:44 - that's what it wants to see
244:46 - so what i'm going to do is go back to my
244:48 - code here we'll be in our staging branch
244:50 - here i'm going to go ahead and edit this
244:53 - file here
244:54 - and
244:55 - we'll just change this to majoran's
245:01 - and we will hit update
245:04 - and we will let that go ahead and deploy
245:07 - so if we go to actions here we can see
245:08 - that it is deploying
245:10 - um and we'll just give it some time okay
245:12 - so we'll see you back here in a bit i
245:14 - mean the other reason could be that
245:15 - we're just not at the main level hold on
245:17 - here
245:20 - if we go back here to deployment slots
245:23 - you know what i think it's just because
245:25 - i was clicked into here and then i was
245:27 - clicked into deployment slots that
245:28 - they're both grayed out yeah it is so we
245:30 - can actually do at that top level there
245:32 - it doesn't hurt to do another deploy
245:33 - though so um we'll just wait for always
245:35 - for that deploy to finish and then we'll
245:37 - come here and adjust that there okay
245:41 - all right so let's take a look at doing
245:42 - some traffic switching here so right now
245:44 - if we were to go to our production we
245:47 - have klingons and if we were to
245:49 - go to our staging
245:55 - we have bajoran so imagine that we only
245:58 - want 50 of that traffic to show up so
246:00 - what we can do is put in 50 percent
246:02 - and uh what i'm gonna do is um
246:06 - do i hit enter here or oh sorry save up
246:08 - here there we go
246:09 - um and so what's going to happen is this
246:12 - should take effect i think right away
246:14 - yep uh and so now we have 50 50 50
246:18 - chance of getting something else here um
246:20 - so i'm just going to keep on hitting
246:21 - enter here if that doesn't work we can
246:23 - try an incognito tab and there we go we
246:26 - got the opposite there and so this is
246:28 - serving up staging right
246:30 - and this is serving up production but
246:32 - they're both on the production url so
246:34 - that's the way you can split the traffic
246:37 - so
246:38 - that's pretty much all i wanted to show
246:39 - you for deployment slots let's now talk
246:40 - about scaling
246:41 - [Music]
246:46 - all right so let's take a look into how
246:48 - we can do some scaling with our app
246:50 - service this is only available if you
246:51 - have beyond standard and or standard and
246:54 - beyond so standard and premium and etc
246:56 - so if we just search for scale we have
246:58 - two options here we have scale up and
247:00 - scale outs the scale up is pretty
247:02 - straightforward that just means to uh
247:04 - make our instance larger and so we
247:06 - already did that when we upgraded from
247:08 - our standard our our b1 over to our s1
247:11 - here right so if i was to go here i'm
247:13 - not going to do that but if i was to do
247:15 - that and upgrade um
247:17 - that would be scaling up right
247:19 - and notice that we're talking about
247:21 - scaling so right now we're limited to 10
247:22 - instances which is totally fine
247:25 - but now let's take a look at scaling out
247:26 - so if we go to scale out here
247:29 - and go to custom auto scale
247:31 - what we can do is we can scale based on
247:33 - a metric so
247:34 - we can add or remove servers based on
247:36 - the demand of the current
247:38 - web applications traffic so we're only
247:40 - paying for servers when we need them and
247:42 - so we have a minimum one a maximum two
247:44 - that seems fine to me but we're going to
247:45 - add a rule here and i want to scale this
247:48 - on
247:49 - the maximum time we're going to do on
247:51 - cpu percentage i just want to have a
247:53 - very easy way to trigger this so we can
247:55 - see a scaling event in action here it
247:58 - has a maximum of 16 i might just lower
248:01 - that down even further well let me type
248:03 - in there new so 16 is what it's going to
248:06 - have to be it's not a big deal but i am
248:07 - going to reduce it down to actually i
248:09 - think sorry i don't know i was going
248:11 - here the the metric threshold to scale
248:13 - an action i'm going to put it at 10
248:15 - sorry
248:17 - okay
248:19 - so here's that line and so we have a and
248:21 - i like how you can drag it but you kind
248:23 - of have an idea that we have a high
248:25 - chance of having this um trigger i just
248:27 - want to do this so that we have a
248:29 - a good chance so if i was to put it here
248:31 - you can notice that it's very easy for
248:32 - us to spike our traffic and and cause a
248:34 - scaling event now i'm gonna set the
248:36 - duration to one minute so we have a much
248:38 - higher chance of uh triggering that
248:40 - there
248:41 - okay
248:42 - set a duration less than five minutes
248:44 - may generate a transient spikes yeah
248:46 - that's fair but i mean i just want to
248:48 - show you a trigger happen
248:50 - and we need a cool down time probably um
248:53 - and it's set to five minutes that's
248:54 - totally fine we're gonna add one
248:56 - and that looks fine to me i'm gonna set
248:58 - this to maximum okay and so now we're
249:00 - very likely to trigger that there we'll
249:02 - go ahead and hit add that
249:05 - instance there and so now that we have
249:07 - that we're going to go ahead and save
249:08 - that
249:20 - and so now that that's there what we
249:22 - want to do is actually trigger a scaling
249:25 - event and so
249:26 - where we're going to see that is under
249:28 - the monitoring tab
249:33 - so if we go to monitoring
249:38 - and what we're going to do is go over to
249:41 - um it should be in
249:44 - sorry i forgot the place where we need
249:45 - to go take a look here is actually in
249:46 - the run history here so if we go here
249:49 - and check one hour we can see how many
249:50 - instances are running
249:52 - uh and i think if i dial it back here it
249:53 - should show me over time as it changes
249:56 - we do have a scale up event that has
249:57 - happened which happened i guess four
249:59 - minutes ago
250:01 - um so it gives you kind of an idea of
250:03 - how many instances are running which
250:05 - right now are two
250:06 - um so maybe our
250:09 - maybe our scaling event is not in the
250:11 - best uh use case there because it's
250:12 - happening too frequently so what i'm
250:14 - gonna do is go ahead
250:15 - and modify that scaling rule
250:18 - um and so i'm just gonna go back and
250:20 - click here and maybe we'll just make it
250:22 - so it is less aggressive so what i'm
250:24 - gonna do is just change it so it's over
250:26 - the duration of five minutes
250:29 - and i'm gonna just put it right above
250:31 - here so that it goes back to one
250:33 - okay
250:35 - and we'll go ahead and save that
250:40 - and so now we go back to our run history
250:42 - here
250:43 - it still shows that it has
250:45 - two
250:46 - as you can see here but i want to see
250:48 - this drop back down to one
250:49 - so it's going to check every five
250:51 - minutes or or within the span of five
250:53 - minutes so what i'm gonna do is just uh
250:55 - wait here and i'll see you back in a bit
250:57 - until we see a scaling action happens uh
250:59 - here okay
251:01 - yeah i'm just sitting here waiting for
251:02 - it to scale down and i don't see it
251:04 - going so it makes me think that i need
251:05 - to go ahead and set a scale
251:07 - scale down action let's take a look at
251:09 - the one that we currently have uh so
251:11 - this one is set oh you can see it's
251:13 - still spiked we don't even have anything
251:15 - going on here but what i'm going to do
251:16 - is just be really aggressive here
251:18 - i'm going to say when it's
251:23 - 50 okay
251:27 - and so here
251:29 - we'll go back here and i'll save that
251:32 - and i just want to see if it scales down
251:34 - i shouldn't have to set a scale down
251:35 - action should just go
251:37 - um
251:38 - and what i'm actually going to do is be
251:40 - a little bit more aggressive i know i'm
251:41 - setting a lot of stuff here but i'm
251:43 - going to just set it to duration of 1
251:44 - minutes
251:45 - so we can see this a lot sooner
251:49 - and we will go back to our run history
251:52 - here and we'll see if we observe a scale
251:54 - down
251:55 - all right so um it's not scaling down
251:57 - here but uh i think it's probably
251:59 - because i need to scale out action so
252:00 - what we'll do is go ahead and add a new
252:02 - rule
252:03 - this thing if we go here
252:06 - and we just look at it
252:10 - um
252:12 - it's not going to decrease it unless we
252:14 - have a scale of action
252:16 - so
252:17 - i don't think it's necessary for us to
252:19 - set one here i think you get the idea
252:21 - but that's for scaling so we're all done
252:23 - here with azure app services all we got
252:25 - to do is go ahead and go ahead and
252:26 - delete it so let's go ahead and delete
252:29 - our app here okay
252:32 - so there's a few different ways we can
252:33 - do it i'm going to do it via resource
252:35 - groups
252:36 - i believe we called it voyager here so
252:39 - click into that
252:41 - and i'm going to go ahead and delete the
252:43 - resource group
252:45 - and here is all the related services
252:48 - and so i will type in voyager
252:51 - and there we go
252:57 - great and so yeah there we go
253:00 - [Music]
253:04 - hey this is andrew brown from exam pro
253:06 - and we're looking at azure app services
253:08 - cheat sheet this is a two pager so let's
253:10 - jump into it azure app services is an
253:12 - http based service for hosting web
253:14 - applications rest apis and mobile
253:16 - backends you can choose a programming
253:17 - language in either a windows or linux
253:19 - environment it's platform as a service
253:21 - so it's the heroku or aws elastic
253:23 - beanstalk equivalent of azure if that
253:24 - helps you remember what it is azure app
253:27 - services makes it easy to implement
253:28 - common uh common features
253:31 - so for integrations and features such as
253:33 - azure devops github integrations
253:36 - docker hub integrations package
253:38 - management easy to set up staging
253:40 - environments custom domains attaching
253:42 - tls and ssl certificates you pay based
253:46 - on the azure app service plan so you got
253:48 - a standard tier a dedicated tier and an
253:50 - isolated tier notice
253:53 - that this tier does not support linux
253:54 - okay
253:56 - azure app services uh supports the
253:58 - following runtime so net.net core java
254:01 - ruby node.js php and python azure app
254:04 - services can also run docker as a single
254:07 - docker container or multi-containers you
254:09 - can also upload your own custom
254:11 - container so you'd upload it or you just
254:13 - deploy it so you create it upload it to
254:15 - azure container registry and then deploy
254:18 - you have deployment slots this allows
254:19 - you to create different environments for
254:20 - your web application you can swap
254:22 - environments this could be how you
254:24 - perform a blue green deployment onto the
254:26 - second slide here app app service
254:28 - environment asc is an app azure app
254:30 - service feature that provides a fully
254:32 - isolated dedicated environment for
254:33 - securing or securely running app service
254:37 - apps at high scale customers can create
254:39 - multiple ases within a single azure
254:41 - region or across multiple azure regions
254:43 - making ase ideal for horizontal scaling
254:46 - stateless application tiers or high
254:48 - requests per second rps workloads ac
254:51 - comes with some pricing tier which is
254:53 - isolated tier ase can be used to
254:55 - configure uh security architecture apps
254:58 - running on asc can have their access oop
255:02 - this should say granted not graded by
255:04 - upstream devices such as a web
255:07 - application firewall a waff
255:10 - and app service ascs can be deployed
255:13 - into an az using a zone pinning there
255:15 - are two deployment types for ase you
255:18 - have external ase and i r i l b asc ilb
255:22 - would be internal load balancer okay
255:25 - azure app services provides many ways to
255:28 - deploy your applications and there's so
255:30 - many i'm not going to listen here but i
255:32 - i probably would recommend that you go
255:33 - review the actual uh content for that
255:36 - because it just they would have made it
255:37 - would have made a whole cheat sheet here
255:39 - uh then the last thing here web jobs is
255:40 - a feature of azure app services that
255:42 - enables you to run a program or script
255:44 - in the same instance as the web app api
255:46 - mobile app there's no additional cost to
255:48 - web jobs so there you go
255:50 - [Music]
255:54 - let's take a look at what a key value
255:56 - store is so a key value store is a data
255:59 - store that is really dumb but it's super
256:02 - super fast okay and so they'll lack
256:04 - features that you would normally see in
256:05 - relational databases like relationships
256:07 - indexes aggregation transactions all
256:09 - sorts of things uh but you know there is
256:12 - a trade-off for that speed okay and so
256:14 - here is kind of a representation of a
256:16 - key value store which uh you have a key
256:18 - which is a unique you know key to
256:20 - identify the value and i'm representing
256:23 - the value as a bunch of ones and zeros
256:24 - because i want you to understand that
256:25 - there aren't really columns it's just
256:27 - key and value
256:28 - so the idea is that imagine that those
256:30 - ones and zeros actually represent
256:32 - a dictionary and that's usually what
256:34 - they are is it associative array hash
256:35 - dictionary underneath
256:37 - okay and so even though it looks like
256:39 - you know what i mean like if this was a
256:40 - relational database you know you could
256:42 - see these as kind of like columns and so
256:44 - if we kind of did that that's how a key
256:47 - value store can kind of mimic
256:49 - um you know a tabular data right
256:52 - but the thing is is that you know there
256:54 - is no consistency between the the rows
256:57 - hence it is schema-less but that's kind
256:59 - of a way to get tabular data from key
257:01 - values but due to their simple design
257:03 - they can scale well well beyond
257:04 - relational databases so relational
257:06 - databases it becomes very hard to shard
257:08 - them and do a bunch of other stuff with
257:09 - them but key value stores are super easy
257:11 - to scale but you know they come with a
257:14 - lot of extra engineering around them
257:16 - because of these missing features
257:18 - [Music]
257:22 - all right let's talk about document
257:23 - stores so document store is a noscope
257:24 - database that stores document as its
257:26 - primary data structure a document could
257:27 - be an xml but it's most commonly json or
257:30 - json like structure and documents
257:33 - stores are sub classes of key value
257:35 - stores so the components of a document
257:38 - store compared to relational database
257:39 - looks like this so the idea is that you
257:42 - have tables is now collections rows are
257:44 - documents columns are fields indexes are
257:46 - still the same name and when you do
257:47 - joins they're called embedding and
257:48 - linking so you know if a key value store
257:51 - can kind of store this why would you do
257:53 - it well there's just a lot more features
257:55 - around the documents itself and so you
257:58 - know how we saw key value store didn't
257:59 - have like it had like nothing like no
258:01 - functionality well document sort brings
258:03 - a lot more of the functionality that
258:04 - you're used to in a relational database
258:06 - you know and so it makes things a little
258:08 - bit easier to work with okay
258:10 - [Music]
258:14 - all right let's take a quick look here
258:15 - at mongodb which is an open source
258:17 - document database which stores json-like
258:19 - documents and the primary data structure
258:20 - for mongodb is called a bson so a binary
258:23 - json is a subset of json so its data
258:25 - structure is very similar to json but
258:28 - it's designed to be both efficient and
258:30 - storage
258:31 - in both storage space and scan speed
258:32 - compared to json and bson has more data
258:35 - types than json has date times byte
258:37 - arrays regular expressions md5 binary
258:39 - data javascript code json's just strings
258:42 - integers and arrays it's very very
258:44 - simple but because it has all these new
258:46 - other data types and it's stored in this
258:48 - binary format it's not plain text it's
258:50 - actually binary data
258:51 - that's the what reason why it the
258:53 - storage space and the scan speed is so
258:55 - fast now if you did use javascript to
258:57 - perform an operation like say insert
258:59 - data this is what it would look like so
259:01 - you have kind of an idea that you're
259:03 - inserting items into a collection there
259:05 - okay
259:06 - just to list out some features of
259:07 - mongodb
259:08 - it supports searches against fields
259:10 - range queries regular expressions it
259:12 - supports primary and secondary indexes
259:14 - it's highly available it's it's high
259:16 - availability can be obtained via rep
259:18 - replica sets so replica to offload reads
259:21 - or access standby in case of failover
259:23 - momentum b scales horizontally using
259:25 - sharding mongodb can run multiple
259:27 - servers via load bouncing mongodb can be
259:29 - used as a file system which is called
259:30 - grid fs with uh with load balancing and
259:33 - data replication features over multiple
259:35 - machines uh for storing files mongodb
259:37 - provides three ways to perform
259:38 - aggregation uh grouping dat and
259:40 - aggregations just grouping data to
259:42 - return a query so aggregation pipeline
259:44 - map reduce single purpose aggregation
259:47 - mongodb supports fixed collections
259:49 - called capped collections going to
259:51 - become claims to support multi-document
259:54 - asset transactions so mongodb when it
259:57 - first came out didn't do all this stuff
259:59 - and people complained about it i like it
260:01 - being very hard to scale but now it's a
260:02 - lot easier to use so you know mongodb is
260:05 - something that is uh more uh
260:08 - a more popular option nowadays than it
260:10 - was a few years ago so there you go
260:12 - [Music]
260:16 - all right let's take a look here at what
260:17 - a graph database is so graph database is
260:19 - a database composed of data structures
260:21 - that use vertices nodes or dots which
260:23 - form relationships to other vertices via
260:25 - edges arcs and lines so some use cases
260:27 - here fraud detection real-time
260:29 - recommendations engines master data
260:30 - management network and it operations
260:33 - identity and access management and
260:35 - there's a lot they're saying like it's
260:36 - really really good for that i am
260:38 - something i want to look into later
260:39 - traceability and manufacturing contact
260:41 - tracing
260:42 - data lineage for gdpr customer 360
260:46 - degree analysis like for marketing
260:48 - product recommendations social media
260:50 - graphing and feature engineering for ml
260:53 - so let's just kind of break down you
260:55 - know the little components here so what
260:56 - you'd have is a node and a node can
260:58 - contain data properties and then through
261:00 - that it would have a relationship
261:02 - through an edge and that relationship
261:04 - can have a direction and also data
261:06 - properties on it and so it's a lot more
261:09 - um
261:10 - verbose like in than a relational
261:13 - database and also just how it can point
261:14 - to stuff so
261:16 - super useful for particular use cases
261:23 - let's take a look here at azure
261:24 - tinkerpop which is a graph computing
261:26 - framework for both graph databases oltps
261:29 - and graph analytics systems olaps so
261:32 - tinkerpop enables developers to use a
261:34 - vendor agnostic distributed framework to
261:36 - traverse query many different graph
261:39 - systems they always say traverse because
261:40 - there's so many it's a tree right
261:43 - so there's a lot of databases that this
261:45 - thing connects to and so here they all
261:47 - are but the ones i want to indicate to
261:49 - you that are important is amazon neptune
261:51 - cosmodb hadoop via spark neo4j which is
261:55 - one of the most popular
261:56 - graphing databases orientdb and titan
261:59 - okay so the thing is is that this isn't
262:02 - a
262:02 - graph database
262:04 - it is a basically adapter to other graph
262:07 - databases and ticker pop includes a
262:09 - graph traversal language called gremlin
262:11 - which is the single language that can be
262:12 - used for all these graph systems so
262:14 - let's talk about gremlin gremlin is a
262:16 - graph traversal language for apache
262:18 - tinker pop and so it looks like this and
262:21 - sometimes you know like even without
262:23 - tinker pop and i think this is with
262:24 - cosmodb that they'll support this
262:26 - language by default so you don't
262:27 - necessarily need to have tinker pop to
262:29 - work with some databases but it's great
262:31 - to have that service if you if or like
262:33 - the framework if you need it so gremlin
262:35 - is is designed to write once and run
262:37 - anywhere w-o-r-a gremlin traversal can
262:40 - be evaluated as a real-time query so
262:42 - lltb or a batch analytics query so over
262:45 - here it's just kind of showing you these
262:46 - are the oltps graph databases over here
262:49 - and then on the right hand side we have
262:51 - olaps okay
262:52 - and so gremlin hosted language embedding
262:55 - means you can use your favorite
262:56 - programming language when you write
262:58 - gremlin okay so there you go
263:00 - [Music]
263:04 - hey this is andrew brown from exam pro
263:05 - and we are looking at azure tables which
263:07 - is a type of storage for nosql key value
263:10 - data store within the azure storage
263:11 - accounts azure table stores
263:13 - non-relational structured data with a
263:14 - schema-less design and there are two
263:16 - ways to interact with azure tables
263:18 - through the storage table storage api or
263:21 - microsoft azure storage explorer which i
263:24 - find is the easiest way to interact with
263:26 - it so just kind of looking at storage
263:28 - explorer there if you wanted to add an
263:30 - entry uh you'd have to provide a
263:32 - partition key which is a unique identify
263:34 - uh fire for the partition with a given
263:36 - table and a row key a unique identifier
263:38 - for an entity within a given part a
263:41 - partition and so you have all your data
263:43 - types here so we see string boolean
263:44 - binary data type uh double uh
263:48 - guids 32 and 64. if we wanted a query
263:52 - you'd have to query along the partition
263:54 - and row key so you could also do some
263:56 - additional filtering here so just notice
263:57 - here that um you know you have your
264:00 - partition key you put in your value like
264:01 - klingon and wharf and then this is not
264:04 - this is just additional properties you
264:05 - added
264:06 - a lot of time the way these key values
264:08 - work is that
264:10 - this will return the results like all
264:12 - the results and then server side and
264:15 - then client side these will be filtered
264:16 - client side i don't know if that's the
264:18 - case with azure table but that's
264:19 - generally how these things work and so
264:21 - there you go
264:25 - hey it's andrew brown from exam pro and
264:27 - we're looking at cosmodb which is a
264:29 - service for fully managed nosql
264:30 - databases that are designed to scale
264:33 - and be highly performant so cosmodb
264:35 - supports different kinds of nosql
264:36 - database engines which you interact via
264:38 - an api so we have the core sql which is
264:41 - their document data store their azure
264:43 - cosmodb api for mongodb their azure
264:46 - table
264:47 - and gremlin okay and this will be using
264:50 - probably tinker pop um so all of these
264:53 - nosql engines uh uh specify capacity so
264:56 - you can do provision throughput for pay
264:58 - for guarantee of capacity or serverless
265:00 - pay for what you use so if you are just
265:03 - playing around the service you can go
265:04 - ahead and choose that serv
265:06 - serverless option and so a lot of times
265:08 - when people talk about cosmodb they're
265:09 - usually talking about core sql so if you
265:11 - say cosmodb it's usually document but
265:14 - understand that there's a bunch of stuff
265:16 - underneath it
265:17 - now if you want to start viewing data
265:19 - and making stuff and playing around with
265:20 - it you'd use the cosmo db explorer which
265:23 - is a web interface that you can find at
265:25 - cosmos.azure.com so after you made your
265:27 - cosm db cluster or container whatever
265:29 - they call it
265:30 - then you can go access your database so
265:33 - here we have the sql api
265:35 - and so that would be the document store
265:37 - and you can just see here that
265:38 - we have we've created a new item here
265:41 - for that data okay
265:43 - and so i just want to show you that if
265:44 - you drop down here you choose container
265:46 - or database so we create a new container
265:49 - also if you are in
265:51 - azure it looks like they just have it
265:53 - here under the data explorer tab so it's
265:54 - the same thing it's the cosmo db
265:56 - explorer just in line okay so you don't
265:58 - have to like go to that url you could
266:00 - just click into your um your it's called
266:03 - account cosmodb account and go to data
266:05 - explorer i just wanted to show you here
266:07 - like if you made a graph database that
266:08 - you can do everything through this
266:09 - explorer for all the different types the
266:11 - interface will change a bit so here we'd
266:12 - add a new vertex right and it's just
266:14 - slightly different okay
266:16 - [Music]
266:20 - all right so the thing about azure
266:22 - tables is that you can
266:23 - use it within either cosmodb
266:26 - okay or you can use it within account
266:28 - storage and the thing is is that um
266:31 - it's a really good comparison to look at
266:33 - these two things because
266:35 - this way we can really understand like
266:37 - how powerful cosmodb is all right
266:40 - so what we'll do is compare the two so
266:42 - over here when you have azure tables in
266:44 - account storage
266:46 - it's fast but it has no upper bounds of
266:47 - latency
266:48 - for azure cosmodb it's going to give you
266:50 - single digit millisecond latency for
266:52 - reads and writes
266:53 - for throughputs
266:55 - it's variable throughput it's limited to
266:57 - 20 000 operations you get a guaranteed
266:59 - uh backed by an sla and no upper limits
267:01 - when you're using cosmo db for global
267:04 - distribution it's a single region
267:06 - and for cosmic db you have 30 plus
267:09 - regions for indexing you only get the
267:11 - primary index or partition and row no
267:13 - secondary indexes and then for
267:16 - cosmodb you get automatic and complete
267:17 - indexing in all properties no index
267:19 - management for querying you get query
267:21 - execution uses index for primary key and
267:24 - scans otherwise and for uh cosmodb you
267:27 - get queries that can take advantage of
267:29 - automatic indexing on properties for
267:30 - fast query times for consistency we got
267:33 - strong with primary region and eventual
267:35 - with secondary regions and with uh
267:38 - cosmodb there's like five you know what
267:40 - i mean there's this uh the consistent
267:42 - levels are a lot more flexible okay
267:44 - for pricing it's consumption based and
267:46 - then for uh cosmodb you have consumption
267:49 - based or provision capacity for this
267:51 - delays it's 99.99 availability and here
267:54 - it it's backed by an sla but some
267:56 - conditions it does not apply okay so you
267:59 - know hopefully that shows you that
268:01 - cosmic db like is very performant is
268:03 - globally available uh single digit
268:05 - millisecond and i i really feel like
268:07 - this is to compete with um adabus
268:10 - dynamodb because it sounds so
268:12 - similar to dynamodb but um yeah there
268:15 - you go
268:16 - [Music]
268:20 - hey this is andrew brown from exam pro
268:22 - and we are on to the azure tables and
268:23 - cosmos db cheat sheet for the dp900 i
268:26 - want to point out something uh that i'm
268:28 - sure you already know about but in the
268:29 - course i spelt cosmos db without the s
268:32 - like everywhere and i'm not going to go
268:35 - back and fix that
268:36 - but i know i'm going to hear like
268:38 - never the end of it for like the next
268:40 - year okay
268:41 - so uh let's start at the top here azure
268:43 - tables it's a key value data store can
268:45 - be hosted on either azure storage
268:47 - account storage it is designed for a
268:49 - single region and single table can be
268:51 - hosted on cosmos db
268:53 - and when it's hosted here it's designed
268:55 - for scale across multiple regions
268:57 - cosmodb a fully managed nosql service
268:59 - that supports multiple nosql engines
269:01 - called apis why they didn't call them
269:03 - engines i don't know
269:04 - coresql api this is the default one it's
269:07 - a document database you can use sql to
269:09 - query documents and when people are
269:10 - talking about cosmodb that's what
269:12 - they're talking about the document
269:13 - database the default one okay
269:15 - graph apis a graph database you can use
269:18 - uh
269:19 - with gremlin to transfer traverse the
269:21 - nodes and edges mongodb api a mongodb
269:24 - database it is a document database
269:26 - tables ai is just as your table's key
269:28 - value but within cosmodb apache tinker
269:31 - pop an open source framework uh to have
269:33 - an agnostic way to talk to many graph
269:35 - databases they probably won't ask you
269:36 - about tinker pop on the exam gremlin
269:39 - graph traversal language to traverse
269:40 - nodes and edges you definitely need to
269:42 - know what gremlin is and be used to
269:44 - seeing what it is like identify what it
269:46 - looks like mongodb an open source
269:48 - document database and the way it works
269:51 - is it has its own
269:52 - data structure its document structure
269:54 - called bson which is binary json
269:56 - a storage and compute optimized version
269:58 - of json introduces new data types cosmo
270:01 - db explorer a web ui to view cosmos
270:03 - databases and there you go
270:08 - [Music]
270:14 - hey this is andrew brown from exam pro
270:16 - and we are taking a look at cosmos db so
270:19 - cosmos db is a
270:21 - service for fully managed nosql
270:23 - databases that are designed to scale
270:26 - and be highly performant so cosmos db
270:29 - supports different kinds of nosql
270:31 - database engines which you're going to
270:33 - interact via an api we'll cover them
270:36 - more than once here but we have our core
270:38 - sql which is a document data store
270:41 - azure's cosmos db api for mongodb which
270:45 - is another kind of document data store
270:47 - azure table which is the old cosmos db
270:50 - that still sticks around which is a key
270:52 - value data store we've got gremlin uh
270:55 - which is a graph data store based on
270:57 - apache tinker pop that's why we were
270:59 - covering it earlier in this course here
271:02 - uh and all of these nosql engines uh
271:05 - specify capacity so we have things like
271:07 - provision throughput so you pay for a
271:09 - guaranteed amount of capacity or you
271:12 - have a serverless option so you pay for
271:13 - what you use so that's an option that
271:15 - you can choose
271:18 - there let's just talk about some of the
271:19 - main advantages of using cosmos db
271:22 - because azure really likes you
271:24 - uh
271:25 - to use cosmos db because they consider
271:27 - like one of their flagship
271:29 - database products so it integrates with
271:31 - many azure services like azure functions
271:34 - aks azure app services integrates with
271:37 - many different database apis like
271:40 - coresql mongodb as we saw cassandra as
271:43 - we saw gremlin as we saw
271:46 - there's sdks for a variety of
271:47 - languages.net java python node.js not
271:50 - ruby so i'm not sure if i'm going to be
271:52 - using anytime soon but there's a diff a
271:54 - lot of different sdks there uh it has a
271:57 - schema less service that applies indexes
272:00 - to your data automatically resulting in
272:01 - fast queries you get a guarantee of 99
272:05 - to the
272:06 - three nines of availability there
272:09 - we have data replication between azure
272:11 - regions and this is all automatic uh
272:14 - data protect protected with encryption
272:16 - at rest and role-based access controls
272:18 - rbacs
272:20 - auto it auto scales um it provides a way
272:23 - to handle variety of different workload
272:24 - sizes
272:25 - so you can see that there's a lot of
272:28 - advantages of using cosmos db
272:33 - [Music]
272:35 - so something that's very interesting
272:36 - about cosmos db api is that it's kind of
272:38 - like an umbrella service for a bunch of
272:40 - different types of databases
272:42 - um and this is a lot this can be
272:44 - confusing for someone that's coming from
272:46 - aws or gcp where like adabus has
272:49 - dynamodb and all it is is a document
272:52 - database where um cosmos db it like
272:56 - supports a variety of different kinds
272:59 - and so when you first create your cosmos
273:02 - db database you have to choose an api
273:05 - and so we'll just walk through the
273:06 - different types of apis here and just
273:08 - give you some information around them
273:10 - so the first one and this is the default
273:12 - one is core sql api so it is a document
273:15 - database but it's interesting because it
273:18 - allows you to use an sql or sql-like
273:22 - language and so
273:23 - this is quite popular because
273:26 - one of the challenges of using document
273:27 - database is that normally you don't get
273:29 - to use sql
273:31 - but with cosmos db
273:33 - you do and so that is something that's
273:35 - really nice then you have azure table
273:37 - api this can be a bit confusing because
273:40 - when cosmos db uh
273:42 - before it came out there was just azure
273:44 - table
273:45 - um and so azure table
273:47 - uh
273:48 - basically is just a key value store and
273:51 - you access that through azure table
273:53 - storages
273:54 - um
273:55 - like through storage accounts
273:57 - but then they decided to make azure
273:59 - table to be more of a document database
274:02 - and to make it highly uh
274:05 - resilient and highly available and
274:08 - highly redundant and so basically core
274:11 - sql api is like the version two of azure
274:13 - table api but this one still exists
274:15 - because it's very cost effective so if
274:18 - you don't need a lot of redundancy and
274:20 - you just need a key value store you need
274:21 - something very inexpensive you can use
274:23 - azure table api but even though you
274:25 - don't really access it through
274:26 - cosmos db it still is
274:29 - part of the cosmos db product because it
274:31 - was the first
274:32 - iteration or version of it you have
274:34 - mongodb api and so
274:37 - um this is uh at
274:40 - it's not mongodb i don't think
274:41 - underneath but it's uh it's compatible
274:44 - with
274:45 - mongodb so the idea is that if you need
274:47 - mongodb this is where you're going to
274:49 - spin that up same thing with cassandra
274:51 - if you need cassandra you would spin
274:52 - that up as well
274:54 - and this is going to use the cassandra
274:55 - queried language
274:57 - and so that is something that is there
275:00 - then you have gremlin api so this would
275:03 - allow you to use a graph database you
275:05 - can see these are really really
275:07 - different things
275:08 - and it is a bit confusing but uh that's
275:11 - just how azure likes to organize their
275:13 - services so hopefully that clears that
275:15 - up for you okay
275:17 - [Music]
275:21 - so let's talk about the core components
275:23 - for cosmos db because it is a bit
275:26 - confusing to try to understand the
275:27 - relationship of all the components
275:29 - for a fully managed database it sure has
275:32 - a lot of moving parts
275:33 - and so the first thing when you go over
275:35 - to the cosmos db
275:38 - portal
275:40 - you're going to spin up a database
275:42 - and so the idea is that when you
275:44 - choose a database the idea is that this
275:46 - is going to be
275:48 - a logical namespace for your collections
275:50 - your users your permissions it's going
275:51 - to determine the api that you do choose
275:55 - uh once you've chosen
275:57 - your uh your database then the idea is
276:00 - that you need containers and normally
276:01 - when you create a database it'll create
276:03 - a container for you and a container is
276:05 - just a unit of compute
276:07 - but there is a lot of stuff that you can
276:09 - attach to that container information
276:10 - when you create it and then you also
276:12 - have collections i find collections to
276:14 - be a bit confusing but they map the they
276:17 - basically map to a container and this is
276:20 - the billable entity so this is the way
276:21 - that you're going to determine the cost
276:24 - um and this is where you're going to be
276:26 - stating that information so when you
276:27 - create a container it'll have a bunch of
276:29 - information below it um and that will be
276:32 - uh
276:33 - stuff that will be attached to the
276:34 - collection entity
276:36 - so hopefully that kind of clears that
276:37 - stuff up
276:38 - i do find containers a bit confusing
276:40 - because again when you create them
276:42 - it's at the same time as your database
276:43 - but we will talk about that because we
276:46 - do we do come across that in the course
276:48 - okay
276:49 - [Music]
276:53 - let's talk about how we're going to
276:54 - access our cosmos db database with
276:58 - cosmos db explorer so it's a web
277:00 - interface to explore and interact with
277:02 - your cosmos db accounts accounts
277:04 - database you know basically the same
277:07 - thing and this is at
277:09 - cosmos.azure.com so they have a
277:10 - dedicated
277:12 - domain sub-domain for it so the idea is
277:15 - you type that in and you would
277:17 - authenticate with your account um
277:20 - azure's always single sign-on so it's
277:21 - pretty easy and then the idea is that
277:23 - you can access
277:26 - your database and interact with it
277:27 - without having to write
277:29 - any any kind of uh sdk code or interact
277:33 - with cli you have kind of a gui to do
277:35 - that so example here is for the core sql
277:39 - and you can interact with all the
277:41 - different types of database in here so
277:42 - it's not just coresql but there's a lot
277:44 - that you can do with it
277:46 - and so the idea is you can create
277:48 - containers from here create new
277:49 - databases um so a lot of similar things
277:52 - you can do from the
277:53 - the um
277:54 - the cosmosd portal with an azure portal
277:57 - so there's some some stuff there it
278:00 - doesn't matter where you create it it's
278:01 - just where you want to do that there
278:04 - there is an easier way to access this so
278:06 - instead of going to cosmos
278:09 - azure.com the way i like to access it is
278:11 - that i'll just be in the azure portal
278:13 - for my database
278:15 - and uh what i will do is access it under
278:18 - the data explorer tab notice here it
278:20 - says azure cosmos db account it gets
278:22 - confusing because they say like an
278:25 - account can or like
278:26 - can contain multiple databases but
278:28 - really it's just one database it's so
278:30 - confusing so you'll hear me interchange
278:32 - the term account and database
278:35 - because the documentation does not
278:37 - clarify it but in the in the
278:39 - gui you can see it says the word account
278:41 - there but yeah this is the way that i
278:42 - would recommend accessing that
278:44 - information
278:45 - um
278:46 - you know you can also again access
278:48 - different databases so like for example
278:50 - we can see this is for gremlin so here
278:52 - i'm creating a new vertex which is a
278:55 - graph graphing thing there and so that's
278:57 - how we can do it there so it's not just
278:58 - for
279:00 - the core sql okay
279:02 - let us talk about partitioning schemas
279:04 - in cosmos db because it's a very
279:06 - important concept when we're talking
279:07 - about new sql databases because
279:10 - um you know it's all about scale and
279:12 - when you're dealing with databases of
279:14 - scale you're dealing with partitions and
279:16 - this is where we have a lot of
279:18 - information about partition keys so the
279:20 - data stored among partitions in amazon
279:23 - cosmos db indexes is grouped by the
279:25 - partition keys in order to improve
279:27 - performance main concepts of partition
279:30 - schemas in azure's cause of cdb is
279:32 - partition keys so these are used these
279:34 - keys are used to group items it works
279:37 - like primary keys if you know from
279:41 - traditional relational databases then
279:43 - there's the idea of a logical partition
279:45 - this is a group of items that have all
279:47 - the same partition key values you have
279:50 - physical partitions
279:53 - so this consists of a set of logical
279:55 - partitions
279:56 - azure cosmos db manages logical
279:58 - partitions which can have
280:00 - one to many relationships we have the
280:02 - idea of replica sets these are made up
280:05 - of a group of physical partitions that
280:06 - are materialized as a self-managed
280:08 - dynamically loaded balanced group of
280:10 - replicas that span across multiple fault
280:13 - domains so i imagine when they say the
280:15 - word physical they're probably saying
280:17 - like
280:18 - it's the actual physical underlying
280:20 - machine and then these are the virtual
280:22 - logical partitions within those machines
280:23 - and then replica sets just means
280:25 - duplicates of that kind of stuff
280:28 - so hopefully this diagram here will
280:30 - maybe help a bit here to make sense of
280:32 - it there's a lot going on here but maybe
280:34 - we can break it down so each physical
280:36 - partition consists of a set of replicas
280:39 - also known as replica sets so over here
280:42 - um we see partition set
280:45 - and then we have partition physical
280:47 - partition
280:48 - um
280:49 - and you can see that it's it's
280:51 - represented over here so confusing but i
280:53 - think this
280:54 - yeah this thing this thing here is the
280:56 - physical partition
280:58 - okay
281:00 - over there
281:01 - um
281:02 - then we have logical partitions so these
281:04 - are mapped to physical partitions that
281:05 - are uh distributed globally so notice
281:08 - that it's these little boxes in here so
281:10 - it was what i said so this is the
281:12 - physical machine and then these are the
281:14 - virtual partitions within them
281:17 - and they're distributed globally
281:18 - partition sets in the image refer to a
281:21 - group of physical partitions that manage
281:23 - the same logical partition keys across
281:25 - multiple regions so hopefully that makes
281:27 - things a bit more clear uh in
281:30 - practicality you probably won't need to
281:31 - remember all of this for the exam you
281:33 - definitely don't need to know this but
281:35 - it's good to just kind of go through it
281:37 - so you have an idea of these terms so
281:39 - there you go
281:40 - [Music]
281:44 - all right let's talk about choosing a
281:46 - partition key this is super super super
281:48 - important to cosmos to be
281:51 - so the idea is that a partition key is
281:53 - made up of two components the partition
281:55 - key path and the partition key
281:57 - value so the idea is that let's consider
282:00 - a document
282:02 - or a key value that we need to try to
282:05 - figure out what our prediction key path
282:07 - would be so we have user id andrew works
282:09 - for microsoft so in this case we're
282:11 - trying to find something that is um
282:14 - like a unique identifier for that item
282:17 - and so
282:18 - i would say a user id would be that
282:20 - thing there so in this case our
282:22 - partition key path would be forward
282:24 - slash user id
282:26 - um
282:27 - so you know
282:29 - that would be part one of it
282:31 - and so the partition key path accepts
282:33 - numerical or underscore characters you
282:36 - could also use nested objects by using
282:38 - the standard path notation of forward
282:39 - slash so that helps there
282:42 - um and so an example here would be
282:44 - forward slash id or we do forward slash
282:46 - user id
282:47 - uh the second part of it is the value so
282:49 - the partition key value can be either a
282:51 - string or numeric types and so you can
282:54 - see in our example it is a string so
282:56 - that's going to be accepted
282:58 - your partition key for all containers
283:00 - should be a property that has a value
283:02 - which does not change
283:04 - you can't change the value of a property
283:06 - if it's your partition key a property
283:09 - that should have a wide range of
283:11 - possible values that's why we said user
283:13 - id because it's going to be pretty
283:15 - unique
283:16 - per
283:17 - per person whereas works for you're
283:19 - going to have a bunch of groups of that
283:21 - like there might be a lot of people who
283:22 - work for microsoft so it's not as unique
283:24 - um
283:25 - spread request units is something that's
283:27 - important are used these are consumption
283:28 - and data storage that will be evenly
283:31 - across all the logical partitions this
283:33 - ensures uh even ru consumption and
283:36 - storage distribution across your
283:37 - physical partitions so hopefully that
283:39 - gives you kind of an idea
283:41 - of
283:42 - how to choose a partition key but now
283:44 - we'll talk about unique keys
283:46 - [Music]
283:51 - so uniques provide developers with the
283:54 - ability to add a layer of data integrity
283:56 - to their database by creating a unique
283:59 - key policy when a container is created
284:02 - you ensure the uniqueness of one or more
284:04 - values per partition key and the unique
284:07 - key is scoped to a logical partition so
284:10 - if you partition the container based on
284:12 - the zip code you end up with duplicated
284:14 - items in each logical partition so in
284:17 - the example here you can see our
284:19 - partition key is going or sorry our
284:21 - unique key is based on the first name uh
284:24 - the address
284:25 - address zip okay so two different values
284:28 - there so because if you had first name
284:30 - there might be a bunch of andrews right
284:32 - and if you want to guarantee that
284:33 - there's only a single andrew you might
284:36 - scope it based on a postal code or zip
284:38 - code you can't update an existing
284:41 - container to use a different different
284:43 - unique key so you really do have to plan
284:45 - ahead to make sure you choose the right
284:46 - thing a unique key policy can have a
284:48 - maximum of 16 path values path values
284:51 - being
284:52 - [Music]
284:53 - i believe the separation of the comma a
284:55 - each unique key policy can have a
284:57 - maximum of 10 unique key constraints or
284:59 - combination
285:00 - combinations so
285:02 - there's that
285:04 - when a container has a unique key policy
285:06 - the ru's charged to the create update
285:08 - deletes an item are slightly higher and
285:10 - we haven't talked about our use yet but
285:12 - we will get to it in the course here so
285:14 - just hold on your keys aren't case
285:16 - sensitive so just consider that as well
285:18 - so there you go
285:23 - [Music]
285:24 - all right now it's time to talk about
285:26 - cosmos db containers because well
285:28 - containers we don't have really any
285:29 - compute to run these databases on and we
285:32 - do talk a bit about containers here so
285:34 - azure cosmos containers are useful for
285:36 - scalability in azure cosmos db both in
285:38 - terms of storage and throughput they are
285:41 - beneficial when you need a different set
285:43 - of configurations for each of your as
285:45 - azure cosmos dbs
285:47 - because they allow you to customize each
285:49 - container individually so azure cosmos
285:52 - containers have some container specific
285:54 - properties and those properties which
285:56 - can be system generated or user
285:58 - configurable vary according on the used
286:01 - api so the idea is when you first create
286:03 - a database
286:04 - um you actually have to create a
286:06 - container with it so notice here we'll
286:08 - create a new one and then we have a
286:10 - bunch of options here which
286:12 - that's what we're going to be talking
286:13 - about is all these options around
286:15 - containers
286:16 - so the properties of contain cosmos
286:18 - could db containers uh
286:20 - you know there are ones that are system
286:22 - defined properties and depending on
286:24 - which api you use some properties might
286:26 - not be directly exposed so uh the
286:29 - example of those system defined
286:31 - properties are here so you can see them
286:32 - underscored like underscore rid i'm sure
286:35 - etag underscore ts underscore self the
286:37 - id and so notice here that these are all
286:39 - system generated this is user
286:41 - configurable and it's going to vary
286:43 - based on these it looks like it's mostly
286:45 - just that sql api and then the ids on
286:48 - all of them there
286:49 - um but you know just to read through
286:51 - this quickly we have unique identifier
286:52 - for the container the entity tag used
286:55 - for optimistic
286:56 - concurrency control the last updated
286:59 - timestamp of the
287:00 - of the container the addressable ui for
287:03 - the container the user defined unique
287:05 - name for the container and there's more
287:07 - than these but these are the ones that i
287:09 - just picked out to show you here uh but
287:11 - next let's talk about capacity for
287:13 - containers okay
287:17 - [Music]
287:19 - so let's talk about capacity for
287:20 - containers but what is capacity capacity
287:23 - defines the amount of underlying
287:25 - resources that are available to support
287:27 - consumption of resources such as compute
287:29 - and storage i made that sound much
287:31 - fancier than it is but it's just what is
287:33 - available to you
287:34 - so like how much compute can i use how
287:36 - much storage can i use and cosmo cb has
287:39 - two capacity modes this is no different
287:41 - than i mean similar to
287:44 - a dynamodb but there are other ones
287:46 - called on-demand it's not called
287:48 - serverless but we have provision
287:49 - throughput and we have serverless and
287:51 - you're going to choose between those two
287:53 - options let's talk about what is
287:54 - different here so for provision
287:56 - throughput for each of your containers
287:58 - you provision some amount of throughput
288:00 - expressed in ru's request units per
288:03 - seconds and so for workloads where you
288:06 - you can reliably predict the traffic uh
288:09 - or you have more flexibility of your
288:12 - options that's where it makes
288:14 - sense to utilize it okay
288:16 - and for serverless this is where you can
288:18 - run your database operations against
288:20 - your containers without having to
288:21 - provision any capacity this is great for
288:24 - low or small workloads or for
288:26 - unpredictable spikes in traffics
288:29 - it's easy to configure but has some
288:30 - limitations so
288:32 - provision throughput makes sense at
288:33 - scale
288:34 - serverless makes sense when you are
288:36 - getting getting started or you're just
288:39 - finding that it's just easier to manage
288:41 - but you're going to really have to
288:42 - decide between those
288:44 - two options which is what we're going to
288:45 - look at here so just to kind of break
288:48 - down some additional options between
288:49 - these two
288:50 - uh when we're talking about geo
288:52 - distribution for project for provision
288:54 - throughput you can run it in unlimited
288:55 - regions where servers can only run in a
288:57 - single region the max storage per
288:59 - container is unlimited for provision
289:01 - throughput but for service it's 50
289:02 - gigabytes for performance we're looking
289:05 - at uh sub sub 10 millisecond latency for
289:08 - re point reads and rights covered by
289:10 - slas service level agreements
289:13 - uh point reads uh is under uh sub 30
289:17 - seconds for rights covered by slos so a
289:20 - little bit different there the building
289:21 - model is
289:22 - very different so for provision
289:24 - throughput building is done on a per
289:26 - hour basis for the ru's provision
289:28 - regardless of how many rus were consumed
289:30 - and serverless is billions done on a per
289:32 - hour basis for the amount of our use
289:33 - consumed by your database operations so
289:36 - in theory serverless can be more cost
289:39 - effective but it really just depends on
289:42 - what your consumption models look like
289:44 - we're not done with capacity we're going
289:45 - to look a little bit more at
289:48 - throughput okay
289:52 - [Music]
289:54 - so there's more to the story for
289:56 - provision throughput because you have uh
289:58 - different
289:59 - throughput strategies that you can
290:00 - choose when you create your database
290:02 - container and we said earlier when you
290:04 - create a database you always have to
290:06 - create a container with it and the two
290:07 - modes are dedicated and shared mode so
290:10 - for dedicated mode throughput is
290:12 - exclusive to this container and backed
290:13 - by slas
290:15 - for shared throughput
290:17 - or shared mode the throughput is shared
290:19 - across all your containers you cannot
290:21 - create throughput strategies after
290:22 - creation so you have to choose wisely
290:25 - and to switch throughput strategies you
290:27 - basically just have to create a new
290:28 - database and migrate that data over or
290:30 - sorry container and create migrate that
290:32 - data over
290:34 - so when you're creating your database
290:36 - and your container there's this little
290:37 - checkbox for shared throughput across
290:39 - containers that's how you know it's set
290:41 - for shared mode
290:42 - shared throughput is not available for
290:44 - capacity mode of serverless because it's
290:46 - all about provision throughput which
290:47 - makes sense so when should you share
290:50 - when should you not share so sharing the
290:52 - database level provision throughput
290:54 - among its containers
290:55 - is um anonymous i don't know why i chose
290:59 - that word it's such a hard word but that
291:00 - means to compare to compare things uh to
291:04 - hosting a database on a cluster of
291:06 - machines so because all containers
291:08 - within a database share the the
291:10 - resources available machine you
291:11 - naturally do not get predictable
291:12 - performance or any specific container
291:16 - but you get better utility or usage of
291:19 - your resources they're not going to
291:20 - waste so i think it's most like a cost
291:23 - like a cost strategy kind of thing
291:24 - because if you have a bunch of
291:25 - containers they're not being used you
291:27 - want it evenly distributed but
291:30 - yeah that's all there is about that
291:31 - feature okay
291:32 - [Music]
291:36 - okay let's talk about reading data from
291:39 - cosmos db there are two different ways
291:41 - to do this we have point reads and
291:44 - queries
291:46 - and this is a little bit confusing and i
291:48 - honestly don't know what always cost
291:49 - points and what don't
291:51 - but impracticality is not that bad but
291:53 - let's take a look here so point reads is
291:55 - a
291:56 - is a key value lookup on a single item
291:59 - id and partition key and queries
292:02 - allows you to return multiple items
292:05 - so let's break it down here so the idea
292:07 - is that when you are doing point reads
292:09 - you have a latency of about 10
292:12 - milliseconds queries it just varies so
292:15 - you know could be higher could be lower
292:17 - who knows
292:18 - uh in terms of the ru's that you're
292:21 - charged and i know we haven't talked
292:23 - about our use but that is the next video
292:25 - so just hold on or come back to this one
292:28 - to help materialize that information but
292:30 - for point reads it's always one
292:33 - request unit okay and so that is very
292:36 - predictable for queries it's 2.3 are
292:39 - used at least and it's going to really
292:41 - vary based on what your query is the
292:43 - number of items returned for a point
292:45 - read is one item queries is unlimited
292:48 - and the idea is that should you include
292:50 - the partition key for point reads it's
292:52 - required for queries it's recommended so
292:55 - hopefully that gives you an idea but
292:56 - those are your two options when reading
293:00 - and i think in the the follow along we
293:01 - will we'll look at these two types of
293:03 - read options okay
293:08 - [Music]
293:10 - all right we keep mentioning request
293:12 - units are used now it's time to actually
293:14 - describe what they are so the cost of
293:16 - all database operations is normalized by
293:18 - azure cosmos db and is expressed by
293:21 - request units so ru's abstract away
293:24 - memory cpu iops iop standing for input
293:28 - output per second
293:30 - and it's basically right sizing all
293:31 - these things for you mean like choosing
293:33 - the right size
293:34 - with a single value based on data
293:36 - processed so database operations uh
293:39 - include reads so the size of the item
293:42 - retrieve so one kilobyte would equal one
293:45 - ru 100 kilobytes would equal 10 ru's so
293:48 - we're just looking at the calculations
293:49 - for read operations for ru's
293:51 - an insert uh is inserting one kilobyte
293:55 - item without indexing cost so this is
293:56 - around 5.5 rus this is interesting
293:59 - because like dynamodb which is adabus's
294:01 - product they have um they do they have
294:04 - reads and writes so they're separate but
294:07 - cosmos db just has read units
294:10 - then there's updating so replacing item
294:11 - costs two times the charge required to
294:14 - insert the same item so there's some
294:16 - math there i don't deletes cost i
294:18 - couldn't find it for queries it's the
294:21 - size of the items retrieved so it's
294:23 - going to greatly vary because we saw
294:25 - earlier that you can have unlimited
294:27 - items retrieved so you're pretty darn
294:30 - expensive to calculate capacity
294:33 - it is very very confusing
294:36 - so the idea is that there is a capacity
294:38 - calculator and this is what you should
294:40 - use on the exam they're not going to ask
294:42 - you a bunch of math questions
294:44 - i point that out because
294:46 - on aws exams for identity b you have to
294:48 - know the math for this not so much but
294:51 - you have this sweet calculator that you
294:52 - can use to
294:53 - figure out the capacity that you'll need
294:55 - okay
294:59 - [Music]
295:00 - so a very very important concept when
295:02 - we're talking about fully managed
295:04 - databases especially nosql databases is
295:06 - consistency when am i going to see my
295:09 - data what am i when is it going to be
295:11 - consistent across
295:12 - all the servers that it's replicating
295:15 - data to
295:16 - so azure cosmos db provides five
295:19 - different consistency levels to maintain
295:22 - data availability and creating
295:23 - performance depending on your
295:25 - requirements and so we have this nice
295:27 - graph to kind of or graph um
295:30 - yeah graph to help visualize
295:33 - the five levels on the left hand side we
295:35 - have high latency lower availability the
295:39 - worst read scalability and on the right
295:42 - hand side we have lower latency
295:44 - a higher availability better read
295:46 - scalability so it's going to be based on
295:48 - what you need
295:50 - and so we'll start with the first one
295:51 - which is strong consistency level so
295:54 - this is
295:56 - linear realizability reads are
295:57 - guaranteed to return the most recent
295:59 - version of an item
296:01 - so strong means a guarantee that
296:04 - what what you read is what you get okay
296:07 - but the latency is going to be higher so
296:09 - it could be a bit slower for it to get
296:10 - back to you
296:11 - then you have boundless staleness so
296:14 - this is a consistent prefix it reads
296:17 - reads leg behind writes
296:19 - by most k prefixes or t intervals so
296:22 - just understand that it's a bit better
296:24 - than strong we have session so it has a
296:27 - consistent prefix it's uh monotonic
296:30 - reads monotonic rights
296:33 - read your rights write right follows
296:35 - reads okay i know this might not make
296:38 - sense but don't worry we're going to go
296:39 - through all of these still we're not
296:40 - done we'll go through the list and then
296:41 - we'll go in more detail on all these you
296:44 - have consistent prefix so updates
296:45 - returns are are some prefix of all
296:48 - updates with no gaps
296:50 - you have eventual so out of order read
296:52 - so
296:53 - you might read something and depending
296:55 - on what
296:56 - partition it hits or what
297:00 - a replica hits the data might not be all
297:03 - up to date
297:04 - and so the way you're going to set your
297:06 - consistency is you're going to choose it
297:07 - here and there'll be additional options
297:10 - this is honestly really confusing again
297:12 - i'm going to keep comparing aws aws
297:14 - dynamodb but it was done by db
297:17 - it's not it's complicated right or even
297:19 - like a bus s3 where it's just like it's
297:22 - it's eventual or it's consistent
297:24 - um so you can see there's a lot of
297:26 - options here but you know of that
297:27 - flexibility you have more choices i
297:29 - guess so let's talk about all these
297:31 - again so strong consistency read
297:33 - operations ensure that the most recent
297:35 - data is returned read operations cost
297:38 - the same as boundless staleness but more
297:40 - than session and eventual consistency
297:43 - write operations can only be read after
297:45 - the data has been replicated by most
297:47 - replicas when they're talking about
297:49 - costs i i guess they mean like the cost
297:51 - to retrieve information
297:53 - i'm not 100 sure
297:54 - again i didn't see on the exam about
297:56 - costs with consistency levels but uh
297:59 - this is the language that is being used
298:00 - in the documentation so that's why i'm
298:02 - bringing it over for boundless staleness
298:04 - read operations are like behind write
298:06 - operations due to time or version
298:07 - differences read operation costs the
298:10 - same as strong consistency and more than
298:12 - session eventual consistency has the
298:14 - highest level of consistency compared to
298:16 - session consistency consistent prefix
298:18 - consistency and eventual consistency
298:20 - recommended for globally distributed
298:22 - applications with high availability and
298:25 - low latency so remember it's for
298:27 - globally distributed applications
298:30 - we have sessions so read operations uh
298:32 - ensure that written data is consistent
298:34 - with the same session consistency is
298:37 - scoped to a user session while other
298:39 - users may encounter dirty data if
298:41 - another session has just written to it
298:44 - uh it's the default consistency
298:46 - consistency level used to uh when using
298:49 - a newly created database reading costs
298:51 - are lower than boundless staleness and
298:53 - strong consistency but higher than
298:55 - eventual consistency
298:57 - we have consistent prefix so read
298:59 - operations ensure that the most recent
299:01 - data replicated among the replicas is
299:04 - returned but it does not guarantee the
299:06 - data is the most recent
299:08 - uh the idea is we have dirty data occurs
299:10 - when one replica changes the data state
299:13 - but this data has not yet been
299:14 - replicated they keep using the word
299:16 - dirty just meaning like
299:17 - when you replicate when you have that
299:19 - piece of data and it's stored in
299:21 - multiple places
299:22 - redundantly some of them are dirty so
299:25 - they're not all up to date
299:27 - has a stronger consistency level of
299:29 - eventual consistency but less than any
299:31 - other then we have eventual read
299:32 - operations does not guarantee any
299:34 - consistency level lowest consistency
299:36 - level low lowest latency and best
299:39 - performance among consistency levels so
299:40 - things are really fast least expensive
299:43 - read operation cost compared to any
299:45 - other consistency levels so there you go
299:48 - there's all the levels um you know
299:50 - hopefully that makes sense again it's
299:52 - very confusing but there you go
299:54 - [Music]
299:59 - let's talk about a very useful feature
300:00 - in cosmos db known as change feed so
300:03 - change feed
300:04 - is a service that monitors changes in
300:07 - all containers and distributes events
300:09 - triggered by those changes to multiple
300:11 - consumers so change feed in azure cosmos
300:14 - db is a persistent record of changes to
300:16 - container in in the order that they
300:18 - occur
300:19 - and the utility of this
300:21 - like this is not unique
300:23 - to a cosmos db a lot of databases might
300:26 - have like extensions or modules
300:29 - dynamodb has something similar called
300:31 - dynamodb streams but the idea is that
300:34 - you have a data source and you have an
300:36 - insert and update and the idea is that
300:38 - that record
300:40 - goes into the change feed and then you
300:42 - can react to that chain to that record
300:44 - and push it to other services and so the
300:47 - idea is just it's like a way of
300:48 - triggering something on an insert so you
300:50 - say okay i insert this data
300:52 - um and so then send this data over to
300:55 - this other service
300:56 - so it's just a way of triggering stuff
300:58 - okay azure has an sdk for net java
301:01 - python no js nothing for ruby they're
301:04 - having lots of trouble with the ruby sdk
301:06 - at some point they'll fix it
301:08 - the change feed is supported for sql api
301:10 - cassandra api db api gremlin api
301:13 - basically all the apis the exception of
301:15 - table api
301:17 - so that's that in terms of its
301:19 - implementation the change feed processor
301:21 - is composed of four components we have
301:23 - the monitor container this is where any
301:25 - insert or update executes the operations
301:28 - are
301:28 - reflected in the change feed i can get
301:30 - my pen out here so if we can see where
301:32 - it is so the monitor
301:33 - the container i guess collections are up
301:35 - here um
301:37 - we have lease containers see they're
301:38 - saying collections because the
301:40 - collection um remember a collection is a
301:43 - mapping to a container so collection
301:44 - container we're kind of just
301:46 - interchanging right here
301:47 - but it stores the state and coordinates
301:49 - the change feed
301:51 - processor which is down here
301:53 - the host an application instance that
301:55 - uses the change feed processor to listen
301:57 - for changes so that's the way it's it
301:59 - knows what to do
302:01 - um the delegate the code that runs when
302:03 - an event in the change feed notification
302:05 - triggers it and so that's the code
302:07 - within the consumer there the change
302:10 - feed processor may be hosted among azure
302:12 - services that support long-running tasks
302:14 - such as azure web jobs azure virtual
302:17 - machines azure kubernetes services
302:19 - azure.net hosted services
302:22 - so
302:23 - we could have looked at the code it's a
302:25 - bunch of c-sharp code that they have as
302:27 - examples it's super not exciting
302:29 - um but you know i just want you to know
302:31 - about this feature
302:33 - but again it's not unique to cosmos db
302:35 - but it is a common thing that you should
302:36 - know about
302:38 - for these kind of fully managed
302:39 - databases okay
302:41 - [Music]
302:45 - hey this is andrew brown from exam pro
302:47 - and we are looking at azure storage
302:48 - accounts which is used for containing
302:50 - all your azure storage data objects such
302:52 - as blobs files queues tables and disks
302:55 - so storage accounts is one of those
302:57 - services where it has a bunch of
302:58 - different storage types within it so
303:00 - it's a quite the multi-purpose service
303:04 - and with each different type of storage
303:06 - it's going to have different features
303:07 - and their own pricing models let's just
303:09 - quickly break down the type of storage
303:11 - we have so we have general purpose
303:14 - version 1
303:15 - version 2 blob storage block blob
303:18 - storage and file storage and so i just
303:21 - want you to know that when you are using
303:23 - storage accounts
303:25 - i'm saying storage type but for whatever
303:27 - reason the ui calls it account kind just
303:30 - to be aware of that small little
303:31 - discrepancy there
303:33 - but storage accounts are going to vary
303:35 - based on features and so the common
303:36 - features we'll see across storage will
303:39 - be
303:40 - supported services so where can i put
303:42 - the storage account
303:43 - and so on the right hand side you can
303:45 - see
303:46 - like if you make a general purpose
303:47 - version 2 what you have access to such
303:50 - as containers cues tables or falsers
303:53 - those options are going to change based
303:54 - on what you're using all right
303:57 - for performance tiers this is how fast
303:59 - you'll be able to do reads and writes
304:02 - you have standard and premium uh you
304:04 - have different kinds of access to yours
304:06 - so how often do i need to quick access
304:09 - these files and then there's replication
304:11 - how many redundant copies should be made
304:14 - and where and the last thing is
304:15 - deployment models so who should deploy
304:18 - the supported service resource manager
304:20 - classic and in generally most cases it's
304:22 - going to be a resource manager
304:24 - [Music]
304:29 - so here i have all the storage types on
304:31 - the left-hand side and we have the
304:34 - feature set and how it's going to vary
304:36 - based on the certain types let's just
304:37 - quickly look through this to see where
304:39 - there are some standouts and you might
304:42 - want to actually review this at the end
304:44 - of this section just so uh you're a bit
304:46 - more familiar with all these types of
304:48 - features so it clicks a bit better but
304:50 - let's just quickly go through this so
304:51 - you're gonna notice for version one this
304:53 - is the only case where you're gonna have
304:54 - a deployment model of classic everything
304:56 - else is gonna be the resource manager
304:58 - from a practical standpoint you're not
305:00 - going to really notice because you're
305:01 - just going to be pressing buttons but
305:03 - underneath
305:04 - that's the only case where it varies for
305:06 - replication you're going to notice that
305:08 - version 2 has the most options with
305:10 - replication and if these don't make
305:12 - sense don't worry we're going to cover
305:13 - all the replication in an upcoming slide
305:16 - for
305:17 - blob block storage you can see it's very
305:19 - limited same with file storage
305:23 - so that's there when we're talking about
305:25 - access tiers how quickly you can access
305:27 - files you're going to notice that it's
305:28 - only available for general version 2 and
305:31 - blob storage where we're choosing these
305:33 - different tiers for these ones it
305:35 - doesn't really matter
305:36 - um because well especially like a file
305:38 - storage because the drive is as fast as
305:40 - it's going to be right
305:42 - for performance tiers you're going to
305:44 - notice that with version 1 and version 2
305:46 - we have standard and premium when you're
305:48 - using file storage and block blob
305:50 - storage you're always using premium and
305:52 - with blob storage which is again a
305:54 - legacy format there that's going to be
305:56 - using standard
305:57 - uh blob storage comes in three different
306:00 - types and you're going to notice that uh
306:01 - based on what you want to use there'll
306:03 - be some variation there
306:04 - um i don't know where page is i think
306:08 - that they both support page but there
306:10 - are three types in there i wouldn't
306:11 - really worry about it too much uh you're
306:13 - gonna notice that file storage only
306:14 - supports file types and then you have
306:17 - version two and this pretty much uh
306:18 - supports everything so you can see
306:20 - general purpose version two is a really
306:21 - great
306:22 - um
306:23 - storage to choose
306:26 - [Music]
306:30 - so we were just talking about storage
306:31 - types now let's talk about the actual
306:33 - storage services that we can actually
306:35 - launch
306:36 - or utilize within these storage types
306:39 - under storage accounts and there are
306:41 - five core storages available to us the
306:43 - first is azure blob this is a massively
306:45 - scalable object store for text and
306:47 - binary data it also includes big data
306:50 - analytics through data lake storage gen
306:53 - 2.
306:54 - um and so azure blob is really great
306:57 - because you just don't have to think
306:58 - about the file system you just upload
307:00 - files and they're treated like objects
307:01 - so well that's really nice then you have
307:03 - azure files and this is a file share
307:06 - and so
307:07 - the great thing about azure files is
307:09 - that if you want to have a bunch of
307:11 - virtual machines
307:12 - and have it so like they have um the
307:15 - same file system sharing all the same
307:16 - files that's what you're going to use
307:18 - that for then you have azure queues to
307:20 - me this really is a database but for
307:22 - whatever reason it's under uh storage
307:24 - accounts and it's a nosql store for
307:25 - schema-less storage of structured data
307:27 - this is another unusual one uh i don't
307:30 - know why azure puts it under here but
307:32 - it's a messaging store for reliable
307:34 - messaging uh between application
307:35 - components to me that's like an
307:37 - application integration service but
307:39 - azure categorizes it as storage and the
307:41 - last one is azure disk and this is block
307:43 - level storage volumes for azure vms and
307:47 - so the idea is that when you want to do
307:48 - the top four the above you're going to
307:50 - be launching storage accounts
307:52 - and for disks you're going to be
307:54 - launching disks it's a bit unusual
307:56 - because um one of the storage accounts
307:58 - say that you can
308:00 - use or store disks uh in i think version
308:03 - or uh general purpose version two i'm
308:05 - not really sure i understand that uh
308:08 - maybe it's talking about like backing
308:09 - them up or something like that but
308:10 - anyway from practical standpoint and for
308:13 - what we need to know is that this is the
308:14 - breakdown here and we do have a full
308:16 - section on azure disks so we will get
308:19 - into that
308:20 - [Music]
308:24 - so now let's take a look at some of the
308:26 - features that are available on account
308:28 - storage the first being performance
308:30 - tiers and generally this is going to be
308:31 - for blob storage and we have two types
308:33 - of performance tiers we have standard
308:35 - and premium so it's as simple as uh just
308:37 - choosing between the two
308:39 - and when we're talking about performance
308:40 - especially when we're talking about
308:41 - storage we want to be thinking about
308:43 - iops and that stands for input output
308:44 - operations per second so the higher the
308:47 - iops the faster a drive can read and
308:49 - write so you can uh definitely assume
308:51 - that premium is going to have
308:53 - a higher amount of iops and so when
308:56 - we're looking at the premium performance
308:57 - these are going to be stored on solid
308:59 - state drives ssds which we have in the
309:01 - picture there it's optimized for low
309:03 - latency higher throughput and the use
309:06 - cases here is going to be interactive
309:07 - workloads analytics
309:09 - ai or ml and data transformation on the
309:12 - other side for standard performance
309:14 - these are running on hard disk drives
309:17 - and you're going to have varied
309:17 - performance based on your access tier
309:19 - and we're going to talk about access
309:20 - tiers very shortly
309:22 - but the tiers would be hot cool and
309:24 - archive and this is great for backup and
309:27 - disaster recovery media content bulk
309:30 - data processing
309:32 - and things like that
309:33 - and so the reason why ssds are
309:36 - generally really really good for
309:38 - premium performance is because they have
309:40 - no moving parts within them and the data
309:42 - is distributed ram uh randomly so if you
309:44 - have to do a read and write the distance
309:46 - for between the read and writes are
309:47 - going to be a lot faster
309:49 - uh and that's generally why you're going
309:51 - to see solid sacrifice with premium
309:52 - performance or things with higher iops
309:55 - and then for
309:57 - hard disk drives it does have moving
310:00 - parts so you see that it has an arm and
310:02 - that arm needs to read and write data
310:03 - sequential sequentially to the disk
310:06 - and so it's very good at writing or
310:08 - reading large amounts of data that is
310:10 - close together that is sequential
310:13 - but you know the idea is that neither uh
310:15 - format is good or bad it's just the the
310:18 - use case that you need so you don't
310:20 - always need to go with ssd sometimes you
310:21 - want to save money and hhd's are
310:25 - or hdds are really good for that
310:28 - [Music]
310:32 - let's take a look at access tiers and
310:33 - there are three types of tiers for
310:35 - standard storage we have cool hot and
310:37 - archive and so you're going to have this
310:39 - option between
310:40 - cool and hot and archived archive might
310:43 - not show up depending on how you
310:44 - configure your storage account
310:46 - so for example if you use a particular
310:48 - type of replication that might not be
310:50 - available
310:52 - to use with archives so just be aware if
310:53 - that option doesn't show up you might
310:55 - have to change some of your settings
310:56 - let's quickly walk through the three
310:57 - options so hot is for data that's
310:59 - accessed frequently and has the highest
311:01 - storage costs and lowest access costs so
311:04 - again it's for data that is frequently
311:06 - accessed or in active use or data that's
311:09 - staged for processing and eventual
311:10 - migration to the cool access tier then
311:12 - you have the cool tier this is data
311:14 - that's in frequently accessed and stored
311:16 - for at least 30 days lower storage costs
311:19 - higher access cost
311:21 - and the use case here is for short-term
311:22 - backup and disaster recovery data sets
311:24 - or older media content not viewed
311:26 - frequently anymore but is expected to be
311:28 - available immediately when accessed
311:31 - and i think there's actually a third use
311:32 - case which is large data sets that need
311:34 - to be stored cost effectively while more
311:36 - data is being gathered for future
311:37 - processing then we have the archive tier
311:40 - this is for data that's rarely accessed
311:42 - and stored for at least 180 days
311:45 - it's the lowest storage cost but it also
311:47 - has the highest access cost
311:50 - and so its use case would be long-term
311:51 - backups secondary backup archival data
311:54 - sets original raw data that must be
311:55 - preserved even after it's been processed
311:57 - into its usable form
311:59 - and compliance and archival data that
312:01 - needs to be stored for a long time and
312:02 - is hardly ever accessed so just make use
312:05 - or note of the cool is at the least 30
312:07 - days and the archive is at least 180
312:10 - days i should have highlighted those for
312:11 - you i don't know why i didn't
312:13 - and so just some other things you need
312:14 - to know so for account level tiering any
312:16 - blob that doesn't have an explicitly
312:18 - assigned tier infers the tier from the
312:20 - storage account access to your settings
312:22 - so you can set blobs at different
312:25 - storage levels
312:27 - blob level tearing you can upload a blob
312:29 - to the tier of your choice and change uh
312:31 - change the tiers happen instantly with
312:33 - exception from moving out of archive
312:36 - the concept of rehydrating this is when
312:38 - you're moving a blob out of archive into
312:40 - another tier and can take several hours
312:42 - uh you have the blob lifecycle
312:44 - management so you can create rule-based
312:46 - policies or transition your data to
312:48 - different tiers so after 30 days we can
312:50 - move it to cool storage and here is just
312:52 - the option here you see where it says
312:53 - 30. i think 30 is the minimum days you
312:56 - have to choose um but it could be wrong
312:58 - and so you have the options down below
313:00 - so move to cool storage move to archive
313:02 - storage delete the blob
313:05 - and just a few other things here when a
313:06 - blob is uploaded or moved to another
313:08 - tier it's it's charged at the new tiers
313:11 - rate immediately upon the tier chains so
313:13 - when you're moving to a cold a cooler
313:15 - tier the operation is billed as a write
313:17 - operation to the destination tier and
313:20 - when the right operation per 10 000 and
313:22 - data right per gigabyte
313:24 - charges for the destination tier applies
313:28 - when moving to a hotter tier the
313:29 - operations billed as a read from the
313:31 - source tier and where the read operation
313:34 - again in the 10k and the data retrieval
313:36 - again in gigabytes charges for the
313:39 - source tier applies and early detection
313:41 - charges for any blob moved out of the
313:43 - cool or archived tier may apply as well
313:47 - and just lastly here uh when we're
313:49 - talking about cool and archive early
313:50 - detection any blob that has moved into
313:53 - the cool tier so this is only for
313:55 - general purpose version two accounts is
313:57 - subject to a cool early detection period
313:59 - of 30 days and any blob that is moved
314:02 - into the archive tier is subject to an
314:04 - archive early detection of 180 days and
314:07 - this charge is pro-rated so
314:09 - you know access tiers aren't the funnest
314:11 - thing to talk about but there is all the
314:12 - information you need to know
314:14 - [Music]
314:18 - let's take a closer look at az copy and
314:20 - this is a command line utility that you
314:22 - can use to copy blobber files to or from
314:25 - a storage account so the first thing
314:26 - you're going to need is the executable
314:28 - file and there you can see they have it
314:30 - for windows linux and mac and then once
314:32 - you download that file what you're going
314:34 - to have to do is also make sure that you
314:37 - have the right level of authorization
314:39 - with for the user account they're using
314:40 - with it so you might need to for
314:43 - download you'll need storage blob data
314:45 - reader for upload storage blob data
314:48 - contributor and storage blob data owner
314:51 - so just be aware that you need to have
314:53 - those uh rules available to you
314:56 - to your user account i think that i have
314:58 - access to everything so i don't think i
314:59 - even i set this it just works but if
315:02 - you're in a larger company and you have
315:04 - more permissive least permissive roles
315:06 - you just need to know about that
315:09 - and so you can gain access
315:11 - via either the azure active directory or
315:14 - a shared access signature so let's just
315:17 - take a look at that right now so the
315:18 - idea here is that we'll type in a z copy
315:20 - login and this is going to ask us
315:22 - whether we want to sign into the web
315:25 - browser so that is uh that should be
315:26 - azure active directory option one and so
315:29 - what you'll do is you'll enter your
315:31 - username and password and you'll then
315:32 - have to enter the code displayed there
315:35 - and so now you're ready to use uh the um
315:38 - the cli and so all you have to do is
315:40 - type in az copy copy and then you have
315:43 - the file and then you give it the
315:45 - endpoint to the storage account uh and
315:48 - the container and the location you want
315:50 - it to go in if you want to download
315:51 - files it's the same command you just
315:53 - reverse the order you say uh this is the
315:55 - the location of the file and want to
315:57 - download it locally
315:59 - all right
316:01 - hey this is andrew brown from exam pro
316:02 - and we're taking a look at lifecycle
316:04 - management which offers rule-based
316:05 - policies that you can use to transition
316:07 - blob data to the appropriate access
316:09 - tiers or to expire data at the end of
316:11 - data lifecycle so
316:13 - what you can do here is transition blobs
316:15 - from cool to hot immediately when they
316:16 - are accessing access to optimize for
316:19 - performance transition blob blob
316:20 - versions and blob snapshots to a cooler
316:22 - storage tier if the objects have not
316:24 - been accessed or modified
316:26 - for a period time to optimize for costs
316:29 - delete blob blob versions and blob
316:31 - snapshots at the end of their life
316:33 - cycles to find rules to be run once per
316:36 - day at the storage account level apply
316:38 - rules to containers or a subset of blobs
316:40 - using prefixes or blob index tags as
316:42 - filters so here's an example of adding a
316:44 - rule and one few things here you might
316:47 - want to read here is the fact that you
316:49 - can apply the rules to all your blob or
316:52 - all blobs in your storage account or
316:53 - limit based on a filter you can choose
316:55 - the blob type or the blob subtype
316:58 - and then there's the actual rules
317:00 - themselves so here you can see if last
317:02 - modified by one day then delete the blob
317:05 - if last modified in the two days then
317:07 - move to cool storage so it gives you
317:08 - kind of an idea of things you can do
317:10 - there okay
317:11 - [Music]
317:20 - in some cases you'll want to be able to
317:22 - retrieve properties and metadata or set
317:24 - meta data for storage accounts so let's
317:26 - take a look at some of the ways we do
317:28 - that with the cli the first is for
317:30 - container properties and so
317:32 - the idea here is we have azure container
317:35 - containers show so here's an example
317:37 - where it is showing container show
317:38 - information so that's
317:40 - the property information here
317:43 - and then let's say we want to get we
317:45 - want to update that data then we would
317:47 - just use the update command
317:49 - and then the idea here is we can pass
317:52 - along data that we want to change
317:54 - then there's also the idea of showing so
317:56 - here we're just showing that metadata on
317:59 - that container
318:00 - i have a little bit more here i suppose
318:02 - so we can also
318:05 - update metadata on a blob so very
318:07 - similar process but that's for a blob
318:09 - and as you can imagine there is a show
318:11 - command as well so there you go
318:13 - [Music]
318:17 - hey this is andrew brown from exam pro
318:19 - and this follow along we're going to
318:20 - learn a bit more about storage accounts
318:21 - and how to copy uh content back and
318:24 - forth using az copy so what i want you
318:26 - to do is go all the way to the top here
318:28 - and we'll type in storage accounts
318:31 - and then from here we're going to create
318:33 - ourselves a new storage account
318:36 - and we'll make a new resource group i'm
318:38 - going to call this one fajo
318:42 - we'll type in fajo here
318:44 - and we can have it in any location that
318:46 - we so desire we're going to stick with
318:48 - standard here general purpose 2 seems
318:50 - okay to me we'll go ahead and hit review
318:52 - and create
318:55 - and then we'll hit create there
319:00 - and i'll see you back here in a moment
319:02 - all right so now that our storage
319:04 - account is ready here we'll go in here
319:06 - and we'll create ourselves a container
319:08 - so i'm going to call this one kievas
319:10 - faggo we're going to leave it as private
319:12 - because we technically don't want anyone
319:14 - to have public access to it
319:16 - and so there is a tool that
319:18 - you should definitely know how to use
319:19 - called a z copy so if i just type that
319:22 - in there quickly here and we go and
319:24 - scroll down here are some versions we
319:26 - can download
319:27 - so if you're on mac linux windows or etc
319:29 - we can use it i want to try to give you
319:32 - a consistent experience on how you can
319:33 - utilize this so
319:35 - i'm going to just try and stumble my
319:37 - throat way through and use it via the
319:39 - cloud shell because i think that would
319:41 - probably be the most consistent way for
319:42 - all of us to use it so make sure you are
319:44 - in bash and we're going to need two
319:46 - things you're going to need this az copy
319:48 - tar file and we're also going to need
319:50 - some kind of file to upload okay
319:53 - so
319:54 - the question is how are we going to get
319:55 - things into our cloud shell well the
319:57 - great thing about cloud shells when you
319:59 - launch it it asks you to create a
320:00 - storage account and we should probably
320:02 - be able to place those files into there
320:04 - so if we go back to storage accounts at
320:05 - the top here
320:08 - and uh we go here this is a storage
320:11 - account so if we go in here if there
320:12 - were containers we go back but i believe
320:15 - what they do is they set you actually up
320:16 - a file share here right
320:19 - and so in here it this is what it's
320:21 - doing so what i'm going to do is upload
320:24 - two files so i'll just go here
320:27 - and one is going to be
320:29 - this which is the kivas fajo which is
320:31 - just an image
320:33 - okay
320:35 - and what we can do is i will download
320:38 - this linux here
320:40 - and once that is downloaded what i'm
320:42 - going to do is also
320:44 - upload that okay so i just drag that to
320:47 - on to my desktop
320:48 - it's a bit easier to access
320:50 - i might just rename it so it's a bit
320:52 - easier to work with
320:54 - so i'm just going to rename the uh the
320:56 - tars gz to a copy here if you can see
321:00 - and uh we'll go back here and we'll just
321:02 - go ahead and upload that okay
321:04 - so we will upload that as well
321:13 - and it's only 11 megabytes so it
321:15 - shouldn't take too long here
321:17 - but what we need to do is find out where
321:19 - these files are so what we'll do is make
321:20 - our way back to our shell i'm going to
321:22 - type in ls
321:24 - and i think it might be mounted here so
321:25 - we'll do ls and there's our file so
321:28 - um what we'll have to do is untie the az
321:31 - copy so we get the binary
321:33 - so i have a handy dandy command over
321:36 - here from
321:38 - i just found one over here because i can
321:40 - never remember this command the tar
321:41 - command so we'll type in tar hyphen x
321:45 - v
321:47 - z f
321:48 - and then the name of the tar so i'm
321:50 - going to start typing a z and then hit
321:52 - tab to auto complete
321:54 - and that should uh unturr it
321:57 - and so now we have our
322:01 - oh we made it into a folder so i'm going
322:02 - to go into that folder there
322:05 - and so now we have our easy copy here
322:06 - okay
322:07 - um so just to make our lives a bit
322:09 - easier i'm going to want to move that
322:11 - easy copy back one directory so i'm
322:13 - going to type in mv for move i'm going
322:15 - to type in az copy and i'm going to do
322:17 - period period backslash so it says go or
322:20 - forward slash you'll go back a directory
322:22 - if we cd and do an ls
322:24 - just do clean again we'll do ls
322:26 - we can now see that it's a bit easier to
322:28 - work with i'm going to delete the
322:30 - that long folder there because we don't
322:32 - need it
322:34 - when it's a folder you got to do a
322:35 - recursive so hyphen r
322:38 - and we'll also go ahead and delete the
322:39 - azcopy.tar file
322:43 - okay
322:44 - and we'll do uh clear
322:47 - and then we'll do an ls hyphen la
322:48 - that'll list everything because what i'm
322:50 - trying to see is whether this file is
322:51 - executable appears that uh we can
322:53 - execute it if we can't it's usually a
322:55 - good habit to do uh chamod u plus x
322:58 - a z copy here
323:00 - and then if we run this again
323:02 - um yeah it looks the same so i think
323:04 - we'll be in okay shape here but uh yeah
323:06 - now we're all set up to um go ahead and
323:09 - use the az copy command so let's give it
323:11 - a go
323:12 - all right so there's two ways we can get
323:14 - um
323:15 - or authenticate with easy copy and if
323:17 - you remember from my handy dandy az copy
323:20 - slide
323:21 - we have two options which is we can use
323:22 - this login or use the shared access
323:24 - signature we're going to give both a try
323:26 - but we'll start out with azlogin okay so
323:28 - we'll do type in az copy
323:31 - login
323:32 - and what that will do is it will
323:34 - prompt us and ask us to sign into the
323:36 - browser and enter this code so i'm just
323:38 - going to
323:38 - grab this link here and we will copy it
323:41 - i'll make a new tab paste it in
323:44 - and once here we'll go ahead and enter
323:45 - in this code okay
323:52 - and we'll just say yep andrew brown and
323:54 - so now that is now synced so now we
323:56 - should be able to use the az copy
323:58 - command uh in order to use this again we
324:00 - pull up my reference here we have a z
324:03 - copy copy the name of the file and then
324:05 - the name of the path we want wanted to
324:07 - go to okay
324:08 - so what we'll have to do is uh in our
324:11 - container here it's called key vas fajo
324:13 - so we'll have to remember that but if we
324:15 - go here we're looking for that publicly
324:16 - accessible url or that fully qualified
324:18 - domain url
324:20 - um i would have swore it was
324:23 - here
324:25 - um
324:26 - oh sorry we're in the wrong storage
324:28 - account by the way we have to go into
324:30 - our actual
324:32 - faggio one here just in case
324:34 - just making sure we're in the right
324:35 - place but let me just go find that url i
324:36 - can't remember if we have to assemble it
324:37 - by hand or if they just have it handy
324:39 - here so just give me a moment
324:42 - all right i'm back and i had to pull up
324:44 - the docs for this one which is not a big
324:45 - deal um but
324:47 - the structure here is we're going to
324:48 - have https the account blob or
324:50 - windows.net the name of the container
324:52 - and the path to the blob okay
324:54 - so that's what we're going to do here so
324:57 - let's give it a go and see if we can get
324:58 - it to work the first time around here so
325:01 - we are typing well before we do that i'm
325:02 - going to type clear and then do ls so
325:05 - we're going to type in a z copy
325:07 - and the command is cp
325:11 - or actually you can type in copy if you
325:13 - want it looks like cp is abbreviation
325:15 - and so the path to the file and so in
325:17 - our case it's going to just be local so
325:18 - i just typed in k and i tabbed and the
325:21 - next thing we have to type is https
325:23 - colon forward slash forward slash the
325:25 - name of the account so it's fajo
325:28 - and then blob
325:29 - dot core
325:31 - dot windows
325:32 - dot net forward slash and then we named
325:34 - the uh container here kivas
325:38 - fajo
325:39 - and then we'll type a forward slash
325:42 - and then we can name the file however we
325:43 - want so i'm going to name it the same as
325:46 - the jpeg okay
325:47 - and so hopefully this just works
325:50 - and
325:52 - i mean it looks like it worked pending
325:54 - says the file transfer might have failed
325:56 - here hold on
325:59 - so it says we're not performed to other
326:02 - we don't have access to perform that
326:03 - there
326:07 - so just give me a moment okay
326:09 - all right so i figured it out um there
326:11 - was one key thing we forgot to do so
326:14 - the reason we're getting a 403 was as
326:16 - plain as it was is we didn't have access
326:18 - to do it so if we just go back to our
326:21 - storage account there
326:22 - um
326:24 - and this is within the container so i'm
326:25 - just going to go all the way back to
326:26 - storage accounts we'll go into fajo here
326:30 - and so on the left hand side you see it
326:31 - says access controls
326:33 - right so if we were to go into here and
326:36 - we go to role assignments we can see
326:39 - what what role that people have but in
326:41 - order to access
326:43 - blob storage you need to like to be able
326:45 - to upload and download you either have
326:46 - to be the storage blob data owner or the
326:50 - storage blob data contributor so if you
326:52 - want to do that you just have to go up
326:53 - to add and do a role assignment and sign
326:56 - yourself this role
326:58 - it does take about five minutes to take
327:00 - effect so even after doing at doing this
327:03 - adding it to my user here i wasn't able
327:05 - to do it right away but after waiting a
327:08 - little bit here i was able to go and do
327:10 - a test like with list
327:12 - and it didn't print anything but that
327:13 - means that there is no error right so
327:16 - we probably do have access another
327:18 - little thing i found out which was nice
327:20 - is that if you go let's say into a
327:22 - container here
327:24 - um and then you just clear that out
327:26 - we'll click into that container and on
327:28 - properties here it actually gives us the
327:30 - url so it makes our lives a little bit
327:31 - easier so we have to type that whole
327:33 - thing out and try to figure it out but
327:35 - let's go back and try to actually do a
327:37 - copy now so that's what we've been
327:39 - trying to do this entire time so what
327:41 - i'm going to do is type in az copy
327:44 - and then we want uh
327:46 - kivasvagio
327:48 - and then i'm going to paste in the url
327:49 - that i just got from there
327:51 - and
327:51 - [Music]
327:52 - i'm just going to put a forward slash
327:54 - here i'm going to type in kivas
327:55 - faggot.jpg
328:00 - um
328:02 - oh you know what we got to put the word
328:03 - copy or cp in front of it
328:08 - and it looks like it completed that
328:10 - looks good to me and so if we make our
328:11 - way back
328:13 - here to
328:15 - um to our container we can see that it's
328:16 - uploaded so there we go we were able to
328:19 - upload it with um active directory so
328:21 - now let's go give it a try with a sas
328:24 - okay
328:25 - all right so let's give this another go
328:26 - here and this time what we're going to
328:28 - do is use an sas
328:30 - so it's as simple as attaching it to the
328:31 - end of the url for the the destination
328:34 - or the source depending on what we want
328:35 - to do
328:36 - but what i'm going to do is i'm going to
328:37 - go ahead and delete this file here
328:39 - just because we already have it and what
328:43 - i'm going to do is go back to my storage
328:45 - account
328:46 - and what we need is a sas here so i'll
328:48 - type it in here i wonder if we can type
328:50 - it the oh yeah we can here that's great
328:52 - so type it here at the shared signature
328:53 - access and we'll say what do we need
328:55 - access to well we're only using blob
328:57 - storage so we get rid of file queue and
328:59 - table
329:00 - allowed resource types will be for
329:02 - container because that's what we're
329:03 - doing
329:04 - uh and we aren't deleting we
329:07 - don't necessarily need a list but we can
329:08 - have that there read write add create
329:10 - that seems okay to me enable deletion of
329:12 - versions um
329:15 - i mean we don't have to turn that off um
329:17 - but
329:18 - we'll just leave it in place
329:20 - i think we will do http or https just
329:23 - have some flexibility here
329:24 - and this looks okay we have a few
329:26 - different keys we're gonna stick with
329:27 - key one and so i'm gonna go ahead and
329:29 - generate out
329:30 - an sas connection string
329:32 - and so this is the token we want see how
329:34 - it has a question mark it's going to
329:36 - help allow us it's going to be creasing
329:38 - allow us to put that on the end of our
329:39 - url
329:40 - so what i will do
329:43 - is i will i don't know if we can write a
329:45 - z
329:46 - copy log out let's see if we can do that
329:48 - oh we can great so just making sure that
329:50 - it's not authenticating that way
329:52 - and so what i'll do is i'll go back up
329:54 - to uh this link here and this should
329:56 - fail right because we don't have access
329:58 - right and so what we should be able to
330:00 - do is just take this string and place
330:02 - this on the end as such and hopefully
330:05 - just works
330:06 - and it says done done done so if we go
330:08 - back to
330:10 - our storage here to see if that is uh
330:14 - working as expected we go into that
330:15 - container
330:17 - um
330:18 - [Music]
330:20 - let's just move this down below here it
330:22 - gets a bit hard to see when you have
330:23 - that open like that
330:25 - over in storage account right now okay
330:26 - so we're going to type in container go
330:28 - here
330:29 - click into
330:30 - uh
330:31 - devos fagio and we don't see the file so
330:35 - let me give it let me give a try here
330:36 - okay
330:37 - all right let's give it another go here
330:39 - um
330:40 - you know i just don't necessarily trust
330:42 - url actually it says fail to perform
330:43 - copy no sas token or oauth provided
330:47 - which is fine but in this case we did
330:48 - and we saw it done done done and so here
330:50 - are the values it's printing out so
330:52 - maybe what we should do is just
330:53 - regenerate this out
330:56 - and be a little bit more uh permissive
330:59 - um and maybe that will give us an easier
331:01 - time because we want read oh this is
331:03 - slightly different a for the container
331:05 - not sure why this looks different all of
331:07 - a sudden but what we'll do is we'll go
331:08 - back here
331:10 - we'll just go close a bit we'll go back
331:12 - into fagio i'm going to go look for the
331:14 - container again
331:18 - and what we'll do is click into the
331:19 - container and then from here we'll do
331:21 - shared access properties
331:23 - um
331:24 - we'll do account key which is fine so
331:26 - key one
331:27 - i want i'm just going to put everything
331:29 - on here to make our lives a bit easier
331:32 - the start date is fine the end date is
331:33 - fine
331:35 - will allow http or https
331:38 - and we'll generate that token so
331:40 - here we have it again
331:44 - so we'll do is we'll just grab this
331:46 - whole thing here
331:48 - actually we probably yeah we'll just
331:50 - grab the token
331:53 - um and then what we'll do is make our
331:55 - way back to
331:57 - here
331:58 - i'm going to hit up
332:00 - and what i'm going to try to do is
332:02 - remove the end here and give this
332:04 - another go okay
332:08 - so paste that in
332:12 - i don't see the question mark on that so
332:15 - this is not uh this is not an easy one
332:17 - okay
332:18 - we'll put question mark here hit enter
332:22 - it just says stopped done done done done
332:25 - so it makes you think that it's working
332:28 - clearly is not um
332:30 - let's just double check the container to
332:32 - see what's there okay
332:38 - no file still
332:41 - well what if we uh grab this one here
332:54 - maybe it's an encoding issue or maybe it
332:55 - doesn't like the way we do the link
332:57 - hit enter
332:59 - still says stopped
333:06 - i don't know
333:07 - all right i'm back so you know what i
333:09 - thought i was doing everything right and
333:10 - so just as a sanity check what i did was
333:13 - i opened up my command prompt
333:15 - and i installed a just a different
333:16 - version of azcopy it's the windows
333:18 - version and using the exact same command
333:22 - it worked so you know i just want you to
333:24 - know like this is kind of
333:25 - something i run into a lot with azure
333:27 - whereas if you try it one way it doesn't
333:29 - work but some other place it works no
333:31 - problem just because azure tries to
333:33 - support all use cases
333:35 - sometimes you'll be spending a lot of
333:36 - time uh second guessing yourself but
333:39 - definitely the command i showed you was
333:40 - correct um but uh you know it could be
333:43 - that the um daisy copy version is out of
333:45 - date so remember when we wrote an az
333:47 - copy here for linux it was suggesting to
333:50 - get a newer version but i had downloaded
333:52 - the latest version so
333:54 - you know it could be that um it's really
333:56 - hard to say uh but i want you to know
333:58 - like that's just how you do an easy copy
334:01 - um and you know don't get frustrated if
334:02 - you get stuck on stuff like this okay so
334:05 - we're all done here and let's go ahead
334:06 - and tear down our stuff
334:09 - and uh we'll just go to fajo here that's
334:11 - the one we set up there and we'll go
334:13 - ahead and delete that resource group
334:15 - and we're all done here okay
334:18 - [Music]
334:22 - hey this is andrew brown and this follow
334:24 - along we're going to learn how to work
334:25 - with azure's sdk so we're going to work
334:28 - with a variety of different languages
334:29 - and we're going to interact um i suppose
334:31 - with a storage account so what i want
334:33 - you to do is make your way over to your
334:35 - portal here and type in storage accounts
334:38 - across the top and we will create
334:39 - ourselves a new repository here or sorry
334:42 - a storage account and we'll call this
334:44 - one my sdk playground
334:47 - and we will say ok and i'll just type
334:49 - the same thing by sdk playground you
334:52 - might have to put some numbers on the
334:54 - end there because these are fully
334:55 - qualified domains yes east us is totally
334:58 - fine standard is fine we'll go ahead
335:00 - ahead and hit review and create so we'll
335:02 - give it a moment there
335:05 - then we'll go hit create
335:07 - and we just have to wait about 30
335:09 - seconds for this to finish
335:11 - while this is going what we can do is
335:13 - make our way over to github because
335:14 - we're going to need a repository to play
335:16 - around in so i'm going to create a new
335:18 - repository
335:19 - and we will choose whoops i don't want
335:22 - to template but i want to go to my own
335:24 - and this will be my sdk azure playground
335:28 - putting the word azure in there so i
335:29 - know what this is later on we'll say add
335:31 - readme we're not going to choose a git
335:32 - ignore file because we'll have to vary
335:34 - it based on a couple different folders
335:36 - and you should have get pod installed or
335:38 - if you don't you can use your local
335:39 - machine it's up to you
335:41 - but it's a lot easier to use git pod
335:43 - here it's free free to use so no reason
335:45 - not to to use getpod all you're doing is
335:47 - attaching this in the front of your url
335:49 - if you want the button you can go get
335:50 - the chrome extension i'm going to go
335:52 - ahead and hit get pod and spin up that
335:53 - environment going back to our resource
335:56 - or storage account you can see it's
335:59 - deployed so we'll go to that resource
336:00 - and what we'll need here is a single
336:02 - container so that is something that we
336:04 - will grab here right away and then um
336:09 - so let's just say container one
336:11 - i'll make it private i'll just make it
336:12 - one with no hyphen because i think
336:14 - that's how i wrote the instructions
336:16 - and so in here we will need to upload
336:19 - some kind of file so i'm just going to
336:21 - upload a data pipe and data stair
336:25 - upload any files you like but you'll
336:26 - have to change the names accordingly
336:28 - what we'll need to do is make our way
336:30 - over to access controls i'm actually
336:32 - going to do this at the storage level so
336:34 - we'll go back up a layer here
336:38 - go back to that resource we'll go to
336:41 - access keys
336:42 - and we are going to need
336:45 - our connection string oh sorry shared
336:47 - access signatures
336:49 - well we'll figure it out here in a
336:51 - moment because we do need to i thought
336:53 - it was under access keys that it shows
336:56 - um oh it's right here connection string
336:58 - okay so it is right here we'll be in
337:00 - good shape so what we'll do is go back
337:02 - to our git pod and we're gonna make uh
337:03 - three folders we're gonna have ruby
337:05 - we're gonna have python
337:08 - and we're going to have
337:10 - uh javascript so we'll just say yeah js
337:13 - for javascript and in each of those
337:15 - we're going to create a new file so so
337:17 - say gem file
337:20 - uh this will be the actually i want to
337:21 - generate that out
337:23 - so we'll uh cd into that directory and
337:25 - i'll say bundle
337:26 - um
337:27 - bundle init so that will create a way
337:29 - for us to manage packages in ruby for uh
337:32 - for python we'll create a new one called
337:34 - um
337:35 - uh
337:36 - requirements.txt
337:39 - and then for uh javascript
337:42 - this will be npm init and so these are
337:44 - the three ways well i should have done
337:46 - hyphen y on this here
337:48 - these will be the three ways that we
337:49 - bring in our external libraries of the
337:51 - azure sdk into each of these programming
337:55 - environments so first for ruby we'll go
337:57 - through that one
338:00 - so for this one we need to install a gem
338:04 - called
338:07 - azure storage
338:11 - storage blob
338:14 - so that will be for that one for python
338:17 - it is called i'm just trying to find the
338:19 - name here
338:21 - it is azure storage
338:23 - blob and we can give it a version here
338:27 - we could also take the version out it
338:28 - probably should still work without the
338:30 - version there
338:31 - and then for our package.json we'll just
338:34 - install that there as well so i'm just
338:36 - going to cd into
338:38 - um python because it's easier to install
338:40 - python packet or sorry yeah not python
338:42 - packages but javascript packages
338:44 - from the cli here so i'm just looking
338:46 - for what it's called so it's mpm
338:49 - install at sign azure storage
338:53 - blob
338:56 - all right so this one's installed we go
338:58 - to package.json we can see it's required
339:00 - uh the gem file in order to install in
339:02 - ruby we have to do bundle install
339:06 - and the reason i'm doing them all in
339:08 - parallels i'm trying to show you there's
339:09 - a pattern right you're installing the
339:11 - package you're basically doing the same
339:12 - thing over and over it's just that the
339:13 - syntax is slightly different for python
339:15 - is going to be python
339:17 - or sorry pip
339:18 - pip install
339:22 - hyphen r requirements
339:25 - and i generally remember all these
339:27 - things you know what i mean like i might
339:28 - not remember all the details but i know
339:29 - that the patterns are the same and that
339:31 - makes it a lot easier and we call this
339:32 - cloud programming because you don't need
339:33 - to know
339:35 - everything in order to do this
339:37 - so the first one that we're going to set
339:39 - up is the ruby file so we'll make a new
339:40 - file here and we'll just say main.rb
339:42 - which is an okay name here
339:44 - and we'll get coding with this one first
339:47 - so what we'll need to do is require
339:50 - and we'll do azure storage blob
339:54 - and here we want the account name
339:58 - and we want to have the account key
340:01 - so we'll go back over to here and i'm
340:03 - going to go ahead and grab the first key
340:04 - so we'll say show
340:07 - and i don't want to put an action here i
340:08 - want to put it down in my environment
340:09 - variable so i'm going to say export
340:12 - account
340:13 - key here equals
340:15 - and then i'm just going to do double
340:16 - quotations and paste it in and hit enter
340:19 - and i'm going to do gp that's for git
340:20 - pod environments account key
340:22 - i'm just setting it twice so that if we
340:24 - have to restart this environment that
340:25 - these keys will persist
340:29 - okay so
340:31 - um
340:32 - what we'll do here is just put in um
340:36 - env parentheses
340:38 - and this will be account key
340:40 - so that'd be the way we um import it
340:42 - into ruby because that's how you do it
340:44 - well this one's account key this is
340:45 - account name so let's say account name
340:47 - and then here will be account key
340:52 - and if we were to write puts account
340:54 - name
340:57 - puts
340:58 - account key that should print those out
341:00 - so we can see if they work so we'll do
341:01 - bundle exect
341:03 - ruby
341:05 - main so this is a pretty common way and
341:07 - i'm doing in the wrong folder we got to
341:08 - make sure we're in the ruby folder
341:09 - before we do that
341:10 - so this is a common way in ruby to run
341:12 - things in the context of the gem file
341:15 - you'll see that's kind of a bit of a
341:16 - trend for all these different uh
341:18 - programming environments so we'll go
341:20 - ahead and hit enter and it should just
341:21 - print out the keys it printed out the
341:22 - first one but not the second one we
341:23 - didn't set the count a name yet so this
341:26 - is the account name for it so we'll do
341:28 - export account name
341:31 - the reason we're doing export is that it
341:33 - actually loads the environment variable
341:34 - so if we go here and say grep
341:37 - account you can see it's actually loaded
341:38 - into our developer environment and then
341:41 - getpod environment is going to persist
341:44 - if we restart this environment so
341:46 - there's that one and then we want to do
341:48 - it
341:49 - for
341:51 - get pod there we go so they're both set
341:53 - if i just double check here and we do
341:55 - this
341:56 - they both show up so that is good
341:59 - and so the next thing we want to do is
342:00 - make a client so we want to establish a
342:02 - connection so i'm going to do blob
342:04 - client equals azure
342:06 - storage blob
342:11 - blob
342:13 - blob
342:14 - service
342:15 - dot create if you're wondering like
342:16 - where do you find these things you just
342:18 - like search on google you say azure
342:21 - azure blob storage ruby
342:24 - like and maybe documentation and you
342:26 - know through there you can kind of find
342:27 - information there's probably like a
342:28 - documentation page
342:30 - if we go here
342:32 - um
342:34 - i know there's one
342:37 - maybe here
342:40 - and then you scroll down
342:42 - blobs
342:45 - so there's all sorts of way yeah this is
342:47 - pretty similar code here so there's all
342:48 - sorts of way to find it you just have to
342:49 - kind of search around and eventually you
342:51 - acquire that knowledge so we need the
342:54 - storage
342:56 - account name and then we need the
342:58 - storage access key
343:04 - so this will be the
343:06 - account name and this will be
343:09 - the account key and then down below
343:11 - we'll just do blob client we'll make
343:12 - sure that works so we'll go back and
343:15 - oops hit up on the bundled exec here
343:18 - and so it prints out some kind of object
343:19 - so we know that is now uh at least
343:22 - the the client's being created so now
343:24 - let's go ahead and actually print
343:25 - something out so we'll go blob client
343:27 - list
343:28 - containers parentheses each
343:30 - do container
343:32 - and so just kind of doing iteration here
343:35 - and just do container name
343:37 - and if we hit up
343:39 - it will print out container one so that
343:41 - container now does exist let's say we
343:43 - wanted to go create a container that's
343:44 - something we can do as well from here so
343:46 - we'll say blob client
343:48 - create container
343:50 - i'll just say
343:51 - sum container name
343:55 - here and we'll do it again and if we go
343:59 - over here and take a look at our
344:01 - containers we should see
344:03 - a second container called some container
344:05 - names that one is working fine
344:07 - let's say we want to print out the
344:09 - content so we'll do blob client list
344:12 - containers actually we already did this
344:13 - up here i'm not going to print it out
344:14 - twice we'll just do it in the interior
344:16 - here might as well just print out the
344:18 - contents of all the blobs we're going to
344:20 - see what is actually in the containers
344:23 - say container name dot each
344:27 - do
344:28 - blob
344:30 - and and then we'll do puts
344:33 - do interpolation blob
344:36 - name
344:38 - then we hit up
344:40 - uh list blobs probably that's probably
344:43 - what it's supposed to be here
344:46 - you can see it's printing out the names
344:48 - here so that's working out fine uh it
344:50 - had a http error i don't know if it
344:52 - actually matters we'll do it again
344:57 - oh you know what it's because we already
344:58 - created this one so we'll just comment
344:59 - this out that was just like a one off
345:01 - there
345:02 - but you can see
345:04 - so it's not gonna create the same folder
345:05 - that's printing the contents of
345:06 - container one there's nothing in
345:07 - container two or some container
345:10 - so that is pretty straightforward there
345:13 - let's go take a look at python
345:15 - so we'll make a new file here we'll call
345:17 - it main.pi
345:20 - normally like if you want to isolate
345:22 - your libraries you create a python
345:24 - environment i didn't do that um i don't
345:25 - really always remember how to do it
345:26 - because i don't work in python that
345:28 - often but i know it exists but that's
345:29 - the idea behind us doing the bundle exec
345:32 - or if we do mpm start which you'll see
345:35 - here is that it's in the context of our
345:37 - isolated libraries that we want to use
345:39 - uh but anyway we'll get started with
345:42 - python here so
345:43 - very similar process we're gonna do
345:45 - azure storage blob
345:47 - base blob service import base blob
345:51 - service a little bit more verbose i'm
345:53 - not a big fan of um
345:55 - i'm not a big fan of python i need to
345:57 - figure out how to get environment
345:58 - variables i'll be back in a second
346:00 - okay so to get environment variables is
346:02 - just import
346:03 - os
346:04 - and then from here we can get uh os
346:08 - and viron
346:10 - squares
346:11 - all right we could just do get i suppose
346:13 - okay it's probably nicer and we can do
346:14 - account name here
346:19 - see the process a little bit different
346:22 - account key
346:25 - instead of puts it will be print
346:27 - so account name
346:28 - [Music]
346:31 - account key
346:33 - starting to look familiar right
346:35 - just slightly different and we'll have
346:37 - to be in the right folder here so we'll
346:39 - make our way over to python this one's
346:40 - going to be
346:41 - python
346:43 - main we don't have to uh
346:45 - you know
346:47 - do it in the context stuff because it's
346:48 - already there no module name azure
346:50 - storage blob
346:51 - blob service line i don't know if we did
346:53 - an install i thought we did so we'll do
346:54 - it again requirements.txt
346:59 - i mean it's totally possible i typed
347:00 - something wrong
347:03 - i'm just double checking azure storage
347:05 - blob
347:06 - base blob
347:08 - service
347:10 - looks correct to me we'll try this again
347:18 - i'm just going to do a quick search here
347:21 - just to see if i typed it wrong
347:26 - just in case it's a versioning issue
347:28 - because we did have a version in there
347:29 - 2.1.0 i just don't want this to mess up
347:33 - so i'm going to go here and go to
347:36 - 2.1.0
347:38 - and we'll try this again because you
347:39 - never know if these requirements ever
347:40 - change
347:45 - i think it wants double equals
347:47 - yeah it does
347:53 - and we'll give this another go okay so
347:56 - now it's working so the versioning does
347:57 - matter we're not on the latest version
347:59 - but honestly it doesn't really matter
348:02 - because
348:03 - you know
348:04 - it doesn't
348:05 - so now that we have those two printing
348:08 - out we can go ahead and make our client
348:11 - or i guess service in this case the
348:13 - wording's a little bit different each
348:14 - one i don't know why but it's just how
348:16 - they do it
348:17 - so here we will have account name
348:20 - [Music]
348:22 - equals account name
348:24 - account key equals account key
348:28 - and we will print out the blob service
348:32 - as similar to before
348:34 - we will go up
348:37 - oh sorry i wanted to do uh python main
348:39 - here
348:41 - and so that prints out no problem
348:44 - now we'll need to iterate through this
348:45 - stuff so we'll say containers equals
348:48 - blob service
348:49 - list containers parentheses for
348:52 - container in containers
348:54 - and this is indent based so there's no
348:56 - like closing statement we just double
348:58 - like this
349:01 - container.name
349:03 - i wasn't trying to run it i think it
349:04 - just ran there below by accident just
349:07 - type exit down below here
349:10 - we will hit um
349:13 - go back to
349:14 - thinks i'm in a shell i'm not in a shell
349:16 - okay so we'll say python
349:18 - main.rb again
349:20 - oh i'm back in the main folder
349:22 - so we'll say python main.rb
349:27 - and it says can't open file or directory
349:30 - python i do oh it's right there
349:32 - sorry pi
349:36 - and uh now it's saying these don't exist
349:41 - so
349:42 - let me just do
349:43 - i think because we entered an
349:44 - environment i don't know what happened
349:45 - here but something happened where see
349:48 - ran this
349:50 - and it put us into the
349:52 - python environment shell and that wasn't
349:53 - something i intended to do so what i'm
349:54 - going to do
349:56 - oh i'm going to have to update our git
349:58 - pod yaml file here i'm just going to add
349:59 - a or ignore i'm just going to commit the
350:02 - stuff to reset the environment because i
350:03 - really don't want to be in a a
350:07 - pi environment like that
350:09 - it was just kind of a mistake
350:12 - and
350:14 - hit add dot get ignore
350:23 - come on let me add this darn file
350:25 - i'm just trying to commit this single
350:26 - file here
350:29 - get add.get ignore
350:31 - so then when we go over here
350:34 - it's a bit more manageable so i'm just
350:35 - going to commit all the code here
350:37 - um
350:38 - ruby and some python
350:42 - and what i'll do is i'll just close this
350:44 - environment
350:45 - and then reopen it
350:47 - and those environment variables should
350:48 - still be set because we did the gitpod
350:50 - enamel
350:51 - or getpod environment
350:55 - so i'll just give it a moment here to
350:58 - respin up the environment
351:05 - all right so our environment is back up
351:07 - so what i'm going to do is make our way
351:08 - back into the python directory and we'll
351:10 - do a
351:13 - pip
351:15 - install
351:17 - our requirements
351:20 - and so hopefully this time we don't get
351:22 - that weird
351:24 - problem we had before here i really
351:25 - don't have to restart
351:27 - notice it's printing the container name
351:28 - so
351:29 - pretty similar to what we did before
351:35 - and so what we'll need to do now is just
351:36 - print out the actual objects like we did
351:38 - with the ruby script so we'll go back
351:40 - here to the main
351:41 - scroll on down
351:44 - and we'll do blobs
351:47 - equals blob
351:49 - service list blobs
351:52 - parentheses container name
351:55 - for blob in
351:59 - blobs
352:02 - we'll say print
352:04 - f
352:06 - i think it's just single quotations i
352:08 - always want to do uh doubles there but
352:09 - it's a single
352:10 - and we'll do um
352:13 - curly's like that
352:14 - blob name so you saw this in the ruby
352:16 - one but it was like it said it was a
352:17 - pound and double quotation so this is
352:19 - called interpolations where you inject a
352:21 - variable into a string each language
352:22 - does it a little bit different so let's
352:24 - take a look here at this point one
352:25 - prints out the object same thing so very
352:27 - similar let's go take a look at
352:30 - javascript so that is our next one here
352:32 - so we'll go over here we'll make a new
352:34 - file we'll call it main.js
352:36 - we will need to update our package.json
352:38 - so we have a way of calling the script
352:40 - so we'll just call start here and we'll
352:42 - just say nodemain.js
352:46 - and then we will go up here
352:48 - and we will load the stuff in so we'll
352:50 - say uh constant
352:52 - curlies blob service client
352:56 - storage shared key credentials
353:01 - credential
353:02 - you got to spell it right or it will
353:04 - mess up for sure
353:06 - equals require
353:08 - at sign double single quotations azure
353:11 - storage blob
353:13 - and we'll do constant account name
353:17 - constant
353:18 - account key
353:23 - and there will be this one and then
353:25 - there will be this one so
353:27 - um
353:28 - how do we load environment variables in
353:29 - javascript i'll be back in two seconds
353:31 - actually i'm surprised i don't remember
353:32 - because i did it in so many other
353:34 - followings it's actually just process
353:37 - it's a problem when you move from a lot
353:38 - of languages you have to kind of look
353:40 - things up multiple times but it's okay
353:41 - because as long as you know what you
353:43 - need to look up
353:45 - env dot account key
353:48 - so those are our two that we're going to
353:49 - load here we need to create a credential
353:52 - key so we'll say key
353:54 - credential equals new
353:58 - storage shared access credential
354:01 - parentheses account name
354:04 - account key and then down below we'll
354:06 - create a blob client equals new blob
354:10 - service client it's interesting like how
354:11 - they're all slightly named difference
354:13 - like in python it's just uh service
354:16 - ruby's just client and then in
354:19 - in javascript it is
354:22 - service client so we'll do a url so this
354:24 - is when i always say like things are
354:26 - fully qualified domain name that's
354:27 - that's what i mean is like you can
354:28 - literally call us
354:30 - services like storage
354:32 - based on a fully qualified domain so
354:34 - we'll do blob
354:36 - core
354:37 - windows.net
354:38 - and we'll go down below here and say key
354:40 - credential
354:42 - here
354:44 - and so we'll do console log
354:47 - blob client i don't know if we'll get
354:48 - anything intelligible out of this but
354:50 - we'll give it a go i think like as long
354:52 - as we don't get an error it should be
354:53 - fine
354:54 - because i don't know if it would return
354:55 - anything in javascript we'll find out
354:57 - here in a second so we'll make our way
354:58 - over to node.js
355:00 - or javascript we'll do an npm install
355:04 - npm install
355:06 - and we must have a syntax error here
355:08 - there's a comma missing here
355:11 - oops i didn't want to build lines it's
355:12 - just being silly we'll go back here to
355:15 - npm install so it's we have to do it
355:17 - because i restarted the environment
355:19 - because those did not persist
355:21 - and
355:22 - what we'll do is do npm start
355:25 - oh we are getting data so there are no
355:27 - errors which is good
355:28 - and we're getting a object back so
355:31 - this looks good these errors are all
355:33 - errors
355:34 - we'll go back to our main js
355:36 - and we will do what we've been doing
355:38 - which is iterating through stuff so the
355:41 - first thing we'll need to do is create a
355:43 - new function we're going to call this
355:44 - async
355:46 - function container inventory
355:49 - i really really don't like
355:51 - having to do asynchronous stuff when i
355:52 - don't have to
355:54 - but
355:55 - that's like a
355:56 - common feature i mean other languages
355:58 - have it but that's something we do often
356:00 - in
356:01 - node.js or javascript so we'll say
356:04 - 408 parentheses constant container in
356:08 - containers
356:11 - curlies
356:13 - console.log
356:15 - container name
356:17 - container
356:20 - dot name
356:24 - and then we will call this function
356:27 - so hopefully this works
356:29 - npm start
356:31 - we we don't need this anymore here it's
356:33 - gonna get our way so i'm gonna take it
356:34 - out
356:35 - uh it doesn't like the end it's supposed
356:36 - to be of
356:38 - if it's not in change of
356:40 - we'll iterate through here and we get
356:41 - our two containers now we need to go get
356:43 - the
356:43 - blobs within it
356:45 - so in here what we'll do
356:48 - i'll just think about this for a second
356:49 - we need to
356:50 - do constant container client equals
356:54 - blob client get container client
356:57 - parentheses container name
356:59 - and we'll say
357:01 - let blobs equals
357:03 - container client
357:05 - list blob flat
357:08 - parentheses container name
357:10 - we'll say 408
357:13 - const blob of blobs
357:21 - console log parenthesis
357:23 - back ticks
357:25 - is it back ticks yeah it's back ticks
357:28 - dollar sign curlies again this is how
357:31 - you know each one has different
357:32 - interpolation right so like python it
357:34 - was
357:36 - single string you had to put this f in
357:37 - the front to do interpolation ruby was
357:39 - just double quotations with a pound
357:41 - curly so they're all just slightly
357:43 - different and hopefully this works
357:46 - no uh container is not defined so
357:48 - [Music]
357:50 - what line is it complaining on it's
357:51 - complaining on the list blob flat right
357:54 - here so i must have spelt it wrong
357:56 - we'll go ahead and paste that in there
358:02 - it didn't paste right
358:05 - we'll just retype it container
358:08 - try it again
358:11 - there we go and so that's all three of
358:13 - them so that's all i really wanted you
358:14 - to do
358:16 - just go through the motions of using the
358:17 - sdk
358:19 - with some language and just to see that
358:20 - there are some patterns to languages so
358:22 - it's not as super hard as you imagine
358:24 - we'll close that it doesn't really
358:25 - matter if we leave it around or we
358:27 - don't delete it but for the resource
358:29 - group here with the storage account we
358:31 - will go ahead and delete
358:32 - i'll go in here we'll go into my sdk
358:35 - play oh and
358:38 - we'll click on the resource group here
358:39 - lots of ways to get to the resource
358:40 - group we'll go ahead and delete
358:43 - our resource group and i'll also delete
358:45 - the storage account so
358:47 - there you go
358:50 - [Music]
358:54 - hey this is andrew brown from exam pro
358:56 - and this fall along we are going to work
358:57 - with blob storage object metadata and
358:59 - specifically with powershell in order to
359:01 - set it and view that it's working so
359:04 - what we're going to need is um an
359:07 - environment in order to run powershell
359:08 - so we're going to do it in cloud shell
359:10 - we'll expand that and you'll have to be
359:12 - on powershell mode if you this is the
359:13 - first time you're running cloud shell
359:15 - ever you might have to
359:16 - accept for a storage account so it might
359:18 - say like create a storage account for
359:19 - this you just say yes
359:21 - notice here that it's running uh install
359:23 - module for you for azure
359:26 - so you know if you're doing this on your
359:27 - local machine and you can do this if you
359:29 - have windows installed so the idea is
359:31 - that you would open up powershell so i
359:33 - type in powershell
359:35 - all right click and run administrator
359:36 - remote if you're on a mac you can
359:38 - install powershell
359:40 - on a mac but to be honest this is so
359:41 - much easier if we just use cloud shell
359:44 - but i just want to show you a couple
359:45 - things so if you were to install
359:47 - um
359:48 - azure you do install module hyphen name
359:52 - az and then this thing takes
359:54 - forever
359:56 - like i swear to you it takes forever and
359:58 - that's why i don't really want to do it
359:59 - this way
360:00 - just bump up the font here to 24.
360:03 - and so we just say yes to all
360:07 - and we'll let that install in the
360:09 - background
360:10 - but i just want you to know that that's
360:12 - how you install all the add azure
360:14 - command commandlets
360:17 - because it's very useful to know
360:19 - and we do it a couple times i actually
360:20 - do do it in one and we have to wait and
360:22 - it takes forever
360:24 - but while that's installing the
360:25 - background we're gonna assume that we
360:26 - did connect a z and install module and
360:29 - now we are just getting some stuff so
360:31 - it's a good habit to whenever you're
360:32 - using type of clear hair whenever using
360:35 - a powershell with azure to make sure
360:37 - that you are on the correct subscription
360:39 - um because it can be in the wrong place
360:41 - so even though i know this is the one
360:42 - we're on i'm just going to set it again
360:44 - so we get the habit of doing it so it's
360:46 - set
360:47 - az context
360:50 - here and we'll do subscription
360:51 - [Music]
360:53 - and we'll do azure sub
360:55 - description one and so that will make
360:57 - sure that that is explicitly set now we
361:00 - want to
361:02 - get access to the resource group and of
361:04 - course we could easily do this
361:07 - through
361:09 - the console but you know i wanted to
361:10 - learn some powershell so i'm going to
361:12 - make a new tab here and what we'll do
361:14 - is make our way over to storage accounts
361:16 - because we're going to need a new
361:17 - storage account for this
361:18 - we'll go ahead and hit create
361:21 - and we will make a new resource group
361:24 - this one's going to be called
361:26 - my blob metadata
361:29 - and we'll say blob metadata
361:33 - 88 whatever it doesn't matter as long as
361:35 - it's not used we'll hit review create
361:38 - and we will
361:39 - create it here
361:40 - and we'll give it a moment here so we
361:42 - can go to the resource group so there
361:44 - are a few things we will have to type in
361:46 - that thing will be done in a moment so
361:47 - we'll need um just type clear here it
361:50 - always messes up i'm going to expand
361:51 - this because we really don't need
361:52 - anything in the background so we want
361:55 - dollar sign resource group
361:57 - equals and then whatever we called it so
362:00 - my metadata blob i think
362:03 - or my blob metadata
362:05 - my blob metadata
362:10 - and we'll hit enter whoops that is not
362:12 - what i want i want an a on the end there
362:14 - and then we need the storage account
362:16 - name
362:17 - so here it will be called
362:21 - go to the resource
362:23 - that is the name right there
362:25 - all we're doing is setting variables
362:26 - here right now
362:30 - and it's not copying properly because
362:32 - you have to right click you can't
362:34 - use hotkeys there why i don't
362:36 - know uh we'll do storage
362:40 - key equals and the idea is we are going
362:43 - to fetch our storage keys we'll do a z
362:45 - storage
362:46 - account key
362:48 - resource group name
362:50 - dollar signed resource group
362:52 - and then name
362:54 - dollar sign
362:57 - storage
362:59 - account i just noticed that i spelled
363:01 - that wrong i'm just going to hit enter
363:02 - because i know that's not going to work
363:04 - i just want to take this c and make it
363:05 - smaller
363:06 - go back here and do storage account
363:09 - hit enter and so that will create that
363:11 - then we need our context and we'll say
363:13 - new a z
363:16 - storage
363:18 - context storage
363:23 - account name dollar sign storage account
363:27 - storage
363:28 - account key dollar sign storage key
363:32 - and it doesn't like something because we
363:34 - need to actually get the value because
363:36 - if we type in storage here
363:38 - and by the way i didn't type this right
363:39 - again
363:41 - storage key but if we do storage key and
363:43 - just see what was returned
363:45 - notice that there's two keys so we'd
363:46 - have to do something like zero dot value
363:48 - here
363:50 - so we'll go here and say
363:52 - square zero
363:54 - dot value
363:57 - and it says cannot provide parameter for
364:00 - name
364:04 - um
364:05 - [Music]
364:08 - oh you know what that doesn't make sense
364:10 - it's not it's not name sorry it's
364:11 - because i'm going back to the wrong one
364:13 - here so this is this one
364:15 - and we really wanted the context
364:19 - just have to be careful what we're doing
364:20 - here there we go so now we have a
364:22 - context
364:23 - so it's just telling us
364:26 - what we're kind of bound to and so now
364:27 - we can do get a z
364:29 - storage container
364:32 - hyphen contacts dollar sign context if
364:34 - you're wondering like where do you get
364:35 - all these like all of these um commands
364:38 - like once you find one you find them all
364:41 - pretty much
364:42 - see here there's like tons tons of them
364:44 - you can read through them and they have
364:45 - tons of examples so you can work your
364:46 - way through and figure it out
364:48 - uh so i was expecting
364:50 - [Music]
364:51 - i guess there are no containers so that
364:53 - kind of makes sense so we'll go over to
364:54 - here and we'll say container one
364:58 - keep it all lower case to make our lives
365:00 - easier
365:02 - and we'll go back over here and we will
365:04 - hit up again you can see there's a
365:06 - container i think we can type in
365:07 - container and then do container one
365:09 - to specify even greater context there
365:14 - except i just have to put a capital c on
365:16 - here maybe oh lower case on this one i
365:19 - think all these parameters don't matter
365:20 - they can all be lowercase so those don't
365:22 - matter
365:24 - so we now have a reference to a
365:26 - container
365:28 - so now what we need to do is
365:31 - um
365:32 - reference a blob so
365:34 - what i'm going to do is
365:36 - type in blob
365:38 - and i think we'll have to upload
365:40 - something first but i'm just going to
365:41 - type something for fun here
365:43 - uh and so i'd probably do like an image
365:45 - i have an image on our desktop that i've
365:47 - been using for a while so data pipe
365:49 - web p
365:51 - is probably what it is
365:52 - and we'll do container it isn't there
365:54 - yet but we'll see what happens if we do
365:56 - it and there's no image or uh thing
365:58 - uploaded we'll do dollar sign context
366:00 - and so it says could not recognize that
366:02 - command this happens a lot but it always
366:04 - has to be like something like a verb or
366:07 - an action and then
366:08 - whatever so it's just saying the file's
366:10 - not there we know that's the case we'll
366:12 - go into our container and we'll say
366:14 - upload we will then go
366:16 - and upload data pipe it's actually a png
366:18 - which is a lot easier on us we'll go
366:21 - back over here again you'll have to
366:22 - upload your own file it'll have its own
366:24 - name so you got to go find an image
366:25 - yourself
366:26 - we'll hit enter
366:28 - and so if we now type in blob we have a
366:30 - reference to that blob
366:32 - so let's get some information about the
366:33 - blob so we'll say blob dot
366:37 - blob client get
366:40 - properties dot value
366:42 - and so we have a lot of information
366:44 - about the blob notice that there is no
366:45 - metadata right here um i think we can
366:48 - just get the metadata if we type in
366:49 - metadata
366:53 - i don't think there is any metadata so
366:54 - it's not returning anything i'm just
366:55 - going to make sure i'm typing it right
366:56 - i'll type something i know is there like
366:58 - etag
366:59 - yeah so it's just there's nothing set
367:01 - so what we'll do is make some metadata
367:03 - so we'll do metadata equals new
367:06 - object system
367:11 - dot collections.generic.dictionary
367:16 - double quotation squares string string
367:21 - so we are creating a new object that
367:23 - expects a dictionary with a string
367:24 - string hopefully that makes sense it's
367:26 - because we have to add
367:28 - a key and a value
367:30 - right because that's how tags mostly
367:33 - work even for metadata so we'll go here
367:35 - and say author
367:38 - exam pro
367:41 - and we'll say dollar sign
367:44 - metadata dot add
367:49 - department
367:52 - i t
367:54 - and we'll just say blob dot
367:57 - blob clients dot set metadata
368:01 - and then we'll actually set the metadata
368:03 - so let's say metadata
368:05 - dollar sign null i don't know why the
368:06 - second one is null i don't really care
368:07 - if we wanted to look it up we could
368:10 - doesn't really matter but it does set
368:12 - the metadata because if we go back
368:14 - to our blob
368:16 - do this now we have metadata on it just
368:18 - to make sure that it is working
368:20 - correctly we're going to just null it
368:21 - out
368:22 - so if we go blob so and now we'll fetch
368:24 - it again
368:26 - just so that we know remotely it's not
368:28 - like a local thing
368:30 - we'll go here
368:32 - and so there you go so that's all i
368:34 - really wanted to show you and hopefully
368:35 - you get a little bit more powershell
368:37 - experience there what we'll do is go to
368:38 - the resource group and clean up here
368:42 - we will go into this one
368:44 - we will delete it
368:56 - give it a moment and there you go
368:58 - [Music]
369:02 - hey this is andrew brown from exam pro
369:04 - and this fall along we are going to
369:06 - copy a blob from one container to
369:08 - another using powershell so what i want
369:10 - you to do is go to the top here and
369:12 - we're going to type in storage accounts
369:14 - and we're going to create ourselves a
369:15 - new storage account and the storage
369:17 - account is going to be called
369:20 - well the resource group will be called
369:22 - my
369:23 - uh
369:24 - blob copy
369:26 - powershell ps4 powershell and we will
369:29 - say blob ps
369:32 - okay a bunch of numbers because the
369:34 - naming is always a pain because these
369:35 - are fully qualified domains review
369:37 - create
369:38 - and wait a moment so you can go ahead
369:40 - and create we'll hit create and we'll
369:42 - wait a little bit
369:44 - here it only takes a few seconds but
369:46 - while it's going we might as well open
369:48 - up our cloud shell environment if this
369:49 - is the first time you've ever opened up
369:51 - cloud shell you will have to
369:53 - um
369:55 - you will have to
369:56 - accept it for storage accounts and by
369:58 - the way off screen i was doing this in
369:59 - another follow along i never came back
370:01 - to it but like if you want to have azure
370:03 - installed you have to install module uh
370:06 - name azure the cloud gallery does this
370:07 - but this takes forever and then once
370:10 - it's installed what you would do is you
370:12 - do a connect
370:13 - and that connect command would be i'm
370:15 - just trying to find it here
370:17 - i think i would know it off the top of
370:18 - my head i do not it's like connect
370:22 - a z account
370:23 - and then what it would do is it would
370:24 - prompt you to log in with microsoft it's
370:26 - not going to work here on my local
370:27 - machine because i don't have
370:29 - something authenticated
370:32 - and so i just don't want to bother with
370:34 - it but anyway we're going to do it here
370:35 - because it's so much easier over here
370:38 - and so we'll go back over to our oops
370:40 - that's from an old follow along we'll go
370:43 - here and open this i guess open it here
370:45 - it's fine we'll go ahead and create a
370:47 - new container so we'll say container one
370:50 - and then we'll go and create container
370:53 - two
370:55 - and we will i guess expand this for the
370:58 - most part and we'll get to it here
371:01 - so now that our containers are created
371:03 - we'll go ahead
371:04 - and make sure that we are in the correct
371:06 - subscription this is something you
371:07 - always should do
371:09 - just to make sure the right place so
371:10 - this is the subscription we're currently
371:11 - set i know it's already set correctly
371:13 - but just to go through the habits of
371:14 - setting it we're going to set the
371:16 - context for the subscription to be
371:18 - explicitly this one
371:21 - because that's something that will catch
371:22 - you off guard as being in the wrong
371:23 - subscription and so we need to set up
371:25 - some
371:26 - variables we'll have resource group
371:29 - equals and we'll have to find the name
371:31 - of it
371:32 - so we called this we'll go the overview
371:34 - here
371:38 - my
371:40 - blob copy ps let's click off here i
371:43 - don't know why it's doing this weird
371:44 - select
371:46 - sometimes it happens
371:49 - there we go
371:50 - so that is the name of that then we need
371:52 - the
371:55 - storage account
371:56 - name and so the storage account name is
372:00 - does it show me here no i can go up one
372:03 - layer
372:05 - go the resource this is the storage
372:07 - account name
372:12 - oh boy
372:13 - i always forget
372:15 - that you cannot
372:17 - uh do command v there you gotta paste
372:20 - manually and so we'll get the storage
372:22 - key here
372:24 - and of course we could just easily get
372:25 - it by going to the left-hand side here
372:27 - and clicking a couple buttons but again
372:29 - we want to practice our powershell so
372:31 - we'll do get a z
372:33 - storage account key
372:36 - minus or hyphen resource group name
372:39 - dollar sign resource group hyphen name
372:42 - storage account
372:47 - okay and if i've typed everything right
372:48 - we'll hit enter
372:50 - uh
372:51 - i don't think i typed everything right
372:53 - did i
372:54 - a parameter cannot be found that matches
372:56 - the name resource group so that doesn't
372:58 - look right to be
373:02 - here
373:07 - and if we do storage key we can see we
373:09 - have the key
373:11 - if we do zero we can get the first one
373:13 - if we do value we can get the value
373:14 - which is something we will have to do
373:16 - and we set the context here for the new
373:18 - azure storage context so we'll do new
373:21 - hyphen az storage context
373:24 - storage account
373:27 - storage account name
373:31 - dollar sign storage account
373:34 - storage account key
373:38 - storage key
373:45 - and
373:46 - oh we have to do zero
373:48 - dot value here and so now if we do
373:50 - context
373:51 - it should show us a proper context here
373:54 - there we go
373:56 - all right so now what we want to do is
373:59 - upload some kind of blob or actually we
374:02 - don't need to upload one we just need to
374:04 - store one so i'm actually going to use
374:05 - an image
374:06 - i keep using the same image
374:08 - in the in my lab i might use a text file
374:10 - instead but it's up to you what you want
374:12 - to use so in here i'm going to go over
374:14 - to the container
374:15 - where are your containers
374:18 - and we will go to the container here and
374:20 - we will
374:21 - click into it
374:22 - and i will upload
374:24 - a new file so we'll say data pipe png
374:27 - upload
374:31 - and now that that's been uploaded i'm
374:33 - going to type in get
374:34 - a z storage blob hyphen blob
374:38 - we call that data hyphen pipe because
374:40 - he's wearing he has a pipe in his mouth
374:41 - in the video or in the image and we'll
374:44 - say container one
374:45 - context dollar sign context and
374:48 - we'll hit enter
374:51 - it's because we've got the hyphen here
374:55 - and we spelt the blob name wrong
374:58 - there we go so it's returning that so
375:00 - that is good
375:02 - and the idea is we want to download it
375:04 - to our local storage so we can move it
375:06 - to the other one so for that we will
375:08 - still have the context
375:10 - but what we'll do is set a destination
375:13 - and say here
375:18 - oh you know it's get azure
375:20 - azure um
375:22 - storage blob content that's how we get
375:24 - the content
375:26 - not context content
375:30 - and uh we'll say yes because i've done
375:32 - this one before so it's already here if
375:34 - we do ls it's now in our cloud shell so
375:36 - the file is right here so you have like
375:38 - a test file and just remove that test
375:40 - file and so now what we need to do is
375:42 - upload to the other container
375:44 - so we'll do is do set
375:47 - storage blob content
375:49 - we'll say file
375:51 - data pipe
375:53 - hyphen container container 2
375:57 - context dollar sign context and we'll
375:59 - give it some properties it's always good
376:01 - to set the
376:04 - content type
376:06 - whoops
376:08 - content type no hyphen i always try to
376:10 - put a hyphen there because i always
376:11 - think i'm doing um
376:14 - a header and we'll hit enter and so it
376:16 - says storage blob context is not right
376:19 - because we messed that up it should be
376:20 - content
376:25 - and i forgot the hyphen over here
376:28 - does not recognize set storage blob
376:31 - content
376:33 - set az storage blob content
376:37 - um
376:38 - content type is invalid
376:41 - i gotta spell content right
376:43 - could you tell i'm dyslexic i actually
376:44 - am dyslexic so i type a lot of stuff
376:47 - wrong like crazy
376:49 - and i still am able to do cloud
376:51 - az storage blob hyphen blob
376:55 - and this is where we upload it so we
376:57 - will
376:58 - do data pipeline here
377:00 - container
377:01 - container 2
377:03 - context
377:04 - dollar sign context
377:06 - and so it says it's uploaded let's go
377:08 - verify if that is true
377:10 - so what we will do is go over to our
377:12 - storage here
377:14 - we will go
377:15 - to the resources we will go to the
377:17 - containers
377:18 - we will check container 2 to see if it's
377:20 - there and it's there so we are all done
377:22 - let's go clean up we'll go to our
377:23 - resource groups
377:24 - we'll look for the resource group we had
377:26 - created here this one is called
377:31 - what is it called
377:37 - i forgot what it was called
377:39 - that or it's just not showing up
377:42 - so what i'll do is i'll make my way over
377:43 - to storage accounts because i don't seem
377:45 - to be able to find it and we will go
377:48 - into
377:52 - where'd it go
377:55 - i'm so confused
377:57 - what's our resource group called
378:03 - refresh
378:07 - i'm going crazy because it was here a
378:09 - moment ago
378:11 - we'll go to resource groups it's right
378:13 - here again this is this is azure for you
378:16 - sometimes
378:17 - you think something's there but it's not
378:19 - there but it is there it just it's the
378:20 - fact that it has to propagate so it is
378:22 - in our account and so you have to have
378:24 - confidence about what is there
378:26 - we'll go ahead and delete
378:28 - and that will delete
378:29 - and there we go we are all
378:31 - done
378:33 - [Music]
378:37 - hey this is andrew brown from exam pro
378:39 - and we are looking at azure active
378:40 - directory and this is a cloud-based
378:42 - identity and access management service
378:44 - to manage users sign-ins and access to
378:46 - ad-related resources so azure active
378:49 - directory is microsoft's cloud-based
378:51 - identity and access management service
378:53 - which helps you your employees sign in
378:55 - and access resources so that could be
378:57 - external resources like microsoft office
378:59 - 365 azure portal sas applications or
379:02 - internal resources so applications
379:04 - within your internal networking or
379:06 - access to workstations on premise and
379:08 - you can use azure ad to implement single
379:10 - sign-on so you can see that azure id is
379:13 - basically like the the one solution to
379:16 - log into everything and uh we actually
379:18 - use it at exam pro we use it with
379:20 - microsoft teams or uh you know for the
379:23 - exam pro platform our emit is tied to it
379:25 - so when we want to log into the mid
379:27 - panel with credentials we have it there
379:29 - we use it with aws to log into there and
379:32 - we use it to log into azure so it has a
379:34 - lot of flexibility and if you're
379:35 - building out applications for
379:37 - enterprises they're likely using a d and
379:39 - so this is the reason why everybody
379:41 - adopts it or needs to understand it so
379:43 - it's a service i really really do want
379:44 - you to understand and know azure active
379:46 - directory comes in four editions we have
379:48 - the free tier and by the way each uh
379:52 - tier that goes up has the features
379:54 - before it but free has mfa sso basic
379:57 - security usage reports and user
379:58 - management then you have the office 365
380:01 - apps which is uh revolves around if
380:03 - you're using that suite so you have
380:06 - company branding sla two sync between on
380:08 - premise and cloud and then the premium
380:10 - tiers which really comes into enterprise
380:12 - or
380:13 - or on-premise hybrid architecture so
380:15 - hybrid architectures advanced group
380:17 - access conditional access premium two
380:19 - identity protection and identity
380:21 - governance only thing i don't like about
380:23 - azure ad is that you can't really create
380:25 - your
380:26 - custom access controls unless you have
380:27 - premium one in premium two but that's
380:28 - just how they do it so there you go
380:31 - [Music]
380:35 - so let's take a look at the use case for
380:36 - azure id and we basically covered it in
380:38 - the introduction but i just want to
380:40 - reiterate it in a different way with a
380:42 - bit of a visual uh so that it really
380:44 - helps it sink into your uh brain there
380:46 - so azure ad can authorize and
380:48 - authenticate to multiple sources so it
380:50 - can authenticate to your on-premise id
380:52 - to your web application allow users to
380:54 - log in with
380:55 - ipds
380:57 - so identity providers could be like use
380:59 - facebook or google login
381:01 - you can use it with office 365 or azure
381:04 - microsoft and so just a visual here
381:07 - notice that uh we have azure id and
381:09 - using azure ad connect we can connect to
381:12 - on-premise through uh app registrations
381:14 - we're able to connect our web
381:16 - application to azure id with external
381:19 - identities we can
381:21 - use facebook or google uh a
381:24 - login and then for cloud applications we
381:27 - can connect to office 365 or microsoft
381:30 - azure
381:31 - [Music]
381:36 - so active directory existed way before
381:38 - azure and so let's just do a quick uh
381:40 - rundown of the history so we have an
381:42 - idea of what we're looking at so
381:44 - microsoft introduced active directory
381:46 - domain services in windows 2000 to give
381:48 - organizations the ability to manage
381:50 - multiple on-premise infrastructure
381:52 - components and systems using a single
381:54 - identity per user so it's been around
381:56 - for 20 years and azure ad takes this
381:58 - approach to the next level by providing
382:00 - organizations with identity as a service
382:02 - so i-d-a-a-s solution for their apps
382:06 - across uh cloud and on-premise and both
382:09 - versions are still used today because
382:10 - they just have different utility and so
382:13 - we have active directory which is for on
382:14 - premise and then you have azure id which
382:16 - is just the cloud hosted version and in
382:19 - many regards these can also be connected
382:21 - together
382:22 - but there you go
382:24 - [Music]
382:28 - so i want to cover some active directory
382:30 - terminology and the honest truth is that
382:33 - for azure you're not going to be uh
382:35 - too worried about these things but
382:36 - they're going to come up in the
382:37 - documentation you're going to kind of
382:39 - wonder what they are and so i just
382:40 - wanted to uh tell you about these up
382:43 - front even though they're not core to
382:44 - study uh so that it just really rounds
382:46 - out your active directory knowledge
382:47 - because active directory is such a core
382:48 - service to azure microsoft products you
382:50 - you should know these things uh so the
382:52 - first thing is all about domain so a
382:54 - domain is an area of network organized
382:56 - by a single authentication database an
382:58 - active directory domain is a logical
383:01 - grouping of 80 objects on a network so
383:03 - just think of it a way as you know how
383:04 - you have resource groups to logically
383:06 - group your azure resources domains are a
383:08 - logical grouping for your 80 objects
383:11 - then you have a domain controller a
383:13 - domain controller is a server that
383:15 - authenticates user identities and
383:16 - authorizes their access to resources
383:18 - very common to have multiples of these
383:21 - because you want to have redundant uh
383:23 - domain controllers so you can log in or
383:25 - availability you're launching domain
383:27 - controller nearby so people can log into
383:29 - different places uh so definitely uh
383:31 - very core to active directory then you
383:33 - have the domain computer this is a
383:35 - computer that is registered with a
383:36 - central authentication database
383:38 - and a domain computer would be
383:41 - an id object
383:43 - so then you have ad objects so this is
383:45 - the basic element of active directory so
383:47 - you have users groups printers computers
383:49 - share folders etc then you have a group
383:51 - policy object a gpo this is a virtual
383:53 - collection of policy settings it
383:55 - controls what an ad object has access to
383:57 - you have your organizational units this
383:59 - is a subdivision within active directory
384:01 - into which you can place users groups
384:03 - computers and other organizational units
384:06 - so it's just another way of doing
384:08 - logical grouping
384:10 - then you have a directory service and
384:12 - this is this provides a method for
384:13 - storing directory data and making this
384:16 - data available to network users
384:18 - administrators a directory service runs
384:20 - on a domain controller so there you go
384:22 - that is the rundown of active directory
384:24 - terminology and again hopefully when you
384:26 - see it in the documentation you can
384:28 - refer back to this or you'll have a
384:30 - better understanding of all the
384:31 - components i would have loved to have
384:32 - made a diagram but i just couldn't find
384:34 - an example of one and so uh i mean i
384:37 - feel like there could be a really good
384:38 - picture for all this stuff
384:40 - [Music]
384:44 - so let's talk about
384:46 - the term tenant and a tenant represents
384:48 - an organization in an active directory
384:51 - and a tenant is dedicated to the azure
384:53 - ad service instance a tenant is
384:55 - automatically created when you sign up
384:57 - for either microsoft azure or microsoft
385:00 - intune or microsoft 365 and each
385:04 - or azure ad tenant is distinct and
385:05 - separate from other azure ad tenants and
385:08 - so if you uh if you were in um
385:12 - azure ad and you clicked on your tenant
385:14 - information that's that's basically what
385:15 - that is right so that's my exam pro one
385:17 - and it has its own special tenant id
385:19 - and we can see that it's licensed for
385:21 - office 365 and so that tells you that
385:23 - i'm using the office 365 uh tier of
385:27 - azure ad
385:32 - so remember that the domain controller
385:34 - is the server that users are going to be
385:36 - using to authenticate to the directory
385:38 - service
385:39 - and so when you create an active
385:40 - directory azure sets one up for you but
385:43 - there's some cases where you might want
385:44 - to set one up yourself and the reason
385:46 - why is that you could be like on like an
385:48 - enterprise where you already have your
385:50 - own active directory on premise but
385:52 - you've decided that you want to move it
385:54 - over to azure ad
385:56 - because you just want a fully managed
385:57 - active directory and uh you want to tap
386:00 - into the cloud but the thing is that
386:02 - some domain services those are features
386:04 - on your domain controller just might not
386:06 - be available and that's where you're
386:07 - going to need to set up your own domain
386:09 - controller and that's where azure active
386:10 - directory domain services come into play
386:13 - because these provide managed domain
386:15 - services
386:16 - and so they have managed domain services
386:18 - such as domain joins
386:20 - group policies uh ldaps uh
386:24 - kerb b ross never can say that properly
386:26 - ntlm authentication and so the great
386:29 - thing is here you can have these domain
386:31 - services but you're not going to have to
386:33 - deploy them manage them patch them
386:35 - they're just going to work so there you
386:37 - go
386:42 - so let's take a look at azure ad connect
386:44 - and this is a hybrid service that is
386:45 - used to connect your on-premise active
386:47 - directory to your azure account so azure
386:49 - ad connect allows for seamless single
386:52 - sign-on from your on-premise
386:53 - workstations to microsoft azure and
386:55 - azure ad connect has the following
386:58 - features we have password hash
386:59 - synchronization so this is a sign in
387:01 - method and it synchronizes the hash
387:03 - of a user on on-premise ad password with
387:06 - azure ad then you have password
387:08 - authentication this is another sign-in
387:10 - method that allows users to use the same
387:11 - password on-premise and in the cloud
387:13 - that's the one i like
387:14 - uh then moving on from password
387:16 - authentication we have federation
387:18 - integration so this is a hybrid
387:20 - environment using an on-premise adfs
387:22 - infrastructure for certificate renewal
387:25 - then you have synchronization this is
387:26 - responsible for creating users groups
387:28 - and other objects and ensures on-premise
387:30 - and cloud data matches so you have the
387:31 - same ad objects both on-prem and in
387:33 - cloud and then you got health monitoring
387:36 - and this is a robust monitoring
387:38 - service and it provides a central
387:40 - location in the azure portal to view
387:41 - this activity and it's called azure 80
387:44 - connect health
387:45 - so yeah the big takeaway is that azure
387:47 - ad connect is used to create a hybrid
387:49 - connection
387:50 - [Music]
387:54 - so now let's take a look at some of the
387:56 - 80 objects starting with users so users
387:58 - represent an identity for a person or
388:00 - employee in your domain and a user has
388:02 - login credentials and can use them to
388:05 - log into the azure portal so here i am a
388:07 - user and you can see it shows how many
388:08 - times i've logged in and i'm part of
388:10 - different
388:11 - ad groups
388:12 - and so you can assign roles and
388:14 - administrative roles to users you can
388:16 - add users to groups you can enforce
388:18 - authentication by
388:20 - like with mfa you can track user
388:22 - sign-ins as you can see on the
388:23 - right-hand side you can track device
388:25 - devices users log in
388:27 - and allow or deny devices
388:29 - you can assign microsoft licenses
388:32 - azure id has two kinds of users we have
388:34 - users that's a user that belongs to an
388:36 - organization and guest users this is a
388:38 - guest is a user that belongs from
388:40 - another organization and we'll cover uh
388:42 - azure 80 roles
388:44 - in the roles section here
388:46 - because that is what's that you're going
388:48 - to be using to apply to these users
388:50 - [Music]
388:54 - so groups in azure id lets resource
388:56 - owners assign a set of access
388:58 - permissions to all members of the group
388:59 - instead of having to provide the rights
389:01 - one by one and so on the right hand side
389:03 - here you can see i have a bunch of
389:04 - groups in exam pro and groups can
389:07 - contain owners and owners have
389:08 - permissions to add or remove members and
389:10 - then the members have rights to do
389:12 - things okay and so for assignment you
389:15 - can assign roles directly to a group you
389:17 - can assign applications directly to a
389:19 - group and to request join groups so the
389:22 - group owner can let users find their own
389:24 - groups to join instead of assigning them
389:26 - to them and the owner can set up the
389:28 - group to automatically accept all users
389:30 - that join or require approval this is
389:32 - really great when you just want people
389:34 - to do the work themselves as opposed to
389:37 - having to do all that manual labor of
389:38 - adding them to groups
389:43 - let's talk about how we're going to give
389:45 - users rights to access resources and
389:48 - there are four different ways to do that
389:49 - the first is direct assignment this is
389:51 - where the resource owner is going to
389:52 - directly assign the user to the resource
389:54 - then you have group assignment this is
389:56 - where the resource owner assigns a a
389:58 - group to the resource which
390:00 - automatically gives all group members
390:02 - access to the resource then you have
390:04 - rule base assignment this is resource
390:06 - owner this is where the resource owner
390:08 - creates a group and uses a rule to
390:09 - define which users are assigned to a
390:11 - specific resource and then you have
390:13 - external uh authority assignment this is
390:15 - this access comes from an external
390:16 - source such as an on-premise directory
390:18 - or sas application and i just want you
390:20 - know that there's four different ways to
390:21 - do it so
390:22 - to get access to resources
390:24 - [Music]
390:28 - let's talk about external identities so
390:30 - external identities in azure id allows
390:32 - people outside your organization to
390:33 - access your apps and resources while
390:35 - letting them sign in
390:37 - and use whatever identity they prefer so
390:39 - your partners distributor suppliers
390:41 - vendors or other guests can bring their
390:42 - own identities such as google or
390:45 - facebook
390:46 - you can share apps with external users
390:48 - that's for b2b stuff
390:50 - if you develop apps intended for azure
390:52 - ad tenants uh for singleton or
390:54 - multi-tenant you can do that as well uh
390:55 - you can develop white label apps for
390:57 - consumers and customers this would be
390:59 - like azure ad b2c
391:02 - so there you go
391:07 - hey this is andrew brown and welcome to
391:09 - the az104 follow along uh and the first
391:12 - video in our journey on learning on how
391:14 - to be a good administrator is learning
391:16 - about azure active directory tenants and
391:19 - so tenants is the way
391:21 - we are going to group all of our users
391:23 - within an organization so this is like
391:25 - the logical division of organizations
391:28 - within uh
391:29 - azure and so let's just make our way
391:32 - over to where tenants are and they're
391:33 - under azure active directory a.d
391:36 - we'll click there and so the idea is
391:38 - that when you first created your azure
391:40 - account you already have a single tenant
391:43 - and that is your organization so here i
391:44 - can have i see i have one here called
391:46 - the hush nook
391:48 - and down below i have a license applied
391:51 - to it so this is the azure 80 premium p2
391:54 - but the idea is you can create multiple
391:55 - ones and so these can act as different
391:59 - organizations and really what a tenant
392:02 - is it's just an instance of azure active
392:04 - directory uh that's exactly what it is
392:06 - underneath but let's go ahead and go
392:08 - create a new one and we have two options
392:10 - here we can create uh uh an azure active
392:12 - directory and i think what really is
392:15 - implied here is that it's a b2b uh when
392:17 - you read the documentation they talk
392:18 - about b2c and b2b b2b means business to
392:21 - business meaning that you're joining
392:23 - businesses and other businesses together
392:25 - and then b2c means business to consumer
392:27 - and so up here in the description they
392:28 - say uh the b2c enables users uh to
392:31 - access applications published by your
392:33 - organization and share administration
392:35 - experiences all right so what we'll do
392:37 - is we'll go to the next step here and
392:39 - we're gonna need to name our
392:40 - organization um i'm gonna name it
392:44 - starfleet
392:46 - and
392:47 - we need our initial domain notice it's
392:48 - going to be
392:49 - on microsoft.com so we will just lower
392:52 - case that if it's already used it might
392:53 - complain we'll see if i can take that
392:55 - and it's already taken so we'll say um
392:59 - uh the uss starfleet
393:02 - uh oh it must be numeric
393:05 - take that out there
393:07 - there we go uh u and three s's that's
393:10 - not easy at all but that's okay i'm
393:12 - going to change my default location to
393:13 - canada i think it still deploys in the
393:16 - us anyway
393:17 - so it's still going to say united states
393:19 - where the data center is going to be
393:21 - we're going to go ahead and hit review
393:22 - create and give it a moment
393:25 - it's saying i'm missing some information
393:27 - there
393:28 - already used by another directory so i
393:30 - still can't call it that
393:32 - boy oh boy so i'll just say starfleet um
393:36 - uh 1984.
393:40 - and i think we should be able to do that
393:42 - and we'll go ahead and create it and so
393:43 - you're gonna have to play around and
393:44 - find the name that's gonna work for you
393:47 - and then we'll just wait for it to
393:48 - create this tenant shouldn't take too
393:49 - long here
394:00 - [Music]
394:04 - and after waiting there a couple of
394:05 - minutes down the right hand side it says
394:07 - it created the tenant so we can just
394:10 - proceed
394:11 - back here and so i'm just going to go
394:13 - back to azure active directory at the
394:15 - top i find it just easier to always
394:17 - search and go back this way
394:20 - and so we'll just wait a moment for this
394:22 - to load
394:24 - but we're still in our our hussnick
394:26 - azure active directory that's my default
394:28 - one there but let's say i wanted to go
394:30 - ahead and switch a tenant i can go now
394:31 - and just switch and so here i have my
394:33 - exam pro one hush nook and starfleet i'm
394:36 - going to go ahead and switch it over to
394:37 - this one
394:39 - and so essentially now i have all these
394:41 - isolate users so i can have different
394:43 - users in my in these other tenants and
394:45 - that's the way you would isolate them
394:46 - out
394:47 - notice that this one is azure ad free uh
394:50 - and so the thing with
394:52 - azure active directory is that the
394:54 - higher up the tier you go the more
394:56 - functionality you have and one that is
394:58 - very popular especially with enterprises
395:00 - is going to have the
395:02 - the p2 tier premium two i just want to
395:04 - show you how you'd go about upgrading to
395:06 - that it does cost money but you also get
395:08 - a free trial i don't believe that you
395:10 - are billed uh during the the time of the
395:12 - trial uh at the end of it then you have
395:14 - to enter in or make an explicit purchase
395:17 - of it let's go ahead here and see if i
395:18 - can do it um it might not give me trial
395:20 - because i'm already running another one
395:21 - but let's go find out so i clicked on
395:23 - licenses on the left hand side we'll go
395:25 - to all products and then we have try and
395:27 - buy and so on the right hand side it
395:29 - says azure 80 premium 2 and we can click
395:33 - on free trial and it's going to tell you
395:34 - why you'd want p2 such as multi-factor
395:38 - authentication policy driven management
395:40 - and end-user self-service
395:43 - and so i'm going to just go ahead and
395:44 - upgrade that you don't have to do that
395:45 - you can just watch me do that here and
395:47 - so i'll just hit activate it's just
395:49 - going to go activate that in my account
395:50 - and now it's activated so if i go back
395:53 - uh to azure active directory here i'll
395:55 - probably click home but i'm just going
395:56 - to go back this way
395:58 - it's a lot more safe that way
396:00 - eventually it should update here so it
396:02 - says azure 80 free
396:04 - but really i've upgraded it's just the
396:06 - delay in the console
396:08 - and so after a few refreshes it will
396:10 - probably show up
396:13 - but what that's going to do is it's
396:14 - going to enable some additional features
396:16 - they don't really matter so much for the
396:18 - course i mean maybe i could show dynamic
396:20 - role assignment if i try to remember how
396:23 - that works um let me think for a moment
396:27 - i think it's under rolls
396:35 - but anyway uh so
396:37 - you know that's how you do that um i
396:39 - just wanted to show you how to create a
396:41 - tenant and then just able to switch
396:42 - between them there
396:44 - [Music]
396:48 - so we create ourselves a tenant let's go
396:50 - ahead and actually create some users and
396:52 - just before we proceed forward to create
396:53 - a user i just want to point out you can
396:55 - tell what tenant you are in if you look
396:57 - in the top right corner so here i'm in
396:58 - my starfleet tenant and there's actually
397:00 - i think you can switch between here
397:04 - yeah that's an easier way to switch
397:06 - between your directories uh but
397:08 - generally that's not how i do it
397:10 - i'll go to the top here and just type in
397:12 - azure active directory
397:14 - and when i want to switch a tenant i
397:16 - just click the switch tenant button and
397:17 - click on what i want but we'll make our
397:19 - way back to the starfleet tenant that i
397:22 - have here here you can see my
397:23 - information and i've obviously activated
397:26 - azure 80 premium p2
397:28 - but before we go ahead and create a user
397:29 - we're going to have to create a group
397:30 - and i'll show you why if i'm going to go
397:32 - ahead i'm just going to speed through
397:33 - this for a second if i scroll on down i
397:36 - have this option where i need to create
397:37 - a assign it to a group i guess i don't
397:39 - have to but the problem is i don't have
397:41 - one and so before i create my user let's
397:44 - go ahead and create ourselves a new user
397:46 - group and so we'll go ahead and create a
397:48 - new group here
397:50 - and we have two options we have security
397:52 - and microsoft 365. it explains right
397:54 - here the difference so 365 is really for
397:57 - giving access to mailbox calendar files
398:00 - sharepoint we're not doing that stuff
398:01 - here we're just sticking with regular
398:03 - security groups which is for azure stuff
398:05 - we'll name this developers
398:08 - and then we'll name this developers
398:10 - and notice here that uh the membership
398:12 - type i have a drop down if you're on the
398:14 - free tier this is going to be grayed out
398:16 - but uh this dynamic user is part of p2
398:19 - and so this allows us to add a dynamic
398:21 - query
398:22 - and the idea here is that if i start
398:24 - having users and i just say if a user
398:27 - is from canada
398:30 - then they'll automatically be added to
398:31 - this group and so that's the query that
398:32 - gets outputted but we're not going to
398:34 - make a dynamic group today
398:36 - we'll just make our way back here to the
398:38 - new group and we'll just manually assign
398:41 - stuff all right
398:42 - i'll go ahead there and create that user
398:45 - our that group there and now that we
398:46 - have that group let's make our way over
398:48 - to users so we'll go back here i'm going
398:51 - to make myself a new user
398:54 - and on the top here we'll go here
398:56 - and we'll name this one kevin
399:00 - and we'll have that lower case
399:03 - and we will call them kevin uxbridge
399:08 - all right
399:10 - and you'll notice here it will auto
399:12 - generate a password it's four letters
399:13 - and four numbers i don't find this
399:15 - personally very secure but the idea here
399:17 - is that a user is going to reset that
399:19 - password right away so it's not a big
399:20 - deal and that's very easy to remember
399:22 - and now i can go here and assign groups
399:26 - and if i want to sign the role i can do
399:28 - that here so we'll open that up we have
399:30 - a bunch of different options maybe we
399:32 - want these two roles here
399:35 - and then we have some additional
399:36 - information i'll go ahead and create
399:37 - this user and so now this user exists
399:40 - what's interesting is that i can go
399:42 - ahead and delete users and then recover
399:44 - them the groups have this as well where
399:45 - if you go here and i actually have a
399:47 - user i deleted previously
399:49 - so it just takes some time to show up
399:51 - but if you go ahead you can hit the
399:53 - delete user here
399:54 - and then uh
399:56 - they will remain in here for uh 30 days
399:58 - and then they'll automatically be be
400:00 - deleted so if someone made a mistake
400:02 - this is a great opportunity to bring
400:03 - them back and so i can just go check box
400:06 - on richaun here and we can just bring
400:08 - her on back restore that user
400:12 - all right and again you know this is
400:14 - sometimes a bit delayed so we might have
400:16 - to hit refresh
400:18 - there she is and so
400:20 - uh you know that is the whole group and
400:22 - user stuff there
400:24 - [Music]
400:30 - okay now let's take a look at the
400:31 - concept of guest users so let's make our
400:34 - way back to users within our tenant and
400:36 - there's this button here for new guest
400:38 - users the idea between behind guest
400:40 - users is that we're able to invite
400:43 - other users from other tenants and this
400:45 - is a much simpler process than using
400:47 - federation federation is the concept of
400:50 - joining active directories together so
400:53 - if there is an on-premise or other
400:55 - external active directory they would
400:57 - somehow want to connect the two together
400:59 - and there's a lot of administrative
401:00 - burden with that but with if everybody's
401:02 - already using azure active directory
401:05 - it's as simple as adding a guest user so
401:07 - what we're going to do is go back to
401:09 - home here i'm just going to go back to
401:11 - azure active directory and we're going
401:13 - to switch tenants i'm going to go back
401:14 - to my hush nook one here
401:16 - and you probably won't see that that's
401:18 - just showing up because it wants me to
401:19 - use mfa
401:21 - and i'm going to go here to users and
401:23 - we're going to go ahead and create
401:24 - ourselves a new user i'm going to call
401:25 - this one a hush nook
401:29 - and we will name it simply that
401:33 - and let's make note of the actual uh
401:36 - email there that's pushnuk
401:39 - onmicrosoft.com
401:41 - and we'll go ahead and create that
401:44 - and now what i'm going to do is i'm just
401:45 - going to copy
401:48 - that email there
401:50 - okay
401:51 - i think i got it yes i do good
401:53 - and we're going to make our way back to
401:55 - azure active directory we're going to
401:56 - switch tenants
401:58 - back to starfleet
402:02 - and we are going to go to users and
402:03 - we're going to add a new guest user i'm
402:05 - going to invite a user
402:07 - and i'm just going to put in their email
402:08 - there
402:11 - and we will say hush
402:18 - and uh we'll just put that both
402:22 - and we you here we we can assign them to
402:24 - groups and roles it's all the same story
402:26 - here i'm just going to go ahead and
402:27 - invite them
402:32 - and so now they've been sent off an
402:33 - invitation notice that they show up
402:34 - under as guest all right so there you go
402:38 - [Music]
402:42 - let's take a look at bulk operations
402:44 - let's say we needed to bring in a bunch
402:46 - of different people into our actual
402:47 - account here and so we have this option
402:49 - of bulk create and if you look on the
402:51 - right hand side we can go ahead and
402:53 - download a csv template that's ready to
402:55 - use i'm going to double click that i
402:56 - already have excel installed here
403:00 - and off the bat what you can see is we
403:02 - have a bunch of options
403:05 - and that we can fill in
403:09 - and here we have one example there
403:11 - and so i think that we could try to give
403:13 - this a go here but we're going to have
403:15 - to make sure we use the same
403:19 - principle username or here thing here so
403:20 - i'm just going to grab this here
403:24 - and let's just make a few people
403:28 - so we need our principal here
403:32 - we'll just say picard
403:36 - whoops
403:37 - did not like that i copy that
403:42 - and we'll do up here we'll say data
403:48 - card
403:50 - data
403:53 - card picard
403:55 - data data
403:58 - just see what else i need we're going to
403:59 - need a password i'm just going to use
404:01 - the same one here
404:03 - set that as no
404:05 - i don't think any of the other things
404:07 - there are
404:08 - needed we'll go ahead and save that
404:11 - and i'm just going to save this
404:13 - somewhere convenient
404:16 - okay
404:18 - and what i'm just going to do is pull
404:20 - this up off screen here
404:22 - i'm just looking for where i saved it
404:24 - just give me a moment
404:26 - and there it is i'm just going to drag
404:28 - it on in here i think i can drag it in
404:30 - like that
404:33 - maybe not as easy
404:35 - just go back to uh it's under my
404:38 - downloads
404:41 - here we go
404:44 - and we'll just hit submit
404:47 - we'll give that a moment shouldn't take
404:48 - too long
404:51 - you can probably view the status here if
404:53 - we click that there and now we're just
404:54 - under bulk operation results i'm just
404:56 - going to give it a refresh
404:57 - you can see that it's not completed yet
405:00 - i can click into it
405:04 - and we'll just have to wait a little bit
405:05 - here okay
405:08 - oh it looks like it just succeeded i'll
405:09 - give it a nice refresh it's going to
405:11 - tell me how did it turn out to success
405:13 - zero failures total requests
405:16 - let's click into that so we can see what
405:18 - it did that looks pretty darn good to me
405:21 - so that is pretty good we'll make our
405:22 - way back to our users
405:26 - and you can see that they're on mass
405:27 - imported so there you go
405:29 - [Music]
405:34 - let's take a look at how we can enable
405:36 - multi-factor authentication for our
405:37 - users and so multi-factor stands for mfa
405:41 - you're probably used to it where it's a
405:42 - secondary step to confirm your identity
405:44 - before logging in either via a phone or
405:47 - a hardware device such as an ubi key
405:50 - and so there's this button here this is
405:52 - multi-factor authentication and it could
405:54 - be disabled for you it very likely is if
405:56 - you're on the uh the free version of
406:00 - your azure ad just going back up here
406:02 - one level
406:04 - you can tell based on what that is here
406:06 - so i'm using azure 80 premium p2 that's
406:09 - definitely going to allow me to have mfa
406:11 - so if you just don't want to turn on p2
406:13 - or it's trying to charge you money don't
406:15 - worry about it just watch me do it here
406:17 - you pretty much learn what you need to
406:18 - know so we'll just go ahead and click
406:20 - that button it's going to bring us to a
406:21 - different screen here
406:23 - and here we have our users and you can
406:25 - see multi-factor authentication is
406:26 - turned off for all of them so we can go
406:28 - ahead here and turn it on for a single
406:30 - one if i click enable
406:33 - and so that enables it for kevin then we
406:35 - have some other additional options like
406:37 - manage user settings and here you can
406:39 - see we have some extra options so
406:41 - require selected users to provide
406:42 - contact methods again delete all
406:44 - existing app passwords generated by the
406:46 - selected users restore multi-factor
406:49 - authentication on all remembered devices
406:51 - so just some additional things another
406:53 - thing you will want to check out here
406:54 - it's not very obvious but we have a
406:55 - service settings button up here
406:58 - and this will give us more additional
406:59 - options to enforce for our users so
407:02 - first we have app passwords so this
407:04 - allows users to create app passwords to
407:05 - sign into non-browser apps so things
407:08 - that are not part of the website here
407:10 - and so you can white list some ip
407:12 - addresses for them uh so that they are
407:15 - trusted from those locations then for
407:17 - verification options this is something
407:18 - that's important we have some options
407:20 - like call to phone so
407:22 - it will actually
407:24 - to do the mfa will actually call your
407:25 - phone and and you'll tell you the
407:28 - letters and numbers that you enter in or
407:29 - it can send you a text message or it can
407:31 - notify you through the mobile
407:33 - application so you can install the
407:34 - companion app
407:35 - from the android or ios store or the
407:38 - windows store i imagine there's a
407:39 - windows store too um and then there is
407:42 - the verification code and mobile app on
407:44 - our hardware token so hardware tokens
407:46 - could be with an ub key right
407:48 - then you can say remember a multi-factor
407:50 - authentication on trust devices for x
407:52 - amount of days that way they don't have
407:53 - to enter it all the time
407:55 - because it might get annoying for some
407:56 - users honestly for me i want everyone to
407:59 - enter it every single time because
408:00 - that's just how i am but let's make our
408:02 - way back here and see how we can do some
408:04 - bulk assignment so what i'll do is i
408:07 - want to enable for a few people here so
408:09 - if i go up to bulk update
408:11 - i can download a sample file
408:14 - and that's going to go ahead and
408:15 - download that
408:18 - and you can see i actually did this
408:19 - previously here so i'll just copy it and
408:22 - what we'll do is just paste that in
408:24 - there
408:25 - so we're just pasting in yeah that's
408:27 - fine we're just pasting in
408:29 - uh their their name right and the mfa
408:31 - status you can always get the name from
408:34 - here just back on the list there
408:36 - and so once we have saved that file we
408:38 - can go ahead and upload that
408:43 - they're the same here so it's not a big
408:44 - deal
408:46 - and we'll just wait a few seconds here
408:48 - this is pretty darn quick
408:53 - usually doesn't take minutes sometimes
408:55 - it takes seconds
408:56 - but we'll just give it a moment here
409:08 - and there we go so that was a long wait
409:10 - i don't know why but i verified the two
409:12 - there and so if we go next now those
409:14 - will be now enabled there's also this
409:16 - option to enforce so let's go there uh
409:19 - after multi-factor authentication is
409:21 - forced users will need to create app
409:23 - passwords to use non-browser
409:24 - applications such as outlook or links
409:26 - and so that's a great option to have so
409:28 - i'll just go ahead and enable that as
409:29 - well
409:31 - so now that's enforced and so that's all
409:33 - you really need to know about mfa
409:35 - [Music]
409:40 - okay so we're back in our tenant here
409:42 - and what i want to do now is actually
409:44 - set some
409:45 - abilities for users to reset their
409:47 - password so they don't have to come and
409:48 - bother the administrator all the time so
409:50 - under your tenant on the left hand side
409:52 - what you want to look for is password
409:54 - reset
409:55 - and now we have a bunch of options here
409:57 - for resetting password
409:59 - so we're going to have to go ahead and
410:00 - enable it we can enable it based on a
410:03 - per group level
410:04 - such as developers but what i'm going to
410:06 - do is set it for everybody here which i
410:08 - think is a lot better option
410:10 - and then down below we have a lot of
410:12 - things we can do here so looking at
410:13 - authentication methods we can choose the
410:15 - number of methods required to reset
410:18 - so maybe they need to use two options in
410:20 - order to reset their password which is a
410:22 - lot more secure
410:23 - and then we can checkbox on
410:26 - possible methods they can use then we
410:29 - could require a certain number of
410:30 - questions uh required here so three and
410:32 - three you can see i was setting
410:33 - something earlier here you can create
410:35 - your own custom questions and then so
410:37 - we'd go ahead here and just add a new
410:39 - one so
410:40 - um you know
410:41 - what is the best um
410:45 - you know song
410:46 - we'll go ahead and hit add and the idea
410:48 - is that you can add uh them there
410:51 - there's also a bunch of predefined ones
410:52 - so if you have a hard time think of any
410:54 - you can definitely checkbox on some here
410:57 - right
411:01 - and so now we have a large pool of
411:03 - questions that can be used there we'll
411:04 - go ahead and save that
411:06 - then under registration we can say
411:08 - require users to register when signing
411:10 - in
411:11 - i think that's a great option to have
411:12 - there and then you can have number of
411:14 - days before users are asked to reconfirm
411:15 - their authentication method and that's
411:17 - defaulted to 180 days so i would
411:20 - probably leave that as on
411:23 - and we can notify users on password
411:25 - reset notify all admins when other
411:27 - admins reset their password very useful
411:28 - this was probably turned off so i'd go
411:30 - ahead and turn that on there
411:32 - you can customize a help desk link so
411:34 - the idea here is that uh when someone's
411:36 - having trouble uh they might say contact
411:38 - your administrator well how do you
411:40 - contact administrator will you provide
411:41 - that information either a url or an
411:43 - email so i've just provided my email
411:45 - there there's some on-premise
411:46 - integrations never used this stuff
411:48 - before because i'm never working with
411:49 - on-prem
411:50 - but you know that stuff is pretty
411:52 - straightforward as well and you have
411:54 - usage and insights so you can actually
411:56 - see when people are resetting their
411:58 - passwords and things like that and what
411:59 - is being used um so that seems pretty
412:02 - darn good but that's pretty much it to
412:04 - it um yeah so there you go
412:07 - [Music]
412:11 - hey this is andrew brown from exam pro
412:13 - and we're looking at the azure active
412:14 - directory cheat sheet this one's three
412:16 - pages long so get ready for it and let's
412:18 - jump into it so active directory just
412:21 - active directory alone which is a d it's
412:23 - microsoft's identity and access
412:24 - management service helps your employees
412:26 - sign in and access resources then you
412:28 - have azure ad and this is the same thing
412:31 - except it's the cloud-based version
412:33 - and one term that you'll hear a lot with
412:35 - this is identity as a service which just
412:37 - means that it's like the serverless or
412:38 - hosted version of this so you don't have
412:40 - to think about it or manage servers
412:42 - azure directory comes into four editions
412:44 - you got the free one the office 365 apps
412:47 - premium 1 which is also known as p1
412:49 - premium 2 which is p2 it's good to know
412:52 - the differences between
412:54 - what features you have here so you
412:55 - should know all the details in between
412:57 - here so i highlighted conditional access
412:59 - because that's an important one on the
413:01 - exam to know azure ad can authorize and
413:03 - authenticate to multiple sources so if
413:05 - you're doing on-prem you're going to be
413:06 - using azure ad connect if it's for a web
413:09 - app you're using app registers
413:10 - registration sorry if you're using
413:12 - facebook or google you're using external
413:14 - identities if you're
413:16 - you can also connect to office 65 or
413:18 - azure microsoft okay
413:21 - for active directory terminologies uh
413:23 - when we're talking about a domain domain
413:24 - is an area of network organized by a
413:26 - single authentication database an active
413:28 - directory domain is a logical grouping
413:29 - of 80 objects on a network then you have
413:32 - domain controller
413:33 - commonly abbreviated as dc domain
413:35 - controller is a server that
413:36 - authenticates user identities and
413:38 - authorizes their access to resources you
413:40 - have a domain computer a computer that
413:42 - is registered with a central
413:43 - authentication database a domain
413:45 - computer would be an ad object then you
413:47 - have 80 objects this is the base element
413:50 - of active directory uh things like users
413:52 - groups printers etc you have a gpo this
413:55 - is a virtual collection of policy
413:57 - settings
413:58 - then you have organizational units this
414:00 - is a subdivision of your ad which you
414:02 - can place users groups computers etc in
414:04 - you have directory service
414:06 - such as active directory domain service
414:08 - adds which provides a method of storing
414:11 - directory data and making the data
414:13 - available to network users a directory
414:15 - service can run on a domain controller
414:17 - that's only page one we're on to page
414:19 - two so a tenant represents an
414:21 - organization a tenant is dedicated uh
414:24 - it's a dedicated azure 80 service
414:25 - instance a tenant is automatically
414:27 - created when you first sign up either
414:28 - with microsoft azure
414:30 - intune or 365. each ad tenant is
414:33 - distinct and separate from other tenants
414:35 - when you perform a lift and shift
414:38 - of 80 to azure not all the features are
414:40 - supported in that case you're going to
414:41 - be using adds so adds provides managed
414:44 - domain services such as domain joins
414:46 - group policies lightweight directory
414:48 - access uh and i never can say that word
414:50 - so i'm not going to try
414:52 - azure ad connect has the following
414:54 - features it has password hash
414:56 - synchronization pass through
414:57 - authentication federation integration
414:59 - synchronization health monitoring you've
415:01 - got the concept of users so that's an ad
415:04 - object remember so an identity of a
415:06 - person or employee in your domain a user
415:08 - has login credentials and can use them
415:09 - to log into the azure portal azure ad
415:11 - has two kinds of users we got a user
415:13 - that belongs to your organization and
415:15 - guest users a user that belongs to a
415:17 - different organization groups let
415:19 - resource owners assign a set of access
415:21 - permissions to all the members of the
415:23 - group instead of having to provide the
415:25 - rights one by one and the group contains
415:28 - owners and it contains members just a
415:31 - moment here
415:32 - there we go just want to make sure i'm
415:33 - recording the videos that i don't end up
415:35 - finding halfway i didn't do it so we got
415:37 - assignments so you can assign roles
415:38 - directly to a group and you can assign
415:40 - applications directly to a group we're
415:42 - on to the last
415:45 - slide here in the cheat sheet told you
415:46 - this one was long request to join groups
415:49 - the group owner can let users find their
415:51 - own their own group to join instead of
415:53 - assigning them the owner can also set up
415:56 - set the group to automatically accept
415:58 - all the users or join to require
416:00 - approval there are four ways to assign
416:01 - resource access rights to your users
416:03 - direct assignment group assignment rule
416:05 - based assignment external authority
416:08 - assignment know all four of these please
416:12 - and so yeah there we go we're at the end
416:13 - here and that's all there is
416:19 - so let's take a look at the type of
416:21 - azure roles and i was honestly first
416:24 - confused with azure roles because i'm
416:26 - from aws and uh we only have one kind of
416:29 - role but azure happens to have three
416:31 - clients and there's definitely good
416:33 - reason for it so let's break that down
416:35 - and the first one is classic
416:36 - subscription administrative roles and
416:38 - this is the original role system now
416:40 - it's not something you're going to be
416:41 - really using but you should know about
416:43 - it because it still does exist then you
416:45 - have azure roles this is the
416:47 - authorization authorization system known
416:50 - as role-based access controls or rbac
416:54 - and it's built on top of azure's
416:56 - resource manager and then thirdly we
416:58 - have azure active directory also known
417:00 - as azure ad roles and these azure id
417:03 - roles are used to manage uh azure 80
417:06 - resources in a directory so let's jump
417:08 - into it
417:09 - [Music]
417:13 - so let's first talk about access
417:15 - controls or iam so im stands for
417:18 - identity access management it allows you
417:20 - to create and assign roles to users and
417:22 - so this is the general interface for im
417:26 - in azure
417:27 - and so we're going to have
417:29 - azure roles which is part of that uh
417:31 - role-based access system and so roles
417:34 - restrict access to resource actions
417:36 - known as operations and there's two
417:39 - types of roles for the azure role based
417:43 - access system you have built-in role so
417:45 - this is managed managed by microsoft and
417:48 - they are read-only and they're
417:49 - pre-created roles for you to use and
417:51 - then we have custom roles and these are
417:53 - a role created by you with custom logic
417:56 - and so the place you're going to find it
417:58 - there is you're going to see
417:59 - it says roles right there and that's
418:01 - what we're looking at down below is a
418:03 - bunch of roles so you see owner
418:04 - contributor reader those are predefined
418:06 - rules for you
418:08 - and so then you have role assignment and
418:11 - this is when you apply a role to either
418:13 - a service principal a user group a user
418:17 - and so that's going to be under uh the
418:19 - tab there such as you applying a role to
418:20 - somebody and then you have deny
418:22 - assignment so these block users from
418:24 - performing specific actions even if a
418:27 - role assignment grants them access and
418:29 - the only way to apply a deny assignment
418:32 - is through azure blueprint so this is
418:34 - just something where you're just setting
418:35 - up like guard rails to make sure that
418:37 - certain things are never used regardless
418:39 - of a role is applied
418:41 - [Music]
418:45 - so now let's take a look at classic
418:47 - administrators and this is the original
418:49 - roll system and you honestly want to be
418:51 - using the new rbac system whenever
418:54 - possible uh but the idea here is that um
418:57 - you're gonna go uh it's in the same
419:00 - place that we just saw the access
419:01 - control and there's a tab called classic
419:03 - administrator and in there
419:05 - you can set up uh
419:08 - administrators here and so you have
419:10 - three types of roles you have the
419:12 - account administrator this is the
419:13 - billing owner of the subscription and
419:15 - has no access to the azure portal you
419:18 - have the service administrator so this
419:19 - is the same access of a user assigned
419:22 - the owner role at the subscription scope
419:24 - and they have full access to azure
419:26 - portal and then you have the code
419:27 - administrator so this is the same
419:29 - access of a user who is assigned to the
419:31 - owner role the subscription scope so
419:33 - it's those very three simple roles and
419:35 - we actually have one because uh when
419:37 - baco who's the other andrew set up our
419:40 - azure account i think it just sets you
419:41 - up one so even though you're not really
419:44 - going to be using it i do believe that
419:45 - it still sets you up one when you make
419:47 - your account
419:48 - so that's just what it is but there you
419:50 - go
419:51 - [Music]
419:55 - so let's take a closer look here at
419:57 - azure role-based access controls because
419:59 - this is something we're definitely going
420:00 - to be using a lot and so role based
420:03 - access control helps you manage who has
420:05 - access to azure resources what they can
420:07 - do with those resources and what areas
420:08 - they have have access to and the idea is
420:11 - that you have a user and you want to
420:13 - assign them a role so you can use a role
420:14 - assignment and a role assignment's made
420:16 - up of three elements you have the
420:18 - security principle the role definition
420:20 - and the scope and we're going to look at
420:22 - those three things in a little bit more
420:23 - detail here in a second and there are
420:25 - four fundamental azure roles which we
420:27 - are going to learn and then azure rbac
420:30 - also includes over 70 built-in roles
420:32 - which we definitely do not need to go
420:34 - into great detail
420:36 - so let's take a look at
420:38 - one of those three elements first which
420:40 - is the security principle and this
420:42 - represents the identity requesting
420:44 - access to an azure resource and when we
420:46 - say identity that's just like a loose
420:47 - term for something and that something
420:49 - could be a user in your azure active
420:51 - directory a group which defines a set of
420:54 - users in your azure active directory a
420:56 - service principle so a security identity
420:59 - used by applications or services to
421:02 - access specified azure resources or a
421:05 - managed identity an identity in your
421:07 - active
421:08 - azure active directory that is
421:09 - automatically managed by azure so
421:11 - service principle basically in azure
421:12 - service and then managed identity is
421:14 - something in your azure active directory
421:17 - then we'll move on to scope and a scope
421:19 - is just a set of resources that
421:21 - access that assess
421:24 - the role of the role assignment applies
421:26 - to and so scope access controls at the
421:29 - management subscription or resource
421:31 - group level so what does that mean and
421:33 - we we have another slide on this i can't
421:34 - remember what section it's in but you
421:36 - have this breakdown of scope where you
421:38 - have management groups subscriptions
421:39 - resource groups or resources so when
421:41 - you're saying i'm setting a scope you're
421:42 - saying what is the scope is it on a
421:44 - management group is it on a particular
421:45 - resource is it a resource group and
421:47 - that's what we're trying to say there
421:50 - and then last the last element there is
421:52 - a role definition and this is a
421:53 - collection of permissions so a rule
421:55 - definition lists operations that can be
421:57 - performed such as read write and delete
422:00 - and roles can be high level like owner
422:03 - specific or like a virtual machine
422:05 - reader and so azure has uh built-in
422:09 - roles and we said there were four
422:11 - fundamental built-in roles and here they
422:13 - are it's owner contributor reader and
422:16 - user admin administrator so you want to
422:18 - know those four and then across the
422:20 - board you have those three
422:23 - operations read grant
422:25 - and then create update delete so you can
422:27 - see the owner can do everything the
422:29 - contributor can
422:30 - both read and create stuff they just
422:31 - can't grant access to other people the
422:34 - reader just has read only access and
422:35 - then a user access administrator is
422:37 - granting other users
422:39 - privileges but themselves are not
422:41 - creating anything all right
422:43 - [Music]
422:47 - so now let's take a look at the third
422:49 - type of roles and that's azure ad roles
422:52 - and so azure 80 roles are used to manage
422:55 - azure ad resources
422:57 - in the directory such as creating or
422:59 - editing users assigning administrative
423:01 - roles to others resetting user passwords
423:03 - managing user licenses
423:05 - and managing domains and so on the right
423:08 - hand side here
423:10 - what we can do is go into azure active
423:12 - directory in azure and under roles
423:14 - administrators we can see there is a
423:16 - bunch of roles predefined for us like
423:18 - application administrator application
423:20 - developer etc there's a lot in there
423:22 - and a few important built-in roles that
423:24 - you should know is the global
423:26 - administrator so this gives you full
423:27 - access to everything the user
423:28 - administrator full access to create and
423:30 - manage users the billing administrator
423:32 - makes purchases
423:33 - manage subscriptions and support tickets
423:36 - and i want you to know that you can
423:37 - create your own custom roles but
423:39 - for whatever reason you have to uh
423:41 - purchase this this to me is a bit
423:43 - unusual because in the aws world uh this
423:46 - is something you don't have to pay for
423:48 - but i guess azure active directory has
423:49 - been around for a long time and so they
423:51 - have this whole big tier
423:54 - in there and so if you want to be able
423:55 - to make custom roles you have to upgrade
423:57 - your azure ad to premium so premium
424:00 - could be p1 or p2 and i'm assuming the
424:03 - higher the number the more controls okay
424:07 - [Music]
424:11 - so let's take a look at the anatomy of
424:13 - an azure role and as you
424:15 - saw in the previous slide it does cost
424:17 - money uh
424:18 - to be able to have the privilege to
424:20 - create
424:21 - custom roles you'd need an um an active
424:23 - directory premium version one or two but
424:27 - it's still a good practice to go take a
424:29 - look at what the contents of an actual
424:32 - role is because you can actually look at
424:34 - the manage rules open them up and and
424:36 - see what they do
424:37 - and you should really look at them and
424:39 - not just take them
424:40 - based on the uh face value of the name
424:43 - of the role so i want you to know that
424:45 - uh
424:46 - azure rolled documents have two
424:48 - different syntaxes whether using
424:50 - powershell or cli and so for the example
424:53 - here on the right hand side is gonna be
424:54 - using powershell and it's very simple
424:57 - and it's just the property name is going
424:59 - to change so see where it says name uh
425:01 - if you were using powershell i have in
425:03 - parentheses would be role name okay so
425:05 - if you look one up and it's using json
425:08 - and the names are different you're going
425:09 - to have to do that translation all right
425:11 - so let's just quickly go through the
425:12 - properties here so the first thing is
425:14 - the name that's going to describe what
425:16 - the role is then you have the id this is
425:19 - not something that you're creating it's
425:20 - going to be auto-generated for you then
425:23 - you have is custom whether it's going to
425:24 - be custom or not the description that's
425:26 - self-explanatory and then the actions
425:28 - this is what you care about because the
425:30 - actions tell you exactly what you're
425:31 - allowed to perform
425:33 - uh
425:34 - so
425:35 - that's a big list there it's pretty
425:36 - self-explanatory but then you have not
425:38 - actions and this is basically you
425:40 - explicitly saying you're not allowed to
425:41 - do these things it's just a guardrail to
425:43 - make things safe do you really need and
425:45 - not actions i don't know that's just how
425:46 - they designed it then you have uh data
425:49 - actions this is an array of strings that
425:51 - specifies data operations that are
425:53 - allowed to be performed within the data
425:55 - within that object and then you have not
425:58 - data actions so things you're not
425:59 - allowed to do with data and then
426:01 - assignable scopes and this is an array
426:02 - of strings that specify the scope for
426:04 - the custom role
426:06 - and you can only define one management
426:08 - group for assignable scopes for a custom
426:10 - role
426:11 - i do want to just point out one other
426:13 - things with these uh
426:15 - these roles and really just has to do
426:16 - with these asterisks just if you're not
426:19 - familiar with them but that usually
426:20 - indicates a wild card permission it's
426:22 - like saying match everything
426:24 - and so
426:25 - when you're doing that it's going to
426:26 - match things like
426:28 - you can use it in actions not actions
426:29 - data actions and not dated actions
426:32 - and so again it just matches everything
426:34 - in that sub thing because you might have
426:35 - a bunch of options you can put there
426:38 - and so for example uh
426:41 - what it's doing like let's see see where
426:43 - it says management or cost management
426:45 - and it has action read write delete
426:49 - run action all that stuff in red would
426:51 - be matched with that asterisk and i know
426:53 - it doesn't match the examples on the
426:54 - right hand side but you just gotta
426:56 - imagine that there's other things there
426:58 - okay
427:03 - now let's just do a quick comparison
427:04 - between azure policies and azure roles
427:07 - uh just to make sure that we're very
427:08 - clear what the difference of these two
427:10 - things are so azure policies these are
427:12 - used to ensure compliance of resources
427:15 - and azure roles these are used to
427:16 - control access to azure resources so
427:20 - on the policy side these are going to
427:21 - evaluate the state by examining
427:23 - properties on the resources that are
427:24 - represented in resource manager and
427:26 - properties of some
427:28 - resource providers they do not restrict
427:31 - actions and we call this operations
427:35 - ensures that resource state is compliant
427:37 - to your business rules without concern
427:39 - for who made change or who has
427:42 - permission to make the change even if an
427:44 - individual has access to perform an
427:46 - action if the result is a non-compliant
427:49 - resource azure policy still blocks the
427:51 - create or update
427:53 - actions there okay
427:55 - on the roll side it's very simple it
427:57 - focuses on managing user actions at
427:59 - different scopes and it does
428:01 - restrictions
428:03 - it does apply restrictions on azure
428:05 - resources okay
428:06 - so azure resource or azure roles
428:08 - controls what you have access to azure
428:11 - policies ensures compliancy
428:13 - [Music]
428:17 - all right just another comparison here i
428:18 - want to compare azure 80 roles to the
428:21 - azure roles the rbac versions so ad
428:24 - roles they are used to control access to
428:26 - a.d resources
428:28 - and azure roles control access to azure
428:30 - resources and here's a good
428:32 - representation here where you have 80
428:35 - roles on the left-hand side and azure
428:37 - roles on the right-hand side
428:38 - and the idea on the left-hand side that
428:40 - actually represents like office 365 but
428:43 - you can see azure ad
428:45 - has
428:46 - some coverage both inside and outside of
428:48 - azure
428:50 - so an ad resource could be something
428:52 - like users groups
428:54 - billing
428:55 - licensing application registration or
428:58 - etc
428:59 - and then on the right-hand side for
429:00 - azure resources this could be virtual
429:02 - machines databases cloud storage cloud
429:04 - networking and etc by default azure
429:07 - roles and eight azure 80 roles do not
429:09 - span azure and 8 and azure id and by
429:12 - default the global administrator doesn't
429:15 - have access to azure resources
429:17 - so
429:18 - that's just good to know and global
429:20 - administrators can gain access to azure
429:22 - resources if granted the user access
429:24 - administrator role so the azure
429:26 - role so there you go
429:28 - [Music]
429:32 - hey this is andrew brown from exam pro
429:33 - and we are looking at the azure roles
429:35 - cheat sheet and when we say azure roles
429:37 - cheat sheet we're actually talking about
429:38 - all the roles not just the azure rules
429:40 - okay because there's three types you've
429:42 - got the classic subscription
429:43 - administrator roles you have azure roles
429:46 - which is also known as role based access
429:48 - controls that's built on top of arm and
429:52 - then you have azure active directory
429:54 - roles okay
429:55 - so yeah you need to know the difference
429:57 - between the three when we're talking
429:58 - about identity access management iam
430:01 - that allows you to create and assign
430:03 - azure roles to users roles restrict
430:06 - access to resource actions also known as
430:08 - operations there's two types of roles
430:10 - you got built-in roles managed managed
430:12 - microsoft roles are
430:14 - read-only pre-created rules for you to
430:17 - use and then you have custom rules a
430:18 - role created by you with your own custom
430:21 - logic for rule assignment is when you
430:23 - apply a rule to a user and a role
430:24 - assignment is composed of a security
430:26 - principle role definition and scope
430:29 - azure's four built-in rules are owner
430:30 - contributor reader and user access
430:32 - administrator i don't have the table
430:34 - here but you should know the difference
430:36 - between them but usually they're pretty
430:37 - self-explanatory based on their name for
430:39 - classic administrators we have three
430:41 - types of roles we've got account
430:43 - administrator service administrator and
430:45 - co-administrator
430:47 - for important azure 80 roles you need to
430:49 - remember you have global administrator
430:51 - user administrator billing administrator
430:53 - and on the exam you might even see more
430:55 - kinds there so i would encourage you to
430:56 - go look up all the different kinds
430:58 - or i might end up making a video going
431:00 - in more in depth for some of them in
431:02 - this course you can create custom azure
431:04 - 80 rolls but you're going to need that
431:06 - p1 or p2 so there you go that's azure
431:09 - rolls
431:13 - [Music]
431:14 - hey this is andrew brown from exam pro
431:16 - and we are taking a look at azure key
431:18 - fault so this helps you safeguard
431:20 - cryptographic keys and other secrets
431:21 - used by cloud apps and services azure
431:24 - key vault focuses on three things so the
431:26 - first is certificate management so
431:28 - easily provision manage and deploy
431:29 - public and private ssl certificates for
431:31 - use with azure and internal connected
431:33 - resources key management so create and
431:35 - control the encryption keys used to
431:37 - encrypt your data
431:39 - secrets management store and tightly
431:40 - control access to tokens
431:42 - passwords certificates api keys and
431:44 - other secrets and certificates contain
431:48 - key pairs key and secret not to be
431:49 - confused with key management and secrets
431:51 - management i do have to list that out
431:52 - here because
431:54 - you'll notice here and i'm getting my
431:55 - pen tool out we have certificates keys
431:57 - and secrets but within a certificate it
431:59 - can contain keys and secrets but just
432:01 - understand that there are three isolate
432:04 - offerings that um azure key vault has if
432:07 - this was another provider such as aws
432:10 - these would all be isolate services but
432:12 - uh you know a common pattern with azure
432:14 - is they like to group a bunch of uh
432:17 - functionality under a single uh service
432:19 - and so you know these things which which
432:22 - should be three services or actually
432:23 - under all azure key vault but there you
432:25 - go
432:26 - [Music]
432:31 - let's talk about hardware security
432:32 - models and fips because this is
432:34 - important for azure key vault so
432:37 - hardware security modules also known as
432:39 - hsm is a piece of hardware designed to
432:41 - store encryption keys here's an example
432:43 - of one
432:45 - called
432:47 - gemalto i'm not sure how to pronounce it
432:49 - but the idea is that these are pieces of
432:51 - hardware that are extremely expensive
432:53 - but are extremely specialized for
432:55 - holding in keys in memory not even
432:58 - writing them to disk so that there's no
433:00 - chance of somebody stealing those keys
433:03 - if they were to take the device and then
433:04 - there's the standard of fips also known
433:07 - as the federal information processing
433:08 - standard
433:10 - so this is a u.s and canadian government
433:11 - standard that specifies the security
433:13 - requirements for cryptographic modules
433:15 - that protect sensitive information and
433:17 - so hsms are
433:20 - they can be multi-tenant uh and if they
433:22 - are this is where uh we will be using
433:25 - fips 140 hyphen 2 level 2 compliant so
433:28 - you have multiple customers virtually
433:29 - isolated on a hsm device that's kind of
433:32 - a way of sharing the cost of one of
433:34 - these very expensive machines and then
433:35 - you have hsms that are single tenants so
433:38 - this is uh where you if you need to
433:39 - maybe meet level three compliancy where
433:42 - you have a single custom customer
433:43 - dedicated on a hsm so there you go
433:50 - [Music]
433:52 - let's take a look at what a vault is for
433:54 - azure key vault so a vault stores
433:56 - secrets and keys that can be protected
433:58 - either by software or by fips 142 level
434:01 - 2 validated hsm so an actual hardware
434:04 - security module azure key vault provides
434:07 - two types of containers vaults so this
434:09 - supports software and hsm backed keys
434:12 - and then you have hsm pools so only
434:14 - support hsm back keys and to activate
434:17 - your hsm you will need to provide a
434:19 - minimum of three rsa key pairs up to a
434:22 - maximum of 10.
434:23 - specify the minimum number of keys
434:26 - required to decrypt the security domain
434:28 - called a quorum
434:30 - and one key thing you know is you do not
434:32 - choose the container on creation you
434:34 - choose between standard and premium when
434:37 - you choose premium and create rsa key
434:39 - pairs you'll be able to begin to create
434:41 - hsn pools because when i was going
434:43 - through the follow alongs i was like hey
434:45 - where are these hsn pools and so that's
434:47 - where i found out about these
434:48 - requirements
434:50 - we're not going to do that because it's
434:52 - just really expensive to do and it's out
434:54 - of the scope of the exam but you know if
434:55 - you are an organization and you're
434:57 - thinking about using hsms those are some
434:59 - of the requirements there okay
435:01 - [Music]
435:05 - all right let us quickly talk about the
435:07 - key vault api so it's a rest api for
435:09 - pragmatically managing key vault
435:12 - resources allowing you to perform
435:14 - operations such as creating a key a key
435:16 - or secret important key or secret
435:18 - revoking a key or secret deleting a key
435:20 - or secret authorizing authorizing user
435:22 - or apps to access its keys or secrets
435:26 - monitor and manage the key usage so
435:28 - here's kind of a diagram and the idea is
435:30 - that uh via the api you'd use the cli to
435:33 - access the api uh it would go into your
435:36 - vault and you know if you had a
435:37 - certificate there a certificate could
435:39 - contain metadata key and secret and you
435:42 - could operate on uh that kind of stuff
435:45 - there so the key
435:46 - uh the key vault rest api supports three
435:48 - different types of authentication manage
435:50 - identity so identities managed by azure
435:52 - id this is the recommended practice
435:54 - because it requires it introduces the
435:56 - least amount of risk service principle
435:58 - and certificate so this is where you use
436:00 - the certificate service principle and
436:01 - secret where you have a user and secret
436:04 - key so there you go
436:10 - so when setting up our vault we have
436:12 - some very interesting recovery options
436:15 - and so i just want to give this a little
436:16 - bit more emphasis of these interesting
436:18 - things and so here it is soft delete
436:20 - days to retain deleted vaults and purge
436:23 - protection so soft deletes allows you to
436:25 - recover or permit delete a key vault and
436:28 - secrets for the duration of the
436:29 - retention period it's enabled by default
436:31 - on creation
436:32 - there is a way to not have soft defaults
436:35 - and i think we discovered that during
436:37 - the follow along i just can't remember
436:38 - this moment here not that it's super
436:40 - important mandatory retention period and
436:42 - so the mandatory retention period
436:44 - prevents the permanent deletion of key
436:46 - vaults or secrets prior to the retention
436:48 - period elapse so just keep them around
436:51 - just in case then you have purge
436:52 - protection so when enabled prevents
436:54 - secrets to be purged by users or
436:56 - microsoft
436:58 - so yeah there you go
437:00 - [Music]
437:05 - let us take a look at pricing for azure
437:07 - key vault so it comes in two pricing
437:09 - tiers standard and premium so the key
437:11 - difference here is premium allows for
437:13 - both software and hms protected keys
437:15 - where standard does not have hms
437:17 - protected keys uh so let's just take a
437:19 - look at the pricing in particular so for
437:21 - rsa uh 2048 that's the size of the key
437:24 - but keys uh you're looking at a cost of
437:30 - .03 cents for 10 000 transactions and
437:33 - that's the same thing for hmhs protected
437:36 - keys if you have an enabled with
437:37 - exception that you have a dollar per
437:39 - month per key
437:40 - for advanced key types so we're looking
437:42 - at rsa
437:43 - 3072 4096 so longer ones or ecc
437:49 - uh keys
437:50 - we're looking at a a greater cost here
437:53 - so um 15 cents per transactions
437:57 - and you're going to still pay that 50
437:58 - protractions but
438:00 - in the hsf protected you're going to see
438:02 - that it's going to have this kind of
438:04 - tiered thing so for the first 250 keys
438:06 - it's five bucks for the next range of
438:08 - keys from 251 to a thousand five hundred
438:11 - it's two dollars and fifty cents and it
438:12 - gets lower and lower with volume some
438:14 - more keys the more savings you'll have
438:18 - some other things here would be things
438:19 - like secret operations and this is this
438:21 - is the same for both software and hsm so
438:24 - there's no difference in cost here but
438:26 - the idea here is that there's a cost for
438:28 - secret operations at zero three cents
438:31 - for certificate renewal it's three
438:32 - dollars per request and then we have
438:35 - managed azure storage account key
438:36 - rotation one dollar per renewal manage
438:39 - hsn pools is dollars and twenty cents
438:42 - this might vary in terms of price based
438:44 - on your region uh but you know i'm just
438:47 - kind of want to show you that there is
438:48 - an additional uh cost for just the keys
438:51 - when you have hsn protected okay
438:54 - [Music]
438:59 - all right we're taking a look at keys
439:01 - for azure key vault one of the three
439:02 - core offerings one being keys secrets
439:04 - and certificates keys we use it to
439:07 - encrypt things uh utilizing with azure
439:09 - services so like if you need to encrypt
439:11 - azure disks or apply it in a variety of
439:14 - things where we have encryption at rest
439:16 - um you know it's useful for this because
439:19 - it'll just easily integrate with those
439:20 - azure services so when creating the key
439:23 - there are three options we have generate
439:25 - so azure will generate the key import
439:27 - import an existing rsi key restore
439:29 - backup so restore a key from backup
439:32 - for keys that are generated by azure you
439:34 - can either use rsa or ec
439:36 - sometimes we see people say ecc but uh
439:39 - in the ui they called it ec rsa is an
439:43 - algorithm and it's actually in a
439:46 - combining of three names of
439:47 - cryptologists or uh the people that made
439:49 - the algorithm and you'll see there's a
439:51 - few different lengths 2048 3072 4096
439:55 - that's the amount of bits that's how
439:56 - long it is then you have ecliptic curve
439:58 - cryptography and then it says p hyphen
440:01 - and we have 256 384 5
440:04 - 5 2 1 two 256k i'm gonna assume that
440:07 - also has to do with length um i don't
440:09 - know exactly how it translates over to
440:11 - bit size but uh i'm sure that's similar
440:14 - for keys generated by azure you can set
440:16 - an activation expiration date probably a
440:18 - good practice because that forces you to
440:20 - rotate your keys after a period of time
440:23 - you can create new versions of keys
440:25 - so that's a cool benefit you can
440:28 - download backups of keys backups can
440:30 - only be restored within the same as your
440:32 - subscription and an azure key vault i
440:34 - think we do that in the follow along uh
440:36 - so for
440:38 - premium vaults you do have additional
440:39 - options for hsm
440:41 - and it's not that fancy it's just like
440:43 - rsa and ec are repeated here but with
440:45 - hyphen hsm to specify it's going to the
440:48 - hardware security module you can
440:50 - generate you can import for the hsm
440:56 - there are microsoft managed keys also
440:57 - known as mmk these are keys managed by
441:00 - microsoft that do not appear in your
441:01 - vault and in most cases are used by the
441:04 - default for many azure services so even
441:06 - if you don't think you're using keys you
441:08 - actually are
441:09 - and so the way
441:12 - if you're using like aws you actually do
441:14 - see them in a key management service but
441:16 - azure decides not to show them and just
441:18 - have it by default there for you
441:20 - customer managed keys cmk are keys you
441:23 - create in azure key vault you need to
441:25 - select a key from a vault for various
441:27 - services
441:28 - so sometimes a customer manage
441:31 - managed means that the customer has
441:33 - imported cryptographic material uh uh
441:37 - but when we're talking about azure when
441:39 - you generate or import a key they
441:40 - consider that a cmk and i'm pointing
441:42 - that out because you know i use other
441:44 - clouds like aws and gcp and for aws
441:48 - cmk indicates imported cryptographic
441:51 - material not um
441:53 - not exactly how
441:55 - azure describes it here
441:56 - in order to use a key as azure service
441:58 - needs an identity within azure id so for
442:01 - permission to access the key from the
442:03 - vault um for infrastructure encryption
442:06 - this is sometimes an option by default
442:08 - uh azure encrypts storage accounts data
442:11 - at rest
442:12 - infrastructure encryption adds a second
442:14 - layer of encryption to your storage
442:17 - accounts data and we'll talk about
442:18 - double encryption because i was like
442:20 - what's double encryption and so then i
442:22 - had to make some slides here for us okay
442:25 - [Music]
442:29 - all right let's talk about double
442:30 - encryption so if we're talking about
442:33 - storage accounts there's this option
442:35 - called infrastructure encryption so if
442:37 - we checkbox that on uh this isn't
442:39 - encrypt storage account data at rest
442:41 - infrastructure encryption adds a second
442:43 - layer of encryption to your storage
442:44 - accounts data we just said that in the
442:46 - last uh lecture here but i'm just trying
442:49 - to cement that this is for storage
442:51 - accounts for azure disks it's called
442:54 - double encryption and so double
442:57 - encryption is where two or more
442:58 - independent layers of encryption are
442:59 - enabled to protect against compromises
443:02 - of one layer or
443:04 - of encryption it's the same thing but
443:06 - for one they called infrastructure
443:08 - encryption and for the other they call
443:10 - it double encryption
443:12 - confusing i know but it's the same thing
443:15 - uh
443:15 - okay using two layers encryption
443:17 - mitigates threats that come with
443:19 - encryption data of course so i just feel
443:21 - like we're repeating ourselves a hundred
443:22 - times here but microsoft has two-layered
443:23 - approach for data rest and transit so
443:25 - for data rest you have disk encryption
443:28 - using customer managed keys and then you
443:30 - have the infrastructure encryption using
443:32 - platform managed keys data in transit
443:34 - the transit encryption uses tls 1.2 that
443:38 - is the latest standard there's a
443:40 - over the years it went between ssl tls
443:42 - it's back in tls's court with 1.2 being
443:45 - the most up-to-date additional layer of
443:47 - encryption provided at the
443:48 - infrastructure layer so should you
443:50 - enable double encryption yes and always
443:53 - every time you have the opportunity
443:55 - utilize it okay
443:57 - [Music]
444:01 - all right let's take a look here at
444:02 - azure key vault secrets so one of the
444:04 - three offerings that azure key vault has
444:06 - so secrets keys and certificates azure
444:08 - key vault provides secure storage of
444:10 - generic secrets such as passwords and
444:12 - database connection strings so here's an
444:14 - example of the form we have uh the name
444:17 - of the secret the value
444:19 - um so pretty clear how that works key
444:22 - vault apis return and accept or accept
444:25 - and return secret values as strings
444:27 - internally key vault stores and manages
444:29 - secrets as a sequence of octets so 8-bit
444:32 - bytes with a maximum size of 25k bytes
444:35 - each yes 25k not 256. key vault services
444:38 - doesn't provide semantics for secrets
444:41 - accepts the data encrypts it stores it
444:43 - and returns a secret id
444:45 - or sorry a secret identifier so an id
444:47 - for highly sensitive data clients should
444:49 - consider an additional layer of
444:50 - protection for data encrypting data
444:52 - using a separate protection key provided
444:54 - to storage
444:56 - in key vault is one example encryption
444:59 - data using separate protection key so i
445:01 - guess you could encrypt the data and
445:02 - then upload the encrypted material uh
445:04 - there so i suppose that might be what
445:05 - they're suggesting key vault also
445:07 - supports a content type field for
445:09 - secrets so clients may specify the
445:11 - content type of a secret to assist in
445:13 - interpreting the secret data when it's
445:15 - retrieved a maximum length of this field
445:17 - is 285 characters
445:21 - all secrets in your key vault are stored
445:23 - encrypted key vaults encrypts secrets at
445:26 - rest with a hierarchy of encryption keys
445:28 - all keys in that hierarchy are protected
445:30 - by modules that are fips 140 hyphen 2
445:34 - compliant the encryption leaf key of the
445:37 - key hierarchy is unique to each key
445:40 - vault encryption root key of the key a
445:43 - hierarchy is unique to the security
445:45 - world protection levels varies between
445:48 - regions and we're talking about china
445:50 - here so china uses fips 140 hyphen 2
445:53 - level 1 and all the regions uses level
445:56 - two or higher i apologize for that
445:58 - spelling mistake there
446:00 - we can apply secret attributes alongside
446:03 - our secrets um and so we have exp that's
446:06 - the expiration time
446:08 - we have nbf not before so default value
446:11 - is now the time before which the secret
446:12 - data should be retrieved enabled whether
446:15 - the secret data can be retrieved uh and
446:17 - we saw some of these options in the
446:19 - actual form itself uh there are also
446:21 - other read-only attributes such for
446:23 - created and uh created an update updated
446:26 - i think it was supposed to say updated
446:27 - there in order uh to access secrets
446:29 - within your application code you would
446:31 - use the azure sdk as an example so here
446:34 - we're importing uh the key vault secrets
446:36 - this looks like
446:38 - um
446:39 - c sharp
446:41 - this is c sharp code sure oh yeah it
446:43 - says over here net okay and so we import
446:46 - azure identity
446:48 - uh key vault secrets
446:50 - we're defining our our options here so
446:53 - delay max delay retries um i don't think
446:56 - that really matters too much but the
446:57 - idea is that we are providing a uri to
447:00 - the actual vault
447:01 - and then
447:03 - we create our credentials we create our
447:06 - client and this example is just getting
447:08 - the secret so we're not um
447:11 - we're not using this to uh
447:13 - insert a secret but just to kind of grab
447:15 - it programmatically
447:16 - you can also use the cli so we have a z
447:19 - key vault secrets show and i'm pretty
447:20 - sure that's what we are using during the
447:22 - follow along but yeah there you go
447:24 - that's secrets
447:25 - [Music]
447:29 - all right we're taking a look here at x
447:31 - 509 certificate so
447:34 - before we can talk about those we need
447:36 - to talk about what is public key
447:37 - infrastructure also known as pki so pki
447:40 - is a set of roles policies hardware
447:43 - software and procedures needed to create
447:45 - manage distribute use store and revoke
447:49 - digital certificates and manage public
447:51 - key encryption so if there's a cloud
447:53 - service that manages certificates it's a
447:55 - pki
447:57 - so
447:58 - what is x 509 certificates it is a
448:01 - standard defined by the international
448:03 - telecommunication union itu for public
448:06 - key certificates
448:09 - or certifications so x-509 certificates
448:12 - are used in many internet protocols
448:15 - extremely commonly used with ssl tls
448:18 - for https
448:20 - signed and encrypted emails code signing
448:23 - and document signing
448:25 - so a certificate contains an identity so
448:28 - hostname organization or individual a
448:31 - public key so rsa dsa ecda etc
448:36 - and then there's the idea of a
448:37 - certificate authority so this is an
448:39 - entity that issues digital certificates
448:42 - a
448:43 - ca or certificate authority acts as a
448:45 - trusted third party trusted both by the
448:48 - subject owner of the certificate and by
448:50 - the party
448:51 - uh
448:52 - relying upon the certificate so there
448:53 - you go
448:55 - [Music]
448:59 - all right let's talk about uh chain of
449:01 - trust for
449:03 - 509 certificates so a certificate
449:05 - authority ca can issue multiple
449:07 - certificates in the form of a tree
449:08 - structure also known as a chain of trust
449:12 - so here is an example for the exam pro
449:14 - dot co website um and uh you know this
449:18 - was issued by amazon and so amazon is
449:21 - the root uh the root authority here and
449:23 - then they have their intermediate
449:24 - certificate which issues us the
449:27 - certificate that is used for
449:29 - our https tls communication
449:32 - to keep
449:33 - encryption and transit to the website
449:35 - let's talk about what a root certificate
449:37 - or root certificate authority is so the
449:39 - root ca
449:40 - is a self-signed certificate and its
449:42 - private key is used to sign other
449:43 - certificates so i'm going to talk about
449:45 - self-signed just means that they've
449:46 - generated a key and they've done it on
449:49 - their machine there and that's what they
449:50 - mean by self-signed because they're not
449:52 - reaching out to anybody else in order to
449:54 - say that the certificate is valid it's
449:56 - important that the private key of the
449:57 - roots are protected uh so like these are
450:00 - the most important keys and to protect
450:03 - them that's why we have this
450:04 - intermediate certificate authority also
450:06 - we like to call these icas so
450:08 - intermediate certificates are signed by
450:10 - the root private key and act as entities
450:12 - that can issue certificates they protect
450:14 - the root certificate because the root
450:15 - certificate does not have to sign every
450:17 - issued certificate then you have your
450:20 - end entry certificate these are the ones
450:22 - that we're going to be using so a
450:24 - certificate issued by the ica used by
450:26 - the end entity the entity in the case of
450:29 - an ssl certificate for a website so ssl
450:32 - tls
450:34 - it's just common to say ssl even though
450:36 - tls 1.2 is the common standard when we
450:38 - were making the videos here but there
450:40 - you go
450:41 - [Music]
450:45 - all right so we're taking a look at the
450:46 - certificate format so here is the
450:48 - certificate information for the
450:50 - exampro.co
450:52 - certificate that we used for
450:55 - https for the
450:57 - data in transit to make sure that our
450:59 - information is secure if you open it up
451:01 - you can see there's a bunch of metadata
451:04 - and so let's talk about what's in here
451:06 - so we have the version number the
451:08 - version number is uh you know indicating
451:11 - that it's an x509 standard
451:14 - notice here it says three so we're using
451:15 - version
451:16 - three we have a serial number a unique
451:19 - serial number assigned to the
451:20 - certificate by the certificate authority
451:22 - so it's this big long thing here we have
451:24 - the certificate algorithm id used to
451:26 - sign the certificate uh you know could
451:28 - be rsa dsa in this case is suggesting
451:31 - that it is rsa 250 uh with sha 50 shaw
451:35 - 256 with rsa the issuer so the name of
451:38 - the civic authority the issued uh
451:40 - issued the certificate to so that's the
451:42 - issuer name up here
451:44 - but this one is saying
451:46 - organization amazon things like that so
451:48 - it doesn't say my name might be down
451:51 - below here somewhere
451:54 - uh yeah i don't see my name in here but
451:56 - you know there's something here uh
451:57 - validity period so how long is this
451:59 - certificate going to be valid for the
452:01 - start and end date the subject uh yeah
452:03 - that's what we want the issue would be
452:05 - amazon because that's what is i guess i
452:07 - was looking for the subject so the
452:08 - subject is who it is to be issued to so
452:11 - that's where we'd want to see exam pro
452:13 - but it gets cut off here so we just
452:15 - can't see it
452:16 - the subject public key so the public key
452:18 - that is meant
452:20 - to be authenticated by the certificate
452:22 - so this field is also named
452:24 - also names the algorithm used for the
452:26 - public key generation
452:28 - then we have other fields like issue
452:29 - unique identifiers subject unique
452:31 - identifier extensions which allow you to
452:33 - associate private information to a
452:34 - certificate
452:35 - but the clear thing to understand is
452:37 - that all this metadata is publicly
452:39 - readable so anyone can generally view it
452:41 - that's uh the purpose of that is so that
452:43 - we can have the means to validate the
452:45 - authenticity of these certificates
452:47 - so let's just talk about some things
452:49 - that can package along with a
452:50 - certificate so that there are ways of
452:53 - validating it so we have this metadata
452:55 - and this metadata gets produced in a
452:57 - hash a hash is a means of taking a bunch
453:00 - of information and basically
453:01 - turning it into a thing that looks like
453:03 - a bunch of random characters but it
453:05 - actually is a fingerprint because that
453:07 - hash if you take the same information
453:09 - it'll always produce the exact same
453:10 - patch so it is a fingerprint to identify
453:13 - the information so
453:15 - you know that's that there the that hash
453:19 - or fingerprint is going to then get
453:21 - signed with the private key the private
453:22 - key that is generated by the
453:25 - certificate authority and that way it
453:28 - guarantees the authenticity of that
453:30 - fingerprint
453:31 - and then the idea is that when you are
453:33 - signing
453:35 - when you are signing
453:37 - that hash it's basically creating a
453:39 - signature okay so that's that
453:40 - authenticity when you sign it you know
453:42 - it's valid
453:43 - so the certificate metadata the
453:45 - signature and then the public key
453:47 - provided by the uh the end user is
453:51 - packaged there and with all that
453:52 - information we have enough information
453:54 - in order to validate the office
453:56 - authenticity of the certificate okay
453:59 - [Music]
454:04 - all right let's take a look at um the
454:06 - extension name for digital certificates
454:08 - because it can be crt cr pamper der
454:13 - it there's so many varieties it's very
454:16 - confusing so let's see if we can unpack
454:17 - some of the complicated history here
454:20 - and make it a little bit more clear so
454:22 - we have the first is pem so privacy
454:24 - enhanced mail and the idea of this
454:27 - format is that it's base64 ascii you can
454:29 - open them up and they're very easy to
454:31 - read very easy to edit
454:33 - and it's the most common format for x
454:34 - 509 certificates uh and for
454:39 - certificate signing requests and
454:40 - cryptographic keys so the pem format
454:44 - normally have the following extensions
454:45 - crt pamser and key key specifically for
454:48 - private keys
454:50 - but there's no hard rule about it so
454:52 - just because something is pam doesn't
454:53 - mean it's going to end with dot pem then
454:55 - you have distinguished encoding rule so
454:57 - dur this is a binary encoding so if you
454:59 - open up the file it's a bunch of binary
455:01 - code you won't be able to make sense of
455:02 - it edit it do anything with it uh it's
455:05 - used for both x509 certificates and
455:07 - private keys der normally have the
455:09 - following extensions so d-e-r-c-r
455:12 - then there are certificate files that's
455:14 - where we have sir and crt these are
455:17 - base64 ascii so they're basically i
455:18 - think they're basically pen and sir and
455:20 - crts are interchangeable extensions so
455:24 - you can either have they basically stand
455:26 - for certificate file right so that's
455:27 - what it's saying so it's like hey c-e-r
455:30 - is a short form certificate file crt is
455:32 - a short-form certificate file right um
455:35 - so that's where they get confusing so
455:36 - either cer or crt these are generic file
455:39 - extensions
455:40 - then you have the personal information
455:42 - exchange pfx this is a microsoft
455:44 - certificate format
455:46 - uh this is the successor so pkcs
455:50 - pound 12 is the successor pfx
455:52 - and so pkcs hyphen 12 is uh we'll use
455:56 - either
455:57 - dot p12 or pfx extensions so there is a
456:01 - lot of confusion about the variety of
456:03 - formats
456:04 - it's just because there was a lack of
456:06 - standardizations early on and so
456:09 - everybody was just trying to create
456:10 - their own formats or their own keys and
456:12 - they weren't being consistent
456:14 - and that's why we just end up with all
456:16 - these weird naming conventions so half
456:17 - the time you cannot tell what something
456:19 - is you have to open the contents of it
456:21 - you have to know what you're using it
456:23 - for
456:23 - and just you know just be aware that it
456:26 - is confusing even for me i've been doing
456:28 - it for like what 20 years and i still
456:29 - can't tell what's what so that's just
456:31 - what it is okay
456:33 - [Music]
456:37 - hey this is andrew brown from exam pro
456:38 - and we are taking a look at certificate
456:40 - signing requests also known as csr is a
456:42 - message sent from an application to a
456:44 - registration so a certificate authority
456:47 - of a private key infrastructure in order
456:49 - to apply for a digital identity
456:52 - certificate the most common
456:54 - time you would deal with a certificate
456:56 - signing request is when you need a ssl
456:58 - tls certificate so that your website has
457:01 - https so it's data in transit
457:04 - so the idea is you'd submit a
457:06 - certificate signing request to your
457:08 - certificate authority so like if you're
457:10 - getting your certificate from godaddy
457:12 - amazon i'm just i'm thinking godaddy
457:14 - actually has certificates but like
457:16 - geotrust site or something you know uh
457:19 - you know you have to submit a csr so a
457:22 - csr contains a public key
457:24 - and application information such as a
457:26 - fully qualified domain name because it
457:28 - like if you're getting a a csr for
457:31 - a domain name they're going to need to
457:32 - know the domain name and so that's the
457:33 - two things that get packaged together so
457:35 - the idea is you
457:36 - create your certificate signing request
457:39 - you request the certificate to the
457:40 - certificate authority the certificate
457:42 - authority will issue you a certificate
457:44 - if it deems that uh it likes your
457:46 - request but there you go
457:53 - hey this is andrew brown from exam pro
457:55 - and we are taking a look at certificates
457:57 - in azure key vault so that's one of the
457:59 - three keys secrets and certificates and
458:02 - so if you're wondering why we did all
458:03 - that x 509 certificate stuff it's
458:05 - specifically so that we are prepared to
458:07 - understand
458:08 - uh you know the purpose of certificates
458:10 - in azure key vault so azure key vault
458:12 - allows you to import generate and manage
458:14 - x 509 certificates so the idea here is
458:17 - you can uh you know generate one you can
458:19 - self-sign one you can uh you use one
458:22 - with a um
458:23 - a certificate authority but let's talk
458:24 - about some other things here so key
458:26 - vault partners with certificate issue
458:28 - providers for tls and ssl certificates
458:30 - so we have digicert and globalsign those
458:33 - being
458:34 - two very large companies you can
458:35 - generate self-signed or through a
458:37 - certificate authority there's no need to
458:39 - manage the security of the private key
458:41 - key vault takes care of it for you it
458:43 - allows the certificate owner to create a
458:45 - policy that directs key vault to
458:48 - to the manage to the managed life cycle
458:50 - of a certificate this allows certificate
458:52 - owners to provide contact info for
458:55 - notification about lifecycle events for
458:56 - expiration renewal or certificate it
458:58 - supports automatic renewal with selected
459:00 - issuers so key vault partners 5x5 x 509
459:04 - certificate providers or certificate
459:06 - authorities so there you go
459:11 - [Music]
459:15 - all right let's talk about the
459:16 - composition of a certificate with an
459:18 - azure key vault and so we use this uh
459:20 - graphic earlier to describe uh the rest
459:23 - api integra uh interacting with just the
459:25 - certificate part of azure key vault but
459:28 - what it did show is the three components
459:30 - that are part of certificate that it
459:31 - will be stored in azure key vault so
459:34 - there we have um azure key vault which
459:37 - allows the operations
459:39 - of keys okay then we have key vault
459:41 - secrets so allows the retrieval of the
459:44 - certificate value as a secret
459:46 - certificate metadata so
459:48 - x509 certificate data so if that's not
459:50 - clear let's just get our pen tool out
459:52 - here so we have the key
459:54 - we have the secret and then we have the
459:57 - metadata okay
459:58 - so those are the three components that
460:00 - are alongside the certificate and so
460:03 - here we have an example of a certificate
460:06 - that
460:07 - i self-signed i generated in there and i
460:09 - just want to show you that you can
460:10 - download the certificate here and either
460:13 - sir format cer or pempfx format and so
460:17 - you see where i'm like is it ascii is it
460:19 - binary what's it going to be because
460:21 - you know when i downloaded it i didn't
460:23 - really get the exact result that i
460:24 - thought i would get but remember that
460:26 - the pem can be used as cer so the format
460:30 - format offering can be confused here but
460:32 - what can help us understand
460:34 - is what we set the content type when we
460:37 - generate our certificate so
460:38 - um like cer will be ascii format i
460:42 - believe um but like when we're doing
460:44 - with pemb or pfx
460:46 - the difference is going to be based on
460:48 - the content type that you choose there
460:50 - so if you choose the content type to be
460:52 - pkcs
460:54 - pound 12 it will use that format if you
460:56 - use if you choose pem that means that
460:59 - it will download in the pem format same
461:02 - thing with the cer so just realize it is
461:05 - a bit confusing and there's that
461:06 - additional option there and
461:08 - um azure really really should have not
461:10 - made these two buttons that confusing it
461:11 - should just be clear what they are but
461:13 - that's just one of those things that uh
461:16 - azure trips you up on okay
461:20 - [Music]
461:22 - all right we're taking a look here at
461:23 - certificate policies so typical policies
461:25 - allow you to set and configure your
461:26 - certificate such as the content type the
461:29 - lifetime of the certification the key
461:30 - type the key size and various other
461:32 - options but it's calling it an issuance
461:34 - policy so there's some inconsistency of
461:36 - naming by azure between their
461:38 - documentation and their ui which is no
461:40 - surprise but we have a bunch of options
461:43 - as we saw so the type of the authority
461:45 - whether it's self-signed and things like
461:46 - that the subject the validity period the
461:48 - content type whether it's enabled the
461:51 - percentage lifetime which i'm not sure
461:53 - what that is we have advanced options
461:54 - where we can say the key type the key
461:56 - size things like that issuance policies
461:58 - only effect certificates that will be
462:00 - issued in the future so it won't modify
462:02 - the existing certificate but the ones
462:04 - that will be
462:05 - generated in the future okay
462:07 - [Music]
462:11 - hey this is andrew brown from exam pro
462:13 - and this fall along we're going to be
462:15 - learning all about
462:17 - azure vault so let's get to it so what i
462:19 - want you to do is go in the top here and
462:21 - type in key vault and here we'll have to
462:24 - go ahead and create ourselves a new
462:25 - vault
462:26 - and so from there we're going to create
462:28 - a new resource group i'm going to call
462:30 - this resource group my example vault
462:34 - and then we will make a vault key here
462:36 - so i'll just say my vault example
462:39 - which is kind of funny because this
462:40 - one's slightly different so you've seen
462:41 - i've done this before so i'm going to do
462:43 - my example vault
462:44 - as the name here
462:46 - and for the region usc is fine for
462:49 - pricing we'll keep it at standard soft
462:52 - delete is enabled
462:54 - and then there's the option for purge
462:56 - protection
462:57 - so we are going to enable purge
462:59 - protection
463:01 - and uh this is going to play into other
463:03 - follow alongs we'll explain that as it
463:04 - goes but purge protection does not allow
463:06 - you to
463:07 - purge things uh easily once it's enabled
463:10 - so what we'll do is go ahead and review
463:13 - and create
463:16 - and we'll go ahead and go review create
463:23 - and we'll give it a moment here
463:29 - and we'll just wait till it's done
463:30 - deploying okay
463:32 - all right so after a short little wait
463:34 - our vault is created and so what i want
463:36 - you to go to the resource and we're
463:38 - going to be using this vault a little
463:40 - bit in some of the follow alongs and in
463:42 - some cases not so much okay
464:01 - [Music]
464:05 - hey this is andrew brown and this fall
464:07 - along we're going to be doing some
464:08 - things with
464:10 - keys with an azure key vault so what i
464:11 - want you to do is make your way over to
464:12 - the keys blade on the left hand side
464:14 - here we're going to generate or slash
464:16 - import a new key we're going to choose
464:18 - the generate option
464:20 - in terms of naming we're going to call
464:22 - this
464:22 - my disk key
464:25 - and we're going to choose
464:27 - rsa2048 that seems totally fine to me
464:29 - everything else seems okay so go ahead
464:31 - and create that
464:33 - key so give it a moment to create
464:35 - doesn't take too long and then what
464:37 - we're going to do is go on the left hand
464:39 - side to im access controls
464:42 - and what we want we're going to want to
464:44 - do
464:45 - is add a new role assignment so we can
464:47 - go ahead and start using this key so
464:50 - what i want you to do
464:51 - is go and look for key vault
464:54 - administrator which is here and we'll go
464:56 - ahead and hit next and then for our
465:00 - user we will choose ourselves so under
465:03 - user i'm going to select the members i'm
465:05 - looking for the account i'm using there
465:06 - i am and you're around go ahead and
465:08 - select that there
465:10 - and so that is all we need to
465:13 - assign it so that we can actually uh
465:15 - work with that key
465:17 - so i think a good idea is to use a key
465:20 - to encrypt a disk so what we'll do is
465:23 - make our way over to disk encryption
465:25 - sets because before you can encrypt a
465:27 - disk you need to have an encryption set
465:29 - so go ahead and create ourselves a new
465:31 - encryption set we'll call we'll use the
465:33 - uh sorry the same
465:35 - um a resource group so it's very easy
465:37 - clean up afterwards we'll call this my
465:41 - disk
465:42 - encrypt
465:43 - set
465:44 - here
465:45 - and in terms of the encryption type
465:46 - we're going to use double encryption
465:48 - because that's a much better you have
465:49 - two keys that encrypted so that's a lot
465:51 - better we are going to choose our vault
465:54 - so we have my example vault there's only
465:56 - one option here
465:58 - and in terms of the key we'll just
466:00 - select my disk key in terms of the
466:02 - version uh we'll select the current
466:04 - version we'll go ahead and hit review
466:06 - create
466:08 - and then we'll go and create that
466:12 - and we'll give it a moment to create
466:13 - that encryption set shouldn't take too
466:15 - long here
466:16 - and after a short little wait uh our
466:18 - resource should be deployed only took
466:19 - about a minute for me if we go here it's
466:21 - going to have this message up here it's
466:22 - very small but it says dissociated disk
466:24 - image snapshot this disk encryption set
466:26 - you must grant permissions to key vault
466:28 - so all we have to do is click that alert
466:30 - and we'll grant permissions and so now
466:32 - we are able uh to use that key
466:35 - um or like to to we're going to have the
466:38 - permissions issues is solved so what
466:39 - we'll do is go to type and create a new
466:40 - disk and so we can apply this key to
466:43 - that encryption so go ahead and create
466:45 - we're going to choose the same resource
466:48 - group here i'm going to call this my
466:50 - example vault and
466:53 - or sorry my example
466:56 - disk because that's a little bit more
466:57 - clear than that
466:59 - and for the availability zone doesn't
467:00 - matter for the source type
467:03 - it doesn't matter as well in terms of
467:05 - the size we
467:07 - want this to be cheap we're not really
467:08 - using this for real so we'll use
467:10 - standard hdd
467:12 - and we'll say okay in terms of
467:14 - encryption this is where things get fun
467:16 - we go to double encryption we choose our
467:18 - key here we'll go ahead review and
467:20 - create
467:24 - and we'll just give it a moment for that
467:26 - to oh we'll hit create and we'll have to
467:28 - wait a little while here for the create
467:29 - that resource so we'll just wait until
467:32 - that is created okay
467:34 - and after a very short while the disk is
467:35 - ready so we'll go to that resource we'll
467:37 - go to the encryption tab to see that
467:39 - encryption is applied so that's all it
467:41 - takes to use a key to encrypt
467:43 - a disk so we are going to still use some
467:46 - of these accounts there's no cleanup yet
467:47 - i'll go back here and i'll see you in
467:48 - the next one
467:52 - [Music]
467:56 - hey this is andrew brown and this follow
467:58 - along we're going to learn about backup
468:00 - and restore key so what i want you to do
468:02 - is go back into the
468:04 - resource group that we just recently
468:06 - created and we're going to make our way
468:08 - over to keys so i'm just sorry we got to
468:10 - get into the vault first then we'll go
468:12 - over to keys
468:13 - and the idea is that we have this key
468:15 - here and so
468:17 - you can see that we have this current
468:18 - version so you can add additional
468:20 - versions but what's going to happen if
468:21 - we try to back this up so when you back
468:23 - this up you're going to get this file
468:25 - here and if you open up this file it's
468:27 - going to look like a bunch of gobbly
468:28 - goop so i'm just going to try to open it
468:30 - here
468:31 - i have it up off screen here so i'm just
468:33 - trying to open it up within uh visual
468:35 - studio code so i'm just going to open up
468:36 - visual studio code again doing this off
468:38 - screen here
468:40 - just give me a moment
468:42 - all right and so this is the file
468:45 - that we encrypted
468:46 - and you take a look here and
468:49 - it's
468:50 - it doesn't look like anything but the
468:51 - idea is that it is our backup of our
468:53 - keys so that we can re-import that
468:57 - and just taking a look at the key name
468:59 - this is what it looks like so this is my
469:00 - example vault my disk key then there's
469:03 - this
469:04 - date and that's key backup so just
469:06 - recognize that's the format and the date
469:07 - is very useful to indicate when you
469:09 - backed it up so let's go ahead and
469:10 - delete this key because the idea is we
469:12 - want to
469:13 - restore that backup and so we have
469:16 - deleted that key
469:17 - there
469:19 - and uh what we're going to do is we're
469:21 - going to attempt a restore so i'm going
469:23 - to go ahead and go
469:31 - occurred while restoring the key the key
469:33 - you're trying
469:34 - to restore already exists why would it
469:36 - throw that error we've clearly deleted
469:39 - it and the reason why is that we have
469:41 - purge protection on we did that in the
469:44 - first
469:45 - first part when we set up this actual
469:47 - vault here i'm going to see if we can
469:49 - find the settings wherever that purge
469:51 - protection is i'm trying to remember
469:53 - where it is purge protection is enabled
469:55 - so we can go here and once you enable it
469:57 - you cannot turn it off it's going to
469:59 - retain it for a certain amount of days
470:01 - and so all you can do is soft delete
470:03 - keys
470:04 - so this key is not actually deleted yet
470:07 - if you go to manage deleted keys you can
470:09 - see the key is over here and if you try
470:11 - to click on purge it is disabled because
470:14 - we cannot remove the key because we have
470:16 - purge protection on but we can recover
470:18 - the key so we'll go ahead and recover
470:20 - and so that will allow us
470:22 - to recover
470:24 - the key
470:28 - and if we refresh here it's going to
470:31 - take a little bit time for that key to
470:32 - restore so we'll just have to wait a
470:35 - little bit and then it will show up
470:38 - there's one other thing i wanted to show
470:40 - you was under policies because you know
470:43 - um if you go under where's policies here
470:46 - um or
470:48 - access policies if you look under our
470:50 - user here and we look at the key
470:51 - permissions
470:53 - there is an option to purge and we don't
470:55 - actually have that turned on right now
470:57 - but if we were to save this
470:59 - and we were to still go to that purge
471:01 - option it would still say the same thing
471:02 - so even if you have purge permissions it
471:04 - does not matter if purge protection is
471:06 - turned on it still will not let you
471:07 - purge but you would need a combination
471:09 - of those in order to
471:11 - uh you know be able to do things there
471:14 - so to really show you how to do that
471:16 - recovery i think what we should do i'm
471:18 - just going to delete our old key here
471:19 - because we don't care about it but we
471:21 - are going to well i guess we could try
471:23 - to import it into the other one so i'm
471:24 - just going to undo that for a second
471:26 - but we are going to go ahead and create
471:29 - ourselves another vault
471:31 - so i'm going to go and type in vault at
471:34 - the top here
471:35 - and we're going to be a little bit more
471:36 - careful when we create this vault so
471:38 - we'll go here and we will choose
471:42 - my example vault i'm going to say my
471:45 - vault no protect
471:49 - and the pricing tier will be standard
471:52 - one day we're going to leave it well 7
471:54 - is the lowest and we'll say disable
471:56 - purge protection because we don't want
471:58 - to have that enabled
472:00 - and we'll see if we can import the key
472:01 - into another vault i'm not sure if we
472:03 - can do that worst case we'll make a new
472:05 - key download the key re-upload it but
472:07 - i'm just curious what would happen if we
472:09 - tried to upload the same key as it's
472:10 - still in another
472:12 - vault i'm not exactly sure
472:16 - all right so this deployment is
472:17 - successful i'm going to go to this
472:19 - resource and go ahead to go to create
472:20 - and we're going to
472:22 - restore from backup and we're going to
472:24 - take this key and see if we can actually
472:25 - import it here so it looks like we can
472:27 - take a key and it can exist in multiple
472:29 - vaults i'm going to go ahead and delete
472:31 - this key
472:32 - and we're going to say are you sure you
472:34 - want to delete this key i'm going to say
472:35 - yes
472:37 - and if we go to manage keys
472:40 - and we refresh it takes a little bit of
472:42 - time here so we'll just wait a moment
472:43 - for this to persist
472:46 - and after a short little wait like about
472:48 - two minutes i refresh and the key is
472:50 - here so if i go here you'll notice the
472:51 - purges option is still not available we
472:53 - can obviously recover
472:55 - um but we don't have purge um
472:58 - protection on so if we go to access
473:00 - policies over here and we go ahead and
473:02 - scroll down and select purge
473:04 - and save our changes we can then go back
473:06 - to keys we'll give it a moment to save
473:08 - we go back to keys
473:10 - we'll refresh it we'll manage our keys
473:12 - and we'll go ahead and purge it and that
473:14 - will permanently purge it there so
473:16 - that's all it takes
473:18 - to do that so there you go
473:22 - [Music]
473:28 - hey this is andrew brown from exam pro
473:30 - and this follow along we are going to
473:31 - learn about key rotation so what i want
473:33 - you to do is make it back to the vault
473:35 - that we were just in the no protect
473:37 - vault and we'll go over to keys and
473:40 - we'll have to create a new key so i'm
473:42 - going to create a new key here called my
473:44 - new key and it's going to be rsa 2048
473:47 - and we're going to go ahead and create
473:48 - that key so the idea of key rotation is
473:51 - that if you have uh if you think your
473:53 - key has been compromised or you have a
473:56 - policy like a company policy that you
473:58 - should rotate them for security purposes
474:00 - after a while
474:01 - you can easily rotate out keys so
474:04 - in order to rotate a key it's as simple
474:06 - as going to
474:07 - your rotation policy here and we can say
474:10 - rotate now and that will immediately
474:12 - rotate the key as you'll see there is
474:13 - now the current version and the older
474:15 - version so that was the key we had
474:16 - before this is the new key we can also
474:19 - set up a rotation policy so if we go to
474:21 - rotation policy here
474:23 - we can set the expiry time to
474:26 - let's put the lowest number in here one
474:27 - so this has to be at least 28 days
474:30 - and then we want to enable enable it and
474:33 - then we want to automatically renew
474:35 - after a certain amount of time
474:37 - so here if i put seven i'm trying to put
474:39 - the lowest number so it goes one
474:41 - and i try seven
474:42 - it says the value cannot be greater than
474:44 - 21 days because there's kind of a
474:46 - minimum and maximum both having a buffer
474:48 - i don't exactly understand the math but
474:50 - i know
474:51 - that um
474:53 - if we change it to something like
474:56 - i'm thinking here
474:58 - expiry time 40 days and there's a larger
475:01 - gap this should be less of a problem
475:03 - so it cannot be greater than that 21.
475:07 - i i knew the math here a moment ago i
475:09 - could have swore it was 28 and 7.
475:11 - so just
475:14 - well i'm not sure that this is broken
475:15 - let's go ahead and save that
475:17 - try 40 up here
475:19 - and hit save
475:20 - value cannot be greater than 33 days
475:23 - okay 33
475:27 - oh boy
475:29 - so let me
475:31 - all right so the problem was i was in
475:33 - months and we'll have to go to days here
475:35 - so here if we put seven it works fine
475:36 - but there is a bit of work to figure
475:38 - this out and it will tell you so you'll
475:40 - have to figure it out we'll go ahead and
475:41 - save that um oh yeah it prompts again so
475:44 - the uh the time before expiry cannot be
475:47 - greater than 21 days cannot be greater
475:49 - than
475:51 - um okay so we
475:54 - put the expiry to 40 we save so there is
475:57 - a bit of work there i don't exactly
475:59 - understand all the math there but i
476:00 - guess it's a way so that you have less
476:03 - problems but that's key rotation in a
476:05 - nutshell so there you go
476:07 - [Music]
476:12 - hey this is andrew brown from exam pro
476:14 - and this follow along we're going to
476:16 - learn about secrets so go back to your
476:18 - previous fault the no protect fault and
476:20 - we'll go to secrets here and we're going
476:22 - to go ahead and generate a new secret
476:25 - and we're going to use this secret
476:26 - within an application so we're going to
476:28 - have manual we'll say my secret
476:31 - and make sure you spell right so my
476:33 - secret
476:35 - secret
476:37 - there we go and for the value i'm going
476:39 - to say hello mars it's a secret value
476:42 - that's the whole point of secrets for
476:44 - the content type we're going to say text
476:45 - plain so it knows that it's text and
476:47 - everything else looks fine so go ahead
476:49 - and save that
476:51 - and we will easily create our
476:53 - cells a secret here so
476:55 - the secret is now available
476:57 - now we just need to use it in some kind
476:58 - of application so what i'm going to
477:00 - recommend is that we go use a git pod
477:03 - template so we're going to say gitpod
477:05 - template.net
477:09 - core cli c sharp
477:15 - c-sharp there we go
477:17 - and from here we'll click into it
477:20 - and uh what we want to do is use this
477:22 - template
477:24 - and we will have it private it's totally
477:26 - fine i'm going to go to exam pro here
477:28 - and i'm going to call it
477:30 - i don't know azure
477:32 - vault secrets dot net
477:36 - oh did i use that before so we'll do new
477:38 - if you don't have to do new that's great
477:39 - i'm going to go ahead and create this as
477:40 - a private repository from a template so
477:43 - it's going to set us up with a base
477:45 - core.net application i think.net 6.0
477:48 - um it's just going to make things a lot
477:50 - easier for us so you can click the
477:52 - button down below actually probably not
477:53 - because i'll go to the wrong repo
477:55 - but um
477:56 - what you'll want to do here is
477:59 - uh if you don't have the chrome
478:01 - extension installed all it's doing is
478:03 - it's just putting this in front of it to
478:05 - launch this get pod environment so get
478:07 - pod is has a free tier so it's very
478:09 - useful to use especially for net because
478:11 - we do have to do a bunch of setup for it
478:13 - so i'm not really good at net so this is
478:15 - the approach i like to take um and the
478:18 - idea is that we just close this and it
478:20 - cleans itself up so it's really really
478:22 - nice so we'll just wait for the image to
478:23 - build
478:24 - and for the environment to run and then
478:26 - we'll get to the next step okay
478:30 - all right so after a couple minutes this
478:32 - environment is ready and i'm just going
478:34 - to go ahead and hit the x there go to
478:35 - terminal and we're going to make sure
478:36 - this dot net app works we're going to do
478:38 - net run and this is just a basic hello
478:41 - world application so we can see that it
478:43 - prints out so we're going to need a
478:44 - couple packages we're going to use net
478:46 - add package to add these here so azure
478:49 - identity
478:51 - and then we will do
478:53 - dotnet
478:56 - dot net add package azure security
479:00 - key vault secrets
479:05 - and the idea is we'll need to modify
479:07 - this file in order to load some stuff
479:10 - so i can't remember if i wrote this
479:12 - myself or if i got it from an azure
479:15 - azure documentation but we'll write it
479:16 - out because it's not too much to write
479:18 - out
479:19 - as using azure core using
479:22 - azure identity
479:23 - using azure security
479:26 - key vault secrets
479:29 - and then under the program we want to
479:32 - modify our static void main so we're
479:35 - going to need a new string
479:37 - and this is going to be called secret
479:38 - name
479:39 - so my secret actually we have a bunch
479:42 - here and
479:43 - i think this is where we probably should
479:44 - set environment variables so key vault
479:46 - name
479:47 - and i'm just going to actually i don't
479:49 - know how to set environment variables
479:51 - in.net i don't want to look it up so i'm
479:52 - just going to write it out
479:54 - and hard code it because it's not going
479:55 - to really matter if we do that so i'm
479:57 - going to say my vault no protect that's
480:00 - what we called it my vault no protect
480:01 - yeah
480:03 - and
480:04 - for
480:05 - uh the key value uri we're going to do
480:07 - this so we'll say
480:09 - https colon forward slash forward slash
480:12 - my vault no protect
480:15 - dot vault dot azure dot net
480:19 - i think we need semicolons on the end
480:20 - here otherwise it complains a bunch
480:23 - um
480:26 - he vault name
480:30 - var
480:32 - secret client options options equals new
480:36 - secret client options parentheses
480:40 - and
480:41 - this
480:42 - i think is a function so we'll open this
480:44 - up here like that
480:46 - and we will try we'll have to set this
480:48 - to retry don't worry if you don't know
480:50 - what's going on because it honestly it
480:51 - doesn't really matter it's it's just
480:53 - like you know you just need to get some
480:55 - experience writing some code
480:57 - and eventually it starts to sink in
481:00 - because it really takes a lot of time to
481:01 - learn code you just have to spend a lot
481:02 - of time doing it
481:04 - so that we're just going through the
481:04 - motions without having to think about it
481:06 - too hard
481:08 - so from second 16 and i'll explain it
481:11 - once we write it all up here
481:14 - but yeah it's very good practice to just
481:16 - write out code
481:19 - max retries is five and we'll say mode
481:22 - equals retry mode dot exponential
481:28 - um and i feel like we have to put
481:31 - semicolons in dot net otherwise it
481:32 - complains so that is
481:36 - that
481:36 - part and then we'll need a client so
481:39 - we'll say new
481:41 - secret clients parentheses new
481:44 - uri key
481:47 - v uri
481:48 - comma
481:50 - new default
481:53 - azure credential parentheses options
481:56 - semicolon
481:58 - uh key vault secret secret equals client
482:02 - get secret parentheses secret name
482:07 - semicolon
482:09 - and down here we'll do console
482:12 - console
482:14 - dot write line
482:17 - get secret
482:21 - colon
482:26 - get secret
482:30 - plus secret value
482:37 - uh value there i didn't mean to spell
482:39 - vault capital v on the value it's giving
482:41 - me some trouble here
482:43 - semicolon and then the idea is we're
482:45 - going to want to enter the secret yeah
482:47 - i'm pretty sure i took this from uh the
482:49 - azure docs i just
482:52 - don't remember where because i know that
482:53 - this looks very familiar and uh but i
482:56 - think i reorganized it so that it was a
482:58 - little bit more streamlined i think
482:59 - there was a little bit more going on
483:00 - with theirs
483:01 - and so we'll say string
483:04 - secret value
483:08 - lowercase secret
483:10 - value
483:12 - equals console
483:14 - read line parentheses semicolon
483:17 - client set secret
483:20 - secret name secret value
483:22 - [Music]
483:26 - and
483:27 - we can do console
483:29 - right
483:32 - set secret
483:35 - semicolon on there we'll do another
483:37 - console log here this will be for the
483:38 - key so let's give this a couple spaces
483:42 - and we'll do plus secret uh name
483:51 - i can spell this one right
483:54 - and we'll do secret value
484:01 - we'll do another console right we're
484:03 - doing lots of these console rights here
484:08 - actually it'll just be pretty much the
484:09 - same lineup here so i'll just go grab
484:11 - this one here
484:15 - client start
484:17 - delete
484:18 - secret
484:21 - secret name
484:25 - console right line
484:28 - start delete secret
484:34 - start delete
484:36 - secret
484:38 - plus key vault name
484:42 - and then we'll go get it one more time
484:44 - so let's take a look at this i mean
484:45 - hopefully we didn't introduce any
484:46 - mistakes by typing this all up by hand
484:49 - but uh the idea and these are supposed
484:51 - to have semicolons on the end
484:53 - because i can see a little red mark
484:54 - there so the idea is that we are
484:55 - importing
484:56 - uh core identity and key vault secrets
484:59 - identity so that we can get access to
485:00 - stuff we're providing our secret the
485:03 - name of our vault the url that it needs
485:05 - to hit to do that um here it's going to
485:08 - get the secret client options
485:10 - and so it's going to have a bit of
485:12 - retries here if it doesn't work out
485:14 - properly down below we have the secret
485:16 - client so we're passing in the key value
485:18 - uri
485:19 - and then we need our credentials here we
485:22 - didn't type anything out for credentials
485:24 - so i'm a little bit surprised by that
485:26 - but maybe that's not an issue
485:29 - and so what i'm going to do here is i'm
485:31 - just going to go ahead and commit all
485:33 - this actually before we do that we're
485:34 - going to create ourselves a get pod yaml
485:36 - file so git
485:38 - pod init and it says it won't already
485:40 - exist so i'm just going to say no there
485:42 - but what we need to do here is install
485:44 - the
485:45 - cli the azure cli so i'm going to make a
485:47 - new task here called azure
485:52 - we'll say init
485:54 - and then here it will be curl hyphen
485:57 - s capital l https colon slash aka
486:02 - at ms aka is like azure's microsoft's
486:06 - short
486:08 - um
486:09 - shortener
486:10 - so we'll say azure cli deb
486:14 - just have install azure cli
486:17 - deb
486:18 - sudo
486:19 - bash okay so that should install it i'm
486:21 - just going to double check to make sure
486:22 - this i typed this out correctly
486:25 - and we will say allow and paste that in
486:27 - there
486:28 - yeah it's working so what we'll do is
486:29 - we'll commit our stuff here
486:33 - add that there
486:35 - so
486:36 - commit
486:37 - code
486:39 - and i guess we do want to run this line
486:40 - so i'm going to go here and run it
486:42 - because the way it's going to get the
486:43 - credentials it's going to grab it once
486:45 - we authenticate with the azure cli
486:47 - because it'll set something locally and
486:49 - that's why we didn't have to pass in a
486:50 - key or anything
486:54 - and maybe before we do that let's just
486:56 - double check our permissions
486:58 - so if we go to the im control here we go
487:01 - to our role assignment
487:03 - or actually um
487:05 - access policies
487:07 - we'll go to angie brown secrets
487:09 - get lists
487:10 - so we should be able to do it we have
487:12 - all the permissions to do it so now that
487:14 - that is done we're going to go a z
487:16 - login hyphen hyphen use device
487:20 - code
487:25 - and hit enter
487:28 - and so here we have this address here
487:31 - that will help us log in so we'll go
487:33 - here to this address we'll copy this
487:34 - code
487:36 - we'll enter the code in
487:38 - we'll say oops that's not what i wanted
487:40 - to do we'll paste that in again
487:43 - and we will hit uh sign in here and
487:45 - we'll say continue
487:49 - and that is all done there
487:51 - so now what we can do is try to run the
487:53 - application so we're going to type in
487:55 - net build to build this application see
487:57 - if we get any errors
487:59 - if we get errors i'm not really
488:00 - surprised because we had done so much
488:02 - typing here but we will go back to our
488:04 - code and take a look here so on line 38
488:07 - um
488:09 - there's no semicolon
488:10 - a very small mistake so i don't mind
488:12 - that if that's all the mistakes we wrote
488:14 - that'll be great
488:16 - fix
488:16 - semicolon missing semicolon
488:22 - and we will commit nsync
488:26 - dotnet builds
488:28 - and
488:29 - all projects are up to date so that
488:31 - looks pretty good to me we'll do run and
488:33 - if it works that'll be great
488:35 - i don't know if we explained the whole
488:37 - program but we'll just run this here
488:46 - and it should run
488:48 - oh you know what it's prompted us
488:50 - probably
488:51 - no there we go so we'll type in hello
488:53 - world
488:55 - uh hit enter
488:58 - great so let's take a look at what this
488:59 - did so the idea is that we have a client
489:01 - and the id and we want to grab the
489:04 - secret so that's the secret name we have
489:05 - up there my secret and if we console log
489:08 - it out it gives us hello mars because
489:09 - that's what we set
489:10 - and then we are prompted to enter our
489:13 - own input
489:14 - and we use this to set the secret and
489:17 - then we immediately fetch and say what
489:19 - is the key what is the value now if you
489:20 - notice it says my secret here
489:23 - hello world
489:25 - and but when we go get the secret again
489:28 - um down below let me just see here so we
489:31 - have and the reason why it's not
489:33 - printing out as expected is because we
489:35 - don't have
489:36 - this is something i'm not a huge fan of
489:38 - in net but you actually have to add the
489:41 - um
489:43 - backslash end like that otherwise it
489:44 - gets a bit confusing here as you can see
489:46 - i'm a little bit confused about our
489:48 - output
489:48 - so we have set secret
489:51 - key value my secret
489:53 - hello world that's new value and then
489:56 - it goes down below and we
489:58 - print out the secret value again from
490:01 - the actual value object so these are the
490:03 - ones that are just locally that we set
490:05 - right but this is the actual object but
490:07 - the object is still saying hello mars
490:08 - but we updated up here and the reason
490:10 - why that happened is because it takes
490:12 - time for those changes to propagate so
490:14 - if we were to call it again it would
490:15 - have the right value but you know it has
490:17 - to replicate it and so that takes a bit
490:19 - of time so sometimes
490:21 - you have to wait uh for the stuff so we
490:23 - go ahead and we delete it and then we go
490:25 - start delete secret so we say that we
490:27 - are deleting it and then we go get the
490:29 - secret but we've deleted the secret so
490:31 - how can we see the secret if we delete
490:33 - it and again it just has to do with
490:34 - propagation if we were to go into our
490:37 - vaults and to go to secrets that secret
490:39 - should not be there anymore so just
490:41 - consider that you have to do that i
490:42 - think the original tutorial had like
490:44 - sleep commands i don't know what this
490:45 - loop command is in um
490:47 - in
490:48 - net but it's something probably like
490:49 - sleep
490:50 - you know
490:51 - dot net sleep
490:54 - right so they were running those
490:57 - in between that this that's like two
490:59 - seconds a thousand milliseconds and so
491:01 - they're doing that to make sure the
491:02 - change is there but i thought it was
491:03 - better to show you the fact that you
491:05 - have to wait a little while um but
491:07 - that's it there so we are all done here
491:09 - so we can go over to our resource group
491:11 - and we can delete
491:14 - uh this resource script and all the
491:15 - resources in it
491:19 - all right and we can close that off
491:21 - so there you go
491:23 - [Music]
491:31 - hey this is andrew brown from exam pro
491:33 - and we're taking a look at azure app
491:34 - configuration this allows you to
491:37 - centralize all your app settings in one
491:38 - location is ideal for multi-environment
491:41 - or multi-geographic geography
491:43 - applications because it provides a
491:45 - dynamic way to change application
491:46 - settings without having to restart them
491:48 - but in general if you're building out
491:50 - any kind of application
491:52 - it is actually really useful it also
491:53 - integrates with azure key vault which
491:55 - stores application secrets which we'll
491:56 - talk about soon enough
491:59 - azure app configurations main benefits
492:01 - are a fully managed service that can be
492:03 - set up in minutes
492:04 - i would say seconds to be honest
492:06 - flexible key representations and
492:09 - mappings tagging with labels point in
492:11 - time replay of settings dedicate ui for
492:13 - feature flag management which is to me
492:16 - the best part of azure app configuration
492:18 - comparison of two sets of configurations
492:20 - on custom defined dimensions enhanced
492:22 - security through azure managed
492:24 - identities encryption of sensitive
492:26 - information at rest and in transit
492:28 - native integration with popular
492:29 - frameworks and when we say popular
492:31 - frameworks we're talking about things
492:32 - like net core asp.net net java spring
492:36 - other javascript node.js python azure
492:38 - functions.net core so
492:40 - no rails no larvael so no next to so i'm
492:44 - not sure what they're saying when they
492:45 - say popular frameworks quotations but
492:48 - you know you can still integrate them
492:50 - because they do have language support
492:52 - but there you go
492:53 - [Music]
492:57 - all right let's take a look at the tiers
492:59 - for azure app configuration and we got
493:01 - to here we got free and standards let's
493:03 - see what the difference is so the first
493:05 - is resources per subscription one for
493:07 - free unlimited for standard for storage
493:10 - per resource 10 megabytes for free
493:12 - versus 10 or sorry one gigabyte for
493:15 - standard revision history seven days
493:16 - versus 30 days for request quotas a
493:19 - thousand versus 30 000 per hour no sla
493:21 - on the free tier first security
493:23 - functionality this stuff is important
493:25 - you get encryption with microsoft
493:26 - managed keys uh hmac and azure
493:29 - ad so active directory authentication
493:32 - role-based access control support
493:33 - managed identity service tags so a lot
493:35 - of stuff in the free tier but in the
493:37 - paid tier you get customer managed
493:39 - managed keys of cmks uh private link
493:41 - support and then for the cost it's free
493:44 - for the free tier of course and for
493:45 - standard it's 120 per resource
493:48 - with uh 0.06 per 10 000 requests with
493:52 - 200 000 requests are included in a daily
493:54 - charge so you do get
493:56 - you might like look at those requests
493:57 - and say oh that stacks up but you get
493:59 - that 200 000 uh free
494:02 - uh daily um and for soft release is not
494:05 - supported in the free tier pricing okay
494:08 - so soft delete mean like i deleted
494:10 - something and i didn't actually want to
494:12 - delete it so let's bring it back it's
494:14 - like putting in the trash can and
494:15 - pulling it right back out but there you
494:16 - go
494:17 - [Music]
494:22 - all right let's take a look at
494:23 - configuration explorer this is where
494:24 - you're going to be looking at your data
494:26 - for the most part that you've inserted
494:28 - and so the nice thing is that you go
494:30 - here you can see the key and the value
494:32 - even though it's hidden if you click
494:34 - values it will show you them all which
494:36 - for me doesn't feel super secure but i
494:38 - guess that's how it works
494:40 - again it just gives me a feeling of
494:42 - not being secure but it is secure um and
494:45 - the idea is that you'll be able to
494:47 - create a key value but actually it's
494:49 - either key value pair or a key vault
494:51 - reference and this is the way that
494:52 - you're able to reference uh secrets
494:55 - within your vault
494:57 - so there's not really much to it it's
494:58 - not super complicated
495:00 - this view is fine it does what it says
495:02 - it does
495:03 - but there you go
495:05 - [Music]
495:09 - all right let's take a look at feature
495:10 - manager for azure app configuration this
495:12 - is the meat and potatoes this is the
495:14 - thing that you really want to be using
495:16 - as your app configuration for it's for
495:18 - feature flags so what is a feature flag
495:20 - a feature flag provides an alternative
495:21 - to maintaining multiple feature branches
495:23 - in source code a condition within the
495:25 - code enables or disables a feature
495:27 - during runtime and this makes it easier
495:29 - to roll back or do a to b testing for
495:31 - new functionality so azure app
495:32 - configuration feature manager allows you
495:34 - to add featured flags which can be then
495:37 - accessed via code so the idea is you say
495:40 - enable a feature flag you name the flag
495:41 - whatever you want so i called flash
495:43 - cards assuming i need a flash card
495:44 - feature
495:46 - you can label them of course and then
495:47 - you are providing
495:49 - code now i didn't see
495:51 - much for things outside of net and or
495:54 - well c-sharp and java so i'm hoping that
495:57 - their sdks or there's some kind of
495:59 - library for ruby python node.js
496:02 - things like that but
496:04 - it might just be only for c sharp so net
496:06 - apps and java now let's take a talk
496:10 - let's talk about an advanced feature of
496:12 - feature manager which is feature filters
496:14 - this allows filtering of features so a
496:17 - feature filter consistently evaluates
496:19 - the state of a feature flag and a
496:20 - feature flag supports three types of
496:22 - built-in filters targeting time window
496:24 - and percentage which is confusing
496:26 - because when you go and turn it on it
496:28 - says targeting time window and custom so
496:30 - the thing is is that
496:32 - both targeting and percentage is within
496:35 - the targeting thing and then custom is
496:36 - its own thing so that is those little
496:39 - inconsistencies that microsoft likes to
496:41 - present you in their documentation so
496:44 - if we were looking at a time window and
496:46 - i think this is really cool but you
496:48 - could say okay this feature starts on
496:50 - this date and it's between this time and
496:53 - i guess starts on and ends on that time
496:56 - so it's like if you just want to run a
496:58 - feature for you know 10 days you could
497:00 - do that
497:01 - then there's a percentage thing so the
497:03 - idea is that you say okay i'm going to
497:05 - define two different groups and for
497:06 - group a they're going to get 50 of this
497:09 - feature in group b they'll get 50 of
497:10 - this feature so it'll change
497:12 - the amount the feature is available uh
497:14 - per per group like how many in that
497:16 - group will get it and then targeting
497:18 - would be like you could say i want it
497:20 - for these users so when they say
497:22 - targeting it literally just says user
497:23 - and you put a name in it's and it and
497:25 - it's just a a text value it's not like
497:28 - it's linked to an azure active directory
497:30 - user anything like that and then they
497:31 - have custom filters which can be created
497:33 - based on different factors such as
497:34 - device use types geography
497:37 - location etc when i clicked on it it
497:40 - wasn't very clear on how to do custom
497:42 - filters but the idea is that you can go
497:44 - beyond these ones if you want to uh you
497:46 - know have different values so there you
497:48 - go
497:49 - [Music]
497:53 - all right let's take a look here at the
497:54 - microsoft graph api which exposes rest
497:57 - apis and sdks to access data from the
497:59 - following microsoft cloud services the
498:01 - first is m35 core services where we have
498:04 - bookings calendar delve excel m365e
498:07 - discovery microsoft search onedrive
498:09 - onenote outlook exchange people planner
498:12 - sharepoint teams to do
498:14 - workspace analytics uh
498:16 - so basically a lot of services for
498:18 - enterprise mobility and security we have
498:20 - advanced threat and analytics threat
498:22 - protection which is atp azure azure 8d
498:26 - identity manager intune then we have
498:28 - dynamics 365 business central for
498:31 - windows 10 services we have activities
498:33 - devices notifications universal print
498:35 - and for support sdks we have it for
498:38 - android ios angular asp.net go
498:40 - javascript node.js java php powershell
498:43 - python and they had ruby but i i looked
498:46 - into it and um they it doesn't work
498:49 - anymore and they say they're going to
498:50 - make a new one but i'm still waiting for
498:52 - it so i always get the short stick when
498:54 - it comes to ruby
498:56 - and the data is accessible via the
498:57 - unified endpoint at
499:00 - graph.microsoft.com but how do we use
499:02 - this stuff let's take a look here so for
499:04 - node.js we would install the microsoft
499:06 - graph client and then here as an example
499:09 - we are sending an email so the idea is
499:11 - getting my pen tool out here we create a
499:13 - client we're defining uh some mail and
499:16 - then we're calling our endpoint um via
499:19 - the api that's basically how you're
499:20 - always going to be doing it post that
499:21 - message and that's how the graph api
499:24 - works so there you go
499:25 - [Music]
499:30 - all right let's take a look at microsoft
499:31 - graph connectors so connectors offers a
499:33 - simple way to bring external data into
499:34 - microsoft graph and enhance m365
499:37 - intelligence experiences so you might
499:39 - want to build a custom connector to
499:41 - integrate with services that aren't
499:42 - available as connectors built by
499:44 - microsoft using microsoft graph
499:46 - connectors rest api and you can use the
499:48 - microsoft graph connector api to create
499:50 - and manage external data connections
499:52 - define and register the schemas of the
499:54 - external data types ingest external data
499:56 - items and microsoft graph sync external
499:58 - groups and microsoft search indexes all
500:01 - your m365 data to make it searchable for
500:04 - users with microsoft graph connectors
500:07 - so your organization can index
500:09 - third-party data so it appears in the
500:11 - microsoft search results so you're like
500:12 - okay
500:14 - where's we have microsoft graph
500:15 - connectors where are they and that's
500:18 - where i ended up on the connector
500:19 - gallery so microsoft has a gallery
500:22 - website containing a hundred plus
500:23 - connectors it's at microsoft.com for its
500:26 - microsoft search forward slash
500:27 - connectors and i was a bit confused at
500:29 - first but now i understand what it is
500:31 - but the idea is that they have a bunch
500:33 - of different connectors from a bunch of
500:35 - different third-party providers so
500:36 - microsoft has some but there's a lot
500:39 - from a bunch of different providers some
500:41 - example connectors could be amazon s3 by
500:44 - ba insights azure blob storage by
500:46 - accenture google cloud sql by ba
500:49 - insights ibm connections by
500:52 - raytheon google drive iba insights azure
500:54 - devops by microsoft and connectors might
500:57 - not be free and their instructions
500:59 - greatly vary based on the implementer so
501:01 - this is not like a unified thing it's
501:03 - you go there and it's like some other
501:05 - app that's used for it so um
501:08 - yeah it's kind of weird because it just
501:10 - doesn't feel very standardized but i
501:11 - guess it's just kind of a library of
501:14 - services that can connect things kind of
501:17 - like zapier but as if there was all
501:18 - these specialized services
501:20 - by individual third-party cloud vendors
501:22 - but there you
501:26 - go well let's take a look here at
501:29 - microsoft graph data connect which
501:30 - augments microsoft graph's transactional
501:32 - model with an intelligent way to access
501:34 - rich data at scale
501:36 - so they're just meaning about getting
501:37 - data out of
501:39 - microsoft and putting it into some kind
501:41 - of tool that can transform it do ai on
501:44 - it things like that so microsoft graph
501:46 - data connect uses azure data factory to
501:49 - copy n365 data to your application
501:52 - storage at your at configurable
501:54 - intervals and the way this is going to
501:56 - work is you're going to have to go to
501:58 - the microsoft graph data connect
502:00 - under your m365 admin center go to
502:04 - settings work settings and services and
502:06 - from there you will turn it on and the
502:08 - idea once it's turned on then you can go
502:11 - over to azure data factory and by adding
502:13 - it as a data set to your pipeline you
502:15 - now have the ability to access your data
502:17 - and if you know how data factory works
502:19 - then you know you can do a bunch of
502:20 - stuff with it store it in
502:22 - a blob storage send it to synapsis do
502:24 - whatever you want
502:26 - from there okay
502:27 - [Music]
502:31 - hey this is andrew brown from exam pro
502:32 - and we are taking a look at microsoft
502:34 - graph so this is a gateway service to
502:36 - programmatically access m365 windows 10
502:40 - enterprise mobility plus security and so
502:42 - here's the big graphic but it's composed
502:45 - of three elements so microsoft graph api
502:47 - the microsoft graph connectors and the
502:49 - microsoft data connect so here in the
502:52 - middle we have graph api this is where
502:54 - you programmatically access various
502:56 - microsoft services you have connectors
502:59 - these are ways of getting external data
503:00 - in to microsoft graph and then you have
503:03 - graph data connect this is a way of
503:05 - storing data
503:07 - here so hopefully that makes it pretty
503:08 - clear
503:10 - i do find microsoft graph a little bit
503:11 - confusing because there's a lot going on
503:13 - here but hopefully as we work through
503:15 - these slides here it will make more
503:16 - sense okay
503:21 - [Music]
503:26 - hey this is andrew brown from exam pro
503:27 - and we are taking a look at azure front
503:29 - door which is a traffic manager traffic
503:31 - accelerator global load balancer and
503:33 - content distribution network cdn and yes
503:36 - it does all those things and i think
503:38 - that what creates a lot of confusion for
503:40 - people when they first look at azure
503:42 - front door uh but the idea here is azure
503:44 - front door is a modern application
503:46 - delivery network platform so the idea is
503:48 - that it can do all these things so
503:49 - provide a secure scalable cdn dynamic
503:52 - site acceleration global https load
503:55 - balancing for your global web
503:56 - application that's the definition that
503:57 - azure has
503:58 - but uh yeah there are going to be
504:00 - services that have overlap with azure
504:02 - front door like azure cdn and things
504:05 - like that
504:06 - but just understand that this is a very
504:08 - robust thing and something that you're
504:09 - going to want to use quite often for
504:11 - your applications so azure front door
504:14 - features caching like cdns with rules
504:16 - expiring policies resilience by
504:18 - distributing incoming traffic across
504:20 - multiple or different azure regions
504:23 - cookie based session affinity for
504:24 - restful applications when traffic needs
504:26 - to be redirected back to the same uh
504:28 - back end so that's for
504:30 - stateful applications health probes
504:33 - which are very common um at either the
504:35 - dns level or the load balancing level
504:37 - but since it is both here uh it has it
504:39 - so to determine uh the healthiest and
504:41 - closest back end to the client request
504:44 - it can have a waf attached to it so
504:46 - protecting your back ends
504:47 - from malicious attacks and
504:49 - vulnerabilities i believe that is using
504:51 - azure's waf policy so you're just
504:53 - basically attaching
504:54 - a policy to that there your url redirect
504:58 - so redirecting traffic based on a
505:00 - variety of things so protocols such as
505:02 - http or https the host name the path the
505:05 - query string
505:06 - not just that there's a lot of stuff in
505:08 - there url rewriting so with the powerful
505:10 - engine for rewriting incoming requests
505:12 - to a different back-end request and so
505:14 - in the simplest sense this is what azure
505:16 - looks like you have a front you have
505:18 - front-ends
505:19 - or domains where you're bringing stuff
505:21 - into
505:22 - azure front door and then you have your
505:24 - back ends
505:25 - so that we can filter stuff there
505:27 - there's actually a lot more components
505:28 - going on there we will talk about them
505:30 - or we'll have a diagram here to show you
505:32 - but this is the basis of azure front
505:34 - door
505:35 - [Music]
505:39 - well let's take a look at the core
505:40 - components for azure front door uh and
505:43 - the reason i specifically made this
505:45 - graphic was because there was no real
505:48 - uh representation or example that i
505:50 - could find and so i thought it was very
505:52 - important because when we start to talk
505:53 - about routing the path looks completely
505:56 - different from this it's similar but
505:58 - it's different so there is uh i don't
506:00 - know why there's inconsistencies there
506:02 - there is azure front door classic but i
506:04 - was using the latest version and so just
506:07 - understand that if you're confused i'm
506:09 - confused too but this
506:10 - is uh the visualization of what i think
506:13 - it works when you're actually using it
506:15 - within the gui okay so the idea is you
506:18 - have a profile and a profile as far as i
506:20 - can understand contains all the other
506:22 - azure front door components and then you
506:25 - have this idea of an end point and this
506:26 - is the pathway from the front end to the
506:29 - back end uh and within your endpoint you
506:32 - need to have some kind of origin so the
506:34 - origin is saying what to point at so
506:37 - what to point it at the back end and
506:39 - even though it says origin
506:41 - origin here
506:43 - it's really origin group so you actually
506:44 - assign origin groups to an endpoint and
506:47 - so it can rotate to multiple origins
506:49 - then you have a route and so the route
506:52 - is where you can apply your rules and
506:54 - rule sets to determine more routing
506:57 - things then we have the ability to have
506:59 - a waff policy before it reaches the back
507:02 - end
507:03 - and so hopefully that makes it a little
507:05 - bit more clear um but uh one thing that
507:08 - is interesting when we talk about
507:10 - routing is that
507:11 - when they talk about the order of data
507:13 - coming in they actually describe it as
507:15 - if the waf policies over here but then
507:18 - when you look at the interface and give
507:20 - me stuff they put it on the right side
507:22 - so whether it is
507:24 - um you know here or there i don't think
507:26 - really matters in practicality but uh
507:29 - you know i was trying to find that out
507:30 - um and the way data gets into azure
507:33 - front door is that it's going to uh hit
507:36 - a edge location and then that edge
507:37 - location is going to then uh trigger i
507:41 - don't know why it says trigger but the
507:42 - idea is to go to the front end it should
507:44 - be more like match
507:46 - uh but i have this kind of like
507:48 - expressway or highway to kind of
507:49 - emphasize that this is going to be very
507:51 - very fast to get to
507:53 - uh your azure resources so hopefully
507:55 - that makes it uh pretty clear about the
507:57 - components there but it'll be crystal
507:59 - clear when we do the follow along okay
508:02 - [Music]
508:06 - all right let's take a look at what
508:07 - tiers we have for azure front door
508:09 - because the different tier is going to
508:11 - determine the feature set that is
508:13 - available to us so azure offers two
508:14 - tiers for azure front door the first is
508:17 - standard or delivery optimized so the
508:19 - idea is you have content delivery
508:21 - optimize offering both static and
508:22 - dynamic contact acceleration we get our
508:25 - global load balancing our ssl offloading
508:28 - domain name domain and certificate
508:30 - certificate management enhanced traffic
508:32 - analytics basic
508:34 - security and capabilities and that
508:35 - sounds like a lot but with premium we
508:38 - get additional security optimized stuff
508:40 - so we get everything we get in standard
508:42 - uh but the idea here is we can uh have
508:45 - extensive security capabilities across
508:47 - our web application firewall you can
508:49 - have bot protection
508:51 - private link support which i believe is
508:52 - for internal routing of traffic keeping
508:55 - things within the azure network an
508:57 - integration with microsoft threat
508:59 - intelligence and security analytics uh
509:01 - standard is really good for most use
509:02 - cases but if you need that additional uh
509:05 - security that's where premium's gonna
509:06 - come into play okay
509:09 - [Music]
509:13 - all right let's talk about how routing
509:15 - works in azure front door because there
509:17 - are a lot of components and it's
509:18 - important to understand the flow and
509:20 - also the different types of routing that
509:22 - are available to us so routing is the
509:23 - path
509:24 - an https request uh
509:27 - from the user will take to reach a
509:28 - backend service configured in azure
509:31 - front door so at the start you have that
509:33 - http request from the user coming from
509:35 - their browser most likely and then the
509:38 - idea is it'll go over the internet and
509:39 - find the closest edge location edge
509:41 - location is just a a point of presence a
509:45 - data center or uh
509:47 - software hardware that is on on the edge
509:49 - of a network that allows you to enter
509:52 - the azure network and so from there you
509:54 - are now going to be in the azure network
509:57 - and so you will be matched with a azure
509:59 - front door profile from there it will
510:01 - then uh if you have waf rules it will
510:03 - evaluate the web application firewall
510:05 - rules and if everything's okay it will
510:08 - then match you to an azure front door
510:10 - route and then from that route it will
510:13 - then go through the engines rule to
510:15 - evaluate where to route things if there
510:17 - is cached information it will
510:19 - immediately return that information
510:21 - right away um but if there's not it will
510:23 - just proceed to the next step which is
510:25 - to send you to an origin group an origin
510:27 - group is just a grouping of origins
510:30 - and an origin is how you send a request
510:33 - to a back end so you know hopefully
510:36 - we'll see these components over and over
510:37 - again so it's very clear how they work
510:39 - together but let's talk about the four
510:41 - traffic routing methods that are
510:42 - available in azure front doors so the
510:44 - first is based on latency so requests
510:46 - are sent to the lowest latency back-ends
510:48 - acceptable with a sensitivity range you
510:51 - have priorities so requests are sent
510:52 - based on a user-defined number you have
510:55 - weighted so these are requests that are
510:57 - distributed to back-ends according to
510:59 - the weight coefficient and session
511:01 - affinity so requests from the same end
511:03 - user gets sent to the same back end so
511:05 - these are for state full-back and
511:07 - stateful applications this kind of
511:09 - functionality for routing like if you're
511:10 - using aws you'd see this functionality
511:13 - in rough d3 but in the case of azure
511:15 - we're seeing these within front door for
511:18 - this modern application delivery stack
511:20 - okay so you know hopefully that gives
511:21 - you an idea but yeah there you go
511:24 - [Music]
511:29 - we are looking at azure front door and
511:31 - it's time to take a closer look at the
511:32 - origin component so the origin is what
511:34 - azure front door will point at to serve
511:36 - up to the end user and origin is the end
511:39 - point that points to your back end so
511:42 - here's the example of the form of the
511:44 - information you'll need to fill out and
511:46 - the most important field here is the
511:47 - origin type because that is
511:50 - what's going to say what are the
511:51 - supported backends or supported origin
511:54 - so here we have azure blob storage azure
511:56 - storage for static website hosting cloud
511:58 - service yes there is a azure service
512:00 - called azure cloud service i cannot
512:02 - remember what it does but
512:05 - it is a service we have azure app
512:07 - services that's a place where we
512:08 - commonly deploy apps
512:10 - static web app
512:12 - api management
512:14 - application gateway public ip address so
512:17 - that would just be pointing to an ip not
512:19 - necessarily a service
512:21 - azure traffic manager azure spring cloud
512:24 - spring cloud is for java applications i
512:26 - believe azure container instances so aci
512:29 - or custom so you just provide a host
512:31 - name to other fields that are very
512:32 - important for origin is the priority and
512:35 - the weight so priority determines who to
512:37 - send traffic to first and this is going
512:38 - to be a number between one and five the
512:40 - lowest number is the higher priority
512:43 - back ends can have the same priority
512:44 - number for weights this allows you to
512:46 - determine the split of traffic
512:47 - distribution between origins of the same
512:49 - priority so if you had a priority one
512:52 - and somebody was 50 50 then they'd get
512:54 - 50 50 split between them but this number
512:57 - can between uh be between one to one
512:59 - thousand the default value is 50.
513:02 - and uh you know this is what it looks
513:04 - like when you add an origin uh to an
513:06 - origin group so you can see the status
513:08 - is enabled the priority is one the
513:10 - weight is one thousand uh but there you
513:12 - go
513:13 - [Music]
513:17 - so we just talked about origins for
513:19 - azure front door let's talk about origin
513:21 - groups so origin groups are a collection
513:22 - of origins and an origin has to belong
513:26 - to an urgent group so you can't just
513:27 - create an origin and let it float around
513:29 - in azure it's always going to be
513:30 - assigned to one
513:31 - and
513:32 - when you create an azure front door
513:33 - profile by default you will have an
513:36 - origin group called default origin group
513:38 - and so here it is and here you can see
513:41 - we have a default route for whatever we
513:43 - set up for this um azure front door
513:46 - so a few other things you need to know
513:48 - because there's more than just grouping
513:50 - it's the fact that you can apply things
513:51 - to that group and the two things you can
513:53 - apply are health probes to check the
513:55 - health of your origins and load
513:56 - balancing settings to balance uh
513:59 - determine the balancing of your origins
514:01 - and in order for inbound traffic to
514:04 - reach an origin group an endpoint needs
514:05 - to be associated to the origin group via
514:07 - a route so there's a button in the um
514:11 - in the interface so you'll see which is
514:12 - the associated endpoint to route it's
514:14 - that simple but we will explore health
514:16 - probes and load balancing settings a
514:18 - little bit more just understand that
514:19 - they belong to the origin group and
514:22 - those aren't just separate features that
514:23 - are floating around in the azure front
514:24 - door
514:26 - ui or gui there okay
514:29 - [Music]
514:34 - all right let's take a look here at
514:35 - health checks also known as health
514:37 - probes and that's what it will be called
514:39 - when you look at it in the interface but
514:40 - in the documentation they'll call them
514:42 - health checks and this is a feature
514:43 - found under origin groups and then it
514:46 - will apply to your origin so this allows
514:47 - you to ping a back end to determine if
514:49 - it has a healthy response
514:52 - if there is a healthy response which
514:54 - would be determined by status 200 okay
514:56 - things will get routed to it if it is
514:58 - not healthy or considered unhealthy
515:00 - then simply traffic will not be routed
515:02 - but instead rather to other healthy back
515:05 - ends if there are other origins
515:07 - configured so you might be asking well
515:09 - what is a http response code just in
515:11 - case you don't know what it is but the
515:13 - idea is that when a user sends an http
515:15 - request a http response is returned and
515:18 - so for http responses they will have a
515:20 - response code to communicate how
515:22 - back-end servers interpreted the request
515:25 - and a response code is a number that
515:27 - coordinates to what has happened and
515:28 - there is a lot of them so but i'm just
515:31 - going to show you the most popular one
515:32 - so 200 for okay 403 for forbidden 404
515:37 - for not found 500 for internal server
515:40 - error i'm not kidding there's like 100
515:42 - of these but these are the most common
515:44 - ones that you've probably seen before
515:46 - but they all mean something okay
515:49 - [Music]
515:53 - all right let's take a look at load
515:54 - balancing settings this is another
515:56 - option that is available under the
515:58 - origin group so this allows you to
515:59 - define what sample set is needed to be
516:02 - used to call the backend as healthy
516:05 - or unhealthy so this is something you're
516:07 - going to be using alongside with health
516:10 - checks or health probes so the idea is
516:12 - you have the sample size so how many
516:14 - times
516:15 - uh or how many times do we need to
516:17 - consider how many of the sample size
516:20 - have to be successful and the latency
516:23 - betw uh between those okay so the
516:26 - latency sensitivity with the value of
516:28 - zero we're just talking about um this
516:30 - last value here that if you set it to
516:32 - zero it means that it will always send
516:34 - to the fastest available backend else
516:36 - front door will round robin traffic
516:38 - between the fastest and the next fastest
516:40 - back ends with the configured latency
516:42 - sensitivity so there you go
516:45 - [Music]
516:49 - so one thing you can do in azure front
516:51 - door is assign custom domain names and
516:53 - actually you can assign multiple custom
516:56 - domain names within azure front door
516:59 - to show you the form it's not too
517:01 - complicated but the idea is that
517:03 - for dns management you can either use
517:05 - azure managed dns or another provider
517:08 - you are specifying the dns zone name for
517:11 - the custom domain you're going to add
517:13 - that custom domain and you do have a
517:14 - couple options here for http where you
517:17 - can have afd or bring your own
517:20 - certificate for
517:22 - enabling https and you can apply either
517:25 - tls 1.0 or 1.2 of course 1.2 is newer so
517:29 - that's what you should be using
517:31 - but there's really not much to say about
517:33 - that other than you can attach custom
517:35 - domains
517:36 - very straightforward very simple okay
517:39 - [Music]
517:43 - all right we're taking a look here at
517:45 - endpoint manager so endpoint manager
517:47 - provides an overview of endpoints you've
517:48 - configured for your azure front door not
517:51 - to be confused with microsoft endpoint
517:53 - manager which is a security service
517:56 - specifically for managing external
517:58 - devices so devices to register your
518:00 - company for organizations that are using
518:02 - outside of your office so no relation to
518:04 - that product just very similar naming so
518:07 - this is an example of an endpoint that
518:09 - you will see in azure front door and the
518:12 - endpoint really
518:13 - means like the entry point and so that's
518:16 - going to be the domain name so if you do
518:18 - not have a custom domain azure provides
518:20 - you one azure fd.net it might vary based
518:23 - on um capacity and things like that but
518:26 - the idea is that it goes from domains to
518:28 - the origin group to the routes to
518:31 - security
518:33 - there so you know hopefully that is
518:35 - pretty darn uh clear endpoint manager
518:38 - lists how many instances of each element
518:40 - are created with an endpoint and the
518:41 - associated status of each element will
518:43 - also be displayed so there you go
518:47 - [Music]
518:51 - let's take a look at routes for azure
518:53 - front door so a route maps domains and
518:55 - matching
518:56 - path patterns to a specific origin group
519:00 - so here uh the idea is that we can
519:02 - define what domain that we want to use
519:06 - for this route we can say what patterns
519:08 - we're matching on so a wild card is
519:10 - going to match on everything is it going
519:12 - to be treated as a redirect to https
519:15 - because if you have http you probably
519:17 - want to go there and you're going to
519:18 - associate that with an origin group and
519:20 - you can provide an origin path
519:22 - and define the
519:24 - forwarding protocol routes can also have
519:26 - caching and compression applied so this
519:28 - is where you would uh start doing that
519:31 - and routes from rule sets can be
519:33 - associated to routes to apply
519:34 - intelligent
519:36 - routing so that's where you get that
519:37 - routing association
519:39 - but yeah there you go
519:41 - [Music]
519:45 - let's take a look at traffic
519:46 - acceleration for azure front door
519:48 - sometimes no also known as global
519:50 - acceleration depending on your closer
519:52 - provider but the idea here is without
519:54 - any requirements to make any
519:56 - modifications to your application code
519:57 - azure front door can speed up global
519:59 - delivery of your application so azure
520:02 - achieves this traffic acceleration by
520:04 - directing traffic to the nearest edge
520:06 - location to on ramp the azure
520:10 - to the into the azure network so the
520:11 - idea is you have
520:13 - somebody that wants to connect they're
520:15 - going to connect to that edge location
520:16 - and an education is just basically a
520:18 - data center a point of presence
520:20 - hardware on the edge of a network and
520:22 - then once you're in there everything
520:23 - from here to the uh origin to the uh to
520:27 - the back end is going to be through the
520:29 - azure network and it's going to be like
520:31 - being on an expressway so traffic that
520:33 - is following within the internal azure
520:35 - network travels at accelerated speed
520:37 - while also taking the most direct path
520:39 - think of it as an expressway and think
520:42 - of
520:43 - these edge locations as on-ramps so
520:46 - there you go
520:47 - [Music]
520:51 - let's take a look here at rule sets for
520:53 - azure front door and this allows you to
520:55 - customize how http requests get handled
520:58 - at the edge and provides more controlled
521:00 - behavior uh to your web application so
521:03 - the idea is that you have a rule set and
521:06 - within this you're going to define rules
521:08 - gonna get my pen tool out here so you
521:09 - name it and you can provide a condition
521:12 - and then an action and we have a lot of
521:14 - options here
521:16 - for things you can do so let's take a
521:18 - look at what those options are for
521:19 - conditions we have a lot of stuff device
521:21 - type http version cookies arg strings
521:25 - query strings a bunch of things we can
521:27 - do with the requests such as the body
521:29 - file name file extension etc so lots of
521:32 - conditions available to us then for
521:34 - operators we have equals contains less
521:36 - than greater than and a whole host of
521:38 - other even including regular expressions
521:42 - for actions uh this is where you really
521:44 - get a very interesting things but you
521:46 - can do cache expiration so the idea is
521:49 - that the action is to expire the cache
521:52 - you can have a cache key query string so
521:55 - the idea is you can cache every unique
521:57 - url so it's just a bit more complex
522:00 - caching you can modify the request of
522:02 - the header or modify the response of the
522:04 - header that might be useful for
522:05 - something to do you can rewrite urls uh
522:08 - so or sorry redirect urls so the idea is
522:11 - that you can send it to different types
522:13 - of redirects such as
522:16 - a temporary
522:18 - one or move like 301 or 307 or 308
522:22 - you can rewrite urls
522:24 - you provide a source pattern and then a
522:26 - destination you can override the origin
522:28 - group there so a lot of stuff you can do
522:31 - within those rule sets
522:32 - [Music]
522:36 - hey this is andrew brown and this fall
522:38 - along we're going to be utilizing azure
522:40 - front door so before we do that we're
522:42 - going to need ourselves a storage
522:44 - account to set up some static website
522:46 - storage so what i want you to do is go
522:48 - to storage accounts
522:49 - and there is a static web app host
522:52 - hosting that we could use but we're
522:53 - going to do the old school way because
522:55 - it's always great to learn a few
522:56 - different ways to do things in azure and
522:59 - so there's probably another follow along
523:00 - where we use this static static site
523:03 - thing there so what we're going to do is
523:05 - create a new
523:07 - uh we're going to create a new um
523:10 - resource group i'm going to call this
523:12 - one
523:12 - my
523:14 - azure front door
523:16 - and we'll say ok and from here we'll
523:18 - have to name it something so we'll just
523:19 - say my azure front door or sorry we'll
523:22 - do static storage
523:25 - storage eight eight eight eight if you
523:27 - can't get the four eights because i'm
523:28 - using it then just give it a different
523:30 - um
523:31 - number sequence because these are
523:32 - treated like fully qualified domains
523:34 - choose like a us region so that we're
523:37 - all doing the same thing for performance
523:39 - we're going to stick with standard it
523:40 - doesn't matter if it's on geo-redundant
523:42 - we're not doing anything fancy there so
523:43 - what i want you to do is go ahead and
523:45 - hit review and create and then give it a
523:48 - moment and hit create and we're going to
523:49 - wait for that to finish deployment and
523:52 - from there we'll then have to enable
523:54 - static website so it doesn't take
523:56 - too long to deploy so we'll just give it
523:58 - a moment
524:02 - and while that's going we do need to
524:04 - create ourselves a
524:07 - index html file so you're going to need
524:09 - some kind of editor so i'm just opening
524:10 - up visual studio code on my computer
524:13 - just give it a moment
524:18 - there and i'm just creating a new file
524:21 - here this is taking it taking a moment
524:22 - to load
524:24 - okay so and what we need to do is just
524:26 - create an index.html file these are
524:28 - really basic you can find them anywhere
524:30 - online but this one's going to say hello
524:31 - mars hello mars
524:33 - very very simple so i'm going to go
524:35 - ahead and save this to my desktop so
524:36 - save as
524:38 - and
524:39 - [Music]
524:41 - show local maybe there we go that's a
524:43 - little bit more sane and we'll just say
524:44 - index.html
524:48 - and we'll now go to the resource
524:51 - and we're going to go to website static
524:53 - as a blade on left hand side so we can
524:55 - do
524:56 - a stack website stuff we're going to say
524:57 - index html i think we have to set that
524:59 - for it to work i cannot remember if
525:00 - that's the case we have a primary
525:02 - endpoint this is uh these matter for
525:04 - later but we'll come back to that and
525:06 - we're going to go over to our containers
525:08 - and we're going to have to create a new
525:09 - container so i'm just going to call that
525:11 - actually we don't because we're going to
525:12 - put it in web so when we turned on
525:14 - static website hosting it already gave
525:16 - us a blob or container a container for
525:19 - us to add our ad or upload our files
525:21 - there so we'll go to upload here and
525:23 - then i'm going to go ahead and select
525:25 - that index.html file and upload it in
525:27 - place
525:29 - we need to change our access level for
525:31 - this uh
525:33 - this uh container to
525:35 - um i think just blob
525:39 - so i'm just double checking here
525:43 - yeah i think it's blob access uh can
525:44 - only be anonymous request container data
525:46 - is not available so we'll say okay i
525:48 - believe that is correct
525:52 - okay uh sometimes you can like we're
525:55 - clicked into it but i think like in my
525:57 - instructions i do it a little bit
525:58 - different so i'm just going to double
525:59 - check to see what the settings are there
526:01 - just so you can see it if you're
526:02 - following my instructions so change
526:04 - access
526:06 - private blob yeah okay so that's fine um
526:08 - so blob access should be okay
526:11 - and so now what we want to do is just
526:12 - test that our our page is working so if
526:14 - we go back to static website here
526:17 - on the left hand side we can grab this
526:19 - primary endpoint and we should be able
526:20 - to post it anywhere here
526:22 - because they have the old tutorial here
526:24 - this is a moment ago so that's why it's
526:25 - a bit confusing but
526:27 - so hello mars is working but this is on
526:28 - static storage this isn't in front of or
526:30 - behind azure front door
526:33 - which is what we'll want to do next
526:35 - so what i want you to do is just close a
526:37 - couple of these tabs out we're going to
526:39 - type in azure front door
526:41 - our front door should be enough front
526:44 - there we go and we have a whole bunch of
526:46 - options we'll just hit create that's a
526:48 - front door we actually do use for the
526:50 - platform we have azure front door we
526:52 - have a quick create i always go custom
526:54 - i don't think i've ever done quick
526:55 - create i just
526:56 - i just have more trust in custom and
526:58 - from here we will choose front door
527:02 - and east us is fine
527:06 - i'm just wondering where our storage
527:08 - account is that's why we should always
527:09 - just kind of
527:10 - set them to be the same place so let's
527:12 - go to storage account
527:14 - i don't think it will matter but i'm
527:15 - just going to double check
527:18 - storage account
527:23 - and this one is in
527:25 - east us so we're going to be okay
527:28 - just make sure they match just so we
527:29 - have less problems
527:31 - and i'm going to call this one my azure
527:34 - front door
527:36 - now there are more functionalities in
527:38 - the premium but standard is fine for us
527:41 - we'll go next to secrets um
527:44 - this is if we want to add a certificate
527:47 - um like you could bring your own
527:49 - certificate um
527:51 - but uh we're not too worried about that
527:52 - we're going to have to add a new
527:53 - endpoint so we're going to say my end
527:55 - point here
527:56 - there's a lot of small steps in here so
527:58 - this should be fun
528:00 - hopefully we don't configure anything
528:01 - wrong here and then we need to add a
528:02 - route
528:04 - say my route
528:06 - and it's going to use the default domain
528:08 - that's totally fine
528:10 - we don't have to do anything it's going
528:11 - to just be on the forward slash asterisk
528:13 - there
528:14 - we do need to create an origin group so
528:15 - we'll go here and create a new origin
528:17 - group so my origin group and then from
528:20 - there we need to add an origin yes it is
528:22 - very
528:23 - squirrelly going through all this
528:25 - my origin the origin type is going to be
528:27 - azure
528:28 - or sorry storage static website and from
528:31 - here we need to choose the right one we
528:33 - called it static storage
528:35 - uh this is fine this is all fine we'll
528:37 - go ahead and hit add
528:39 - and down below we have some load
528:41 - balancing we don't care about any of
528:42 - that
528:43 - so you can see status is enabled all
528:45 - this should be okay the protocol here
528:47 - for the health probe should be http um
528:50 - actually
528:51 - let me double check because remember
528:53 - this is where i ran into some trouble
528:56 - i believe
528:58 - yeah http i think it's https okay i
529:01 - think i read somewhere that's like
529:03 - stack storage doesn't use http but
529:04 - clearly it does because we went to the
529:06 - link earlier it was https so i might
529:08 - have the screenshots wrong but i've
529:10 - corrected them in the actual
529:11 - instructions on my site we'll go ahead
529:13 - and save that i think it's https
529:15 - and then the the protocol here um we'll
529:18 - just leave it matching it's totally fine
529:20 - origin path is fine we'll go ahead and
529:22 - add that there
529:24 - and we'll go review create
529:26 - we'll give it a moment and we'll create
529:39 - all right so after waiting about a few
529:41 - minutes like two three minutes it looks
529:43 - like as your front door setup so we'll
529:45 - go to the resource i'm just hoping this
529:46 - works sometimes you have to play around
529:48 - with the settings but if you see the
529:49 - endpoint hostname we'll go ahead and
529:51 - grab that there paste it on in
529:53 - and we get a 404 so something's not
529:55 - working just right
529:57 - so we'll have to go do some debugging
529:59 - which i was hoping we didn't have to do
530:00 - so we go the front door manager this is
530:02 - the same setup we have here before there
530:04 - are like different ways to get to it
530:07 - but we'll have to just kind of go
530:08 - through and debug it so we do have my
530:10 - route my origin group so we can click
530:12 - into our origin group and then from here
530:14 - we'll expand it
530:16 - and we have our route
530:18 - so there's something
530:20 - that is not correct and that's what we
530:22 - have to figure out
530:24 - so i'm just going to double check my
530:26 - instructions because this was a bit
530:28 - tricky to figure out
530:30 - um and it really came down to like these
530:32 - protocols here
530:35 - so
530:39 - let me keep checking and i mean i think
530:41 - the probe is working correctly so if we
530:42 - go in the origin
530:44 - if this wasn't working correctly then it
530:45 - wouldn't it wouldn't show green
530:48 - so go back up a step here
530:50 - you know what maybe we should just wait
530:51 - a little bit because sometimes it takes
530:53 - time to propagate
530:54 - and i don't i don't 100 trust that it's
530:56 - not working because i feel like we
530:58 - configured it exactly right so i'll open
531:00 - up a new tab here
531:06 - it says 404 still
531:14 - my origin group
531:20 - that's fine
531:27 - we'll go here
531:29 - yeah so what i'm going to do is go back
531:30 - to our storage account i just want to
531:32 - double check
531:33 - what that string was
531:36 - for the static website i'm pretty sure
531:37 - it was https
531:40 - it is hps so there's no reason that
531:42 - should not work
531:53 - this is what i was trying to uh spare
531:54 - you the uh the debugging of this
531:57 - and it's very common it's not just azure
531:58 - anything that has a cdn sometimes it's
532:01 - it's difficult to
532:02 - figure that stuff out we'll go ahead and
532:04 - add that route
532:05 - and we'll just carefully look at what we
532:07 - have here so
532:08 - patterns match that is fine
532:11 - it's the correct endpoint
532:14 - both protocols are accepted that's
532:16 - totally fine redirect https that's
532:18 - totally fine
532:19 - match that doesn't matter
532:22 - so it should
532:24 - just
532:26 - work
532:27 - the only thing we didn't do was enable
532:29 - caching which i think wouldn't hurt to
532:30 - do
532:31 - because it is a cdn we don't take
532:33 - advantage of it if we don't turn it on
532:34 - so we'll do that and say ignore a query
532:37 - string i mean that's not going to fix
532:38 - this problem if the routing's messed up
532:41 - again i'm just hoping that maybe it just
532:43 - has to do with propagation
532:51 - i'll go back to the overview here
532:54 - and we will open this again
532:58 - oops
533:01 - grab this
533:02 - let's just make sure it's doing hps
533:04 - we're working to restore all services as
533:06 - possible right now what do you mean our
533:08 - services aren't available right now
533:09 - let's go look at azure status
533:12 - page here
533:17 - and we'll take a look at front door
533:22 - as your front door it's saying that it's
533:25 - fine
533:27 - u.s east
533:29 - there's no green beside it
533:31 - good
533:33 - so we go down here i guess it's a it's a
533:34 - non-regional service so if it's green
533:36 - it's green
533:38 - so you know i'm going to play around
533:39 - with this for a little oh 404 web so
533:41 - this is better
533:42 - right it's just saying it can't find the
533:44 - content
533:46 - well i mean i guess that makes sense
533:48 - because that was there so we'll just
533:49 - clear that out oh now it's loading so i
533:51 - think it what the issue
533:52 - is there was no issue it's just
533:54 - propagating to all the servers and it
533:55 - took some time so
533:57 - i think that's what really threw me for
533:59 - a loop um when i was originally doing it
534:00 - so just give it some patience and it
534:03 - will work eventually so we are all done
534:04 - here so we can go ahead and clean this
534:06 - up
534:12 - and i'm just looking for the resource
534:14 - group here we'll go ahead and delete it
534:17 - and there you go that's azure front door
534:21 - and i'll see you in the next one okay
534:26 - [Music]
534:29 - let's take a look here at application
534:31 - insights and this is an application
534:33 - performance management service so it's
534:35 - an apm and it's a sub service of azure
534:37 - monitor let's talk about what apms are
534:39 - so these are monitoring and
534:42 - they do monitoring management of
534:43 - performance and availability for
534:44 - software apps apm strives to detect
534:46 - diagnose complex application performance
534:48 - problems to maintain an expected level
534:51 - of performance so why use application
534:54 - insights will automatically detect
534:55 - performance anomalies and includes
534:57 - powerful analytics tools to help you
534:59 - diagnose issues and to understand what
535:01 - users do with your app designed to help
535:03 - you continuously improve performance and
535:06 - usability works for apps4.net node.js
535:09 - java python hosted on on-premise hybrid
535:11 - or any public cloud and i know that it
535:14 - works for other ones that are not part
535:16 - of the supported languages so there's
535:17 - one for ruby but the thing is is like
535:20 - azure's only providing official support
535:22 - for a bunch of languages but you might
535:24 - still have libraries out there for them
535:26 - and it integrates with your devops
535:28 - processes and can monitor analyze
535:31 - telemetry from mobile apps by
535:32 - integrating with visual studio app
535:35 - center and if you're running an app you
535:37 - definitely want to have an apm installed
535:39 - if you ever use data dog or skylight or
535:42 - new relic that's what apms are or what
535:45 - those providers offer but let's take a
535:48 - closer look here and this is kind of an
535:50 - example of an application and what
535:53 - you're going to see is that we have a
535:55 - front end of backend and workers just to
535:56 - kind of represent
535:58 - how you can instrument your applications
536:00 - and when we say instrument it just means
536:01 - install like this piece of code that
536:03 - runs on it to send data back to
536:05 - application insights
536:07 - and the idea is that when you instrument
536:08 - it you're just installing the instrument
536:10 - package sdk or in some cases you can
536:13 - just turn it on where it's supported so
536:14 - you don't necessarily have to install an
536:16 - app you might just press a button and it
536:19 - will
536:19 - install it within uh azure services
536:22 - and there's many ways that you can view
536:24 - your telemetry data so the idea is that
536:26 - the agents send that information to
536:28 - application insights and then you're
536:30 - gonna be able to leverage that in alerts
536:31 - power bi visual studio rest api
536:33 - continuous exports a lot of a lot of
536:36 - services can ingest application insights
536:38 - so apps can be instrumented from
536:40 - anywhere so if you're running on aws you
536:42 - can install it on your servers there
536:44 - when you set up application sites
536:46 - monitoring for your web apps you create
536:47 - an application site resource in
536:49 - microsoft azure so it's a physical
536:51 - resource and you open this resource in
536:53 - the azure portal in order to see an
536:54 - analyzed telemetry collected from your
536:56 - app and the resource is identified by
536:58 - the instrumentation key also known as
537:00 - the i key and i just got a big old list
537:03 - here just to tell you all the things you
537:04 - can do with it so what does application
537:06 - insight monitor well request rates
537:08 - response time failure rates dependency
537:10 - rates response times failure rates
537:12 - exceptions page views load performance
537:14 - ajax calls user and sessions count
537:16 - performance counters host diagnostics
537:19 - diagnostic trace logs custom events
537:22 - metrics
537:23 - uh so there you go that's a big old list
537:25 - and on the right-hand side is where can
537:27 - i see my telemetry and we saw a little
537:29 - list but let's go through the big list
537:30 - and that's smart detection manual alerts
537:32 - application map profiler user usage
537:35 - analysis diagnostic search for instance
537:36 - data metrics explorer for aggregated
537:38 - data dashboards live metrics streams
537:41 - analytics visual studio snapshot
537:43 - debugger power bi rest api continuous
537:47 - export and there you go so you can see
537:49 - that it can collect and you can use it
537:53 - in a lot of places definitely definitely
537:55 - install if you're using are you running
537:57 - a web application
537:59 - [Music]
538:03 - let us quickly talk about open telemetry
538:05 - so open telemetry also known as otal is
538:08 - a collection of open source tools apis
538:09 - that's the case instrument generate
538:11 - collect and export telemetry data
538:14 - so otel standardizes the way telemetry
538:16 - data metrics logs choices are generated
538:18 - and collected
538:19 - it uses a thing called wire protocol
538:21 - which refers to a way of getting data
538:22 - from point to point
538:24 - um an application insight supports otel
538:26 - as an alternative to azure analytics sdk
538:28 - for instrumentation so open telemetry is
538:31 - very popular in the cloud native space
538:32 - and kubernetes um and it probably is the
538:35 - future of metrics so
538:38 - it's a standard and so we're seeing
538:40 - adoption
538:41 - top not not just by azure but by aws and
538:45 - google cloud so
538:46 - uh it is worth knowing about this
538:48 - alternative method
538:50 - will show up in the exam maybe not but i
538:52 - definitely want you to know what open
538:54 - telemetry is okay
538:55 - [Music]
539:00 - let's talk about instrumentation for
539:01 - application insights because if you
539:03 - don't do this you're not going to be
539:04 - getting data to your application
539:07 - insights so you instrument your
539:09 - application by adding the azure
539:10 - application insights sdk and
539:12 - implementing traces it supports a
539:14 - variety of languages.net java python
539:16 - node.js javascript no ruby i really like
539:19 - ruby support please
539:21 - so you do npm install application
539:22 - insight save that's the javascript
539:24 - example here i'm requiring it we're
539:27 - doing some setup and then from there
539:29 - what you can see with the client
539:31 - is we are tracking an event we're
539:33 - tracking an exception we're tracking a
539:35 - metric we're tracking a trace we're
539:37 - tracking dependency we're tracking
539:38 - requests so there's a variety of things
539:40 - we can do but you know there is auto
539:42 - instrumentation we'll talk about that uh
539:44 - soon but um you know this is the idea
539:48 - generally is that this is what
539:49 - instrumentation looks like okay
539:51 - [Music]
539:55 - so wouldn't it be nice if we didn't have
539:57 - to instrument our app that it would just
539:59 - work and that's where auto
540:00 - instrumentation comes in it allows you
540:02 - to enable application monitoring and
540:03 - application insights without changing
540:05 - your code
540:06 - and it's supported for
540:08 - a variety of different things so
540:11 - support's going to vary and you'll have
540:13 - to just come back and check if this is
540:15 - even true still
540:17 - i mean i did just get this table but you
540:20 - know azure moves really fast let's take
540:21 - a look at what i'm looking at
540:23 - right now so across the top we can see
540:26 - net.net core java node.js python and
540:29 - there is a term called
540:30 - onbd which means on by default
540:34 - so another thing is going to vary on
540:36 - based on service so for azure app
540:38 - services pretty good support across the
540:40 - board except for python which is
540:42 - surprising
540:43 - for linux not for net because
540:46 - it just doesn't let you have it and not
540:48 - for python for azure functions we only
540:51 - have it in public preview for java for
540:53 - azure spring cloud
540:56 - uh it's only for
540:58 - java again which makes sense because
541:00 - spring is a java thing so
541:02 - it's these will never get supported here
541:05 - they should really say n a azure
541:07 - kubernetes service so only for net core
541:09 - and maybe through the agent uh virtual
541:12 - machine windows on-premise virtual
541:13 - machine
541:15 - um windows they're the same here and
541:18 - then for standalone agent we have
541:19 - generally available for java so uh not a
541:23 - lot of support but i mean there is some
541:25 - so you know if you're in the dot-net
541:26 - world or in the java world looks like
541:28 - you're going to be in really good shape
541:30 - but there you
541:31 - [Music]
541:34 - go hey this is andrew brown and in this
541:37 - follow along what we're going to do is
541:39 - learn about using application insights
541:41 - so application insights allows us to get
541:44 - visible data about how our application
541:46 - is running like distributed tracing and
541:48 - so we're going to deploy some kind of
541:50 - workload and then apply application
541:52 - insights to it so what we'll do is go
541:54 - ahead and go to app services and deploy
541:56 - a new uh service so let's just type in
541:59 - the top your app services
542:02 - and we'll go here we'll go ahead and
542:04 - create ourselves a new app service
542:07 - and i'm going to make a new resource
542:09 - group here so it's easy for us to clean
542:10 - up later so my app insights
542:14 - and then from there we'll need a web
542:16 - name so we'll say my app insights 8888
542:20 - if you cannot get this name just change
542:22 - the values to something that works for
542:23 - you because these are fully qualified
542:25 - domains it's like registering domain
542:26 - names so make sure it works for you
542:29 - we're going to publish code here today
542:31 - for the stack we're going to choose no
542:32 - just to make our lives super easy i'm
542:34 - choosing 16 lts if there's a newer
542:36 - version should work
542:37 - fine
542:38 - no does not change that much
542:40 - in terms of versions but you know
542:42 - depends how far in the future you are
542:44 - in terms of region uh canada uh sorry
542:47 - central u.s is fine
542:49 - it's always chooses by random i'm in
542:51 - canada but i'm going to stay in central
542:52 - u.s because i know that the things i'm
542:54 - going to be using will be available
542:56 - there in terms of the sku in sizes
542:59 - greatly affects the cost we'll click
543:00 - into here because we really do not want
543:02 - to pay much at all
543:04 - so we'll give it a moment to load it'll
543:06 - take a few seconds not well it usually
543:08 - doesn't but for right now it certainly
543:11 - does so we'll give it a moment to load
543:13 - i'm going to make my way over to dev
543:14 - test and we're going to choose the
543:15 - cheapest option here
543:17 - which is going to be f1 so
543:20 - you can see there's not a whole lot it's
543:21 - just memory storage you know we don't
543:23 - get custom domain we don't get manual
543:24 - scaling we don't need that stuff we need
543:26 - it nice and simple so we'll go ahead and
543:29 - choose free f1 that sounds good to me i
543:31 - like the word free we'll go to our next
543:33 - step which is deployment
543:38 - in fact i don't think anything has to be
543:40 - changed under deployment but we'll click
543:41 - it here
543:42 - not sure why azure is a bit slower today
543:44 - but that's just how it's going
543:46 - so we'll give it a moment to think and
543:47 - we will wait
543:57 - wow it is really really slow here right
543:59 - now okay so for deployment um i mean you
544:02 - could hook hiccup hook up github actions
544:04 - and things like that i don't think we
544:06 - need to do any of that so i'm going to
544:07 - make my way over to networking
544:09 - and we do want to inject
544:11 - oh sorry not networking monitoring
544:13 - because we don't need networking
544:14 - monitoring but we want to enable
544:15 - application insights this is going to
544:17 - set up some stuff for us um so it's
544:19 - going to set us an application insights
544:21 - what name space i think it'll set up
544:23 - something for us so we don't have to
544:25 - worry about that so we'll go ahead and
544:26 - hit review create and hit review
544:33 - and so now we just need to wait for that
544:35 - to deploy so i'll see you back here when
544:38 - that is done okay
544:41 - after a very short wait maybe like a
544:42 - minute or two that has been uh completed
544:45 - so we'll go ahead and go to that
544:46 - resource and i just want to show you uh
544:49 - over here in the configuration tab the
544:51 - environment variables that are going to
544:52 - get passed
544:54 - to our application so if you go here
544:57 - these are the environment variables that
544:58 - get passed along and notice we have one
544:59 - called application insights connection
545:01 - string instrumentation key and the
545:03 - reason this is important is because we
545:05 - don't have to um in our environment set
545:08 - it so like since we're using
545:10 - app services and we enabled
545:12 - application insights it's already going
545:14 - to set those values and the great thing
545:16 - about the sdk is it's going to pick up
545:18 - those values so we don't have to
545:20 - explicitly set them at least i don't
545:22 - think so if we do set them we'll use the
545:23 - environment variables i haven't decided
545:25 - as of yet for application insights um it
545:28 - set us up a
545:30 - i don't think they call them workspace
545:31 - name namespace i don't really care what
545:33 - the name is called
545:34 - but the idea is that we have an
545:36 - environment set up
545:37 - in order to gather information so what
545:40 - we're going to do is just go back here
545:43 - and notice down below it says node is in
545:45 - progress agent is enabled so just
545:47 - indicate that that is working as
545:48 - expected that's a good message to see
545:50 - not that we'd ever see any other message
545:52 - there
545:53 - and so now that uh we
545:56 - we know that everything is deployed or
545:58 - at least our environment is ready let's
546:00 - go ahead and create a new application so
546:03 - what i'm going to do is make my way over
546:04 - to github here and we are going to
546:06 - create ourselves a new repository now i
546:08 - don't know if i have an old one sticking
546:09 - around here so hopefully i don't but
546:11 - we'll see here i'm going to say my
546:15 - app insights would be something i would
546:17 - name it and so i already have one there
546:18 - so what i'm gonna do is i'm just gonna
546:21 - make a new tab here
546:23 - okay i'm just gonna go to uh anyone here
546:25 - this is my marketing website i'm gonna
546:27 - go my app insights i'm just going to
546:29 - rename the old one so it's out of the
546:30 - way
546:32 - so you can follow along exactly the same
546:33 - way as me here i'm just going to do
546:34 - settings we'll go down here we'll just
546:36 - say old
546:39 - all right and we'll rename that
546:41 - and i'll close that out and so what
546:43 - we'll do is go back here and this this
546:44 - is fine now i know that it says it's not
546:46 - but i know it is we're going to add our
546:47 - email file we're going to go down to get
546:49 - ignore and add node
546:50 - i might add i might have done this after
546:52 - the fact so i might have added the steps
546:53 - out of order here but uh that should
546:55 - improve that there i'm just going to do
546:56 - hyphen old i'm going to
546:58 - take that out there there we go and
547:00 - we'll go ahead and create this
547:01 - repository
547:02 - so um
547:04 - i want you to get getpod installed so
547:06 - gitpod is a really great
547:08 - great service
547:09 - for having a cloud developer environment
547:11 - it has a generous free tier so there's
547:14 - no reason not to sign up
547:16 - and really this is the future of cloud
547:18 - development so make sure to go get a get
547:20 - pod account but you really don't have to
547:22 - because when you launch this environment
547:23 - either launches it temporarily
547:26 - or you can connect your account and then
547:27 - persist that data but um anyway
547:31 - if you install the getpolychrome
547:32 - extension you'll get this button but you
547:34 - don't need to install the chrome
547:35 - extension all you have to do
547:37 - is you just need to put this in front of
547:39 - your address so this is the current
547:41 - address we're at like this is the
547:42 - current repository
547:44 - whoops
547:45 - this is the current repository link and
547:47 - all this button is doing
547:49 - uh copy link address is just putting
547:52 - this part in front here okay so this is
547:54 - going to open up a git pod environment
547:56 - this launches extremely fast and we're
547:58 - going to create a new node.js
548:00 - application and deploy it to
548:02 - app services so we'll give it a moment
548:04 - here
548:06 - doesn't take too long
548:09 - here we go and what we'll do is we're
548:12 - going to use the mpx uh express
548:14 - generator to generate a new express
548:17 - express application express.js
548:20 - generator so we'll type that in there
548:22 - we'll say yes
548:24 - and it's going to create us a skeleton
548:25 - app just say yes just say yes all the
548:27 - way down we'll get a bunch of stuff in
548:28 - here and then we'll do our npm install
548:31 - you can do npm installer npmi if you
548:33 - want to make it nice and short
548:35 - we'll give it a moment there
548:38 - okay and then we'll make sure this works
548:39 - we'll type in npm start this is going to
548:42 - prompt us to open our browser so we'll
548:43 - say open browser
548:45 - so express yes that application is
548:47 - working up this link up here so this is
548:49 - our application but it's not
548:50 - instrumented to send any data so that's
548:53 - what we need to do so the next thing
548:54 - we're going to do is just kill this
548:55 - control c
548:57 - uh yeah control c on my keyboard
548:59 - and we'll type in npm install i'm typing
549:01 - application
549:03 - insights hyphen hyphen save and what
549:04 - that's going to do is install that
549:06 - dependency for application insights so
549:08 - if i go over to my package.json here you
549:10 - can see that it is a dependency that is
549:12 - installed so now we need to configure
549:14 - that so what we'll do
549:16 - is go over to our um
549:19 - app.js file and so we got to insert this
549:21 - somewhere here
549:22 - um so what i'm going to do is i'm going
549:25 - to go all the way to the top that's
549:26 - probably a good place to put it and
549:27 - we'll say let i like how it says all var
549:29 - var is really old
549:31 - app insights equals require
549:35 - double quotations or single it doesn't
549:36 - matter it's not going to hurt either way
549:38 - application insights and that will
549:41 - define our variable here the next thing
549:43 - is we need to set it up so we'll say app
549:45 - insights
549:47 - dot setup and then we need our
549:49 - connection strings that's where we're
549:50 - going to put this thing here
549:52 - our connection string
549:57 - and then we need a bunch of other things
550:00 - so i'm not going to type these all out
550:01 - because there's a lot of them so we'll
550:02 - just say
550:03 - application insights instrumentation
550:07 - node.js
550:09 - and i'm sure the documentation is going
550:10 - to help us out there
550:12 - so go here i'm just scrolling for the
550:14 - usual configuration here it is that's
550:16 - all i've been typing here this entire
550:17 - time that's probably where i got it
550:18 - originally so we'll go ahead and we'll
550:20 - just paste that on in there allow
550:23 - take out the first line that way there's
550:24 - no spelling mistakes
550:27 - and so we have auto dependency
550:29 - correlation auto collect requests a
550:31 - bunch of interesting stuff here it's
550:33 - going to do distributed tracing we don't
550:34 - need the semicolon we'll just take it
550:36 - off there for fun and i'll save that and
550:38 - so now the next thing we need is the
550:39 - connection string in order to make sure
550:41 - this application works which is going to
550:42 - go here so we'll make our way back over
550:44 - to this tab and the connection string is
550:46 - actually in the configuration that were
550:48 - earlier here in this blade so give it a
550:50 - moment to load
550:53 - there we go and so we want the
550:55 - connection string so we'll click that
550:58 - i think before that you could use the
551:00 - instrumentation key but now connection
551:01 - string is what is preferred
551:03 - um i want to copy the value okay i just
551:05 - clicked into it to make it a bit easier
551:06 - for me and what i'm going to do this is
551:08 - really dirty but i'm going to just paste
551:10 - it in here and we're going to just do it
551:11 - in a few different steps just to make
551:13 - sure that this is working correctly
551:14 - before we switch this over to
551:16 - environment variables
551:18 - and one other thing that we need to do
551:20 - if we want to actually get any data is
551:22 - we'll have to turn send live metrics to
551:24 - true now if you are doing
551:26 - instrumentation you might not want live
551:28 - metrics i don't know if it costs more
551:30 - but for our development purposes it's
551:32 - not a big deal to turn this to true so
551:34 - that is now enabled and uh you know our
551:37 - dot get ignore should have a bunch of
551:39 - stuff in it like node module so we're
551:40 - not committing that stuff i'm going to
551:42 - commit this now it does have the
551:43 - connection string in here i'm not too
551:45 - worried about it because we are going to
551:47 - um delete it out
551:49 - later on so we'll just say um
551:51 - save my changes but i am pointing to
551:54 - pointing out to you that we are
551:55 - committing this
551:57 - string here which is bad practice right
551:59 - because if somebody got that then they
552:00 - could mess with this but again
552:02 - uh we're going to tear all this stuff
552:04 - down so it's not that big of a deal and
552:06 - so we should be okay
552:08 - so now that this is
552:10 - installed what we're going to do is do
552:11 - an npm start that's going to start up
552:13 - our application
552:15 - and then we're going to go open in the
552:16 - browser again so here's our application
552:18 - now we'll go back to application
552:19 - insights
552:21 - or not application sites but into
552:24 - our app service our actual application
552:26 - on the left hand side there's going to
552:27 - be application insights and from here
552:29 - what we can do is click the view
552:31 - application insights to go to the
552:33 - workspace the namespace whatever they
552:34 - want to call it and this is the way
552:36 - we're going to be able to see some
552:37 - information so live metrics on the left
552:39 - hand side will show us data in real time
552:42 - so now it says it's connected to your
552:43 - app so we'll go back here
552:45 - refresh so the app connects and we
552:48 - should get some data
552:50 - there we go so these are the requests
552:51 - we're making right here so i hit enter
552:53 - enter enter enter enter enter enter we
552:55 - go back here here's those requests
552:57 - coming in it's giving a 304 for the
553:00 - css um so yeah mostly yeah there's our
553:04 - stuff so we know that it's working
553:05 - because we can see information there
553:08 - so that's pretty darn straightforward
553:09 - now let's go back to our application set
553:11 - this up properly because we really
553:12 - shouldn't be setting our string like
553:13 - this this is pretty dirty so what i'm
553:15 - going to do is just delete this out here
553:17 - and i'm just going to type in um
553:20 - process.env
553:22 - and then we're going to say
553:23 - we'll match the name of what it's
553:25 - supposed to be called in our application
553:27 - so i'll click back to make it back to
553:28 - our actual application
553:30 - we'll go to configuration
553:35 - and from here i'm going to just go ahead
553:36 - and grab that string string name i just
553:38 - clicked into it i just don't want to
553:41 - type it wrong and we'll go ahead and
553:42 - paste that in there and so this is what
553:43 - we are going to set and just did ctrl c
553:46 - to kill there but this is what we're
553:46 - going to set as our environment variable
553:48 - so i'm doing gp gp stands for git pod
553:50 - it's a way of setting environment
553:52 - variables and we'll do double quotations
553:53 - always do double quotations especially
553:55 - with connection strings when setting
553:56 - environment variables so part of the
553:58 - string does not get cut off because it
553:59 - will cut off at the first forward slash
554:01 - and then you'll just end up with like
554:02 - that and that's not enough right so i've
554:05 - copied that i'm going to go back here
554:07 - paste it in we'll hit enter and so
554:09 - that's setting the key now if i do grep
554:12 - or sorry env so this show like this
554:14 - prints all the environment variables so
554:16 - env
554:17 - pipe pipe is just a vertical line
554:20 - grep and i type in connection any part
554:22 - of the string notice that it's not set
554:24 - yet so just because we said gpenv does
554:27 - not mean it's set uh what you have to do
554:29 - is restart the environment but we can
554:31 - temporarily set it export this is how
554:33 - you'd set it in bash you type export and
554:35 - you'd give it the
554:38 - the name here like this and you do
554:40 - equals double quotations
554:44 - and this will this will make the
554:45 - environment work but the only problem is
554:46 - that if we restart this uh workspace
554:49 - we'll lose that on the next time we
554:50 - restart it so this will definitely work
554:53 - but
554:54 - let's just do it let's just restart the
554:55 - environment anyway so we have process e
554:57 - and v up here but we have to import that
554:59 - as well so i'm going to say let
555:01 - process equals require
555:03 - process process is just a way of getting
555:05 - access to the process
555:08 - process being the application that's
555:09 - running to access the environment
555:11 - variables that are set which are getting
555:12 - passed along to the application by
555:15 - app services
555:16 - so
555:17 - that is good there
555:20 - and so i think we are in good shape the
555:23 - only other thing is i want to generate a
555:24 - git pod file so i'm going to just do gp
555:26 - for uh git pod and we'll just set some
555:29 - things up this is going to save us some
555:30 - time when we relaunch our environment so
555:32 - i'm just doing uh pipes here
555:34 - uh on the end here that allows us to do
555:36 - multi-line even though we're only doing
555:37 - a single line we could probably emit the
555:39 - pipe if we wanted to but i'm just
555:40 - putting it in anyway
555:43 - and so
555:45 - we'll just indent there coloring usually
555:47 - tells us something's wrong and this
555:49 - starts up on port 3000. i want it in the
555:51 - browser
555:53 - i don't want to do preview that means
555:55 - opens in a little window here that's
555:56 - kind of gross
555:57 - and so this looks good i think i
555:59 - accidentally removed a line here because
556:00 - we'll say tasks up here and so now we
556:02 - have our get pod uh uh yaml file so
556:05 - we'll go ahead and add all these files
556:06 - here so um
556:08 - uh instrument
556:10 - i think that's everything we need to do
556:12 - there so we'll just go ahead and commit
556:14 - that
556:15 - we'll sync our changes
556:18 - and i'll do a
556:21 - get push just to double check double
556:23 - check that this stuff was synced and
556:24 - committed so if you go back to our
556:26 - actual uh get or github if you go
556:29 - context it just brings us back to the
556:30 - github repository
556:32 - that gitpod yaml file set so we know
556:34 - that our changes have been committed
556:36 - highlighting looks fine and this is
556:38 - green this is this kind of looks blue so
556:40 - hopefully that works but anyway what
556:43 - we're going to do is go ahead
556:45 - and
556:46 - open this again sorry we'll close this
556:48 - tab
556:49 - and we'll reopen up and get pod so this
556:51 - will launch a new workspace if you still
556:54 - have the old tab open it might ask you
556:55 - to physically press a button called new
556:57 - workspace but generally it opens a new
556:58 - workspace so it's setting up a new
557:01 - container completely for us
557:03 - so give it a moment it doesn't take
557:05 - too too long
557:09 - and
557:10 - what we'll do here is
557:13 - wait for the terminal to open
557:17 - okay notice it ran the server did npm
557:19 - install did everything for us i'm just
557:21 - going to open a new terminal here on the
557:23 - on the right hand side and bash whoops
557:25 - well i did not mean to split it but
557:27 - that's okay
557:28 - i don't know how to unsplit it so just
557:30 - drag it around that doesn't matter
557:32 - yours will look a little bit different
557:33 - than mine
557:34 - and so all i want to do is do env grep
557:37 - um
557:38 - con
557:42 - because i want to see that connection
557:43 - with set we'll do two ends
557:46 - so there it is application insights
557:48 - connection string so i'll just go ahead
557:50 - and close this one i'm getting a bit
557:51 - confused here with that open and our app
557:53 - should still work if we go on the left
557:54 - hand side here to remote explore it
557:56 - didn't give us the prompt here notice
557:57 - that this little red thing says all
557:59 - right you're blocking this pop-up
558:00 - continue blocking no always allow us to
558:02 - pop up on this
558:04 - and i'll just click the uh a little
558:06 - world button to see that it's running so
558:08 - it's still working we're going to go
558:09 - double check to make our
558:10 - instrumentations working with our new
558:12 - connection string that's being passed
558:13 - through as an environment variable so on
558:15 - the left hand side go back to
558:16 - application insights
558:18 - and we'll give it a moment here below
558:20 - we'll go back to viewing the data on the
558:21 - left hand side we're going to go to live
558:23 - metrics
558:25 - it says it needs to reconnect we'll hit
558:26 - refresh here if it connects then it must
558:29 - be working
558:30 - so refresh refresh refresh go back the
558:33 - data is coming in so therefore it is
558:35 - working so we are in good shape here so
558:38 - um our application is configured
558:40 - correctly but it's not actually deployed
558:43 - um because we are passing the invite
558:45 - like we're passing our own environment
558:47 - variables in so let's actually get it
558:48 - deployed
558:49 - to
558:50 - um app services so what we'll do is
558:53 - click back to get back to our app
558:55 - services
558:56 - which is here nope oh yeah this is it
558:58 - says app service that's good on the left
559:00 - hand side we want to deploy it so we'll
559:02 - go to deployment center
559:06 - and what we'll do is we'll choose the
559:08 - code and we'll go to github and from
559:09 - here we need to authenticate so omek's
559:11 - my personal account that's just the way
559:12 - i get to it and so i need to select an
559:14 - organization it's exam pro i need to
559:16 - select a repository
559:19 - this is going to be my
559:22 - app
559:25 - insights that's the new one
559:26 - we'll choose the main branch
559:28 - it knows what the runtime is
559:31 - and we'll go ahead and save that
559:33 - so as soon as we do that it should
559:35 - automatically start deploying
559:37 - it's using github actions as the build
559:39 - provider so if we go back to the
559:40 - repository here we go to actions we can
559:42 - see
559:43 - we have one queued up and so this should
559:45 - run on its own we don't need to do
559:46 - anything
559:47 - so all we're doing is waiting for this
559:49 - to deploy
559:51 - and once it's done then our application
559:53 - should be
559:53 - deployed to
559:55 - the our url and then we'll check to see
559:58 - if it works and if it works then we are
560:00 - in good shape and we're done the basics
560:02 - of application inside so i'll be back
560:04 - here in a little bit okay
560:07 - all right so it's done i was waiting
560:08 - here for quite a while but i think it
560:10 - was done in a few minutes you just have
560:11 - to hit the refresh here so you know
560:14 - don't be afraid to go up here and hit
560:16 - enter and and see if it's deployed but
560:18 - now that it says it's deployed i mean
560:20 - this url's on git pod so that's no good
560:22 - for us what we'll do is go back to our
560:24 - application all the way top to the
560:25 - overview tab or blade i should say we'll
560:28 - go to browse and see if it's deployed so
560:30 - we're hoping we see express
560:32 - js the first load might be a bit slow so
560:35 - we'll give it a moment to think
560:39 - it sure is thinking hard
560:46 - and we'll just close it again and reopen
560:48 - it
560:49 - we are using the free tier so i'm not
560:51 - sure if it's like spinning up the
560:52 - instance that's probably what it's doing
560:54 - it's probably um
560:56 - spinning up uh the whatever the
560:58 - underlying environment is to get it
561:00 - running so it is running here now the
561:02 - next question is is it tracking so we'll
561:03 - go over to application insights
561:06 - and we'll go back to viewing our insight
561:08 - data
561:09 - and we will go all the way down to alive
561:11 - metrics
561:14 - it says it's not connected we'll hit
561:15 - refresh
561:18 - give it a moment here see if it finds
561:19 - any data
561:22 - we'll hit enter
561:24 - there we go so we're having a couple
561:26 - requests in there so uh we have
561:28 - application sites working and it's
561:29 - deployed so we're in pretty good shape
561:31 - here we're going to keep this
561:32 - environment around because we need it
561:34 - for a few uh follow follow alongs here
561:37 - but if you want you can tear this down
561:40 - but you'd have to do all the setup again
561:42 - but you can see it doesn't take too darn
561:43 - long but there you go
561:45 - [Music]
561:49 - hey this is andrew brown and we're going
561:50 - to take a look at sampling with
561:52 - application insight so this is a very
561:53 - short follow along but very useful for
561:55 - you to know so we have this application
561:58 - and what i want you to do is open it up
562:00 - in git pod you should know by now how to
562:01 - open things up and get pot and we'll uh
562:04 - give it a moment here and we're going to
562:06 - configure this to do sampling so
562:09 - sampling is when you don't send every
562:11 - single trace
562:12 - to application and sites or every single
562:14 - request information and the reason why
562:16 - you'd want to do this is basically to
562:18 - save money because if you had
562:20 - all your data going there
562:22 - it just it's just too much information
562:24 - right so um and that cost will stack up
562:27 - a lot faster so you know applications
562:29 - might send only 50 requests only 22
562:32 - percent of requests it's up to you to
562:34 - decide how much data you need to have
562:36 - something that's accurate but you
562:37 - definitely don't need all of it so i'm
562:38 - taking the start off the end of here and
562:40 - i'm just going to have app
562:41 - insights start here so it is set here
562:44 - and then what i'll do is do app
562:46 - insights
562:48 - app insights i think we should do before
562:50 - i think in my code in the code example i
562:52 - have it in the wrong order but i don't
562:53 - think it even matters but we'll do
562:55 - client config it might matter actually
562:57 - sampling
562:59 - um percentage
563:02 - equals 50. so we're saying only sample
563:05 - 50 of the time i'm just going to double
563:07 - check make sure that spell right app
563:08 - insights
563:09 - default client config sampling
563:11 - percentage equals 50 and so now if we if
563:14 - we set that there
563:16 - and i'm not going to uh i'm not going to
563:18 - test it in the albums go test it and get
563:20 - pod here but what we'll do is we'll go
563:22 - back to our application
563:24 - into the application sites tab
563:27 - view that we'll go to the um
563:30 - live metrics
563:32 - and if we go back over here and
563:36 - stop our application start it back up
563:39 - okay
563:40 - and we go here this should connect
563:45 - and so what i want to do i don't know
563:46 - i'd say it's not supported it's
563:47 - definitely supported we'll hit one
563:50 - two
563:50 - three four and i'll go back
563:53 - and notice that i hit four but i only
563:54 - have two requests so
563:57 - that's why we're only getting 50 of it
563:58 - because it's going to cut out 50 50 of
564:01 - the requests and i think yeah it should
564:03 - always do that so that's all i wanted to
564:05 - show you
564:06 - because that is something that is a very
564:07 - basic configuration that you should know
564:09 - for application insights uh but we're
564:12 - going to keep the environment around
564:13 - because we have a few more things to do
564:15 - but yeah there you go
564:17 - [Music]
564:21 - hey this is andrew brown and we are
564:23 - going to look at usage analysis with
564:24 - application insights so usage analysis
564:26 - allows you to get more rich information
564:29 - about the usage of your application so
564:31 - i'm just here in application insights
564:33 - for our app that we've been using if you
564:35 - go to the left-hand side and scroll on
564:36 - down you'll see this usage tab so we
564:38 - have users sessions all this interesting
564:42 - stuff it allows us to instantly create a
564:44 - cohorts which are extremely useful for
564:46 - visualizing information if you've never
564:47 - seen a cohort before like user cohort
564:51 - i don't think it matters what cohort we
564:52 - look at but i just want to show you an
564:54 - example of one
564:55 - so this is a cohort and the idea is that
564:57 - if you have an application you want to
564:59 - be able to see whether users are coming
565:01 - back every single day right so you go
565:03 - okay this person was here on day one
565:06 - and the same person came back or 23 of
565:08 - people came back in the second day uh 18
565:10 - of the people came back in the third day
565:12 - and it gives you kind of a map of um
565:14 - of course very or like information about
565:16 - your user behavior so very very
565:18 - important very useful for your
565:20 - application and application insights
565:22 - allows you to collect that information
565:24 - now um what we're going to be doing here
565:26 - is we're going to be instrumenting uh
565:29 - the client side because
565:31 - uh we never
565:32 - we instrumented the server side when we
565:34 - installed it into the our back end
565:35 - server code but we never did in the
565:37 - front end so that's the first thing
565:38 - we're going to need to do so i just need
565:40 - the the code snippet to do that so um
565:43 - what i'm going to do i'm going to go
565:44 - back to our repository because we're
565:45 - going to need that open here so we'll go
565:47 - back to my my app insights i'm going to
565:49 - open that up and get pod while that's
565:50 - going i'm going to type in application
565:53 - insights
565:54 - insights instrumentation node.js
565:58 - and we should easily be able to find
566:00 - the code for this here
566:03 - i want a client-side though
566:11 - i'm just looking for it so because
566:13 - there's all sorts of pages here so it
566:14 - can be a bit well let's say usage how
566:17 - about application insights
566:18 - um usage
566:22 - usage analysis here's the page and
566:24 - that's the code we want so it's on this
566:26 - page here this is an analysis with
566:28 - application insights so i'm going to
566:30 - copy that code and that's going to go
566:32 - into i'm going to make a new workspace
566:33 - here
566:36 - that's going to go into the front end of
566:38 - our website so somewhere in the
566:39 - index.html
566:44 - or if they're called jade files and
566:45 - it'll go into there so it is a
566:49 - uh oops jade file it's just opening the
566:51 - browser for us here so what i'll do is
566:53 - go index jade is just this syntax
566:55 - language that is used by um
566:58 - this configuration of express.js i'm
567:01 - going to go ahead and paste this block
567:02 - in here and we actually have to format
567:04 - this a bit because jade is not going to
567:06 - like this so what we'll do is type in
567:07 - script here at the top this is how this
567:09 - language works for javascript tags we'll
567:11 - do parentheses
567:13 - type equals sql quotations text
567:17 - javascript
567:19 - okay
567:21 - and then from here what we'll do is just
567:23 - go ahead
567:25 - and select this
567:37 - okay
567:38 - and i'm just looking where we should put
567:40 - it so they probably want it in the head
567:41 - so we'll put it right here
567:43 - and
567:44 - um yeah so that's fine that looks good
567:46 - to me
567:48 - we're also going to need
567:50 - um
567:51 - to create some kind of cohort so what
567:53 - we'll do i'm going to just commit this
567:56 - add client side tracking
568:04 - okay and so we'll stop our app start it
568:06 - back up
568:12 - okay
568:14 - and back over here i just kind of want
568:15 - to show you where you can set up we're
568:17 - not going to be able to see a real
568:18 - cohort because we need multiple days to
568:20 - collect information but let's just kind
568:22 - of play around as if we do have the data
568:24 - so over here on the left-hand side we'll
568:25 - go to sessions and it should tell us how
568:27 - many sessions we have now there was no
568:30 - information showing it before but now
568:31 - we're seeing two
568:32 - okay so um i'm not sure if it's counting
568:35 - both the server side and the client side
568:37 - but we are getting data so that's good
568:40 - and if we went up here to create a
568:41 - cohort
568:43 - we have a template gallery so we could
568:44 - choose a variety of different ones so we
568:47 - might want to try a blank sessions
568:48 - cohort sessions are people that are
568:51 - connected to the application users are
568:53 - identified users
568:55 - um and so you know we can add parameters
568:58 - to filter things down but we don't have
569:00 - to do anything we can also like run the
569:01 - query to see what information we get
569:02 - back but we'll go ahead and just save
569:06 - this cohort so we'll just say
569:08 - my cohort whatever you want to name it
569:10 - doesn't matter because we're not going
569:10 - to see anything interesting anyway
569:13 - and so this is our cohort but you can
569:15 - see there's nothing interesting so if we
569:16 - want to see something
569:18 - that kind of looks like a cohort what we
569:20 - can do is make our way over to
569:22 - workbooks
569:24 - i think yeah over here
569:26 - i'm not sure if this is just the same
569:28 - thing uh but in a different place but if
569:30 - you go down below here they have usage
569:32 - like active users and things like that
569:35 - i'm looking for retention so maybe
569:36 - cohort analysis
569:39 - and so down below you can kind of start
569:41 - seeing a table here right of what that
569:43 - looks like so i find the workbooks a lot
569:45 - more useful than the usage tabs but
569:48 - yeah i just want you to know about this
569:50 - usage analysis because it's super useful
569:52 - that logins or application insights has
569:55 - that functionality so you know hopefully
569:57 - that is good enough there but that is
569:59 - all we're doing here
570:01 - so i'm just going to go home close these
570:02 - tabs
570:03 - and i'll see you in the next one okay
570:06 - [Music]
570:10 - hey this is andrew brown and this follow
570:12 - along we are going to do a bit more with
570:13 - application insights by putting in
570:15 - custom events which is very useful for
570:17 - tracking so what i want you to do is
570:19 - open up application insights for our
570:22 - project that we've been working with
570:23 - we'll also have to go back to our get
570:25 - github repository and open that up in
570:28 - git pod as uh you should know how to
570:30 - open git pod by now
570:32 - and we'll give it a moment there to load
570:34 - and we'll enter in some code
570:36 - so we do have server side client-side
570:38 - tracking we did the client-side tracking
570:39 - in the uh usage
570:41 - um the usage section there but if we
570:44 - expand this here we're going to want to
570:46 - put something in the index jade file and
570:49 - so here what we will do is add a new
570:51 - script tag so that's going to be script
570:53 - this is jade language i don't care if
570:55 - you learn it but you just got to follow
570:57 - along for this purpose
570:59 - coding is not the lesson here but we
571:02 - have text javascript that allows us to
571:03 - do some javascript there and i don't
571:06 - know why it's saying tab when i type
571:08 - that is not very useful for me but i'm
571:10 - just going to hit do space space and
571:11 - type in app
571:13 - insights track
571:15 - event we'll auto complete that there
571:17 - we'll do parentheses curlies
571:19 - name double quotations client count and
571:22 - so every time this page loads it's going
571:24 - to do track event with this name and
571:26 - that's going to count up it's called
571:27 - client count that's how we know it's on
571:28 - the client side so that's the index page
571:31 - there
571:32 - and so now we need to do server side so
571:34 - there's a few things we have to do here
571:35 - we've got to go to app.js
571:37 - and in here we need to set a variable
571:39 - that's going to get passed along
571:41 - so we'll go down below here and do app
571:43 - dot set parentheses
571:47 - app insights and this is specific to
571:49 - express.js so again i don't care if you
571:51 - remember how this works but just a way
571:53 - that we can pass a variable that we're
571:55 - going to access in our routes so we go
571:57 - app insights default client
572:01 - and
572:02 - i'm going to make sure we have that yeah
572:03 - it's up here so we're passing this in
572:05 - here so that's good
572:08 - we'll go over to our routes and in our
572:10 - route center index page we need to
572:13 - add
572:14 - app insights we'll do app insights
572:16 - and we're going to grab it from
572:19 - that app variable we just set so we do
572:21 - app request app get app insights and
572:25 - then we'll track it it looks exactly the
572:26 - same as the other one we are using
572:28 - javascript on the front a front end and
572:30 - the back end so it makes things a little
572:31 - bit easier and this one's going to be
572:33 - called name
572:35 - double quotations single quotations
572:36 - doesn't matter
572:38 - server
572:39 - count
572:40 - okay so now we have it server-side
572:42 - client-side i'm going to go up here just
572:43 - commit my code
572:45 - client and
572:47 - server-side tracking
572:51 - we'll commit that
572:54 - and what we will do is we'll do an npm
572:56 - start to restart up the application now
572:58 - that we have both sides of it we do have
573:00 - a problem here on line eight of index uh
573:03 - index
573:04 - index jade file so we'll go here take a
573:06 - look at what it's complaining about
573:08 - um
573:09 - does not like something
573:12 - so
573:13 - i'm just taking a look at my my code off
573:15 - screen here
573:18 - it's lining it it's this line it doesn't
573:19 - like
573:21 - so
573:24 - why not um
573:30 - hmm
573:33 - i mean i don't see a problem
573:37 - so what happens i'm just going to copy
573:39 - my my my oh you know what it's
573:41 - indentation see where it says block
573:43 - content
573:44 - it's supposed to be this level here
573:45 - that's probably the problem
573:47 - so we'll stop it we'll start it
573:50 - we'll give it a second
573:53 - it's still crying it's a big old crybaby
573:55 - here but that's okay we'll work through
573:56 - it
573:58 - now i hit tab it's actually tabbing
574:00 - properly because i wanted to do tab
574:01 - before i don't know why it was uh
574:03 - doing that
574:05 - still doesn't like it
574:08 - so i do have the code off screen here so
574:10 - i'm just going to copy paste it
574:12 - i mean you'll have if you're on my
574:13 - platform you'll have it
574:15 - so you just say allow
574:18 - oh there's a period on the end
574:21 - of course can barely tell that was there
574:23 - okay so we'll save that
574:26 - again i don't care if you learn
574:27 - j-templating it's
574:29 - not a skill you have to remember so that
574:31 - is there i'm just going to keep hitting
574:32 - enter
574:33 - because i want to observe this custom
574:36 - event data so we go back over here into
574:38 - app insights and we should be able to
574:41 - see this
574:42 - somewhere um
574:44 - this would be under
574:46 - where this would be under somewhere i'm
574:49 - trying to find it
574:54 - it would be under
574:57 - which blade
575:00 - huh i didn't write it out of my
575:01 - instructions so we'll have to click
575:02 - around here to find it
575:04 - um
575:05 - i
575:06 - think
575:09 - well it's event data so it's probably
575:10 - under events
575:11 - yeah that's where it is now i'm
575:13 - remembering
575:15 - and
575:16 - we what we can do is filter this down
575:18 - and say any custom event here if we
575:20 - scroll on down we'll go view more
575:22 - insights i don't know why they have a
575:23 - little button there to get that
575:24 - information but we scroll on down we
575:25 - have server accounts the server account
575:27 - is definitely working
575:29 - we don't know why we don't see client
575:31 - count so what we'll do is go back here
575:33 - and we'll right click and see if there's
575:34 - any errors
575:36 - there is an error app insights is not
575:37 - defined and so it should be defined in
575:40 - the head because
575:42 - we have this script from before
575:45 - right
575:46 - you know what we didn't put the
575:48 - instrumentation key in
575:50 - so that's not good because in the last
575:52 - follow along we were supposed to do it
575:54 - but it didn't really matter because we
575:55 - didn't have any data to look at so
575:56 - that's why we didn't notice so we'll go
575:58 - back to our overview
576:00 - and we'll grab the instrumentation key
576:02 - here
576:02 - go back to our code
576:04 - and we will go
576:06 - up to app.js
576:08 - and this is client side so it's a bit
576:10 - harder to
576:11 - pass it in as environment variables
576:12 - we're not even going to try to do like
576:13 - to do that because it's the
576:15 - instrumentation key it doesn't matter
576:17 - and we're going to go back to the
576:21 - uh
576:22 - layout here and down below we'll just
576:24 - replace that
576:26 - okay we'll stop start it
576:29 - hit refresh
576:31 - right click to see if it's working now
576:33 - let's give it a refresh here
576:37 - still saying app insights is not defined
576:38 - which is not true
576:40 - but there is an error here we'll click
576:42 - and do it
576:45 - it's not showing me the line which would
576:47 - be nice
576:54 - okay so we'll go back here and there's
576:56 - something wrong here it doesn't like
576:57 - something you know what it's probably
576:58 - missing the period
577:00 - there we go
577:02 - i can't believe we never noticed that
577:03 - before
577:04 - so fixes i'm very confident that is the
577:06 - fix
577:08 - so we'll go back here inspect
577:11 - refresh
577:13 - already lost that let's do inspect again
577:15 - no errors here that's good we'll hit
577:17 - refresh refresh refresh refresh refresh
577:19 - refresh refresh we'll go back to
577:21 - application insights
577:23 - and
577:24 - we will go back to events
577:27 - if we can find it there it is
577:30 - and i'm going to go down to any custom
577:32 - event scroll on down
577:34 - more insights
577:38 - and it might just need some time to
577:39 - propagate because we're not getting any
577:41 - errors now so it must be working
577:44 - if it didn't track it it would give us
577:45 - an error
577:51 - client client count so it is showing up
577:53 - so it is counting now so that's how we
577:55 - do custom event tracking so there you go
577:58 - and for the most part i mean the only
577:59 - thing we didn't look at was applica
578:01 - application map
578:03 - take a quick look here
578:04 - this would show you like connections in
578:06 - between stuff so here we have like the
578:07 - web to the client if we set up other
578:10 - things like a database and stuff like
578:11 - that we would have something more
578:12 - interesting that's more of advanced
578:14 - thing
578:15 - and you know maybe i'll do that in
578:16 - another follow along but this one is
578:17 - done so let's go ahead and clean this
578:19 - all up you can delete this repository
578:21 - it's no big deal if you're worried about
578:23 - the connection string you can delete
578:24 - that whole yeah the repository but since
578:26 - we are tearing everything down it's not
578:28 - going to matter and i know we replaced
578:29 - it as an environment variable but
578:31 - remember it's in the git history so
578:33 - someone went back it got access to your
578:35 - github repository it went back in time
578:36 - they could find it but um what i'll do
578:39 - is i'll go to resource groups
578:43 - we'll look for our app insights here
578:46 - and we will hit delete resource group
578:49 - we'll say my app insights go ahead and
578:51 - delete delete and there's our cleanup so
578:53 - there you go that was application
578:56 - insights
578:58 - [Music]
579:01 - so azure monitor is uh quite a beefy
579:04 - service there's a lot going on in it and
579:06 - so it's a comprehensive solution for
579:07 - collecting analyzing acting on telemetry
579:09 - data from your cloud and on-premise
579:11 - environments so here's an example of one
579:13 - of the things it can do which is provide
579:14 - you a visual dashboard but it can create
579:17 - smart alerts automate actions log
579:19 - monitoring and a lot of different things
579:21 - and many azure services by default are
579:23 - already sending their data to azure
579:25 - monitor so you can use it right away
579:27 - [Music]
579:31 - now i just want to introduce you to the
579:33 - concept of the pillars of observability
579:35 - this isn't a thing specific to azure but
579:37 - it's something that in devops that you
579:38 - need to understand and it's just going
579:40 - to help you contextualize the offerings
579:42 - of azure monitor and other cloud service
579:44 - providers so what is observability that
579:47 - is the ability to measure and understand
579:48 - how internal systems work in order to
579:50 - answer questions regarding performance
579:52 - tolerance security and faults with a
579:54 - system and application and in order to
579:56 - obtain observability you need three
579:58 - things you need metrics logs and traces
580:00 - and you have to use them together using
580:02 - them in isolate does not gain you of
580:04 - observability so let's go define those
580:06 - three things the first thing is metrics
580:08 - and that is a number that is measured
580:10 - over a period of time if we measure the
580:11 - cpu usage and aggregate it over a period
580:13 - of time then we would have the average
580:15 - cpu metric then you have logs these are
580:18 - text files where each line contains
580:19 - event data about what happened at a
580:21 - certain time and then you have
580:24 - traces and this is a history of requests
580:26 - that is
580:27 - that
580:28 - is that travels through multiple apps
580:30 - and services so we can pinpoint
580:32 - performance or failures and i like to
580:34 - make the joke that it looks kind of like
580:36 - the triforce of observability once
580:37 - you've constructed it there at the end
580:39 - [Music]
580:44 - let's look at the anatomy of azure
580:45 - monitor which is a little bit complex
580:47 - but i'm sure we can work our way through
580:48 - it so the first thing is that we need uh
580:50 - sources of data so these this is what uh
580:53 - what data can be sent into azure so you
580:56 - probably want application data operating
580:57 - system data uh data from azure resources
581:00 - at your subscription level your tenant
581:02 - level which is going to be associated
581:03 - with active directory and custom sources
581:06 - and once you get those into azure
581:07 - monitor you have to store them somewhere
581:10 - you're going to put them in logs and
581:11 - monitors and these are just data stores
581:13 - that are within azure monitor that
581:14 - you're going to be able to work with
581:16 - and once you have that data in there
581:17 - you're going to be able to leverage
581:19 - different services and do different
581:20 - things
581:21 - and so we will call these functions
581:24 - so you can perform insights
581:26 - visualizations
581:27 - analysis response and integration so
581:31 - we'd look at insights uh we're getting
581:33 - insights into our virtual machines our
581:34 - containers our applications uh for
581:37 - visualization you might be making
581:39 - dashboards you could be using power bi
581:41 - or creating workbooks for analysis you
581:43 - might be using the log analysis or the
581:45 - metric analysis tools for responses
581:48 - you might want to create alerts or start
581:50 - auto scaling and for integrations you
581:52 - might want to use logic apps or export
581:55 - apis to connect things up so there you
581:57 - go
581:58 - [Music]
582:01 - so now let's take a closer look at all
582:03 - the different sources because this is
582:05 - going to really help us understand the
582:07 - utility at each level
582:09 - and i think that makes everything very
582:11 - crystal clear so remember that list of
582:13 - sources where we went to application to
582:14 - custom sources we're going to start at
582:15 - the top of that list and look at
582:17 - application code which is all about
582:19 - performance and functionality of your
582:20 - application and code so that's like
582:22 - collecting traces application logs and
582:25 - user telemetry
582:27 - and so here is the visual representation
582:29 - and so on the left-hand side we have our
582:31 - sources in the middle we have our
582:33 - storage and the right-hand side we have
582:34 - our our services that we're going to be
582:37 - doing things with that data so looking
582:40 - on the left you're in your application
582:42 - you'll probably want to go ahead and
582:43 - install the instrumentation package this
582:45 - is going to allow us to collect data
582:46 - into application insights and
582:48 - application insights is a application
582:50 - for
582:51 - getting rich data about our applications
582:53 - then you have availability tests this
582:55 - tests the response responsiveness of
582:57 - your application from different
582:58 - locations on the public internet
583:00 - and that's really useful if you're
583:01 - saying like if you have your application
583:03 - you're running it in
583:06 - the east of canada and the west of the
583:08 - us and you want to make sure that the
583:09 - response times are the same or lower
583:11 - that's going to help you get there
583:14 - for your metrics this is going to be
583:15 - describing your performance and operate
583:16 - and operations uh and custom metrics of
583:19 - your application for your logs you're
583:21 - going to be storing operational data
583:23 - about your application such as page
583:25 - views application requests exceptions
583:26 - and traces and then the things you're
583:29 - going to be putting in storage is you're
583:30 - going to send the application data
583:32 - to your azure storage for archiving or
583:34 - what you'll do is you'll store your your
583:36 - availability tests if you need to go and
583:38 - analyze them further
583:40 - or you could create a debug snapshot of
583:42 - your data so that you can then go debug
583:45 - that at a later point okay
583:48 - [Music]
583:51 - so let's take a look at how we're going
583:53 - to monitor our operating systems and
583:54 - this is for the guest operating system
583:56 - not the host operating system uh so when
583:59 - you're dealing with virtualization um uh
584:02 - the vm has its own os but the actual
584:05 - underlying hardware has its own os and
584:06 - that's the host one and that's not what
584:08 - we're looking at
584:09 - you don't need to monitor that one
584:10 - that's up to azure to do or your cloud
584:12 - provider we're looking at the guest one
584:14 - the one we actually can control
584:16 - so the operating system is going to need
584:18 - a couple of tools installed so or agents
584:20 - so we'll want to install the log
584:22 - analytics agent so we can use that with
584:23 - log analytics we'll probably want to
584:25 - install the dependency agent that allows
584:27 - us to monitor processes on the machine
584:29 - so those are the programs that are
584:30 - running so like you know let's say mysql
584:33 - redis maybe you have a ruby on rails app
584:35 - whatever
584:36 - um and so that's that
584:38 - and i just want you to know that these
584:39 - agents can be installed anywhere so it
584:42 - can be installed on azure on premise or
584:43 - even on other cloud providers like aws
584:46 - if you want performance counters you're
584:48 - going to have to install the diagnostic
584:49 - extension which is i think a good thing
584:51 - to do
584:52 - and just to make note the log analytics
584:54 - agent is going to store that data into
584:56 - logs then you can use that log analytics
584:58 - later
584:59 - if you want to
585:01 - have the or store store the health state
585:04 - information then you're going to be
585:06 - using the azure diagnostic extension and
585:08 - they're going to put that in azure
585:08 - storage
585:10 - and if you want to use azure event hub
585:13 - so event hub is a way of connecting your
585:15 - app to other destinations
585:18 - you're going to stream it but you're
585:19 - going to need the diagnostic extension
585:20 - for that all right
585:22 - [Music]
585:26 - let's take a look at azure resources and
585:28 - how we're going to monitor those so um
585:30 - you're going to have resource logs and
585:31 - these provide insights to internal
585:33 - operations for azure resources
585:35 - and these logs are automatically created
585:36 - for you but you will have to set up
585:38 - diagnostic settings to specify
585:40 - destination for them to be collected for
585:41 - each resource
585:43 - for
585:44 - metrics you're going to get those
585:45 - automatically you don't have to do any
585:47 - additional work to configure them and
585:48 - you're going to be able to analyze those
585:50 - in metrics explorer
585:52 - for your log data you're going to use
585:53 - log analytics to look for trends and
585:56 - other analyses
585:57 - you can also copy your platform metrics
585:59 - to logs for analysis
586:01 - and uh your log resource logs are going
586:04 - to be archived with azure storage for
586:06 - long-term backup
586:08 - if you want to uh
586:10 - use um or or send or trigger uh or send
586:14 - your data to other destinations you can
586:16 - use event hubs that's generally what
586:17 - event hubs is for so there you go
586:19 - [Music]
586:22 - taking a very quick look at how we
586:24 - monitor our azure subscription this is
586:26 - going to be for uh the the service
586:28 - health
586:29 - of different resources that you're using
586:31 - like are they okay are they running and
586:34 - things about azure active directory so
586:36 - that's about it
586:37 - [Music]
586:41 - i'm going to take a look at
586:43 - our azure tenant and how we would
586:44 - monitor that and if you remember our
586:46 - azure ad section that's where we define
586:48 - tenant because tenant is
586:49 - highly coupled to active directory and
586:52 - so this is going to be for tenant-wide
586:54 - services such as active directory so
586:56 - reporting that contains history of sign
586:58 - in activity audit trails of changes made
587:00 - within a particular tenant things like
587:02 - that so there you go
587:07 - so last on our list here is custom
587:09 - sources and basically if if none of the
587:11 - other previous categories fit where you
587:13 - want to collect data uh this is where
587:15 - you can just collect data using the
587:17 - azure monitor api and so basically you
587:20 - are using a rest client and then it's
587:22 - going to store it in analytics at log
587:25 - analytics or azure monitor so it's just
587:26 - really custom data custom storage
587:29 - [Music]
587:33 - so there's two fundamental types of data
587:35 - that we care about when working with
587:37 - azure monitor and that is logs and
587:39 - metrics and so azure has two
587:41 - services to deal with that one is called
587:43 - azure monitor logs and so this service
587:46 - collects and organizes log performance
587:48 - data for monitored resources data logs
587:50 - are consolidated from different sources
587:52 - into workspaces which we'll talk about
587:55 - platform logs from azure services is
587:57 - something you'll collect it'll collect
587:59 - also log performance data from vm agents
588:02 - the usage and performance data from
588:03 - applications also can be consolidated
588:05 - and these workspaces can be analyzed
588:07 - together together using a query language
588:09 - which we definitely cover in this course
588:11 - and you work with these log queries and
588:14 - the results interactively using
588:17 - a subservice called log analytics which
588:20 - is something we'll definitely be
588:21 - covering
588:22 - then there's azure monitor metrics so
588:24 - this is for the metric side so collect
588:26 - uh numeric data from monitor resources
588:28 - into time series database metrics are
588:31 - numerical values collected at regular
588:33 - intervals and describe some aspects of
588:35 - the system at a particular time uh
588:36 - lightweight and capable for of
588:38 - supporting near real real-time scenarios
588:41 - useful for alerting and fast detection
588:44 - of issues and you can analyze them
588:46 - interactively using metrics explorer and
588:48 - we cover that as well so really these
588:50 - two services are like the data stores or
588:53 - the databases for the respected types of
588:55 - data and then each of them have the sub
588:58 - services which is for exploring and some
589:00 - other additional services but let's get
589:02 - into that more detail now
589:04 - [Music]
589:08 - so when you use log analytics you're
589:10 - going to probably want a workspace and
589:11 - this is a unique environment
589:13 - for azure monitor log data each
589:15 - workspace has its own data repository
589:17 - and configuration and data sources and
589:19 - solutions that are configured to store
589:21 - their data in a particular workspace so
589:23 - it's really interesting because if you
589:25 - go over to azure monitor you can use uh
589:28 - log analytics without creating a
589:29 - workspace but i believe that if you want
589:31 - to
589:33 - isolate a lot of your data and
589:35 - install like maybe like collect it
589:37 - outside of azure services for other
589:39 - things you're going to need a workspace
589:41 - and it's going to also have a lot more
589:42 - robust options so that's something
589:44 - you're going to end up doing is creating
589:46 - workspaces and it's a good habit to do
589:49 - but there's not a lot to talk about
589:50 - there but let's move on to actual the
589:52 - query language which is really the meat
589:54 - of log analytics
589:55 - [Music]
589:59 - i want to quickly touch on log analytics
590:01 - and so this is a tool used to edit and
590:03 - run queries within azure monitor logs
590:06 - and so the idea is it kind of looks like
590:09 - something you use to connect to a
590:10 - database because it really is structured
590:12 - like a database with tables and columns
590:14 - and things like that and it has this its
590:16 - own query language called a kql and the
590:19 - idea is that you input
590:22 - your queries and it's going to output
590:23 - results for you and that's something
590:25 - we're going to look at in greater detail
590:26 - is that
590:27 - kql language so that we know how to use
590:29 - that panel let's go talk about
590:31 - workspaces
590:36 - let's take a closer look at custo and
590:38 - it's query language so azure monitor
590:40 - logs is based off of the azure data
590:42 - explorer and along came with it is the
590:45 - cousteau query language also known as
590:48 - kql and this is the way we're going to
590:51 - uh filter and uh sort and do things with
590:54 - our logs so kusto is based on a
590:56 - relational database management system
590:59 - and it supports entities such as
591:00 - database tables and columns there's also
591:03 - this thing called clusters and uh kql
591:06 - actually has a lot utility in azure
591:08 - because it's not just in monitor logs
591:10 - and data explorer you can use it in log
591:13 - analytics log alert rules workbooks
591:15 - dashboards
591:17 - logic apps powershell azure monitor log
591:20 - apis so it's definitely something you
591:22 - can be using across the board in azure
591:24 - and so they have some basic operators
591:28 - they have lots of operators that you can
591:29 - use so you can do calculated columns
591:31 - searching filterings on rows group by
591:33 - aggregates join functions and we're
591:35 - going to be looking at a lot of the
591:36 - operators in more detail after this
591:38 - slide here
591:39 - but anyway
591:40 - the queries execute in the context of a
591:42 - cousteau database that is attached to
591:45 - acousto cluster and we will talk about
591:47 - clusters database tables and columns up
591:49 - next
591:51 - [Music]
591:55 - let's take a look at what makes up
591:57 - something for cousteau and so we have a
591:58 - bunch of entities here clusters database
592:00 - tables columns and functions and so i
592:02 - have this nice visual to help us uh kind
592:05 - of see how they all work together so the
592:07 - top we have clusters and these are
592:08 - entities that hold multiple databases
592:10 - you can also have multiple clusters
592:12 - but it's just not being shown there in
592:14 - that graphic then you have the databases
592:16 - themselves these are named entities that
592:18 - hold tables and store functions you have
592:20 - tables these are named entities that
592:22 - hold data and a table has an ordered set
592:24 - of columns and zero or more rows of data
592:26 - each row holding one data value for each
592:29 - of the columns of the table then there
592:31 - are the columns themselves and these are
592:32 - named identities that have a scalar data
592:35 - type columns are referenced in the query
592:37 - relative to the tabular data stream and
592:39 - that is the context of the specific
592:41 - operator referencing them then we have
592:43 - stored functions and these are named
592:44 - entities that allow reuse of cousteau
592:46 - queries or query parts
592:48 - and then you got these external tables
592:50 - and these are tables that uh live
592:52 - outside of your cluster
592:54 - i think that uh you're referencing them
592:56 - from uh storage accounts and they're in
592:58 - blob storage so they pro i think there
593:00 - could be like csv files and stuff like
593:01 - that but these external tables are used
593:03 - for exporting data from cousteau to
593:05 - external storage so storage uh storage
593:07 - accounts
593:08 - as for querying external data without
593:09 - ingesting it actually into cousteau so
593:12 - hopefully that gives you an idea the lay
593:14 - of the land there
593:15 - [Music]
593:19 - let's take a look at the type of data
593:20 - types that we can use in cousteau but
593:22 - let's just define what scalar and data
593:24 - types are so scalars are quantities that
593:27 - are fully described by a magnitude or
593:29 - numerical value alone
593:31 - so the idea is that it just means like a
593:32 - single value then you have data types
593:34 - and a data type defines how a piece of
593:36 - data is interpreted so an integer number
593:38 - could be a data type if you've ever used
593:40 - programming languages like data types
593:42 - and stuff like that or an sql database
593:44 - you should be familiar with those so
593:46 - incusto data types are used for various
593:48 - things so they can be for columns or
593:50 - function parameters that expect specific
593:52 - data types
593:53 - uh
593:54 - that they want them to be
593:55 - and so let's just go through the quick
593:57 - list of data types because there are
593:58 - quite a few here and i've summarized
594:00 - them quickly so the first is billion and
594:01 - this represents a true false value then
594:03 - you've got date times your date and this
594:05 - represents a date and these are stored
594:07 - in ut
594:08 - utc time zones you got decimals these
594:11 - are numbers that like are 12.88
594:14 - or something with a decimal in them you
594:16 - have integers which is like a whole
594:18 - number
594:19 - then you have longs which are also
594:21 - integers but with a greater range you
594:24 - have your guids or your uids and these
594:27 - are
594:28 - unique values so it's like a random
594:31 - hash there that you have and the idea is
594:33 - just so that you have unique values but
594:35 - they're not like one two three four five
594:37 - so people can't guess the size of your
594:40 - tables then you have reels these are
594:42 - double position floating point numbers
594:44 - so these are really really big num
594:45 - numbers something you probably use if
594:47 - you're doing um
594:49 - things with uh finance or numbers then
594:51 - you have strings and these are unicode
594:54 - strings and they're limited by default
594:56 - to one megabyte and the way you do them
594:58 - is you have them wrapped in quotations
594:59 - we have that hello world you have time
595:01 - spans these are interesting these are
595:03 - little time intervals represented so
595:05 - like 2d would equal 2 days 30 m would
595:07 - equal 30 minutes one tick would equal
595:09 - 100 nanoseconds and they have a variety
595:11 - of those
595:12 - then you have dynamic and this is a
595:14 - special type that can do a bunch of
595:15 - things so it can accept primitive scalar
595:17 - data types so the idea is it's kind of
595:19 - like a
595:22 - data type that can accept any value
595:24 - or it can accept an array of any value
595:26 - or it can accept a property bag which
595:29 - you know if you're from the javascript
595:30 - world looks like a json object so you
595:32 - have a key and value and it can even be
595:34 - nested and then last which is a data
595:37 - type but you're not setting these as
595:38 - your columns which is the null and
595:40 - that's a special value that represents a
595:41 - missing value and any of these data
595:43 - types can be a null value so like you
595:45 - look at boolean it can be true false or
595:47 - null and date time can be a date time or
595:50 - date or null and that goes for all of
595:52 - them
595:53 - [Music]
595:57 - let's talk about cousteau control
595:59 - commands this is really part of the uh
596:02 - kql language but it's a way of kind of
596:04 - working with the the databases and
596:06 - tables and stuff like that if you've
596:08 - ever used postgres and you've done like
596:09 - forward slash
596:10 - du or you have these commands that
596:12 - didn't really have to do necessarily
596:14 - querying but like managing the databases
596:16 - and tables and stuff that's what uh
596:18 - control commands are so i'll give an
596:20 - example so here we have a control
596:22 - command to create acousto table so they
596:24 - always start with a period so period
596:26 - create table logs and then it makes two
596:28 - columns and that when you're using the
596:30 - query um like log analytics and stuff
596:32 - like that you just type period and then
596:33 - start typing it
596:35 - and it'll show you all the list of
596:37 - control commands it was really hard to
596:39 - find the documentation a full list
596:40 - otherwise i would have picked some out
596:42 - for you
596:43 - but generally you can just kind of
596:44 - explore that way or the most common ones
596:46 - you're going to come across is all going
596:47 - to be in the documentation
596:49 - even though there's a very long list of
596:50 - control commands a very common one
596:52 - you're going to be using a lot is show
596:54 - so for example we can do dot show tables
596:56 - and then count the number of tables
596:58 - there are and so i just want to show you
597:00 - that if you type dot show
597:03 - it'll show you what it can take as a
597:04 - second parameter
597:06 - and there's a huge list there so just an
597:08 - idea like how you can work the control
597:10 - commands
597:11 - [Music]
597:14 - take a look here at custom functions and
597:16 - these are reusable queries or query
597:18 - parts and kusa supports several kinds of
597:21 - functions the first being stored
597:22 - functions which are user-defined
597:24 - functions and these are stored and
597:26 - managed as one kind of database schema
597:29 - entity
597:30 - then just to define user user-defined
597:33 - functions uh one step further they come
597:35 - in two categories we have scalar
597:37 - functions so these uh have input scalar
597:40 - data types and output scalar data types
597:42 - and then tabular functions so these
597:44 - uh take input tabular data and they
597:46 - output tabular data tabular data is just
597:49 - when you're working with multiple rows
597:50 - in a table
597:52 - then you have query defined functions
597:53 - which are also user defined functions
597:55 - and uh that are defined and used within
597:57 - the scope of a single query so very
598:00 - similar to stored functions but
598:02 - they're it's all based on scope okay
598:05 - uh then the last on our list here is
598:06 - built-in functions which are hard-coded
598:08 - and these are defined by cousteau and
598:09 - cannot be modified users and they just
598:11 - give you a lot of utility let's take a
598:13 - look at some of these built-in functions
598:15 - starting with special functions so
598:16 - select cousteau entities so you might
598:19 - want to say i want to select a cluster
598:20 - and then i want to select a database and
598:22 - then that table
598:24 - then you have aggregate functions which
598:25 - perform calculations on a set of values
598:27 - and return a single value so
598:29 - maybe you want to do a count so you see
598:31 - where it says count and parentheses
598:34 - then you have windows functions and
598:35 - these operate on multiple rows and
598:36 - records in a row
598:39 - in a row and so one really popular one
598:41 - is row number and so this could count
598:43 - the number of rows maybe you have to
598:44 - figure out the row numbers in relation
598:46 - to your query
598:47 - and so that's something you could do
598:49 - there
598:50 - [Music]
598:53 - let's take a look at some scalar
598:55 - operators and there's quite a few here
598:57 - and these are for working with scalar
598:58 - data types to do comparisons
599:01 - and there's a bunch of different
599:02 - categories so let's just uh quickly go
599:04 - through them here the first one one is
599:06 - bitewise operators that's where you're
599:07 - working with zeros and ones and flipping
599:09 - them around to do different things so
599:10 - you've got the binary end the not the or
599:13 - the shift left the shift right uh the
599:16 - xor and these make a lot of sense if you
599:18 - know how to work with binary math if you
599:20 - don't don't worry about them but i just
599:21 - want you to know if you do they're
599:23 - they're there for you use next are
599:24 - logical operators so this is what you're
599:27 - going to be more familiar with like
599:28 - equality so equals in equal
599:30 - and or or so this is pretty darn simple
599:33 - then you have date time and time span uh
599:35 - arithmetic
599:36 - and so
599:37 - we have add or subtract date times you
599:39 - can do that you can add subtract divide
599:42 - or multiple time span so add one day
599:45 - plus two day makes three days
599:47 - uh yeah then you have numerical
599:48 - operators and these work on inch longs
599:50 - and reals
599:52 - so you can add subtract multiply and
599:54 - divide you can do module where it's
599:57 - determining whether it's a whole number
599:58 - or not so or divisible by a whole number
600:01 - so like let's say you have 17 and you
600:02 - put it through there you say module
600:05 - 17 by like modulus 2
600:08 - if it's not divisible by two like
600:10 - perfectly resolves into zero it will
600:12 - return either zero or one then you have
600:14 - less greater equal not equal uh less or
600:18 - equal than
600:19 - uh greater or equal so you got the idea
600:21 - there equals to one of the elements so
600:23 - that's in so you do the in it's very
600:26 - familiar if you're used to using sql
600:28 - then you have the opposite where you put
600:30 - an exclamation in front of it actually
600:32 - there's a lot of extra operators that
600:33 - just have the same thing with an
600:35 - explanation to mean the opposite
600:36 - especially with string operators
600:38 - so here's a bunch of string operators
600:40 - and they almost all have exclamations uh
600:42 - a variant with explanation in front of
600:43 - them then you have your between
600:45 - operators this matches the input
600:47 - that is inside an inclusive range so you
600:50 - can say between 1 and 10 or between
600:52 - these date times so there you go
600:54 - [Music]
600:58 - let's take a look at cousteau tabular
601:00 - operators and i really want you to pay
601:02 - attention to this one because this is
601:03 - where all the power of kql happens so
601:07 - these perform comparisons against a
601:08 - bunch of rows that's why it's called
601:10 - tabular operators
601:12 - and there's a lot of them and we're
601:13 - going to look at the most common ones
601:15 - because it would take me forever to go
601:16 - through all of them and you're not going
601:18 - to remember them all so let's just look
601:19 - at the ones we actually care about
601:20 - working with so the first is count this
601:22 - returns the count of rows in the table
601:24 - then you have take this returns up to a
601:25 - specified number of rows of data you
601:27 - have sort this uh will
601:30 - take the rows of the input into the
601:31 - order
601:32 - by one or more columns you see buy
601:35 - damage property descending so let's just
601:37 - sort
601:38 - then you have project returns a specific
601:40 - set of columns and where filter a table
601:42 - to subset of rows that satisfy
601:45 - a predicate before we move on the next
601:46 - slide i just want to point out like if
601:48 - you've worked with sql
601:49 - these are very familiar right like take
601:51 - is like limit sort is like order by pro
601:53 - project is like select okay
601:55 - moving on to the next one here we have
601:57 - top returns the first and records sorted
601:59 - by uh specified columns this is kind of
602:02 - like a shorthand i think it like makes
602:04 - uh take and sort and just abbreviates it
602:07 - to one line you have extend creates a
602:09 - new column by computing a value so
602:12 - notice that it says duration equals n
602:14 - time minus start time and then we're
602:15 - using duration somewhere else then you
602:17 - have summarize aggregate groups of rows
602:20 - that's kind of like group by
602:22 - in
602:23 - regular sql and then render so renders
602:25 - results as a graphical output and to me
602:28 - that's a really cool one
602:30 - [Music]
602:34 - moving on to metrics we're going to be
602:36 - looking at metrics explorer so message
602:37 - explorer is a subservice of azure
602:39 - monitor allows you to plot charts
602:41 - visualize correlating trends and
602:42 - investigates spikes and dips in metric
602:44 - values so the idea is you can create a
602:47 - really cool graph like that and you can
602:49 - make a variety of different ones based
602:51 - on how you want to chart it out let's
602:53 - talk about how would we define one of
602:55 - these metric visualizations in metric
602:57 - explorer so you got this cool bar and
602:59 - the idea is you got to fill it all the
603:01 - way to the end and then that will
603:02 - visualize it for us the first is the
603:04 - scope so you're going to open this up
603:06 - and it's going to allow you to um select
603:08 - resources so it'll show you like
603:10 - subscription and stuff like that and
603:12 - resource resource groups but always
603:14 - makes you select a resource at the at
603:16 - the end and some services you can select
603:18 - multiple resources and some you can only
603:20 - select a single instance so the storage
603:22 - account there it's only going to be a
603:23 - single one there so i had one called
603:25 - davestrom institute i made
603:27 - and then there you're going to choose a
603:28 - namespace so this is a specific group of
603:31 - metric data within a resource so notice
603:33 - like it makes sense for a storage
603:35 - account that would show account blob
603:36 - file queue table it's going to vary
603:38 - based on your service then you actually
603:40 - have the metric you care about so we
603:41 - have availability uh
603:44 - aggress ingress etc and a bunch of other
603:46 - things you're going to choose one of
603:46 - those and then you choose how you want
603:48 - to aggregate aggregate it so average min
603:51 - max etc and again this is going to be
603:53 - totally different based on what
603:54 - resources you choose but that's
603:56 - generally how it works
603:58 - [Music]
604:01 - let's take a look here at azure alerts
604:03 - and this helps us be notified when there
604:05 - are issues found within the
604:07 - infrastructure or application and this
604:09 - allows us to identify address issues
604:11 - before the users of your system notice
604:12 - them and so they come in three flavors
604:14 - we've got metric alerts log alerts and
604:16 - activity log alerts and when alert is
604:19 - triggered you can be notified or have it
604:20 - take action so here is kind of the
604:22 - anatomy of an alert and we have the
604:24 - alert rule this defines who we should
604:26 - monitor like the the service and uh
604:29 - uh
604:30 - like the the definition of when it is
604:32 - triggered which is going to be the next
604:33 - part here so a resource such as a
604:36 - virtual machine designated as the target
604:38 - resource will emit signal so it's going
604:40 - to be emitting a data payload and it
604:42 - could be of the following types could be
604:44 - a metric a log activity log application
604:46 - insights you can kind of see how that
604:48 - ties to the types of alerts then you
604:50 - have the criteria or logical tests this
604:52 - gets evaluated and determines are we in
604:55 - a triggered state it could be like
604:56 - percentage cpu greater than 70 percent
604:59 - then you have your action group which
605:00 - contains actions um and those actions uh
605:04 - will be uh performed when it is
605:06 - triggered and actions could be things
605:07 - like run uh run an automation run book
605:11 - use azure functions itsm logic app web
605:14 - hooks or secure web hook
605:15 - on the other side there we have um this
605:18 - box over here and this is all about the
605:19 - state of your alert
605:21 - and so we have monitor conditional alert
605:23 - state so monitor's condition is set by
605:25 - the system an alert state is set by the
605:27 - user but the idea that is there
605:29 - is so you can define where it is because
605:31 - you might want to
605:33 - have a history of saying okay i've
605:34 - resolved this issue so i'm marking this
605:37 - as closed and that'd be like an alert
605:39 - state there and so there you go
605:41 - [Music]
605:45 - this is probably not going to be on your
605:46 - exam but it's super cool to show and
605:48 - it's only one slide away so let's talk
605:49 - about it which are azure dashboards
605:51 - these are virtual workspaces to quickly
605:52 - launch tasks for data for day-to-day
605:54 - operations and monitor resources and
605:56 - build custom dashboards based on
605:58 - projects tasks or user roles so the idea
606:00 - is you can go in here and you have like
606:02 - this little tile uh editor and you can
606:04 - drag stuff over so you can see i can
606:06 - build like a video in and a link to the
606:08 - help support and put a clock and some
606:10 - metrics i care about and some markdown
606:12 - so it's a really good way of building
606:14 - out these customized dashboards based on
606:16 - a user role to really help you focus on
606:18 - what you have to deal with
606:20 - within your infrastructure on azure
606:23 - [Music]
606:27 - let's take a look at azure workbooks
606:29 - just because this is in the scope of
606:30 - azure monitors so we should cover it so
606:32 - workbooks provide a flexible canvas for
606:34 - data analysis and the creation of rich
606:36 - visual reports within the azure portal
606:38 - to allow you to tap into multiple data
606:39 - sources from azure and combine them into
606:41 - a unified interactive experience
606:43 - and so the key word here is story it
606:46 - tells a story about the performance and
606:48 - availability about your applications and
606:50 - services
606:51 - so this thing kind of looks like a
606:53 - dashboard but it isn't and it's highly
606:55 - customizable but it's not very clear in
606:57 - here but really what it is it's like
606:59 - it's like the form of a document and the
607:01 - idea is that imagine that you have a
607:03 - document and then you can embed
607:04 - analytics in it
607:06 - that are real-time analytics so that you
607:08 - can visualize uh uh and and kind of
607:12 - investigate and discuss performance and
607:14 - availability and stuff like that you can
607:16 - kind of think of it like if you ever
607:18 - worked with jupiter notebooks but it's
607:19 - really for performance and monitoring
607:22 - um it's kind of like that okay and these
607:25 - things are highly customizable and
607:26 - really useful and uh if you ever use
607:28 - datadog databa dog has its own thing i
607:31 - think they call them notebooks as well
607:32 - they have workbooks they're called
607:34 - notebooks it's the same thing but it's a
607:35 - really great tool for
607:38 - really understanding your your
607:39 - performance
607:40 - [Music]
607:44 - hey this is andrew brown from exam pro
607:46 - and we're looking at azure monitor cheat
607:47 - sheet this one's a three parter so let's
607:50 - jump into it azure monitors
607:52 - comprehensive solution for collecting
607:53 - analyzing and acting as telemetry for
607:56 - your cloud on-premise environments and
607:58 - even though i don't have it in the slide
607:59 - here it is an umbrella service i mean
608:00 - there's a lot of things underneath this
608:02 - and that's why we have three pages of
608:04 - information and create visual dashboards
608:06 - smart alerts automated actions log
608:08 - monitoring
608:09 - to obtain observability you need to know
608:11 - metrics logs and traces you can have use
608:13 - them all together using them in isolate
608:15 - but that does not give you observability
608:17 - metrics is a number that is measured
608:18 - over a period of time logs as a text
608:20 - file that contains event data which was
608:23 - happening and traces is a history of
608:25 - requests that that travels through
608:26 - multiple apps and services
608:28 - so we can pinpoint performance or
608:29 - failures
608:30 - azure monitor collects two fundamental
608:32 - types of data from sources logs and
608:34 - metrics which kind of matches up with
608:36 - our
608:37 - theoretical information here right so
608:39 - azure monitor logs collects and
608:41 - organizes log and performance data for
608:42 - monitoring resources data logs are
608:44 - consolidated from different sources into
608:46 - workspaces platform logs and azure
608:48 - services logs and performance data and
608:50 - virtual machine agents usage and
608:52 - performance data for applications can be
608:54 - consolidated in a workspace you can be
608:56 - analyzed they can be analyzed together
608:59 - using a sophisticated query language
609:01 - which we'll talk about here in a moment
609:02 - or review i suppose um
609:05 - work with log queries and their results
609:08 - uh interactively using log analytics
609:10 - azure monitor metrics collects and
609:12 - that's the second part here right so the
609:14 - two fundamentals azure monitor metrics
609:17 - collects new numeric data from monitored
609:19 - uh resources in a time series database
609:22 - messages are new numerical values
609:24 - collected at regular intervals and
609:26 - describe some aspects of a system at a
609:28 - particular time lightweight and capable
609:30 - of supporting near-real-time scenarios
609:33 - useful for learning and fast detection
609:34 - of issues you can analyze them in
609:36 - interactively via the metrics explorer
609:39 - onto page two log analytics is a tool in
609:42 - the azure portal used to edit and run
609:44 - log queries with data in the azure
609:46 - monitor logs login elixirs use the
609:48 - language called q
609:50 - kql
609:51 - log analytics uh i think that's cousteau
609:54 - right so log analytics workspace is a
609:56 - unique environment for the monitor logs
609:57 - data each workspace has its own data
609:59 - repository and configuration data
610:02 - sources and solutions are configured to
610:04 - store in their workplace and i really
610:06 - wish i put the word data like in here
610:08 - it's not important for uh the associate
610:11 - but for other exams it helps you
610:12 - understand if you think of it as a data
610:14 - lake azure monitor logs is based on the
610:16 - azure data explorer and log queries are
610:18 - written using the cousteau query
610:20 - language
610:21 - kql can be used in a log analytics log
610:24 - alert rules workbooks azure dashboards
610:26 - and all over the place kosovo is based
610:28 - on a relational database management
610:29 - system so you'll see database tables and
610:31 - columns some query operations include
610:33 - calculated columns searching and
610:34 - filtering on rows grouped by aggregates
610:36 - joint functions if you're used to using
610:37 - sql you know what i'm talking about
610:39 - cousteau queries execute in the context
610:41 - of some cousteau database that is
610:43 - attached to acousto cluster crystal is
610:45 - generally composed of the following
610:46 - entities clusters database tables
610:48 - columns functions let's talk about them
610:49 - quickly here
610:50 - clusters are entities that hold
610:52 - databases databases are are named
610:54 - entities that hold tables and stored
610:55 - functions stored functions are named
610:57 - entities that allow reuse
610:59 - queries or query parts tables are named
611:02 - entities that hold data columns are
611:04 - named entities that hold scalar data
611:06 - types and then you have external tables
611:08 - which are identities that reference data
611:10 - stored outside the cousteau database
611:11 - these both count as tables that's why
611:13 - it's not in the list up here
611:15 - but generally these are pretty
611:17 - self-explanatory
611:18 - metric explorers is a sub-service of
611:20 - azure monitor that allows you to plot
611:22 - charts visualize correlating trends and
611:24 - investigate spikes and dips in the
611:26 - metrics values to visualize the metric
611:27 - you need to define the scope the
611:29 - namespace the metric and the aggregation
611:32 - we're on to the last page here
611:34 - alerts and notify you when issues are
611:36 - found within your infrastructure or
611:37 - application they allow you to identify
611:39 - address issues before the users or
611:41 - system notice them azure has three kinds
611:43 - of alerts metrics logs and activity
611:46 - log alerts
611:47 - sorry there is a diagram if you remember
611:51 - our follow along i actually break down
611:53 - all the uh
611:55 - structure of
611:56 - an alert you should go review that i
611:58 - didn't put in the cheat sheet because it
611:59 - just would have been too much here
612:01 - azure dashboards are a virtual workspace
612:03 - to quickly launch tasks for day-to-day
612:05 - operations and monitor resources azure
612:07 - workbooks provide a flexible canvas for
612:09 - data analysis and the creation of rich
612:12 - visual reports within the azure portal
612:14 - it tells it and just to highlight what
612:16 - azure workbooks are for it tells the
612:18 - story about the performance availability
612:20 - about your applications and services
612:22 - then we have application insights and
612:24 - this is an application performance
612:26 - management an apm service and it is a
612:28 - subservice of azure monitor that's why
612:30 - all these things are under azure monitor
612:32 - automatically detects performance
612:33 - anomalies includes powerful analytics
612:35 - tools to help you diagnose issues to
612:37 - understand what users do with your app
612:39 - designed to help you continuously
612:40 - improve performance and usability works
612:42 - for apps on the dotnet node.js java
612:45 - python hosted on premise hybrid and
612:47 - public cloud works everywhere basically
612:49 - integrates with your devops processes
612:51 - can monitor and analyze telemetry from
612:54 - mobile apps by integrating the visual
612:55 - studio app center to use application
612:58 - insights you need to instrument your
612:59 - application
613:00 - to instrument you need to install the
613:02 - instrument package sdk or enable
613:04 - application insights using application
613:06 - insight agents when supported apps can
613:08 - be instrumented from anywhere when you
613:10 - set up your application insights
613:12 - monitoring for your web app you create
613:14 - an application insight resource into
613:15 - azure monitor
613:17 - you open the resource in the azure
613:19 - portal to
613:20 - order in order to see an analyze
613:22 - telemetry uh collected from your app and
613:25 - last the resource is identified by the
613:26 - information instrumentation key the ik
613:29 - so there you go
613:31 - [Music]
613:35 - hey this is andrew brown from exam pro
613:37 - and we are taking a look at azure api
613:38 - management so this integrates existing
613:41 - back-end services into modern api
613:44 - gateways and uh this service uh there's
613:47 - a lot to it so we're going to be doing
613:49 - quite a bit here maybe more than we have
613:51 - to but it is a very powerful service
613:53 - especially if you're trying to be a
613:55 - developer on azure so we're going to
613:57 - just make sure we spent a good amount of
613:58 - time with it so it follows the api
614:00 - approach of decoupling the front end and
614:02 - back-end teams with the help of api
614:04 - mocking
614:06 - the azure api management handles the
614:08 - full management of your apis it
614:09 - centralizes
614:11 - the securing versioning documentation
614:13 - and compliance from your backend
614:15 - services in a single endpoint so very
614:18 - powerful tool but let's get to it
614:23 - [Music]
614:24 - so let's go over uh key concepts or key
614:27 - components of api management i do not
614:29 - have a fancy visual for this it's just
614:31 - very hard to visualize but uh we will
614:33 - learn all this stuff as we go through it
614:35 - so let's talk about what we have here so
614:37 - we have an api that represents a set of
614:39 - operations
614:41 - api operation which connects an api
614:43 - endpoint to its back end we have a
614:45 - product which is a logical grouping of
614:46 - apis
614:48 - a single or group of apis make up a
614:50 - product so this is how your apis are
614:53 - presented to developers so it can either
614:54 - be public or private we have a backend
614:56 - that represents backend services in your
614:58 - api
614:59 - there are groups these are used to
615:01 - manage the visibility of products to
615:02 - developers such as administrators who
615:04 - have full access to the api management
615:06 - developers for users with access to the
615:08 - developers portal with permissions to
615:09 - build applications guests users without
615:12 - access to the developer portal portal
615:14 - but with the reading permissions and
615:16 - some services
615:17 - there's the idea of developers so this
615:19 - belongs to a product group and each
615:21 - developer has a primary secondary key to
615:24 - call that products api there are
615:26 - policies uh
615:28 - configurations and validations that are
615:29 - applied
615:30 - in progress to incoming requests and a
615:34 - com responses which you will see
615:36 - in closer detail in the upcoming slides
615:38 - there are name values so these are key
615:40 - value pairs used with policies values
615:42 - can be a result of an expression uh
615:44 - there are gateways so this is where your
615:47 - api calls it and are received and
615:49 - policies are applied to incoming
615:51 - requests then there's the developer
615:53 - portal so this is where developers can
615:54 - access all the apis and products listed
615:56 - by your apim alongside the api's
616:00 - operations documentation developers can
616:01 - also request access to your apis from
616:03 - the developer portal but again uh we'll
616:07 - figure this out as we go through okay
616:09 - [Music]
616:13 - all right let's take a look here at the
616:14 - echo api service so when you create
616:17 - uh apim gateway that's the first thing
616:19 - you'll do you'll get by default uh this
616:21 - thing called echo api and i was like
616:23 - whoa what is this thing i wasn't sure
616:24 - what it was
616:25 - and i thought it was kind of interesting
616:27 - that's why i expand upon it here which i
616:29 - didn't really see much of it in the
616:30 - documentation so i had to
616:32 - piece things together but the idea is
616:33 - that echo api provides a bunch of
616:36 - existing endpoints and these are mocked
616:38 - endpoints to a non-production azure
616:40 - service used to test azure api
616:42 - management so you know if you don't have
616:44 - an api yet but you want to interact with
616:46 - it it's going to go to the service
616:48 - called
616:50 - echoapi.cloudapp.net forward slash api
616:52 - and again it's just a dummy a dummy
616:54 - application to test against so that's
616:56 - what that is so if you see it you don't
616:58 - need it you can absolutely delete it but
617:00 - you always get one when you create a
617:02 - gateway okay
617:03 - [Music]
617:07 - all right let's take a look at the
617:08 - feature comparison for
617:11 - apim because it's one of those services
617:13 - where there's a lot of features but
617:14 - they're not always available to you
617:15 - depending on what plan subscription what
617:17 - kind of user you are so let's take a
617:19 - look at how the features change based on
617:22 - uh what plan you're using so across the
617:24 - top we have consumption developer basics
617:27 - standard and premium red is what's not
617:30 - available for that particular plan green
617:32 - is what is
617:34 - so for azure ad we do not have it for
617:36 - consumption or basic
617:39 - for virtual network support it's just
617:42 - it's just not supported for consumption
617:43 - basic and standard
617:45 - for multi-region deployment availability
617:47 - zones we're only getting that for
617:48 - premium for multiple custom domains
617:51 - premium as well and also for developer
617:53 - developer portal the built-in cache and
617:56 - built-in analytics are not available in
617:58 - the consumption plan if you want a
618:01 - self-hosted gateway better be on the
618:03 - premium basically almost developer
618:05 - always says yes because you have to play
618:07 - around with things right
618:08 - um so tls settings is for everyone
618:11 - external caches for everyone client
618:13 - certificate authentication policies are
618:15 - for everyone which is very generous back
618:17 - in restore you're not going to get in
618:18 - the consumption
618:20 - model and that's not all we got a few
618:22 - more here so if you want management over
618:24 - get direct management api azure monitor
618:27 - logs and metrics static ip or web
618:29 - sockets api you're not getting those in
618:31 - the consumption model now graph api
618:34 - graph graphql
618:36 - api is available for all of them so
618:38 - there you go
618:40 - all right let's take a look at api
618:41 - authentication so in order to
618:43 - authenticate with their apis we
618:44 - configure those settings under our
618:46 - subscription setting so it's as simple
618:48 - as doing a checkbox there so if this
618:50 - description is required only developers
618:52 - with a valid access key can use it and
618:55 - so the idea here is we can configure
618:57 - where the api will receive
618:59 - those access keys which can be sent as
619:01 - the header or query string
619:04 - so there are some options there if this
619:06 - thing is not checked then that means
619:08 - anyone that is anonymous has request
619:11 - requests will be allowed so it's as
619:13 - simple as having that checkbox there and
619:16 - having a key so there you go
619:19 - [Music]
619:23 - all right so let's take a look here at
619:24 - groups for apim so groups are used to
619:26 - manage the visibility of products to
619:28 - developers and so we can break these
619:29 - down into some broad categories we have
619:31 - administrators so they manage api
619:33 - management service instances and create
619:35 - the api's operations and products that
619:38 - are used by developers then we have
619:40 - developers they are authenticated for
619:42 - the developer portal the users that
619:45 - build applications using uh your apis
619:48 - developers are granted access to the
619:50 - developer portal and build applications
619:51 - that call the operations of an api then
619:54 - you have guests so these are
619:56 - unauthenticated developer portal users
619:58 - such as prospective customers visiting
620:00 - the developer portal they can be granted
620:02 - certain read-only access such as the
620:04 - ability to view apis but not call them
620:08 - so administrators can also create custom
620:10 - groups or use external groups in an
620:12 - associated azure active directory tenant
620:14 - to give developers visibility and access
620:16 - to api products a user can belong to
620:18 - more than one group so there you go
620:21 - [Music]
620:26 - all right let's take a look here at
620:27 - front ends and backhand starting with
620:29 - front end so front ends to find the
620:31 - route or end point and the documentation
620:33 - configuration around that endpoint so
620:35 - the idea here is we have front end and
620:38 - we have a
620:39 - single method here uh which is post
620:41 - forward slash resource
620:43 - and if we were to open that up you can
620:45 - see that we have i'm just getting my pen
620:48 - tool out here we have a description for
620:50 - our documentation the display name the
620:52 - name
620:53 - of i can't remember what the difference
620:55 - for the name is but it's called create
620:56 - resource we can set its url that's
620:59 - method and then down below we have
621:01 - additional parameters so api
621:03 - does not host apis but it creates
621:05 - facades for your apis that's a key thing
621:07 - to remember api so api management it
621:10 - does not host apis it's created facade
621:14 - for your apis
621:15 - so let's take a look at the back end so
621:17 - for back ends you can set the following
621:19 - types you can set a custom url so point
621:20 - to a server where your service is
621:22 - running you can say to go to azure
621:25 - resources integrate directly with a
621:26 - resource such as azure functions app
621:29 - service container wrap logic app i'm
621:32 - just missing the p there but it's two
621:34 - p's
621:35 - so you have the idea up here where we
621:37 - can see custom url azure resource azure
621:40 - service fabric okay
621:42 - we have some additional options here so
621:44 - we can authorize credentials
621:46 - that present requests credentials to the
621:49 - backend service
621:51 - there are options like headers so that's
621:54 - http headers we can fetch
621:56 - from named values we can query based on
621:59 - query string you can fetch from name
622:00 - values there as well
622:02 - you have control of client certificates
622:04 - so x 509 certificate certificates which
622:07 - we do talk about in this course
622:10 - which are stored in azure key vault
622:11 - which is the section that we're talking
622:12 - about there so you're just seeing those
622:14 - options headers queries and client
622:16 - certificates
622:17 - but yeah there you go
622:21 - [Music]
622:23 - all right let's take a look here at
622:24 - policies for apim so api management
622:27 - policies allow you to change the
622:29 - behavior in multiple stages of your
622:30 - endpoints request lifecycle you can
622:33 - update any part of the request response
622:35 - message such as the headers bodies urls
622:38 - and there are four areas where policies
622:39 - can be applied we have inbound for
622:41 - incoming requests backend before
622:43 - requests reach your backend outbound
622:45 - before sending responses back to the
622:46 - client and error when a request
622:48 - encounters an error just to kind of
622:51 - visualize it we have our front end
622:52 - incoming processing backend outbound
622:55 - processing and so the idea is that
622:58 - here
622:59 - for the inbound processing that's where
623:00 - we have policies apply that's the one
623:02 - they're talking about right here
623:04 - and then we have for the back end so
623:06 - before requests reach your backend
623:09 - fbm processing and then uh error is not
623:12 - visualized because it would be whenever
623:14 - an error would occur
623:16 - here okay azure has a collection of
623:18 - policy groups which contain many
623:20 - policies you can apply and we got a big
623:22 - list here we have access restriction
623:23 - policies advanced authentication caching
623:27 - cross-domain transformation dapper
623:29 - integration validation policies graphqls
623:31 - there's a lot of great policies
623:33 - that we can already utilize
623:35 - when an error occurs no other policies
623:37 - are applied except the error policies
623:39 - however other policies were in effect
623:41 - prior to the error they will not be
623:43 - removed
623:44 - product level policies apply to all api
623:47 - operations within a product so now let's
623:49 - go take a look at an example of a policy
623:52 - okay
623:53 - [Music]
623:57 - all right let's take a quick look here
623:58 - at a policy example so you're going to
624:00 - see uh this policy here
624:03 - and we're doing it right on the echo api
624:05 - so this is for an outbound policy to
624:08 - cache the response in a get operation
624:11 - i believe that all those
624:14 - policies are in the documentation you
624:16 - can easily copy and paste them in uh we
624:18 - do do a follow along for policies that
624:19 - was something i definitely covered i
624:21 - just don't remember if it was part of
624:23 - the system or
624:25 - i mean it must be because it says here
624:27 - retrieve resource cash but anyway the
624:30 - idea here is that um this policy stuff
624:32 - is in an xml language
624:34 - um not super important to learn what it
624:36 - is but the fact is is that there is a
624:38 - lot of available policies available for
624:42 - us so we can pretty much copy paste and
624:43 - figure it out from there
624:46 - if that is clear and here is kind of
624:48 - another visualization that i just want
624:50 - to show you
624:51 - relating to our policies here uh just
624:54 - because it kind of maps up my little
624:56 - graphic where we have the front end the
624:57 - inbound processing the outbound
625:00 - processing here but if you notice here
625:02 - it says policies i know it's really
625:03 - small but you can see it says base cache
625:05 - lookup rewrite url and here it says base
625:08 - and then cash store so the idea is you
625:09 - can see what policies are being
625:12 - applied there so there you go
625:15 - [Music]
625:19 - all right so what we're going to do here
625:21 - is look at all the possible
625:23 - policy groups that are provided by
625:26 - azure uh it might say policy groups i
625:27 - mean the policies within those policy
625:29 - groups so you get an idea of what kind
625:31 - of policies you can do to transform
625:33 - manipulate
625:35 - filter
625:36 - during a transit of a request from a
625:39 - response through apim and we're not
625:42 - going to look at all the code examples
625:43 - but i'll pull out a couple per
625:45 - policy group here
625:47 - we're starting with access restriction
625:48 - policy so the first is check http header
625:51 - so enforce existence or value of http
625:53 - header then we have limit call rate by
625:55 - subscriptions that prevents api usage
625:57 - spikes by limiting call rate on a per
626:00 - prescription basis that's pretty cool
626:02 - when you look at other providers like
626:04 - aws where they have api gateway there's
626:06 - like a fixed limit on there i don't
626:08 - remember there being any kind of policy
626:10 - to do that so i like how you have the
626:11 - flexibility there to choose hopefully
626:13 - it's on there by default in that base i
626:15 - don't know limit call rate by keys or
626:17 - prevent api uses spikes by limiting call
626:19 - rate on per key basis restrict caller ip
626:23 - so filter so allow deny calls from
626:25 - specific api addresses or add an address
626:28 - range
626:29 - that could prove very useful set usage
626:32 - quota by subscription so allow
626:35 - use allows you to enforce a renewable or
626:38 - lifetime call volume and or bandwidth
626:40 - quota on a per subscription basis set
626:43 - usage quota by key so allows you to
626:46 - enforce a renewable or lifetime call
626:48 - volume and or ban with quota on a per
626:50 - key basis validate jwt so enforces
626:54 - existence and validity of a jwt
626:58 - javascript web token yeah extracted from
627:01 - either a specified http header or a
627:03 - specified query parameter gwt is very
627:06 - common for authentication of
627:08 - client-side applications validate client
627:11 - certificates so enforces that a
627:13 - certificate presented by a client to an
627:15 - api management instance matches
627:17 - specified validation rules and claims
627:20 - let's take a look at that restricted
627:21 - caller ip
627:23 - very very simple we define ipfilter we
627:26 - have an action the ip address that is
627:29 - allowed in this case or a range so that
627:32 - one is again very simple these are not
627:34 - hard to figure out we have validate jwt
627:38 - um so there
627:40 - there is some stuff there and uh it is
627:42 - what it is okay um so we'll go on to
627:45 - advanced policy so here we have control
627:47 - flow so
627:48 - conditionally applies policy statements
627:50 - based on the evaluation of bool
627:51 - expressions
627:52 - forward request so forwards the request
627:54 - to the backend service
627:56 - limit concurrency so prevents enclosed
627:58 - policies from executing by more than the
628:01 - specified number of requests at a time
628:03 - log to event hub so sends messages in
628:06 - the specified format to a message target
628:08 - defined by a logger entity
628:10 - emit metrics so send custom metrics to
628:13 - application insights and execution mock
628:16 - response so aborts pipeline execution
628:19 - returns a mocked response directly to
628:22 - the caller retry
628:24 - retries execution of the enclosed policy
628:27 - statements and if
628:28 - and until the condition is met execution
628:31 - will repeat at the specified time
628:33 - intervals and up to the specified retry
628:36 - count return response so aborts pipeline
628:39 - execution and returns the specified
628:41 - response directly to the caller let's
628:44 - take a look at limit concurrency that
628:46 - seems like a good one to have
628:48 - so here we have an inbound
628:51 - then we define our back end we set our
628:52 - limit concurrency it's setting the
628:54 - forward request to a timeout of 120 so
628:57 - preventing close policies from executing
628:58 - by more than specified number of request
628:59 - time we have mock response that one
629:02 - would be very very useful uh you know
629:04 - like you just want to you know you don't
629:06 - want to have a real response you just
629:07 - want to put whatever you want in there
629:09 - so it gives it back a status 200 for
629:12 - application json
629:14 - more more advanced policies here so send
629:17 - one-way request send a request to a
629:19 - specified url without waiting for
629:20 - response
629:21 - send request send a request to specify
629:24 - url
629:25 - send http proxy so allow you to route
629:28 - forward
629:29 - forward request via http proxy set
629:32 - variable persist a value in that name
629:34 - context variable for later access so
629:37 - here's an example one where we're
629:38 - setting that variable so we're saying is
629:39 - it mobile and we want to know is it
629:41 - mobile so the idea is it'll take that
629:43 - value from that agent and then we'll
629:45 - have that variable available for us to
629:47 - determine was the request mobile
629:50 - we have wait so wait for enclose send
629:52 - requests get value from cash or control
629:54 - policies to complete before proceeding
629:57 - set request method allow you to change
630:00 - the hp method for a request set status
630:02 - code changes http http status code to
630:05 - the specified value trace adds custom
630:09 - traces into the api inspector output
630:11 - application insight telemetries and
630:14 - resource
630:15 - blogs
630:17 - looking at authentication policies and i
630:20 - don't know why that's highlighting oh
630:21 - you know i probably have a graphic above
630:22 - it that's why um so we have authenticate
630:24 - with basics so authenticate with basic
630:26 - service using basic authentication very
630:28 - common for test environments
630:30 - authenticate with client certificates so
630:32 - authenticate with a back back end
630:34 - service using client certificate
630:35 - authenticate with managed identity very
630:37 - popular use case um for
630:41 - for azure services so authenticate the
630:43 - backend service using managed identity
630:45 - so there's an example basic auth that's
630:46 - a username password you've probably seen
630:48 - it if you've ever got a site where they
630:49 - just give you like kind of like an alert
630:51 - you have to enter those two in and for
630:53 - this one you can see that it's going to
630:55 - vault
630:56 - for the managed identity
630:59 - caching policies so get from cash
631:02 - perform cash look up and return a valid
631:03 - cash response when available probably
631:05 - very popular policy to be used store to
631:08 - cash cash response according to
631:09 - specified cash control configuration get
631:12 - value from cash retrieve a cash item by
631:14 - key
631:15 - store value in cash store an item in the
631:17 - cash by key remove value from cash
631:20 - remove an item in the cash by key so
631:22 - here the examples obviously very big for
631:24 - get from cash but here it says uh
631:28 - um vary by developer cash key lookup so
631:30 - i guess you're performing you're giving
631:33 - it these things
631:35 - now sometimes they're not always very
631:36 - clear when you're reading them but
631:38 - that's fine
631:39 - uh for cash store values the store value
631:42 - you provided a key
631:44 - there is the value how the duration i'm
631:46 - assuming is how long it lists for maybe
631:47 - the ttl
631:49 - cross-domain policies
631:51 - so we allow cross-domain calls
631:53 - this is something you probably really
631:55 - really would want to enable especially
631:56 - if you're building
631:57 - applications that are not on the same
631:59 - domain so i could see people using this
632:01 - quite a bit makes the api accessible
632:03 - from adobe flash microsoft's
632:06 - silverlight browser-based clients did
632:08 - not expect that as a description of
632:10 - course i was thinking of cores that's
632:11 - why ad cross origin resource sharing
632:14 - support and operation or api allow
632:16 - cross-domain calls i feel like this
632:18 - one's going to happen a lot i didn't
632:19 - know microsoft silverlight still existed
632:21 - if people don't know silverlight is a
632:23 - competitor adobe flash open i guess
632:25 - adobe flash probably does html5 now i'm
632:27 - just thinking of old fash flash players
632:30 - jsonp adds json with padding support to
632:33 - an operation for an api to allow
632:35 - cross-domain calls from javascript
632:37 - browser-based clients
632:39 - so that is the cross-domain one very
632:41 - simple
632:42 - there's cores and if you've ever seen
632:44 - cores this looks the same if you're on
632:46 - aws if you're anywhere else it's the
632:49 - same xml it looks the same
632:51 - so it looks big but it's not as scary
632:53 - and you'll come across it quite often
632:55 - especially with apis
632:57 - transformation policies convert json to
632:59 - xml converts requests or response bodies
633:02 - from json to xml convert xml to json
633:06 - you get it find and replace a string and
633:08 - a body mask url in the content
633:11 - set backend service
633:13 - set body these are very clear set http
633:16 - header
633:17 - set query string parameter
633:19 - if you want to read these you can but
633:20 - they're very straightforward rewrite url
633:22 - that's probably a very popular one but
633:24 - maybe the example is too large so i
633:25 - don't show it here transform xml and x
633:28 - xslt let's look at uh xml json because i
633:31 - thought that one was kind of interesting
633:33 - so you say xml to json
633:35 - and i guess it just turns it into json
633:38 - http header is something very common
633:40 - that you'll be doing
633:42 - you can just set a header for whatever
633:43 - values need to pass along
633:45 - we have zapper integration policies so
633:48 - send requests to a service
633:50 - send message to pub subtopic
633:52 - trigger output binding and there's an
633:54 - example of a trigger output binding
633:56 - validation policies this last group
633:59 - validate content validate parameters
634:02 - validate headers validate status code
634:04 - validate graphql request
634:06 - so we'll take a look at parameters so
634:07 - here's an example where
634:09 - we have a parameter and it says prevent
634:12 - prevent
634:14 - um
634:15 - detect detect
634:17 - uh prevent ignore ignore ignore so i
634:20 - guess the idea is it's just saying like
634:22 - if those parameters are present allow
634:23 - them if not ignore them validate status
634:26 - code i figured this would be like a
634:27 - little bit more interesting this one but
634:29 - it's not
634:30 - status code so prevent
634:32 - unspecified status code action so do not
634:35 - allow it to have an unspecified
634:38 - code action
634:39 - i i didn't know that that's possible i
634:41 - thought you all status codes are always
634:43 - returned but uh you know i guess you're
634:45 - learning something new every day but
634:46 - those are all the policy groups and
634:48 - hopefully you get an idea of policies
634:50 - you can apply because that's what that's
634:51 - the greatest power of api is those
634:53 - policies okay
634:57 - [Music]
634:59 - all right let's take a look at different
635:01 - ways we can define apis within apim so
635:04 - the idea here is that you're ready uh to
635:08 - create an api you have a gateway you
635:09 - want to create an api
635:10 - so you hit that add new api button you
635:12 - got a bunch of options here so you can
635:14 - define one
635:16 - manually the http endpoints web sockets
635:19 - graphql
635:21 - of course that's going to vary what you
635:22 - want
635:23 - sockets is great for real time stuff
635:25 - maybe you're making a video game maybe
635:26 - you're making a chat app graphql is
635:28 - becoming a popular alternative to
635:32 - rest based applications like standard
635:35 - endpoints so graphql
635:37 - allows you to define a query to query
635:40 - data
635:41 - um you know personally it's not my
635:42 - favorite but i like the fact that it's
635:44 - all rolled up under one
635:46 - api gateway in api in azure
635:50 - the next thing is we can define based on
635:52 - a schema standard for importing defining
635:56 - apis we have open api version three
635:59 - wadl wsdl
636:02 - and the the latter two i don't know much
636:04 - about and so um following this
636:07 - uh video we're gonna go talk about these
636:10 - three standards here um because i do
636:12 - think it's important to know all those
636:13 - three because that's the real way you're
636:15 - gonna really get uh apis into here
636:17 - you're not really gonna be doing it
636:18 - manually
636:20 - and the last one is really great if you
636:22 - want to spin up an api that you know you
636:25 - want to integrate right away very common
636:27 - ones would be
636:28 - app service
636:30 - actually i guess all three of them
636:31 - really to be honest logic app or app
636:34 - service or function app and i'm pretty
636:36 - sure in this we will show you with
636:38 - function app in the follow along there
636:39 - so those are some options there just to
636:41 - get quickly started but let's take a
636:44 - deeper look at these um
636:46 - open definition standards for defining
636:48 - apis and then importing them starting
636:50 - with open api
636:52 - [Music]
636:56 - all right let's take a look here at open
636:57 - api so here is an example of one written
637:00 - in yaml i have a feeling that it can
637:02 - also be done in json but just taking a
637:05 - quick look here you can kind of get an
637:06 - idea you have paths
637:08 - that is an endpoint or uh yeah a method
637:10 - if you want the method that to be used
637:12 - would be posts here you can provide a
637:14 - summary description the contents should
637:17 - it be application json schema
637:19 - information
637:20 - and what responses should come back so
637:23 - pretty straightforward it gets a little
637:25 - more complicated than that but that is
637:26 - the best snapshot i can provide to you
637:29 - of that language so open api
637:31 - specification oas defines a standard
637:34 - language agnostic interface to restful
637:36 - apis which allows both humans and
637:38 - computers to discover and understand the
637:39 - capabilities of the service without
637:41 - access to source code documentation or
637:44 - through network traffic inspection and
637:47 - open api is really the leading one this
637:49 - is what pretty much everybody uses
637:50 - nowadays no matter what cloud service
637:52 - provider you go to they're going to have
637:54 - an option for their api gateway to
637:55 - import open api swagger and open api
637:58 - used to be the same thing but as of
638:00 - version three of open api swagger and
638:02 - open api are two different things so
638:04 - open api is a specification swagger are
638:06 - tools for implementation
638:08 - for the specification if you want to use
638:12 - i think it's like swagger it's by
638:14 - bearsmart and bearsmart i believe is the
638:16 - one that came up with open api to begin
638:18 - with
638:19 - um and so open api can be represented as
638:21 - either json or yaml as i said earlier
638:24 - but there you go
638:28 - [Music]
638:30 - let's take a look at wadl and wsdl
638:33 - they're very similar that's why we're
638:35 - giving them one slide together so web
638:37 - application description language wadl
638:39 - and then web service description
638:40 - language wsdl these are specifications
638:44 - defined by the web consort consortism
638:48 - uh was it w3 w3g w3 something w3 w3c
638:53 - probably and i believe that wadl was
638:56 - originally made by sun microsystems i
638:58 - don't know who made wsdl but it
639:00 - definitely is under the w3c and here's
639:03 - an example of wsdl
639:05 - very
639:06 - very very heavy because it's all xml
639:09 - most people like json yaml that's why
639:10 - open api is so popular but just take a
639:13 - look here at wadl and wsdl the idea is
639:16 - that you will have some things that are
639:17 - similar so
639:19 - with wadl you define an application for
639:21 - wsdl
639:22 - the equivalent is definition here we
639:24 - have grammars here we have types here we
639:26 - have resource here we have interface
639:28 - method operation
639:30 - request response input output param
639:33 - element param simple type to me wadl
639:37 - looks more
639:38 - like a uh
639:40 - like how we would describe in terms of
639:42 - terminology i cannot remember
639:44 - which one is more advanced because one
639:46 - has more capabilities than the others
639:48 - but to be honest you're not going to be
639:50 - using any of these you're going to be
639:50 - using open api but i just wanted to show
639:53 - you them and the fact that you can
639:54 - translate their xml over
639:57 - to that there in this table so hopefully
639:59 - that gives you kind of an idea there you
640:00 - go
640:06 - let's take a look at the developer
640:07 - portal so the developer portal is an
640:09 - automatically generated fully
640:10 - customizable website with the
640:11 - documentation of your apis it's where
640:13 - api consumers can discover your apis
640:16 - learn how to use them request access to
640:18 - try them out so here is an example of
640:20 - the default one that you get to be
640:23 - honest i did not know how to use this
640:26 - i tried really hard because i thought it
640:27 - was a very cool idea this is actually
640:29 - the second iteration of the developer
640:31 - portal so there was an older one
640:33 - that was different from this but this is
640:35 - the new one and i like the idea but in
640:38 - execution i'm not sure exactly what to
640:40 - do here but the idea here is you need to
640:42 - publish you'll need to publish for the
640:44 - developer portal to be publicly viewable
640:46 - you can save revisions of the portal to
640:47 - quickly roll back to previous versions
640:49 - you can apply custom domain for your
640:51 - developer portal so idea here is uh you
640:54 - know there's
640:55 - on the portal overview to see it there's
640:57 - a link there that you click and that's
640:59 - how you view it
641:00 - not available in the consumer plan
641:02 - available in all the other plans i guess
641:04 - the idea behind the developer portal is
641:07 - like if you're trying to sell a cloud
641:10 - service but it's api driven and they
641:12 - just pay access the developer portal
641:14 - or i guess it's also to read the
641:15 - documentation for people that need to
641:17 - utilize it
641:18 - but yeah in theory i like the idea
641:21 - execution i don't know
641:23 - [Music]
641:27 - all right so uh there is authentication
641:29 - for the developer portal that you can
641:31 - set there's a few different ways to do
641:32 - it you can do active directory b2c
641:35 - identity providers like google microsoft
641:37 - facebook basic authentication which is
641:39 - the default version if you're not
641:41 - familiar with basic auth the idea is you
641:42 - can enter a username and password that
641:44 - gets prompted to you
641:46 - um so not super complicated there's also
641:48 - delegated authentication which allows
641:50 - you to use your own web app sign in sign
641:52 - up process product subscription instead
641:54 - of the built-in developer portal
641:55 - built-in functionality
641:57 - and just to figure out where that is
641:59 - under the developer portal you have
642:01 - identities you could add those you can
642:03 - see there's delegation down below
642:06 - so not super complicated
642:08 - but yeah there is authentication for the
642:11 - api and then there's authentication for
642:12 - the developer portal so just make sure
642:14 - you understand the distinction of those
642:16 - two because it does get a bit confusing
642:18 - [Music]
642:22 - let's talk about caching so we have a
642:24 - built-in cache and an external cache so
642:26 - api and operations in the api management
642:28 - can be configured via response caching
642:30 - we were looking at policies we had an
642:32 - indicator as to how caching works
642:34 - response caching can significantly
642:35 - reduce latency for api callers and
642:38 - backend load for api providers so we're
642:40 - gonna just provide a caching policy to
642:42 - the outbound or other places built-in
642:45 - cache is volatile and is shared by all
642:47 - units the same region in the api
642:48 - management service so for that reason
642:50 - you have the ability to set an external
642:52 - cache via redis
642:54 - so that would be using
642:56 - azure's reddit cache service
642:58 - so using external cache allows you to
643:00 - overcome the few limitations of built-in
643:01 - cache so avoid having your cache
643:03 - periodically cleared during api
643:04 - management updates have more control
643:06 - over your cache configuration
643:08 - cache more data than your api management
643:10 - tier allows
643:11 - using cache with the consumption tier of
643:14 - api management enable caching in the api
643:17 - management for self-hosted gateways you
643:19 - simply need to provide a connection
643:20 - string to your redis cache
643:22 - so there you go
643:24 - [Music]
643:28 - all right so after a really really
643:30 - really long wait like 40 minutes and i
643:33 - was able to do like so many other follow
643:34 - alongs while waiting
643:36 - um this is ready so
643:38 - very long time 30 40 minutes we'll go to
643:40 - the resource here
643:42 - and so we have this environment set up
643:44 - and we're gonna have to uh actually have
643:46 - something to route to
643:48 - and so the idea is that we want to set
643:50 - up a container app and so
643:53 - before we can uh go ahead and do that
643:56 - we're going to need to create a
643:57 - container app so what you'll do is open
643:59 - a new tab
644:02 - all right and we're going to search for
644:03 - containers and then we have contain oops
644:05 - container apps and so what we'll do is
644:08 - go ahead and open that
644:10 - and then from there we'll go ahead and
644:11 - create ourselves a new container
644:13 - application
644:15 - container app is a little bit better
644:16 - than container instances i don't know if
644:17 - this came afterwards it just serves a
644:19 - different purpose but
644:20 - container apps is pretty nice so what
644:22 - we'll do is say uh we'll create or
644:24 - actually we don't need to create one but
644:25 - we'll use the same namespace
644:27 - so my apim
644:31 - and we will call this one um
644:35 - my container
644:37 - doesn't really matter what we call it
644:38 - for the region of
644:41 - does this have a region or is this a
644:42 - global service feels like it should be a
644:44 - global service
644:46 - i don't think we chose the region so
644:48 - central u.s i'm just going to stick to
644:50 - the same thing just in case
644:54 - just a good way when you're doing labs
644:57 - and we're gonna start with a simple
644:58 - hello world container so we're just
645:00 - gonna make it super easy for us
645:02 - don't have to do any coding to get an
645:03 - application running we're gonna go ahead
645:04 - and do create
645:08 - and so now we just need to wait for that
645:09 - to deploy it shouldn't take too long so
645:12 - i'll see you back here in a moment
645:15 - all right so after waiting i don't know
645:16 - like four or five minutes it looks like
645:18 - our environment is set up or our
645:19 - container is deployed so we clicked into
645:22 - it and there should be uh there it is
645:24 - application url so we just open that a
645:25 - new tab you should see this and that is
645:28 - what we're going to try
645:29 - to route to for
645:32 - apim
645:33 - so what we'll do is make our way back
645:35 - over to apim we'll go ahead and create a
645:37 - container app and here we can go browse
645:40 - and select our container
645:41 - so to make it super super simple for us
645:44 - super easy as you can see i'm going to
645:46 - look at full for a second to see if
645:47 - there's anything interesting there
645:48 - nothing that i care about we'll go ahead
645:50 - and hit create
645:52 - so yeah they make it really really easy
645:54 - and so once the that's configured we
645:56 - just need to observe that this api
645:59 - actually works
646:02 - so we'll give it a moment
646:11 - okay there we go so after a few minutes
646:12 - there it is so let's just test that our
646:14 - api is working now this web app is an
646:17 - html page so it's not really what we'd
646:18 - want to return but it's the easiest
646:20 - example that we can set up here uh so
646:23 - i'm going to go to the get request here
646:25 - and from there i'm going to go to test
646:28 - and we are going to see it's going to
646:30 - send a request here notice that it's
646:32 - sending the subscription key so this is
646:33 - what we normally have to pass along
646:35 - which is over here we'll do that in a
646:37 - different follow along here we'll go
646:39 - ahead and hit send
646:41 - and we'll give it a moment
646:44 - and we get some data back so it's the
646:46 - html page it's what we're looking for
646:47 - welcome to azure container app so that
646:49 - is great so we're done here we'll keep
646:51 - this environment around for the next few
646:53 - follow alongs but there you go
646:56 - [Music]
647:01 - hey it's andrew brown and we are
647:02 - continuing on with apim and uh this time
647:05 - around what we want to do is actually
647:06 - work with some subscription keys because
647:09 - here
647:10 - we are just hitting the button that's
647:11 - not how we're going to use our api right
647:13 - so um let's go over and set up a new
647:15 - product before we do that i'll go over
647:17 - to subscriptions let's just show you
647:19 - that we do have some keys here and
647:21 - notice there's one called service we
647:23 - came with a product and unlimited one
647:25 - but the service one is actually being
647:27 - used uh in our apim so when we were
647:29 - testing that was the key that it was
647:30 - using so if we go back to apis
647:35 - and we could click into our here my
647:36 - container and we go to get
647:39 - somewhere here
647:41 - if we go around click around here
647:43 - settings
647:44 - test down below
647:47 - send i think gives us some information
647:49 - so we'll know what key it's using
647:52 - so here
647:53 - we can see the key somewhere
647:57 - or it should tell us i do know that's
647:59 - using the service key
648:00 - i can't remember how i know that but i
648:02 - know it okay
648:04 - and so um the idea is that we want to
648:07 - create our own key i guess at some point
648:09 - but we'll go for product and we'll make
648:10 - a new one i'm going to call this
648:12 - developers
648:13 - actually call it junior developers
648:15 - because originally was developers but
648:17 - i'm just going to do junior developers
648:18 - just so we're not conflicting we'll hit
648:19 - publish so that's okay
648:21 - and we need to add our api so here we'll
648:23 - choose my container we'll go ahead and
648:25 - create it so we'll just say for junior
648:27 - developers even though it's going to
648:28 - really have full access
648:32 - and this is the way that you can like um
648:34 - catalog out your apis or or assign them
648:37 - to different groups so it's a good way
648:39 - to organize your stuff is to create them
648:40 - in products
648:42 - and so if we click into here you can see
648:44 - that there's apis that's assigned we can
648:46 - assign policies if we want globally for
648:49 - everything we can change we have some
648:51 - settings for access controls and we have
648:53 - um subscriptions
648:56 - so we do already have the subscription
648:58 - key we don't have to go and create a new
649:00 - one
649:00 - um but we should probably look at access
649:02 - controls because right now it's just set
649:04 - up as administrator access we're gonna
649:05 - go ahead ahead here and add another one
649:07 - called developers i believe you can add
649:09 - or create your own groups
649:11 - but it's not super complicated so
649:13 - there's no point in really talking about
649:14 - it we do want to go get the primary key
649:17 - for this so here what i want you to do
649:19 - is click into here
649:20 - and oops actually that's not what i want
649:23 - there was nothing to change i don't know
649:25 - why i hit save there that was pointless
649:26 - but what we'll do is go back to junior
649:28 - developers here back in the products
649:30 - here we'll do the subscription side here
649:32 - because we need to see
649:33 - the key which is here junior developers
649:35 - so if we go here and say show keys now i
649:38 - can get the primary key so we need that
649:39 - key because we actually want to make an
649:41 - api request and this time we're going to
649:43 - use powershell for fun so i'm opening up
649:45 - cloud shell up here in the top left
649:47 - corner make sure it's set to powershell
649:50 - it always doesn't hurt to learn some
649:51 - powershell
649:54 - we could do it in bash but let's do
649:56 - powershell it's a bit easier to do that
649:57 - and we'll give it a moment to spin up if
650:00 - you're doing this for the first time it
650:01 - might ask you to make a storage account
650:03 - so you might have to press some buttons
650:04 - here so just press whatever buttons it
650:06 - wants to get to this part okay
650:09 - and it doesn't normally take that long
650:11 - to spin up there we go
650:13 - so the first thing we're going to set is
650:14 - a url i'm just waiting for it to give me
650:16 - some
650:17 - more information yep still going
650:21 - there we go so we're going to set a new
650:22 - variable whoops we'll type clear first
650:25 - we'll type a new variable
650:27 - and this needs to be well first we'll do
650:29 - a subscription key because we already
650:31 - have it in our clipboard subscription
650:32 - key double quotations paste that in
650:35 - it didn't actually grab it
650:37 - so that's fine i guess we'll go grab
650:39 - instead
650:42 - from apim the
650:44 - gateway url so we'll get the gateway url
650:46 - that's the uh url that we need to get to
650:49 - notices of the overview blade and their
650:51 - apim app in order for us to uh
650:55 - that's like the api endpoint you'd hit
650:57 - right so we'll type in url
650:59 - and we'll paste that in
651:01 - it really doesn't like the right uh that
651:03 - so we'll have to right-click paste
651:04 - probably was working before for the
651:05 - subscription url we just probably had to
651:07 - right click
651:09 - so we'll do subscription url equals
651:12 - double quotations and we will grab this
651:14 - key here again so i'm just going to
651:15 - scroll on down here grab the primary key
651:18 - you can use the secondary key but you
651:19 - really
651:20 - really don't need it so let's just paste
651:23 - that in here
651:24 - so there we go that looks like a key so
651:26 - we'll hit enter
651:28 - and now we need to set some headers
651:32 - so i'm going to do at sign
651:36 - uh
651:37 - curly's
651:39 - o
651:40 - o p a p i m
651:44 - subscription key
651:47 - equals dollar sign subscription key
651:50 - you're wondering what is all this stuff
651:51 - here well um in powershell i believe
651:53 - this is called a dictionary another
651:55 - language they might call it a hash you
651:57 - might think of a json object but it's
651:58 - just a data structure that we need to
652:01 - utilize in order to pass this along i'm
652:02 - just double checking to make sure i
652:04 - named it all correctly we'll hit enter
652:06 - and now we'll use the powershell command
652:09 - called invoke
652:11 - web requests i'm hitting tab to
652:12 - autocomplete it's really nice that it
652:14 - does that it'll do that even for the
652:16 - flag so i'll do url here we'll do method
652:19 - i think we don't have to specify the
652:20 - method because it'll already be get but
652:22 - we'll do it anyway to be verbose we'll
652:24 - do headers here we'll do headers and
652:26 - we'll hit enter and we have access and i
652:28 - do the missing subscription key
652:30 - so
652:32 - here we have a small little problem
652:36 - uh so i just got to double check to see
652:39 - what the mistake is you know what
652:41 - i was staring at this thing like there's
652:42 - something wrong here it's ocp
652:46 - what ocp stands for i have no idea
652:48 - well we'll hit enter and we'll try again
652:50 - and maybe we'll just print out the
652:52 - headers to make sure it's correct
652:53 - there's no value in there so
652:55 - we'll print out subscription key to make
652:57 - sure that is correct
653:00 - oh because i wrote subscription url
653:04 - this is what happens when you don't type
653:07 - okay so not a big deal
653:11 - just learning a bit more as we go
653:14 - and i just want to set that again and
653:16 - then i'm going to print it out to make
653:17 - sure it's correct there it is
653:20 - and we'll invoke this and we should get
653:22 - back
653:24 - html
653:26 - with 200.
653:28 - so it says welcome to azure container
653:30 - app so that's how you can do requests of
653:31 - course that's with powershell
653:33 - traditionally you might use postman or
653:34 - you use your api like an api to do that
653:37 - uh but there you go so yeah that is how
653:40 - uh we work with a key we generate our
653:41 - own subscription key and work work with
653:43 - it to make a request
653:45 - not in the ui
653:46 - okay so i'll see you in the next one
653:50 - [Music]
653:54 - hey this is andrew brown and we are
653:56 - still working with apim and this time we
653:58 - are going to
653:59 - create a policy because policies are
654:02 - some of the most powerful things that
654:03 - api allows you to do so what i want you
654:05 - to do is on the left hand side in our
654:08 - our gateway that we created we'll go to
654:10 - apis we'll go over to my container
654:13 - and what we'll do is we'll just click
654:15 - onto any of them like get container
654:17 - actually i want to make a new one i'm
654:18 - going to call a new one called mock
654:20 - because we're going to do a mock request
654:23 - so do forward slash mock
654:25 - and if we go there and go ahead and
654:26 - create this
654:30 - so if we were to use this right now it
654:32 - shouldn't do anything if we test this
654:34 - what would we get back
654:37 - we got a 404 not found because there's
654:38 - no page there so what we'll do
654:41 - is we'll go here
654:43 - and we are going to go back to the
654:44 - design and we're going to add our own
654:46 - outbound process process so it doesn't
654:48 - matter what comes in we're going to
654:50 - always have a mock response come back
654:51 - we're going to do that by adding a
654:52 - policy so go add a policy you can see
654:54 - there's some basic ones like setting
654:55 - headers and things like that we're going
654:57 - to go to other policies i'm going our
654:59 - policy structure back and this is for
655:00 - the outbound i don't know why we click
655:02 - through that to get to this
655:03 - but the idea is that we're going to add
655:05 - a new policy here so what i'm going to
655:06 - do is type in mock response apim policy
655:12 - and i'm just looking for all the let or
655:15 - all the ones here so here's a bunch of
655:16 - policies and we're just looking for the
655:18 - mach one
655:21 - it's not a very uh um verbose or sorry
655:24 - like very flexible one just basically
655:26 - set the uh code but it does work so what
655:29 - we'll do is just paste that on in there
655:35 - and we'll say we want this to be a 200
655:37 - and we can say whatever we want to get
655:39 - back so like if we want a different type
655:40 - i type like um
655:43 - i don't know uh
655:45 - csv file content type
655:50 - it'd be text csv it doesn't matter we
655:51 - can do whatever we want application ms
655:53 - word right like
655:55 - i'm not sure what would happen but let's
655:56 - do it for fun
655:59 - we could have just done javascript but
656:00 - no let's make it complicated
656:02 - and so now
656:05 - if we were to go to this mock request
656:09 - we're going to test it
656:11 - and we'd send it let's see we get back
656:14 - it says 200
656:16 - mock and
656:18 - i mean it doesn't complain about the
656:19 - content type but let's just do this up
656:22 - in cloud shell just to make sure that we
656:24 - know what we're doing
656:25 - so we were doing that before so we're
656:26 - going to do it again
656:30 - okay
656:32 - it's good practice so we'll just test
656:33 - our new endpoint make sure it works this
656:35 - drag is sometimes weird so i clicked it
656:37 - and then it's just like it's still
656:38 - dragging it's very frustrating sometimes
656:39 - you have to
656:41 - freaking open it up like that so silly
656:44 - but um yeah what we'll do is we'll just
656:47 - do a bit of uh powershell again and uh
656:49 - get this working so i'm just double
656:51 - checking on how to do that so the first
656:53 - thing is we need the url what if we hit
656:55 - up oh we still have our stuff from
656:56 - before so yeah you want to set a url for
656:59 - the gateway you want to set
657:02 - a subscription key from before to then
657:04 - the last follow along and then we need
657:07 - the headers
657:08 - but it's slightly different here because
657:10 - the url needs to be forward slash mock
657:14 - and then what we can do is invoke a
657:16 - request we get back at 200
657:18 - so no issues there
657:20 - so
657:21 - there you go
657:22 - uh that's all it takes to set a policy
657:24 - there's obviously more complex policies
657:26 - there but this is a simple example
657:27 - because you really got to go digging
657:29 - through these here and it's not really
657:30 - worth it to do that unless you actually
657:32 - do it for real
657:34 - but there you go so what we'll do is
657:35 - clean this up we're all done
657:38 - with apim
657:40 - look for that there
657:42 - delete this
657:47 - and
657:48 - there you go so that is apim
657:52 - and i'll see you in the next one
657:54 - [Music]
657:59 - something throughout this course that
658:00 - keeps getting mentioned at least for the
658:02 - az204 is distributed application runtime
658:05 - or also known as dapper
658:07 - and uh you know it's not going to show
658:09 - up on any of the exams but the idea is
658:11 - that uh it's good to know what it is it
658:13 - is a microsoft thing that they had built
658:16 - its
658:17 - distributed application runtime short
658:18 - for dapper provides an api that
658:20 - simplifies microservice connectivity so
658:23 - dapper is a portable event driven
658:25 - runtime that makes it easy for any
658:27 - developer to build resilient stateless
658:30 - and stateful applications that run on
658:32 - the cloud and edge and embrace the
658:33 - diversity of the language and developer
658:35 - frameworks and it's interesting they say
658:38 - uh diversity of languages because it
658:39 - doesn't support my favorite language
658:40 - which is ruby yet it supports php kind
658:44 - of explain that one to me but dapper
658:46 - provides basically a bunch of
658:47 - functionality so the idea is that you
658:49 - are going to write your application code
658:53 - up here like whatever language you want
658:55 - and then the idea is that you can
658:56 - communicate via a grpc or http
659:00 - http ai or api to a bunch of standard
659:05 - [Music]
659:06 - services so like services service
659:08 - invocation state management pub sub
659:10 - resource binding triggers actors
659:12 - observability secrets configuration and
659:14 - allows you to connect to edge
659:15 - infrastructure now
659:17 - basically these are just basically
659:19 - wrappers
659:21 - so to speak for um uh you know
659:23 - standard functionality that you'd want
659:25 - to have alongside a microservice
659:27 - application
659:29 - dapper is not taking off as you would
659:31 - think it is um i don't know anybody
659:34 - that's really like that interested in
659:36 - adopting it because it is
659:39 - like
659:39 - this thing is more in the cloud native
659:41 - space like like kubernetes cncf
659:44 - and the thing is is that unless there's
659:46 - buy-in with that community and that this
659:48 - is truly agnostic um you know
659:51 - you're gonna have a hard time to see
659:52 - adoption unless it meets those things it
659:54 - just does not it's very interesting i
659:56 - think it'd be really cool to build an
659:57 - application on top of dapper for your
659:59 - micro service applications but i
660:00 - strongly doubt anyone would be using
660:02 - this because the future of this project
660:05 - is debatable but it is mentioned
660:06 - throughout this course so i just wanted
660:08 - to give you a little bit more attention
660:09 - there we're definitely not going to dig
660:11 - any code with dapper but there you go
660:13 - [Music]
660:17 - hey this is andrew brown from exam pro
660:19 - and we are taking a look at azure event
660:21 - grid which is a service that allows you
660:23 - to manage event routing from any source
660:25 - to any destination so event grid
660:27 - provides a simple and strong
660:29 - customizable event delivery process that
660:31 - allows you to manage at a minimum level
660:34 - which type of events will be received as
660:35 - well as which subscribers will receive
660:38 - those events so here is an example
660:40 - graphic uh you can see event grid is
660:42 - kind of sitting in the middle there
660:43 - where um events are being captured and
660:46 - being uh managed by event hub and it's
660:48 - routing to other places
660:51 - so event grid is ideal for event driven
660:53 - architectures where you can subscribe to
660:55 - azure resource events and publish them
660:57 - to an event handler or web hook you can
661:00 - also use custom topics to create custom
661:02 - events that will be published in your
661:04 - event grid
661:06 - and it supports event fan out with
661:08 - 24-hour retry
661:10 - reliability to ensure that events are
661:13 - delivered it is a low-cost serverless
661:15 - product that supports dynamic
661:17 - scalability so there you go
661:19 - [Music]
661:23 - let's talk about event source and
661:25 - handlers so azure event grid is divided
661:27 - into two categories events sources of
661:30 - services that emit data and event
661:32 - handlers services that receive data and
661:34 - in between them that is where event grid
661:36 - is so let's talk about um what there is
661:39 - that can uh emit sources and this will
661:42 - just give you an idea of how
661:44 - much event grid can integrate with azure
661:46 - services so we have blob storage
661:48 - resource groups subscriptions event hub
661:51 - media service iot hub service bus azure
661:54 - maps azure container registry signal r
661:57 - azure app configuration azure machine
662:00 - learning azure communication services
662:03 - azure cash for redis
662:05 - cloud events azure policy
662:08 - and custom events so basically anything
662:10 - you want to get in there and i'm sure
662:11 - there's more data support for other
662:12 - services but as you can see we're
662:14 - limited for space
662:15 - but it can
662:16 - receive messages from a lot of places
662:19 - for event handlers the services that
662:20 - receive data it could be for serverless
662:22 - code so think azure app functions things
662:25 - like that
662:26 - we have workflow and integration so
662:29 - think service bus
662:30 - logic apps buffering and
662:33 - competing consumers so event hub storage
662:36 - queue other services and applications so
662:38 - i think hybrid connections like web
662:40 - sockets or web hooks or even automation
662:44 - so yeah there you go
662:46 - [Music]
662:50 - all right let's take a look at the key
662:52 - concepts for azure event grid so here we
662:54 - have this fancy diagram at the top here
662:56 - we have domains so these are used to
662:57 - group event grid topics that are related
662:59 - to the same application for easier
663:01 - management then you have topics these
663:02 - are the end points where events are
663:04 - going to be sent to there's different
663:06 - types of topics we have system topics
663:08 - these are built-in topics provided by
663:09 - azure services this is the most common
663:11 - kind you're going to be using because
663:13 - they're super easy to use we have custom
663:15 - topics for applications and third-party
663:17 - topics and for those third-party topics
663:20 - they emit their own partner events
663:22 - uh from third-party sasses to publish
663:25 - events so they're similar to system
663:27 - topics it's just that they're with third
663:29 - parties um but uh yeah there's some
663:32 - variation there we have events these are
663:34 - the actual event data that occurs within
663:36 - the service not visualized here but you
663:38 - get the idea uh publishers this is the
663:40 - service that published the event so you
663:42 - cannot see them on this diagram but the
663:44 - idea is imagine you have a publisher
663:46 - here and they are uh they are the ones
663:48 - where the event sources is coming from
663:50 - okay we have event sources this is where
663:52 - the event took place uh event
663:54 - subscriptions these are the mechanism
663:58 - that the routes uh that route the events
664:01 - sorry about that there so over here you
664:03 - can see subscriptions
664:04 - uh subscription expiration so this is
664:06 - where you set an expiration for the
664:08 - event subscription you have event
664:10 - handlers which is over here um you could
664:12 - say consumers if you'd like
664:15 - is the app or service that receives the
664:16 - events you have event delivery this is
664:18 - the delivery of events in batches or in
664:21 - single events so it just depends on how
664:23 - you want to
664:24 - send those messages you have batching so
664:27 - this is uh sending a group of events in
664:30 - a single request and there you go
664:33 - [Music]
664:37 - hey this is andrew brown from exam pro
664:40 - and we're going to take a look at event
664:41 - grid basics so let's get to it so the
664:44 - first thing we're going to want to do is
664:45 - we're going to search up subscriptions
664:48 - because we're going to need to make sure
664:49 - that
664:50 - like in order for us to use event grid
664:52 - that we have it turned on
664:54 - and so we'll go into our subscription
664:56 - and we're going to go under
664:59 - resource providers
665:02 - providers
665:04 - ah there it is
665:05 - and this is all the stuff that is
665:06 - registered all the providers that are
665:08 - registered so what we're doing is just
665:09 - make making sure that event grid is
665:10 - turned on because that one's not always
665:12 - turned on by default and just make sure
665:14 - that it's registered okay so you'll know
665:16 - that it's registered because it'll have
665:17 - a green
665:19 - check mark here and it'll say registered
665:20 - and so once that is done we can proceed
665:22 - to create
665:24 - a storage account because we're going to
665:27 - integrate a storage account
665:28 - into our event grid so what we'll do is
665:31 - create a new storage account
665:33 - and we're going to create a new resource
665:35 - group i'm going to call this resource
665:37 - group event grid basics
665:41 - and we're going to name our storage
665:43 - account event
665:45 - event
665:46 - grid basics
665:50 - all right and yes you can have hyphens
665:52 - you can't have anything else
665:55 - and we'll just make sure we spelt that
665:56 - right event
665:59 - event grid basics i can't remember if
666:01 - these are fully qualified domains if
666:03 - they are you might have to add some
666:04 - numbers on the end there but it is what
666:06 - it is we're doing uh usc i mean it just
666:09 - randomizes every time but this is where
666:11 - you should probably set it to
666:12 - we have standard premium we'll leave it
666:14 - as standard and everything all seems
666:15 - fine so we'll go ahead and create review
666:19 - and create and then it's going to allow
666:21 - us to review we'll go ahead and hit
666:23 - create
666:24 - and it's going to create the resource
666:26 - group here we'll just wait for it to
666:27 - finish uh deploying and then we'll go
666:29 - into the resource
666:31 - all right looks like it is finished
666:33 - deploying so we're going to go ahead and
666:34 - go to that resource we're going to go to
666:35 - containers and we're going to create a
666:36 - couple containers the first is the first
666:38 - one is going to be called basic
666:40 - we're going to leave it as private just
666:42 - make sure it's basic not basics
666:44 - and we'll create another container
666:45 - called basic alt the idea is that we're
666:47 - going to use an event grid in order to
666:49 - move one file from one storage account
666:51 - to another
666:52 - and that's going to be facilitated by
666:54 - log via a logic app because that's going
666:56 - to be the easiest way to use event grid
666:58 - so what i want you to do is search for
667:01 - logic apps
667:03 - up here
667:04 - and we're going to go ahead and add a
667:06 - new logic app
667:09 - and what we want to do
667:11 - is choose our resource group and we'll
667:12 - just go event grid basics here
667:15 - we are going to name this event grid
667:18 - basics
667:21 - um maybe we'll do lg to indicate that it
667:24 - is a logic app or lg maybe uh l yeah lg
667:28 - is fine like short for logic
667:30 - and from here we have a workflow or a
667:32 - docker container we're going to stick
667:34 - with a workflow
667:38 - just to knock on wood we're going to
667:39 - just put in the same region as our
667:42 - storage account so
667:43 - east us we have standard consumption i'd
667:45 - rather do consumption for this
667:48 - so you pay only as much as you use
667:50 - because we don't need enterprise level
667:51 - serverless application here we just need
667:53 - a consumption model we're going to leave
667:55 - this to disable so that is totally fine
667:58 - there's nothing else to do here so we'll
667:59 - go review plus create
668:03 - and we'll go ahead and create this logic
668:06 - app and we just have to wait for this to
668:08 - finish deploying
668:10 - all right so that should have been very
668:12 - very quick like under uh 10 seconds
668:14 - there so we've gone into the resource so
668:16 - just click go to the resource and so we
668:18 - have this
668:19 - very fun interface and so what we need
668:21 - to do is start with a common trigger
668:23 - there's a few different ways to get to
668:24 - it but there should be something
668:26 - on the front here
668:30 - i don't know if they redesigned this
668:31 - recently
668:34 - so i'm just gonna search start with a
668:36 - commentary oh yeah it's up here okay i'm
668:38 - being silly um and so
668:40 - what we want to do is uh because this is
668:43 - an event grid uh follow along we want to
668:45 - click on when an event grid occurs
668:49 - and so this is the designer where we can
668:51 - make things a lot easier for ourselves
668:52 - and so we're going to have to first sign
668:54 - in to authenticate so i'm this is my
668:56 - tenant example training inc so we'll go
668:58 - ahead and get connected there
669:01 - let's give it a moment
669:03 - we'll select andrew brown which is
669:04 - totally fine
669:06 - and now that is connected so that is
669:08 - great so once we are signed in we can
669:10 - click continue and we're going to go
669:12 - ahead and
669:14 - select our subscription here
669:17 - and we need to choose a
669:19 - resource here so
669:21 - i guess in this case it's going to be
669:23 - event grid
669:26 - event grid
669:29 - or
669:31 - hmm
669:35 - i could have swore yeah yeah i think
669:37 - that's what we want to do let me just
669:39 - double check here oh you know what it's
669:40 - just not for some reason i'm i'm
669:42 - searching it's not auto completing
669:44 - properly okay i just wasn't sure there
669:46 - and as far as i can remember this would
669:49 - be probably an event grid topic
669:53 - and then we need to give this a resource
669:57 - name so
670:00 - um
670:02 - let me just think about this for a
670:03 - moment okay
670:04 - all right so i think i understand where
670:06 - my confusion was it was because we click
670:07 - continue and i never i didn't see event
670:09 - quit anymore so i thought we had to
670:11 - configure it when it was already
670:12 - configured right so this is where we
670:14 - were so we're not we at this stage like
670:16 - event grid is already hooked up so it's
670:18 - ready to be triggered so this is the
670:19 - step that follows into it which is where
670:22 - we want to do our storage account so
670:24 - that's where i was getting
670:26 - confused so we'll choose our
670:28 - subscription here it's okay you know if
670:30 - you never get confused just step step
670:32 - back a couple steps and just double
670:33 - check what you're doing happens to me
670:36 - all the time so um what we want to do is
670:40 - actually
670:41 - connect storage uh storage accounts so
670:43 - we type in storage accounts here
670:46 - great uh we'll have that selected and
670:48 - then we need to select our storage
670:49 - account so this one's called event grid
670:51 - basics
670:53 - and then we're gonna have to enter in
670:55 - um some additional information event
670:57 - type so we want to have it happen when
670:59 - we add something to
671:01 - the container so the basic container so
671:03 - we'll do blob created
671:06 - and then from there we need to actually
671:07 - filter out the information so we need to
671:09 - add a new parameter and i think we'll
671:11 - have to do it on the prefix filter so a
671:13 - filter like whatever yeah so that's
671:15 - probably a good idea
671:17 - because then we could
671:18 - place it into a particular place and i
671:20 - believe that
671:22 - there are very specific
671:26 - filters that you can do
671:28 - for this because
671:30 - if i recall
671:33 - there's like standardized ones yeah see
671:34 - here like it's always going to be
671:35 - forward slash blob services default
671:37 - containers etc and you'll know that
671:39 - because you know if you read the
671:41 - documentation and you have to do that
671:42 - stuff you'll figure that out so i'm just
671:44 - going to type it by hand here blob
671:46 - services
671:47 - default containers and then we can put
671:49 - our container name so basic
671:51 - uh and i believe we have it without the
671:54 - s there
671:56 - so
671:57 - uh services see i don't trust my writing
671:59 - here so i'm just gonna copy paste it in
672:02 - okay
672:03 - and that looks good to me
672:07 - so i think that is what it needs to be
672:09 - so we'll go ahead and hit the next step
672:12 - so the idea is anything in that folder
672:15 - like when something's added to that
672:17 - folder then follow up with this
672:18 - operation right
672:23 - um and maybe before we do that we should
672:26 - probably um
672:27 - you know observe
672:29 - that this stuff works that's probably a
672:31 - good idea so what i'm going to do here
672:33 - is i'm going to make my way back over to
672:34 - our storage account so just close this
672:36 - tab here i'm going to open a new tab
672:40 - and we're going to make our way over to
672:41 - storage accounts
672:43 - and we'll go to event grid basics
672:46 - and we will go to containers
672:49 - and we'll click into our basic container
672:51 - because i want to just see that this is
672:52 - working
672:55 - and i'm going to need a file to upload
672:56 - so let me just go grab an image really
672:58 - quick
672:59 - all right so i just grabbed an image off
673:00 - the internet so i just have data here
673:02 - but before we upload we probably should
673:04 - save what we have because if we don't
673:06 - save it we're not going to be able to
673:07 - observe it so i went back to logic app
673:08 - and we just hit save in the top left
673:10 - corner
673:11 - so we'll give it a moment to save and it
673:14 - looks like it's saved now i'm just going
673:16 - to go back over to here
673:18 - if we can
673:20 - look at some of the code that gets
673:22 - executed i'm just trying to remember
673:25 - where it is because once it executes we
673:27 - want to um
673:28 - see what happened right so what i'll do
673:30 - is i'm going to go all the way back over
673:32 - here and i'm just going to go and drag
673:34 - or actually i'll hit the upload button
673:36 - so i don't trust that that there and
673:37 - i'll drag it onto here nope i still
673:39 - don't trust it so what i'm going to do
673:41 - is just click the files and i'm just
673:43 - going to grab it this way and say open
673:45 - and we'll do upload
673:48 - and so that is now uploaded and so
673:51 - there is somewhere where we can observe
673:54 - um where stuff has happened so i'm just
673:57 - trying to remember where it is
674:00 - um
674:01 - [Music]
674:04 - i mean we could run the trigger
674:07 - yeah we probably should run the trigger
674:08 - right
674:11 - run
674:23 - i think it's running so we'll just give
674:24 - it a moment okay
674:27 - you know i was thinking about it it
674:28 - doesn't make sense we shouldn't have to
674:29 - run it because it should just happen
674:31 - automatically i think it's on the
674:32 - overview page
674:34 - ah okay
674:35 - so if we look here
674:38 - we can see the run history and so and
674:40 - there's also trigger history of when the
674:41 - things are triggered so we could we
674:43 - could manually fire it but doesn't make
674:44 - sense so i think this is the run that we
674:46 - just did if we click into here yes this
674:48 - is what it is so here we can see what
674:50 - what has happened so if we expand it
674:52 - we can see the inputs right so it's we
674:54 - have a blob created
674:56 - um it might show some information so
674:59 - here we can see data yep and it's a webp
675:02 - file and so it's gotten this far through
675:04 - and so that's a great way to kind of
675:05 - like debug so you can
675:07 - uh logic run app you can do it each step
675:09 - but right now we are using um event grid
675:11 - to do that integration right
675:14 - we're just doing it through logic logic
675:15 - app because it's a lot easier
675:17 - so
675:18 - now that we have that what we should do
675:20 - is go back to our designer and we're
675:23 - going to have to add the follow-up step
675:26 - um so we have this
675:29 - oh yeah okay so that's the first step
675:30 - event grid doesn't show up there which
675:32 - is weird but um so we have this step
675:34 - here from our storage account and so the
675:36 - next step what we want to do is put it
675:38 - into
675:40 - another container so that will be the
675:43 - tricky part
675:44 - um so i'm just trying to remember what
675:47 - we do so we'll hit next step
675:50 - and
675:53 - um
675:56 - i think what we need to do is initialize
675:58 - a variable first because we're going to
675:59 - have to get some way to grab
676:01 - the name of the string because if we go
676:04 - back to our run
676:05 - over here
676:08 - just give it a moment here
676:10 - and we go into a run again here
676:15 - we need to extract some data to pass
676:17 - along because there are some limitations
676:19 - in terms of how json gets passed along
676:21 - or data gets passed along and so
676:23 - what we want is we just want this part
676:25 - of the name we want to say take this
676:28 - name as the identifier so that when
676:30 - we're copying stuff over it will work
676:32 - and so what we'll have to do is store
676:34 - that into an intermediate variable
676:36 - so
676:37 - we'll just type in variables here
676:47 - and
676:48 - i'm just seeing
676:50 - oh yeah so they look like this because i
676:52 - can remember they might be in the
676:52 - built-in yeah that looks a lot better
676:55 - and so we need a variable and it's
676:58 - initialize variable
677:00 - and we're going to name this file name
677:04 - and this is a string of course
677:06 - and now we need to insert the value
677:08 - so in here what we need to do is write
677:11 - an expression in order to extract that
677:12 - information out
677:14 - um so what we'll do is go to the
677:16 - expression tab and over here you can see
677:18 - we have all sorts of expressions that we
677:20 - can use so i'm going to type in last
677:22 - parentheses and then in there we'll do
677:25 - split
677:26 - parentheses
677:27 - and then what we're looking for is
677:29 - trigger body
677:32 - and then we'll do question mark
677:36 - square braces
677:38 - single quotations subject how did i know
677:41 - how to do that
677:43 - i
677:44 - looked it up i looked it up somewhere
677:46 - and you know i just don't feel like
677:48 - there's much reason to to teach this
677:50 - part because
677:51 - you mean if you really need to know you
677:52 - can go here um and learn all about it
677:55 - but a lot of times like if you need
677:57 - something you can just say i need this
677:59 - kind of function somebody's already done
678:00 - it right because there's so many common
678:02 - use cases so i probably search something
678:04 - like how do i get the name out of the
678:06 - the thing you know like for the blob and
678:09 - somebody had that there but it makes
678:10 - sense to me so let's hit okay here and
678:13 - it should turn purple because it is dime
678:15 - expression if you type it in here
678:16 - probably won't work correctly you have
678:18 - to type in here and then hit okay so it
678:20 - shows up like that uh but you notice we
678:23 - typed in like trigger body so if we go
678:24 - back over to our run here
678:27 - um
678:28 - this is the body here so when they say
678:29 - trigger body they're talking about here
678:31 - and then it was just grabbing that
678:33 - subject line there
678:34 - all right
678:36 - so that would be the second step and
678:38 - that gets it into a variable but the
678:40 - next part is we need to actually um
678:44 - get the blob content and then insert it
678:46 - and then create a new blob so we'll do
678:48 - is hit next step and we'll type in blob
678:50 - and see if we can find anything here
678:58 - and from here we need to get the get
679:01 - blob content using path version 2.
679:05 - so i'm just going to scroll down here
679:06 - and take a look for it there it is
679:08 - based on the path
679:11 - and we'll go down here and um
679:16 - i guess it would be access key oh
679:18 - because we're setting up a connection
679:19 - for the first time
679:21 - so
679:22 - [Music]
679:24 - enter name for connection
679:28 - um
679:30 - i know what the storage account is
679:33 - but what is the connection uh connection
679:34 - name i do not remember give me two
679:36 - seconds okay there wasn't much to help
679:38 - me here because what i remember before
679:39 - was that you click it and you'd
679:40 - authenticate it like the event grid but
679:42 - it's not doing that so maybe we just
679:43 - have to name it something
679:45 - so i don't know we'll just say azure
679:47 - storage account maybe it just wants a
679:48 - name
679:50 - maybe it doesn't really matter
679:53 - oh yeah like there's sign in
679:55 - that's what i want
679:57 - so connection name yeah so we'll say uh
679:59 - you know
680:00 - storage account event grid
680:03 - okay
680:05 - because if we can just single sign on
680:07 - let's do that that's super easy
680:12 - and we'll click that there
680:15 - okay so this is starting to look how i
680:17 - expect it to look
680:18 - and so we need the storage account name
680:21 - i don't know why it's not showing me any
680:23 - names here
680:25 - but that's okay we'll just go over back
680:27 - to our storage account here and it's
680:29 - called event grid basics
680:31 - so we'll type in event grid basics
680:37 - event
680:39 - grid basics
680:43 - uh
680:45 - that's custom value sure
680:47 - i mean that's what its name is i'm not
680:49 - sure why it's not auto completing
680:52 - but here what we need to do is we need
680:54 - to provide the path so it's going to be
680:56 - forward slash basic
680:58 - ah and so now there's our environment
681:00 - very or that variable so we'll just
681:01 - click that there
681:03 - so that will make it super super easy
681:07 - now notice that it is showing basic now
681:09 - so i just clicked here the folder we
681:10 - typed it in manually but we could have
681:12 - clicked uh here and then put the
681:13 - environment variable in or the uh this
681:16 - initialized variable in here but i did
681:18 - type that manually and it still did work
681:20 - correctly so we are okay here infer the
681:22 - content type sure why not
681:25 - um it doesn't matter if they do
681:28 - so this gets the content so now this
681:30 - gets the path and so the next thing is
681:32 - actually to create the blob
681:34 - you can't like do an easy clone you have
681:36 - to do at this um intermediate step
681:39 - that's just how it works
681:41 - um and so what we'll need to do is go to
681:42 - our built-in ones here we'll type in
681:44 - blob again
681:46 - maybe standard
681:51 - and this time we want to create a blob
681:55 - so there it is
681:57 - uh block bob no we just want a blob
682:00 - and so what we'll do is
682:03 - i guess we have to connect again i'm
682:04 - surprised it's not showing the name yeah
682:06 - it's just the name that's fine so we'll
682:08 - go back over here
682:11 - i just don't want to type it wrong so
682:12 - we'll just copy paste it in
682:14 - event grid basics
682:17 - grid basics
682:18 - enter custom value because it's giving
682:20 - us so much trouble for no particular
682:21 - reason
682:22 - make sure there's no space on the end
682:23 - there there now works fine
682:25 - um in this case what we want is basic
682:28 - alt
682:31 - and the blob name can be the file name
682:33 - which is totally fine
682:35 - and the blob content will be the file
682:37 - content
682:39 - and i don't think we need anything else
682:42 - so what we'll do is go ahead and click
682:45 - off and we will save
682:49 - alright so that's just the way we're
682:51 - going to have to do it
682:54 - so what we can do is go back to our
682:56 - overview
682:58 - and
682:59 - we'll go back to our basic folder
683:02 - and we'll delete data
683:04 - say ok
683:07 - and we'll go upload
683:09 - we'll select our file again
683:11 - we'll grab it
683:13 - we will upload and then we will make our
683:15 - way back over to
683:16 - our logic app
683:21 - close this tab here so we don't get too
683:22 - mixed up
683:24 - refresh the page
683:26 - and it failed so it failed for some
683:27 - reason so something has not been
683:28 - configured correctly
683:31 - it failed on the initialize variable so
683:34 - something's wrong there so unable to
683:35 - process the template language expression
683:36 - in the actions initialize variable uh
683:38 - inputs at line zero column zero
683:40 - templating function split is not defined
683:42 - as not valid so it's possible i just
683:44 - spelt it wrong
683:45 - so what we'll do is go back to our event
683:47 - grid we'll go back to our logic app
683:49 - designer here
683:51 - initialize variable we will click it
683:54 - and we probably just built it wrong
683:59 - spilt
684:03 - spilt what if we do lit spilt
684:06 - split
684:08 - so if that's wrong we'll just scroll on
684:10 - down and we'll just take a look
684:15 - i could have swore that it
684:16 - auto-completed for us
684:22 - oh you know what it is spelled wrong it
684:23 - should be s-p-i-l-i-t
684:26 - split
684:28 - all right
684:29 - and i'm just double-checking to see if
684:31 - there's any other problems here
684:33 - nope looks fine to me so go ahead and
684:36 - say update we will save it in the top
684:37 - left corner
684:38 - we'll go back we'll delete our file here
684:40 - we'll say okay
684:43 - and we'll have to select a new file
684:45 - we'll click open let's double check make
684:47 - sure that's been saved it looks like
684:48 - it's been saved we'll hit upload we'll
684:50 - go back to our overview page
684:52 - it's already running super fast by the
684:54 - way
684:55 - and we'll click into it and we'll see if
684:58 - we get any other failure so there's
684:59 - another failure that's totally okay
685:01 - so we'll just expand it this request is
685:04 - not authorized to perform this operation
685:05 - using the permissions
685:08 - so it does not like the permissions i
685:10 - gave it
685:11 - totally fine
685:13 - so we will go back to our app designer
685:16 - we will go to this second step here even
685:19 - though it did select this properly
685:22 - so we'll change the connection
685:26 - i guess we'll add a new one
685:31 - so we did 80 integrated oh let's do
685:33 - managed identity you must enable managed
685:35 - identities in the logic app to use
685:36 - managed dna's authentication you must
685:38 - grant
685:39 - required access to the identity in the
685:42 - target resource
685:45 - okay
685:47 - there is an identity tab so we can go
685:49 - over there and take a look there quickly
685:51 - i don't remember it being that hard to
685:53 - do
685:54 - a system assigned managed identity is
685:55 - restricted to one person one person
685:57 - resource is tied to lifecycle you can
685:58 - grant permissions etc
686:00 - etc um
686:04 - can we just turn that on and hit save
686:08 - well it can be granted access resources
686:10 - protected by a z sure let's give it a go
686:21 - all right so um it seems like we have to
686:23 - assign some role stuff
686:26 - so we can try and assign a role um
686:30 - can we do the subscription level
686:34 - contributor
686:36 - okay
686:39 - so there's a few different ways you can
686:40 - authenticate so hopefully this will be
686:42 - the easiest way to do it
686:44 - we'll refresh here did it assign it
687:02 - and i don't think it said it
687:06 - all right so give me a moment and let me
687:09 - see what i can figure up okay
687:12 - you know just to make this easier i
687:14 - think what we should do is just do the
687:15 - access key because that seems like the
687:17 - easiest way to do it i was just hoping
687:19 - that we could have you know
687:21 - just did a simple sign in here but it's
687:22 - not a big deal so we'll hit change
687:24 - connection we're going to add a new
687:25 - connection
687:26 - just say storage account
687:30 - event
687:32 - event grid
687:35 - key
687:36 - and so this is going to want the azure
687:37 - storage account name so this one will be
687:40 - the name of
687:42 - the storage count if we can find it
687:44 - it's called event grid basics
687:48 - and then we need the azure storage
687:50 - account access key
687:52 - so
687:53 - there's probably a tab called keys yep
687:57 - and we will show the key and we will
687:59 - copy the key
688:03 - if i don't have to pass long keys i like
688:05 - to not do that please check your account
688:08 - info again
688:09 - storage account access key should be a
688:11 - correct base64 encoded string come on
688:13 - give me a break here
688:15 - i am doing what you asked me to do
688:19 - so we will try this again
688:27 - this thing just hates me today give me a
688:29 - second okay
688:31 - you know what it was really short so i
688:33 - really don't trust it so let's just do i
688:35 - just cleared it out there i didn't do
688:36 - anything else what we're going to do is
688:38 - go back here click the copy
688:41 - and then right click
688:43 - and paste that's so much longer okay
688:45 - that has to be the right key we'll hit
688:46 - create
688:49 - and we'll give it a moment
688:52 - okay great
688:54 - so
688:54 - that's for that one
688:57 - um but this has to have the right
688:58 - connection as well so what we'll do is
689:00 - just change the connection
689:03 - you have a few here eh um and the one we
689:06 - want is
689:09 - the one that's valid so we'll go this
689:11 - one down below as you can see a few
689:13 - attempts here
689:15 - and we'll save it
689:17 - and we'll go back
689:19 - to our overview here i'm just gonna
689:21 - close that tab out we're gonna close
689:23 - this out we're gonna go back into here
689:25 - we're gonna go into our containers we're
689:27 - gonna go into our basic
689:29 - we're gonna go ahead and delete this
689:30 - we'll say okay
689:34 - and we will upload a new file we will
689:36 - choose the new file we'll choose data
689:40 - upload it
689:41 - we'll go back over here
689:43 - and i want to see the latest run here
689:46 - we'll give this a refresh
689:50 - is it running
689:52 - it looks like it's running it's hard
689:53 - because this one looks like it just
689:54 - failed and and now the the messaging is
689:57 - getting really muddy here
689:59 - what is it doing so we'll click off here
690:02 - sometimes the portal is a bit funny is
690:04 - it just triggering over and over again
690:07 - did we make an infinite loop
690:10 - uh oh
690:11 - okay i think we have a problem here
690:15 - well if we go here is it basic or it's
690:17 - basic alt
690:19 - this one's basic
690:22 - so
690:24 - what's the problem
690:26 - we'll refresh
690:31 - failed why did it fail
690:35 - conflict
690:37 - another active upload session exists
690:39 - please retry after some time
690:43 - okay
690:45 - uh well let's just go take a look here
690:48 - go back
690:50 - it's here so it's it's here
690:53 - so it clearly has worked why it's
690:55 - triggering multiple times i don't know
690:57 - um don't particularly like that
691:01 - there we go let's go ahead and delete
691:03 - this one here
691:04 - and it's just it's just going over and
691:06 - over and over again
691:08 - so there's something wrong with my
691:10 - workflow
691:23 - so this looks fine to me
691:25 - that looks fine to me
691:30 - maybe it's triggering oh you know what
691:32 - the the parameters out of here so this
691:35 - is supposed to have a prefix here so
691:36 - what's happening is that it's triggering
691:39 - on any time a basic one is set up or a
691:42 - basic alt one and it's just stuck in an
691:44 - infinite loop which is really really bad
691:46 - um so
691:47 - we did do this earlier but for whatever
691:49 - reason um
691:50 - the changes still are not here
691:53 - so what we'll need to do is set up that
691:55 - prefix so what we'll do is type in
691:58 - blob
692:00 - services default
692:03 - containers
692:05 - basic
692:07 - because we really don't want to trigger
692:08 - it on any but that that container there
692:11 - and uh
692:13 - did it save it
692:16 - it doesn't look yeah i mean it should be
692:18 - there so what we'll do is go ahead
692:23 - like why is it not filtering
692:29 - oh
692:30 - i guess it wants a filter based on name
692:36 - but we gave it its name so
692:40 - i'm not sure what else
692:43 - we would have to type there
692:46 - okay um
692:51 - i'm just gonna put um dot
692:55 - it's the prefix filter so data i guess i
692:57 - don't know like
692:58 - it's not letting me save okay there we
693:00 - go we'll save that i just want to stop
693:02 - the infinite loop there for a moment
693:08 - so we'll go back over to the overview
693:11 - and we'll just make sure we're not
693:13 - running up our bill here
693:15 - and i'm just refreshing i just want to
693:16 - see that's not triggering anymore so it
693:18 - stopped triggering which is good
693:20 - and we'll go back over to here
693:22 - and we'll look at this prefix filter
693:24 - because i i remember having to do this
693:25 - so a filter like sample etc etc so we'll
693:29 - type it in again i guess
693:32 - blob services defaults
693:34 - containers
693:36 - basic
693:43 - it's very odd because like we typed it
693:46 - oh you know what i probably did i typed
693:48 - in the filter parameter here and we're
693:50 - supposed to add it then put it in there
693:51 - so it's just me getting confused by the
693:52 - ui
693:53 - silly me okay so what we'll do is go
693:55 - back to the overview
693:57 - and this time we just want to see
693:59 - trigger once so go back to basic all
694:01 - we'll go ahead and delete this we'll say
694:03 - okay
694:04 - and we'll go back to our event grid
694:06 - we'll go in or
694:08 - our event grid our basic our basic
694:10 - container we're going to go ahead and
694:11 - delete
694:12 - uh data here again
694:14 - and we're going to go upload one more
694:16 - time
694:18 - it's actually good that we had that
694:19 - problem because i got to show you
694:21 - uh why filters are so important um when
694:24 - we're dealing with uh
694:26 - the app logic there um or logic apps so
694:29 - we'll go ahead and hit upload we'll go
694:31 - make our way back over here we're gonna
694:32 - give this a refresh
694:34 - and now we have a new one and it only
694:36 - happened once and that's what we wanted
694:38 - to happen so we go back over to here
694:42 - and go to basic halt there it is so
694:44 - that's a means to which we can use event
694:46 - grid to integrate stuff you can see
694:48 - logic app is extremely useful for
694:50 - developers
694:51 - building all sorts of tools but we are
694:54 - all done here
694:55 - and what we'll do is make our way over
694:57 - to our resource group
694:58 - and we are going to just go ahead and
695:00 - clean up so we'll go into event grid
695:02 - basics
695:03 - and we'll go ahead and delete this
695:05 - resource group
695:12 - there we go and it's going to go ahead
695:14 - and delete there
695:15 - uh yeah and there you go
695:21 - [Music]
695:25 - hey this is andrew brown from exam pro
695:27 - and we are taking a look at azure event
695:28 - hub so event hub is an event in jet
695:30 - store that can consume millions of
695:33 - events from anywhere and process them in
695:35 - real time or micro batching thanks to
695:38 - the auto inflate feature which
695:40 - automatically scales the numbers of
695:41 - throughput to meet your changing needs
695:44 - so it provides an easy way to connect
695:45 - with your apache applications and
695:47 - clients here is kind of a overview
695:50 - illustration of event hubs so the idea
695:53 - over here is you have different
695:54 - protocols so h-p-s-a-m-q-p
695:59 - kafka for your event producers to then
696:01 - uh enter into azure event hubs you have
696:05 - partitions within your hubs you have
696:06 - consumers uh and then there is the
696:09 - received events the event receivers so
696:12 - you know hopefully that gives you a
696:13 - quick overview let's go dive a bit
696:15 - deeper
696:16 - [Music]
696:21 - let us take a look at the pricing for
696:22 - azure event hub but before we do i just
696:25 - want to expand on a few initialisms so
696:28 - we know what they stand for we have
696:29 - capacity unit for cu processing unit for
696:31 - pu and throughput unit for tu so across
696:35 - here we have four different pricing
696:37 - plans basic standard premium
696:40 - dedicated lots of azure services like to
696:42 - have a whole lot of
696:44 - specialized plans uh built into their
696:46 - services uh but as you can see as uh you
696:50 - go right the more expensive it goes
696:52 - uh but there's some key differences so
696:54 - for azure event hub if you're on basic
696:56 - you do not get capturing
696:58 - capturing cost a bit more uh so i mean
697:00 - we'll cost something at standard at
697:02 - premium dedicated it's included if you
697:04 - want to use apache kafka you're not
697:06 - going to get it in basic if you want a
697:08 - schema registry you're not going to get
697:09 - that in basic if you need extended
697:11 - retention you do not get a basic or
697:13 - standard so hopefully that gives you an
697:15 - idea of the differences of these tiers
697:18 - [Music]
697:22 - let's take a look at the key concepts a
697:24 - lot of these will cover in future slides
697:26 - so this one is a bit text heavy so do
697:27 - not worry
697:28 - but azure event hub helps you build your
697:30 - big data pipeline to analyze logging
697:32 - anomalies use anomalies users and a
697:35 - device telemetry where you only pay for
697:37 - what you use so some of the key concepts
697:40 - here are name space so it is an endpoint
697:42 - for receiving and distributing events to
697:44 - the event hubs we have event hub where
697:46 - your events will be delivered event hub
697:48 - cluster which is a dedicated event hub
697:50 - with 99.99 sla event hub capture this
697:53 - allows you to automatically capture and
697:55 - save streaming
697:57 - events
697:58 - we have uh event hubs for apache kafka
698:00 - so this is compatibility with apache
698:02 - kafka's
698:03 - setup event publishers these are
698:06 - applications or services that publish
698:08 - events to event hub publisher policy
698:10 - this is a unique id used to identify
698:12 - publishers partitions are used to
698:14 - organize the sequence of events in event
698:16 - hub event consumers are applications or
698:18 - services that read events from event hub
698:21 - consumer group which are
698:23 - the enable consuming applications to
698:25 - each have a separate view of the event
698:27 - stream stream offset which holds the
698:29 - position of an event inside a partition
698:32 - checkpoint is the process of
698:33 - distinguishing between read and unread
698:36 - events but let's go look at these all in
698:38 - more detail
698:39 - [Music]
698:44 - let's talk about scaling because that's
698:45 - a very important concept in vent hub so
698:47 - the idea is that there's this feature
698:49 - called auto inflate it's basically just
698:50 - a check box here and will automatically
698:52 - scale up to the maximum
698:54 - to you so um
698:57 - throughput units based on the traffic
698:59 - demand it's not available for basic
699:01 - pricing as we saw over there
699:03 - we also this option of if we're in the
699:05 - premium tier you're going to notice
699:07 - there is no auto inflate because it's
699:08 - already there by default and the unit is
699:11 - different it's processing units so
699:12 - you'll just slide that left and right
699:14 - that's just kind of reflect what we saw
699:15 - in the pricing
699:17 - section okay
699:22 - [Music]
699:23 - let's talk about an actual event hub so
699:26 - what we do is go name our hub and we got
699:28 - a few options here we have partition
699:29 - count so partitions are
699:31 - our data organization mechanisms that
699:34 - relates to downstream parallelism
699:35 - required and consuming applications
699:37 - sounds very fancy but it's just a way of
699:39 - segmenting your data so that you have a
699:41 - faster concurrent reads
699:44 - you have retention message retention so
699:46 - this is the period for which your events
699:49 - will retain so that means that when
699:52 - they're past that period they are poof
699:53 - they are gone then you have capture so
699:55 - capture enables you to automatically
699:57 - deliver the streaming data in event hubs
700:00 - to azure blob storage or azure data
700:02 - lakes store account of your choice with
700:04 - the ad flexibility of specifying a time
700:06 - or size interval setting up capture is
700:08 - really fast there are no administrative
700:10 - costs to run it it scales automatically
700:12 - automatically with event hubs throughput
700:14 - units event hubs captures is the easiest
700:16 - way to load streaming data into azure
700:19 - and enables you to focus on data
700:21 - processing rather than on data capture
700:23 - so just a great way to offload into
700:25 - azure blob storage or
700:27 - azure data lake store there you go
700:30 - [Music]
700:34 - let's take a look at producers because
700:35 - that is what is going to emit events
700:37 - that will get into event hub so on the
700:39 - right hand example we have some code
700:42 - because the idea is that you will have
700:43 - to instrument your producer to send out
700:46 - events so here you can see we've created
700:48 - a client we'll have to have a connection
700:50 - string the event hub name so we know who
700:52 - we're talking to
700:54 - you would create a producer and then
700:56 - here what we're seeing is a batch job so
700:59 - they create a batch job and they push
701:01 - some data on there and then they send
701:03 - uh send uh send the batch and then when
701:06 - they're done they close the client if
701:07 - there are any errors we can get some
701:09 - exception handling this is a javascript
701:11 - example so a producer also known as a
701:14 - publisher emits data to the stream
701:16 - publishers can publish events using the
701:18 - following protocol so https so most
701:21 - azure sdks use https as this would
701:24 - probably use underneath you have amqp
701:28 - which is a very popular uh queuing
701:30 - protocol kafka protocol so if you're
701:32 - using kafka or you're using that
701:34 - protocol then that's another way to get
701:36 - in here and we just walked through that
701:38 - example you can publish events either
701:41 - one at a time or batches as we see here
701:43 - on the right hand side there's a limit
701:45 - of one megabyte regardless of whether
701:46 - it's a single event or a batch beyond
701:48 - one megabyte events will be rejected for
701:51 - automation or for authorization
701:53 - publishers use either azure ad with auth
701:56 - 2 issued jwt tokens or shared access
702:00 - signature sas which is something we see
702:02 - a lot in azure services let's just talk
702:04 - about some other things so https versus
702:07 - amqp for publishing events for amqk it
702:10 - requires the establishment of a
702:11 - persistent bi-directional socket in
702:13 - addition to tls or ssl
702:18 - mqamqp has higher network costs when
702:21 - initializing the session amqp has higher
702:24 - performance or frequent publishers
702:26 - and can achieve much lower latencies
702:28 - when used with asynchronous publishing
702:30 - code https requires additional tls
702:34 - overhead for every request so it's going
702:35 - to be up to you to decide whether you
702:37 - want to use https
702:39 - or amqp protocol
702:41 - for publisher policies event hub enables
702:44 - granular control over events publishers
702:47 - through publisher policies
702:49 - and they these publisher policies are
702:51 - run time features designed to facilitate
702:53 - large numbers of independent event
702:55 - publishers with publisher policies each
702:57 - publisher uses its own unique identifier
703:00 - when publishing events to an event hub
703:02 - using the following mechanisms and i
703:04 - guess there was supposed to be more
703:05 - there maybe but
703:07 - i think that's pretty much it because we
703:09 - don't really need to know much more
703:10 - about
703:11 - publisher policies to be honest uh if
703:14 - there's an opportunity if we do follow
703:16 - along that is opportunity maybe we will
703:18 - take a detour into that to see if it's
703:20 - worth our time but as far as i'm aware
703:22 - of i don't think it's that important
703:23 - there we go
703:24 - [Music]
703:28 - let us take a look at a consumer so
703:30 - here's an example of consumer code very
703:32 - similar
703:33 - kind of similar to the last one here so
703:35 - we are uh creating a consumer client
703:38 - and so here we need a connection uh
703:42 - connection string container name where
703:43 - we're gonna put this stuff looks like
703:45 - we're going to put in azure blob storage
703:47 - we're going to create that client and
703:49 - then we have the subscription so the
703:50 - idea is that
703:52 - it was going to listen for data to
703:54 - consume
703:55 - okay
703:56 - and then over here we can see that it
703:58 - updates a checkpoint we'll talk about
703:59 - checkpoints soon enough but let's just
704:01 - talk about consumers in general so
704:03 - consumer also known as a reader receives
704:05 - data to process from the stream all
704:07 - event hub consumers connect via the amqp
704:10 - 1.0 session and events are delivered
704:12 - through the session as they become
704:13 - available the client does not need to
704:15 - pull for data availability so that's
704:18 - that we'll next talk about consumer
704:19 - groups
704:20 - [Music]
704:24 - well let's talk about consumer groups
704:25 - here this is very tech savvy sorry but
704:27 - there's not much to show
704:29 - a consumer group is a view state
704:31 - position or offset of an entire event
704:32 - hub consumer groups enable multiple
704:34 - consuming applications to each have a
704:36 - separate view of the event stream and to
704:38 - read the stream independently at their
704:40 - own pace with their own offsets so in a
704:42 - stream processing architecture each
704:44 - downstream application equates to a
704:46 - consumer
704:47 - group there's always a default consumer
704:49 - group an event hub and you can create up
704:51 - to the maximum number of consumer groups
704:52 - for the corresponding pricing tier there
704:55 - can be at most five concurrent readers
704:57 - on a partition per consumer group
704:59 - however it's recommended there's only
705:00 - one active receiver on a partition per
705:02 - consumer group
705:03 - some clients offered by azure sdk are
705:05 - intelligent consumer agents that
705:07 - automatically manage the details of
705:08 - ensuring that each partition has a
705:11 - single reader and then all partitions of
705:13 - an event hub are being read from this
705:15 - allows your code to focus on processing
705:17 - the events being read from the event hub
705:20 - so it can ignore many of the details of
705:22 - the partition so there you go
705:24 - [Music]
705:28 - let's talk about offsets for azure event
705:30 - hub so an offset is the position of an
705:32 - event within a partition and so here's
705:35 - an example a very ugly graphic from the
705:38 - documentation but it makes our point
705:40 - clear the idea is you have a partition
705:42 - you have um
705:44 - you have an event within the partition
705:46 - there's all these little lines represent
705:47 - event and that's where our offset is so
705:49 - offsets enable a consumer or reader to
705:51 - specify a point in the event stream from
705:53 - which they want to begin reading events
705:55 - you can specify the offset as a time
705:56 - stamp or as an offset value consumers
705:59 - are responsible for storing their own
706:01 - offset values
706:02 - outside of the event hub service within
706:04 - a partition each event includes an
706:06 - offset so there you go
706:07 - [Music]
706:11 - all right let's talk about checkpointing
706:13 - uh there is really not a lot to say
706:15 - about it um or to visualize it but it's
706:18 - kind of part of offsets so checkpointing
706:20 - is a process by which readers mark or
706:21 - commit their position within a partition
706:24 - event sequence checkpointing is the
706:26 - responsibility of the consumer and
706:27 - occurs on a per partition basis with a
706:29 - consumer group responsibility means that
706:31 - for each consumer group each partition
706:34 - reader must keep track of its current
706:35 - position in the event stream
706:37 - and can inform the service when it
706:40 - considers the data stream complete
706:42 - [Music]
706:46 - hey this is andrew brown from exam pro
706:48 - we're taking a look at schema registry
706:49 - something that is uh not uncommon when
706:52 - we're looking at event buses there's
706:53 - something similar in aws um and the idea
706:56 - here is that you're able to enforce a
706:58 - particular schema of your data
707:00 - and this will help avoid um
707:04 - uh things breaking for your consumers
707:07 - and have a standard way of
707:10 - expectation of what something is so
707:12 - the idea here is skip registry provides
707:14 - a centralized repository for schemas
707:16 - this provides the flexibility for your
707:17 - producer consumer applications to
707:19 - exchange data without having to manage
707:21 - and share schemas between them and also
707:23 - evolve at different rates that's the key
707:25 - thing there evolve at different rates
707:28 - so here we have our schema group we have
707:31 - a schema we'll talk about schema group
707:32 - in a second and then you have schema
707:33 - versions and then there's the actual
707:35 - schema itself so schema groups are under
707:38 - your namespace and can be accessed by
707:40 - all topics event hubs under that
707:41 - namespace so here you can see we're just
707:44 - setting some options here and then
707:46 - here's the schema itself so here we have
707:49 - a very simple example where we have a
707:51 - type of record and name ordered and we
707:52 - have a couple of fields
707:54 - but the idea again is schema registry
707:56 - really does help you
707:58 - kind of enforce what you expect data to
708:00 - look like for your producers for your
708:02 - consumers and there you go
708:04 - [Music]
708:08 - hey this is andrew brown from exam pro
708:10 - and we are taking a look at apache kafka
708:12 - so the reason we're talking about it is
708:13 - just because
708:14 - event hub has a compatibility mode for
708:16 - it so apache kafka is an open source
708:18 - streaming platform to create high
708:19 - performance data pipelines streaming
708:21 - analytics data integrations and mission
708:23 - critical applications kafka was
708:25 - originally developed by linkedin in open
708:26 - source in 2011. kafka was written in
708:30 - scala and java so to use it you'll need
708:32 - to know how to write java and the idea
708:34 - is quite simple we have producers we
708:35 - have consumers we have topics there's a
708:37 - cluster there's partitions looks similar
708:40 - to event hub and other kind of streaming
708:43 - platforms and kafka data is stored in
708:44 - partitions of course that span multiple
708:47 - machines because it's for distributed
708:48 - computing you have producers
708:51 - via the kafka producer api you have
708:54 - consumers via the consumer api messages
708:57 - are organized into topics producers will
708:58 - push messages to topics consumers will
709:01 - listen on topics so let's talk about
709:03 - that kafka compatibility so event hub
709:05 - provides an endpoint compatible with
709:07 - apache kafka producers and consumers
709:09 - apis for version one and above so
709:13 - event hub kafka compatibility is
709:14 - alternative to running your own apache
709:16 - cluster so
709:18 - you know we said that they're very
709:19 - similar so eventhough calls them name
709:21 - space kafka calls them uh clusters so
709:24 - you can kind of see that there's that
709:25 - translation over but the idea is that
709:26 - when you enable this it will use all the
709:29 - stuff that kafka is used to seeing
709:32 - and then the idea there is that you
709:33 - don't have to run a kafka cluster you
709:34 - just run event hub
709:36 - if you've already written code that
709:38 - works with kafka that's the idea is that
709:40 - you can drop in event hub as a managed
709:42 - replacement now does
709:44 - event hub work exactly like kafka i'm
709:46 - not a hundred percent sure
709:48 - so there could be some feature
709:49 - differences um but in terms of uh
709:52 - protocols or endpoints being able to
709:54 - replace kafka with a vent hub if you
709:55 - need to um that's why they have this
709:58 - functionality here so there you go
710:04 - hey this is andrew brown from exam pro
710:06 - and we are looking at partitioning or
710:07 - partitions for event hub so vent hub
710:09 - organizes sequences events sent to an
710:11 - event hub into one or more partitions as
710:13 - new events arrive they're added to the
710:15 - end of the sequence this idea is that
710:17 - you have those partitions older's on the
710:19 - left mirrors on the right partitions
710:21 - hold the following data about the event
710:24 - the body of the event a user-defined
710:25 - property bag describing the event
710:28 - metadata such as its offset its
710:29 - partition its number in the stream
710:31 - sequence
710:32 - service side timestamps at which it was
710:34 - accepted partitioning allows for
710:36 - multiple parallel logs to be
710:38 - used for the same event hub and
710:40 - therefore multiplying the available raw
710:43 - input output throughput capacity you can
710:45 - use a partition key to map incoming
710:47 - event data into specific partitions for
710:49 - the purpose of data organization and the
710:52 - partition key is a sender supplied value
710:55 - passed into an event hub so there you go
711:02 - [Music]
711:03 - hey it's andrew brown from exam pro
711:04 - we're taking a look at event retention
711:06 - for event hub so published events are
711:08 - removed from an event hub based on a
711:10 - configurable time based retention policy
711:13 - and the default value and shortest
711:14 - possible retention period is one day so
711:16 - that's 24 hours for event hubs standard
711:19 - the maximum retention is seven days and
711:21 - we saw that um in the pricing page where
711:24 - retention can be different based on
711:25 - those things so for premium and
711:27 - dedicated the maximum retention is 90
711:29 - days if you change the retention period
711:32 - it applies to all messages including
711:34 - messages that are already in event hub
711:36 - you cannot explicitly delete events the
711:38 - reason for event hubs limit on data
711:40 - retention based on time is to prevent
711:42 - large volumes of historical customer
711:44 - data getting trapped in a deep store
711:46 - that's only indexed by tam's timestamp
711:48 - and only allows for sequential access
711:50 - which
711:51 - doesn't sound too good if you need to
711:54 - archive events beyond the allowed
711:56 - retention period you can automatically
711:58 - have them stored in azure storage or
712:00 - data lake by turning on event hubs
712:02 - capture feature which we talked about
712:04 - earlier if you need to search or analyze
712:07 - such deep archives you can easily import
712:08 - them into azure synapse
712:11 - or other similar stores and analytics
712:14 - platforms so there you go
712:16 - [Music]
712:20 - hey this is andrew brown from exam pro
712:22 - and this follow along we are going to
712:24 - learn all about azure event hub uh so
712:26 - what i want you to do is go to the top
712:28 - here and type in event hub and we are
712:29 - going to create ourselves a new event
712:31 - hub
712:32 - namespace so we'll go here and hit
712:34 - create and we'll create a new resource
712:36 - group as we always do called my event
712:38 - hub
712:39 - and then for the namespace name we'll
712:41 - say
712:42 - my event hub if it doesn't let you
712:45 - do that you'll have to put some numbers
712:47 - on the end i'm just putting some numbers
712:48 - here because often these are taken up
712:51 - doesn't matter what location is just
712:53 - choose whichever one we're going with
712:54 - basic because there's not a huge
712:55 - difference between the pricing terms in
712:57 - terms of feature sets that we want to
712:59 - use today so go ahead and create this
713:01 - namespace so we'll give it a moment
713:03 - and we'll go ahead and hit create
713:06 - and then we'll just wait for this
713:08 - namespace to provision
713:10 - all right so after waiting about a
713:11 - couple of minutes there our namespace is
713:12 - deployed we're going to go ahead and
713:14 - create ourselves an event hub i'm going
713:15 - to call this one
713:17 - my hub we'll go ahead and hit review and
713:19 - create
713:21 - and we'll create
713:23 - and these crate very very quickly so
713:25 - we're not going to have to wait too long
713:26 - we need to set some shared access
713:28 - policies here so i'm going to go into
713:30 - the hub here i'm going to hit manage i'm
713:32 - going to call this
713:33 - my sas
713:35 - sas shared access policy and there we
713:37 - will now have the ability to have a
713:39 - primary key and connection key so we can
713:42 - actually connect
713:43 - to it so what we're going to do is go to
713:45 - github and i want to create a new
713:48 - repository
713:50 - and we'll go ahead here
713:53 - and we'll go down below go to exam pro
713:55 - we'll say
713:56 - my event hub
713:58 - that's already taken because i've done
713:59 - this before
714:00 - so for mine i'm going to call it new
714:02 - and we'll go down to private we're going
714:03 - to use node.js so let's type in node.js
714:05 - so that it ignores the node modules
714:08 - you're going to want to have getpod or a
714:10 - visual studio code installed on your
714:12 - computer the easiest way is to get pod
714:14 - because these environments are temporary
714:16 - and it's free to utilize it so
714:19 - if you can go get the chrome extension
714:20 - or
714:22 - if you don't want to install the chrome
714:23 - extension all you got to do is attach
714:24 - this to the end of the repo to launch a
714:26 - github or get pod environment so give
714:29 - that a moment to launch
714:31 - and there we go so i do have some code
714:34 - uh for this
714:37 - so i'm just looking for it off screen
714:38 - here
714:41 - and
714:42 - we're gonna need to have a couple files
714:44 - here we're going to need a new file
714:45 - called
714:46 - send.js
714:48 - and we're getting a new file here called
714:49 - receive.js
714:51 - i'm not typing the full word receive
714:52 - because i'm always really bad at
714:53 - spelling it so i'm just trying to save
714:55 - myself some trouble
714:58 - and
714:59 - we're also going to have to initialize a
715:01 - new
715:03 - package.json file so there we go
715:05 - and we're going to have to get
715:07 - a couple things installed yeah so we'll
715:10 - need mpm install
715:12 - azure
715:13 - event
715:14 - event hubs
715:16 - azure
715:18 - storage blob
715:20 - azure event hubs
715:24 - checkpoint
715:26 - store blob
715:29 - save dev and make our lives a bit easier
715:31 - it seems like i typed something wrong
715:32 - here i'll just hit up
715:34 - i forgot the forward slash here
715:37 - and so we'll just go event hub
715:41 - azure
715:42 - javascript
715:46 - because i believe yep
715:49 - i kind of use this one but i modified it
715:51 - to make it a little bit easier i think
715:52 - this is the one i was doing
715:55 - no i don't think so tutorial
716:00 - might be this one
716:02 - yeah it looks like this um so this is
716:05 - just the javascript there so for send we
716:07 - will grab this code here
716:09 - we're not going to do exactly the way
716:10 - they do it but pretty close
716:13 - and then there is a receive code so go
716:15 - down below and we'll grab this as you
716:16 - can see there's a lot going on here
716:22 - that will be our receive a couple things
716:24 - we need to properly set so these will be
716:26 - all environment variables so what we'll
716:27 - do is go to the top here and we'll just
716:29 - do constant
716:30 - process
716:32 - requires process that's going to allow
716:34 - us to import our environment variables
716:38 - so we'll just say
716:40 - copy let's save that
716:43 - we'll paste that in there as well
716:45 - and then this is where we need to
716:47 - replace all environment variables as you
716:49 - can see i always have things pinging up
716:50 - on me here so just close my teams out
716:59 - and just line these to make this a
717:00 - little bit faster
717:02 - so default will be what we'll keep here
717:05 - this will be our storage connection
717:07 - string
717:09 - say process env
717:12 - storage
717:15 - connection string
717:18 - then this one will be
717:19 - process env
717:22 - container name
717:24 - then we need
717:25 - process
717:27 - env event hub name
717:29 - and up here we'll have process
717:32 - env
717:34 - event hub
717:36 - connection string
717:37 - and then we'll go over to our send here
717:39 - we'll do something very similar so
717:40 - process
717:42 - env
717:44 - event hub connection string
717:48 - and then we'll have process env
717:50 - event hub
717:52 - name
717:54 - so we need to set all of these i'm just
717:56 - going to copy this for a moment here i'm
717:57 - going to make a new file
718:00 - file new file
718:02 - sure it doesn't really matter we're just
718:03 - using this as a quick scratch pad and so
718:06 - what i want to do here is just
718:08 - delete out this part
718:11 - and we're going to do export on the end
718:14 - here we'll take out this one here
718:16 - and the idea is that we'll just set them
718:18 - all here and then we will
718:21 - make our lives a little bit easier when
718:22 - we have to mask something
718:24 - set these
718:26 - so we do have the connection string
718:28 - because we saw it over here
718:29 - so we will grab the primary one it
718:31 - doesn't matter which one primary
718:33 - secondary
718:34 - and that is for the
718:37 - event hub
718:39 - we called the
718:43 - the event hub was called my hub i
718:45 - believe
718:46 - we'll just double check
718:48 - what the hub was called yeah it's called
718:51 - my hub up there
718:52 - we'll need a storage account so what i'm
718:54 - going to do
718:55 - push this thing we get out of here get
718:57 - out of here i'm not
718:58 - i'm not trying to save a freaking file
719:01 - there we go hit escape a bunch of times
719:03 - and we'll go back here and we'll create
719:04 - ourselves a new storage account i'll
719:06 - actually make this in a new tab so we
719:07 - can see what we're doing
719:11 - so we'll go over to storage accounts
719:15 - and we'll create ourselves a new storage
719:17 - account we'll create this storage
719:18 - account in the same name space so we'll
719:21 - go down to my event hub we'll just say
719:24 - my
719:26 - or we'll just say my event hub 8888
719:30 - again you might have to change it based
719:32 - on your standards but um or like what is
719:34 - available to you
719:35 - we'll go ahead and go review
719:37 - create
719:40 - and for the container name
719:42 - we'll probably just call container
719:45 - maybe container one
719:48 - we just have to wait for this crate to
719:50 - grab that connection string
719:53 - so this usually doesn't take too long
719:54 - just a couple seconds
720:00 - okay so there we are we'll go over to
720:03 - access keys and we'll grab the
720:05 - connection string from here i believe
720:06 - this one should work
720:08 - uh let me just double check
720:12 - yeah i think this will work so what
720:14 - we'll do is go back over here if it
720:16 - doesn't we'll find out pretty soon and
720:17 - we'll just generate out a shared policy
720:19 - uh we'll go ahead and paste that on in
720:21 - here
720:23 - and just double checking that this is
720:25 - the correct one this looks identical to
720:27 - that one that can't be correct so we'll
720:28 - go back here this is the storage account
720:31 - so we'll go oh i have to actually hit
720:32 - the copy button that's what it didn't do
720:35 - we'll go ahead and paste that in and so
720:37 - theoretically this should work so we'll
720:38 - go ahead and copy these we'll drag our
720:41 - terminal up a bit we'll paste these in
720:43 - here
720:45 - and
720:46 - what i'll do is just double check that
720:48 - they're here so we'll say env grep
720:50 - event hub
720:53 - so those are both set and then we'll do
720:56 - storage
720:58 - that one is set and we'll do container
721:01 - that one is set so these are all in good
721:03 - shape for our storage account we still
721:04 - have to create the container so go here
721:07 - and create a new container say container
721:09 - one we'll go ahead and create that we'll
721:12 - make our way back over here and then
721:14 - instead of just having export we'll do
721:16 - gp
721:17 - env this is just in case we have to
721:18 - restart the environment for any reason
721:20 - so that these environment variables get
721:22 - exported twice so we'll paste that in
721:24 - there
721:24 - i believe those are all set i had to hit
721:26 - enter on the last one there
721:28 - and let's see if our code works so we'll
721:30 - do a send
721:32 - um
721:33 - actually we have to set up
721:35 - two scripts here so that we can actually
721:37 - call them so one called here is called
721:39 - send
721:41 - this will be node send js
721:45 - and we'll have receive so rec just
721:47 - because i always spell receive wrong and
721:48 - i just don't want to have to type it a
721:49 - thousand times we'll have that there so
721:51 - now what we'll do is do npm run send
721:55 - see if that works
721:58 - it says a batch of three events have
722:00 - been sent we'll go and confirm that over
722:01 - an event hub if it worked if we go to
722:03 - the overview it should show us some
722:05 - messages were received
722:07 - sometimes there's a bit of a delay
722:10 - so we'll just give it a teeny tiny
722:11 - amount of time we'll hit refresh here
722:14 - because we know we sent them but while
722:15 - we're waiting for those to kind of
722:16 - propagate what we'll go back here is
722:18 - just kind of look at the code because we
722:19 - didn't really look at it
722:21 - so the way it works is you are defining
722:24 - a client and it will be the producer
722:27 - client
722:27 - um and then down below so we say okay a
722:30 - producer is someone that produces events
722:32 - it's very common in a messaging system
722:33 - to have a producer and consumer we're
722:35 - going to create a batch job and we're
722:36 - going to add them all to the batch job
722:39 - then we're going to send them all at
722:40 - once and it's going to close and it'll
722:42 - enable to complete and if there are any
722:44 - errors it will alert us about it
722:46 - so we'll go back over here and we'll do
722:48 - a refresh
722:50 - so i want to see
722:52 - messages messages would normally show up
722:54 - here so since i don't trust it i'm just
722:56 - going to run it again i mean clearly
722:58 - worked because there was no errors
723:04 - and we'll go back over here and
723:07 - not here but we'll go back here we'll
723:08 - refresh
723:11 - and i'm just waiting to see
723:14 - something here
723:24 - processing data this is like something
723:25 - that's really powerful with them
723:28 - event hub here so
723:32 - still don't see the messages just give
723:34 - me a second to debug this i've done this
723:36 - lab like four times so it should work
723:38 - but uh you know sometimes sometimes it's
723:41 - trouble so just a moment
723:44 - all right so i literally did nothing and
723:45 - now it's actually showing up in the uh
723:47 - messaging queue so you know or the hub
723:50 - the funnel
723:51 - so that's just something you have to
723:52 - consider is that sometimes you just have
723:53 - to be a little bit patient let's see if
723:55 - we can go receive those messages now by
723:56 - running the other
723:58 - scripts so what i'm going to do here is
723:59 - do npm
724:01 - run receive
724:03 - and it should receive the messages as
724:05 - long as something isn't typed
724:06 - incorrectly so we'll go back over here
724:08 - we've seemed to introduce a little
724:10 - mistake so i'll go ahead and save that
724:11 - we'll hit up
724:14 - and it should receive the events so i
724:15 - should print the three out
724:17 - so there we go
724:18 - and so we are streaming this or the
724:20 - consumer is technically um storage
724:22 - storage accounts but if we go to the
724:24 - storage account there's nothing really
724:25 - uh um
724:27 - intelligible in terms of what's in here
724:29 - so like there are stuff in here
724:31 - checkpoints
724:33 - i guess it saved a checkpoint
724:35 - i personally don't know what i'm looking
724:36 - at so i'm not exactly sure what the
724:38 - point of doing that i guess it's just
724:39 - saying the checkpoint is like the last
724:41 - point it wrote
724:42 - but if we just take a look at the code
724:44 - here quickly you can see it's called
724:46 - consumer so we get a consumer client
724:48 - there's a blob checkpoint store
724:51 - then we have the consumer client we are
724:52 - subscribing so it's saying hey are there
724:54 - any events let's consume them if there
724:56 - are no events throw a console log so
724:59 - tell us about it iterate through them
725:01 - then update the checkpoints so move it
725:03 - to the next point um just say hey this
725:05 - is where it is now and that's pretty
725:07 - much all i want you to do we can go
725:09 - ahead and save this code we'll say
725:12 - eventhub code doesn't matter what you
725:14 - name it there we'll sync the changes and
725:16 - we'll go ahead and clean up
725:18 - so we'll go back to our resource groups
725:21 - we'll go to event hub
725:22 - and we will then go
725:24 - and delete this resource group
725:27 - and we'll go here and
725:29 - there you go
725:32 - [Music]
725:36 - hey this is andrew brown from exam pro
725:38 - and we are taking a look at azure
725:39 - notification hubs so the goal of the
725:41 - service is to send notifications to
725:43 - mobile apps in order to increase the app
725:45 - engagement and usage you can use tags to
725:48 - filter which apps or users that will
725:50 - receive notifications and those apps can
725:52 - be registered in azure notification hub
725:54 - in two different ways
725:57 - the first way is installation model so
725:59 - that's where the app sends an
726:00 - installation request to azure
726:02 - notification hub including all the
726:04 - information needed for the installation
726:06 - to be completed this model is easy to
726:08 - set up and maintain
726:10 - and then from there we have the
726:12 - registration model uh so this app sends
726:14 - a azure notification hub registration
726:16 - request with the pns handlers and tags
726:20 - and azure notification hub responds with
726:22 - the registration id uh it's great for
726:24 - native or generic templates that can be
726:27 - used in order to define
726:29 - the message so um you know azure
726:32 - notification hub in order for us to
726:34 - actually uh show off how to use this
726:37 - thing we'd actually have to set up a
726:38 - mobile app and it's not easy to do so
726:41 - this is something where we do not have a
726:42 - follow along for i don't think it's a
726:44 - big deal because i really didn't see it
726:45 - on the exam but it is part of the
726:47 - material so we definitely should know
726:49 - what it does so we'll just continue on
726:51 - here and learn a little bit more about
726:52 - azure notification hub okay
726:54 - [Music]
726:59 - all right let's talk about the supported
727:00 - platforms that azure notification hub
727:02 - supports this gives you an idea of how
727:06 - how it can be used with different uh
727:07 - mobile frameworks uh because you know we
727:09 - all don't use iphones there's androids
727:11 - and other things like that so for
727:13 - android we're gonna see support through
727:14 - firebase cloud messaging if you do not
727:17 - know what firebase is it's google's
727:19 - cloud's platform as a service it's a way
727:21 - of building out um
727:23 - applications similar to uh it was
727:25 - amplify or supabase but uh the component
727:28 - for messaging is through that sdk for
727:30 - cloud messaging this used to be called
727:32 - google cloud messaging so if you go
727:34 - through the documentation
727:36 - you might see this this does not exist
727:38 - anymore you have to use firebase cloud
727:40 - messaging okay for ios we have the apple
727:43 - push notification service apns and so
727:46 - through that it's going to be
727:47 - notification hubs for ios 13. and uh the
727:51 - way that works is through objective c
727:53 - sdk um i don't think there's support for
727:56 - swift at least i could not find it but
727:58 - this is only important if you are a
728:00 - native mobile application developer
728:02 - but basically the idea is the objective
728:04 - sdk is how you would interact with it
728:06 - there's also notification up for
728:09 - zamarin i always say zamarin wrong for
728:12 - ios applications samarin is
728:15 - i'm assuming the x is pronounced like an
728:17 - s i don't know why i just assume but um
728:20 - that is a
728:21 - uh a
728:23 - technology a framework built by uh
728:25 - microsoft that allows you to build out
728:28 - native applications for a variety of
728:30 - things like ios and android and things
728:31 - like that and windows phone of course
728:34 - for windows phone the way we're going to
728:37 - communicate is through the microsoft
728:38 - push notification service so mpns
728:42 - for windows because i suppose you can
728:44 - build a windows application not just a
728:46 - windows phone this is through the
728:48 - universal windows platform so uwp app uh
728:51 - and from there that is the windows push
728:54 - notification service of wns so you know
728:57 - this is just to give you an idea of the
728:58 - broad stuff and get you exposure to a
729:00 - lot of these terms again i don't think
729:02 - that's going to show up in the exam but
729:03 - good to know for azure notification hub
729:06 - okay
729:07 - [Music]
729:11 - all right let's take a look at how
729:13 - message flow works for azure
729:15 - notification hub so notice this tower
729:16 - here so this is um the platform
729:20 - notification service by your provider so
729:22 - you're using apple they have their own
729:24 - microsoft has their own et cetera et
729:26 - cetera and the idea is that notification
729:28 - hub is connected to that and that's how
729:30 - notification gets pushed out so
729:31 - notification i was just pushing to this
729:33 - tower
729:34 - and then this tower
729:36 - pushes out to tablets that are
729:37 - subscribed okay
729:40 - let's go through the flow here because
729:42 - there's some interesting things about
729:43 - handles we need to know so the idea is
729:45 - that your app says hey i want to
729:47 - subscribe to push
729:49 - to push notifications so it sends a
729:51 - request to your platform notification
729:53 - service and it's going to get back a
729:55 - handle that handle's going to vary so if
729:57 - you're using windows uh
729:59 - notification service you get back a url
730:01 - a uri and for apples you're going to get
730:03 - back a token so then you have to store
730:05 - that token into your application so it
730:07 - comes back to your phone your tablet
730:09 - whatever and then it goes okay to the
730:11 - back end and then stores that token here
730:13 - and then the idea is that you can then
730:14 - use that token that whenever your app
730:17 - wants to notify all devices that are uh
730:20 - registered
730:21 - for push notifications it's going to
730:23 - then push it out
730:25 - and with the handle it's going to go to
730:27 - the platform notification service and
730:28 - then that's going to roll it out and
730:30 - then all your apps are going to
730:33 - receive that information so hopefully
730:34 - that makes it uh pretty clear how that
730:36 - works okay
730:38 - [Music]
730:42 - all right let's take a look at some key
730:43 - concepts or components that are in azure
730:45 - notification hub to just kind of cement
730:47 - what we learned about the message flow
730:49 - and just the service in general so azure
730:51 - notification has these key concepts we
730:54 - have routing this is the process of
730:55 - selecting which registrations will
730:57 - receive the push notifications we have
730:59 - the id of broadcast all registrations
731:01 - will receive the push notification a tag
731:03 - registrations that contain the specific
731:05 - tag receive that push notification a
731:07 - great way to
731:09 - narrow down who you want to notify there
731:12 - are tag expressions so registrations
731:14 - that match the tag expression will
731:15 - receive a push notification so that is
731:18 - an even further way of uh narrowing down
731:20 - who we want to
731:22 - uh receive a push notification there's
731:24 - the notification hub name space so you
731:26 - have to create a group of one or more
731:28 - notification hubs in order to send stuff
731:30 - out tags are used to label applications
731:33 - so that they can be routed more
731:34 - efficient efficiently which is what we
731:36 - were talking about here just a moment
731:38 - ago but tag and tag expressions you have
731:40 - templates these define the format on how
731:42 - the client will receive the push
731:43 - notification
731:45 - and it's platform specific um okay
731:48 - each platform is this is platform
731:50 - independent so platform specific i think
731:52 - is probably a better word there
731:54 - each platform has its own platform
731:56 - notification service which is in charge
731:58 - of push notifications so the idea is
732:00 - that uh push notification hub or azure
732:03 - notification hub is just basically an
732:04 - adapter to all these a variety of
732:06 - services that are offered by these other
732:08 - uh telecom or notification companies
732:11 - okay
732:13 - [Music]
732:17 - hey this is andrew brown from exam pro
732:19 - and we are taking a look at queuing so
732:21 - both azure storage queue and azure
732:22 - service bus have overlapping
732:24 - responsibilities because they're they're
732:26 - both messaging or queuing as a service
732:29 - and so just before we jump into both of
732:31 - these services i wanted to
732:33 - distinguish the differences between
732:35 - these so you can kind of identify the
732:37 - use case for them so we have azure
732:39 - storage queue this is a simple message
732:41 - queuing services store large numbers of
732:43 - messages then you have azure service bus
732:45 - this is for broader messaging service
732:47 - that uh supports cueing pub sub and more
732:50 - advanced integration patterns so let's
732:52 - talk about the storage queue side of it
732:53 - so this can handle millions of messages
732:55 - there's no guarantee of the order of
732:57 - messages there's at least once delivery
732:59 - which is really nice 500 terabytes max q
733:02 - size 64 kilobyte message uh kilobyte
733:06 - yeah kilobyte message um and 48
733:09 - kilobytes if you're doing base64
733:11 - encoding so that's the size it can be
733:13 - after the encoding unlimited queues
733:16 - unlimited concurrent clients uh at least
733:18 - based access mode so we got 30 seconds
733:21 - to seven days uh set for the entire
733:24 - queue
733:25 - and then on the right-hand side looking
733:26 - after service bus we it can offer first
733:29 - and first out which is uh very useful
733:31 - guarantees the order of messaging be
733:33 - being consumed that's what a first in
733:34 - first out means at least once or at most
733:37 - once delivery um we have one gigabyte to
733:41 - 80 gigabytes of max q size 256 kilobytes
733:45 - to one megabyte so a lot larger than
733:47 - azure storage queue we only have a
733:49 - maximum of 10 000 max queues but that is
733:52 - a lot of cues i don't know what you're
733:53 - doing with ten more than ten thousand
733:54 - five thousand concurrent clients uh lock
733:57 - based access mode and that is um with 60
734:01 - seconds can change settings per message
734:04 - and it has dead letter uh support which
734:06 - is very useful state managed support
734:09 - message group support uh deduping
734:11 - support purging uh queue support and
734:14 - transaction supported so
734:18 - um you know the thing is is that
734:20 - it looks like service bus wins in all
734:22 - categories but again remember that
734:24 - storage queue is simple and but because
734:26 - it's simple by design it can handle
734:28 - millions of messages and if you don't
734:30 - care about first and first out it makes
734:32 - sense um so you really have to decide in
734:34 - your use case there probably is a
734:36 - pricing difference which we i think that
734:38 - we do cover in this course but you know
734:40 - hopefully you get an idea that these two
734:42 - services do the same thing but in
734:44 - different ways okay
734:46 - [Music]
734:51 - hey this is andrew brown from exam pro
734:52 - and we are taking a look at azure queue
734:54 - storage which is a simple uh message
734:56 - queue that allows you to exchange cloud
734:58 - messages it's used for application
735:00 - integration between services and it
735:03 - allows that via an authenticated https
735:07 - endpoint or protocol and it can accept
735:09 - messages of 64 kilobytes in size so the
735:12 - q storage is a service or a product
735:16 - underneath the azure storage account so
735:18 - you first have to create a storage
735:19 - account uh and then from there you
735:21 - create your queue but you'll be using
735:22 - the same access keys and connection
735:24 - strings like the rest of the other
735:25 - storage account resources
735:27 - there are three ways of handling
735:29 - messages for azure storage
735:31 - azure queue storage the first is peak
735:34 - and as it says it will retrieve or look
735:36 - at a message in the queue without
735:37 - removing or locking it you have delete
735:40 - so that will delete a message
735:42 - that is being processed and received so
735:44 - that is a message in the front of the
735:46 - queue uh but it gets locked temporarily
735:48 - while it is being used okay um and just
735:52 - to kind of give you a visual of how
735:53 - simple it is to use this queue you can
735:55 - uh in
735:57 - the ui in the azure portal you can open
735:59 - it up write a message set an expiry and
736:02 - just say okay
736:03 - but the way you're generally going to be
736:05 - using it is not through
736:07 - uh the ui because these are for
736:09 - application integration that means
736:10 - you're going to be
736:11 - writing either sdk code or utilizing the
736:14 - cli uh and i believe that azure queue
736:17 - storage is uh one of the ones where you
736:19 - can actually use cli we do cover that as
736:21 - the difference between um the other
736:24 - queueing service that you can use in
736:25 - azure um but here's an example of the
736:28 - azure sdk it's a python example so just
736:30 - getting my pen tool out here it is a bit
736:32 - brief but the idea is that we have our q
736:34 - client we have our connection string url
736:36 - which we're getting from our storage
736:38 - account we're putting the name of the
736:40 - queue and then we just send a message so
736:42 - it's that simple and that is a python
736:44 - example let's just talk about some of
736:45 - the components within a queue there's
736:48 - not a lot here
736:49 - so it's not too complicated but the idea
736:51 - is you have a queue this could be
736:52 - accessed by using a url format of the
736:55 - following so you'll have your storage
736:56 - account here
736:58 - qcor windows.net and then the queue name
737:01 - um would this show up in the exam
737:03 - probably not but it's generally good to
737:05 - know just in case because azure exams
737:08 - are very tricky
737:09 - and then we have also this url here as
737:13 - an example so
737:14 - i think it's reflecting the example on
737:16 - the right hand side here so the idea
737:18 - here is that we have my accounts this is
737:20 - my account and then we have images to
737:22 - download so images to download would be
737:24 - one particular
737:25 - queue okay so just to kind of hit home
737:28 - that url there
737:30 - we have a storage account so this is
737:31 - required
737:33 - in order for us to make the queue as we
737:34 - mentioned you have the queue itself this
737:36 - contains a set of messages the queue
737:38 - names must be in all lower case that's
737:40 - just one of those
737:41 - uh interesting um one of those
737:44 - interesting use cases or edge cases
737:47 - messages can be up to 64 kilobytes in
737:49 - format so it's very small so this is
737:51 - where you might want to use the other
737:53 - queueing service uh within azure which i
737:55 - can't remember but we do cover it when
737:56 - we get to that section uh and there is
737:58 - some um versioning that's a bit
738:00 - different so if you're using uh before
738:02 - 2017 the ttl on it is a max of seven
738:06 - days if you're using uh after 27 2017 um
738:11 - it can be any positive number or
738:12 - negative one indicating that they don't
738:14 - expire and it doesn't say um i guess it
738:18 - can be whatever number you want to be i
738:20 - don't know what the maximum is but
738:22 - i mean what's important to know is that
738:23 - the old one gives you seven days so
738:25 - you're going to be wanting to use the
738:26 - latest version to have the most
738:28 - flexibility the default time to live is
738:30 - seven days if the parameter is not
738:32 - specified which is totally fine let's
738:34 - take a look at some of the cli commands
738:36 - because uh you know what i'm making this
738:37 - course is specifically for the az204 uh
738:41 - which means developers so you're getting
738:43 - lots of hands-on stuff so you should
738:44 - really know these cli commands
738:46 - we do cover them in the follow along
738:47 - here but the idea is we have uh the az
738:52 - storage because we're using storage
738:53 - count message
738:54 - actions so that's going to be
738:55 - automatically for the queue
738:57 - so we have clear so delete all messages
738:59 - from the specified queue
739:01 - uh delete to delete a specific message
739:03 - get so retrieve one or remember messages
739:05 - from the front of the queue
739:07 - peak so retrieve one or more messages
739:09 - from the front of the queue but do not
739:10 - remove them do not change their
739:12 - visibility do not lock them put so add
739:14 - new messages to the back of the message
739:16 - queue
739:17 - update updates the visibility time of a
739:19 - message so these systems are very very
739:21 - very simple but they're also highly
739:23 - scalable which is really great
739:25 - um but yeah there you go
739:27 - [Music]
739:31 - hey this is andrew brown from exam pro
739:33 - and in this follow along we're going to
739:34 - learn all about storage queues so what i
739:36 - want you to do is log into the azure
739:38 - portal and at the top here type in q and
739:41 - so that's going to show you it's going
739:42 - to bring you to storage accounts because
739:44 - it is not a separate service it's uh
739:47 - underneath storage accounts so what
739:50 - we'll do here is we'll create ourselves
739:52 - a new storage account
739:53 - so we'll hit new or create there give it
739:56 - a moment to load and we're going to
739:58 - create ourselves a new namespace i'm
739:59 - going to call this one az204
740:02 - um az204
740:04 - azure storage
740:08 - account
740:10 - uh actually we'll just say um
740:12 - storage account queuing or queue
740:15 - we'll say okay
740:17 - there this will be called uh az204
740:22 - storage account queue
740:25 - and it doesn't like
740:29 - the hyphens maybe
740:32 - there we go and we'll let it launch in
740:34 - whatever region it wants to here it's
740:35 - going to be e
740:36 - east u.s for performance we'll stick
740:39 - with standard we'll stick with a
740:40 - redundancy of geo-redundant storage it's
740:42 - not going to matter for those options
740:44 - for using the queue we'll go to advanced
740:46 - to see if there's anything interesting
740:47 - there
740:48 - nothing in particular i was just
740:50 - checking if there's anything for the
740:51 - queue so we'll go ahead and hit review
740:53 - and create we'll scroll on down and hit
740:55 - create and we'll give that a moment to
740:58 - create
740:59 - so
741:00 - uh in order to
741:01 - utilize the queue there isn't much in
741:03 - the ui that we can do in the portal and
741:06 - so we are going to need to utilize the
741:08 - azure cli
741:09 - so
741:10 - um you know there's a few ways we can go
741:12 - about this um we could launch up visual
741:15 - studio code but since it's all cli
741:17 - driven what i'm going to do is go ahead
741:19 - and actually open up cloud shell and
741:21 - that's the way we're going to interact
741:23 - with it now this is your first time
741:24 - spinning up cloud shell it might ask you
741:27 - to create a storage account so just let
741:29 - it do that
741:30 - and now this resource is complete we'll
741:32 - click into here
741:34 - and on the left hand side we're going to
741:36 - make our way over to cues
741:39 - and i'm going to create a new queue i'm
741:40 - going to call that my cue and we'll hit
741:42 - ok
741:44 - so
741:45 - for this
741:47 - you know there wasn't really a follow
741:48 - along in the microsoft documents so i
741:51 - just kind of made my own
741:53 - and so for that information i have a
741:56 - repository
741:59 - and you'll find this throughout the
742:00 - course
742:00 - which is call i'm just looking up here
742:02 - on the side here
742:07 - the developer associate so some of these
742:09 - are reworkings of microsoft ones or
742:11 - they're my own creations so this one in
742:12 - particular which is azure storage queue
742:15 - is a bit more unique and so the things
742:17 - that we're going to need to do is we're
742:19 - you're going to need to have a
742:20 - connection string in order for us via
742:22 - the cli to connect it because all the
742:23 - commands we're using like
742:26 - put peak get all these things are going
742:29 - to rely on it in terms of what we want
742:32 - to run i'm going to type in azure
742:36 - q
742:37 - message cli because this is mostly what
742:39 - you'll want to know how to use is how to
742:42 - interact with messages there is a bunch
742:44 - of different cli commands um for
742:47 - azure storage like working with the
742:48 - queue and stuff like that but i'd rather
742:49 - just work with messages because that's
742:51 - the only part that we can't do here and
742:52 - that's where we need to understand the
742:54 - difference between peak get
742:55 - put and things like that so
742:59 - the first thing we'll do is get a
743:00 - connection string url and i believe that
743:04 - is for the queue so we'll go into queue
743:05 - here click into the queue
743:07 - because sometimes it is
743:10 - the access key that is um at the storage
743:12 - account level and sometimes
743:14 - like if you go here sometimes it is
743:17 - so there's keys here sometimes it's the
743:19 - cue
743:20 - so i think
743:23 - it is uh the q key that we want uh and
743:26 - if it isn't we'll find out very shortly
743:28 - here if we made a mistake so what we'll
743:29 - do is we'll go back to our resources
743:31 - here we'll click into the queue and in
743:33 - the queue again and we'll look at access
743:35 - policies and so we can add a policy here
743:38 - nope you know what it was it absolutely
743:40 - is the
743:42 - storage account that we need so we go
743:44 - down here and we type in access
743:47 - and we have access keys
743:50 - so here it is and between different
743:52 - azure services sometimes this is called
743:54 - access keys and sometimes it's called
743:55 - something else we're going to discover
743:57 - that as we do other follows just
743:59 - understand this is not always consistent
744:01 - based on
744:02 - the ui but what we need is a connection
744:04 - string
744:05 - so we'll click show up here
744:07 - and what we'll do is grab this string
744:09 - and so what i want to do is type in
744:11 - export
744:13 - q connection string
744:16 - equals and then double quotations very
744:18 - important that you put those double
744:19 - quotations
744:21 - because if you don't this equals will
744:23 - terminate here and it will only think
744:24 - this is a part that is set here so i'm
744:26 - going to hit enter and so that's going
744:28 - to set us an environment variable uh for
744:30 - this so if i did env hyphen grep and put
744:33 - in this
744:35 - this is how we can check if it was set
744:36 - correctly
744:38 - okay if you've never t have you ever
744:40 - done this if you type in env that shows
744:41 - all of your environment variables and so
744:43 - all grep is doing is saying find the one
744:45 - that's called this i could even do it
744:46 - like this
744:48 - right partially and it would return it
744:50 - here so that's one thing the next thing
744:52 - is we need the queue name so we named it
744:54 - i believe my queue so if we go back to
744:56 - our storage accounts here
745:00 - and
745:02 - let's just click back on here such a
745:04 - pain sometimes clicking around here
745:05 - we'll go to cues and we named it my cue
745:08 - so i'm going to set export q name equals
745:11 - and we don't need double quotations here
745:13 - but i'm going to do it anyway my q will
745:16 - make sure that is set correctly so we'll
745:17 - type in grep
745:18 - cue name and there it is
745:21 - so some of the first commands we'll want
745:22 - to use is things like put because put
745:25 - will allow us to enter stuff in so we
745:28 - said that there are these messages here
745:30 - so we'll go to the put command here you
745:32 - can see there's a lot of options when
745:33 - you want to know what you need to put in
745:35 - all i do is just copy the base thing and
745:37 - paste it in
745:39 - and then it'll just tell us
745:42 - so i'll just give it a moment here
745:44 - and so complains it says
745:46 - we need um the queue name we need the
745:48 - content
745:49 - uh and i already know that we we need
745:51 - the connection string url even though
745:52 - it's not saying it there because it
745:54 - might say hey you need actually down
745:56 - here it says connection string
745:58 - account name things like that
746:00 - so um just to make this easier i'm gonna
746:02 - go open editor here and this is a
746:04 - terrible terrible editor but we're
746:06 - working with uh like just some text here
746:08 - it makes our lives a little bit easier
746:10 - so what i want to do is just type it up
746:12 - here
746:13 - before we paste it and of course this is
746:14 - all the repository but i'm just doing it
746:16 - by hand and i recommend that you do it
746:18 - by hand like i'm doing so that you have
746:20 - a better chance to commit to memory so
746:22 - we type in azure storage message message
746:24 - put i'm going to do
746:26 - a backslash so we can do multi-line here
746:28 - we do connection string
746:31 - equals and we set that as an environment
746:33 - variable called uh
746:36 - q
746:37 - connection string when we do this dollar
746:39 - sign that allows us to access the
746:40 - environment variable
746:42 - that we said earlier
746:44 - and we did that is because we have to
746:45 - run a series of commands and it was just
746:48 - a lot easier to pass this in
746:50 - consistently as opposed to passing in
746:52 - that raw string every time so we need to
746:55 - provide the queue name so this would be
746:57 - q underscore name
747:00 - and then we need to provide it content
747:02 - so we'll say hyphen hyphen content
747:04 - equals and i'm gonna say hello world
747:09 - and so we will copy the contents there
747:13 - i'm going to just type clear here so i
747:14 - can see what i'm doing
747:15 - we'll go ahead and paste that on in
747:17 - there we'll hit enter
747:20 - and uh yeah so it is now in the queue
747:23 - now one thing that i was not able to
747:25 - figure out was how do you observe how
747:27 - many questions are in the queue i'm not
747:29 - exactly sure so if we go into here i'm
747:31 - not sure if we get any statistics
747:33 - oh well we have a message right here so
747:35 - we can see there is one in the queue but
747:37 - what i was trying to say is that when i
747:39 - was using the cli i was trying to find a
747:40 - cli command uh to find that and i just
747:42 - never did but it's nice to see that we
747:44 - can see it visible here
747:47 - in the actual console here i want to
747:49 - have more than one message here because
747:52 - i want to show you
747:54 - how visibility works
747:56 - and so what we'll do is just paste
747:58 - command v there is no if you notice
747:59 - right click there's no paste here which
748:01 - is kind of weird and so i'm going to do
748:02 - here hello mars
748:05 - as our second one
748:06 - and we'll copy this here copy it paste
748:08 - it on below hit enter
748:12 - and so now we have two queued up if we
748:14 - refresh we can see we have two messages
748:16 - in our cube we can even add a message
748:18 - here if we wanted to so we could say
748:20 - hello jupiter
748:24 - and hit uh
748:25 - [Music]
748:27 - maybe i have to scroll down here
748:29 - oh no it vanished i gotta do it again
748:35 - maybe bring it down all the way down
748:36 - here add message there we go hello
748:38 - jupiter we'll say okay
748:41 - and notice that we have some options
748:42 - there where we can set the expires in x
748:44 - amount of time or whether it never
748:46 - expires and things like that
748:48 - but anyway what we'll do is um
748:52 - now we should probably attempt to peak
748:54 - or get so what we'll do is go back to
748:57 - the top here
748:59 - because i think they list them out so
749:00 - we have guests so retrieves one or more
749:02 - message from the front of the queue and
749:05 - then you have peaks it retrieves one
749:06 - armor message from the front of the cube
749:07 - but does not alter the visibility of the
749:10 - message so when we say visibility the
749:12 - idea is that when somebody accesses a
749:14 - message
749:15 - they're now looking at it right and so
749:17 - nobody else can read it and the reason
749:19 - why that happens is to avoid people two
749:22 - people doing the same job by accident
749:25 - but if we use peak that means that um it
749:28 - doesn't become unvisible
749:30 - um it will just remain the same so let's
749:32 - do peak first
749:34 - and so what we'll do is we'll just copy
749:36 - this here
749:38 - and paste this in
749:41 - oh we have to control v there
749:43 - and we'll just get rid of the content
749:45 - line because we don't need to pass
749:46 - content we still need the cue we still
749:47 - need the connection string
749:49 - and i'm going to change this to peak
749:52 - and so what peak should do is it should
749:53 - return us what's on the front of the
749:54 - queue so this is the top up here
749:58 - we'll go ahead and copy it then we'll
749:59 - right click and paste down below we'll
750:00 - hit enter and so we get hello world if
750:03 - we were to hit up and run it again we
750:05 - would still get hello world because it
750:07 - has not been marked as
750:08 - uh invisible right it's still visible so
750:11 - now let's go ahead and change this to
750:13 - get
750:18 - oh we got to paste ctrl v
750:20 - so we do get here
750:23 - and
750:24 - copy can
750:26 - control c or command c depending on what
750:27 - you're using and we'll paste that in
750:29 - there and hit enter
750:31 - and so now it says hello world as it did
750:33 - before both times we ran hello world it
750:35 - was always returning the same one
750:37 - okay notice it says dq count is one so
750:40 - we'll go here refresh
750:41 - and notice that hello world is not
750:44 - appearing now
750:46 - right so it's because it's not visible
750:48 - it's still there it's just not visible
750:50 - right
750:52 - um and so the next thing we can do is if
750:54 - we
750:55 - hit enter again
750:57 - now types in hello mars
751:00 - okay
751:02 - and uh the next thing we can do here is
751:06 - uh clear the queue if we want so that's
751:07 - probably something that we should do
751:09 - so if we were to go here and
751:14 - copy this
751:17 - type in clear
751:25 - and hit enter
751:26 - that should clear the entire queue
751:30 - so hopefully that gives you an idea of
751:31 - how the queueing system works
751:34 - of course you can use the sdk so like if
751:36 - we were to type in
751:38 - azure storage queue
751:41 - documentation
751:47 - and we were to go to tutorials there's
751:48 - probably samples there's probably like
751:50 - examples i always go to javascript
751:51 - because it's the easiest one to read
751:54 - and if we go down to messages here
751:57 - send a message
751:59 - not really great that they don't have
752:00 - this in line but the way it works is you
752:03 - probably import the library at the top
752:06 - so you'd install with npm install azure
752:09 - storage queue you have the client the
752:11 - credentials and then down below here
752:13 - you're creating your message
752:15 - i guess you have to create the client
752:17 - queue first up here
752:19 - and then you pass along your message
752:21 - sent and then it says whether it's been
752:22 - sent or not here you can see that
752:24 - they're using peak
752:25 - right very similar receive message all
752:28 - very similar here
752:30 - so hopefully that gives you
752:32 - kind of an idea i think like forget or
752:35 - sorry like um notice like it's called
752:37 - receive message as opposed to get so in
752:39 - the cli
752:41 - uh we did get but this is really receive
752:44 - right so just so that translates over
752:47 - these queues are extremely simple but
752:49 - they're designed to be extremely simple
752:50 - when we look at service bus you'll see
752:52 - that it's a lot more complex but one of
752:54 - the key differences
752:56 - at least pragmatically with service bus
752:59 - and storage queue is that service bus
753:02 - allows you to use the cli to insert
753:04 - messages where when we do service bus we
753:06 - have to write code using the sdk and we
753:09 - will do that
753:10 - but yeah that's all there is really to
753:12 - this one that i want to show you what
753:13 - we'll do is make our way over to our
753:14 - resource groups
753:16 - i'm going to just close off our shell
753:18 - here we're going to look for the one we
753:20 - just created here so this is azure
753:22 - storage account cue
753:24 - we'll go ahead and delete that
753:28 - delete
753:32 - and it should all be good of course
753:34 - always double check five minutes from
753:36 - now make sure that stuff is deleted
753:37 - because azure is notorious for sometimes
753:40 - not deleting things or you think that
753:41 - things are being deleted but this won't
753:43 - show up immediately it will take some
753:44 - time to propagate through their servers
753:47 - but there you go
753:49 - [Music]
753:53 - hey this is andrew brown from exam pro
753:55 - we're looking at azure service bus so
753:57 - azure service bus is a fully managed
753:59 - enterprise message broker that allows
754:00 - you to publish and subscribe to topics
754:03 - and queue messages it can scale with
754:06 - your applications with asynchronous
754:08 - messages and built-in integration with
754:10 - azure services so here is an example
754:13 - of a service bus and so you know the way
754:16 - it works is you have producers you have
754:19 - consumers and so producers are sending
754:21 - messages or events into an event bus the
754:24 - event bus is designed to broker that
754:27 - information so
754:28 - pass it along to the ones that are
754:30 - subscribed so these applications which
754:32 - are consuming are subscribed to specific
754:36 - messages and they pull data to um get
754:39 - them into those and target end
754:42 - applications some things we need to know
754:44 - is you can handle single or batch
754:46 - messaging load balancing messages topic
754:49 - subscriptions manage uh managed sessions
754:51 - and transactions with guarantees that it
754:54 - compiles with industry standards and
754:55 - protocols like the advanced message
754:57 - queueing protocol so amqp
755:00 - java message services so jms
755:03 - and
755:04 - other protocols there so they're just
755:06 - saying like yeah if you're already using
755:07 - very particular protocols you can
755:09 - integrate with those let's talk about
755:11 - key concepts for service bus so the idea
755:14 - is that you have a namespace which works
755:16 - like a server with nqs and topics you
755:19 - have cues which contain the messages you
755:21 - have senders who send the messages
755:23 - receivers who receive the messages
755:26 - topic which is a queue with multiple
755:28 - receivers that work like a queue a
755:30 - subscription is a receiver in a topic a
755:33 - batch is a group of messages
755:35 - a safe batch validates if each message
755:38 - can be included in the batch
755:40 - sessions allow you to use first and
755:42 - first out and group your messages in a
755:44 - queue
755:45 - peak returns a message to the queue
755:47 - without removing it dead letter q a
755:49 - queue for messages that were unable to
755:51 - be delivered through the normal queue
755:53 - peak and lock so this retrieves a
755:54 - message from the queue without removing
755:56 - it and locks it so other receivers
755:58 - cannot receive it receive and delete so
756:00 - receives and deletes messages from the
756:02 - queue
756:03 - auto delete on idle so sets a time span
756:06 - to delete the queue if not used
756:08 - duplicate deletion detection history so
756:11 - checks if the message was not sent
756:12 - earlier before sending it a message so
756:14 - there's a lot of stuff here
756:16 - but don't worry we're going to kind of
756:17 - cover it the most important stuff here
756:19 - in the upcoming slides and in the fall
756:21 - long okay
756:22 - [Music]
756:27 - let us talk about namespaces for azure
756:29 - service but so simply put namespace is a
756:32 - container for all messaging components
756:34 - so cues and topics but there's uh you
756:37 - know a few options here i don't have a
756:38 - visual but there's some
756:40 - text we have to go through here to
756:42 - understand some of the things that
756:43 - namespaces do so a single namespace can
756:46 - contain multiple cues and topics and
756:49 - namespaces are often used as application
756:51 - containers so a service bus namespace is
756:55 - a capacity slice of a large cluster made
756:58 - up of a dozen of all active virtual
757:01 - machines that you control
757:02 - it may optionally span three
757:05 - azs or availability zones in azure
757:09 - you get all of the
757:10 - the benefits of running the message
757:12 - broker at massive scale in terms of
757:14 - availability and robustness a service
757:16 - bus is serverless messaging so you don't
757:18 - need to worry about the underlying
757:20 - complexity which is kind of interesting
757:22 - because they're mentioning about
757:23 - spanning across availability zones and
757:26 - virtual machines but uh you know it's
757:29 - just a component
757:30 - that's you know you don't have to worry
757:32 - about the management of it but you do
757:33 - need to know a little bit about how it
757:35 - works underneath but there you go
757:40 - [Music]
757:42 - hey this is andrew brown from exam pro
757:43 - and we are taking a look at cues for
757:45 - azure service bus so there's two types
757:47 - of uh messaging systems in azure service
757:50 - bus why they didn't make these two
757:51 - separate services i have no idea but
757:54 - one is for messaging and one is for pub
757:55 - sub and and the one that is for
757:57 - messaging is called cues so cues are
757:58 - used to send and receive messages
758:01 - messages are stored in queues until the
758:02 - receiving application is ready to accept
758:04 - and process them messages and cues are
758:07 - ordered and timestamped on arrival once
758:09 - accepted by the broker the message is
758:10 - always held durably in triple redundant
758:12 - storage across uh multiple azs if
758:16 - its namespace is zone enabled so if you
758:19 - don't enable zone enabled at the name
758:21 - space level you're not going to get that
758:22 - kind of durability or redundancy service
758:25 - bus never leaves messages in memory or
758:27 - volatile storage after they've been
758:28 - reported to the client is
758:30 - accepted
758:31 - and messages are delivered in pull mode
758:34 - only delivering messages when requested
758:36 - that's the key difference between a
758:37 - queue and pub sub cue uh the receiver
758:41 - you know the application has to pull say
758:43 - hey are there any messages in the queue
758:44 - and that's how they get them in a pub
758:46 - sub model they just get it sent to them
758:48 - like a
758:49 - subscription like a newsletter
758:51 - subscription to your door let's take a
758:53 - look at the queue configuration itself
758:56 - so you can see we can set the delete max
758:59 - delivery count things like that the
759:00 - queue size
759:01 - important things to note is the ttl so
759:03 - how long a message will stay in the cube
759:05 - before it's removed from the queue or
759:06 - removed or moved to dead letter q
759:09 - you can enable dead lettering down below
759:12 - you can enable partitioning here you can
759:14 - set the lock duration very similar to
759:16 - what we'll see for uh pub sub i think at
759:18 - the subscription level but there you go
759:21 - [Music]
759:26 - hey this is andrew brown and we are
759:27 - taking a look at topics so topics can be
759:30 - used to send and receive
759:32 - messages and the idea here is you have a
759:34 - queue which is often used to
759:36 - facilitate point-to-point communication
759:39 - and topics are useful in publish to
759:42 - subscribe so one-to-many communication
759:45 - so if you want one-to-one communication
759:47 - use cues if you want pub sub then you
759:50 - use topics and both these options are
759:52 - available for azure service bus so
759:54 - topics are not available at the basic
759:57 - pricing tier you need to have standard
759:58 - or premium so if you're trying to find
760:00 - that option for pub sub that's where
760:02 - we're going to have it remember that we
760:04 - said that
760:05 - queues use a poll model well topics use
760:08 - a push model so you don't have to
760:10 - request it just gets pushed out to you
760:12 - because they're subscribed to it like
760:14 - having a newsletter delivered to your
760:15 - door so multiple independent
760:17 - subscriptions can be attached to a topic
760:20 - and work in the same way as cues from
760:22 - the receiver's side a subscriber to a
760:24 - topic can receive a copy of each message
760:26 - sent to that topic subscriptions are
760:28 - named as entities and so here's an
760:31 - example of sender and receiver you can
760:33 - define rules on a subscription
760:36 - a subscription rule has a filter that
760:38 - specifies a condition for a message to
760:40 - be copied into the subscription as well
760:42 - as an optional action that modifies
760:44 - message
760:46 - metadata so let's take a look at what
760:48 - the form is for topic so the idea is you
760:50 - name your topic you have a max size
760:52 - between one to five gigabytes you can
760:54 - have a time to live so after that period
760:57 - of time those those messages or event
761:00 - data is just goes poof so it's just gone
761:03 - you can
761:04 - avoid duplication by turning on enabled
761:07 - duplication or duplicate detection you
761:09 - can enable partitioning and partitioning
761:11 - is intended for scale so if you have a
761:14 - lot of messages
761:15 - you might need to partition
761:18 - based on your volume okay
761:20 - [Music]
761:24 - let's take a quick look at subscriptions
761:26 - for azure service bus so i just pulled
761:28 - up the form here because a subscription
761:30 - is just the um
761:32 - the application that is receiving that
761:33 - information so you just define one
761:35 - in the interface here but there are some
761:37 - values that i want to point out such as
761:39 - max delivery count which can be between
761:40 - one to two thousand uh this managed
761:43 - session so with sessions enabled the
761:44 - subscription can guarantee first in
761:47 - first out so fifo delivery messages so
761:50 - that is where that gets turned on you
761:52 - can also enable dead lettering here
761:54 - um and you can have a lock duration and
761:57 - adjust that there which you know i
761:59 - didn't highlight here but it's here
762:01 - there but we will see these things a
762:03 - little bit closer when we do the fall
762:04 - longs but yeah a subscription is you
762:07 - know
762:08 - an application that is
762:10 - receiving information getting pushed
762:12 - information to it that it will use okay
762:15 - let us explore the pricing for azure
762:17 - service bus so they have three different
762:19 - pricing tiers basic standard and premium
762:22 - the more expensive tiers provide more
762:24 - functionality which i would hope that
762:25 - would always be the case for most
762:26 - services but on the right hand side we
762:29 - have our available pricing uh and you
762:31 - can see it's based on
762:33 - a metric so like mu's per per month and
762:36 - things like that or operations per month
762:38 - and so here is the chart so let's take a
762:40 - look at what the differences are so i
762:42 - highlighted topics in yellow to show you
762:44 - that it is not basic it's not in the
762:47 - basic tier you have to have standard in
762:49 - order to utilize that and you know for
762:52 - your purposes as a
762:54 - person learning as a service bus you
762:56 - absolutely need to
762:58 - uh give topics and queues a go because
763:00 - qs is the classic messaging model and
763:02 - topics is pub sub and the only way
763:04 - you're going to do that is by using
763:06 - standard which we will do in the follow
763:07 - alongs but notice in basic we don't get
763:10 - topics transactions deduping sessions
763:13 - for to send via no resource isolation
763:17 - geo disaster recovery java messaging
763:19 - support az support and then some of
763:21 - those aren't supported in standard but
763:24 - then in
763:25 - [Music]
763:27 - premium we get basically everything and
763:29 - then notice like the big bump in terms
763:30 - of the message size between basic
763:33 - standard and premium so
763:35 - again the thing i really want to
763:36 - remember is that
763:38 - in basic there are no topics okay
763:44 - [Music]
763:49 - all right let's take a look here at dead
763:51 - letter q this is just a general concept
763:54 - that is common for messaging's pub sub
763:57 - systems
763:58 - and is enabled in azure service bus for
764:02 - both queues and pub sub but a dead
764:04 - letter queue is a service implementation
764:06 - to store messages that fail to deliver
764:09 - and common reasons for failed messages
764:11 - would be things like message that is
764:12 - sent to a queue that does not exist
764:15 - the queue length limit is exceeded the
764:17 - message length limit is exceeded the
764:19 - message is rejected by another queue
764:21 - exchange
764:22 - the message reaches a threshold read
764:25 - counter number because it's not consumed
764:27 - sometimes this is called a blackout
764:29 - queue or backup queue sorry
764:32 - the message expires due to per message
764:34 - ttl so time to live
764:36 - the message is not processed
764:38 - successfully so dead letter queues hold
764:40 - messages that uh that have failed to
764:43 - deliver a dead letter queue could be
764:45 - used for things like monitoring failed
764:47 - attempts requesting failed attack
764:49 - re-cueing failed attempts so try it
764:51 - again or trigger a follow-up action so
764:54 - maybe a remediation or a response so
764:57 - very useful feature but there you go
764:59 - [Music]
765:04 - let us talk about azure service bus
765:06 - command cli or cli commands because if
765:09 - you are doing the az204 the developer
765:11 - it's very important to know these these
765:13 - commands because they may ask you them
765:16 - so here i have a bunch of example
765:18 - commands so just quickly looking at this
765:20 - here we have creating a resource group
765:23 - creating a service bus namespace a queue
765:26 - an authorization rule
765:28 - and so it's pretty straightforward you
765:29 - know az service bus etc let's take a
765:32 - look at
765:33 - some commands we should know or sub
765:35 - commands i should say so
765:37 - when we say command the command is
765:39 - service bus and the sub command is what
765:40 - follows that so we have ones for geo
765:43 - recovery alias so we want to set the geo
765:45 - recovery for it we don't have an example
765:47 - here on the right hand side for
765:49 - migrations uh for namespace for queue
765:53 - for topic and so on the right hand side
765:55 - there we have one for namespace we have
765:57 - them one for creating a queue
766:00 - but you'll notice here that we don't
766:01 - have anything that kind of manipulates
766:03 - or adds messages to cues so the azure
766:06 - service bus does not have cli commands
766:07 - to send messages to the keyword topic
766:10 - unlike azure storage queue which i found
766:12 - out which was kind of interesting so to
766:14 - send messages to the queue you need to
766:16 - use an sdk like node.js so you'd install
766:18 - it
766:19 - and then you'd have to
766:22 - write some code and we do do that in the
766:23 - follow along but i just want you to
766:24 - remember that
766:26 - azure service bus you cannot send
766:28 - messages to the queue or topics via the
766:31 - cli
766:32 - and you know that kind of sucks because
766:35 - you know if you want to do some easy
766:36 - test uh it's unfortunate that
766:38 - functionality is not there but anyway
766:40 - that's the difference between that and
766:42 - azure storage queue so there you go
766:49 - hey this is andrew brown from exam pro
766:50 - and we are going to take a look at
766:52 - service bus so what i want you to do is
766:53 - type in service plus at the top here
766:55 - what's interesting is that this is the
766:56 - old icon they have a new icon so just
766:59 - realize that there's some
766:59 - inconsistencies there and that's not my
767:01 - fault that's azure's fault
767:03 - but the first thing we need to do is
767:04 - create a namespace because a service bus
767:06 - is kind of like you know storage account
767:08 - where you can have a variety of
767:10 - different kinds of storages well you can
767:11 - have uh more than one type of messaging
767:14 - system and so we have our traditional
767:16 - one uh like event messaging similar to
767:20 - storage cube but with first and first
767:21 - out functionality and we have pub sub
767:24 - via topic so what you'll do is create a
767:26 - new service bus namespace
767:29 - and i'm going to create a new resource
767:31 - group and call this the az204
767:33 - um
767:34 - service bus
767:37 - and we'll say message or sorry cue
767:39 - because we're going to do a queue and
767:40 - then we're going to do a topic
767:41 - separately
767:42 - and for this i'm going to call the
767:44 - namespace
767:46 - service bus queue to keep it simple
767:49 - we'll let it launch wherever it wants to
767:50 - launch and notice there are multiple
767:52 - pricing tiers
767:53 - depending on the tier affects the
767:55 - functionality so if we do basic we're
767:58 - only going to have access to queue we're
767:59 - not going to have access to
768:01 - um
768:02 - topics and so this is totally safe and
768:04 - fine to do
768:05 - like even if we did premium it's fine
768:07 - because it's based on your consumption
768:09 - it's not based on um you know you're
768:11 - just having it holding around so we'll
768:13 - get the basic one here and we'll go to
768:15 - networking i don't think there's
768:16 - anything interesting there we'll go
768:17 - ahead and review and create
768:20 - and we'll let that create click create
768:21 - again
768:23 - and it's deploying as that is deploying
768:25 - which will not take too long what i've
768:27 - done is set up a private repository here
768:30 - you'll probably see me use this
768:31 - throughout the course it's literally an
768:32 - empty repository because i already have
768:34 - the code done i've been doing the
768:35 - follow-alongs and documenting them here
768:37 - in the free azure developer associate
768:39 - but when you're doing follow-ons with me
768:41 - you should do them with uh do them from
768:42 - scratch and then if you need to you can
768:44 - reference the stuff here so um i have
768:47 - this separate repository i have a git
768:49 - pod account which has a free tier you
768:51 - can totally do this in your own visual
768:53 - studio code on your little local machine
768:55 - the reason i'm doing a git pod is i
768:57 - always want to show you how to set up
768:58 - the cli and those other tools
769:00 - and when you launch git pod it gives you
769:02 - a blank environment so i'm just going to
769:05 - launch that up there as that's going
769:07 - we'll go back here and take a look and
769:08 - see if this is ready just hit refresh
769:10 - here it is still going
769:13 - but we already have our environment and
769:15 - while that is going in the background i
769:16 - want to go install the azure cli so we
769:20 - don't even have a single file here i'm
769:21 - just going to say readme.md so i can see
769:23 - what is going on here
769:25 - maybe we'll just dump things in here as
769:26 - we go i'm going to go get the azure
769:29 - cli
769:30 - linux because this is running linux
769:32 - ubuntu
769:34 - here so something you should always
769:36 - check is like
769:37 - what linux version
769:39 - am i running if you're on windows of
769:41 - course this is going to be different but
769:42 - even windows using the windows subsystem
769:44 - linux is using ubuntu as well so what
769:47 - i'll do here i'll go to the first link
769:49 - nope that's not the one i want maybe the
769:50 - second one
769:53 - there's usually like a command here i
769:55 - can run uh
769:57 - which
769:59 - linux version
770:00 - am i running
770:03 - let's try this one here
770:08 - it's usually like
770:10 - so maybe it says here cat proc version
770:14 - it really does vary based on what you're
770:15 - using and so here i'm going to go file
770:18 - uh or sorry terminal new terminal
770:22 - we'll paste that in there hit enter and
770:24 - so here it says linux 513 ubuntu
770:28 - 11. uh 18 so
770:31 - [Music]
770:33 - i know that this is ubuntu
770:35 - i don't really like that one there
770:37 - um
770:39 - let's try this instead
770:42 - because this just doesn't read very well
770:44 - there we go we're running ubuntu 2004 i
770:46 - already knew that
770:47 - um but i just want to double check and
770:49 - the reason that matters is that when
770:50 - you're installing the cli it might
770:52 - matter what version you're using
770:54 - so we're going to go here and i'm going
770:56 - to go to linux and the instructions
770:59 - might vary this one says 16 18 20. so
771:01 - they're all the same here and we have
771:02 - this one liner here that we'll install
771:05 - and what i'll do here is drop it in here
771:07 - and hit
771:10 - enter not sure if this font is too small
771:13 - so while that's going i'm going to see
771:14 - if i can bump up our font here
771:30 - i'm looking for the terminal font size
771:32 - here terminal
771:35 - let's just say 20 here
771:37 - there we go
771:38 - and so the azure cli should be installed
771:40 - so i'll just type in clear so let's say
771:42 - azure
771:43 - or az to run it
771:45 - looks good to me so we'll type in azure
771:47 - login um i don't want to log in with
771:50 - um that way i want to log in with device
771:52 - so we'll do a z login device because
771:56 - if you're on your regular computer you
771:57 - can just click a button and go to the
771:59 - browser but i'm not gonna be able to do
772:00 - that so i'm gonna have to do a device
772:02 - login
772:04 - device
772:09 - well i'll have to do it the wrong way
772:11 - first to do it the right way
772:12 - so hit enter because the problem is if i
772:14 - go here it's going to go to localhost
772:16 - because it's trying to launch in my
772:18 - local machine
772:19 - so it does that and that's no good and
772:21 - so here it says do the az login use
772:23 - device code okay so that's the one i
772:25 - really wanted to use
772:28 - so hyphen typing use device code enter
772:32 - and that will give us a code so what we
772:33 - do is we will need uh this link here so
772:35 - i'll have to expand that
772:37 - to here
772:42 - and then i'm going to go ahead and grab
772:44 - that code
772:52 - continue
772:53 - let's now go back this will authenticate
772:55 - it'll just take a second here
773:01 - close the tab here there we go maybe i
773:03 - have to close the tab
773:04 - uh and so now i'm authenticated so i
773:06 - should be able to uh do whatever i want
773:09 - um what i need to do next
773:12 - is
773:14 - create ourselves a message queue so
773:16 - we'll go to the resource here
773:18 - and notice here the entities it always
773:19 - says q now if we had
773:22 - uh other than the basic the standard
773:24 - plan then we would see topics here
773:26 - we'll go click into q click create a new
773:28 - queue i'm going to call it my queue
773:30 - we have some options here the queue size
773:32 - can go up to five gigabytes the max
773:34 - delivery count so this is the maximum
773:35 - deliveries uh time to live that is how
773:38 - long they live in the queue before they
773:40 - are dropped out or they are dropped into
773:42 - a dead letter uh a system there we have
773:46 - lock duration so the set the amount of
773:47 - time a message is locked for other
773:49 - receivers you can enable partitioning uh
773:52 - that's pretty complicated but we'll go
773:53 - ahead and create our cue
773:56 - and so this should be pretty darn quick
773:58 - there is our q we'll click into it and
774:01 - you'll notice that there isn't really a
774:03 - way to view messages there's not a way
774:05 - to
774:06 - add messages uh here we have the service
774:09 - bus explorer
774:10 - which
774:11 - i guess technically you can send and
774:13 - receive here i had not noticed this
774:15 - before at least it was not working for
774:16 - me so i suppose we could send a message
774:18 - here saying like hello world
774:21 - this literally wasn't here last time i
774:22 - checked here um
774:25 - and we can go ahead and just hit
774:27 - send
774:32 - okay and notice here it says there's one
774:34 - active message and we can receive it say
774:37 - yes
774:38 - and so
774:39 - it says it received the message it's not
774:41 - showing us the answer
774:43 - so i guess there kind of is something
774:45 - here i guess they're still working on it
774:47 - but uh mostly what we're going to have
774:48 - to do is do things programmatically
774:52 - so that is why we have this account so
774:55 - what i want you to do is open a new tab
774:58 - here we're going to type in azure
775:00 - service bus
775:01 - documentation because we're going to
775:03 - grab some code there modify it make it
775:05 - our own so it's a bit easier to work
775:07 - with so here i'm in the service bus
775:08 - we'll go to tutorials
775:12 - i'm not sure if this one is the right
775:14 - one
775:16 - azure service bus documentation queue
775:24 - it's the same thing here
775:27 - but this doesn't look right
775:31 - what is service bus
775:41 - i mean it is the right page but it had a
775:43 - couple tutorials here
775:44 - uh that i had here so we'll type in like
775:47 - azure service bus uh tutorial
775:50 - topics
775:54 - sometimes things aren't where you think
775:55 - they're supposed to be
776:00 - okay i'll type in service bus
776:07 - seems like the same page again
776:09 - oh it was quick starts sorry so we have
776:12 - tutorials here and then we have quick
776:13 - start so under the quick starts this is
776:15 - where i was finding uh the example code
776:16 - that i thought was okay notice that we
776:18 - could do everything via the cli um that
776:21 - is not that fun but i mean this only
776:24 - does the creation of it doesn't
776:25 - necessarily do sending and receiving
776:26 - messages notice so that we only can use
776:28 - code so we'll use javascript because i
776:30 - think that will be the easiest to use uh
776:32 - so i already have node node comes
776:34 - pre-installed on git pod you'll have to
776:36 - figure that out for yourself on your own
776:38 - machine or you can just use git pod as
776:40 - well because it does have a generous
776:41 - free tier what i'll do is go ahead and
776:44 - paste on in
776:46 - this command it doesn't seem to want to
776:48 - paste today so i'll hit copy
776:50 - and then we'll go back here and go right
776:52 - click
776:53 - enter paste hit enter and so what that
776:55 - will do is install that library
776:59 - if you're not very familiar with node.js
777:01 - package.json is the package manager and
777:03 - this is showing that this requirement is
777:05 - there i want to install one other thing
777:07 - uh called env this will make our lives a
777:09 - lot easier
777:11 - um
777:12 - for
777:13 - node it comes for different things but i
777:14 - just wanted for
777:17 - javascript here so then we'll do npm
777:19 - install emv hyphen save
777:22 - that's just a safe way for us to pass
777:23 - along our environment variables and so
777:25 - now both of these are installed so what
777:27 - we'll do is we'll go back over to this
777:30 - code and we'll scroll on down and they
777:31 - have one called send and they have one
777:33 - called receive so what i'll do is create
777:35 - myself a couple files here so we'll have
777:37 - send.js
777:40 - and we'll have receive.js
777:46 - and then what we'll do is go ahead and
777:48 - copy this is the send code so we'll put
777:50 - this in the send js file
777:55 - and then down below we have the receive
777:58 - code
778:01 - and we will paste that on in there so
778:04 - i'll just make this bigger and we'll
778:05 - take a quick look here at what it's
778:07 - doing so what this does is it imports
778:10 - the sdk for service bus we need to set a
778:13 - connection string we need to set the
778:15 - queue name here is a bunch of messages
778:17 - that we are going to be passing along
778:19 - here we establish a service bus client
778:21 - very common in all sdks to set up a
778:23 - client first then we are creating a
778:26 - sender and then here we are doing uh
778:29 - create batch messages so it's a way of
778:30 - sending messages in batch very
778:32 - efficiently so we have a for loop here
778:34 - and
778:35 - uh so it says there's a batch
778:37 - and then it says try to add the message
778:39 - to the batch if it's not wait until it's
778:42 - ready then send the message
778:44 - okay so pretty straightforward for that
778:47 - code receive is going to be similar
778:49 - so connection string queue name create
778:51 - that client create a receiver
778:55 - and then from there we will set up a
778:57 - handler for the receiver an error and so
778:59 - then we'll subscribe and we will listen
779:02 - for the message and handler so even
779:03 - though
779:04 - we are doing cues it's called a
779:07 - subscription still so just don't get too
779:09 - mixed up with that
779:10 - what i want to do is just make sure that
779:12 - we're passing our environment variables
779:14 - in safely our configuration so this is
779:16 - pretty standard or
779:18 - good best practices when working with
779:19 - any language the idea is you don't want
779:21 - to hard code your values so i'm going to
779:23 - do process env
779:25 - and we'll do connection string here
779:28 - and then we'll do
779:29 - process env
779:31 - cue name this is the way you grab
779:33 - environment variables in javascript for
779:35 - every language it's a little bit
779:37 - different
779:40 - okay
779:42 - and i believe these are the same so i'm
779:45 - just going to go ahead and grab that
779:46 - there like this
779:50 - and i'll paste that on in here
779:53 - and i want to load environment variables
779:55 - so i'll make a new file here called env
779:57 - this is all part of that env dot thing
780:00 - then we're looking at emv dot
780:02 - i'm just pulling that up again here
780:04 - or dot env you're gonna get the right
780:06 - one
780:07 - because we need this line here require
780:09 - dot
780:10 - uh dot env config that will load the
780:13 - environment variables uh it'll load it
780:15 - from that env
780:16 - file so we will go above here and hit
780:19 - paste
780:23 - and then we'll go to the receive here
780:25 - and do this as well
780:29 - and in here we need to define these so
780:31 - i'm just going to copy this so i don't
780:32 - have to type it out by hand we will
780:34 - paste that on in here and so i just need
780:36 - the q q name
780:44 - and connection string we'll just say
780:46 - equals and then equals
780:48 - so our q is called my q
780:54 - and then we need to go grab the
780:56 - connection string
780:59 - so i'm just thinking here this is
781:01 - probably yeah it's at the name space
781:03 - level
781:05 - and we'll go to shared access policies
781:07 - notice it's called shared access
781:08 - policies remember when we're doing the
781:10 - storage key it was like called key
781:12 - access so it's totally different
781:14 - interface this is what i'm talking about
781:15 - where
781:16 - azure is inconsistent we're clicking on
781:17 - the root manage shared access key
781:20 - probably could create your own so it
781:21 - doesn't have full privileges but for
781:23 - this purpose we're just going to use
781:24 - this one on the left hand side we have a
781:26 - primary and secondary we're going to use
781:28 - the primary one
781:30 - and we will go back here and we will
781:32 - paste on that value in
781:35 - so i'll paste that in there notice we
781:37 - don't have to do double quotations here
781:39 - it should
781:40 - already escaping double quotations but
781:42 - we're doing the cli when we did the
781:44 - storage accounts that wasn't something
781:45 - that we could do um so we have these two
781:48 - values here so they should get loaded
781:49 - when we use them
781:51 - and this should all be good so we'll
781:53 - type in node send js and hopefully it
781:56 - just works
781:57 - fingers crossed and so it sent a batch
781:59 - of messages to the queue so we'll go
782:01 - back over to our queue here and see if
782:03 - we can see anything
782:06 - and we'll click into here
782:12 - i'm just trying to see so there's 10
782:14 - active messages that are here right now
782:17 - and so what we'll do is we'll receive
782:19 - all those messages so we'll go up and
782:20 - hit node
782:22 - receive js
782:27 - and so this code is now receiving those
782:29 - messages from
782:30 - the service bus queue and we're just
782:32 - going to wait here because it takes time
782:33 - for whatever reason to
782:36 - finish here but we'll give it a little
782:37 - bit of time here to figure out that it's
782:38 - done
782:43 - still waiting
782:47 - there we go and so
782:48 - that's all there really is to it so
782:51 - that is q and we will do this again but
782:54 - next time with topics
782:55 - so what i want you to do is make your
782:57 - way over back to your resource groups
782:59 - we'll find the one that we just created
783:01 - which was uh this one here
783:04 - az204 service bus queue
783:07 - we'll delete this service group
783:10 - hit delete delete and there you go and
783:12 - as always double check to make sure that
783:14 - you've uh for sure delete that stuff
783:17 - and that's it for
783:19 - service bus q will do topics next okay
783:21 - [Music]
783:25 - hey this is andrew brown from exam pro
783:27 - and we are looking at service bus and
783:29 - this time we're looking at topics so
783:31 - what i want you to do just like before
783:32 - is go to the top we'll type in service
783:34 - bus
783:35 - and we'll go to the service bus service
783:37 - you can still see the old one is there
783:39 - it should be deleting that's how slow
783:42 - this thing is but we'll create a new one
783:44 - and we will create a new resource group
783:47 - and we will call it az204
783:51 - service bus topic we'll say okay we'll
783:54 - name this service
783:57 - bus topic
783:58 - and this time it already exists if it
784:01 - does just dump a bunch of numbers here
784:02 - on the end
784:04 - because it's unique based on that so
784:05 - that's not having a domain name if
784:06 - somebody has it you're gonna have a
784:08 - problem and so here i'm going to go to
784:09 - standard because in order for us to use
784:12 - those additional features we will need
784:13 - to be on the standard plan so we'll go
784:15 - ahead and hit review and create
784:18 - and that'll take a little bit of time to
784:19 - create but while that's going i'm going
784:21 - to
784:22 - launch my environment here so this was
784:24 - the one i was just using a moment ago
784:26 - with github and so what i'm going to do
784:29 - is i'm just going to go back to our repo
784:32 - here i'm just doing it off screen
784:33 - because i don't want to expose all my
784:35 - stuff here
784:38 - and again if you want to you can do this
784:40 - in your local
784:42 - visual studio code i just want to show
784:43 - everything from scratch every time so
784:44 - here's my empty repo with git pod and so
784:46 - i'll just close this one and it will
784:47 - vanish all that code is now gone i'll
784:49 - let get pod this will launch a new
784:51 - environment it's trying to tell me to
784:52 - open the last one nope i'm gonna make a
784:54 - new one for this workspace here and
784:56 - we'll get going here in a moment so this
784:59 - namespace has been created so we'll go
785:01 - ahead and hit create
785:05 - and
785:06 - uh i guess it's still making i thought i
785:10 - already deployed it there but i guess i
785:11 - didn't
785:15 - okay while that's going we'll go ahead
785:17 - and install the azure cli so we'll type
785:19 - in azure cli linux because that is what
785:22 - we're using here today
785:25 - we'll go to linux here we'll scroll on
785:27 - down grab this one liner here to install
785:30 - it i'm going to open up my terminal
785:33 - your terminal might be somewhere else
785:37 - allow i'm going to go ahead and paste
785:38 - that on in there that's going to install
785:41 - our azure cli
785:43 - as that's installing it will be done
785:44 - here in a moment
785:47 - let's see if this is done this is still
785:49 - creating the
785:50 - namespace i think
785:54 - and this is still installing shouldn't
785:56 - take too long
785:58 - while this is going we can start
785:59 - grabbing the code for this so for this
786:01 - we'll type in azure um azure
786:04 - service bus
786:06 - documentation and this i found it under
786:09 - the quick start
786:11 - so we will go to quick start serve
786:13 - topics and services because there's some
786:15 - code here that i want under the
786:17 - javascript
786:19 - and we have one for
786:21 - send to topic so we'll just grab that
786:23 - name there
786:25 - make a new file sent to topic.js
786:28 - and we'll go down here to this other one
786:31 - this is
786:34 - receive from subscription you're going
786:37 - to notice this is very similar to doing
786:38 - a queue
786:39 - the difference is that you can have
786:40 - multiple subscriptions consume
786:43 - the same stuff
786:45 - our cli is done so we'll type in a z
786:47 - login um
786:48 - i can't remember what it is we'll hit
786:50 - enter here i know this is the wrong way
786:52 - but i just can't remember what it is i
786:53 - want use device code that's what it is
786:55 - so i'll go ahead and grab this
786:57 - and i'll just hit ctrl c to exit out of
786:59 - that and then paste that in there
787:02 - and then we'll do it the way that we
787:03 - actually want to do it so then we'll go
787:04 - ahead and grab this for device login
787:08 - we will provide the code as suggested
787:11 - here
787:15 - we will hit
787:16 - continue we will close this
787:20 - and it will say that we are
787:21 - authenticated so we are now
787:23 - authenticated we can use at az
787:26 - or the azure cli
787:27 - um
787:30 - i don't know if we need the cli well we
787:32 - have it anyway at least we had to
787:33 - authenticate so at least that was out of
787:35 - the way i'm not sure we're actually
787:36 - going to use the cli in this one
787:38 - um but what we'll do is go to the
787:40 - resource here
787:43 - and we want to create a topic this time
787:45 - around and i'm going to call this my
787:47 - topic
787:48 - we have a topic size between one to five
787:50 - you have a ttl
787:52 - you can do
787:54 - make sure there aren't duplicates we're
787:55 - gonna go ahead and create this topic
788:02 - okay so just like last time we need to
788:05 - install a couple things
788:07 - so if we go to the top here this should
788:08 - be like an npm install here it is npm
788:10 - install service bus
788:12 - so we'll paste that on in there
788:15 - we'll need our dot env so lookup.env
788:17 - again and we will just install
788:18 - npminstall.env
788:20 - for environment variables hit enter
788:23 - we'll create ourselves a new.env
788:26 - file and we will need to look at what
788:29 - environment variables we are going to
788:30 - need so uh looks like we didn't copy the
788:33 - send to topic content at least i didn't
788:36 - so we will go back over to here and go
788:39 - up here and get sent to topic
788:41 - paste that on in there
788:43 - we'll go all the way to the top and i'm
788:45 - looking for what we need here so here we
788:47 - have one which is
788:48 - connection string whoops
788:51 - do not mean to delete all that so we'll
788:52 - hit process env
788:55 - connection string
788:58 - and we have to spell it right or it's
788:59 - going to have a problem
789:02 - and then we have process env
789:07 - topic name
789:09 - i think in this one it actually also has
789:10 - subscriptions so we'll have to have a
789:12 - third one so i'm just going to copy
789:14 - these two over so i'm to
789:16 - type it 100 times
789:20 - and this one will be
789:22 - process env subscription name you're
789:24 - noticing i'm not having the um semicolon
789:27 - that is optional in javascript so it
789:30 - won't break anything if i don't have
789:31 - them there
789:33 - subscription name
789:36 - and so these are the three that we'll
789:38 - need i'm gonna just split this to make
789:39 - my life a little bit easier and i'm just
789:41 - gonna copy this here paste
789:45 - not exactly how i wanted to paste it but
789:46 - that's totally fine
789:49 - i'm gonna copy
789:50 - this i don't think we made a
789:52 - subscription yet but i know we're going
789:54 - to call it my subscription to make our
789:55 - lives easy
789:56 - so we'll say
789:58 - my topic
790:00 - my subscription
790:02 - and we'll grab this in a moment so we'll
790:04 - go back over
790:06 - to
790:08 - azure we got too many tabs open here
790:09 - while we're here we might as well go
790:11 - grab this code um before we go back to
790:13 - azure so i'm going to grab this dot env
790:15 - config that is going to load
790:17 - our configure
790:18 - configuration environments so we go to
790:20 - the top here
790:21 - paste that on in there for both files
790:26 - again best practice never to hard code
790:28 - your values always pass it in like that
790:29 - with environment variables we'll go back
790:31 - to our service bus the first thing we
790:33 - need well we made a topic we're going to
790:34 - need a subscription so if we go to topic
790:36 - here
790:37 - and click into it we can create
790:39 - ourselves a subscription so we'll go
790:41 - here we'll say my subscription
790:44 - notice we have max delivery account we
790:45 - have to set this between a value of 1
790:47 - and two thousand i'm going to say 12 for
790:48 - fun
790:49 - we can set the idle notice here if we
790:52 - want to
790:53 - have first and first out we would check
790:54 - box enable sessions we're going to leave
790:56 - that alone
790:57 - does not matter too much for our demo
791:01 - so we'll go ahead and create that
791:02 - subscription it is created now what i
791:04 - need you to do is go back to the service
791:05 - bus topic namespace
791:07 - and from there on the left hand side
791:09 - we're going to shared access policies
791:12 - we're going to click into the root
791:13 - manage shared access we're going to grab
791:15 - the primary connection string key the
791:17 - secondary would work too it's just and
791:18 - second optional one because they always
791:20 - give you a two
791:21 - we'll paste that on in there
791:23 - and if this is all correct this should
791:25 - just work so now what we'll do is type
791:27 - in
791:28 - node
791:30 - um actually we did want the cli
791:32 - installed because i wanted to show you
791:34 - uh that there was stuff in the queue
791:35 - whereas with um storage queue uh when
791:38 - we're doing azure storage queue i
791:39 - couldn't show you because um i just
791:41 - didn't know of a command i believe at
791:44 - least i think that was the case but hey
791:45 - we'll take a look and see what we can
791:47 - see okay
791:48 - um so
791:50 - actually we didn't do it at any time i
791:52 - wonder which one i did that for let's
791:54 - just double check here i have um
791:57 - off-screen here somewhere
791:58 - [Music]
792:00 - my instructions because i run it for one
792:01 - of these maybe it was for the queue that
792:03 - we did it
792:06 - yeah i didn't do the last one but i did
792:08 - an azure service bus q show and the idea
792:10 - was to show you that there was a message
792:11 - count ten so you could see the queue but
792:13 - i think that since we saw it in the ui i
792:15 - just wasn't too worried about it now did
792:18 - i do it for this one i'm not sure yeah
792:20 - we do a topic show so we'll do that for
792:21 - fun
792:23 - but first we need to insert our messages
792:25 - which are part of the topic send i don't
792:27 - think we read through these so let's
792:28 - just quickly read they look very very
792:30 - similar to the last one so you have your
792:33 - messages that you want to send you
792:34 - create yourself a client
792:36 - you create yourself a sender you create
792:39 - a batch message send message
792:41 - like it's basically identical
792:44 - like i can't even tell the difference
792:46 - here except here we're supplying a topic
792:48 - name so
792:49 - um
792:52 - i mean i see it here
792:54 - ah here create sender the topic name is
792:56 - specified there so i just imagine that
792:58 - instead of providing a topic name you
793:00 - provide the queue name and that's how it
793:01 - knows the difference but anyway what
793:03 - we'll do is go ahead and execute this
793:04 - code we'll say node send to topic
793:07 - js
793:09 - okay so it sent the stuff now we didn't
793:11 - do this last time so let's do it this
793:12 - time around so we'll type in azure
793:14 - service bus topic show
793:17 - and here we'll need to set the resource
793:19 - group so
793:20 - this was called i don't know let's go
793:22 - take a look here
793:23 - what is our service group called
793:27 - it is called az204
793:31 - bring that down a bit here az204
793:37 - service bus topic we need to specify the
793:40 - namespace
793:42 - name
793:43 - so that's just called service
793:46 - bus topic hyphen
793:48 - 723
793:52 - because we couldn't get the number we
793:53 - want and then we need to specify the
793:54 - name so
793:56 - i assume it's the name of the topic so
793:58 - the top is my topic and i'll hit enter
794:04 - and it says service business name is
794:06 - misspelled
794:08 - or not recognized by the service did you
794:10 - mean service
794:11 - bus
794:13 - yeah i gotta spell that right hit enter
794:17 - and that looks fine but i just want to
794:18 - specify it as output yaml just hit up on
794:21 - your keyboard if you want to go back to
794:22 - those previous commands let enter so
794:24 - this is a little bit easier to read
794:26 - and so what we're looking for here is
794:28 - just kind of like the message count
794:32 - does it show us here is the description
794:34 - count
794:36 - i don't see it
794:38 - so i guess it's not visible in the same
794:40 - way as uh the queue is like if we did
794:42 - this and we didn't do the last one it
794:44 - would we just thought like that message
794:45 - count there
794:47 - but let's take a look at what we can see
794:49 - in the cli to just see what information
794:51 - or a ui that our portal so we can see
794:53 - some information here so we have one
794:55 - subscription here we'll click into here
794:57 - um we have max size
795:00 - incoming request 12.
795:05 - um
795:08 - yeah i don't really see it message count
795:09 - 10
795:10 - max delivery count 12.
795:14 - okay so i guess there was 10 and that's
795:16 - the 12. so i guess that's where it's
795:18 - being counted let's go run the other one
795:20 - to receive i'm going to just double
795:21 - check to make sure that we set those we
795:23 - did that's all good
795:25 - so we'll type in node receive
795:27 - if you're wondering how i'm auto
795:28 - complaining without typing that i just
795:29 - hit tab on my keyboard so it's receiving
795:31 - those
795:34 - good and that finish i'm going to go
795:35 - ahead refresh see if there's any
795:37 - difference here
795:38 - notice that the message count is zero so
795:40 - when the topics were there it was held
795:42 - in the subscription saying 10 is here
795:44 - and they've yet to be delivered when we
795:46 - ran it they were received and so that
795:48 - number cleared out that's all we really
795:50 - need to learn for um uh topics so we are
795:53 - done with topics so let's make our way
795:55 - over to resource groups
795:58 - and we'll go over to our service bus
796:01 - topic
796:02 - go ahead and delete this here
796:13 - there it is deleting we are all good to
796:16 - go
796:16 - um and you know just as always don't
796:19 - ever trust azure to delete these things
796:20 - go back and check in three four minutes
796:22 - make sure it's deleted so you just don't
796:23 - have things lingering around
796:25 - but yeah there you go
796:28 - [Music]
796:32 - all right let us compare event grid
796:34 - event hubs and service bus because these
796:36 - all are event driven services for
796:38 - application integration and use event
796:40 - buses as a means to work with event data
796:43 - but it gets confusing because i guess
796:45 - you could say they all have overlapping
796:48 - responsibilities so let's kind of clear
796:50 - that up so we have event grid event hubs
796:53 - and azure service bus so the idea is
796:56 - that event grid is a serverless event
796:58 - bus
796:59 - and i would say that you'd probably want
797:01 - it for azure service to service
797:02 - communications the idea is that you just
797:05 - have let's say
797:06 - you know you're like integrating from
797:08 - service to service so you're not
797:09 - deploying necessarily fully custom web
797:12 - applications uh or traditional
797:14 - applications onto virtual machines it's
797:16 - more for like uh cloud native cloud
797:19 - first kind of builds
797:21 - they're dynamically scalable they're low
797:24 - cost at least one once delivery of a
797:27 - message
797:29 - for event hubs this is streaming data so
797:31 - we get low latency can receive and
797:33 - process millions of events per second at
797:36 - least once per delivery of an event and
797:38 - streaming you know because of the nature
797:40 - of it is generally more expensive than
797:41 - traditional things like
797:43 - aq or a pub sub for web applications
797:46 - which is what a azure service bus is so
797:49 - reliable asynchronous message delivery
797:51 - that requires polling and i guess
797:53 - pushing technically for pub sub advanced
797:56 - messages features like first in first
797:58 - out batching sessions transactions dead
798:00 - lettering temporal control routing
798:03 - filtering uh duplicate uh detection at
798:06 - least once delivery of a message so it
798:09 - really sounds like and there's one
798:10 - option here or optional order delivery
798:12 - of message so it sounds like azure
798:13 - service bus has it all but really it's
798:15 - not it's not at the same scale as event
798:18 - hubs and it's not at the same like level
798:21 - of
798:22 - um
798:23 - like hands-off kind of approach that
798:25 - event grid gives you with serverless
798:27 - but you have all those options there and
798:29 - you will use them all but there you go
798:31 - [Music]
798:36 - hey this is andrew brown from exam pro
798:39 - and we're taking a look here at the
798:40 - introduction to redis so what is redis
798:43 - well redis is an open source in-memory
798:45 - database store and it acts as a caching
798:47 - layer or a very fast database it has a
798:49 - lot of utility but those are the two
798:51 - most common
798:52 - and one thing is that because it's in
798:55 - memory
798:56 - that data is highly volatile so there is
798:58 - a chance of a data loss so even though
799:01 - it is super fast within milliseconds
799:04 - like five milliseconds 10 milliseconds
799:07 - um you know it's not used as a primary
799:10 - database but you know there are tools
799:12 - out there that have made them more
799:13 - durable uh so it really is up to you uh
799:16 - but the other thing about redis is that
799:18 - it's a key value store so uh it's not
799:22 - like using a relational database um and
799:25 - so it has its own type of data
799:26 - structures data types so we have string
799:29 - sets sorted sets lists hashes bitmaps
799:32 - bit fields
799:34 - hyperlogs geospatial indexes streams uh
799:38 - and the ones in red i do want to go over
799:40 - with you because i think they're worth
799:42 - your time but the idea is that to use
799:44 - redis you're either going to use
799:46 - like a programming library they're
799:47 - pretty much available in every language
799:50 - um and or you could use the redis cli so
799:54 - getting my pen tool out here for a
799:55 - second here you can see we're logging to
799:57 - the right of cli we're doing a ping to
799:58 - the server it gives back a pong uh and
800:01 - here we're setting a simple key value so
800:03 - we say my key some value some value is
800:06 - the string we are setting and so it is
800:08 - that simple it's a very simple database
800:10 - but it's super super fast but let's take
800:13 - a look at some of those data structures
800:15 - or data types so we can see uh how we're
800:18 - going to interact with this database
800:23 - all right let's take a look at our
800:25 - different types of data types data
800:27 - structures that we have available to us
800:29 - in redis the first is strings the most
800:31 - basic one the most important one because
800:33 - everything is basically strings at the
800:34 - end of the day here in redis strings are
800:36 - binary safe so they can contain data
800:38 - such as jpeg image serialized ruby
800:40 - objects so they can represent different
800:42 - things that can represent numbers which
800:44 - is kind of odd and strings have a max
800:46 - length of 512 megabytes to get a lot of
800:48 - room there often you're doing a get in
800:50 - the set here so you say get
800:52 - uh you know get this uh key you get back
800:55 - that string set this key with that value
800:57 - so very simple to use um strings can
801:00 - represent numbers and so now you have
801:02 - these special functions for strings
801:03 - called atomic counters so you can do
801:05 - things like increment decrement uh
801:07 - increment by certain amounts so very
801:09 - very clear here so notice it is a string
801:11 - but we can increment it and now return
801:13 - 11. and it interprets it as an integer
801:16 - which is
801:18 - interesting there so the most common
801:19 - string commands would be things like get
801:21 - set
801:22 - append depend would just add something
801:24 - to the end of the string or check check
801:26 - if a string exists based on a key so it
801:28 - exists you have lists so lists are
801:30 - ordered collection of strings uh they're
801:33 - not going to give you a guarantee of
801:34 - unique uh
801:35 - unique strings or unique uh elements
801:37 - within that list there so you can have
801:39 - duplicates but um
801:42 - you do things like l push l range it's
801:44 - kind of like having an array uh in other
801:46 - languages here they're doing a range
801:48 - notice it does negative one but it
801:49 - actually returns all of them so that's
801:50 - kind of a trick there to
801:52 - uh get
801:53 - all the elements in a range but you
801:55 - could say like one to two and then it
801:56 - would just return one and two
801:58 - common commands for lists would be l pop
802:01 - r pop l push l uh
802:04 - uh pos was just for position so el pop
802:06 - would uh remove the last element and
802:08 - return it uh or on the left hand side so
802:10 - the first one our pop would be on the
802:13 - right hand side so if you did an l pop
802:14 - and return hello you did an rpop will
802:16 - return world and also remove it from the
802:18 - list l push would add a new thing on the
802:21 - end help uh pause is creating a position
802:24 - so you provide a string it'll tell you
802:26 - it's positioned in the list then you
802:28 - have sets so
802:29 - seems like they're like lists but
802:30 - they're unordered so there's no
802:32 - guarantee that if you request the stuff
802:35 - in the list it's going to be in the
802:36 - order that you expect it but
802:39 - the advantage though is that things are
802:41 - unique in the list so if you push the
802:42 - same thing
802:44 - you add the same thing to that set
802:46 - like world twice it'll only be there
802:48 - once that's when you need a unique
802:50 - amount of things
802:51 - in a collection there most common
802:54 - commands here is s add so add a member s
802:56 - members show us all the members s move
802:58 - move a member from a from one set to
803:01 - another s pop similar to
803:04 - um
803:05 - our pop uh like what we saw with um list
803:08 - so just remove the
803:10 - one or or multiple random members from a
803:13 - set
803:14 - we have
803:15 - hashes so hashes represent a mapping
803:17 - between string fields and string values
803:20 - um redis was originally
803:22 - coded in ruby so they're basically like
803:24 - ruby hashes
803:25 - but if you don't know ruby hashes
803:27 - there's json objects probably seen those
803:29 - before ruby hashes of course python
803:31 - dictionary so the idea is that you set a
803:34 - you say okay i'm making a hash i'm going
803:36 - to set a field which is just a key it's
803:38 - string field two uh and it's value there
803:41 - so um pretty straightforward common
803:44 - commands for hashes would be like h h
803:46 - get to get a value hdl to delete a value
803:49 - hm set to set multiple values hm get to
803:52 - set or to get multiple values
803:54 - um
803:55 - h valves to get only the values h key
803:59 - or probably keys to get uh
804:02 - only the
804:03 - the fields which are keys in this case
804:05 - here we have sorted sets so these are
804:08 - similar to um sets
804:10 - but um
804:11 - they have an associated score so when
804:13 - you add something you use a zed ad for
804:14 - this but you put it you give it a score
804:16 - like here one one two
804:18 - and uh these are really great for
804:19 - leaderboards um or where you need to
804:22 - score things and sort that way so common
804:24 - commands here would be things like zed
804:25 - ads to add something with a with a
804:28 - certain score
804:29 - z rem is for removing
804:31 - z range is when you want to return a
804:33 - range of elements in a sorted set score
804:36 - zed rank would be return the rank of a
804:38 - member in the sword set score uh uh
804:41 - set sorted so it's not giving you i said
804:43 - sort of stored but it's sorted so it's
804:45 - not giving you its score it's giving you
804:47 - where it's ranked within the list
804:50 - and then you have z-score and so that
804:52 - that you provide um
804:55 - the
804:55 - the value and it would tell you what its
804:57 - score is so those are all the data types
804:59 - that you should get some hands-on with
805:01 - there are some interesting ones like
805:02 - stream and things like that but uh
805:05 - this is pretty much the core stuff for
805:07 - redis redis is not hard to learn or use
805:10 - um so you just gotta jump in there and
805:12 - use it okay
805:16 - [Music]
805:18 - all right so we just took a look at
805:19 - redis now let's talk about azure cash
805:22 - for redis so azure cash for redis is
805:23 - based on the popular open source redis
805:25 - cash or data store it gives you access
805:27 - to secure dedicated redis cache that
805:30 - microsoft manages and that you can
805:32 - access from any azure application all
805:34 - major cloud service providers have a
805:36 - managed redis
805:38 - service or database because it's just
805:40 - super useful and we'll talk about the
805:42 - use cases here in a moment azure cache
805:44 - for redis is suited for high throughput
805:47 - and low latency requirements where the
805:49 - same data is often requested so remember
805:51 - those key three things to know when
805:53 - you're supposed to use
805:54 - reddit there are
805:56 - serverless databases
805:58 - um
805:59 - like nosql databases sorry that
806:02 - can operate extremely quickly
806:05 - like with a guarantee of a certain
806:06 - amount of time but
806:08 - redis is just like super fast it's
806:10 - faster than everything else out there
806:11 - because it's in memory
806:13 - azure cache for redis is commonly used
806:15 - for these scenarios and i really do want
806:17 - you to pay attention because these are
806:19 - very common very practical and as a
806:21 - developer you really should know these
806:23 - so
806:24 - the a very common use case is storing
806:26 - session data if you have a web
806:28 - application and it's deployed to uh
806:31 - multiple virtual machines um a hard part
806:34 - about having an application running
806:36 - across multiple virtual machines or
806:37 - containers is that um knowing what was
806:40 - the last like where the where the
806:42 - session is stored the actual uh state of
806:45 - a user and so you can't have it running
806:46 - on one virtual machine because then the
806:48 - other ones won't know about it so you
806:50 - need to store it somewhere else so redis
806:52 - is a very common use case for that
806:54 - storing cached html or json uh this very
806:58 - common use case to speed up response
807:00 - times another thing that we normally see
807:02 - would be mem
807:03 - uh
807:04 - memory dme mdb
807:07 - it's another caching solution
807:08 - specifically for html um but yeah this
807:11 - is something you put in front of your
807:12 - web application to
807:14 - speed up web pages or api calls another
807:16 - one is a job or message queueing system
807:19 - if you are in the ruby on rails world a
807:21 - very common queueing system is called
807:23 - sidekick and it uses
807:26 - redis or another thing again in ruby on
807:29 - rails if you want to use action cable
807:31 - which is for doing pub sub like real
807:33 - time like building a chat like a chat
807:36 - system or a game that's backed by redis
807:39 - um so those solutions aren't going to
807:41 - change you can usually use redis for
807:43 - those things another one is using it to
807:46 - be in front of a database to reduce
807:48 - recontention recontention means like too
807:50 - many reads are hitting the database so
807:51 - it's slowing everything down so you can
807:53 - put a cache in front of that to improve
807:55 - response times for fetching data to
807:58 - just take uh take the load off the
808:00 - database maybe you have a database
808:02 - that's expensive to call
808:04 - so it's going to save you money
808:06 - and so you can put this in front of
808:07 - cosmodb or azure sql very common
808:11 - cases or any kind of database really
808:13 - that returns data
808:15 - and so this is actually called the aside
808:17 - cache pattern something you want to kind
808:18 - of remember uh where
808:20 - it's going to sit in front of the
808:21 - database so if the date if the data is
808:24 - up to date um
808:27 - and it's there in the the redis store
808:29 - it'll take it from there if not it'll go
808:31 - to the database and the database can be
808:33 - proactive and send data to redis so that
808:35 - it doesn't have to
808:37 - do a hit or miss it can just have the
808:39 - data there knows that it's there
808:41 - just to kind of visualize this just to
808:43 - see where those are
808:45 - so i'm just gonna get my pen tool out
808:46 - here so the first is like
808:48 - where uh um
808:49 - redis or azure cache redis is sitting
808:52 - between your application and the request
808:54 - so maybe you're hosting on azure app
808:55 - services the idea is that before
808:58 - um a request makes it into your
809:00 - application it's going to check that
809:01 - cache and so that can reduce the load on
809:03 - your application improve the load times
809:06 - if if there's things that are
809:07 - computationally expensive
809:09 - then there's your database so over here
809:11 - between your application to your
809:13 - database
809:14 - it's going to hit the redis cache first
809:16 - if there's data it will return it if
809:18 - there's not it will go straight to the
809:19 - database and then the other common use
809:22 - case here is to have a fast database for
809:24 - real-time data maybe for queuing
809:26 - jobs um
809:28 - video games leaderboards things like
809:30 - that so
809:31 - uh yeah that's pretty much it for azure
809:33 - cashflow redis just gotta get some
809:35 - hands-on experience with it but very
809:36 - simple uh not too complicated very
809:39 - useful
809:40 - [Music]
809:43 - you

Cleaned transcript:

hey this is andrew brown your cloud instructor exam pro bringing you another complete study course and this time it's the microsoft azure developer associate also known as the az204 made available to you on freecodecamp this course is designed to help you pass and achieve microsoft issued certification and the way we're going to do that is by doing lecture content follow alongs a practice exam and have cheat sheets for the day of the exam so you can prove on your resume your linkedin you have that as your knowledge you can go get that cloud job or get that promotion and tell you a bit about me i was previously the cto of multiple ed tech companies 15 years industry experience five years specializing in cloud i'm a nativist community hero and i've published many many free courses just like this one i love star trek and coconut water and i just want to take a moment here to thank viewers like you because it's you that make these courses possible and the best way to support more courses just like this one is to purchase the additional study materials at az204 so you can get access to study notes flash cards quizlets downloadable lecture slides downloadable cheat sheets practice exams ask questions get learning support but if you sign up right now for free you'll get a practice exam and cheat sheet there's no credit card required the free stuff does not have a trial limit so you do not have to worry about it vanishing it's not a demo uh if you have if there are updates to this course they'll be in the youtube descriptions on free code camp so watch out if there are corrections additions modifications to make sure you're using the latest version of this course and if you want to keep up to date on upcoming courses the best way to find that is on twitter i'm at andrew brown on twitter and if you do pass i love to hear uh you know your success story um or what you'd like to see next so there you go hey this is andrew brown from exam pro and we're at the start of our journey asking the most important questions first which is what is the azure developer associate so it's a microsoft certification about the azure platform focused on multiple ways to deploy web apps to azure a deep dive into azure functions i mean seriously deep a broad look into application integration services which azure has a lot of to the point where they have ones that cover the same purpose and lots of handson with azure cli sdks or programmatic interactions with azure services so anything to do with the azure api the course code for the azure developer is the ac204 not to be confused with the 104 but it is very complementary to the other course which we'll talk about when we talk about the roadmap and i do want to point out that microsoft azure is a very code and script driven uh platform compared to aws and gcp so it's better having developer knowledge and i actually consider it a must when we're working with microsoft and i'll explain why here in a bit so who's the certification for we'll consider the 204 if you are a web app developer looking to pick up cloud skills or transition to a cloud developer role you are a cloud developer cloud engineer who needs to integrate azure services into your app or deploy your app to azure or you need to learn a lot about azure functions to build service workloads but as a note i kind of feel that this is an essential certification at the associate level and so looking at a roadmap i always suggest people to start with the az900 which is the fundamentals because it's going to really help you get not just introduced to azure but also the test taking experience because it is a much difficult and different experience than the other two providers and generally from that we usually recommend to go with the administrator the 104 because that is a broad amount of services and that is the most common use case why people are on microsoft azure because they're usually i.t dev shops but as a complimentary usually after that i would recommend to go to the developer the az204 because the nature of azure is that a lot of the ui it like is script driven so when you are in the portal uh you have to touch script more often than not or you might run into features that simply do not exist in the portal so you have to use scripting so really to be a proficient pers person working on azure you need developer programming experience and so i always pair these two together now where you want to go after that is up to you a lot of people like to go the solutions architect or the devops engineer expert there's a bunch of other associates and i consider these kind of like uh like midlevel specialties where you can do those there's of course more certifications than we're looking at that is actually here but you know these are the most general ones here so you know hopefully that gives you an idea that you should probably take the developer after your associate and then after that you go wherever you want in terms of difficulty the azure developer associate is i would say two times harder than the aws developer associate and three times more difficult than the gcp associate and i'm talking about the exams not necessarily the application of being a developer in the platform but just the fact that the way azure makes their exams is they really want to test for practical knowledge so it's not about conceptual or strategic information not to say that there isn't those kind of questions on exam but it's a huge focus on do you know how to actually set things up and do you know the nuances of them so you're going to see a lot more labs than normal in my courses for this particular study course how long does it take to pass well it depends on you but to give you a general idea if you're a beginner so let's say you had the az 900 but not much handson experience you've never written code or attack role then you're looking 50 hours even if you've taken the easy to 104 it's still going to take this long because it's a different uh different beast than the 104. if you're experienced so you already have practical knowledge working on azure uh you've deployed apps to azure you have a strong background in programming you're looking at 20 hours and so i like to uh set a goal of somewhere in between so 30 hours average study time so for lectures in lab that's going to uh take up 50 of your time so you know we're looking at it's probably more uh labs and lecture and then the other part of it is 50 practice exams so i recommend you study one to two hours for 20 days and really do spread that out because if you do too much of it together uh you have a a good chance of forgetting information so you know make sure you spread that out and make sure that it's it becomes part of your longterm memory so where do you take the example you can do it in person or test center or online from the convenience of your own home so microsoft delivers exams via two different uh providers psi online pearson vue and those are online proctor exam systems both of these providers have their own test center networks so whether you want the online experience or in person it's just to be up to you i strongly recommend if you can to go in person it's less stressful if you have the opportunity if you can't you have to do your home it's up to you which one you want to choose they're more or less the same i like pearson vue but some people like psi so it just depends if you take a couple you'll decide which one you like and if you're wondering what the word proctor means it means there's a supervisor somebody is monitoring you as you taste taste uh take the exam to make sure you're not cheating that's the whole idea behind it to make sure these are legitimate uh scores so what does it take to pass example you're going to have to watch video lecture memorize key information you have to do handson labs and follow along within your own account you i would recommend paid exams to simulate the real exam i'm going to help you out by giving your first exam for free no credit card required just sign up on exam pro and um you know you can go get access to that and other free additional content that i strongly recommend that you do for the content outline we got five domains each domain has its own weighting that determines how many questions in a domain will show up and azure is interesting because they do a range of questions so it's not a guaranteed of a certain amount of questions on exam it's going to be between ranges so the first developer or sorry develop azure compute solutions develop azure storage implement azure security monitor troubleshoot and optimize azure solutions connect and consume to azure services and thirdparty services in terms of the grading you've got to get about 70 percent to pass and they use scaled scoring so uh you know it's not always exactly on the dot but you for the most part if you get 70 percent you should pass in terms of um amount of questions is between 40 to 60 you probably see 55 questions so you can afford to get about 12 to 18 wrong some questions are worth more than one point there's no penalty for wrong questions some questions cannot be skipped and for the formatted question you've got multiple choice multiple answer drag and drop hot area case studies all sorts of kinds of questions that you'll encounter much more difficult than the az 900 for sure in terms of the duration you get three hours so that's about one minute per question but of course different question formats are going to be different uh you have 180 minutes for the exam time your seat time is 210. so you have about um 30 minutes um in terms of the whole time that you said schedule so even if you have 180 minutes you have to consider the entire time to get logged in and all that other stuff so time to review instructions reading accept the nda complete the exam provide feedback at the end and i'm telling you if you're taking this online show up early because so often you're fiddling around with your license to try to get to scan properly and then they don't like it so you have to scan it twice so you know if you can show up an hour early and make sure you block that time this exam is valid for i believe two years before recertification so you know there you go hey this is andrew brown from exam pro and before we dive into azure functions we need to understand what is serverless so serverless architecture generally describes fully managed cloud services and i say generally because um you know that definition of serverless can be highly contested about what can be serverless and what can not be serverless so it's not a boolean answer of yes or no but some services are more uh serverless than others to a degree and so you know the way to help you understand serverless is the way i define it is i look at the following characteristics one thing for a cloud service to be serverless is that it should be highly elastic and scalable highly available highly durable and secure by default another thing is that it abstracts away the underlying infrastructure and are built based on the execution of your business task so you're not really worried about how many cpus and things like that maybe they are abstracted into like virtual cpus or away from what the original hardware software is using the idea is that it's a simpler value then the idea is that serverless can scale to zero meaning that when it's not in use you're not paying for anything and the most important thing is you're paying for value so you do not pay for idle servers and just to kind of reinforce that idea that um that uh it's a degree uh my friend daniel who's really big in the service loves to describe it as like the energy rating system so the idea is that uh when you go to buy an appliance such as um you know a washer or dryer they'll tell you how energy efficient it is and that's kind of the idea behind serverless services and uh you know we're going to be talking about function as a service but function of service does not necessarily guarantee that it's a serverless service but we'll talk about that in the next slide okay all right let's talk about function as a service also known as fast so here's kind of a diagram that i like to use to visualize it but we'll get to it in a moment here so what is function as a service it allows developers to focus on just writing pieces of codes also known as functions and it has event driven integration trigger functions based on event data or to emit event data so it's not just a matter of having a piece of function or code that you write in a box but the fact that it has to be event driven generally multiple functions are orchestrated together to create a serverless application sometimes also known as microservices functions generally only only run when needed and so function as a service is not serverless on its own so fast function as a service it's only serverless if it's fully managed and it scales to zero just to take a closer look at this graphic here so the idea is that if we're talking about the underlying infrastructure it's of course running on some kind of physical server and from there you'll have a host operating system and then you it could be a hypervisor and then from there you could have a virtual machine running a container runtime like the docker daemon um or you know it could be there are some os's that are optimized to run a container runtime without a hypervisor but the important part is the idea is that you're just deploying these little pieces of functions and they generally will go into a container so you might have a container runtime that is specifically configured for ruby or it might be configured for java or python or net and then the idea is that you're dropping that code and that code's getting scheduled into that container but hopefully that gives you kind of the general idea of what function as a service is hey this is andrew brown from exam pro and we are taking a look at azure functions and this is a function as a service fast offering that allows developers to focus on writing code and not worry about maintaining the underlying computing infrastructure and so here is our visualization so we can kind of break down the anatomy of how azure functions work so the first thing you'll need is an function app and this defines the underlying compute for a collection of functions so a function act defines the hosting runtime and global configurations then you have the functions themselves these represent code along with application runtime configuration so the idea is you can say i want this to be a python function a net function etc etc there's always going to be a trigger so a trigger is the chosen event data that will cause the function to execute and you can only have one trigger you have input bindings these are one or more multiple data sources that will be passed to the function when a trigger occurs so the idea is you can pull in data from a variety of different azure services at the time of trigger which is uh quite nice and then you have the output bindings these are one or more data syncs that will receive outputted data from the function on successful execution they say syncs you could say consumers if you like and also azure functions at least as of today has four different versions you really do want to just use the latest version there could be a version 5 out by now i don't know but you know just be aware that you're always using the latest version but in practicality you probably won't be able to tell the difference between the versions but just make sure you're using the latest okay all right let's take a look at storage considerations for azure functions because every function app requires a storage account to operate and if that account is deleted your functions are just not going to work so kind of a small visualization of your function app linked to a storage account so azure functions uses the following storage types in the storage account it's going to vary based on use case so for blob storage it maintains binding state and function keys for azure files file share used to store and run function app code in a consumption plan premium plan as your files is set up by default but you can create an app without azure files under certain conditions for cue storage this is used by task tubs in durable functions we have a little section on durable functions and then for table storage used by task hubs and durable functions as well so there you go okay let's take a look here at the anatomy of a single azure function so here is a screenshot of visual studio code because this is where you're going to be writing your functions you can write them somewhere else i sure don't know where but they have really strong integrations with visual studio code but the idea here is that we have a visual studio code plugin installed that allows us to manage our remote functions and we have local projects but let's take a look at some of the files that are in here so we can understand what kind of files matter to our functions the first is functions.json this is configuration of a single function defining bindings and we talk a lot about bindings uh in the azure functions section of this course there's the code itself in this case it's a javascript file we have the func ignore that's just like a git ignore file but it's to ignore files like files to be not packaged right so like when it actually deploys the function you don't want those files included but you might use them in local development we have host.json this is global configuration of all functions at the function app level then there's the local project itself this is just where the function is stored locally and a lot of times you're moving that code to uh to the remote storage on azure and if we were just to open that up you can kind of see the same files being mirrored so we have the name of the folder so host json function json index etc local settings json package json things like that so i'll just kind of erase a little bit of this out of here but you get the idea and it'll make a lot more sense when we start making functions which we absolutely do in this course critical for the az204 so but we'll do that soon all right let's take a look at authorization levels this determines what key if any needs to be present when making a request when a function is getting invoked and the authorization levels can be one of the following we have nominus function and admin so anonymous means that anybody can invoke the function so if it's an https request in order to trigger it anybody can trigger it for function you'll need a specified api key this is the default value when you're creating your functions generally recommended as the means to use or you can have it so that it has to use a main key to be required yes it's called master key i'm just going to use the word binky and for integration if you're trying to figure out how to change the authorization levels after you've deployed a function you're just going to click on your https trigger and from there you can switch the authorization level now that's not always the case there's some cases where you're not able to change it via the portal um and that just has to do with you know the type of runtime you're running whether it's a custom run time but yeah that's where you're going to do it there and you can change it after the fact after creation okay let's talk about how we would debug our azure function well you have this ability to turn on uh streaming logs and this allows you to see near realtime logging when errors occur and it's not as simple as that because there are two options for streaming a file log the first is builtin logging stream and so the idea is that you have the app service platform lets you view a stream of your application log files and then there's live metrics stream so when your function app is connected to application insights you can view log data and other metrics in near real time in the azure portal using live metrics stream and log streams can be viewed both in the portal and in most cases the local developer environments such as visual studio code now when i was doing this i was trying to turn it on and i did not get any logs in near real time but i think the reason why was because the hosting that i had it backed on wasn't on app service platform so i think it really does matter where you host it um and so you know maybe we'll explore that in the follow along to see if we can see your realtime logs but yeah there you go all right let's take a look at the key concepts for azure functions these are not exactly my words but i will uh provide exceptions here as we describe this stuff but the idea here is that we want to get broader knowledge about azure functions so azure functions are lightweight and can be serverless and they can be and this is only going to be dependent on how you host them and we'll talk about that in a moment azure functions are easier to write and deploy usually write deploy than full web applications uh that can be true the idea though is that you are uh you might have to do less coding in some regards but then you might have to do more in other places like application integration so i would say that it shifts the responsibilities of what you have to work on but you it is better i think azure functions are fast execute because there is no at large application startup time initialization or other events fired before the code is executed now true that large applications there are those downsides to them but the the tradeoff here is that if you're using serverless functions then you will be dealing with cold starts um and so cold starts just means that uh you know when you're when you have a function that hasn't been used in a while that's going to have to boot up an environment so you know there is that tradeoff now in many cases you can prewarm or have continuously running compute but if you do that then you're not necessarily leveraging serverless like to be able to scale to zero but you know you do get tradeoffs azure functions execution is triggered when an event is fired that is true azure functions do not need any infrastructure and it has zero maintenance well they do have infrastructure it's just that you don't generally have to worry about the infrastructure in terms of azure's implementation of functions you do have to think a little bit more in general about the infrastructure underneath as opposed to google cloud or aws but at least you have a lot of options on azure and it has zero maintenance well the infrastructure that's true but um the your code itself you know if you're using python 2 and microsoft says hey we're going to expire or uh deprecate the python 2 runtime you have to use python 3 well then you're going to have to upgrade that stuff so that is your maintenance so there is management code but that's no different than any other provider azure functions can be built tested deployed in the azure portal using a browser only if you're using windows so uh if you are hosting on linux there are definitely lots of limitations in the azure portal if you're using windows then there's no problem in most cases you should just use windows because you're not going to be able to tell the difference in most cases but if you want to take the full advantage of being able to work in the portal absolutely use windows azure functions are easy to upgrade and doesn't affect other parts of the website again this is subjective based on uh the code that you have to upgrade right so or functionality that they change so you know generally it's true it can be easier in isolate to update parts of your application because they're all little functions but there are exceptions to that azure functions use industry standard protocols probably https here and communicate with other apis databases and libraries that's true you only pay while your functions are running that is true if you use the serverless model if you're continuously running virtual machines behind the scenes then you are going to be paying all the time azure functions automatically scale to meet the demand of traffic that is true underneath azure will provision more service containers that is true azure function scale scale to zero cost can depend on your hosting azure function has builtin monitoring via azure monitor which it absolutely does and it can integrate or has builtin ci cd via azure devops which is very nice azure functions are event driven and will be triggered based on event data and emit event data absolutely and if they didn't then it wouldn't be functioned as a service but there you go all right let's take a look here at use cases both business and technical for azure functions so for business use cases you can use them for scheduled tasks reminders and notifications lightweight web apis sending background emails running background backup tasks doing backend calculations there are technical use cases such as sending emails starting uh starting a backup ordering processing task scheduling database cleanup send notifications messages iot data processing so azure functions are best suited for smaller apps that have events that can work independently of other websites originally i had some text in here that said like azure functions were not suited for large applications but more of these menial jobs around a larger application which is not true you can build a whole application out of functions now whether that's easy to do with azure functions is another story because it has to do with cold starts and things like that but i i definitely think you can but this is just kind of give you creative ideas of how you can use azure functions outside of the normal use case of just building a web application so there you go all right let's take a look at vs code extension because this is going to be the primary way that you are going to be working with azure functions so in order to productively work with azure functions you'll need the visual studio code azure functions extension and so what you'll do there is install that once it's installed and you have an azure icon in your activity bar with the side bar drawer you can look for functions i'm going to get my pen tool out here so the idea is you install this and then now you have at the top here the functions tab so you can manage azure functions it's possible to use azure portal to create and update windows host hosted azure functions but in practice it's not an easy experience with linux it's impossible this is a very kind of different experience compared to something like aws or google cloud where you can do basically everything in the cloud but with azure functions you really really really rely on visual studio code you can use visual studio for net and csharp app functions i've never done it myself but it is an option out there so there you go let's talk about runtime so what is an application runtime an application runtime is a compute environment configured with the necessary software and system configuration to run a specific type of application code so here uh if we were to create an azure function we have some options and you can see runtime stack so azure provides multiple applications and runtimes for popular programming languages just going to highlight that out there because i don't see my my arrow but uh it has things for uh net so that's c sharp java node.js that's both javascript typescript powershell core python there's my uh there's my nice arrow there um unfortunately does not have ruby at the time of recording this i really would like them to have it but if you want something that's not there what you can do is implement your own custom handler i believe they have an example either for rust or go in the documentation or let's say you want to do ruby that is something that's on the 204 exam but in practice it's really really really hard to get a working custom handler and i say that by i tried to make a ruby one it didn't work i tried their tutorial it didn't work i reached out to the person that wrote the tutorial they couldn't get to work i went to support they couldn't get it to work so in theory custom handler sounds great if you can figure it out um so you know if there isn't a custom handler uh follow along you'll know why um but uh yeah that is an option on the table the runtimes provided by microsoft are just docker containers you can see them on the docker hub um and you know a lot of times you know containers and functions are strongly related because they're generally almost always running on containers okay all right let's take a look at windows versus linux hosting so when you create an azure function app you can choose either windows or linux as your host and this actually it makes a big difference based on the capabilities that are available to you and we're talking about things where there could be performance differences functionality limitations feature differences but generally you know when we're using microsoft it's always good to attempt to use windows when possible because their whole ecosystem is built around windows and not so much linux one example here is that if you are using a linux hosting for azure function apps you cannot edit the function after it's been deployed and that is a very major inconvenience so just consider that when deploying functions i usually can't tell much the difference between linux and windows so i'll just choose windows so i get that additional functionality if you need to determine the exact os that is being used what you can do is go to docker hub repository because again all these uh all these images or runtimes are stored on docker hub and there we can see the windows images and the linux images and notice for linux there is a lot more variation where windows is just the nano server 1809 so that's the windows server 1809 whereas linux there's a little bit more options and mostly debian but you do have one down here that's ubuntu but there you go all right let's take a look at templates for azure functions so azure provides function templates to get you started with common functionality scenarios this is going to happen when you first create your function now you don't have to use one but definitely this is the route that you should want to go especially if you're choosing http trigger which is very common so to choose the template you're going to be doing that in visual studio code so you don't do it in the portal but let's just kind of walk through the templates we have available so we have http so triggered by http request and returns an http request you have a timer so triggered based on a schedule blob storage triggered when files are uploaded updated in a blob storage container cosmos db triggered when processing a new modified cosmodb document cue storage triggered by azure storage queue messages event grid so triggered by event from event grid so many azure services can be triggered as a function through event grid okay as we cover event grid in this course a lot of sources can be ingested into event grid event grid is serverless event bus integrated with azure service as we do cover at other parts intercourse uh event hub so triggered by an event hub event so this is for streaming service bus queues are triggered by a bus queue this is messaging service bus topics the triggered by uh an event from a bus topic so this is a pub sub model then you have send grid so triggered by an email event in a third party service center if you ever send grid it's just for sending out emails and so there's a lot there does it set up the bindings for you probably i never really noticed or checked but i would think that if you were to set one up for blob storage you would get the binding set up for you in your function json but we will look actually next at function json configuration because that is what is next in the course so for every single function there is a function configuration file called function json and so there's the structure there it defines the functions triggers bindings and other configuration settings when you choose a template you're going to get some default stuff there let's talk about three main things under bindings so you have the type of binding which is just the name of the binding which will affect the other attributes that are underneath often that you'll have a direction like in and out uh and then there's the name and the name is going to vary so for c sharp it's an argument name for javascript it's the key valued list but it's what is used to bound data in the function and we will definitely cover a lot about bindings because bindings is super important but just so you know function configuration file again is for trigger bindings and other configuration settings another file that you'll find in your function is a hosts dot json and uh this is for configuring global configuration options for all functions within the function app here is an example one uh so as extension for http you can see it has a rope prefix things like that it's setting a custom header and there is a lot of stuff you can configure here for options so we can do aggregators application insights blob console cosmodb extensions and then you know http cues retry send grid a whole host of things and you know not super important for the exam but definitely in practice you will be going this fall uh and configuring things need be but there you go let's take a look at plan services for azure functions and we have three to choose from consumption function premium and app service plan very different very interesting uh options for you to decide where you want your compute to run and how your compute to run for functions you do not see a bus or google cloud doing that but the flexibility here is very interesting so the first is consumption plan and this would be considered serverless and because it's serverless it can scale to zero and because it can scale to zero that means that uh generally we will have cold starts so you only pay for the time your code or application is running billing is based on the number of executions and the duration of each execution and the amount of memory used just pay while you have functions running and scale it automatically even through long loading times then you have premium plan these are functions premium this is where we have prewarmed uh prewarmed compute underneath so the user has designated a set of prewarmed cases which are already online and ready to react instantly azure provides any additional computing services that are required when your function is running you pay for the constantly prewarmed instances including any additional instances needed to scale the azure app in and out azure functions host instances are added and removed based on the number of incoming events so um you know the idea there is that you're not waiting for the container runtime to spin up it's already running but it's not going to scale to zero and you know i mean you can still say that it's serverless in some regard i would consider that now being a fully managed service but not necessarily serverless but you know that's debatable the last here we have dedicated plan this is with app service plan and this is where we're doing vm sharing and this one is extremely unique uh you do not see this in any other cloud provider but this is where when you use app service for other apps your functions will run on the same plan virtual machine at no extra cost you may scale it out manually by adding more virtual machine instances for an app service plan you may also have auto scale enabled optional when you have existing underutilized virtual machines which also operate other instances of the app service so i think what happened was um people were using apps app service right platform as a service and they just had un under they weren't utilizing all of the virtual machine compute and so a customer must have said hey can i just take my functions and utilize or utilize the unutilized compute on my plan and that's azure's like sure and they made that a thing so it's just like an interesting way to save money but it is one of those things where it is a little bit hard to understand because it's not uh as isolate or modular in terms of functionality where we see other cloud service providers but those are your three plans so there you go all right let's take a look at triggers and bindings for azure functions we talked about them before but let's give them a little bit more attention so triggers and bindings let you avoid hardcoding access to other services and abstracting away boilerplate code keeping your functions lean because the idea is that you won't have to add that code into your functions so here is our graphic which represents a function we have input bindings output bindings we have a trigger there so what is a trigger a trigger is a specific type of event which causes the function to run it defines how a function is invoked and a function must only have one trigger uh triggers can have associated data which is often provided as the payload of the function and you know as we saw in the introduction that we can have multiple sources of inputs that that get added there in this example here we have http but we're bringing in data from blob storage as well and obviously the main trigger uh which http is an input as well so you can have additional inputs at the same time then we'll take a look here is what is a binding um and so my arrow's a little bit messed up but the idea is binding is defined if your function is connected to another service so we have input bindings and output bindings so the data from bindings is provided to functions as parameters bindings are optional and a function can have multiple input and output bindings optional but often used which we'll find out but let's take a look at what supported bindings are in the next video all right now let's take a look at what supported bindings are available for azure functions there's quite a few here and we'll quickly go through them just so you have a scope of idea so we have blob storage azure cosmos db um or cosmodb i always want to add that s in there azure sql dapper which is a run time we cover it in the course it's a runtime for distributed applications for microservices event grid event hub uh http web hook that's gonna be the most common one you'll use iot hub kafka mobile apps notification hub that's for push notifications queue storage messaging rabbitmq that is an open source um viewing system sendgrid which is for sending emails service bus which is both queuing and pub sub signal r which we don't really talk about much in this course but it's an open source.net library to send asynchronous notifications to clientside web applications table storage timer uh twilio all right and so what's also important to note and sorry i could not make this graphic better i really really did try to present this better but it's just there's so many here but um the idea is that some don't support triggers so like azure sql does not support triggers mobile apps notifications don't table storage and twilio does not then you can see we have a big gap of inputs for a bunch here and a bunch there it's not super important to know for the exam but the fact that that you don't get triggers inputs and outputs across all services and there are some cases where the api version matters so like if you are using version one some things are not going to be supported and even some cases version two like mobile apps notifications don't have um uh don't have support so in some cases you do have to use uh version one functions but most cases you'll be using version four so hopefully that gives you kind of a broad idea of supported bindings but we're not done with bindings we'll be looking at more of it okay let us talk about binding directions so all triggers and bindings have a direction property in the function json file the direction of the triggers is always in and the input output binds either is in and or out and some bindings support a special direction called in out which a little bit confusing but it's nice to have a extra option this idea is we have a direction and triggers again are always in and then you have in out and in and out um the trigger is defined alongside the input and output binding so trigger will have the same as the input type but with trigger appended so for example if we have blob that would be our input binding then we'd have blob trigger as the trigger and that's how you would know the difference right so again just iterating see where it says uh a type here if it was just http then it would just be a a binding but if it's a http trigger then it's a trigger with a with also an in binding if you use the in and out only the advanced editor is available via the integrate tab in the portal just so you know because there's a visualization of integrations that we see in the azure portal and so they're talking about that in and out option there okay okay so now what i want to do is just go through a few scenarios of triggers and bindings to help cement the idea of their utility so we'll take a look at our first scenario so every hour you want to read a new log file delivered by your application and you'll need to transform the data to be ingested in your nosql or sql database that resides in cosmodb so here is our visualization we have our function which is python for fun it's going to output the cosmodb so we need a binding out to there it has a trigger um here that's just triggered by an acp request to the timer and we got the blob storage so you use a trigger timer i guess actually the http shouldn't even be there i think that's just a mistake but you use a trigger type timer because it's a scheduled job that will run at a specified time the trigger will be in for the blob storage and out for cosmodb because whenever function runs it'll be on a timer and when it executes it'll read data from blob storage process the data from blob storage and then write some data in cosmo db so the key thing here is that this runs on a schedule you're not you're not even though it's there you're not uh invoking it like with a an api request this is invoked on schedule let's take a look at scenario two every time someone signs up to your application you want to trigger an email so here there is an http request coming in and it's going to send out to sendgrid to send out that email so you want to develop an api that allows you to send an email after request is received and you'll use http trigger because it's an api that will be triggered based on this request for the bindings you uh you won't be accessing any data well the function starts so the in direction is none and you'll be uh you'll use send grid for the out direction which allows you to send messages via email looking at our third scenario consider a scenario in which you're using a cue service and you want a function to process a storage message in one queue and enter a new message in another so the idea is you have a queue again it's fun it's triggering something and it's going to output to something so in this case you'll want to use um into it says consider uh scenario q service where you want to function the storage message in one queue and enter in another so probably this this icon's old but we probably should have this icon repeated twice because this is the sendgrid logo so just imagine that we have we took this icon and we put it over here very sorry for that uh graphical error but um the idea here in this case you want a trigger type of queue and a binding type of cue for the direction out because you're not accessing the queue in the in direction you read the data from one queue and process it to create a new message and then to write to a different queue which may or may not be connected to another service or function so there you go all right let's take a look here at bindingexpressions so in function json file and the function parameters code you can use binding expressions that resolve to values from various sources so most expressions are identified by wrapping them in curly braces i say most because there's an exception where you do not use curly braces but the idea is that it allows you to have kind of dynamic content within your functioned json file and there are a variety of different binding expressions we have app settings trigger file name trigger metadata json payloads guid current date and time let's go take a look at what those look like the first is app settings so when you want to change configuration based on the environment you're going to use percentage science this is the only case where you use percentage signs uh it's confusing because it's the first example instead of curly so notice that there's curly sorry percentage signs there um and then for trigger file name this can be used to change the path of a file name works for both in and out directions so here we have the curly's here to say for the file name for trigger metadata many triggers provide additional metadata values these values can be used as input parameters for c sharp f sharp or properties for context bindings and objects in javascript so for example if we're doing azure queue storage sugar supports the following properties q trigger dq id etc etc uh and then you'll notice that they are available here's the cq trigger is here q trigger that's typed in camel case mode for json payloads when a trigger payload is json that means the the the data being passes json you can refer to its properties and configuration of other bindings and the same function in the code so you know if the payload was blob name and then hello helloworld.txt do blob name and that would put that here okay um and if some of your properties and your json payloads are objects you can use the dot notation so that's just a common thing for json or yeah json or javascript um for for if you want a globally unique identifier you can just do rand guid and you'll get something that looks like that if you want to have the current date and time you do date time you're going to get the current date and time in this format so there you go let's take a look at local settings file this is a file we saw uh when we were looking at the anatomy of the local files and so the local settings file stores app settings and settings used by your local development tools this file is called local.settings.json is expected to be at the root of your project folder because the local settings json may contain secrets such as connection strings you should never store this in a remote repository so make sure you add it to your dot git ignore file here's an example of one so first we have is encrypted so when the setting is true all values encrypted with the local machine key for values these are a collection of application settings used when running a project so notice we have function worker runtime azure web storage uh or job storage mybinding connections things like that the host customize the functions host process when you run it locally and then connection strings used only by frameworks that typically get connected strings from the connection strings area there okay okay so as your function core tools lets you develop and test your functions on your local computer from your command prompt or terminal so it's a cli uh and you type in func and allows you to do a whole host of things so let's take a look at the commands we have for top level commands we have init so create a new function project in a specific language logs get the logs for functions running in kubernetes cluster new creates a new function in the current project based on a template run enables you to invoke functions directly which is similar to running a function using the test tab in the azure portal start starts the local runtime host and loads the function project in the current folder deploy which was replaced with function kubernetes deploy then we have command groups that contain their own set of sub commands so we have func azure so this is when you're working with azure resources durable for durable functions extensions for managing extensions if you're doing working with kubernetes and azure functions you can do that if you are messing with settings that you do settings if it's with templates for this available templates you can do that so there you go all right let's talk about custom handlers these are lightweight web servers that receive events from function hosts and they can be written as long as it supports http primitives and these are really great in situations where you want to have a run time that is not supported for like a language like go rust ruby uh or a runtime environment for a specific technology where you need a bunch of libraries preloaded like ruby on rails or deno and so within your function function json the idea here is you'll have a custom handler section and this is going to just basically define the executional execution path because it basically is running a program custom handlers you can use triggers and inputs and output bindings via the extension bundles okay so let's kind of visualize that so we really understand that this is like a separate thing that's running so the function will pass along the payload to the custom handler the communication between the function and custom handler is via https requests and response the custom handler runs on white lightweight web servers the underlying compute will likely vary based on what service plan is chosen so it's going to be other places i couldn't really figure out where they were the custom executable for the custom handler is bundled along with your function code so there's that there uh some some things about the application structure to implement a custom handler your application must have the following a host json file a local settings json file a function json file uh a command script or executable which will run on the web server uh and then the following diagram shows how these files look on the file system for a function named myq and a custom handler so here you can just kind of see it's not the prettiest but i mean it gets the point across the idea is you have a folder here's your function json and this one happens to use an executable in practice custom handlers are super super hard to do uh and i think i mentioned this before but you know i was trying to set up a ruby one and it didn't work and i try to set up the the tutorial one that's in the docs for rust or go that didn't work i reached out the person that made it that didn't work i reset the azure support they didn't know how to do it so um if people running custom handlers if anyone figures it out please tell me uh hopefully i do figure it out and we get included in this course if we don't just understand if i couldn't do it then i don't it's not a big deal if you don't know how to do it okay but for me i personally would have loved to run ruby because that's my favorite language to use and that's what i would use on azure functions okay so let's say you are making a custom handler you deploy it and you end up having an error that says runtime is unreachable what do you do well not a whole lot but i can i can tell you that there are some things that you can narrow down because a runtime could not be running for multiple reasons um so uh you know the idea is that the runtime couldn't start and so there are some common reasons that azure suggests that it could be so it could be storage account was deleted storage account application setting was deleted storage account credentials are invalid storage account is accessible daily execution quota is full app is behind a firewall so there are your possible reasons for things to go wrong uh i just wanted to share that you know i ran into this and i i didn't necessarily get through it but i i wanted to share it to you possible solutions okay hey this is andrew brown from exam pro and we are taking a look at durable functions so durable functions is a serverless compute extension of azure functions that allow you to write stateful functions the extension introduces two types of functions so we have orchestrator functions this defines stateful workflows implicitly representing state via control flow and entity function which manage the state of entity explicitly representing state they define workflows in code so no json schema or designers are needed but honestly i would like one because other providers like aws have like no code solutions so it's kind of weird they try to make it like sound like an advantage but it isn't really an advantage they can call out functions synchronously and asynchronously uh the output from uh from the called functions can be saved to local variables they automatically checkpoint their progress whenever the function awaits local state is never lost if the process if the process recycles or the vm reboots so durable functions currently support the following languages so we have for csharp javascript python fsharp powershell and powershell seven to use durable functions you need to install a library specific to your language in the root of your function app project such as npm install durable function so there you go all right so if we are going to be having stateful functions we are going to need a variety of different patterns for setting up different kinds of serverless workloads let's take a look at them the first is function chaining so this is the pattern of executing a sequence of functions in a specific order often the output of one function needs to be applied to the input for another function durable functions allow us to implement this pattern concisely in code then you have fan out fan in so this is this is a pattern of executing multiple functions in parallel and then waiting for them all to finish fanning out can be completed with normal functions by having the function send multiple messages to a queue fanning in is much more difficult because we have to write code to track when the q triggered functions end and store function's output the durable function extension handles this pattern with relatively simple code we have async http api so this pattern addresses the problem of coordinating the state of longrunning operations with external clients a common way to implement this pattern is to have an http call trigger the longrunning action then redirect the client to status endpoint then they can pull to learn when operation when the operation is complete durable functions provide builtin apis that simplify the code we write for interacting with longrunning function executions the monitor pattern refers to a flexible recurring process in a workflow such as polling until certain conditions are met a simple scenario such as a periodic cleanup job can be addressed with a regular time trigger but it is interval but its interval is fixed making managing instance lifetime difficult durable functions enables flexible recurrence intervals task lifetime management and the ability to create multiple monitor process from a single orchestration a little bit more here so we have human interaction so many automated processes involve some form of human interaction humans are not always available and responsive as cloud services which makes invoking humans an automated process tricky so automated process must must allow for this and they often do so by using timeouts and compensation logic then we have aggregators so stateful entities this pattern is about aggregating event data over a period of time into a single addressable entity the data being aggregated may come from multiple sources be delivered in batches or may be scattered over long periods of time the aggregator may need to act on event data as it arrives external clients may need to query the aggregator data so there you go let us take a look at azure functions on kubernetes now i don't think this is on the exam but i just thought it was very interesting a thing that azure functions can do uh and so i just kind of wanted to run quickly through it so you can deploy any azure function app to a kubernetes cluster running on k running keda if you don't know kita it is kubernetes eventdriven autoscaling uh service and allows you to set up autoscaling based on events from various cloud native services and thirdparty services so in this case it could be for azure azure function apps and so when we were looking at the core uh core tools azure power tools whatever you want to call it um they had an opera they had the option to deploy to uh kubernetes so i'm gonna assume that you'd probably be running an aks azure kubernetes service uh but let's just read through some of the things here so kubernetesbased functions provide the functions runtime in a docker container with event drive driven scaling through kita kita can scale into zero instances with no events occurring and out to n instances so it scales to zero it does this by exposing custom metrics for kubernetes auto scaler so horizontal autoscaler using function containers with kita make it possible to replicate serverless function capabilities in kubernetes cluster these functions can be deployed using aks virtual node features for serverless infrastructure i'm really curious like how they're running and like other things there but that's outside of the scope of this course but you know if you're into kubernetes and i do have a kubernetes course um you know it's just interesting to see that that is a functionality there and so i just wanted to call that out for our kubernetes fans okay hey this is andrew brown from exam pro and we are looking at azure virtual machines which makes it easy for you to choose your own os compute memory and storage and launch a server within minutes all right so let's take a quick look here at vms for azure and this is a highly configurable server that relies on virtualization meaning that you're running a server without having to actually buy and maintain physical hardware that runs it you will still have to manage things at the software layer so you would have to apply your own os system patches and installing the configuration packages but the nice thing is that you're not dealing with that hardware it's just going to work for you and some things i want you to know about azure virtual machines is that the size of the virtual machine is determined by the azure image uh and so the image defines the combination of vcpus memory and storage capacity the current limit on a per subscription basis is 20 vms per region i would think that if you wanted more you could just use another subscription i don't know if they have service limit increase for that but that might be possible as well azure vms are billed at an hourly rate an instance uh or a single instance that you launch is going to have availability of 99.9 percent when you're using premium disk if you have standard disk i have no idea what it is um but it's going to be less than that if you want to get 99.95 availability you're going to have to launch two instances within an availability set and you can attach multiple manage this to an azure virtual machine now just to give you uh like a quick visual of what's actually happening when you launch an instance because when you do launch a virtual machine there's going to be other networking components that are going to be created or associated with you and you definitely need to know all these components here so i have this nice diagram we're going to do a quick run through with it and i want you to just know that when you do launch a virtual machine it actually does give you a list of all the components that it creates this is actually very common with most azure services and sometimes what they'll do is they'll put it within a resource group for you so that they're all grouped together which is very nice but let's just run through these common components that you need to know such as the network security group and this is going to be attached to your network interface it acts as a virtual firewall with rules around ports and protocols so that is what's protecting our virtual machine you have the network interface that handles ip protocol so that's how the virtual machine talks to the internet or other things on the network you have the virtual machine itself or the instance that's a the version or the uh instance that is currently running it's going to get a public ip address assigned to it so that's how um people from the internet can actually access the virtual machine and then you have to launch it within a virtual network uh and so that's you're going to have that vnet so you'll either you'll either choose one or you'll be creating one uh during that wizard process so there you go so now let's take a quick look at the options for operating systems on azure vm and let's just define what an os is uh so the os is the program that manages all other programs in a computer and the most commonly known operating systems are windows mac os and linux and when you want to launch a virtual machine the way you determine what operating system you use is based on the image that you choose okay and azure has a marketplace so they have so many different kinds of operating systems you're going to have every possible option you want and microsoft has also partnered with specific companies to make sure that they're providing you images that are updated and optimized for the azure runtime so let's do a quick run through of these supported or partnered os's so we have seuss we have red hat ubuntu debian freebsd then you have flat card container linux rancher os which is for containerization but nami binami is more like images that have preloaded software on them they're very popular for their bitnami wordpress you have mesosphere and then you have images that have docker with it so you have a lot of options open to you there and i always forget about jenkins and jenkins is on the end there now if you want to bring your own linux version you can totally do so all you have to do is package it into a virtual hard disk or a vhd if you've never heard of vhds uh these are these are just uh virtual hard disk formats that you can create using hyperv software which would be on your windows machine and just be aware that azure doesn't support vhdx which is a newer format it only uses vhd okay so there you go so let's take a quick look here at cloud and knit and this is something that's not going to be on your exam but it's something you absolutely should know and need to know because it's an industry standard and it's just something that uh you might like if i didn't show it to you no one's gonna show it to you so let's just get to it and so cloud init is this multidistribution method for crossplatform cloud instance initialization and it's supported across all major uh public cloud providers so azure aws gcp it's provisioning systems for private cloud infrastructure and bare metal installations so what is cloud instance initialization well this is the process of preparing an instance with configuration data for the operating system and the runtime environment and so uh the idea is that you're going to have cloud instances that are initialized from a disk image and instance data right so the image is whatever the the the vm image is but you're going to have data such as metadata user data and vendor data and i'm not going to get into all the data i just want you to know the one which is the one you'll be working with is user data and so user data is a script that you want to run when an instance first boots uh and so um if you've ever used aws and you launch an instance you'll see a box in the the wizard that says user data and that's what this is uh in azure i don't think they make it as clear i don't think they call it user data but if you're pragmatically doing it with um arm templates when you're doing infrastructure as code that is what you're doing you're using cloud in it underneath i just wanted to make that association to you so when you see that word user data you think cloud init and so cloud in it really only works with linux distributions so um it should work with all the linux distributions on azure i'm pretty sure it's not being used for windows machines but there you go azure virtual machines come in a variety of different sizes that are optimized for specific use cases and the way azure groups these are into types so that would be like general purpose compute optimized and sizes and sizes is kind of a weird word because i've seen in other places in their documentation called series or sku family so you'll see me using those three terms interchangeably they definitely confused me when i was looking at it at first but let's just go quickly through some of them there's a lot of them so they're not going to be all of them on the list here but we'll just quickly go through it so the first is general purpose these are balanced cpu to memory ratio testing development small to medium databases and low to medium traffic web servers this is pretty much what you're going to be using unless you work for a company that has a lot of money and the one you're going to see the most is called b1 because it's super super cost effective and that's the one i'm always using when i'm launching linux machines you'll see me using it through follow alongs here in this course the next one here is compute optimize so that's high cpu uh uh to memory or ratio good for medium traffic web servers network appliances batch processes and app servers so this just has more cpu here uh the most common one is the fsv2 or the f series then you have memory optimized so high memory to cpu ratio great for relational databases being able to hide large caches and in memory analytics and we have a variety of skus there then you have storage optimized so these are high a disk throughput and if and ideal for big data sql no school databases database warehousing and large transactional databases and we just have lsv2 for those then you have the specialized ones for gpu for heavy graphic rendering video editing model training and interferencing with deep learning so available with single or multi multiple gpus and so we have a variety of skus there and then you have your hpcs your high performance compute and these are the the most fast and powerful cpu virtual machines with uh with optional high throughput network interfaces all right and i just want to point out that there are more here um but uh i'm just showing you ones that are occurring so there are previous series um here like we're not showing basic a all right like the a series is not being shown here now when you want to actually go see this full list here it's very easy you just launch up a virtual machine and uh you're going to be able to choose two things you're going to choose the first the image so that's going to be what do you want to run right so it's going to be ubuntu or windows machine and then you're going to choose your standard size image i believe that if you choose certain images not all sizes will be available to you because some images have to be optimized for those uh series sorry i'm just drinking some tea here uh and so there's that b1 series and you can see in canadian dollars it's 9.72 which is a pretty good price for me and you can explore all the costs in azure which is really nice you don't have to go to a separate marketing website you can just open it up and you can just sort by the lowest cost which is what i always do and they have a lot of filters there so there you go let's quickly talk about azure compute units so azure compute units also known as acus provides a way of computing comparing compute so cpu performance across azure skus remember we said skus is the same thing as sizes or series so you'll just see us varying the terms on that acu is currently standardized on a small so standard a1 and you saw prior that a1 is a previous series so it's not something that you generally be launching but everything's based off of that one and those are given a value of 100 it's just an arbitrary number to um give it a point against other machines and all other skus then represent approximately how much faster that sku can run a standard benchmark i don't fully understand the math behind uh their their stuff here but i generally get the idea here and i'll just show you quickly a quick comparison so here we have the a1 to a4 series against the d1 d14 series a1 is our baseline at at 100 acus and it has a ratio one to one from vcpus to core and so you can see that for d1 to d14 it says 160 points to 250 points and so if i did the math here right i think i did uh it's going to be basically 60 to 150 percent more performant than choosing an a1 to a4 so that's how that works so there you go so here's a neat feature that azure virtual machines allows you to do and you can actually do this with a lot of different services with azure and it's the azure mobile app and so what you'll notice is that when you're using virtual machines they might have this little qr code and if you have the app installed what you can do is scan that code but honestly you don't even need to scan the code if you just have the app installed connect your account you can log in on your phone and check out the virtual machine how it's performing and you can take basic actions there i believe as well you can access the cloud shell so it's pretty darn cool um and so i just wanted you to know about that and if you haven't just go ahead and install it on your phone i think it's available for um android and ios i have an android phone so i didn't check ios but um yeah go check it out so sometimes when you're launching a virtual machine you'll have the option between generation one versus generation two when you're choosing images and i want you to know about uh this stuff here and to understand it we need to know a little bit about hyperv so hyperv's is microsoft's hardware virtualization product and it lets you create and run a software version of a computer called a virtual machine and each virtual machine acts like a complete computer running an operating system and programs and so if you're still kind of wondering what hyperv is if you've ever heard of virtualbox which is used for running vms on mac and linux this is the this is basically the exact same offering it's just windows offering okay and it already comes preinstalled on windows 10. so you might have to toggle something on i can't remember it because it's been a long time since i've had to configure it but the point is you have a windows machine you're already ready to start using hyperv and hyperv has two generations of virtual machines so the first one was generation one and this uh this will support most guest operating systems and then generation two supports most 64bit versions of windows and more current versions of linux and freebsd operating systems and there are these two here but neither one or two is bad so they both have their own use case so it's not like you should always use two and there's a big list online but we're not going to get into that full list but anyway so we see that hyperv has generation one and two vms but azure also has them as well but they're not onetoone the same as hyperv generation so if you look up hyper v uh like the hyper v generation um feature set you might not have that in azure so i wouldn't rely on that list but what i definitely want you to know between azure gen 1 and gen 2 that's a key difference is that gen 1 is biosbased architecture and gen 2 is uefi based boot architecture and if you've ever built a video gaming machine you definitely know the difference between these two if you don't it's just the way the machine boots up it's the first screen you see um and it's going to change what kind of configuration options you have and you you uh uefi has a lot of great features in it so for instance it has secure boot which verifies the bootloader is signed by a trustee authority and you can have larger boot volumes up to 64 terabytes and if you want to know more you'll just have to look up uefi because i'm not covering it in this course i just want you to know that's the the key difference there and hyper vvms are packaged into virtual hard disk formats so that's going to be either vhd or vhdx files okay one thing you might want to be able to do is actually get into your virtual machine and we have a lot of different options available to us in fact we have three ways to connect to our virtual machines we have rdp ssh and bastion and so if you don't know these things we're going to walk through them right now the first is secure shell and this is a protocol to establish a secure connection between a client and server this is how you can remotely connect to a azure virtual machine via a terminal ssh happens on port 22 via tcp port 22 is something you should absolutely remember as ssh and you will generally use an rsa key pair to gain access you can also use username and password but most people don't do that anymore another way of getting access to your machine is by using rdp which is remote desktop protocol this is a proprietary protocol developed by microsoft which provides a user with a graphical interface to connect to another computer over a network connection basically what it means is that it's going to open up a window and you're going to be able to see like another windows desktop in it and control it remotely so this is how you can remotely connect windows server to a virtual desktop rdp happens on port 3389 via tcp and udp i think there might be some other ports but this is the port that you need to know i just remember from aws because there were some extra ports i had to configure but that's a different story then you have uh your bastion which is your third option and so azure bastion which is actually an azure service is a service that you can deploy lets you connect to a virtual machine using your browser and azure portal and so the reason why this is so useful is that if you have let's say a chrome desktop which doesn't actually it can't actually use ssh client or can't use the rdb client this just gives you another way of using the browser so you don't have to install those clients but we'll get into that shortly here okay so let's take a closer look at ssh and the most common way to access your vm or to authenticate yourself uh so that you're allowed to get into your vm with ssh is by using ssh key pair and so the idea is that you're going to have to generate out two keys that's why it's called a key pair there's two keys and so you'd use that command ssh hyphen keygen hyphen t rsa and that is something that is definitely etched in my memory it's definitely something you want to memorize because it'll be with you throughout your entire career and so you're going to have a private key and a public key and so the way these keys work is that the private key should remain on your local system and not be shared with others and the public key is going to be stored on the vm and when you go ahead and ssh you're going to provide your private key as part of the ssh command and it's going to try to match it against the public key to authenticate you and so that is what you normally do ssh hyphen i provide the public key see where it says dot pub that's how you know it's a public key and then there's the actual address of the server um and that's how you're gonna gain access okay so let's take a look at the process of using rdp so the first thing you'll want to do is download the rdb file so when you go and try to connect to that virtual machine you're going to go ahead and download the rdp file that's going to download this rdp uh this file that has rdp extension you're going to double click that file and you're going to be prompted to fill in your username and password this uh this username and password is what you specified when you created your virtual machine and i just want you to know that if you are on a mac you can install the remote desktop client from the apple store but if you're on windows you already have rdp client installed there's nothing extra that you have to do so let's take a closer look here at azure bastion and this is an intermediate uh hardened instance that allows you to connect to your target server via ssh or rdp and it will provision a webbased rdp client or ssh terminal i'm going to tell you honestly i never really thought of this use case for bastions prior to this but the reason like bastions are good or have still a utility these days is because some people are using a google chromebook and they're not going to have a terminal so they can't use ssh or if they're on windows they can't sell the putty client or if they're on windows they can't they won't have the remote desktop client and so the only way is through the web because a google chromebook's all a browser and so this is one of those utilities for a bastion now bastions don't necessarily are just for this use case they're definitely a very good and secure way to connect to your virtual machines and having a way of auditing who has access to what but this is the use case that azure is putting forward with bastion here and so the idea is that when you want to create a bastion you're going to have to add it to a subnet uh in your vnet or you're gonna have to make a new subnet and it has to be called azure bastion subnet and it has to have at least a size of four slash 27 so 32 addresses so that's what you're gonna have to do so um you'll just have to go in and add in that subnet there and then once you do uh you can go ahead and launch your bastion so let's first take a look at rdp uh and so rdp uh we saw how to do it but let's see what it looks like with with azure bastion so what you do is you say connect with azure bastion for your windows machine that's going to assume that you want rdp you're going to just put in your username and password as you normally would and boom you're in your machine that's all it takes let's take a look at how you deal with ssh so you know if you're using a linux server it's going to obviously want to use ssh because you don't have rdp for that and so you can actually use either the ssh private key or the the username and password i recommend using always ssh private key let's take a look at that process so you'll enter your username in and then you can switch over to ssh private key and then the dot pem file which is downloaded locally to your machine which is your private key you can just uh select it and then that will upload it and use it in the comparison and boom you're in your machine so that's just kind of a cool service that they have there let's just do a quick comparison between windows and linux servers this is going to be very obvious for people that are used to running windows workloads but for those who uh grew up on linux i just want to make sure you understand some of the caveats of windows that you might not be used to so obviously azure allows you to run both windows and linux instances or vms and so let's just talk about windows first so when you launch a windows server just like your desktop computer it needs a windows license to run whether it's windows 10 or whatever windows server you're using but i just want you to know that when you launch your your server you don't necessarily have to have the license on hand and so if you launch a server without a license um all it's going to do is say windows is unactivated and you're going to take some manual step to activate that license and i just want to tell you that so that you're not afraid of launching a windows machine because you think you're going to get charged a license fee uh it's not going to happen you have to take some manual intervention to do that so do not worry about launching that windows machine you can bring your own license via a hybrid license so some people like enterprises already own their licenses and they just want to uh they already made a deal with microsoft and so they just want to reuse those instead of using whatever azure provides because it can be cheaper you're going to set a username and password because you're using rdp to access the machine so you're not using your ssh key pairs um windows machines require larger instances okay so if you're running windows 10 or whatever it needs a lot more memory and so you're gonna have to run it on a b2 which is a lot more expensive than a b1 and that's because it runs a full desktop environment all the windows servers do this and so this is generally why i like running linux so just be aware not to keep your windows server running for very long if you don't have a lot of money and you're just learning to pass the certification let's talk about linux really quickly here most versions of linux require no type of license so i think like you can get red hat you might need a license for that if you want support you set either a username password or ssh key pair you can utilize smaller vm sizes because you're not running a full desktop experience could you run like a full desktop experience possibly i haven't ever tried to do it on cloud myself uh unix and like space uh are unix and linuxbased systems traditionally are terminal based environments right so you're going to be sshinging into them and you're not going to have something visual okay so there you go so let's talk about update management at the start of this section for virtual machines i had said that there was some uh requirements ongoing software patches and things that you had to perform on your virtual machines and so that is something that you have to take care of but if you wanted to automate that process this is where you can use update management so that's what it'll do it'll install operating system updates and patches for both windows and linux and not only can it do it for your virtual machines on azure but it can do it on premise and even in other cloud providers okay and the way this is going to work is you're going to go to your operations tab where it says guest plus host updates and you're going to click on update management now update management looks like it's its own service but it's actually using azure automations under underneath and so that's what is installing the agent and the agent is the microsoft monitoring agent mma so that's how it knows what to do you have to get that installed on your machine so update management will perform a scan for update compliance a compliance scan is by default performed every 12 hours on a windows machine and every three hours on a linux machine it can take between 30 minutes and six hours for the dashboard to display updated data from managed computers and i just wanted to point out a little bit about azure automation so because it does more than just obviously update management but azure automation you can enable update management change tracking inventory start stop vms during off hours features for your servers and virtual machines and these features have a dependency on on a log analytics workspace and therefore required linking the workspace with an automation account so there is a little bit extra that we have to have but there you go hey this is andrew brown from exam pro and we're going to be launching our own bastion using azure bastion services so there's two ways to set this up we can go to bastions over here and create a bastion this way or we can create one after we've created a virtual machine i prefer the latter so let's go ahead and do that and launch ourselves a new virtual machine and so we could either use launch a windows server or a linux server today i'm going to be launching a windows server and what we'll do is go down here i'll make a new group we'll call it the enterprise and as we do that we'll just name this uh enterprise d and we'll launch that in canada centra or us or central us that's fine with me uh 2019 data center gen 2 is totally fine if you go here you're trying to find it you go hit select and we'll choose gen 2 data center it is expensive but uh we're not going to be using this for very long for the username i'm going to put data and for the i'll just make it azure user to make our lives a bit easier and then we'll put testing capital t one two three four five six testing one two three four five six and we will go down below we're fine with the settings here we're gonna go next to disk we're gonna leave the disk to premium that's fine uh we'll let it create a new network that's totally fine management is okay and we'll just actually go hit review and create and now we'll just hit create so that it will go ahead and do that it'll tell us that it's in progress and we'll just wait a little bit here i'll see you back in a moment all right so our instance is ready so let's go ahead and go to this resource here and then on the left hand side you'll have connect and so i'm going to show you and you don't it's not necessary for you to do uh this step because you're going to i'm going to show you how to connect be the bastion but i'm going to go ahead and download this file the rdp file this will only work if you're on windows by the way well i guess it'll work on mac but you'd have to install the um uh the rdp service for the users with windows there and so here this is azure user we're going to type in testing with a capital t just double check that there i'm going to log in make sure that this works uh do we'll do that one more time oh you know it's testing one two three four five six there we go we'll say yes and we'll just make sure that we can uh remote desktop into this just before anything else and there we go so that's all good to me i don't need to see any more we'll go over to bastian we'll say use bastion and this is going to set up a bastion service in order to use bastion you need to have another address space defined for it it makes it really easy to make it here so i'm just going to go 10.0.1.04 and we'll go ahead and hit ok and so down below it's going to choose an address space we have a security group um i'm just going to put it for none i don't think i want one on that and if we scroll on down here we have the resource group so we're going to put in the same resource group and we'll go ahead and create that so now before this they didn't have this really nice wizard used to have to go and and create all those things individually in your virtual network but this is really nice it does take a bit of time for this to provision so i'll see you back here in a bit that took a bit of time for that to create that bastion but it is ready to go and so now that we have it we can go ahead and utilize uh this connection here and so right away it i think it's setting up for rdp here so what we'll do is type in azure user and then capital t testing one two three four five six we'll go ahead and hit connect and so notice that i didn't have to use an external application i could just run it in right here it's all in the web browser so that's pretty much how the bastion works i can't remember the pricing on bastion i think it's a little bit of money so i don't want to keep this laying around here but this is great if you let's say you're on a chromebook which are becoming really popular where you can't install native applications uh or you're just having issues because you're on like linux or something like that so there you go that's all there is to it we'll go ahead and clean this up and so i'm just going to go here find the resource group and we'll go ahead and delete i'm just making sure that bastion's within there so it is good and there we go so we just launched a virtual machine for linux now let's go ahead and launch one for windows i'm going to go to the top here and type in virtual machines we'll go to the first link i'm going to hit add add virtual machine and what we'll do is we'll create a new group the last one i had was called bajor i'm going to call this one cardassia cardassia and i'm going to name this machine also named cardassia and this time what we want to do is is we want to move over to a windows server i find the easiest one to learn with is the windows 10 pro server just because i find these ones a little bit daunting so i'm going to go windows 10 pro and then what we're going to do is go choose a larger size this is not going to work we cannot run a windows server on a b1 ls so we're going to have to go a little bit larger and we don't have to go too much larger here but the idea here is that there's going to be a more expensive spend here so we're not going to be running to keep this running for long but here we have the b2s that is the appropriate size to run this anything smaller i don't think is going to work and we are going to put in a password here so i'm just going to put in cardassia and we'll do cardassio123 put a capital on it i guess we'll just do this here and i'm just going to go back and lower case this one and we're going to allow the inbound port of 3389 because that is what rdp needs i'm going to confirm that i have a windows license i actually don't but the thing is you can still launch one for your test purposes it'll just complain saying you're not activated so there are some limitations but it's good enough for us to learn okay and so now that that is all great we'll go next to disks we're going to go with premium or standard ssd this time uh we're going to go ahead and hit next and go to networking it's going to create us a new vnet which is a great idea we're going to let it create a a network security group on the nick just like before we'll go ahead and hit next uh we'll leave all these options alone this all seems fine to me uh and we'll hit next review and create and we'll go ahead and create this server all right and so that's gonna go ahead and create it so i'll just see you back here in a moment when that's done deploying all right so after a short little while here it looks like our windows server is now deployed so what we can do is go to that resource if you wanted to see what it's deployed it's the same stuff as always you have your network interface card your virtual networks nsg the ip address but let's actually go to that resource now and so let's see how we can gain access to this virtual machine and so what we can do is use rdp luckily i am on a windows machine and so i already have the rdb client that i can use so all you got to do is download the rdb file and then once we have that file i can just double click it and i can open this up if you're on a mac you can download the app in the app store and so i'll go ahead and type in my password so my username was cardassia and then my password was capital c ardassia one two three we'll hit okay and then it'll give us another one we'll say yes and now we are in our virtual machine so there you go how cool is that i'll just give it a moment to load up but this is a full uh windows 10 pro uh and as i said before you know we don't actually have a license so if you're afraid of spinning it up because you're gonna get charged a license fee uh for windows you do not have to worry that's not gonna happen you have to do some manual intervention uh for that to happen so we'll just wait a little while here for this to load um it is not we're not using the most powerful machine so it does take a little bit of time and so we just hit accept here and here we are so we are on uh we have our nice windows machine here whoops i don't know if it has any games let's go take a look maybe play minesweeper um no maybe maybe you have to download the store i'm not that familiar with windows machines but um so there you go so we'll go ahead and close that and you know if we were using the bastion it's the same process you saw how we used it with ssh but if we had the bastion and it's so much work to set one up we already did that before uh but all you do is enter your credentials in on the page just as we did and it and it's just a lot easier that way uh so let's go ahead and just tear down this machine we're all done with it so i'm just going to hit um uh delete and uh if we find that resource group we should be able to easily delete them all i find the easiest ways to go up here go to all resources and then there's the resource group there and then hit delete resource group and then i'll type in the name of it which is cardassia and i'll delete all those resources but after that's done always just take a double check on your all resources tab and just make sure that those resources are gone just because sometimes they stick around but there you go that's as simple as it was to launch a windows machine hey this is andrew brown from exam pro and let's take a look here at virtual machines which i consider the backbone of most cloud service providers and azure keeps it really simple by calling it virtual machine so we can go up here and type in virtual machine and make our way over here but right now i don't have any options because i'm using a tenant that doesn't have a subscription applied to it so what i'm going to do is go switch back to my original tenant and this one has this subscription applied to it and so what i'll do is just click back up here and now we'll just type in virtual machines and i can now see uh i have options of creating virtual machines let's go ahead and go create a linux one first and then we'll go ahead and create a windows one and then we'll see how we can connect to it all right so first we'll go to the top here hit add we'll click on virtual machine and we're gonna be present with a lot of options so we'll have to choose a subscription and so there is mine i want to probably create a new resource group here i'm going to call this one bajor and i will name this uh bajor again and i'm going to launch this in us east i'll just set it to one availability zone for the time being then here we have what we can choose as an image i can click on see all images and choose from a variety of them so if i didn't want to use ubuntu i could launch something else like debian or something like that but really i just want to stick with uh ubuntu because i'm fine with that version with 18. uh then here's what what really matters is the size because that's going to affect our cost so if we click on see all sizes we have this nifty table where we can sort the cost it's just loading the cost here it's dynamic this is going to be based on uh what your base subscription is so if you're in canada you're going to see canadian prices in the us you can see us prices etc etc and i care a lot about cost here so i'm just going to sort this by cost and here we have the b1 ls which is very cost effective we have a ram of 0.5 gigabytes and some other options there so we'll go ahead and select that there and we have a couple options we can use ssh public key or we can uh utilize a password and so i think what we'll do is use an ssh public key because that's pretty much the standard there we're going to name the username beige or if it lets us probably won't probably want some additional options there nope it's okay oh great and we'll go ahead and generate a new key pair and i'm just going to name that one bajor and uh we have some options here for inbound rules uh so you could set to none this is just setting up the nsg for you but we're probably going to want to have that port open for ssh because that's how we're going to make our way back in here if we're running a um like an apache server we'd want to have port 80 open so we can go ahead and do that we'll take a look at now disks so here we have options between premium standard and standard hhd i just want this to be cost effective so i'm just going to go with standard hdd but generally you you want to have at least a standard or premium ssd when you're running real web development workloads then there's encryption here and so it's always turned on by default which is great they also have this option of double encryption with the platform managing customer manage key we're just going to leave that as default enable alter discompatibility that's not something we need to do here because we are not using ultradisc and here you can see that you can attach multiple uh disks here so i can go and do that but that's not something i need to do today and some other advanced options which we do not care about we'll go over to networking and so it's going to end up creating us a new vnet for us and we'll create a new subnet for us and assign it a ip address uh it will also set up a nics network security group so the next security group is not going to be applied at the subnet level it's going to be applied at the nic which is attached to the um dc2 there and so we'll just leave it to basic we're going to allow inboard inbound ports for port 80 and 22 that was carried over from earlier we can put this behind a load balancer but i don't think we're going to do that right now we'll go over to management uh we have some additional options here for monitoring uh this is all okay here we can set it to auto shutdown actually i'll leave that alone you can also enable backups here we'll go advanced and now we have this option here for custom data i covered a section on cloud init and they don't call this user data but most other providers will call this user data so we could provide a bash script or additional information here if we wanted to then down below there's some host group options we're not going to worry about that in proximity placement group this is really important if you need to have instances nearby uh i think this is pretty common with um what's it called high capacity workloads hfc i can't remember the initialism right now but we covered in the core content then we can tag our resource here uh we'll just leave that alone i don't care about tagging too much but generally it's good to tag in practice and then we will get to review and create our server here we'll go ahead and hit create and then we'll have to download our private key so we can utilize it later and so that's downloaded there and now we're just waiting for it to deploy this and i'll see you back here in a moment so we had to wait a little bit there and finally our deployment is complete and we can go ahead and just review all the things that it created so notice that it created the virtual machine it created a network interface a nic for us the nsg the network security group the virtual network and also a public ip address when i do clean up a lot of things i always miss are these ip addresses and i know that azure gave me a warning that said hey you're about to spend uh 700 yearly on ip addresses because you weren't releasing them so when we do the cleanup step i'll definitely emphasize about deleting those ip addresses and how to go about that let's go take a look at the actual resource now so here we are and you can see we have a lot of options the left hand side such as the disk so we can see the disk options there and there's other additional security options let's go take a look at how we can go ahead and connect to the server and so there's different options here so we have rdp ssh and bastion since we are using a linux machine we're not going to be using rdp that's really for windows um but the trick here is that um i would need to have a client on my computer to connect like um i think it's called putty if you're on a linux based machine it's a lot easier uh and certainly i have the linux subsystem installed so i could probably um connect that way but i figured let's just go ahead and connect via bastion because i think this is a pretty darn cool feature so let's go ahead and create ourselves a bastion and this will take a little bit of time here but we'll go ahead here and just set up subnet so to associate a virtual network to a bastion they must contain a subnet with the azure bastion subnet so they actually have a special subnet for it so what we'll do is we'll just go back to our um our server here which we call bajor and i think we can find the subnet through here so on the lefthand side if we go to networking we probably could find it that way um so i'm just looking for that security group um in there it should be um maybe it's not there if it's not there um well you know we could just go over make our way over to subnets it's not a big deal because it's called bajor it's pretty darn easy to find to begin with and so under subnets here what we need to do is add a special one here and just gonna remember how this works um so you need to create a subnet called azure bastion subnet with a prefix of at least 27. so we'll go ahead and add a new subnet and we'll call it that uh we'll take out the space there and the range is 10 0 1 0 etc so we'll just do it on two zero dot um two dot zero four slash twenty seven uh ten zero zero two is not contained yeah it overlaps so we'll do two let's not contain the virtual network address space oh right so we have to add the address space first oops we'll just hit cancel here that's okay we'll discard that we'll make our way over to address space and we'll go ahead and add 10.0.2.0424 they'll give us a pretty darn large range there and so now what we'll do is go back to our subnet and we'll go ahead and create that there and it said it only needed 27 so we'll just give it only 27. we don't need to go bigger than we need and that should be okay we'll go ahead and hit save and it shouldn't take too long so now that we have that we can go back here and we'll give give this another go here uh it's there so it shouldn't be complaining maybe what we'll do is just start from the the start here again yeah there you go the azure portal is like that a lot where you'll have something set up and it has the old state of it and so you just have to trust yourself that you know what you're doing and you have to go back if you don't have a lot of confidence a lot of times you'll get stuck and you'll think okay i don't have it right but i always just try again and hit refresh because the azure portal is very inconsistent so we're just going to wait for this to create this does take a little while to create so i'll see you back in a moment so after waiting five minutes our bastion is now created and so what we can do is without even using a putty client or having to use linux directly we can just connect through via the bastion so here we'll see we have some options here so we want to do ssh private key from local file okay and what we can do is go ahead and select our bajor key and then i'll just scroll down here and hit connect oh um and i think we made the username bajor and we'll go ahead and connect now and it's complaining about a popup here so we'll go up here and say always allow and we'll try that again and then we'll say allow again and so now that we're into our server here let's go ahead and try to install apache and see if we can get at least the default page running um so this is using ubuntu if my memory serves me correctly it should be app get install apache 2 and we'll just hit y for continue and we'll just wait for this install doesn't take too darn long and after a short little wait there it finally did install also if you notice this little icon here we have a little clipboard here i don't seem to ever use that there so that's fine now when you install apache we might have to go ahead and start it up so let's just take a look to see if it actually is in the running directory here so we'll go to cd var www and so that's where the default directory is right but we can just check to see if it's running by doing a ps aux i think it's httpd or we can say apache here and so it looks like it's already running so that's pretty great for us and since it's running on port 80 and we've opened up port 80 we should probably be able to access that here so let's go back to our actual virtual machine so we'll go to virtual machines and we have that virtual machine running i'm just going to click into it because i just want to find out its public ip address so here it is there and for lucky this will just work just copy the clipboard button right there and look at that we have the default page isn't that cool so that's all there really is to it and i could even update this page you don't have to do it but i'm just going to update it for fun actually i probably have to restart the server so maybe i won't do that but yeah so we connected through the bastion so that was pretty darn easy we probably could have also used um the cloud shell to connect um but maybe we should we could give that a go as well since we're all done here let's go ahead and do some cleanup the first thing i want to do is the easiest way is actually to go to all resources here at the lefthand side and this really gives you an idea of everything that's running your account so i actually have other stuff in here that's not relevant but the idea is that all of our stuff is running within a resource group and so i'm just taking a look there i'm not saying this is all resources here um see if you see resource group here yeah they're all there right there so i can go ahead and click that and so everything more or less should be selfcontained within here see all that stuff you can even see the vnet is part of it as well and so if i go ahead and delete this resource group it should delete all this stuff so i'm just going to type bajor to confirm and we'll go ahead and delete and that should do a good job of cleaning up all those files i'm not sure if it'll delete the ip it should right there but if it doesn't what i recommend is after everything is deleted just go back here to all resources and just double check to make sure they all vanish because when this is done they're all going to start to vanish from this list and if there's anything remaining you'll know because it's still here right so just be careful about that that's all i want you to know and so that's the linux part and so let's go ahead and actually now set up a windows server hey this is andrew brown from exam pro and it's really important that we know how to monitor our virtual machines so we can kind of check their performance or do some diagnostics on them and so a couple things we're going to look into we're going to set up some virtual machines and we're going to look into automation accounts uh log analytics and metrics and alerts okay so the first thing we'll need to do before we do anything else is make sure that we have particular resource providers enabled so go to the top here and as always go to your subscriptions and we will click into our subscription go all the way down the bottom click on resource providers and there's two in particular we're looking for uh one is for alerts so it's alerts management so make sure that is registered if it's not press that button and then we want insights so make sure insights is enabled as well they should be by default but just in case they're not uh you should go ahead and do that so we're going to need some virtual machines i'm going to set up three and the reason i want to do that is just to show you that there's many different ways in azure to do the exact same thing and sometimes when you do something it just works and sometimes it just doesn't and it can work again and not again so i want to give you a few different options available to you so the first thing we're going to spin up is a virtual machine and this one in particular we'll make a new group we're going to call it dax and so we're going to choose for this one an ubuntu linux i'm going to switch it over i'm going to choose a minimal install so we'll choose this one over here which is the ubuntu server minimal and uh what i'll do is name this and there's a bunch of different daxes in star trek so we're going to go with kurzon dax here and we'll put all this in east us so wherever it defaults here so if that's where it wants to go that's where it's going to be but let's pay attention to where that is okay let's make sure that we're using a very inexpensive instance i'll give it a second to load if you do not see the cost just drag these over until you do it just depends on your screen right um so once this loads it will tell us that this is probably the cheapest i think that is the cheapest because something's going to go lower than at 500 megabytes of ram so i think we're already set to it i'm not going to wait for the load to take forever notice the red line there is no error is that's just what it shows and so i'm going to go for password authentication here and what i'm going to do for this account is i'm going to type in um dax as the username and then testing with a capital t e s t i g one two three four five six because it wants an upper case a lower case some numbers and a certain length and that's what works for me so now that i have those filled in ssh we're gonna keep that open we're gonna go to disk you can leave it on premium if you want i'm going to go to standard i want to save some money just in case i forget to turn this machine off it's going to set up some networking for us which is totally fine we're going to go to management and i believe so there should be guest os install here it is not here it's the next step i believe where is it i know you're in here let's go back one step here i have a feeling it's on this page because you can install the um guess os metrics right away so i'm just seeing here i cannot seem to remember where it is i might have skipped over it because i'm going so quick here um well maybe it's not set here so when we go to do the other virtual machine we'll see if it's actually there but for this one we'll just launch it without it which is totally fine i was going to do one that way anyway and so we'll go ahead there and create that virtual machine okay now while that is going and by the way if you think you call that number that's not me so uh you know you can call if you want but it won't go to me so that's going to go ahead and deploy that what i'm going to do now is go and launch another virtual machine in the same uh in the same environment so this one this time around is going to be again it's going to be linux we're going to choose dax here and this one is going to be jadziadax and we this time around are going to choose ubuntu server lts so this one's a minimal install and this one here has a bit more to it we're going to choose gen2 uh this is now 4.86 which is strange it looks like it was the same one before i don't know why the other one was still more expensive because where it said 40 dollars but maybe i was mistaking the red means nothing as per usual it's just as you're uh tricking us we're going to call the username here dax testing with a capital testing you know what i'm trying to say here and i'm going to do uh testing one two three four five six and we will go next to disks we'll stick with premium for this time it doesn't really matter if we go to advanced here there's no uh interesting options there this is all fine this looks good we go to management so now notice here so we didn't have it for the minimal install but for this one we do we can actually put in the guest os diagnostics and what it will do is it will have to create itself a storage account so that will be in azure storage to store our diagnostic data and so that seems like a good idea to me so what we'll do is we're going to go to next but notice we didn't have it for the minimal install probably because it doesn't have the um the agent what's called the wag agent installed for it to do that by default so just notice that it varies on some machines we're going to go to next we're going to hit review and create and we're not done yet we're going to create one more virtual machine and this time it's going to be a windows once i hit the create button here uh we're going to go back to virtual machines and this time i'm going to launch a windows machine and i'm going to use we're going to put in the same group here i'm going to call this one uh tobin or let's call it little dax here and this time i'm going to choose a windows server so i'm going to go to windows server here we have a lot of options i'm not that great with windows servers i'm going to choose a gen 2 windows server here it's not going to let us get away with this b1 l it's not big enough you need a lot more compute and memory i think it's like two and four uh but i just remember it always choose ds ds2 v2 um which is pretty hefty it's pretty expensive maybe i could choose something cheaper like the ds1 no i need to i need two cpus so ds2 v3 that's when you're launching a windows server if you don't do that they'll complain and that'll probably be pretty darn expensive but we got to make sure that we just shut this off and get through this as quick as possible i'm going to choose the same thing as always so i'm going to i'm going to put in dax and then for the password i'm going to do testing 1 2 3 capital on that t 4 5 6 testing 1 2 3 4 5 6 because it wants 12 characters we're going to let it do on the rdp that's totally fine uh sure premium ssd we're not going to keep this around for very long we're going to scroll on down go to management and notice that you can enable os diagnostics here i'm not going to do that here just so i can show you show you it in the other variation we're going to go to next we're going to go hit review and create and we're going to go ahead and create this instance whenever it lets us do it there we go okay we'll hit create and now it's deploying so we have these three virtual machines we're creating and so what i'm going to do is make my way back to virtual machines here and so i have the two there i'm going to give it a refresh make sure i can see the third one creating we don't see it just yet but these ones are running so what i'm going to do is open up kurzone i'm going to open up jedzia and one has the guest um the the uh the guests ins or the what's it called diagnostic settings it's the guest metrics installed and the other one does not so if i scroll down here so i just want to show you there's a lot of variation and sometimes you know you do it one way it works another way it doesn't so for jetsea it was installed on default so that's the one that is using ubuntu standard and then this is the minimal install where it's saying hey do you want to install the metrics and you have to pick a storage account if you didn't then you would have the option to create one here it's called dax diag so i just want to show you we're going to go ahead and enable those guest level monitoring metrics okay so we have a storage account and that's where the data is going to go um and this will probably fail because i don't think it has the necessary things installed but i just wanted to show you you could do it that way but what another thing we're going to need is we're going to need um a log analytics workspace so all a log analytics workspace is is a place to store your logs it is essentially a data lake for a lot of different resources so you can put all sorts of resources under here and that way you can search across them see how this failed if you ever have a failure like this the way you investigate this as you go in here even though it says assass token for the storage account couldn't generate it's kind of a misleading thing here because if i actually click in up into my notifications we go to our activity log we can see kind of what something has happened here so i know this failed it might not show up here yet sometimes it takes time to show up so we'll revisit this later but it would show red errors here and if we looked in there it would tell us that python's not installed on that machine because it's a minimal linux install so if you run into those problems that's the reason why some vms are a bit easier to work with than others but we're looking at log analytics and again it's just a place to collect your logs but i want to see because i think jitsi is the one that actually is the one that is working properly but if we go down to metrics or logs it's going to say hey you if you want to use logs you're going to have to go ahead and create you have to enable it but what it's really doing is just creating a log analytics workspace so if i click this here what's going to tell me is say hey you don't have one we'll create a default one for you um i don't want to do it that way what i want to do is create one myself and then link it through there or what i could do is make it through automation so what i'm going to do is i think what i'm going to do is i'm going to do through automation so there's another service called automation accounts and this particular service is for configuration management update management and it's a way for us to keep our vms up to date and so this one's going to be called dax oh it's too short it doesn't like that dax automation and i'm going to choose the resource group dax it's eses and here it's going to say azure run as account so this is going to give it more permissions so that it can run as a contributor role and that way it will have access to resources to do what it wants to do so we'll go ahead and deploy that and then once this is deployed we'll create our our workspace through this because i found an issue where if i made this and tried to link it just didn't link and i don't want to have those problems so i'm just going to make my life a little bit easier by making it through here so under here if we go to update management it's going to say hey you need a workspace in order to use update management if you're wondering what update management is it keeps your vms up to date by applying patches so if we drop down here we're just going to go ahead and create a new workspace and i'm going to do it that way because it'll be automatically associated with that workspace on creation and uh we'll let that go there for a bit but while that's setting up a new workspace for us if you go over to run books this is the power of well i'm not i'm not it wants to make that thing there so i'm just going to click ok there don't worry about that discard but the whole power of this is that it can apply patches it can it can manage the state of your vms but if you need to do something like run something like a a series of run commands they have a bunch of different run books here uh so there's a variety of different kinds and i think we can click into them if there's something about sql that sound kind of how to use an escrow command on a automation runbook so that's just a runbook so runbook just means like a series of run commands something that you're automating on your server and you do it through here um so update management looks like it is now working uh we didn't press that enable button uh initially but it worked there but if we wanted to have vms under here we'd have to go ahead and add them manually um and so we're just gonna wait for those to show up alternatively if we go back to our server or sorry one of our vms we should be able to add it this way it will just tell us to go that way anyway um so there should be like update management or something in here could be configuration management um yeah that still would be through something right so it'd probably be still through automation but i could have swore there was maybe guest and host updates yeah so here it's just saying go to azure automation so you don't really set it through here you have to associate it through azure automation and so these are ready to enable so i'm going to say go ahead and enable those i don't know where our windows server is we definitely spun one up it's not a big deal if we don't have it it's not really crucial to this tutorial but it would have been nice to show um the windows 10 there i'm gonna go see what happened to our windows server oh it is there okay leela's there so we'll take a look at that in a second uh but we're just adding those there we're not going to really apply any patches i just want to show you how to associate them there um and just to highlight the fact that there are run books which are pretty cool and you can also do other things like if you need to like install python packages or stuff like that uh to to do stuff you could do that as well um so there's a bunch of interesting things in there but it's kind of like a side dish to the follow along here so this would have created our workspace so now let's go over to our workspaces and we have one here and if you're just curious what it looks like if we go here something super exciting you just put the name and region in and it has the default page you go so you're not really missing out on creating one uh by hand there but underneath here what we can do is see if our virtual machines are actually associated uh with this here so if we go under virtual machine you're gonna see that these ones are connecting which is interesting because i didn't do anything to connect them i'm not sure why that's happening but i'm going to hit connect here and that will connect that a lot of times i'll come in here i have to manually connect them i don't know if it's because we put them in the uh what do you call it within the uh the automation there if we go back to automation which ones did i choose i already forgot update management um they're not showing up here now but i know they're there maybe we triggered it when we were doing there but anyway the point is is that you have to make sure that those are um configured so that it's actually sending data over to the workspaces so that we can then use the logs and things like that so if we go back to our virtual machines over here you can see they're connecting and this takes forever while this is going let's go take a look at storage accounts and actually a better way would just be to go look at the virtual machine individually here so we'll just go back to my virtual machines and i go to the resource group we scroll on down here we should see a storage account within here let's just type in storage there it is whoops and uh the thing is is that we can actually view some of the data in here so if you go to open explorer you'd have to install this it's for windows i don't i think it's for mac as well but once you have the storage explorer uh open you can just click open here and it'll open up once it's installed on your computer of course and what i can do if we have any data it might not be collecting any data yet but if we go down to our storage accounts here oh here it is down below and expand it under tables see how this wad that is the name of the um uh the agent that is collecting data and here you can see that it's already collecting stuff all right and it's collecting for jet zia dax and so i'm just kind of looking at the data that's collecting nothing super interesting but the thing is is that we're gonna make our way over to logs because by default azure collects host metrics so things about the instance like cpu usage but there's things that it doesn't collect like memory and other things like that so that's why we need to turn on this setting here the diagnostic settings so we can collect those metrics these are called guest metrics so we have memory network file disk disk and storage so if you wanted to know how much storage was left on the on the actual uh disk on with the with the virtual machine or how much memory was being used you'd need that uh set up here uh once this is enabled uh you can send data over with uh into azure monitor and other places um it's not showing app analytics i think that's for or log insight whatever the name of the service is it's for uh that one i think is for the windows one so we'll see that when we go over there but syncs just means we're gonna send data uh from here to to um azure monitor right but we'd have to have preview to see that uh nothing that interesting here uh but if we go over to logs now um this should not be showing up here anymore so if this is still showing an enable screen then we got to go back to our log analytics we'll click into here we'll go down to virtual machines and we'll see if it's connected yet so it still says it's connecting right this one's ready though and i haven't even done anything with that one yet so what we'll do while we're waiting for these linux ones to uh to spin up we'll go over back over here to our virtual machines and we'll go find that uh windows machine which i don't know why it's not showing up here there it is and what we'll do is we'll go ahead and connect to it so i am on a windows machine so i'm going to shape here and i'm going to click on rdp and i'm going to download that file i'm going to double click this file on my windows machine and i'll be able to open this up instantly it was called dax testing one two three oops testing one two three four five six looks good to me we'll hit okay we'll say yes because if you want to collect um uh particular metrics like guest metrics you have to enable like when you're on windows server you have to enable it to uh to send performance counters so i'll give it a moment here it doesn't take too long while that's going we should look to see if we actually have um the uh the settings set up here the diagnostic settings and so this is what i'm talking about the enable guest level monitoring so we want cpu utilization disk and network so we'll go ahead and click that there and while that's going to go back here and so this has now started up i don't know much much about windows servers but i do know how to turn on at least last time i did i know how to turn on the performance counter so see here it says online performance counters are not started so i can right click this and then just say start performance counters and that will have it start sending data over to our storage accounts assuming that this is enabled here so this is going to go ahead and install wag into the windows server so it can send back metrics uh and so we got a lot of things going on here but let's go back to jadzia and um i want to see now if i have any if i can use logs now there we go so it's now connected to the workspace so it knows where to send that data and you can pull up when you go to logs it'll show different types of default queries if you're in the actual group here you'll have the same screen but it'll show you across all possible resources so it's a little bit easier to figure out like useful queries for virtual machines when you do it through here and there's some ones there's some that we can run right away so if we go over to um the diagnostics we could or actually availability is a good example so heartbeats will tell you if the server is live so a server will have heartbeat so every whatever second or minute it's going to say hey i'm still alive that's what a heartbeat is so we'll go ahead and run that and we'll give it a moment here i'm not sure why it's crying that doesn't work i'll just click off and try that again so go back to availability we'll hit run and it has the right scope if you're wondering how it knows what to check that's all based on scope there and for whatever reason it just decides that it doesn't really want to run right now that's a bit frustrating but if it's not working for this one we can always check our other one which is curizon and we can go see if this one works that's why i made multiple servers because i knew one of these might not work right away so i'll go ahead and run this one here it says no logs are configured so that makes me think that our workspace here if we go back over to here and we go down to virtual machines that they're actually not linked oh they still are connecting there you go so it's a bit misleading it looked like we could query but we still can't um let's go take a look at this virtual machine here our little one still and let's see if we can do anything with it yet so we go down to diagnostic settings it looks like it's installed and so if we go to performance counters these are the counters that it should be collecting data on you could do custom if you want and change those values but we're just going to leave it basic for the time being go ahead and save that and i believe we can do logs over here on this one so we'll go over to availability i'm going to load that into the editor this time instead of hitting run i can hit run and it will return and says that it has a count to seven so seven heartbeats that's good if you wanted to create uh rules you could easily do that there's actually uh some nice ones here if we go into here um this won't work unless our our our metrics are working our guest metrics but there's a bunch down below here i'm just trying to see one that might be interesting for us to set so you might go here and i don't think i like that one say this one here i'm just going to clear this all the way out oops so here they even say to create this alert just hit the plus new button here so we go here and sometimes it does this it doesn't actually bring it over which is weird so you have to go back i told you like this the the ui for azure is super super buggy so if you don't have any confidence uh don't worry it's it happens to all of us um so we'll go back to our alerts and i'm going to load that into the editor and what i'm going to do is just copy it and um we're going to go ahead and hit run and so here we're getting some data back i think it's going to complain about the time generated but if we go and create a new alert oh man it just does not want to uh copy anything over a and so it's like i can't even paste we'll go back to here i'm going to copy this i noticed this because i had it double clicked maybe just just creates one what you have selected and so it should complain about this but it's not so that's great and so we can go here and we can put a threshold to five i'm just putting any value here and that gives you a monthly cost for that alert so you hit review and create please correct the following details where oh the alert name okay my alert and we will hit review and create we'll hit create and so there we can create an alert so we could create alert like our memory is running out of stuff and those are kind of useful things to do so talking about metrics we're going to make our way over here and notice we have host metrics and guest metrics so host metrics are general ones that are available to us and guest metrics are memory and stuff like that so if we have that you're going to notice now we have like disk and memory and things like that if we didn't have that installed we wouldn't be able to access this information so what i can do is go ahead here and try disk time i don't know if it'll show us anything right now because it might have not collected enough information go over to virtual host we'll have different metrics here so a standard one would be like cp usage okay so that one is displaying there the poor performance metrics always take a little bit more time to collect so i'm not sure if i'll stick around to show you that but if we go over to availability sometimes we need to go debug performance like you can hit run and this will tell you if it's actually collecting information so see right now it shows me nothing so it hasn't collected anything for process memory or processor but if it did then we could go and use these other ones down below here so we have free disk space so that's a really useful one to have which we can't get metrics on that right now but that's what we've been trying to do is collect that information so let's go back to our workspace and what i'm going to do is take a look at our virtual machines that are connected and this one has an error um and that's kurzon and i'm not surprised because again it's a minimal windows linux instance so probably couldn't install the agent so that's expected but i just wanted to show you that edge case if we go into the jet zeo one which is the one we were waiting for this entire time we should hopefully be able to see some metrics so if we go down here and we choose yes metrics i want something with memory here uh like percentage okay so it's still not showing me anything and that's fine if we're not getting any metrics what we can do is we can go log into this instance and give it some data so that it does something all right so what i'll do is go all the way to up to the top here into my overview i'm going to copy this public ip address we're going to open up our cloud shell and make sure that it's in bash all right and we're we're going to type in ssh um dax and then we're going to paste the ip address in we'll hit enter we'll say yes and the password is testing123456 with a capital t and what we can do here is we can install a tool called stressng what that tool does is it will help us do stress testing and so that allows us to make some metrics so we'll go ahead and install that via snap snap is the new package manager for ubuntu used to be aptget or apt it doesn't take too long to install and while it's installing i actually have a premade uh line here to check the memory or to fill up the memory i'm just going to go ahead and paste this line in so this is stress hyphen ng vm bytes and it uses awk it's going to do memory availability and it's going to fill it up to 90 percent so we want to have 90 memory usage i'm going to go hit enter i'm just going to let that run now it's not outputting anything but it definitely is running there we go and so we're just going to leave it there and so soon enough we should have some information and i think if i go there yep it's still there so it's fine so now if we go over to um down to our logs and we go back to performance and we just choose this one here we might see something now again it takes time so you know it looks in the 15 minute uh window here but i just want to see that it's collecting anything for me it doesn't matter what it collects just collects something for me you know so i think it'll just take a little bit of time while we're doing that we should make our way back over to our workspace because there's some other settings that are kind of interesting that i noticed if you go over to agent configuration um you can collect additional information over here so we have performance counters for windows and linux i think we have to set the memory one here if we wanted to actually appear here so i don't know if there's a way to set all of them but i'm gonna just choose some here so i'm gonna do memory used we'll do logical use space here we'll go here and do free space i think that might be the reason why it's not showing up for us maybe process time here we did use memory used percentage of use space we will do um do reads and writes so here we have some for our linux so i think if i we set that then we should be able to see some information uh syslogs is just a type of logs that you can collect we're not going to look into those today but yeah that's the management there we could also try setting some here maybe one for memory i can't remember what the one for memory is called here because they do all of this kind of stuff like windows has so much stuff i can't make sense of it i think we might want paged owls and page so that went called there bytes we'll give that a go and we'll see if that starts collecting information i'm going to go back over to uh here and oh this is kurzon that's the windows machine we're going to go to the linux machine here and we'll give it another go see if we get anything this time around so we'll go back to performance we'll say what is being collected run okay and so i think we're just going to have to wait a little bit so i'm just going to stop the video here and i'll see you back in maybe 1520 minutes okay all right so i'm still waiting here and i don't seem to be seeing any data but that's okay because there's another way we can kind of approximate this and we don't have to wait forever especially if you don't have a lot of time like me the thing is is that azure has a log analytics tutorial and within here they have a demo environment that i'm very familiar with and so this can approximate exactly what i'm trying to show you here if you notice at the top here this is microsoft azure monitoring logs demo log blade so if we go to queries and we go down to virtual machines and we'd look at what data is being collected we can hit run here and this is the kind of information we are trying to see right this is what we want to see what it's actually collecting that's okay we're not doing remote desktop anymore here and if we go to virtual memory available we'll just clear that out there and run that this is kind of the information we're trying to find all right so i just wanted to show you that when you collect those metrics that you can run these queries and this is that language up there with cousteau and that's pretty much it that's all i really wanted to show you so let's go ahead and tear down all our stuff it's all selfcontained within a resource group so what we can do is go to all resources in the top left corner there and we called it dax so we can search for dax here whoops actually it's all over the place we just click here into the name and we'll hit delete resource group type in dax we have a lot of resources created for this one and there you go so yeah hopefully that helps you understand uh those relationships between automation accounts where we can run run books do patching uh you know stuff like that install python stuff um you know the fact that log log analytics is basically a data lake for all your data that you can create alerts uh that you have metrics you have guest os metrics which collect those three values the memory the network and the additional cpu information or disk space and just the variance there so you don't get stuck so there you go hey this is andrew brown from exam pro and this follow along we're going to learn about managed identities or system managed identities so that we don't have to pass credentials or hard code them into our virtual machines or other environments so what we're going to do is launch ourselves a new virtual machine this is going to be a windows server so go ahead and hit create we'll create a new virtual machine and from here we'll say create new and i say my managed identity probably say id so i'd have to spell the whole word and we'll just say my managed id and from here central us is fine the zones are fine but we're not going to use ubuntu we're going to hit see all images and from here we'll choose windows 10 and there's a lot of options here so i want windows 10 pro um it's a bit hard to decide so i'll go gen 2 2 1 h2 hopefully you'll be okay doing this as well um it's using standard b2s 46 so just make sure you're not overspending so that should be fine i think we need at least a b2s to run this um but we're not going to keep it around for very long so username i'm going to go admin user and for the password capital t testing one two three four five six i can do that again capital t testing one two three four five six exclamation mark just because of the requirements such a pain that's what i always do in these follow alongs and we're going to use rdp to connect to it because we do have to uh remote into it we'll just say i confirm we'll hit review create of course we don't actually have a license but we'll be okay because we're not doing anything too serious and we'll give it a moment here and we'll hit create so that is going to create while that is going i'm going to uh we'll actually have to wait a little bit but we'll make our way over to the resource group because this is where we're going to be adding an im policy at some point so if we go over to i am over here and we'll wait we'll actually wait for this to provision so this takes quite a few minutes so i'll see you back in maybe 5 10 minutes because we are launching windows server they just take longer so after waiting you know five to ten minutes the environment is ready so we'll go ahead and go to that resource and we're going to want to go connect and choose rdp this is going to be remote desktop protocol i believe that's what it stands for if it doesn't not a big deal just know it's called rdp and so we'll have an rdp file we'll double click it we'll hit connect this is going to open up the room remote desktop client and so we will type in admin user and then capital t testing one two three four five six if you're on a mac you're like how do i do this well it is in the mac store just type in remote desktop it can open here i'll just show you what i mean over here we do rdp mac you can see that it's free to download so it's just you download that and you can open those rdp files so we are just waiting for this to spin up this takes a little bit of time so just give it a few minutes all right so after waiting a little while the environment is ready so hopefully you can see what is going on here and so what we want to do is open up powershell so we'll type in powershell here and we will right click and run as administrator this does have an outbound connection the internet so i know that uh we are okay for our connection and then this is where you will need to install azure modules because it's not preinstalled and this part really sucks because we have to wait a lot more again so i hope you like waiting a lot but anyway what we'll do is right click this properties here and we will bump this up because this font is super super small and uh we'll type in install module a z and i'll install all the commandlets for azure or azure and we'll wait for some prompts the prompts are a bit finicky hopefully we can just do a for all but you might have to hit yes a couple times but this part is just so so so slow there we go so notice it doesn't let me say all we'll just say y and i know that it's going to prompt us again so we'll just wait a little bit here the progress will show up at the top it's kind of glitchy um yeah so now we'll do a for all and so now we just wait a long time now it seems like it's not doing anything but you will eventually see something appear up at the top here that shows you that there's progress but you just have to be very patient yeah so it'll look like that i'm just showing you what it looks like and i'm gonna go back and pause till this is all done because it takes forever all right so i think it's done it's sometimes hard to tell because it'll just kind of like blank out here and it's not doing anything so i'm not 100 sure but what i'm going to do i'm going to see if i can just open it twice this is just the powershell experience right and um to test this we'll just do a connect and see if it works so i'm going to do connect i just want to close the old one if it's actually still doing something connect account identity and so if this prompts us with something then we know that it's working correctly so identity not found and that's fine so the thing is if we just just did this it would actually prompt us to log in so see it's prompting us to log with microsoft so this this worked um but we need to actually connect an identity to this virtual machine so that's what we're gonna do so we'll minimize this we'll go back over here so go into your uh the resource group that we had before under iem and we're going to add a new role assignment and from here we will uh choose contributor because that will give us enough access for stuff we'll choose manage identity and we will choose system identity no no no no that's not what i want because it should show the virtual machine here maybe it's because um we opened this before it was done creating so that's why it's not showing up so we'll try this one more time we'll go to contributor because it should show the virtual machine manage identity and select and i want to see the virtual machine weird so i'll click way out i'll go back in i'll go to i am again we'll go to oh you know what we launched this virtual machine we forgot to check box something so when you launch the virtual machine you're supposed to checkbox manage system identities here so this would be where would it be i only ever do it through the setup i never ever have to retroactively do it oh what a pain identity maybe system managed so we'll just say on and we'll save it uh grant permissions do you want to enable a system energetic yes but normally when you you do the virtual machine hold on here we'll just go pretend like we're setting up a new one we're not actually going to set one up but when you are setting it up and we go over to advanced no no management we would check box check box it on uh here that's not what we did so we forgot to do that but we turned it on here so hopefully it works without us having to restart the image if we go to role assignments over here i mean that's just another way of getting to this page so it doesn't really matter for the resource group what we'll do is go back to our resource group we will find it here and we will go to i am and we will add a role assignment and we will add contributor we will go next we will say manage identity we will select we will go to virtual machine we will select this one we'll hit select we'll review and assign we'll review and assign and then it will sign us contributor access so now we should be able to connect our identity and then also get access to some stuff um because what we really want is to do get a z like to just show that we can get anything because right now we do this get ac subscription we're not we're not um connected in any way so we'll go here and we'll do identity fingers cross it just works yes it does that was nice if we didn't have to restart the machine and so now if we do get ac subscription it should work so i don't see it working do i type it wrong the term is not recognized oh i must have typed it wrong subscription here let's get this description so if we were not authenticated that would not work and so it does work now so we are pretty much all done here well we could probably set some resources just to make sure this is working properly so maybe go get a z resource like that there so list some resources and it's only listing what's in the um the resource group because that's all it has permissions too it's not going to show things outside of that because that's the scope when we did it at the resource group and that's why we went at the im level there so we are all done we will go ahead and close this out okay we'll make our way back to our resource group we're already on it we'll go ahead and delete this resource group say delete and there you go hey this is andrew brown from exam pro and we are looking at virtual machines for azure cheat sheets so let's jump into it so azure virtual machines allows you to create linux and windows virtual machines the size of the virtual machine is determined by the image the image defines the combination of vpcus memory storage capacity the current limit on a prescription is 20 vms per region vms are built at an hourly rate a single instance vm has an availability of 99.9 percent when all storage disks are premium two instances deployed into availability set will give you 99.5 95 availability you can attach multiple managed disks to an azure vm when you launch an azure virtual machine other networking components will be either created or associated to your virtual machine such as nsgs interface cards ips and vnet you can bring your own linux by creating your own virtual hard disk azure vm comes with a variety of sizes that are also uh optimized for specific use cases and there's a lot of different ones here uh so it's not super important but i mean you should know the broad categories here uh azure compute unit acu provides a way of comparing uh cpu performance across azure skus and the standard one that the one they're all benchmarked against is standard a1 all other skus then represent approxim approximately how fast faster the sku can run on the standard benchmark you can install the azure mobile app so you can monitor your vms on the go hyperv is microsoft's hardware virtualization product it lets you create and run software versions of a computer called virtual machines that's how it all works there are two types or two generations of hyper vm generation one which works with most operating systems and generation two that supports most 64bit versions of windows and current versions of linux and freebsc uh operating systems hyper v hyper vvms are packaged into virtual hard disk formats such as vhd vhdx files not on the exam but just understand that uh hyper vm generations are different from the azure hyper vm generation so it doesn't have all the features of hyper vm okay there are three ways to connect to virtual machines the first is via secure shell it's not sure shell it's secure shell but yeah so that'll have it get fixed at some point ssh happens on port 22 and rsa key pairs are commonly used for authorized access then you have rdp which is a graphical interface to connect to another computer over the network connection this is how you can remotely connect to the windows server via virtual desktop this happens on port 3389 on tcp and udp now i mark these red because i want you to remember these two so make sure you know them okay azure bastions and we'll clear that because now it's getting a bit of a mess here there we go so ask your azure bastion a service you can deploy lets you connect to a virtual machine using your browser and azure portal supports both ssh and rdp useful when you only have a browser like a chromebook or do not have permission to configure or install software then you have update management this allows you to manage and install operating system updates and patches for both windows and linux virtual machines that are deployed in azure onpremise or in other cloud providers update management will perform a scan or update compliance a compliance scan by default is performed every 12 hours on a windows and three hours on a linux machine that can take between 30 minutes and six hours for the dashboard to display updated data from the managed computer so there you go that is the virtual machines chi chi so let's take a look at arm templates uh and before we jump into it let's just make sure we're familiar with infrastructure as code because that's what arm templates are all about and this is the process of managing and provisioning computer center data centers such as azure through machine readable definition files such as a json file and in this case an arm template because that's all it is a json file and rather than physical hardware configuration or interactive configuration tools and just to simplify that even further basically you're writing a script that's going to set up your cloud services for you and with iacs they usually come in two different flavors we have declarative that's where exactly what you write is what you get and imperative so you're going to generally define what you want and then the service will guess what you want it's going to fill in the blanks to save you a lot of time writing those scripts and so arm templates are json files that are that define azure resources that you want to provision uh and azure services that you want to configure and with arm templates they are going to be declarative so you get exactly what you define uh they're gonna stand up tear down or share entire architectures in minutes and they're going to reduce configuration mistakes and you know exactly what you have defined for a stack to establish an architectural baseline for compliance so you definitely want to use arm templates or iac whenever you can and try to avoid using the console unless you're doing development or test workloads but let's just run through the big list of what you can do with an arm template i know this is a big boring list but let's just get it get through as quickly as possible just so we know what we can do with arm templates so with arm templates that are declarative so what you see is what you get you can stand up tear down share entire environments minutes you can reduce configuration mistakes you can establish an architectural baseline for compliance they're modular so you can break up your architecture multiple files and reuse them they're extendable so you can add powershell and bash scripts to your templates uh it has testing so you can use the arm ttk to test make sure exactly what what you've deployed is what you wanted you have preview changes so before you create infrastructure via templates you see what it will create builtin validation so it will only deploy your template if it passes track deployments keep track of changes to your architecture over time policy is code so apply azure policies to ensure you you remain compliant uh and then you have microsoft blueprints which we we did cover here uh which establishes relationships between resources and templates so it's just one step further where arm templates don't have any relationship with the resources where blueprints is like a better version of our templates and then you have ci cd integration exportable code so exporting the current state of the resource groups and resources authoring tools i believe is the last on our list here so you can use visual studio code that has advanced features for authoring arm templates makes it a lot easier to write them so there you go so now we were talking about what arm templates can do let's actually take a look at what one looks like and so just to get a good snapshot or overview let's just define what the skeleton is the general structure of an arm template so here on the right hand side we have json and you can see that we have a structure there so we have schema content version api profile parameters variables functions resources outputs and let's go down the list here and see what all these things are so the schema describes the properties that are available within a template and so the idea is that you have that json link there and it's going to say we expect the schema the actual structure to have these parameters then you have the content version this is the version of the template and you can provide any value for this element it's totally abstract it's just your way of keeping track of the version of your current template then you have the api profile you use this value to avoid having to specify api versions for each resource in the template you have parameters these are values you can pass along to your template you have variables this is where you transform your parameters or resource properties using function function expressions you have functions these are userdefined functions available within the template you have resources these are the azure resources you want to deploy or update and then you have outputs and these are the values that are returned after deployment so let's go more into deep into some of these things so let's take a deeper look here at the a resource section within your arm template and so a resource is an azure resource you want to provision could be a virtual machine could be a database and so here on the right hand side look at where it says resources and we're going to take a a breakdown of the actual columns or tributes that we can set so the first thing is the type and this is going to follow the format of the resource provider and resource type so there you can see that we are setting up a storage account then we have the api version and this is the version of rest api that we're going to use for that resource and each resource provider publishes its own api version so you've got to go look up each one because they all could be different then you have the name of the resource and so i believe there that is using a variable so that is going to be dynamic then you have the location so most resources have a location property and that's the region that you wanted to be deployed in then you have other properties and other properties can will be based on different kinds of resources so for a storage account you want to be able to set the kind and within the properties you're going to have a bunch of other options there and it's just going to vary okay but anyway let's take a closer look next at parameters so let's take a closer look here at parameters and these allow you to pass variables to your arm templates so on the right hand side we have our arm template and we saw before resources but if you look above there we have a thing called parameters and so there we are setting a parameter and then down below in the resource we are accessing um a parameter and so let's just actually talk about some of the parameter options we have so the first thing you'll want to do is set a type and on the right hand side you can see that we're saying as a string and so we could define that as a string secure string in blue object secure object and array and there are some other things that we can provide when we when we are defining your parameters that we're not showing in that diagram but we have default values so this is if you do not provide if there's no value being passed in this is what it should use then you have allowed value so this is where it's an array and you're restricting it to only the values that are there then you have the min value so if you let's say it's an integer you want to say this is the minimum value so it can only like nothing lower than 10 then you the max value so nothing above 10 you have the minimum length so if you're working with strings you might say it has to be a minimum of five characters then you have the maximum so let's say you have a maximum of five characters uh then you have the description so this is just going to be something that when you are using the arm template within the portal you know what it's for so if you don't have a great description for that parameter or name and you want to provide more information that's what you can do so there you go so let's take a look here at functions for arm templates and these allow you to apply transformations to your arm variables and they come in two flavors we have template functions which are built in functions and then user defined functions which are custom functions that we're creating to allow what other kind of functionality that we want so the way functions work is you give the function name so here it's called equals and then you have parentheses and you pass in uh what you want to transform so if you see parentheses that's how you know that is a function so let's talk about some of the builtin functions and so there are a lot available for us and they're generally selfexplanatory so i'm not going to be showing examples but let's just go quickly to the list so you have array functions like array concat contains create array empty first etc then you have comparisons like less equals less lesser equals greater greater or equals you have date functions uh you have deployment functions and for example parameters and variables are actually functions so that's kind of interesting then you have a logical operator so end or if not or numerical um functions like add div float int min max object so contains empty intersection and then you have resources so extension resource id providers references etc like that uh and there's one more i think we have string so we have base64 concat contains etc i'm not going to show you how to do user defined functions i don't think that's that important but i just want to show you that you have a lot of functions available to you okay so let's take a look at variables and variables are used to simplify your arm templates and the idea is that you're transforming parameter and resource properties using functions and then assigning them to reusable variables so under your variables section notice that we have storage name and on the right hand side we're just using a bunch of functions and assign it to storage name so the idea is that when we have to go call them in our resources it's going to be a lot shorter than if we had to write that whole thing in there and what's more interesting is that you can nest your variables within a json object so on the lefthand side you have this variables environment settings test instance sizes etc and then on the righthand side we have a parameter that we're defined here and then down below you can see that we're calling that parameter and then we're calling variables environment settings and then using the square braces to then call within that nested object i know that seems really confusing but just take a moment to look at it and it will make sense and you'll just understand that there's a lot of power there so there you go so last on our list here for uh arm templates is outputs and outputs return values from deployed resources so you can use them programmatically uh this is very simple the idea here is that you have outputs and let's say you want uh to output the resource id so you can say that it's a string and in the value you could use a variable or a bunch of functions and the idea is that when you use the cli so imagine here you are using the cli and you're saying show me this deployment group and i want to see the output of value so see where it says query says properties output resource id value the idea is that now i'm going to be able to pragmatically access it after i've created the resource and this is a great way of chaining things so you might have a bunch of arm templates you want to run in sequence and you're going to need the outputs to go into the next one and that is what you're going to use outputs for so there you go hey this is andrew brown from exam pro and we are looking at azure resource manager templates also known as arm templates and this helps you deliver infrastructure as code meaning that when you have a resource such as a virtual machine or a storage account instead of manually configuring it every single time through the portal what you can do is provide a configuration file that defines all the properties that you want it to be configured with and the idea is that you can keep this file and share with other other people so they can easily create the same resources as you and then you know exactly how your stuff is is configured so what we're going to do is uh launch a new template now you can't go up here and just type in arm because these arm templates are managed at different levels so at one level is a script subscription or they're resource groups so when you have a resource group you have deployments within them and that's where these templates are deployed but just to deploy one from here what we're going to do is type in deploy why they didn't make it so you can type in arm i do not know but if you go down here we have deploy a custom template and so from here we have some common templates so if i click into web app and i go edit a template we already have some stuff prefilled in i'm just going to go back and discard that go back to select a template and we're going to build our own and by default we'll have that schema that content version which is 1.0.0.0 our parameters and our resources so today i want to launch a virtual machine and what you normally would have to do is go here uh and look up what is that you want to create so if it's this uh microsoft compute virtual machine you'd go through here and you'd have to make sure you have all these uh properties so you define the resource here right the type and then you define the properties that you want and down below you can go through here and see them all that's a lot of work i don't want to do that so i'll go to add resource here drop this down and click uh where's it virtual machine where are you there you are and i'm going to call this one wharf and then wharf and wharf because it's not just going to create a virtual machine it's going to create the other things that i need with it as well such as the storage account the network interface and the virtual network so you can see that we have a bunch of parameters here so the name the type the name the admin username the password and the os version oh you know what i think i chose a windows one i do not want a windows one i want a linux one because that is easier for me to work with here so we choose ubuntu so i'll just fill this in again all right and so um back up here you know we have the ubuntu version between some versions here and then there's the type so that's for uh replication then we have variables here so if we go to vm size this is the vm it will it will set here variables are either you can have string values or you can use functions to transform other parameters into other stuff that you'll reference throughout your template then down below we have those resources here so what we'll do is actually i'm going to copy this because it's very highly likely we're going to want to make some kind of change and so i have vs code over here on the on the left or right hand side i'm just going to paste that on in there and what we will do this is a json file make things a little bit easier here great and what i'll do is just move that off screen and we'll go ahead and we will save this and we'll see if we can deploy this so i'm going to type in wharf here and we'll launch in canada east i'll name this wharf we will name the username warf but lowercase and then we'll do testing one two three four five six capital on the t notice that it is hidden there and then we will choose 14 which is defaulted here and lrs we'll go ahead and do review and create and we'll hit create here so this is going to fail i already know because it has a misconfiguration it'll tell us how but while that's going we'll take a look at our input so this is the values that were inputted these are the outputs if we had defined any which we have not um and if we go back to our template i just wanted to show you that we have that secure string so when we were typing our password that's why we didn't see it so just things like that so i'll go back up here and our deploy failed why what happened so we open it up here the requested vm size standard d1 is not available in the current region so the template we have is not that great it needs some configuration because we can't use d1 i think that doesn't exist anymore and so what we really want to use is the standard b1 ls all right standard b1 ls so i'm going to cut that and for the time being i'm going to go back to our original template and this is one big template i'm going to look for those variables oh they're all the way at the bottom here nice and so i'm going to just go ahead and paste that in b1 ls just double check making sure i spelled that right standard standard b1 ls looks good to me so i'm going to move that off screen and the question is what do we do what do we do when a deploy fails so let's go take a look at what has happened here so this all got deployed into a resource group and under here this is where our deployments are so when we look at this template it we can see that it failed we could click into here get the same information and if we click into here it just brings us back to where we just were but if we go look at what was actually deployed under our resource group under the overview we'll notice that it created the virtual network the storage account and the network interface so when it fails it creates what it can but it doesn't roll back okay so the question is is then how do you do cleanup so you might think i'll go to deployments and what i'll do is go ahead and delete that template and we can go ahead and do that which by the way you can't edit this template all you can do is all we can do here see i just want to show you that you cannot edit it we can download it and stuff like that but so you might think well if i go ahead and delete that template just making sure we're in the right place here you might think that might roll back those resources but it doesn't it just deletes the template so if you really want to get rid of the stuff what you got to do is go ahead and delete all these resources manually so um i wish it kind of had a rollback feature but that's just how it is but there are some nice things that uh azure does here which we'll talk about in a moment so i think we have adjusted it to the correct value now so hopefully this is going to be all we need to make it work so what we'll do is go to our deployments here and we can't do it here but so we'll go back to the top here and type in deploy and we'll go to custom template and what we'll do is build our own template in our editor and i'm just going to copy the contents here okay we'll go copy and i will go paste and we'll make sure that this is all good looks fine to me we'll go ahead and hit save and we will choose wharf so we don't have to make a new one and we will fill in uh the name as worf the username is warf i'll call warf2 just in case helps us keep track of what we're doing here testing one two three four five six with a capital on the t 14 l rls lrs and we'll go ahead oh we have one issue here cannot deploy a resource group worth deleting um we'll go back and we will hit create here i don't think i deleted the resource group let me just go double check i almost i'm almost certain i deleted all the contents of it right oh so there's already one here so we're just waiting for that to delete just gonna go delete for us please thank you it failed to delete we'll go take a look as to why resource is not found uh we'll go back to our resource groups give us a refresh here okay so you know what i must have deleted the resource group which is totally fine i i could have saw i only deleted the contents of it but we'll just call this worth regular then we'll go ahead and hit create here and so this time i have better feeling about this and so we will just have to wait a little bit it won't take too long i'll see you back in a moment okay so after waiting a little bit here our our thing seems to be deployed so if we go to resource groups we can see that our virtual machine is deployed so that's pretty much all there is to it one other thing i'd like to show you is that whatever you have whatever is in your resource group you can actually export the template so if you did configure something manually all you'd have to do is find the resource go up here to up it is export template and there's your template so it just has that single resource in there i can't remember if if i go into here if i select multiples um and i go x exper where's it export template look it's gonna include all that stuff so if you already have existing resources that you provisioned and you want to have them that's what you can do notice that some things won't be included in the template when you do that but you can just go ahead and download them and then you have them for later so yeah that's all there really is to arm other than learning the uh the nitty gritties of the actual language that's just how you work with it there so what i'm going to do is make my way over to my resource group here and i'm just going to go ahead and delete this here and we're all good to go hey this is andrew brown from exam pro and we are looking at azure resource manager templates also known as arm template so the thing we need to know is infrastructure as code this is the process of managing and provisioning computer data centers through machine readable definition files such as json files rather than physical hardware configuration or interactive configuration tools iec comes in two flavors we've got declarative which is what you define is what you get and imperative so you say generally what you want and it will guess or it'll fill in the blanks for you uh for arm templates uh they are json files as we're suggesting above there that want to provision azure services you want to configure they are declarative so you get exactly what you define an arm template is made of the following json structure you should know the structure pretty well because you might see questions where they're showing you that code so be sure you know all these parts schema describes the properties that are available within a template content version the version of the template you can provide any value for this element api profile use this value to avoid having to specify api versions for each resource in the template parameters values you can pass along to your template variables uh things you can transform parameters or resource properties using functions and expressions uh and i did include them in this the cheat sheet but there is a bunch of variations on variables and functions but i think that's practical skills you should just grab we don't want this in a cheat sheet functions used to find functions available within the template there are so many functions it wasn't even worth pulling them out and putting some here resources the azure resources you want to deploy or update and under resources you've got type of resource api version the name the location other properties which can be a bunch of different stuff so there's no consistency there and then outputs which are values that are returned after deployment so you can do things with them pragmatically so there you go that is the arm templates cheat sheet hey this is andrew brown from exam pro and we're looking at azure container instances also known as aci and this allows you to package deploy and manage cloud applications using containers or the way i like to think of it as fully managed docker as a service azure container instances allow you to launch containers without the need to worry about configuring or managing the underlying virtual machines and you're going to be able to design isolate containers for simple applications task automations and build jobs let's talk about some of the uh reasons why you'd want to use containers over vms so containers can be provisioned within seconds where vms will take several minutes containers are built per second where vms are built per hour so you'll save a lot more money containers have granular and custom sizing vpc vpcus memory and gpus where vm sizes are predetermined uh so those are the benefits between containers and vms aci can be utilized for both windows and linux containers you can persist storage with azure files using aci containers and honestly if you have containers or functions you have to have an external storage mounted to persist that's just the way you do it acis are accessed via fully qualified domain names which is one of the things i really appreciate about azure services because mostly services are like that azure provides quick start images to help you start uh launching example apps but you can also source containers from azure container registry docker hub and privately hosted container registries it looks like that the choice option there let's just talk about container groups because this is pretty much the only major component you have to worry about these are a collection of containers that get that get scheduled on the same host and the containers uh the containers in a container group share life cycle resources local network storage volumes so the idea is that you have these tightly coupled containers so all of them act as a service within that container group so here you can see an example of a couple containers that are mounting azure files on different directories there and underneath all of that is running on an azure virtual machine container groups are similar to kubernetes pods yeah and it says similar to but not really the same thing multicontainer groups can currently support only linux containers which is kind of a bummer but that's just what it is and there are two ways to deploy a multicontainer group you can use arm templates when you need to deploy additional azure service resources or just a yaml file when you want to deploy when your deployment only includes container instances let's take a look at container restart policies and what these do is allow you to change how the policies uh restart and there's three different ways we have always uh never and on failure so the first one here always means always restart the container and the idea is to keep your container running as long as possible and the reason why you'd want that is if you're running a web server some other providers would call that a service then we have never so run only one time this is great for background jobs and so other providers would just call this a task then you have on failures so containers that encounter an error that's when it should restart and so it's as simple as just choosing that option uh when you uh are creating that container let's take a look at container environment variables also known as nvars that allow you to pass configuration details to your containers and you can do this through the portal the cli powershell which that's always the case with azure and so it's as simple as just putting in your uh key and your value uh and one other thing i want to point out is that you can also pass in secured environment variables so the idea is that sometimes you don't want them plain text so if you have like so you have that stripe secret key you don't want anyone ever seeing that in production so the idea is that through and i don't think you can do this through the portal but you can do this through the cli or powershell is provide the secure environment variables over the regular one which isn't environment variables and that way you can pass it securely so it's never exposed to human eyes let's talk about persisting storage and we talked about that a little while there when we were looking at azure files but uh containers are stateless by default so when a container crashes or stops all i'll state is lost to persist a state you need to mount an external volume and there's quite a few different things we can mount as azure files secret volumes empty directory a cloud git repo so you got a few options there and to mount a file volume you need to do this via powershell or cli you're going to give the following detail so when you launch the container there's going to be nothing in the portal so you got to do it this way all right let's talk about some of the cli options that we can use to troubleshoot our containers a lot of this stuff is probably uh accessible of the portal but it's great to know how to do this via the cli so the first thing is getting the log so you can use azcontainer logs and that's going to bring you logs back then you can use az container attach and this is going to give you diagnostic information about when the container is starting up probably with a cloud in it information then you have a z container exact and this allows you to execute uh remote commands but what you can do is use bin sh which actually will allow you to have an interactive container it's like having the terminal right to the container which is very very very useful and the last is just going grabbing metrics from azure monitor so azmonitor metrics list you could use the portal for that but i just wanted to show you all those things because they might show up on the exam hey this is andrew brown from exam pro and we're going to take a look at azure container instances so here it is so all we got to do is go to container instances we'll hit add and the nice thing is that azure provides us with a hello world one so it's very easy for us to get started it's a linux machine and it looks like it's pretty inexpensive there so we'll stick with that i'm going to create a new group here we're going to call it banana and we'll name the container instance banana and east u.s 2 seems fine to me you'll notice we're on a quick start image if we wanted we could use something from the docker hub and provide our own link but we'll just stick with the quick start image for today we're going to go ahead and hit next to networking just to see what we have as options you can make it public or private we'll go to advanced hold on here yep those are just the ports you can expose we'll go to advanced and for the restart policy we can set on failure always or never we can pass in environment variables and i've covered this a lot more in detail in the lecture content so we don't need to really dive deep into this and we'll go ahead and create this instance and so we'll have to wait a little while here and i'll see you back in a moment okay and so after a short wait our container instance is ready we'll go to that resource there and take a look around so on the left hand side we can go to our containers and there we can see it running we can see the events down below of what's going on so you can see that it's pulled the image it successfully pulled it and it started the container some properties nothing interesting there the logs if we wanted to see stuff and if we wanted to connect to the instance we could also go here and hit connect which is kind of nice i don't have any purpose to do that right now so and it's also not going to work the way we're doing it but i just wanted to show you you had those opportunities uh you can do identity so that means manage it with role based access controls but what i want to see is actually this hello world working i'm assuming there must be a hello page i've never looked at it before so we're going to go here grab the public ip address and paste it on in the top and there we go so we have deployed a instance onto azure container instances or a container i should say so nothing super exciting to talk about here but we do need to know the basics uh there if we wanted to deploy other containers it's just the one there so that's all you really need to do but yeah so yeah hopefully that gives you an idea there i'll just go back to the list here so we can see it and we'll go ahead and just uh delete that probably do it for the view the resources on the lefthand side like i always like to do and we will go into banana here and we will delete banana and there you go hey this is andrew brown from exam pro and we're looking at azure container images cheat sheet and this is a two pager so let's jump into it so aci allows you to launch containers without the need to worry about configuring or managing the underlying virtual machine acis is designed for isolate containers for simple applications task automation build jobs containers can be provisioned within seconds where the vms can take several minutes containers are built per second where vms are built per hour so you get greater savings when you're using containers containers have granular and custom sizing vpc memory and gpu when vm sizes are predetermined aci can deploy both on windows and linux you can persist storage with azure files acis are accessed via fully qualified domains so it's just like a domain name then we have container groups these are a collection of containers that get scheduled on the same host machine the container is a container group share so you got lifecycle resources local network storage volumes container groups are similar to kubernetes pods multicontainer groups currently support only linux containers this is one of the cases where uh it's not windows it's linux okay there are two ways to deploy multicontainer groups you have uh that via an arm template and yaml file i imagine you can do them via blueprints as well a container restart policy specifies what a container should do when their process has been completed aci has three restart policies always never and on failure okay we'll go on to the second one here so azure containers are stateless by default when a container crashes or stops all state is lost to persist state you need to mount an external volume you can mount the following external volumes so either azure file secret volume empty directory or cloud git repo for a container tribute shooting these are just azure cli commands you should really really know so i'm just going to list them out here you've got ac container logs ac container attached az container exact which is for execute at az monitor metrics list so there you go that is the aci images cheat sheet hey this is andrew brown from exam pro and we're looking at azure container registry also known as acr which create and maintain azure canadian registries to store and manage your private docker container images and related artifacts so azure container registry is a managed private docker registry service based on the open source docker registry 2.0 and the idea here is that you can use container registries with your existing container development deployment pipelines and it also has this thing called registry tasks to build container images in azure and the idea here is that you can pull images uh from your container registry to various deployment targets like kubernetes dc os which i forget what it stands for but it's basically an operating system for containers docker swarm uh and there's also many azure services that directly support acr so aks azure app service azure batch azure service fabric and a lot more so it's just a matter of saying hey use this from acr and it works right away uh developers can also push to container registry as part of a container development workflow with delivery tools such as azure pipelines and jenkins and it's so easy to push to container registry there are many ways to work with acr via the cli the power azure powershell azure portal azure sdk and there's also this really good extension called docker extension for visual studio code something you absolutely want to install and maybe i'll make a full slide on it because i like it so much let's take a look here at acr tasks which allow you to automate os and framework patching for your docker containers and uh we have a few different options here so the first is quick tasks these allow you to push a single container image to a container registry on demand and you don't need to be running a local docker engine installation to do it so that's a really good benefit uh you can also trigger some automated build actions so maybe you push some source code updates you update the container base image you want to do it on a a timer that's on a schedule you can also have multistep tasks so you have to go for the various steps to complete that task for each acr task it has an associated source code context this is the location of the set source files used to build a container image or other artifact tasks can take advantage of run variables so reuse task definitions and standardized tags for images and artifacts so there you go hey this is andrew brown from exam pro and this along we are going to be working with acr or also known as container registries so type in acr at the top actually that doesn't work type in container and we will go to the registries and we'll create ourselves a new registry now list to have multiple container repositories or images hosted here on azure so i'm going to just type in my acr and we'll say ok and the register name is going to be my acr i'm just going to put a bunch of numbers here so maybe 8080 it probably doesn't like the hyphens there we go all the names are crazy you just have to flip around to figure it out we're going to choose basic because we can do everything we want on basic we'll go ahead and hit review and create and we'll give it a moment to allow it to start deploying and we'll just wait till this is done deploying our registry all right so that took less than five seconds and we'll go ahead and go to that resource and so what we need to do is go to access keys we're going to enable admin users so that we can actually log in via docker to our image so now what we need to do is get some repositories going and if we go over to repositories tab i don't think there's much we can do here i don't know if you can create repositories directly here i always just push yeah so you don't you don't have a button to say create a repository like github you actually have to push to it so i'm going to go to github and i'm going to create myself a new repository i'm going to call under the other example we'll say my acr it's already there so i'll say new and we'll go down below actually doesn't matter we'll just go ahead and hit create and from there i'm going to open this up in git pod all you have to do to open something up in gitpod is to attach this end here you can also do this in your local environment but you do have to have docker set up and so that's why it's easier to use git pod now cloud shell in azure actually has docker installed but doesn't run a docker instance so it's not that easy to work with so it really is easier to just work with git pod here or locally if you have docker installed so we're going to say docker login and then we're going to provide the login server we'll hit enter it's going to ask for our username so that is our username there so i'm going to paste it in it's going to ask for our password so i'm going to copy that paste that on in there hit enter and so now we can push to that repository i'm going to pull a image called hello world that's just a standard application to say hello world if we do like docker run i t d um hello world i think that's what it is to type it whoops i can actually spell it right then we could probably run it over here there's like a docker tab so we can see if it's running notice that it exited if we view the logs it probably just says hello world yeah hello from docker so there you go that's a simple way of running a docker container and terminal will be reused for other tasks press any key to close it so i just hit space there i t d or t i think is for the tag name so it's called hello world so we type docker images it shows up there in i's for i don't know interactive d is for daemon so it runs in the background but anyway we want to push this to azure so the way we're going to do that is we're going to type in docker tag because we have to tag this image and we're going to choose the image id here and then we need to provide the repository name so i'm going to assume it's the login server and then we just say example hello world hit enter and we can say docker push and then copy this here and that should push it over to azure we'll go back over here give it a refresh so repository is there so that's all it takes to push so this is one part but we'll figure the other part next here soon okay hey this is andrew brown from exam pro and what we're going to do is use the last um follow along in order to reference an image in our container here because we pushed an image but what if we want to reference one as an application in an actual docker file because that is a very common use case so in our gitpod environment which we still have i'm going to type in dockerfile and in here what we can do is reference our image i always forget what the top line is export run import something so i'm going to just look for example i'm going to go to um docker ruby because i know how to use this image pretty pretty uh pretty good so we'll just go with it and it's from i always forget that so we'll type in from and the idea is that we want to reference something like this but i know if we use the hello world one it's a bit finicky to build so i don't want to use this one but i'm going to say ruby or um yeah ruby's fine here and so we're going to pull the the ruby image so we say docker pull ruby and we'll have to push this one so we'll pull it give it a moment and now we'll type in docker um images and we need to grab the ruby image id and we'll say docker tag this one and we gotta go up and grab this part here paste that in i could just say ruby here and then we can do docker push ruby and while that's going i'm just going to go here and type in run echo hello ruby because the idea is we just want to build a docker image that's from a remote location so what i'm going to do is just save this so we'll say uh save my docker file it's like the only file we saved here just waiting for that to finish pushing and then what we're going to do is we're going to uh you know close this environment and then launch it again because i don't want to have any of those images there i don't have to figure out how to delete them it's like docker rmi it can be really finicky about deleting images but the idea is i just want to make sure they're not there so that when we pull we know that it actually is pulling from there not locally we will have to do a docker login here in a moment so type in we'll type in docker login and we'll go back over here and we will log in say allow we'll go back to here we'll grab the username we'll go back over to here we'll grab the password so that's good and um hey where's our dockerfile our dockerfile seems to be gone i guess we forgot to sync it and that's totally fine we'll just type it in again it's not a big deal from here we'll say ruby and i'll say run echo hello ruby and so now what i want to do is i want to do docker build period and so if we are logged in and we can pull it so it's in great shape so it's the fact that we are pulling that image and that was the other part i wanted to show you there so it doesn't matter if this finishes because it obviously is working but we'll give it a second here to uh finish downloading there we go um and so that's all so we'll go ahead and close this and we'll make our way back over to azure and we will go and type in resource groups and we will choose my acr and we'll go delete resource group and we'll delete that we're done so there you go hey this is andrew brown from exam pro and we're taking a look here at azure container registries cheat sheet so acr is a managed private docker registry service based on the open source docker registry 2.0 it uses azure container registries with your existing container development or sorry yeah development and deployment pipelines use azure or use acr tasks to build containers images in azure pull images from an azure container registry to various targets uh and targets could be kubernetes uh dc os which is i can't remember the name of the full name of that operating system docker swarm uh for acr tasks this allows you to automate os and framework patching for your docker containers you can trigger them you can trigger automated builds by source code updates updates to container based images triggers on a schedule you can create multistep tasks each acr task has an associated code context and tasks can take advantage of run variables that's the end of it there you go see you in the next section hey this is andrew brown from exam pro and we're looking at azure app services which allows you to quickly deploy and manage web apps on azure without worrying about the underlying infrastructure and specifically this is known as a platform as a service so azure app service is an http based service for hosting web applications rest apis and mobile backends and you can choose your programming language and it can either be windows or linux environment and it's a platform as a service so if you've ever used heroku is basically the heroku of azure and so azure app service takes care of a lot of the underlying infrastructure for you so it can do security patches of the os and languages load balancing auto scaling automated manager and then um there's a lot of things that you can implement and so for integration such as azure devops github integrations docker hub package management easy to set up staging environments custom domains attaching uh ssl certificates so you can see that it just basically takes care of everything for you if it's really hard for you or time consuming for you to do all this on your own in azure and the way it works is you pay based on an azure app service plan honestly i find these really confusing um i really like how aws does elastic bean stock because you're just paying for the underlying services but azure has all these crazy uh tiers here but you have shared tier which is free and shared doesn't support linux you have the dedicated tier which is basic standard premium premium version two and three and the isolated tier and so uh another thing i need to note is that azure app services is not just for traditional vms for monoliths you can also run docker single or multi containers and when you set up a project you're going to choose your domain name on the azure websites.net obviously you can override that with your custom domain name but there you go let's talk about azure app services runtimes and so let's define what runtime is a runtime is software instructions that are executed while your program is running and so runtime generally means what programming language libraries and frameworks you're going to be using and so runtimes for azure app services are predefined containers that has your programming languages and probably some commonly used libraries for those languages installed and probably commonly used languages for its web frameworks and so the idea is that you're going to choose your runtime it could be net net core java ruby which i'm very disappointed in azure because uh as of shooting this video they do not support uh ruby for application insights but yet they have it here in azure app services node.js php python and there's all the logos if you like seeing the logos and so azure app services generally has multiple versions so they have like ruby 2.6 2.7 for php they have a lot of versions for node.js they have a lot of versions but i just want to point out uh that it's pretty common for cloud providers to stop or to retire the old ones at some point to stop supporting them uh you know that's just because they want to keep things modern and the other thing is like it also helps you keep with your best security practices because really you should always be trying to upgrade to the latest version uh for those security patches and and things like that so there you go now let's say uh you wanted to use a language that wasn't supported on um azure app services like you wanted to use uh elixir what you could do is create your own custom container either for windows or linux uh and so uh you just go ahead and create your own docker container in your local environment you can push it to an azure container registry and then the idea is you can deploy your container image to your app service so i just wanted you to know that you could do that um if there's some languages or other things or maybe you're using a language but you need uh some bundle or packages that are uh they're just baked into the uh into the container so there you go let's talk about deployment slots so deployment slots allow you to create different environments for your web application and associate a different host name this is useful when you need a staging or a quality assurance environment or maybe you need to like a developer environment any kind of environment you want so think of it as a way to quickly clone your production environment for other uses and so down below here you'd have your deployment slots and there's your slots to maybe you have apps staging beta so that's the different host names there and so the idea is that not only do you have other environments but there's also this thing called swapping and the idea is like imagine uh you decide like you make a clone of your production environment and then you deploy the latest version to it and then when you decide that it's good it's in good shape then what you can do is swap it out with your current production environment and then just retire your old one this is called blue green deployment and so that is a great way to do deployments and i just wanted you to be aware of that so i wanted to talk about app service environment because i just wanted to show you how azure app service is not just for a little toy apps or your small startup but they can be really useful for larger enterprises so app service environment is an azure app service feature that provides fully isolated and dedicated environment for securely running app service uh at high scale and this is going to allow you to host web or windows web apps linux web apps docker containers mobile apps and functions and app service environments are appropriate for application workloads that require very high scale isolation and secure network access high memory utilization and again you know when you think of platform as a service you don't think at this kind of scale so it's really impressive that azure does this customers can create multiple ases within a single azure region or across multiple azure regions making ases ideal for horizontal scaling stateless application tiers in support of high requests per second rp rps workloads and ase comes with its own pricing tier and that's the isolated tier and ac can be used to configure security architecture apps running ase can have their access gated by upstream devices such as a web application firewall also known as a waff app or ssases can be deployed into availability zones using zone pinning i don't know what that is a zone pinning but that sounds good to me and there are two deployment types for ase we have external ase and ilb ase let's go take a quick look at what those look like so here's a big diagram and look in the middle there that's where our azure service environment is it's in our own vnet and a subnet and the idea is that you if the reason it's called an external ase is because it exposes the esc hosted app on an internet accessible ip address and then down below if you wanted to connect this this is generally what people are probably using for but what you want to do is you can connect it to your onpremise via a site to site or express route vpn so that's something you can do with it and the other part is like you can because the esc is within the same vnet it also has access to resources within the vnet without any additional configuration so that's really nice and then the second uh a second one here it looks it's exactly identical but there's one key difference and it has this ilb there and an ilb stands for internal load balancer uh and that is basically the only difference so uh there you go all right let's take a look here at uh deployment for azure app services and let's just first define deployment so that's the action of pushing changes or updates from a local environment or repository into a remote environment and azure app services has a lot of options for us and that's to me is the most powerful thing about azure app services because it's easy to deploy a web app to a virtual machine backed by a database but figuring out deployment is a very time consuming and tricky thing and azure app services gives us so many options unbelievable amount of options it is so so great of them to do that for us so we can run from a package deploy zip or or rar deploy via ftp deploy via cloud sync deploy continuously from like github or use azure pipelines deploy using custom containers using docker hub azure container registry deploy from a local git repo deploy using github actions deploy using github action containers or deploy with an arm template so we're not going to look at all of these how they work but let's look at a few of them so we just have an idea how robust of these options are the first one we're going to look at is running from a package and this is when files in the package are not copied to the www uh root directory instead the zip package itself gets mounted directly as a read only www route directory so basically all other deployment methods and app service have to deploy to the following directory if it's if you're on a windows machine it's going to be home site wwe root if you're on linux it's going to be home site www root and so since the same directory is used by your app at runtime it's possible for deployment to fail because of file lock conflicts and for that and for that reason uh the app might behave unpredictably because some of the files are not yet updated so this is the reason why you might want to create a package um it's just because it it circumvents the issue of just replacing files in the folder another method is the zip and rar deployment this uses the kudu servers service that powers continuous integration based deployments and qd is used for a lot of things but it's an engine behind git deployments and azure app services and it's open source project that can also run outside of azure and so cuda supports the following functionality for zip file deployments and it supports a lot more than just zips but it does deletion of files left over from a previous deployment option to turn on the default build process which includes package restore deployment customization including run deployment scripts deployment logs a file size of uh was it two gigs two gigs and uh you can use it with the cli the api uh vrs with curl or the azure portal uh so by the way that's that's you uploading a zip uh but you're essentially using kudu underneath okay uh and so let's go take a look at another deployment method which is file transfer protocol uh ftp has been around for forever and this is pretty much how people thought you were supposed to deploy your apps in the late 90s and early 2000s i don't think it's a really great way to deploy but the point is is that if you want to do it this way or you have a use case that makes sense you can do it so the idea is that the deployment center you would uh say i want to use ftp and you just you get an ftp endpoint your username and password and that's your credentials you use your ftp client to connect uh very old school but it's an option for you which is really nice another way which to me is a bit bizarre but it's cool that you can do it is you can use dropbox or onedrive to deploy using cloud sync so the idea is that you have dropbox it's a third party cloud storage service onedrive's the same thing it's just microsoft's uh thing and so you go to deployment center configure dropbox or onedrive and when you turn it on it will just sync or it'll just create a folder in your dropbox drive and that will get synced so that's the one for onedrive that's the one for dropbox and this is going to sync to that www route there i would have loved to take in some screenshots but i couldn't find how to turn the service on but i know it exists i just was i thought it was so bizarre but yeah there you go that is the deployment methods there so the way you pay when you use azure app service is you need an azure app service plan and that's going to determine you know how much you pay and what's going to be available to you they got three tiers which we're going to go uh through here shortly and we did mention them earlier i honestly do not like this whatsoever this tells you this is a microsoft product because it has these uh wonky pricing tiers i hope in the future they'll change it but that's just what it is and so let's go learn it so basically what you do is you have like this big wizard uh that tells you all the stuff that you can have it tells you what's included and stuff like that but let's work our way through it so the first thing is the shared tier and there's two types here we got free and shared and so there's the free tier that red one there it's called f1 it gives you one gigabyte of disk space up to 10 apps on a single shared instance no sla for availability and each app app has a compute quota of 60 minutes per day so there you go you get some free tier there then there's the shared tier this provides up to 100 apps on a single shared instance no sla availability each app has a compute quota of 240 minutes per day uh and the thing is is that i didn't know where the button is for that so like the next thing right beside it is the dedicated tier and i thought that's what it would be because it says 100 total acus so is it i'm not sure but anyway the point is there is a shared tier where it is i don't know uh and you can't use the shared tier on uh on the on linux based system so you're using windows i'm using windows you gotta use bigger instances anyway which i don't know i don't like that too much but that's what it is uh moving on uh over we're now into the dedicated tiers and look it's right beside the free tier that's the green one it says b1 and if you expand it actually has a couple other tiers there so i just wanted to show you that there was uh three uh there and so uh for dedicated tiers we've got basic standard premium premium version two premium version three and we're looking at basically that's what that is more disk space unlimited apps three levels in this tier that offer very amount of compute power memory disk usage it must be b1 b2 b3 uh and then the next thing over is uh the standard and i we had we had to switch tabs there onto the production tab notice the terminologies don't really match the tiers uh and so uh with standard you can scale out to three dedicated instances has an sla of 99.95 availability and three levels in this tier that are offering varying amounts of compute power memory and storage disk and so then uh that's that tier there and we're on to our last here which is the premium tier and this scales to uh 10 dedicated instances it has availability sla of 99.95 percent and multiple levels of hardware so that's the dedicated tiers then we're on to the last thing which is isolated and this is really only going to be used i think for ase so the uh the isolated tier has dedicated azure virtual networks uh full network compute isolation scales out to 100 instances availability sla of 99.95 and again i think it's just for those ases um but there you go that is all the tiers and hopefully it makes sense to you but it is a little bit tricky to figure out what to choose but you don't really get to like pick at a granular level that's what i don't like but it is a really great service azure app service it does figure everything out for you so maybe it's okay for you so let's say you're using azure app services to host um one of your applications you need to run a random script how are you going to do that that's where web jobs comes into play so it comes at no additional cost and web jobs is not yet supported for linux which is of no surprise here because microsoft is all about windows but i hope one day they will support it the following types they support as of today are command files bat files executables we've got powershell files bash files php files python files javascript files and java files when you go ahead and create your job you're going to have between two types you've got continuous and triggered a continuous means it's going to run continually until stopped which is pretty clear and this particular mode supports debugging so if you need to bug in that's the mode you're going to be using for trigger this only runs when a trigger occurs and you have different kinds of triggered here so you notice we have scheduled and we can enter a quran job in and this will also expose a web hook that can be called to enable scenarios like scheduling i think that would be like manual triggers it doesn't support debugging though so looking at web job scales and this is for continuous only uh we have a multiinstance and single instance so multi is will scale your web jobs across all instances for your app service plan and single instance will only keep a single copy of your web job running regardless of the azure app plan instance count so there you go hey this is andrew brown from exam pro and we are going to be learning about azure app services in this follow along uh and it's a service that's supposed to make it easy for you to deploy web applications i say supposed to because it really depends on your stack azure has more synergies with other stacks than others so like if you're like me and you like ruby on rails you're going to find a lot of friction with rails and linux but if you're using something like windows servers or python or net you're going to have a much easier time still really great service just wish they'd make it a bit more broad there but let's hop into it so before we can go use that service let's make sure that it's activated and so we'll go over here and we'll go to azure subscription and then down below we're going to go to resource provider now you'd think what you could do is just type in app services and you'd be wrong because the the service is under a particular provider so if you want to figure out what provider it is we can go um azure resource providers and they have a page on documentation here that lists them all so if i search for azure app services it's under web and domain registration so we're going to make sure this is registered if we're using a custom domain which we are not today we need this one activated so going back here i will type in web and you can see it's registered so if yours is not registered go ahead and hit that i believe this by default is generally registered with new azure accounts so i don't think that is an issue for you but we'll go back up here close these additional tabs and we will type in azure app services and we will look for that service so there it is and we'll go ahead and hit add and so i'm going to give it a new name i just made it a moment ago but i'm going to try again and try to use the same name so we're going to call this voyager great and then i'm going to go ahead and name this voyager and i already know that that is taken so i'm going to type in delta flyer and these are fully qualified domains so they are unique with azure app services you can run a docker container we're doing code this time around and what i like to use is ruby but again you know if i want to use the ci cd i'm not going to be able to use the deployment center with ruby so that is not possible and so we're going to go with python and run either a flask or a django app i haven't decided yet i am in canada so let's go to canada east and down below here we have the plans generally the plans will tell you the cost underneath look you'll notice that it's loading but i just want to show you that there are some discrepancies in terms of pricing so if i was to go to azure app services pricing and we were to pull this up here we can kind of see the pricing here okay and if we scroll on down right now we're looking at a premium v2 uh and oh no i don't need help i'm okay you'll notice that it's 20 cents per hour so if i go here and do that times 730 because there's 730 hours in the year that's 146 dollars i believe this is showing me in usd dollars yeah and and here it's showing me 103 canadian which is lower um so it could be that because i'm running in a canada east region it's the price is different but you can imagine that if i had this at this cost at uh what did we say here at 146 usd to cad i'd actually be paying 182 dollars so you got to watch out for that kind of stuff but i'm pretty sure this is what the cost is so just be aware that if you look stuff up in here it's not necessarily reflective so you got to do a little bit more work to figure that out if we wanted to go here we cannot choose the free tier when we're using linux if we're using windows i believe we can use it we're working with linux today so that's just how it's going to be for the b1 this is totally fine but we want to utilize deployment slots deployment slots is an advanced feature of the production version and that's the only way we're going to be able to use it here this is 20 cents per hour again so i don't want to be doing this for too long but i think what we'll do is before we do that we can just do an upgrade to dev to prod so we can experience that i'm going to go and just choose b1 okay so go next we do not need any application insights for the time being and it will not let us so it's okay we'll go next review and create and we'll go ahead and create this resource here and i will see you back when this is done so um our resource is now set up we'll go to resource and now that we're in here you'll notice if we hit browse we're not going to see anything because we do not have anything deployed which makes sense right so we're going to actually have to go ahead and deploy something so we're going to make our way over to the deployment center and it's just going to tell us that we have yet to configure anything and that's totally fine we're going to go to settings it'll give it a moment and so the thing is is that we're going to need something to deploy i did not create an app but the great thing is in the azure documentation they have a bunch of quick starts here all right and apparently they have one for ruby as well but today we are looking at python uh and so they actually have an example repository for us here which is github.com azure samples python docs hello world and i mean i can go make a repo for you but we might as well just use the one that is already provided to us so i'm just going to pull this up to show you what's in it it's a very very simple application even if you don't know anything about building web apps i'm going to walk you through it really easily here okay so we're going to open up app.pi so we are using flask if you've never heard of flask it is a very minimal python framework for creating web apps uh very uninspiring uh home page here but it gets the job done it's going to create a default route for us which uh we have there we're going to call hello here and we're going to have hello world so that's all that's going on here very very simple and we have a requirements this is our package manager i don't know why python uses txt files it's very outdated to me but that's what they use and here we have flask all right so we're going to use that repo it's a public repo so it should be very easy for us to connect so we'll drop down go to github and uh the next thing we need to do is authorize github all right so i ran into a bit of trouble there because i could not uh authenticate my uh github account but you know what i just made another github account so that made it a lot easier i'm going to go ahead here hit github and we're going to try to authorize it and so now i'm logged into this new one called exampro dev and we'll go ahead and authorize this application and we're now in good shape this repository doesn't have anything in it so if i want to clone something i guess i'll probably have to fork that repo so we'll give it a moment to authorize and while that's going i think that's what i'm going to do i'm going to go and fork the example repo if i can find the link again here myself i believe it is that's still authorizing over there i'm still looking for it so it was like examples or something samples or examples all right so i found a way around the problem i just made a new github account so that's all i had to do um and i just won't be using my primary account until i get my phone back but so what we'll do is go hit connect i'll hit authorize and it didn't prompt me because it already connected to this new one called exam pro dev you might have to put your credentials in here and it's going to ask me to select some things it's a new account so there are no organizations there are no repositories there are no branches totally brand new so what i'm going to need to do is get a repo in there so we'll just go ahead and fork the azure samples one so that is azure samples python docs hello world if i type that right we're in good shape i'm going to go ahead and fork this repository i'll say got it and then i'll move this off screen here this is now cloned you should see it cloned here and we'll go back here and this probably isn't live so there's no refresh button here so we'll have to hit discard and we will give this another go here and we will select our organization which is our name there is the repository uh should be main branch is kind of outdated i'm sorry but it's called master that's what it is not my fault that's azure's fault okay um and i think that's it i don't know if we need a workflow configuration file i don't think so i'm just going to double check here no i don't think so and what we'll do is we'll just go ahead and save that and so now we are set up for deployment all right so now that that's all hooked up if we were to go to browse we're actually still seeing the default page a deployment hasn't been triggered just yet so the way it works is it's using github actions so if we click into our we call it main branch i know they got the wrong name but we're going to click into our github workflows and then below here we can see we have a yaml file and this is for github actions integration here and so what it's doing is it's specifying the branch what how it's going to build it's going to run on ubuntu latest the steps it's going to do is going to check it out it's going to set up the python version it's going to build it it's going to do that stuff and so in order for this to um take action we'd actually have to go ahead and make some kind of manual change which we have yet to do so eh so what we'll do is we'll go back to our main here and uh it should be as simple as uh just changing something here so it's not i'm not sure how it's supposed to know that it's supposed to be doing the hello oh i guess yeah sorry so this means it's going to route over to here so i'm just going to make any kind of change here doesn't matter what it is just one space we'll go ahead and give it a commit and if i go back to my latest commits we should see that i made that change there it is we'll go back over here and this should be deploying so if we go over to logs here you can see one's in progress right now okay and so that's what we're weighing we're just going to see that finish there we could probably open the logs and get some more information there and so it just brings you back over to github actions and so here's github actions and it's performing the stuff here so we're just going to give it time here and i'll see you back in a moment so we didn't have to wait too long it only took one minute and 29 seconds if we go back over here um we might need to do a refresh and so we can see this is reflected over here and so if we go back to it doesn't really matter if we go to settings or logs here but i'm going to hit browse and see if my page is deployed it still is not so we do have a small little problem here and it's really going to just have to do with how the app is served so that's what we need to figure out next all right so our app is not currently working and there's a few approaches we can take and the thing i can think right away is we should go an ssh into that instance if you scroll on down here from developer tools you can go to ssh and click this button and that's going to ssh you right into that machine right away you can also access ssh via the um cli command so i believe it's like it's like a z web app um ssh it'll do the exact same thing you do that from the cloud shell but that's not what we're doing today if i give this an ls in here and we're in linux we can see we have our app here and what i would do is i would see what's running so i i would do a puma uh or sorry not puma ps ox grep uh python and you can notice that we have uh g unicorn that's running so that is where our python instances are running so you're not looking for flash you're looking for python here and if we wanted to make sure that was working we just type in curl local host um and so that is going to return a port 80. so that tells me that because like curl just means like let's go look at that page it should return some html like print out the html to us so that means the app is not running so what you could do is run flask run and it's going to start on port 5000 right so what i can do is i can go up uh back to my deployment center here and i'm going to go get that link here and just ignore the fact that it's working it's it's not working right now i know for certain it's not but if we do 5000 that won't resolve because port 5000 isn't open so we can't really just put 5000 in there and the default server here would be 5000 so if i stop this and i specify port 80 right then this will start up the app on port 80 and so now when you go here okay it will work this is not a great way because of course as soon as you kill it here uh technically the site should stop running um and so you'll run into that step uh so what we need to do is provide a configuration to g unicorn which is a python thing again it's not so important that you know how like what these things are but the idea is that you understand as administrator you want to make sure you have an app that runs after you do a deploy and so in this particular one we need a startup.txt and interestingly enough there is a example code by the same author of the other one we were looking at here i believe it's the same person or it might not be but they have a startup txt right and so in here you can see that it binds on port zero zero zero it starts up four workers starts up the app all right and so that's something that we can go ahead and do so what i will do is i will go back to my github repository that we have here and i can just go ahead and add a new file so i'm going to say add a file create a new file here we'll call it startup.txt i'm going to copy this command here and paste it in there so g unicorn will bind the workers and start up on the app um startup app is being ran by something here so if i go back here i think they have a startup pi here and that's all that it is doing um i think i want to i could do it this way i suppose let me just see here there's just a slightly different egg so they actually have like a full app going on here and i just want a very simple flask app so i think what i can do is put flask run here port 80 and that should start up the app there i'm going to go ahead and commit that file okay and as soon as i commit that if i go back to my actions it created that startup file there so it should trigger a build it's queued up um and i'll just put this tab up here so we'll be back here in two seconds and if i give this a nice refresh yeah you can see it deploys in progress so this doesn't take too long we'll just wait close that there we'll just wait a few minutes we click logs it just opens it back up here and we'll see how that goes all right so your deploy may have finished there but the thing is is that we're not going to really know if a change has taken effect unless we actually go ahead and update our code so what i want you to do is go to your code tab go to your app.pi we'll hit edit and i'm going to go ahead and change this to vulkan and then we'll scroll on down hit commit changes and we'll make our way back over to our deployment center and we'll give it a refresh here and we're just going to wait until this one is complete and we will double check to make sure that that has changed if it's not we will take action to fix that okay all right so we just waited a little while there for that deploy to happen and if we go to our website here it is taking effect so that's all we had to do to get it working so that's pretty good um so that is deployment so let's talk about deployment slots in order to utilize this feature we're going to actually have to upgrade our account because we cannot utilize them at this uh the basic plan here we got to go to standard or premium so let's go ahead and give that an upgrade so here's the b1 we're gonna go to production here um and i think yeah we're gonna have to choose this one here uh very expensive so the thing is we're gonna just upgrade it temporarily unless there's more options down below that are cheaper yeah these are the standard tiers let's go with this one here because it's only eighty dollars again we're not going to be doing this for long but i want to show you how to do staging slots and auto scaling okay so we'll go ahead and apply that there and now it says that it's applied so if i go back to our app here and we click on deployment slots sometimes it doesn't show up right away if it doesn't that's not a big deal you just wait a bit but today it's super fast so we're going to go ahead and add a new slot we're going to call it staging we're going to deploy from our production branch here and i'm going to go ahead and create that there and we'll just wait until that's done okay great so we waited a little bit there and our slot is created so i'm going to just hit close there and so now let's go take a look and see if we can actually see the application here so i just clicked into it i click browse and we're getting the default page so nothing is actually really deployed to it so how are we going to do that that's the the main question here so what i'm going to do is i'm going to make my way over to the deployment center and you can see that it's not configured for this slot so we're going to have to set it up all over again even though it copied over configuration settings it didn't copy over the code so we go to github we'll choose our organization again i'm going to choose the repository we're going to choose that main branch again there we're going to let it add a workflow and notice that this time it's going to call it staging.yaml so there'll be a separate workflow that gets created we're going to go ahead and save that there and what we can do is again click onto our branch name there and if we click into our workflows we'll note now notice that we have a staging example it's the same thing but it should be able to now deploy so the whole purpose of these deployment branches is that it helps us we can deploy different versions of our apps but also it's just a place where we can view things before we actually roll them out so we want to make sure 100 that they are working correctly i don't think this will automatically push out let me just go to my actions to see if this is deploying notice that we have two workflows now we have staging here uh and yeah it looks like it's going to deploy here so we'll just wait a little bit but maybe what we can do is try to have a slightly different version uh for each one here okay but we'll just let that finish and i'll see you back in a moment all right so our deploy finished there so now if we go back to our website here we go browse we should see that application it says hello vulcan and if we go and take out this we still have hello vulcan so how can we have a a variant of this so that we can push out to that so what i'm going to do is i'm going to go back to my application here i'm going to go to code and i'm just going to make a minor change um i don't say also does that spell right startup doesn't look correct to me um so maybe i'll go and adjust that file but it doesn't seem to be affecting anything which is i'm a bit surprised there so what i'll do is i'm going to go and edit that file and give it the proper name can i rename this file yes i can so we'll call that startup file i thought we needed that for deploying i guess it just works without it which is nice if we go back here i'm going to go and actually just want to edit my app here again and i'm going to go and edit this and we'll say hello pandoria or hello and dorians maybe and so if i go back to my actions the question what is it deploying is it going to deploy the production or the staging and it looks like it's going to do both looks like it's doing both here but one way we could tell is we can go to our logs here and we can see that um so we did two deploy so there's one change here if we go back to our main application our deployment center here we go over to our logs you can see that they're both deploying so it doesn't seem like it's a great thing that that's how it works so the question is then how would we um facilitate that deploy right how could we do that i suppose what we could do is just make a separate staging branch so if i go over to code here i don't think we can just make branches through here so what i'm going to have to do is go ahead and oh i can create a branch right here so we'll just type in staging and we'll go create ourselves a new branch and now we're in this branch and what i'm going to do is go ahead and modify this and we're just going to call this hello klingons okay we'll go ahead and update that and so this should be a separate branch so you think what we could do is go in and just change our settings so that it deploys from that one we'll go back to our deployment slots we'll click into staging here and we need to change our configuration settings um i think we could just do it from here hold on here i could have swore it specified the branch if we go to deployment center here i think it's set up on that other branch there i think we just adjusted here so yeah i think we could just adjust these settings we can't discard them but maybe what we can do is just go in and modify that file so we will go into our code here and we will go ahead and click into here go into staging and we'll just change what the branch is called so we'll just say staging and we'll hit start commit and we will save that and we'll see if it actually reflects those changes there so we will go here and hit refresh we'll see if it picks up staging now if we go to settings it's not picking it up so um i'm not sure i don't think perform a redeploy operation we don't want to redeploy so maybe what we'll do is just we'll have to do a disconnect here because it's collect it has the wrong one here so save workflow file um okay we'll just go ahead and delete it it's not a big deal we'll just have to make a new one here we'll go to github we'll choose our organization again or repository our staging branch this time around we'll let it add one see it says we could use an available workflow so we could have kept it there and added it there um and we'll go ahead and save that so now we'll have two separate branches there and we'll give that some time to deploy because that will now trigger a deploy off the bat and so i'll see you back here in a moment all right so after a short little wait here it looks like our app is done deploying so we'll go over here we'll make sure that this is our staging server is good and we want to see that our production is different perfect so we now have a way to deploy to each one but imagine that we want to swap our traffic so we're happy with our staging server and we want to roll that out to production and that's where we can do some swapping so what we'll do is click the swap button and we're going to say the source is the staging and this is our target production we're going to perform that swap right now we can't do a preview because we don't have a particular setting set that's okay and it's kind of showing if there are any changes so set of configuration changes we don't have any so that's totally fine as well we'll go ahead and hit swap and that's going to swap those two i believe it has has zero downtime so we'll be in good shape if that happens there and we'll just give it a moment to do that great so after a short little wait there the swap is complete and so if you remember clearly this was our production right and so if i was to hit refresh it so now say klingons and if i go to my staging server it should be the other way around right good so now imagine that i want to just split the traffic that's something else that we can do um so notice over here we have these percentages here i'm not sure why it won't let me change those so maybe i'll have to look into that so i'll be back so i'm not sure why it's not showing us that traffic slot there but what i'm going to do is just maybe try to trigger a deploy back into our staging and maybe that's what it wants to see so what i'm going to do is go back to my code here we'll be in our staging branch here i'm going to go ahead and edit this file here and we'll just change this to majoran's and we will hit update and we will let that go ahead and deploy so if we go to actions here we can see that it is deploying um and we'll just give it some time okay so we'll see you back here in a bit i mean the other reason could be that we're just not at the main level hold on here if we go back here to deployment slots you know what i think it's just because i was clicked into here and then i was clicked into deployment slots that they're both grayed out yeah it is so we can actually do at that top level there it doesn't hurt to do another deploy though so um we'll just wait for always for that deploy to finish and then we'll come here and adjust that there okay all right so let's take a look at doing some traffic switching here so right now if we were to go to our production we have klingons and if we were to go to our staging we have bajoran so imagine that we only want 50 of that traffic to show up so what we can do is put in 50 percent and uh what i'm gonna do is um do i hit enter here or oh sorry save up here there we go um and so what's going to happen is this should take effect i think right away yep uh and so now we have 50 50 50 chance of getting something else here um so i'm just going to keep on hitting enter here if that doesn't work we can try an incognito tab and there we go we got the opposite there and so this is serving up staging right and this is serving up production but they're both on the production url so that's the way you can split the traffic so that's pretty much all i wanted to show you for deployment slots let's now talk about scaling all right so let's take a look into how we can do some scaling with our app service this is only available if you have beyond standard and or standard and beyond so standard and premium and etc so if we just search for scale we have two options here we have scale up and scale outs the scale up is pretty straightforward that just means to uh make our instance larger and so we already did that when we upgraded from our standard our our b1 over to our s1 here right so if i was to go here i'm not going to do that but if i was to do that and upgrade um that would be scaling up right and notice that we're talking about scaling so right now we're limited to 10 instances which is totally fine but now let's take a look at scaling out so if we go to scale out here and go to custom auto scale what we can do is we can scale based on a metric so we can add or remove servers based on the demand of the current web applications traffic so we're only paying for servers when we need them and so we have a minimum one a maximum two that seems fine to me but we're going to add a rule here and i want to scale this on the maximum time we're going to do on cpu percentage i just want to have a very easy way to trigger this so we can see a scaling event in action here it has a maximum of 16 i might just lower that down even further well let me type in there new so 16 is what it's going to have to be it's not a big deal but i am going to reduce it down to actually i think sorry i don't know i was going here the the metric threshold to scale an action i'm going to put it at 10 sorry okay so here's that line and so we have a and i like how you can drag it but you kind of have an idea that we have a high chance of having this um trigger i just want to do this so that we have a a good chance so if i was to put it here you can notice that it's very easy for us to spike our traffic and and cause a scaling event now i'm gonna set the duration to one minute so we have a much higher chance of uh triggering that there okay set a duration less than five minutes may generate a transient spikes yeah that's fair but i mean i just want to show you a trigger happen and we need a cool down time probably um and it's set to five minutes that's totally fine we're gonna add one and that looks fine to me i'm gonna set this to maximum okay and so now we're very likely to trigger that there we'll go ahead and hit add that instance there and so now that we have that we're going to go ahead and save that and so now that that's there what we want to do is actually trigger a scaling event and so where we're going to see that is under the monitoring tab so if we go to monitoring and what we're going to do is go over to um it should be in sorry i forgot the place where we need to go take a look here is actually in the run history here so if we go here and check one hour we can see how many instances are running uh and i think if i dial it back here it should show me over time as it changes we do have a scale up event that has happened which happened i guess four minutes ago um so it gives you kind of an idea of how many instances are running which right now are two um so maybe our maybe our scaling event is not in the best uh use case there because it's happening too frequently so what i'm gonna do is go ahead and modify that scaling rule um and so i'm just gonna go back and click here and maybe we'll just make it so it is less aggressive so what i'm gonna do is just change it so it's over the duration of five minutes and i'm gonna just put it right above here so that it goes back to one okay and we'll go ahead and save that and so now we go back to our run history here it still shows that it has two as you can see here but i want to see this drop back down to one so it's going to check every five minutes or or within the span of five minutes so what i'm gonna do is just uh wait here and i'll see you back in a bit until we see a scaling action happens uh here okay yeah i'm just sitting here waiting for it to scale down and i don't see it going so it makes me think that i need to go ahead and set a scale scale down action let's take a look at the one that we currently have uh so this one is set oh you can see it's still spiked we don't even have anything going on here but what i'm going to do is just be really aggressive here i'm going to say when it's 50 okay and so here we'll go back here and i'll save that and i just want to see if it scales down i shouldn't have to set a scale down action should just go um and what i'm actually going to do is be a little bit more aggressive i know i'm setting a lot of stuff here but i'm going to just set it to duration of 1 minutes so we can see this a lot sooner and we will go back to our run history here and we'll see if we observe a scale down all right so um it's not scaling down here but uh i think it's probably because i need to scale out action so what we'll do is go ahead and add a new rule this thing if we go here and we just look at it um it's not going to decrease it unless we have a scale of action so i don't think it's necessary for us to set one here i think you get the idea but that's for scaling so we're all done here with azure app services all we got to do is go ahead and go ahead and delete it so let's go ahead and delete our app here okay so there's a few different ways we can do it i'm going to do it via resource groups i believe we called it voyager here so click into that and i'm going to go ahead and delete the resource group and here is all the related services and so i will type in voyager and there we go great and so yeah there we go hey this is andrew brown from exam pro and we're looking at azure app services cheat sheet this is a two pager so let's jump into it azure app services is an http based service for hosting web applications rest apis and mobile backends you can choose a programming language in either a windows or linux environment it's platform as a service so it's the heroku or aws elastic beanstalk equivalent of azure if that helps you remember what it is azure app services makes it easy to implement common uh common features so for integrations and features such as azure devops github integrations docker hub integrations package management easy to set up staging environments custom domains attaching tls and ssl certificates you pay based on the azure app service plan so you got a standard tier a dedicated tier and an isolated tier notice that this tier does not support linux okay azure app services uh supports the following runtime so net.net core java ruby node.js php and python azure app services can also run docker as a single docker container or multicontainers you can also upload your own custom container so you'd upload it or you just deploy it so you create it upload it to azure container registry and then deploy you have deployment slots this allows you to create different environments for your web application you can swap environments this could be how you perform a blue green deployment onto the second slide here app app service environment asc is an app azure app service feature that provides a fully isolated dedicated environment for securing or securely running app service apps at high scale customers can create multiple ases within a single azure region or across multiple azure regions making ase ideal for horizontal scaling stateless application tiers or high requests per second rps workloads ac comes with some pricing tier which is isolated tier ase can be used to configure uh security architecture apps running on asc can have their access oop this should say granted not graded by upstream devices such as a web application firewall a waff and app service ascs can be deployed into an az using a zone pinning there are two deployment types for ase you have external ase and i r i l b asc ilb would be internal load balancer okay azure app services provides many ways to deploy your applications and there's so many i'm not going to listen here but i i probably would recommend that you go review the actual uh content for that because it just they would have made it would have made a whole cheat sheet here uh then the last thing here web jobs is a feature of azure app services that enables you to run a program or script in the same instance as the web app api mobile app there's no additional cost to web jobs so there you go let's take a look at what a key value store is so a key value store is a data store that is really dumb but it's super super fast okay and so they'll lack features that you would normally see in relational databases like relationships indexes aggregation transactions all sorts of things uh but you know there is a tradeoff for that speed okay and so here is kind of a representation of a key value store which uh you have a key which is a unique you know key to identify the value and i'm representing the value as a bunch of ones and zeros because i want you to understand that there aren't really columns it's just key and value so the idea is that imagine that those ones and zeros actually represent a dictionary and that's usually what they are is it associative array hash dictionary underneath okay and so even though it looks like you know what i mean like if this was a relational database you know you could see these as kind of like columns and so if we kind of did that that's how a key value store can kind of mimic um you know a tabular data right but the thing is is that you know there is no consistency between the the rows hence it is schemaless but that's kind of a way to get tabular data from key values but due to their simple design they can scale well well beyond relational databases so relational databases it becomes very hard to shard them and do a bunch of other stuff with them but key value stores are super easy to scale but you know they come with a lot of extra engineering around them because of these missing features all right let's talk about document stores so document store is a noscope database that stores document as its primary data structure a document could be an xml but it's most commonly json or json like structure and documents stores are sub classes of key value stores so the components of a document store compared to relational database looks like this so the idea is that you have tables is now collections rows are documents columns are fields indexes are still the same name and when you do joins they're called embedding and linking so you know if a key value store can kind of store this why would you do it well there's just a lot more features around the documents itself and so you know how we saw key value store didn't have like it had like nothing like no functionality well document sort brings a lot more of the functionality that you're used to in a relational database you know and so it makes things a little bit easier to work with okay all right let's take a quick look here at mongodb which is an open source document database which stores jsonlike documents and the primary data structure for mongodb is called a bson so a binary json is a subset of json so its data structure is very similar to json but it's designed to be both efficient and storage in both storage space and scan speed compared to json and bson has more data types than json has date times byte arrays regular expressions md5 binary data javascript code json's just strings integers and arrays it's very very simple but because it has all these new other data types and it's stored in this binary format it's not plain text it's actually binary data that's the what reason why it the storage space and the scan speed is so fast now if you did use javascript to perform an operation like say insert data this is what it would look like so you have kind of an idea that you're inserting items into a collection there okay just to list out some features of mongodb it supports searches against fields range queries regular expressions it supports primary and secondary indexes it's highly available it's it's high availability can be obtained via rep replica sets so replica to offload reads or access standby in case of failover momentum b scales horizontally using sharding mongodb can run multiple servers via load bouncing mongodb can be used as a file system which is called grid fs with uh with load balancing and data replication features over multiple machines uh for storing files mongodb provides three ways to perform aggregation uh grouping dat and aggregations just grouping data to return a query so aggregation pipeline map reduce single purpose aggregation mongodb supports fixed collections called capped collections going to become claims to support multidocument asset transactions so mongodb when it first came out didn't do all this stuff and people complained about it i like it being very hard to scale but now it's a lot easier to use so you know mongodb is something that is uh more uh a more popular option nowadays than it was a few years ago so there you go all right let's take a look here at what a graph database is so graph database is a database composed of data structures that use vertices nodes or dots which form relationships to other vertices via edges arcs and lines so some use cases here fraud detection realtime recommendations engines master data management network and it operations identity and access management and there's a lot they're saying like it's really really good for that i am something i want to look into later traceability and manufacturing contact tracing data lineage for gdpr customer 360 degree analysis like for marketing product recommendations social media graphing and feature engineering for ml so let's just kind of break down you know the little components here so what you'd have is a node and a node can contain data properties and then through that it would have a relationship through an edge and that relationship can have a direction and also data properties on it and so it's a lot more um verbose like in than a relational database and also just how it can point to stuff so super useful for particular use cases let's take a look here at azure tinkerpop which is a graph computing framework for both graph databases oltps and graph analytics systems olaps so tinkerpop enables developers to use a vendor agnostic distributed framework to traverse query many different graph systems they always say traverse because there's so many it's a tree right so there's a lot of databases that this thing connects to and so here they all are but the ones i want to indicate to you that are important is amazon neptune cosmodb hadoop via spark neo4j which is one of the most popular graphing databases orientdb and titan okay so the thing is is that this isn't a graph database it is a basically adapter to other graph databases and ticker pop includes a graph traversal language called gremlin which is the single language that can be used for all these graph systems so let's talk about gremlin gremlin is a graph traversal language for apache tinker pop and so it looks like this and sometimes you know like even without tinker pop and i think this is with cosmodb that they'll support this language by default so you don't necessarily need to have tinker pop to work with some databases but it's great to have that service if you if or like the framework if you need it so gremlin is is designed to write once and run anywhere wora gremlin traversal can be evaluated as a realtime query so lltb or a batch analytics query so over here it's just kind of showing you these are the oltps graph databases over here and then on the right hand side we have olaps okay and so gremlin hosted language embedding means you can use your favorite programming language when you write gremlin okay so there you go hey this is andrew brown from exam pro and we are looking at azure tables which is a type of storage for nosql key value data store within the azure storage accounts azure table stores nonrelational structured data with a schemaless design and there are two ways to interact with azure tables through the storage table storage api or microsoft azure storage explorer which i find is the easiest way to interact with it so just kind of looking at storage explorer there if you wanted to add an entry uh you'd have to provide a partition key which is a unique identify uh fire for the partition with a given table and a row key a unique identifier for an entity within a given part a partition and so you have all your data types here so we see string boolean binary data type uh double uh guids 32 and 64. if we wanted a query you'd have to query along the partition and row key so you could also do some additional filtering here so just notice here that um you know you have your partition key you put in your value like klingon and wharf and then this is not this is just additional properties you added a lot of time the way these key values work is that this will return the results like all the results and then server side and then client side these will be filtered client side i don't know if that's the case with azure table but that's generally how these things work and so there you go hey it's andrew brown from exam pro and we're looking at cosmodb which is a service for fully managed nosql databases that are designed to scale and be highly performant so cosmodb supports different kinds of nosql database engines which you interact via an api so we have the core sql which is their document data store their azure cosmodb api for mongodb their azure table and gremlin okay and this will be using probably tinker pop um so all of these nosql engines uh uh specify capacity so you can do provision throughput for pay for guarantee of capacity or serverless pay for what you use so if you are just playing around the service you can go ahead and choose that serv serverless option and so a lot of times when people talk about cosmodb they're usually talking about core sql so if you say cosmodb it's usually document but understand that there's a bunch of stuff underneath it now if you want to start viewing data and making stuff and playing around with it you'd use the cosmo db explorer which is a web interface that you can find at cosmos.azure.com so after you made your cosm db cluster or container whatever they call it then you can go access your database so here we have the sql api and so that would be the document store and you can just see here that we have we've created a new item here for that data okay and so i just want to show you that if you drop down here you choose container or database so we create a new container also if you are in azure it looks like they just have it here under the data explorer tab so it's the same thing it's the cosmo db explorer just in line okay so you don't have to like go to that url you could just click into your um your it's called account cosmodb account and go to data explorer i just wanted to show you here like if you made a graph database that you can do everything through this explorer for all the different types the interface will change a bit so here we'd add a new vertex right and it's just slightly different okay all right so the thing about azure tables is that you can use it within either cosmodb okay or you can use it within account storage and the thing is is that um it's a really good comparison to look at these two things because this way we can really understand like how powerful cosmodb is all right so what we'll do is compare the two so over here when you have azure tables in account storage it's fast but it has no upper bounds of latency for azure cosmodb it's going to give you single digit millisecond latency for reads and writes for throughputs it's variable throughput it's limited to 20 000 operations you get a guaranteed uh backed by an sla and no upper limits when you're using cosmo db for global distribution it's a single region and for cosmic db you have 30 plus regions for indexing you only get the primary index or partition and row no secondary indexes and then for cosmodb you get automatic and complete indexing in all properties no index management for querying you get query execution uses index for primary key and scans otherwise and for uh cosmodb you get queries that can take advantage of automatic indexing on properties for fast query times for consistency we got strong with primary region and eventual with secondary regions and with uh cosmodb there's like five you know what i mean there's this uh the consistent levels are a lot more flexible okay for pricing it's consumption based and then for uh cosmodb you have consumption based or provision capacity for this delays it's 99.99 availability and here it it's backed by an sla but some conditions it does not apply okay so you know hopefully that shows you that cosmic db like is very performant is globally available uh single digit millisecond and i i really feel like this is to compete with um adabus dynamodb because it sounds so similar to dynamodb but um yeah there you go hey this is andrew brown from exam pro and we are on to the azure tables and cosmos db cheat sheet for the dp900 i want to point out something uh that i'm sure you already know about but in the course i spelt cosmos db without the s like everywhere and i'm not going to go back and fix that but i know i'm going to hear like never the end of it for like the next year okay so uh let's start at the top here azure tables it's a key value data store can be hosted on either azure storage account storage it is designed for a single region and single table can be hosted on cosmos db and when it's hosted here it's designed for scale across multiple regions cosmodb a fully managed nosql service that supports multiple nosql engines called apis why they didn't call them engines i don't know coresql api this is the default one it's a document database you can use sql to query documents and when people are talking about cosmodb that's what they're talking about the document database the default one okay graph apis a graph database you can use uh with gremlin to transfer traverse the nodes and edges mongodb api a mongodb database it is a document database tables ai is just as your table's key value but within cosmodb apache tinker pop an open source framework uh to have an agnostic way to talk to many graph databases they probably won't ask you about tinker pop on the exam gremlin graph traversal language to traverse nodes and edges you definitely need to know what gremlin is and be used to seeing what it is like identify what it looks like mongodb an open source document database and the way it works is it has its own data structure its document structure called bson which is binary json a storage and compute optimized version of json introduces new data types cosmo db explorer a web ui to view cosmos databases and there you go hey this is andrew brown from exam pro and we are taking a look at cosmos db so cosmos db is a service for fully managed nosql databases that are designed to scale and be highly performant so cosmos db supports different kinds of nosql database engines which you're going to interact via an api we'll cover them more than once here but we have our core sql which is a document data store azure's cosmos db api for mongodb which is another kind of document data store azure table which is the old cosmos db that still sticks around which is a key value data store we've got gremlin uh which is a graph data store based on apache tinker pop that's why we were covering it earlier in this course here uh and all of these nosql engines uh specify capacity so we have things like provision throughput so you pay for a guaranteed amount of capacity or you have a serverless option so you pay for what you use so that's an option that you can choose there let's just talk about some of the main advantages of using cosmos db because azure really likes you uh to use cosmos db because they consider like one of their flagship database products so it integrates with many azure services like azure functions aks azure app services integrates with many different database apis like coresql mongodb as we saw cassandra as we saw gremlin as we saw there's sdks for a variety of languages.net java python node.js not ruby so i'm not sure if i'm going to be using anytime soon but there's a diff a lot of different sdks there uh it has a schema less service that applies indexes to your data automatically resulting in fast queries you get a guarantee of 99 to the three nines of availability there we have data replication between azure regions and this is all automatic uh data protect protected with encryption at rest and rolebased access controls rbacs auto it auto scales um it provides a way to handle variety of different workload sizes so you can see that there's a lot of advantages of using cosmos db so something that's very interesting about cosmos db api is that it's kind of like an umbrella service for a bunch of different types of databases um and this is a lot this can be confusing for someone that's coming from aws or gcp where like adabus has dynamodb and all it is is a document database where um cosmos db it like supports a variety of different kinds and so when you first create your cosmos db database you have to choose an api and so we'll just walk through the different types of apis here and just give you some information around them so the first one and this is the default one is core sql api so it is a document database but it's interesting because it allows you to use an sql or sqllike language and so this is quite popular because one of the challenges of using document database is that normally you don't get to use sql but with cosmos db you do and so that is something that's really nice then you have azure table api this can be a bit confusing because when cosmos db uh before it came out there was just azure table um and so azure table uh basically is just a key value store and you access that through azure table storages um like through storage accounts but then they decided to make azure table to be more of a document database and to make it highly uh resilient and highly available and highly redundant and so basically core sql api is like the version two of azure table api but this one still exists because it's very cost effective so if you don't need a lot of redundancy and you just need a key value store you need something very inexpensive you can use azure table api but even though you don't really access it through cosmos db it still is part of the cosmos db product because it was the first iteration or version of it you have mongodb api and so um this is uh at it's not mongodb i don't think underneath but it's uh it's compatible with mongodb so the idea is that if you need mongodb this is where you're going to spin that up same thing with cassandra if you need cassandra you would spin that up as well and this is going to use the cassandra queried language and so that is something that is there then you have gremlin api so this would allow you to use a graph database you can see these are really really different things and it is a bit confusing but uh that's just how azure likes to organize their services so hopefully that clears that up for you okay so let's talk about the core components for cosmos db because it is a bit confusing to try to understand the relationship of all the components for a fully managed database it sure has a lot of moving parts and so the first thing when you go over to the cosmos db portal you're going to spin up a database and so the idea is that when you choose a database the idea is that this is going to be a logical namespace for your collections your users your permissions it's going to determine the api that you do choose uh once you've chosen your uh your database then the idea is that you need containers and normally when you create a database it'll create a container for you and a container is just a unit of compute but there is a lot of stuff that you can attach to that container information when you create it and then you also have collections i find collections to be a bit confusing but they map the they basically map to a container and this is the billable entity so this is the way that you're going to determine the cost um and this is where you're going to be stating that information so when you create a container it'll have a bunch of information below it um and that will be uh stuff that will be attached to the collection entity so hopefully that kind of clears that stuff up i do find containers a bit confusing because again when you create them it's at the same time as your database but we will talk about that because we do we do come across that in the course okay let's talk about how we're going to access our cosmos db database with cosmos db explorer so it's a web interface to explore and interact with your cosmos db accounts accounts database you know basically the same thing and this is at cosmos.azure.com so they have a dedicated domain subdomain for it so the idea is you type that in and you would authenticate with your account um azure's always single signon so it's pretty easy and then the idea is that you can access your database and interact with it without having to write any any kind of uh sdk code or interact with cli you have kind of a gui to do that so example here is for the core sql and you can interact with all the different types of database in here so it's not just coresql but there's a lot that you can do with it and so the idea is you can create containers from here create new databases um so a lot of similar things you can do from the the um the cosmosd portal with an azure portal so there's some some stuff there it doesn't matter where you create it it's just where you want to do that there there is an easier way to access this so instead of going to cosmos azure.com the way i like to access it is that i'll just be in the azure portal for my database and uh what i will do is access it under the data explorer tab notice here it says azure cosmos db account it gets confusing because they say like an account can or like can contain multiple databases but really it's just one database it's so confusing so you'll hear me interchange the term account and database because the documentation does not clarify it but in the in the gui you can see it says the word account there but yeah this is the way that i would recommend accessing that information um you know you can also again access different databases so like for example we can see this is for gremlin so here i'm creating a new vertex which is a graph graphing thing there and so that's how we can do it there so it's not just for the core sql okay let us talk about partitioning schemas in cosmos db because it's a very important concept when we're talking about new sql databases because um you know it's all about scale and when you're dealing with databases of scale you're dealing with partitions and this is where we have a lot of information about partition keys so the data stored among partitions in amazon cosmos db indexes is grouped by the partition keys in order to improve performance main concepts of partition schemas in azure's cause of cdb is partition keys so these are used these keys are used to group items it works like primary keys if you know from traditional relational databases then there's the idea of a logical partition this is a group of items that have all the same partition key values you have physical partitions so this consists of a set of logical partitions azure cosmos db manages logical partitions which can have one to many relationships we have the idea of replica sets these are made up of a group of physical partitions that are materialized as a selfmanaged dynamically loaded balanced group of replicas that span across multiple fault domains so i imagine when they say the word physical they're probably saying like it's the actual physical underlying machine and then these are the virtual logical partitions within those machines and then replica sets just means duplicates of that kind of stuff so hopefully this diagram here will maybe help a bit here to make sense of it there's a lot going on here but maybe we can break it down so each physical partition consists of a set of replicas also known as replica sets so over here um we see partition set and then we have partition physical partition um and you can see that it's it's represented over here so confusing but i think this yeah this thing this thing here is the physical partition okay over there um then we have logical partitions so these are mapped to physical partitions that are uh distributed globally so notice that it's these little boxes in here so it was what i said so this is the physical machine and then these are the virtual partitions within them and they're distributed globally partition sets in the image refer to a group of physical partitions that manage the same logical partition keys across multiple regions so hopefully that makes things a bit more clear uh in practicality you probably won't need to remember all of this for the exam you definitely don't need to know this but it's good to just kind of go through it so you have an idea of these terms so there you go all right let's talk about choosing a partition key this is super super super important to cosmos to be so the idea is that a partition key is made up of two components the partition key path and the partition key value so the idea is that let's consider a document or a key value that we need to try to figure out what our prediction key path would be so we have user id andrew works for microsoft so in this case we're trying to find something that is um like a unique identifier for that item and so i would say a user id would be that thing there so in this case our partition key path would be forward slash user id um so you know that would be part one of it and so the partition key path accepts numerical or underscore characters you could also use nested objects by using the standard path notation of forward slash so that helps there um and so an example here would be forward slash id or we do forward slash user id uh the second part of it is the value so the partition key value can be either a string or numeric types and so you can see in our example it is a string so that's going to be accepted your partition key for all containers should be a property that has a value which does not change you can't change the value of a property if it's your partition key a property that should have a wide range of possible values that's why we said user id because it's going to be pretty unique per per person whereas works for you're going to have a bunch of groups of that like there might be a lot of people who work for microsoft so it's not as unique um spread request units is something that's important are used these are consumption and data storage that will be evenly across all the logical partitions this ensures uh even ru consumption and storage distribution across your physical partitions so hopefully that gives you kind of an idea of how to choose a partition key but now we'll talk about unique keys so uniques provide developers with the ability to add a layer of data integrity to their database by creating a unique key policy when a container is created you ensure the uniqueness of one or more values per partition key and the unique key is scoped to a logical partition so if you partition the container based on the zip code you end up with duplicated items in each logical partition so in the example here you can see our partition key is going or sorry our unique key is based on the first name uh the address address zip okay so two different values there so because if you had first name there might be a bunch of andrews right and if you want to guarantee that there's only a single andrew you might scope it based on a postal code or zip code you can't update an existing container to use a different different unique key so you really do have to plan ahead to make sure you choose the right thing a unique key policy can have a maximum of 16 path values path values being i believe the separation of the comma a each unique key policy can have a maximum of 10 unique key constraints or combination combinations so there's that when a container has a unique key policy the ru's charged to the create update deletes an item are slightly higher and we haven't talked about our use yet but we will get to it in the course here so just hold on your keys aren't case sensitive so just consider that as well so there you go all right now it's time to talk about cosmos db containers because well containers we don't have really any compute to run these databases on and we do talk a bit about containers here so azure cosmos containers are useful for scalability in azure cosmos db both in terms of storage and throughput they are beneficial when you need a different set of configurations for each of your as azure cosmos dbs because they allow you to customize each container individually so azure cosmos containers have some container specific properties and those properties which can be system generated or user configurable vary according on the used api so the idea is when you first create a database um you actually have to create a container with it so notice here we'll create a new one and then we have a bunch of options here which that's what we're going to be talking about is all these options around containers so the properties of contain cosmos could db containers uh you know there are ones that are system defined properties and depending on which api you use some properties might not be directly exposed so uh the example of those system defined properties are here so you can see them underscored like underscore rid i'm sure etag underscore ts underscore self the id and so notice here that these are all system generated this is user configurable and it's going to vary based on these it looks like it's mostly just that sql api and then the ids on all of them there um but you know just to read through this quickly we have unique identifier for the container the entity tag used for optimistic concurrency control the last updated timestamp of the of the container the addressable ui for the container the user defined unique name for the container and there's more than these but these are the ones that i just picked out to show you here uh but next let's talk about capacity for containers okay so let's talk about capacity for containers but what is capacity capacity defines the amount of underlying resources that are available to support consumption of resources such as compute and storage i made that sound much fancier than it is but it's just what is available to you so like how much compute can i use how much storage can i use and cosmo cb has two capacity modes this is no different than i mean similar to a dynamodb but there are other ones called ondemand it's not called serverless but we have provision throughput and we have serverless and you're going to choose between those two options let's talk about what is different here so for provision throughput for each of your containers you provision some amount of throughput expressed in ru's request units per seconds and so for workloads where you you can reliably predict the traffic uh or you have more flexibility of your options that's where it makes sense to utilize it okay and for serverless this is where you can run your database operations against your containers without having to provision any capacity this is great for low or small workloads or for unpredictable spikes in traffics it's easy to configure but has some limitations so provision throughput makes sense at scale serverless makes sense when you are getting getting started or you're just finding that it's just easier to manage but you're going to really have to decide between those two options which is what we're going to look at here so just to kind of break down some additional options between these two uh when we're talking about geo distribution for project for provision throughput you can run it in unlimited regions where servers can only run in a single region the max storage per container is unlimited for provision throughput but for service it's 50 gigabytes for performance we're looking at uh sub sub 10 millisecond latency for re point reads and rights covered by slas service level agreements uh point reads uh is under uh sub 30 seconds for rights covered by slos so a little bit different there the building model is very different so for provision throughput building is done on a per hour basis for the ru's provision regardless of how many rus were consumed and serverless is billions done on a per hour basis for the amount of our use consumed by your database operations so in theory serverless can be more cost effective but it really just depends on what your consumption models look like we're not done with capacity we're going to look a little bit more at throughput okay so there's more to the story for provision throughput because you have uh different throughput strategies that you can choose when you create your database container and we said earlier when you create a database you always have to create a container with it and the two modes are dedicated and shared mode so for dedicated mode throughput is exclusive to this container and backed by slas for shared throughput or shared mode the throughput is shared across all your containers you cannot create throughput strategies after creation so you have to choose wisely and to switch throughput strategies you basically just have to create a new database and migrate that data over or sorry container and create migrate that data over so when you're creating your database and your container there's this little checkbox for shared throughput across containers that's how you know it's set for shared mode shared throughput is not available for capacity mode of serverless because it's all about provision throughput which makes sense so when should you share when should you not share so sharing the database level provision throughput among its containers is um anonymous i don't know why i chose that word it's such a hard word but that means to compare to compare things uh to hosting a database on a cluster of machines so because all containers within a database share the the resources available machine you naturally do not get predictable performance or any specific container but you get better utility or usage of your resources they're not going to waste so i think it's most like a cost like a cost strategy kind of thing because if you have a bunch of containers they're not being used you want it evenly distributed but yeah that's all there is about that feature okay okay let's talk about reading data from cosmos db there are two different ways to do this we have point reads and queries and this is a little bit confusing and i honestly don't know what always cost points and what don't but impracticality is not that bad but let's take a look here so point reads is a is a key value lookup on a single item id and partition key and queries allows you to return multiple items so let's break it down here so the idea is that when you are doing point reads you have a latency of about 10 milliseconds queries it just varies so you know could be higher could be lower who knows uh in terms of the ru's that you're charged and i know we haven't talked about our use but that is the next video so just hold on or come back to this one to help materialize that information but for point reads it's always one request unit okay and so that is very predictable for queries it's 2.3 are used at least and it's going to really vary based on what your query is the number of items returned for a point read is one item queries is unlimited and the idea is that should you include the partition key for point reads it's required for queries it's recommended so hopefully that gives you an idea but those are your two options when reading and i think in the the follow along we will we'll look at these two types of read options okay all right we keep mentioning request units are used now it's time to actually describe what they are so the cost of all database operations is normalized by azure cosmos db and is expressed by request units so ru's abstract away memory cpu iops iop standing for input output per second and it's basically right sizing all these things for you mean like choosing the right size with a single value based on data processed so database operations uh include reads so the size of the item retrieve so one kilobyte would equal one ru 100 kilobytes would equal 10 ru's so we're just looking at the calculations for read operations for ru's an insert uh is inserting one kilobyte item without indexing cost so this is around 5.5 rus this is interesting because like dynamodb which is adabus's product they have um they do they have reads and writes so they're separate but cosmos db just has read units then there's updating so replacing item costs two times the charge required to insert the same item so there's some math there i don't deletes cost i couldn't find it for queries it's the size of the items retrieved so it's going to greatly vary because we saw earlier that you can have unlimited items retrieved so you're pretty darn expensive to calculate capacity it is very very confusing so the idea is that there is a capacity calculator and this is what you should use on the exam they're not going to ask you a bunch of math questions i point that out because on aws exams for identity b you have to know the math for this not so much but you have this sweet calculator that you can use to figure out the capacity that you'll need okay so a very very important concept when we're talking about fully managed databases especially nosql databases is consistency when am i going to see my data what am i when is it going to be consistent across all the servers that it's replicating data to so azure cosmos db provides five different consistency levels to maintain data availability and creating performance depending on your requirements and so we have this nice graph to kind of or graph um yeah graph to help visualize the five levels on the left hand side we have high latency lower availability the worst read scalability and on the right hand side we have lower latency a higher availability better read scalability so it's going to be based on what you need and so we'll start with the first one which is strong consistency level so this is linear realizability reads are guaranteed to return the most recent version of an item so strong means a guarantee that what what you read is what you get okay but the latency is going to be higher so it could be a bit slower for it to get back to you then you have boundless staleness so this is a consistent prefix it reads reads leg behind writes by most k prefixes or t intervals so just understand that it's a bit better than strong we have session so it has a consistent prefix it's uh monotonic reads monotonic rights read your rights write right follows reads okay i know this might not make sense but don't worry we're going to go through all of these still we're not done we'll go through the list and then we'll go in more detail on all these you have consistent prefix so updates returns are are some prefix of all updates with no gaps you have eventual so out of order read so you might read something and depending on what partition it hits or what a replica hits the data might not be all up to date and so the way you're going to set your consistency is you're going to choose it here and there'll be additional options this is honestly really confusing again i'm going to keep comparing aws aws dynamodb but it was done by db it's not it's complicated right or even like a bus s3 where it's just like it's it's eventual or it's consistent um so you can see there's a lot of options here but you know of that flexibility you have more choices i guess so let's talk about all these again so strong consistency read operations ensure that the most recent data is returned read operations cost the same as boundless staleness but more than session and eventual consistency write operations can only be read after the data has been replicated by most replicas when they're talking about costs i i guess they mean like the cost to retrieve information i'm not 100 sure again i didn't see on the exam about costs with consistency levels but uh this is the language that is being used in the documentation so that's why i'm bringing it over for boundless staleness read operations are like behind write operations due to time or version differences read operation costs the same as strong consistency and more than session eventual consistency has the highest level of consistency compared to session consistency consistent prefix consistency and eventual consistency recommended for globally distributed applications with high availability and low latency so remember it's for globally distributed applications we have sessions so read operations uh ensure that written data is consistent with the same session consistency is scoped to a user session while other users may encounter dirty data if another session has just written to it uh it's the default consistency consistency level used to uh when using a newly created database reading costs are lower than boundless staleness and strong consistency but higher than eventual consistency we have consistent prefix so read operations ensure that the most recent data replicated among the replicas is returned but it does not guarantee the data is the most recent uh the idea is we have dirty data occurs when one replica changes the data state but this data has not yet been replicated they keep using the word dirty just meaning like when you replicate when you have that piece of data and it's stored in multiple places redundantly some of them are dirty so they're not all up to date has a stronger consistency level of eventual consistency but less than any other then we have eventual read operations does not guarantee any consistency level lowest consistency level low lowest latency and best performance among consistency levels so things are really fast least expensive read operation cost compared to any other consistency levels so there you go there's all the levels um you know hopefully that makes sense again it's very confusing but there you go let's talk about a very useful feature in cosmos db known as change feed so change feed is a service that monitors changes in all containers and distributes events triggered by those changes to multiple consumers so change feed in azure cosmos db is a persistent record of changes to container in in the order that they occur and the utility of this like this is not unique to a cosmos db a lot of databases might have like extensions or modules dynamodb has something similar called dynamodb streams but the idea is that you have a data source and you have an insert and update and the idea is that that record goes into the change feed and then you can react to that chain to that record and push it to other services and so the idea is just it's like a way of triggering something on an insert so you say okay i insert this data um and so then send this data over to this other service so it's just a way of triggering stuff okay azure has an sdk for net java python no js nothing for ruby they're having lots of trouble with the ruby sdk at some point they'll fix it the change feed is supported for sql api cassandra api db api gremlin api basically all the apis the exception of table api so that's that in terms of its implementation the change feed processor is composed of four components we have the monitor container this is where any insert or update executes the operations are reflected in the change feed i can get my pen out here so if we can see where it is so the monitor the container i guess collections are up here um we have lease containers see they're saying collections because the collection um remember a collection is a mapping to a container so collection container we're kind of just interchanging right here but it stores the state and coordinates the change feed processor which is down here the host an application instance that uses the change feed processor to listen for changes so that's the way it's it knows what to do um the delegate the code that runs when an event in the change feed notification triggers it and so that's the code within the consumer there the change feed processor may be hosted among azure services that support longrunning tasks such as azure web jobs azure virtual machines azure kubernetes services azure.net hosted services so we could have looked at the code it's a bunch of csharp code that they have as examples it's super not exciting um but you know i just want you to know about this feature but again it's not unique to cosmos db but it is a common thing that you should know about for these kind of fully managed databases okay hey this is andrew brown from exam pro and we are looking at azure storage accounts which is used for containing all your azure storage data objects such as blobs files queues tables and disks so storage accounts is one of those services where it has a bunch of different storage types within it so it's a quite the multipurpose service and with each different type of storage it's going to have different features and their own pricing models let's just quickly break down the type of storage we have so we have general purpose version 1 version 2 blob storage block blob storage and file storage and so i just want you to know that when you are using storage accounts i'm saying storage type but for whatever reason the ui calls it account kind just to be aware of that small little discrepancy there but storage accounts are going to vary based on features and so the common features we'll see across storage will be supported services so where can i put the storage account and so on the right hand side you can see like if you make a general purpose version 2 what you have access to such as containers cues tables or falsers those options are going to change based on what you're using all right for performance tiers this is how fast you'll be able to do reads and writes you have standard and premium uh you have different kinds of access to yours so how often do i need to quick access these files and then there's replication how many redundant copies should be made and where and the last thing is deployment models so who should deploy the supported service resource manager classic and in generally most cases it's going to be a resource manager so here i have all the storage types on the lefthand side and we have the feature set and how it's going to vary based on the certain types let's just quickly look through this to see where there are some standouts and you might want to actually review this at the end of this section just so uh you're a bit more familiar with all these types of features so it clicks a bit better but let's just quickly go through this so you're gonna notice for version one this is the only case where you're gonna have a deployment model of classic everything else is gonna be the resource manager from a practical standpoint you're not going to really notice because you're just going to be pressing buttons but underneath that's the only case where it varies for replication you're going to notice that version 2 has the most options with replication and if these don't make sense don't worry we're going to cover all the replication in an upcoming slide for blob block storage you can see it's very limited same with file storage so that's there when we're talking about access tiers how quickly you can access files you're going to notice that it's only available for general version 2 and blob storage where we're choosing these different tiers for these ones it doesn't really matter um because well especially like a file storage because the drive is as fast as it's going to be right for performance tiers you're going to notice that with version 1 and version 2 we have standard and premium when you're using file storage and block blob storage you're always using premium and with blob storage which is again a legacy format there that's going to be using standard uh blob storage comes in three different types and you're going to notice that uh based on what you want to use there'll be some variation there um i don't know where page is i think that they both support page but there are three types in there i wouldn't really worry about it too much uh you're gonna notice that file storage only supports file types and then you have version two and this pretty much uh supports everything so you can see general purpose version two is a really great um storage to choose so we were just talking about storage types now let's talk about the actual storage services that we can actually launch or utilize within these storage types under storage accounts and there are five core storages available to us the first is azure blob this is a massively scalable object store for text and binary data it also includes big data analytics through data lake storage gen 2. um and so azure blob is really great because you just don't have to think about the file system you just upload files and they're treated like objects so well that's really nice then you have azure files and this is a file share and so the great thing about azure files is that if you want to have a bunch of virtual machines and have it so like they have um the same file system sharing all the same files that's what you're going to use that for then you have azure queues to me this really is a database but for whatever reason it's under uh storage accounts and it's a nosql store for schemaless storage of structured data this is another unusual one uh i don't know why azure puts it under here but it's a messaging store for reliable messaging uh between application components to me that's like an application integration service but azure categorizes it as storage and the last one is azure disk and this is block level storage volumes for azure vms and so the idea is that when you want to do the top four the above you're going to be launching storage accounts and for disks you're going to be launching disks it's a bit unusual because um one of the storage accounts say that you can use or store disks uh in i think version or uh general purpose version two i'm not really sure i understand that uh maybe it's talking about like backing them up or something like that but anyway from practical standpoint and for what we need to know is that this is the breakdown here and we do have a full section on azure disks so we will get into that so now let's take a look at some of the features that are available on account storage the first being performance tiers and generally this is going to be for blob storage and we have two types of performance tiers we have standard and premium so it's as simple as uh just choosing between the two and when we're talking about performance especially when we're talking about storage we want to be thinking about iops and that stands for input output operations per second so the higher the iops the faster a drive can read and write so you can uh definitely assume that premium is going to have a higher amount of iops and so when we're looking at the premium performance these are going to be stored on solid state drives ssds which we have in the picture there it's optimized for low latency higher throughput and the use cases here is going to be interactive workloads analytics ai or ml and data transformation on the other side for standard performance these are running on hard disk drives and you're going to have varied performance based on your access tier and we're going to talk about access tiers very shortly but the tiers would be hot cool and archive and this is great for backup and disaster recovery media content bulk data processing and things like that and so the reason why ssds are generally really really good for premium performance is because they have no moving parts within them and the data is distributed ram uh randomly so if you have to do a read and write the distance for between the read and writes are going to be a lot faster uh and that's generally why you're going to see solid sacrifice with premium performance or things with higher iops and then for hard disk drives it does have moving parts so you see that it has an arm and that arm needs to read and write data sequential sequentially to the disk and so it's very good at writing or reading large amounts of data that is close together that is sequential but you know the idea is that neither uh format is good or bad it's just the the use case that you need so you don't always need to go with ssd sometimes you want to save money and hhd's are or hdds are really good for that let's take a look at access tiers and there are three types of tiers for standard storage we have cool hot and archive and so you're going to have this option between cool and hot and archived archive might not show up depending on how you configure your storage account so for example if you use a particular type of replication that might not be available to use with archives so just be aware if that option doesn't show up you might have to change some of your settings let's quickly walk through the three options so hot is for data that's accessed frequently and has the highest storage costs and lowest access costs so again it's for data that is frequently accessed or in active use or data that's staged for processing and eventual migration to the cool access tier then you have the cool tier this is data that's in frequently accessed and stored for at least 30 days lower storage costs higher access cost and the use case here is for shortterm backup and disaster recovery data sets or older media content not viewed frequently anymore but is expected to be available immediately when accessed and i think there's actually a third use case which is large data sets that need to be stored cost effectively while more data is being gathered for future processing then we have the archive tier this is for data that's rarely accessed and stored for at least 180 days it's the lowest storage cost but it also has the highest access cost and so its use case would be longterm backups secondary backup archival data sets original raw data that must be preserved even after it's been processed into its usable form and compliance and archival data that needs to be stored for a long time and is hardly ever accessed so just make use or note of the cool is at the least 30 days and the archive is at least 180 days i should have highlighted those for you i don't know why i didn't and so just some other things you need to know so for account level tiering any blob that doesn't have an explicitly assigned tier infers the tier from the storage account access to your settings so you can set blobs at different storage levels blob level tearing you can upload a blob to the tier of your choice and change uh change the tiers happen instantly with exception from moving out of archive the concept of rehydrating this is when you're moving a blob out of archive into another tier and can take several hours uh you have the blob lifecycle management so you can create rulebased policies or transition your data to different tiers so after 30 days we can move it to cool storage and here is just the option here you see where it says 30. i think 30 is the minimum days you have to choose um but it could be wrong and so you have the options down below so move to cool storage move to archive storage delete the blob and just a few other things here when a blob is uploaded or moved to another tier it's it's charged at the new tiers rate immediately upon the tier chains so when you're moving to a cold a cooler tier the operation is billed as a write operation to the destination tier and when the right operation per 10 000 and data right per gigabyte charges for the destination tier applies when moving to a hotter tier the operations billed as a read from the source tier and where the read operation again in the 10k and the data retrieval again in gigabytes charges for the source tier applies and early detection charges for any blob moved out of the cool or archived tier may apply as well and just lastly here uh when we're talking about cool and archive early detection any blob that has moved into the cool tier so this is only for general purpose version two accounts is subject to a cool early detection period of 30 days and any blob that is moved into the archive tier is subject to an archive early detection of 180 days and this charge is prorated so you know access tiers aren't the funnest thing to talk about but there is all the information you need to know let's take a closer look at az copy and this is a command line utility that you can use to copy blobber files to or from a storage account so the first thing you're going to need is the executable file and there you can see they have it for windows linux and mac and then once you download that file what you're going to have to do is also make sure that you have the right level of authorization with for the user account they're using with it so you might need to for download you'll need storage blob data reader for upload storage blob data contributor and storage blob data owner so just be aware that you need to have those uh rules available to you to your user account i think that i have access to everything so i don't think i even i set this it just works but if you're in a larger company and you have more permissive least permissive roles you just need to know about that and so you can gain access via either the azure active directory or a shared access signature so let's just take a look at that right now so the idea here is that we'll type in a z copy login and this is going to ask us whether we want to sign into the web browser so that is uh that should be azure active directory option one and so what you'll do is you'll enter your username and password and you'll then have to enter the code displayed there and so now you're ready to use uh the um the cli and so all you have to do is type in az copy copy and then you have the file and then you give it the endpoint to the storage account uh and the container and the location you want it to go in if you want to download files it's the same command you just reverse the order you say uh this is the the location of the file and want to download it locally all right hey this is andrew brown from exam pro and we're taking a look at lifecycle management which offers rulebased policies that you can use to transition blob data to the appropriate access tiers or to expire data at the end of data lifecycle so what you can do here is transition blobs from cool to hot immediately when they are accessing access to optimize for performance transition blob blob versions and blob snapshots to a cooler storage tier if the objects have not been accessed or modified for a period time to optimize for costs delete blob blob versions and blob snapshots at the end of their life cycles to find rules to be run once per day at the storage account level apply rules to containers or a subset of blobs using prefixes or blob index tags as filters so here's an example of adding a rule and one few things here you might want to read here is the fact that you can apply the rules to all your blob or all blobs in your storage account or limit based on a filter you can choose the blob type or the blob subtype and then there's the actual rules themselves so here you can see if last modified by one day then delete the blob if last modified in the two days then move to cool storage so it gives you kind of an idea of things you can do there okay in some cases you'll want to be able to retrieve properties and metadata or set meta data for storage accounts so let's take a look at some of the ways we do that with the cli the first is for container properties and so the idea here is we have azure container containers show so here's an example where it is showing container show information so that's the property information here and then let's say we want to get we want to update that data then we would just use the update command and then the idea here is we can pass along data that we want to change then there's also the idea of showing so here we're just showing that metadata on that container i have a little bit more here i suppose so we can also update metadata on a blob so very similar process but that's for a blob and as you can imagine there is a show command as well so there you go hey this is andrew brown from exam pro and this follow along we're going to learn a bit more about storage accounts and how to copy uh content back and forth using az copy so what i want you to do is go all the way to the top here and we'll type in storage accounts and then from here we're going to create ourselves a new storage account and we'll make a new resource group i'm going to call this one fajo we'll type in fajo here and we can have it in any location that we so desire we're going to stick with standard here general purpose 2 seems okay to me we'll go ahead and hit review and create and then we'll hit create there and i'll see you back here in a moment all right so now that our storage account is ready here we'll go in here and we'll create ourselves a container so i'm going to call this one kievas faggo we're going to leave it as private because we technically don't want anyone to have public access to it and so there is a tool that you should definitely know how to use called a z copy so if i just type that in there quickly here and we go and scroll down here are some versions we can download so if you're on mac linux windows or etc we can use it i want to try to give you a consistent experience on how you can utilize this so i'm going to just try and stumble my throat way through and use it via the cloud shell because i think that would probably be the most consistent way for all of us to use it so make sure you are in bash and we're going to need two things you're going to need this az copy tar file and we're also going to need some kind of file to upload okay so the question is how are we going to get things into our cloud shell well the great thing about cloud shells when you launch it it asks you to create a storage account and we should probably be able to place those files into there so if we go back to storage accounts at the top here and uh we go here this is a storage account so if we go in here if there were containers we go back but i believe what they do is they set you actually up a file share here right and so in here it this is what it's doing so what i'm going to do is upload two files so i'll just go here and one is going to be this which is the kivas fajo which is just an image okay and what we can do is i will download this linux here and once that is downloaded what i'm going to do is also upload that okay so i just drag that to on to my desktop it's a bit easier to access i might just rename it so it's a bit easier to work with so i'm just going to rename the uh the tars gz to a copy here if you can see and uh we'll go back here and we'll just go ahead and upload that okay so we will upload that as well and it's only 11 megabytes so it shouldn't take too long here but what we need to do is find out where these files are so what we'll do is make our way back to our shell i'm going to type in ls and i think it might be mounted here so we'll do ls and there's our file so um what we'll have to do is untie the az copy so we get the binary so i have a handy dandy command over here from i just found one over here because i can never remember this command the tar command so we'll type in tar hyphen x v z f and then the name of the tar so i'm going to start typing a z and then hit tab to auto complete and that should uh unturr it and so now we have our oh we made it into a folder so i'm going to go into that folder there and so now we have our easy copy here okay um so just to make our lives a bit easier i'm going to want to move that easy copy back one directory so i'm going to type in mv for move i'm going to type in az copy and i'm going to do period period backslash so it says go or forward slash you'll go back a directory if we cd and do an ls just do clean again we'll do ls we can now see that it's a bit easier to work with i'm going to delete the that long folder there because we don't need it when it's a folder you got to do a recursive so hyphen r and we'll also go ahead and delete the azcopy.tar file okay and we'll do uh clear and then we'll do an ls hyphen la that'll list everything because what i'm trying to see is whether this file is executable appears that uh we can execute it if we can't it's usually a good habit to do uh chamod u plus x a z copy here and then if we run this again um yeah it looks the same so i think we'll be in okay shape here but uh yeah now we're all set up to um go ahead and use the az copy command so let's give it a go all right so there's two ways we can get um or authenticate with easy copy and if you remember from my handy dandy az copy slide we have two options which is we can use this login or use the shared access signature we're going to give both a try but we'll start out with azlogin okay so we'll do type in az copy login and what that will do is it will prompt us and ask us to sign into the browser and enter this code so i'm just going to grab this link here and we will copy it i'll make a new tab paste it in and once here we'll go ahead and enter in this code okay and we'll just say yep andrew brown and so now that is now synced so now we should be able to use the az copy command uh in order to use this again we pull up my reference here we have a z copy copy the name of the file and then the name of the path we want wanted to go to okay so what we'll have to do is uh in our container here it's called key vas fajo so we'll have to remember that but if we go here we're looking for that publicly accessible url or that fully qualified domain url um i would have swore it was here um oh sorry we're in the wrong storage account by the way we have to go into our actual faggio one here just in case just making sure we're in the right place but let me just go find that url i can't remember if we have to assemble it by hand or if they just have it handy here so just give me a moment all right i'm back and i had to pull up the docs for this one which is not a big deal um but the structure here is we're going to have https the account blob or windows.net the name of the container and the path to the blob okay so that's what we're going to do here so let's give it a go and see if we can get it to work the first time around here so we are typing well before we do that i'm going to type clear and then do ls so we're going to type in a z copy and the command is cp or actually you can type in copy if you want it looks like cp is abbreviation and so the path to the file and so in our case it's going to just be local so i just typed in k and i tabbed and the next thing we have to type is https colon forward slash forward slash the name of the account so it's fajo and then blob dot core dot windows dot net forward slash and then we named the uh container here kivas fajo and then we'll type a forward slash and then we can name the file however we want so i'm going to name it the same as the jpeg okay and so hopefully this just works and i mean it looks like it worked pending says the file transfer might have failed here hold on so it says we're not performed to other we don't have access to perform that there so just give me a moment okay all right so i figured it out um there was one key thing we forgot to do so the reason we're getting a 403 was as plain as it was is we didn't have access to do it so if we just go back to our storage account there um and this is within the container so i'm just going to go all the way back to storage accounts we'll go into fajo here and so on the left hand side you see it says access controls right so if we were to go into here and we go to role assignments we can see what what role that people have but in order to access blob storage you need to like to be able to upload and download you either have to be the storage blob data owner or the storage blob data contributor so if you want to do that you just have to go up to add and do a role assignment and sign yourself this role it does take about five minutes to take effect so even after doing at doing this adding it to my user here i wasn't able to do it right away but after waiting a little bit here i was able to go and do a test like with list and it didn't print anything but that means that there is no error right so we probably do have access another little thing i found out which was nice is that if you go let's say into a container here um and then you just clear that out we'll click into that container and on properties here it actually gives us the url so it makes our lives a little bit easier so we have to type that whole thing out and try to figure it out but let's go back and try to actually do a copy now so that's what we've been trying to do this entire time so what i'm going to do is type in az copy and then we want uh kivasvagio and then i'm going to paste in the url that i just got from there and i'm just going to put a forward slash here i'm going to type in kivas faggot.jpg um oh you know what we got to put the word copy or cp in front of it and it looks like it completed that looks good to me and so if we make our way back here to um to our container we can see that it's uploaded so there we go we were able to upload it with um active directory so now let's go give it a try with a sas okay all right so let's give this another go here and this time what we're going to do is use an sas so it's as simple as attaching it to the end of the url for the the destination or the source depending on what we want to do but what i'm going to do is i'm going to go ahead and delete this file here just because we already have it and what i'm going to do is go back to my storage account and what we need is a sas here so i'll type it in here i wonder if we can type it the oh yeah we can here that's great so type it here at the shared signature access and we'll say what do we need access to well we're only using blob storage so we get rid of file queue and table allowed resource types will be for container because that's what we're doing uh and we aren't deleting we don't necessarily need a list but we can have that there read write add create that seems okay to me enable deletion of versions um i mean we don't have to turn that off um but we'll just leave it in place i think we will do http or https just have some flexibility here and this looks okay we have a few different keys we're gonna stick with key one and so i'm gonna go ahead and generate out an sas connection string and so this is the token we want see how it has a question mark it's going to help allow us it's going to be creasing allow us to put that on the end of our url so what i will do is i will i don't know if we can write a z copy log out let's see if we can do that oh we can great so just making sure that it's not authenticating that way and so what i'll do is i'll go back up to uh this link here and this should fail right because we don't have access right and so what we should be able to do is just take this string and place this on the end as such and hopefully just works and it says done done done so if we go back to our storage here to see if that is uh working as expected we go into that container um let's just move this down below here it gets a bit hard to see when you have that open like that over in storage account right now okay so we're going to type in container go here click into uh devos fagio and we don't see the file so let me give it let me give a try here okay all right let's give it another go here um you know i just don't necessarily trust url actually it says fail to perform copy no sas token or oauth provided which is fine but in this case we did and we saw it done done done and so here are the values it's printing out so maybe what we should do is just regenerate this out and be a little bit more uh permissive um and maybe that will give us an easier time because we want read oh this is slightly different a for the container not sure why this looks different all of a sudden but what we'll do is we'll go back here we'll just go close a bit we'll go back into fagio i'm going to go look for the container again and what we'll do is click into the container and then from here we'll do shared access properties um we'll do account key which is fine so key one i want i'm just going to put everything on here to make our lives a bit easier the start date is fine the end date is fine will allow http or https and we'll generate that token so here we have it again so we'll do is we'll just grab this whole thing here actually we probably yeah we'll just grab the token um and then what we'll do is make our way back to here i'm going to hit up and what i'm going to try to do is remove the end here and give this another go okay so paste that in i don't see the question mark on that so this is not uh this is not an easy one okay we'll put question mark here hit enter it just says stopped done done done done so it makes you think that it's working clearly is not um let's just double check the container to see what's there okay no file still well what if we uh grab this one here maybe it's an encoding issue or maybe it doesn't like the way we do the link hit enter still says stopped i don't know all right i'm back so you know what i thought i was doing everything right and so just as a sanity check what i did was i opened up my command prompt and i installed a just a different version of azcopy it's the windows version and using the exact same command it worked so you know i just want you to know like this is kind of something i run into a lot with azure whereas if you try it one way it doesn't work but some other place it works no problem just because azure tries to support all use cases sometimes you'll be spending a lot of time uh second guessing yourself but definitely the command i showed you was correct um but uh you know it could be that the um daisy copy version is out of date so remember when we wrote an az copy here for linux it was suggesting to get a newer version but i had downloaded the latest version so you know it could be that um it's really hard to say uh but i want you to know like that's just how you do an easy copy um and you know don't get frustrated if you get stuck on stuff like this okay so we're all done here and let's go ahead and tear down our stuff and uh we'll just go to fajo here that's the one we set up there and we'll go ahead and delete that resource group and we're all done here okay hey this is andrew brown and this follow along we're going to learn how to work with azure's sdk so we're going to work with a variety of different languages and we're going to interact um i suppose with a storage account so what i want you to do is make your way over to your portal here and type in storage accounts across the top and we will create ourselves a new repository here or sorry a storage account and we'll call this one my sdk playground and we will say ok and i'll just type the same thing by sdk playground you might have to put some numbers on the end there because these are fully qualified domains yes east us is totally fine standard is fine we'll go ahead ahead and hit review and create so we'll give it a moment there then we'll go hit create and we just have to wait about 30 seconds for this to finish while this is going what we can do is make our way over to github because we're going to need a repository to play around in so i'm going to create a new repository and we will choose whoops i don't want to template but i want to go to my own and this will be my sdk azure playground putting the word azure in there so i know what this is later on we'll say add readme we're not going to choose a git ignore file because we'll have to vary it based on a couple different folders and you should have get pod installed or if you don't you can use your local machine it's up to you but it's a lot easier to use git pod here it's free free to use so no reason not to to use getpod all you're doing is attaching this in the front of your url if you want the button you can go get the chrome extension i'm going to go ahead and hit get pod and spin up that environment going back to our resource or storage account you can see it's deployed so we'll go to that resource and what we'll need here is a single container so that is something that we will grab here right away and then um so let's just say container one i'll make it private i'll just make it one with no hyphen because i think that's how i wrote the instructions and so in here we will need to upload some kind of file so i'm just going to upload a data pipe and data stair upload any files you like but you'll have to change the names accordingly what we'll need to do is make our way over to access controls i'm actually going to do this at the storage level so we'll go back up a layer here go back to that resource we'll go to access keys and we are going to need our connection string oh sorry shared access signatures well we'll figure it out here in a moment because we do need to i thought it was under access keys that it shows um oh it's right here connection string okay so it is right here we'll be in good shape so what we'll do is go back to our git pod and we're gonna make uh three folders we're gonna have ruby we're gonna have python and we're going to have uh javascript so we'll just say yeah js for javascript and in each of those we're going to create a new file so so say gem file uh this will be the actually i want to generate that out so we'll uh cd into that directory and i'll say bundle um bundle init so that will create a way for us to manage packages in ruby for uh for python we'll create a new one called um uh requirements.txt and then for uh javascript this will be npm init and so these are the three ways well i should have done hyphen y on this here these will be the three ways that we bring in our external libraries of the azure sdk into each of these programming environments so first for ruby we'll go through that one so for this one we need to install a gem called azure storage storage blob so that will be for that one for python it is called i'm just trying to find the name here it is azure storage blob and we can give it a version here we could also take the version out it probably should still work without the version there and then for our package.json we'll just install that there as well so i'm just going to cd into um python because it's easier to install python packet or sorry yeah not python packages but javascript packages from the cli here so i'm just looking for what it's called so it's mpm install at sign azure storage blob all right so this one's installed we go to package.json we can see it's required uh the gem file in order to install in ruby we have to do bundle install and the reason i'm doing them all in parallels i'm trying to show you there's a pattern right you're installing the package you're basically doing the same thing over and over it's just that the syntax is slightly different for python is going to be python or sorry pip pip install hyphen r requirements and i generally remember all these things you know what i mean like i might not remember all the details but i know that the patterns are the same and that makes it a lot easier and we call this cloud programming because you don't need to know everything in order to do this so the first one that we're going to set up is the ruby file so we'll make a new file here and we'll just say main.rb which is an okay name here and we'll get coding with this one first so what we'll need to do is require and we'll do azure storage blob and here we want the account name and we want to have the account key so we'll go back over to here and i'm going to go ahead and grab the first key so we'll say show and i don't want to put an action here i want to put it down in my environment variable so i'm going to say export account key here equals and then i'm just going to do double quotations and paste it in and hit enter and i'm going to do gp that's for git pod environments account key i'm just setting it twice so that if we have to restart this environment that these keys will persist okay so um what we'll do here is just put in um env parentheses and this will be account key so that'd be the way we um import it into ruby because that's how you do it well this one's account key this is account name so let's say account name and then here will be account key and if we were to write puts account name puts account key that should print those out so we can see if they work so we'll do bundle exect ruby main so this is a pretty common way and i'm doing in the wrong folder we got to make sure we're in the ruby folder before we do that so this is a common way in ruby to run things in the context of the gem file you'll see that's kind of a bit of a trend for all these different uh programming environments so we'll go ahead and hit enter and it should just print out the keys it printed out the first one but not the second one we didn't set the count a name yet so this is the account name for it so we'll do export account name the reason we're doing export is that it actually loads the environment variable so if we go here and say grep account you can see it's actually loaded into our developer environment and then getpod environment is going to persist if we restart this environment so there's that one and then we want to do it for get pod there we go so they're both set if i just double check here and we do this they both show up so that is good and so the next thing we want to do is make a client so we want to establish a connection so i'm going to do blob client equals azure storage blob blob blob service dot create if you're wondering like where do you find these things you just like search on google you say azure azure blob storage ruby like and maybe documentation and you know through there you can kind of find information there's probably like a documentation page if we go here um i know there's one maybe here and then you scroll down blobs so there's all sorts of way yeah this is pretty similar code here so there's all sorts of way to find it you just have to kind of search around and eventually you acquire that knowledge so we need the storage account name and then we need the storage access key so this will be the account name and this will be the account key and then down below we'll just do blob client we'll make sure that works so we'll go back and oops hit up on the bundled exec here and so it prints out some kind of object so we know that is now uh at least the the client's being created so now let's go ahead and actually print something out so we'll go blob client list containers parentheses each do container and so just kind of doing iteration here and just do container name and if we hit up it will print out container one so that container now does exist let's say we wanted to go create a container that's something we can do as well from here so we'll say blob client create container i'll just say sum container name here and we'll do it again and if we go over here and take a look at our containers we should see a second container called some container names that one is working fine let's say we want to print out the content so we'll do blob client list containers actually we already did this up here i'm not going to print it out twice we'll just do it in the interior here might as well just print out the contents of all the blobs we're going to see what is actually in the containers say container name dot each do blob and and then we'll do puts do interpolation blob name then we hit up uh list blobs probably that's probably what it's supposed to be here you can see it's printing out the names here so that's working out fine uh it had a http error i don't know if it actually matters we'll do it again oh you know what it's because we already created this one so we'll just comment this out that was just like a one off there but you can see so it's not gonna create the same folder that's printing the contents of container one there's nothing in container two or some container so that is pretty straightforward there let's go take a look at python so we'll make a new file here we'll call it main.pi normally like if you want to isolate your libraries you create a python environment i didn't do that um i don't really always remember how to do it because i don't work in python that often but i know it exists but that's the idea behind us doing the bundle exec or if we do mpm start which you'll see here is that it's in the context of our isolated libraries that we want to use uh but anyway we'll get started with python here so very similar process we're gonna do azure storage blob base blob service import base blob service a little bit more verbose i'm not a big fan of um i'm not a big fan of python i need to figure out how to get environment variables i'll be back in a second okay so to get environment variables is just import os and then from here we can get uh os and viron squares all right we could just do get i suppose okay it's probably nicer and we can do account name here see the process a little bit different account key instead of puts it will be print so account name account key starting to look familiar right just slightly different and we'll have to be in the right folder here so we'll make our way over to python this one's going to be python main we don't have to uh you know do it in the context stuff because it's already there no module name azure storage blob blob service line i don't know if we did an install i thought we did so we'll do it again requirements.txt i mean it's totally possible i typed something wrong i'm just double checking azure storage blob base blob service looks correct to me we'll try this again i'm just going to do a quick search here just to see if i typed it wrong just in case it's a versioning issue because we did have a version in there 2.1.0 i just don't want this to mess up so i'm going to go here and go to 2.1.0 and we'll try this again because you never know if these requirements ever change i think it wants double equals yeah it does and we'll give this another go okay so now it's working so the versioning does matter we're not on the latest version but honestly it doesn't really matter because you know it doesn't so now that we have those two printing out we can go ahead and make our client or i guess service in this case the wording's a little bit different each one i don't know why but it's just how they do it so here we will have account name equals account name account key equals account key and we will print out the blob service as similar to before we will go up oh sorry i wanted to do uh python main here and so that prints out no problem now we'll need to iterate through this stuff so we'll say containers equals blob service list containers parentheses for container in containers and this is indent based so there's no like closing statement we just double like this container.name i wasn't trying to run it i think it just ran there below by accident just type exit down below here we will hit um go back to thinks i'm in a shell i'm not in a shell okay so we'll say python main.rb again oh i'm back in the main folder so we'll say python main.rb and it says can't open file or directory python i do oh it's right there sorry pi and uh now it's saying these don't exist so let me just do i think because we entered an environment i don't know what happened here but something happened where see ran this and it put us into the python environment shell and that wasn't something i intended to do so what i'm going to do oh i'm going to have to update our git pod yaml file here i'm just going to add a or ignore i'm just going to commit the stuff to reset the environment because i really don't want to be in a a pi environment like that it was just kind of a mistake and hit add dot get ignore come on let me add this darn file i'm just trying to commit this single file here get add.get ignore so then when we go over here it's a bit more manageable so i'm just going to commit all the code here um ruby and some python and what i'll do is i'll just close this environment and then reopen it and those environment variables should still be set because we did the gitpod enamel or getpod environment so i'll just give it a moment here to respin up the environment all right so our environment is back up so what i'm going to do is make our way back into the python directory and we'll do a pip install our requirements and so hopefully this time we don't get that weird problem we had before here i really don't have to restart notice it's printing the container name so pretty similar to what we did before and so what we'll need to do now is just print out the actual objects like we did with the ruby script so we'll go back here to the main scroll on down and we'll do blobs equals blob service list blobs parentheses container name for blob in blobs we'll say print f i think it's just single quotations i always want to do uh doubles there but it's a single and we'll do um curly's like that blob name so you saw this in the ruby one but it was like it said it was a pound and double quotation so this is called interpolations where you inject a variable into a string each language does it a little bit different so let's take a look here at this point one prints out the object same thing so very similar let's go take a look at javascript so that is our next one here so we'll go over here we'll make a new file we'll call it main.js we will need to update our package.json so we have a way of calling the script so we'll just call start here and we'll just say nodemain.js and then we will go up here and we will load the stuff in so we'll say uh constant curlies blob service client storage shared key credentials credential you got to spell it right or it will mess up for sure equals require at sign double single quotations azure storage blob and we'll do constant account name constant account key and there will be this one and then there will be this one so um how do we load environment variables in javascript i'll be back in two seconds actually i'm surprised i don't remember because i did it in so many other followings it's actually just process it's a problem when you move from a lot of languages you have to kind of look things up multiple times but it's okay because as long as you know what you need to look up env dot account key so those are our two that we're going to load here we need to create a credential key so we'll say key credential equals new storage shared access credential parentheses account name account key and then down below we'll create a blob client equals new blob service client it's interesting like how they're all slightly named difference like in python it's just uh service ruby's just client and then in in javascript it is service client so we'll do a url so this is when i always say like things are fully qualified domain name that's that's what i mean is like you can literally call us services like storage based on a fully qualified domain so we'll do blob core windows.net and we'll go down below here and say key credential here and so we'll do console log blob client i don't know if we'll get anything intelligible out of this but we'll give it a go i think like as long as we don't get an error it should be fine because i don't know if it would return anything in javascript we'll find out here in a second so we'll make our way over to node.js or javascript we'll do an npm install npm install and we must have a syntax error here there's a comma missing here oops i didn't want to build lines it's just being silly we'll go back here to npm install so it's we have to do it because i restarted the environment because those did not persist and what we'll do is do npm start oh we are getting data so there are no errors which is good and we're getting a object back so this looks good these errors are all errors we'll go back to our main js and we will do what we've been doing which is iterating through stuff so the first thing we'll need to do is create a new function we're going to call this async function container inventory i really really don't like having to do asynchronous stuff when i don't have to but that's like a common feature i mean other languages have it but that's something we do often in node.js or javascript so we'll say 408 parentheses constant container in containers curlies console.log container name container dot name and then we will call this function so hopefully this works npm start we we don't need this anymore here it's gonna get our way so i'm gonna take it out uh it doesn't like the end it's supposed to be of if it's not in change of we'll iterate through here and we get our two containers now we need to go get the blobs within it so in here what we'll do i'll just think about this for a second we need to do constant container client equals blob client get container client parentheses container name and we'll say let blobs equals container client list blob flat parentheses container name we'll say 408 const blob of blobs console log parenthesis back ticks is it back ticks yeah it's back ticks dollar sign curlies again this is how you know each one has different interpolation right so like python it was single string you had to put this f in the front to do interpolation ruby was just double quotations with a pound curly so they're all just slightly different and hopefully this works no uh container is not defined so what line is it complaining on it's complaining on the list blob flat right here so i must have spelt it wrong we'll go ahead and paste that in there it didn't paste right we'll just retype it container try it again there we go and so that's all three of them so that's all i really wanted you to do just go through the motions of using the sdk with some language and just to see that there are some patterns to languages so it's not as super hard as you imagine we'll close that it doesn't really matter if we leave it around or we don't delete it but for the resource group here with the storage account we will go ahead and delete i'll go in here we'll go into my sdk play oh and we'll click on the resource group here lots of ways to get to the resource group we'll go ahead and delete our resource group and i'll also delete the storage account so there you go hey this is andrew brown from exam pro and this fall along we are going to work with blob storage object metadata and specifically with powershell in order to set it and view that it's working so what we're going to need is um an environment in order to run powershell so we're going to do it in cloud shell we'll expand that and you'll have to be on powershell mode if you this is the first time you're running cloud shell ever you might have to accept for a storage account so it might say like create a storage account for this you just say yes notice here that it's running uh install module for you for azure so you know if you're doing this on your local machine and you can do this if you have windows installed so the idea is that you would open up powershell so i type in powershell all right click and run administrator remote if you're on a mac you can install powershell on a mac but to be honest this is so much easier if we just use cloud shell but i just want to show you a couple things so if you were to install um azure you do install module hyphen name az and then this thing takes forever like i swear to you it takes forever and that's why i don't really want to do it this way just bump up the font here to 24. and so we just say yes to all and we'll let that install in the background but i just want you to know that that's how you install all the add azure command commandlets because it's very useful to know and we do it a couple times i actually do do it in one and we have to wait and it takes forever but while that's installing the background we're gonna assume that we did connect a z and install module and now we are just getting some stuff so it's a good habit to whenever you're using type of clear hair whenever using a powershell with azure to make sure that you are on the correct subscription um because it can be in the wrong place so even though i know this is the one we're on i'm just going to set it again so we get the habit of doing it so it's set az context here and we'll do subscription and we'll do azure sub description one and so that will make sure that that is explicitly set now we want to get access to the resource group and of course we could easily do this through the console but you know i wanted to learn some powershell so i'm going to make a new tab here and what we'll do is make our way over to storage accounts because we're going to need a new storage account for this we'll go ahead and hit create and we will make a new resource group this one's going to be called my blob metadata and we'll say blob metadata 88 whatever it doesn't matter as long as it's not used we'll hit review create and we will create it here and we'll give it a moment here so we can go to the resource group so there are a few things we will have to type in that thing will be done in a moment so we'll need um just type clear here it always messes up i'm going to expand this because we really don't need anything in the background so we want dollar sign resource group equals and then whatever we called it so my metadata blob i think or my blob metadata my blob metadata and we'll hit enter whoops that is not what i want i want an a on the end there and then we need the storage account name so here it will be called go to the resource that is the name right there all we're doing is setting variables here right now and it's not copying properly because you have to right click you can't use hotkeys there why i don't know uh we'll do storage key equals and the idea is we are going to fetch our storage keys we'll do a z storage account key resource group name dollar signed resource group and then name dollar sign storage account i just noticed that i spelled that wrong i'm just going to hit enter because i know that's not going to work i just want to take this c and make it smaller go back here and do storage account hit enter and so that will create that then we need our context and we'll say new a z storage context storage account name dollar sign storage account storage account key dollar sign storage key and it doesn't like something because we need to actually get the value because if we type in storage here and by the way i didn't type this right again storage key but if we do storage key and just see what was returned notice that there's two keys so we'd have to do something like zero dot value here so we'll go here and say square zero dot value and it says cannot provide parameter for name um oh you know what that doesn't make sense it's not it's not name sorry it's because i'm going back to the wrong one here so this is this one and we really wanted the context just have to be careful what we're doing here there we go so now we have a context so it's just telling us what we're kind of bound to and so now we can do get a z storage container hyphen contacts dollar sign context if you're wondering like where do you get all these like all of these um commands like once you find one you find them all pretty much see here there's like tons tons of them you can read through them and they have tons of examples so you can work your way through and figure it out uh so i was expecting i guess there are no containers so that kind of makes sense so we'll go over to here and we'll say container one keep it all lower case to make our lives easier and we'll go back over here and we will hit up again you can see there's a container i think we can type in container and then do container one to specify even greater context there except i just have to put a capital c on here maybe oh lower case on this one i think all these parameters don't matter they can all be lowercase so those don't matter so we now have a reference to a container so now what we need to do is um reference a blob so what i'm going to do is type in blob and i think we'll have to upload something first but i'm just going to type something for fun here uh and so i'd probably do like an image i have an image on our desktop that i've been using for a while so data pipe web p is probably what it is and we'll do container it isn't there yet but we'll see what happens if we do it and there's no image or uh thing uploaded we'll do dollar sign context and so it says could not recognize that command this happens a lot but it always has to be like something like a verb or an action and then whatever so it's just saying the file's not there we know that's the case we'll go into our container and we'll say upload we will then go and upload data pipe it's actually a png which is a lot easier on us we'll go back over here again you'll have to upload your own file it'll have its own name so you got to go find an image yourself we'll hit enter and so if we now type in blob we have a reference to that blob so let's get some information about the blob so we'll say blob dot blob client get properties dot value and so we have a lot of information about the blob notice that there is no metadata right here um i think we can just get the metadata if we type in metadata i don't think there is any metadata so it's not returning anything i'm just going to make sure i'm typing it right i'll type something i know is there like etag yeah so it's just there's nothing set so what we'll do is make some metadata so we'll do metadata equals new object system dot collections.generic.dictionary double quotation squares string string so we are creating a new object that expects a dictionary with a string string hopefully that makes sense it's because we have to add a key and a value right because that's how tags mostly work even for metadata so we'll go here and say author exam pro and we'll say dollar sign metadata dot add department i t and we'll just say blob dot blob clients dot set metadata and then we'll actually set the metadata so let's say metadata dollar sign null i don't know why the second one is null i don't really care if we wanted to look it up we could doesn't really matter but it does set the metadata because if we go back to our blob do this now we have metadata on it just to make sure that it is working correctly we're going to just null it out so if we go blob so and now we'll fetch it again just so that we know remotely it's not like a local thing we'll go here and so there you go so that's all i really wanted to show you and hopefully you get a little bit more powershell experience there what we'll do is go to the resource group and clean up here we will go into this one we will delete it give it a moment and there you go hey this is andrew brown from exam pro and this fall along we are going to copy a blob from one container to another using powershell so what i want you to do is go to the top here and we're going to type in storage accounts and we're going to create ourselves a new storage account and the storage account is going to be called well the resource group will be called my uh blob copy powershell ps4 powershell and we will say blob ps okay a bunch of numbers because the naming is always a pain because these are fully qualified domains review create and wait a moment so you can go ahead and create we'll hit create and we'll wait a little bit here it only takes a few seconds but while it's going we might as well open up our cloud shell environment if this is the first time you've ever opened up cloud shell you will have to um you will have to accept it for storage accounts and by the way off screen i was doing this in another follow along i never came back to it but like if you want to have azure installed you have to install module uh name azure the cloud gallery does this but this takes forever and then once it's installed what you would do is you do a connect and that connect command would be i'm just trying to find it here i think i would know it off the top of my head i do not it's like connect a z account and then what it would do is it would prompt you to log in with microsoft it's not going to work here on my local machine because i don't have something authenticated and so i just don't want to bother with it but anyway we're going to do it here because it's so much easier over here and so we'll go back over to our oops that's from an old follow along we'll go here and open this i guess open it here it's fine we'll go ahead and create a new container so we'll say container one and then we'll go and create container two and we will i guess expand this for the most part and we'll get to it here so now that our containers are created we'll go ahead and make sure that we are in the correct subscription this is something you always should do just to make sure the right place so this is the subscription we're currently set i know it's already set correctly but just to go through the habits of setting it we're going to set the context for the subscription to be explicitly this one because that's something that will catch you off guard as being in the wrong subscription and so we need to set up some variables we'll have resource group equals and we'll have to find the name of it so we called this we'll go the overview here my blob copy ps let's click off here i don't know why it's doing this weird select sometimes it happens there we go so that is the name of that then we need the storage account name and so the storage account name is does it show me here no i can go up one layer go the resource this is the storage account name oh boy i always forget that you cannot uh do command v there you gotta paste manually and so we'll get the storage key here and of course we could just easily get it by going to the lefthand side here and clicking a couple buttons but again we want to practice our powershell so we'll do get a z storage account key minus or hyphen resource group name dollar sign resource group hyphen name storage account okay and if i've typed everything right we'll hit enter uh i don't think i typed everything right did i a parameter cannot be found that matches the name resource group so that doesn't look right to be here and if we do storage key we can see we have the key if we do zero we can get the first one if we do value we can get the value which is something we will have to do and we set the context here for the new azure storage context so we'll do new hyphen az storage context storage account storage account name dollar sign storage account storage account key storage key and oh we have to do zero dot value here and so now if we do context it should show us a proper context here there we go all right so now what we want to do is upload some kind of blob or actually we don't need to upload one we just need to store one so i'm actually going to use an image i keep using the same image in the in my lab i might use a text file instead but it's up to you what you want to use so in here i'm going to go over to the container where are your containers and we will go to the container here and we will click into it and i will upload a new file so we'll say data pipe png upload and now that that's been uploaded i'm going to type in get a z storage blob hyphen blob we call that data hyphen pipe because he's wearing he has a pipe in his mouth in the video or in the image and we'll say container one context dollar sign context and we'll hit enter it's because we've got the hyphen here and we spelt the blob name wrong there we go so it's returning that so that is good and the idea is we want to download it to our local storage so we can move it to the other one so for that we will still have the context but what we'll do is set a destination and say here oh you know it's get azure azure um storage blob content that's how we get the content not context content and uh we'll say yes because i've done this one before so it's already here if we do ls it's now in our cloud shell so the file is right here so you have like a test file and just remove that test file and so now what we need to do is upload to the other container so we'll do is do set storage blob content we'll say file data pipe hyphen container container 2 context dollar sign context and we'll give it some properties it's always good to set the content type whoops content type no hyphen i always try to put a hyphen there because i always think i'm doing um a header and we'll hit enter and so it says storage blob context is not right because we messed that up it should be content and i forgot the hyphen over here does not recognize set storage blob content set az storage blob content um content type is invalid i gotta spell content right could you tell i'm dyslexic i actually am dyslexic so i type a lot of stuff wrong like crazy and i still am able to do cloud az storage blob hyphen blob and this is where we upload it so we will do data pipeline here container container 2 context dollar sign context and so it says it's uploaded let's go verify if that is true so what we will do is go over to our storage here we will go to the resources we will go to the containers we will check container 2 to see if it's there and it's there so we are all done let's go clean up we'll go to our resource groups we'll look for the resource group we had created here this one is called what is it called i forgot what it was called that or it's just not showing up so what i'll do is i'll make my way over to storage accounts because i don't seem to be able to find it and we will go into where'd it go i'm so confused what's our resource group called refresh i'm going crazy because it was here a moment ago we'll go to resource groups it's right here again this is this is azure for you sometimes you think something's there but it's not there but it is there it just it's the fact that it has to propagate so it is in our account and so you have to have confidence about what is there we'll go ahead and delete and that will delete and there we go we are all done hey this is andrew brown from exam pro and we are looking at azure active directory and this is a cloudbased identity and access management service to manage users signins and access to adrelated resources so azure active directory is microsoft's cloudbased identity and access management service which helps you your employees sign in and access resources so that could be external resources like microsoft office 365 azure portal sas applications or internal resources so applications within your internal networking or access to workstations on premise and you can use azure ad to implement single signon so you can see that azure id is basically like the the one solution to log into everything and uh we actually use it at exam pro we use it with microsoft teams or uh you know for the exam pro platform our emit is tied to it so when we want to log into the mid panel with credentials we have it there we use it with aws to log into there and we use it to log into azure so it has a lot of flexibility and if you're building out applications for enterprises they're likely using a d and so this is the reason why everybody adopts it or needs to understand it so it's a service i really really do want you to understand and know azure active directory comes in four editions we have the free tier and by the way each uh tier that goes up has the features before it but free has mfa sso basic security usage reports and user management then you have the office 365 apps which is uh revolves around if you're using that suite so you have company branding sla two sync between on premise and cloud and then the premium tiers which really comes into enterprise or or onpremise hybrid architecture so hybrid architectures advanced group access conditional access premium two identity protection and identity governance only thing i don't like about azure ad is that you can't really create your custom access controls unless you have premium one in premium two but that's just how they do it so there you go so let's take a look at the use case for azure id and we basically covered it in the introduction but i just want to reiterate it in a different way with a bit of a visual uh so that it really helps it sink into your uh brain there so azure ad can authorize and authenticate to multiple sources so it can authenticate to your onpremise id to your web application allow users to log in with ipds so identity providers could be like use facebook or google login you can use it with office 365 or azure microsoft and so just a visual here notice that uh we have azure id and using azure ad connect we can connect to onpremise through uh app registrations we're able to connect our web application to azure id with external identities we can use facebook or google uh a login and then for cloud applications we can connect to office 365 or microsoft azure so active directory existed way before azure and so let's just do a quick uh rundown of the history so we have an idea of what we're looking at so microsoft introduced active directory domain services in windows 2000 to give organizations the ability to manage multiple onpremise infrastructure components and systems using a single identity per user so it's been around for 20 years and azure ad takes this approach to the next level by providing organizations with identity as a service so idaas solution for their apps across uh cloud and onpremise and both versions are still used today because they just have different utility and so we have active directory which is for on premise and then you have azure id which is just the cloud hosted version and in many regards these can also be connected together but there you go so i want to cover some active directory terminology and the honest truth is that for azure you're not going to be uh too worried about these things but they're going to come up in the documentation you're going to kind of wonder what they are and so i just wanted to uh tell you about these up front even though they're not core to study uh so that it just really rounds out your active directory knowledge because active directory is such a core service to azure microsoft products you you should know these things uh so the first thing is all about domain so a domain is an area of network organized by a single authentication database an active directory domain is a logical grouping of 80 objects on a network so just think of it a way as you know how you have resource groups to logically group your azure resources domains are a logical grouping for your 80 objects then you have a domain controller a domain controller is a server that authenticates user identities and authorizes their access to resources very common to have multiples of these because you want to have redundant uh domain controllers so you can log in or availability you're launching domain controller nearby so people can log into different places uh so definitely uh very core to active directory then you have the domain computer this is a computer that is registered with a central authentication database and a domain computer would be an id object so then you have ad objects so this is the basic element of active directory so you have users groups printers computers share folders etc then you have a group policy object a gpo this is a virtual collection of policy settings it controls what an ad object has access to you have your organizational units this is a subdivision within active directory into which you can place users groups computers and other organizational units so it's just another way of doing logical grouping then you have a directory service and this is this provides a method for storing directory data and making this data available to network users administrators a directory service runs on a domain controller so there you go that is the rundown of active directory terminology and again hopefully when you see it in the documentation you can refer back to this or you'll have a better understanding of all the components i would have loved to have made a diagram but i just couldn't find an example of one and so uh i mean i feel like there could be a really good picture for all this stuff so let's talk about the term tenant and a tenant represents an organization in an active directory and a tenant is dedicated to the azure ad service instance a tenant is automatically created when you sign up for either microsoft azure or microsoft intune or microsoft 365 and each or azure ad tenant is distinct and separate from other azure ad tenants and so if you uh if you were in um azure ad and you clicked on your tenant information that's that's basically what that is right so that's my exam pro one and it has its own special tenant id and we can see that it's licensed for office 365 and so that tells you that i'm using the office 365 uh tier of azure ad so remember that the domain controller is the server that users are going to be using to authenticate to the directory service and so when you create an active directory azure sets one up for you but there's some cases where you might want to set one up yourself and the reason why is that you could be like on like an enterprise where you already have your own active directory on premise but you've decided that you want to move it over to azure ad because you just want a fully managed active directory and uh you want to tap into the cloud but the thing is that some domain services those are features on your domain controller just might not be available and that's where you're going to need to set up your own domain controller and that's where azure active directory domain services come into play because these provide managed domain services and so they have managed domain services such as domain joins group policies uh ldaps uh kerb b ross never can say that properly ntlm authentication and so the great thing is here you can have these domain services but you're not going to have to deploy them manage them patch them they're just going to work so there you go so let's take a look at azure ad connect and this is a hybrid service that is used to connect your onpremise active directory to your azure account so azure ad connect allows for seamless single signon from your onpremise workstations to microsoft azure and azure ad connect has the following features we have password hash synchronization so this is a sign in method and it synchronizes the hash of a user on onpremise ad password with azure ad then you have password authentication this is another signin method that allows users to use the same password onpremise and in the cloud that's the one i like uh then moving on from password authentication we have federation integration so this is a hybrid environment using an onpremise adfs infrastructure for certificate renewal then you have synchronization this is responsible for creating users groups and other objects and ensures onpremise and cloud data matches so you have the same ad objects both onprem and in cloud and then you got health monitoring and this is a robust monitoring service and it provides a central location in the azure portal to view this activity and it's called azure 80 connect health so yeah the big takeaway is that azure ad connect is used to create a hybrid connection so now let's take a look at some of the 80 objects starting with users so users represent an identity for a person or employee in your domain and a user has login credentials and can use them to log into the azure portal so here i am a user and you can see it shows how many times i've logged in and i'm part of different ad groups and so you can assign roles and administrative roles to users you can add users to groups you can enforce authentication by like with mfa you can track user signins as you can see on the righthand side you can track device devices users log in and allow or deny devices you can assign microsoft licenses azure id has two kinds of users we have users that's a user that belongs to an organization and guest users this is a guest is a user that belongs from another organization and we'll cover uh azure 80 roles in the roles section here because that is what's that you're going to be using to apply to these users so groups in azure id lets resource owners assign a set of access permissions to all members of the group instead of having to provide the rights one by one and so on the right hand side here you can see i have a bunch of groups in exam pro and groups can contain owners and owners have permissions to add or remove members and then the members have rights to do things okay and so for assignment you can assign roles directly to a group you can assign applications directly to a group and to request join groups so the group owner can let users find their own groups to join instead of assigning them to them and the owner can set up the group to automatically accept all users that join or require approval this is really great when you just want people to do the work themselves as opposed to having to do all that manual labor of adding them to groups let's talk about how we're going to give users rights to access resources and there are four different ways to do that the first is direct assignment this is where the resource owner is going to directly assign the user to the resource then you have group assignment this is where the resource owner assigns a a group to the resource which automatically gives all group members access to the resource then you have rule base assignment this is resource owner this is where the resource owner creates a group and uses a rule to define which users are assigned to a specific resource and then you have external uh authority assignment this is this access comes from an external source such as an onpremise directory or sas application and i just want you know that there's four different ways to do it so to get access to resources let's talk about external identities so external identities in azure id allows people outside your organization to access your apps and resources while letting them sign in and use whatever identity they prefer so your partners distributor suppliers vendors or other guests can bring their own identities such as google or facebook you can share apps with external users that's for b2b stuff if you develop apps intended for azure ad tenants uh for singleton or multitenant you can do that as well uh you can develop white label apps for consumers and customers this would be like azure ad b2c so there you go hey this is andrew brown and welcome to the az104 follow along uh and the first video in our journey on learning on how to be a good administrator is learning about azure active directory tenants and so tenants is the way we are going to group all of our users within an organization so this is like the logical division of organizations within uh azure and so let's just make our way over to where tenants are and they're under azure active directory a.d we'll click there and so the idea is that when you first created your azure account you already have a single tenant and that is your organization so here i can have i see i have one here called the hush nook and down below i have a license applied to it so this is the azure 80 premium p2 but the idea is you can create multiple ones and so these can act as different organizations and really what a tenant is it's just an instance of azure active directory uh that's exactly what it is underneath but let's go ahead and go create a new one and we have two options here we can create uh uh an azure active directory and i think what really is implied here is that it's a b2b uh when you read the documentation they talk about b2c and b2b b2b means business to business meaning that you're joining businesses and other businesses together and then b2c means business to consumer and so up here in the description they say uh the b2c enables users uh to access applications published by your organization and share administration experiences all right so what we'll do is we'll go to the next step here and we're gonna need to name our organization um i'm gonna name it starfleet and we need our initial domain notice it's going to be on microsoft.com so we will just lower case that if it's already used it might complain we'll see if i can take that and it's already taken so we'll say um uh the uss starfleet uh oh it must be numeric take that out there there we go uh u and three s's that's not easy at all but that's okay i'm going to change my default location to canada i think it still deploys in the us anyway so it's still going to say united states where the data center is going to be we're going to go ahead and hit review create and give it a moment it's saying i'm missing some information there already used by another directory so i still can't call it that boy oh boy so i'll just say starfleet um uh 1984. and i think we should be able to do that and we'll go ahead and create it and so you're gonna have to play around and find the name that's gonna work for you and then we'll just wait for it to create this tenant shouldn't take too long here and after waiting there a couple of minutes down the right hand side it says it created the tenant so we can just proceed back here and so i'm just going to go back to azure active directory at the top i find it just easier to always search and go back this way and so we'll just wait a moment for this to load but we're still in our our hussnick azure active directory that's my default one there but let's say i wanted to go ahead and switch a tenant i can go now and just switch and so here i have my exam pro one hush nook and starfleet i'm going to go ahead and switch it over to this one and so essentially now i have all these isolate users so i can have different users in my in these other tenants and that's the way you would isolate them out notice that this one is azure ad free uh and so the thing with azure active directory is that the higher up the tier you go the more functionality you have and one that is very popular especially with enterprises is going to have the the p2 tier premium two i just want to show you how you'd go about upgrading to that it does cost money but you also get a free trial i don't believe that you are billed uh during the the time of the trial uh at the end of it then you have to enter in or make an explicit purchase of it let's go ahead here and see if i can do it um it might not give me trial because i'm already running another one but let's go find out so i clicked on licenses on the left hand side we'll go to all products and then we have try and buy and so on the right hand side it says azure 80 premium 2 and we can click on free trial and it's going to tell you why you'd want p2 such as multifactor authentication policy driven management and enduser selfservice and so i'm going to just go ahead and upgrade that you don't have to do that you can just watch me do that here and so i'll just hit activate it's just going to go activate that in my account and now it's activated so if i go back uh to azure active directory here i'll probably click home but i'm just going to go back this way it's a lot more safe that way eventually it should update here so it says azure 80 free but really i've upgraded it's just the delay in the console and so after a few refreshes it will probably show up but what that's going to do is it's going to enable some additional features they don't really matter so much for the course i mean maybe i could show dynamic role assignment if i try to remember how that works um let me think for a moment i think it's under rolls but anyway uh so you know that's how you do that um i just wanted to show you how to create a tenant and then just able to switch between them there so we create ourselves a tenant let's go ahead and actually create some users and just before we proceed forward to create a user i just want to point out you can tell what tenant you are in if you look in the top right corner so here i'm in my starfleet tenant and there's actually i think you can switch between here yeah that's an easier way to switch between your directories uh but generally that's not how i do it i'll go to the top here and just type in azure active directory and when i want to switch a tenant i just click the switch tenant button and click on what i want but we'll make our way back to the starfleet tenant that i have here here you can see my information and i've obviously activated azure 80 premium p2 but before we go ahead and create a user we're going to have to create a group and i'll show you why if i'm going to go ahead i'm just going to speed through this for a second if i scroll on down i have this option where i need to create a assign it to a group i guess i don't have to but the problem is i don't have one and so before i create my user let's go ahead and create ourselves a new user group and so we'll go ahead and create a new group here and we have two options we have security and microsoft 365. it explains right here the difference so 365 is really for giving access to mailbox calendar files sharepoint we're not doing that stuff here we're just sticking with regular security groups which is for azure stuff we'll name this developers and then we'll name this developers and notice here that uh the membership type i have a drop down if you're on the free tier this is going to be grayed out but uh this dynamic user is part of p2 and so this allows us to add a dynamic query and the idea here is that if i start having users and i just say if a user is from canada then they'll automatically be added to this group and so that's the query that gets outputted but we're not going to make a dynamic group today we'll just make our way back here to the new group and we'll just manually assign stuff all right i'll go ahead there and create that user our that group there and now that we have that group let's make our way over to users so we'll go back here i'm going to make myself a new user and on the top here we'll go here and we'll name this one kevin and we'll have that lower case and we will call them kevin uxbridge all right and you'll notice here it will auto generate a password it's four letters and four numbers i don't find this personally very secure but the idea here is that a user is going to reset that password right away so it's not a big deal and that's very easy to remember and now i can go here and assign groups and if i want to sign the role i can do that here so we'll open that up we have a bunch of different options maybe we want these two roles here and then we have some additional information i'll go ahead and create this user and so now this user exists what's interesting is that i can go ahead and delete users and then recover them the groups have this as well where if you go here and i actually have a user i deleted previously so it just takes some time to show up but if you go ahead you can hit the delete user here and then uh they will remain in here for uh 30 days and then they'll automatically be be deleted so if someone made a mistake this is a great opportunity to bring them back and so i can just go check box on richaun here and we can just bring her on back restore that user all right and again you know this is sometimes a bit delayed so we might have to hit refresh there she is and so uh you know that is the whole group and user stuff there okay now let's take a look at the concept of guest users so let's make our way back to users within our tenant and there's this button here for new guest users the idea between behind guest users is that we're able to invite other users from other tenants and this is a much simpler process than using federation federation is the concept of joining active directories together so if there is an onpremise or other external active directory they would somehow want to connect the two together and there's a lot of administrative burden with that but with if everybody's already using azure active directory it's as simple as adding a guest user so what we're going to do is go back to home here i'm just going to go back to azure active directory and we're going to switch tenants i'm going to go back to my hush nook one here and you probably won't see that that's just showing up because it wants me to use mfa and i'm going to go here to users and we're going to go ahead and create ourselves a new user i'm going to call this one a hush nook and we will name it simply that and let's make note of the actual uh email there that's pushnuk onmicrosoft.com and we'll go ahead and create that and now what i'm going to do is i'm just going to copy that email there okay i think i got it yes i do good and we're going to make our way back to azure active directory we're going to switch tenants back to starfleet and we are going to go to users and we're going to add a new guest user i'm going to invite a user and i'm just going to put in their email there and we will say hush and uh we'll just put that both and we you here we we can assign them to groups and roles it's all the same story here i'm just going to go ahead and invite them and so now they've been sent off an invitation notice that they show up under as guest all right so there you go let's take a look at bulk operations let's say we needed to bring in a bunch of different people into our actual account here and so we have this option of bulk create and if you look on the right hand side we can go ahead and download a csv template that's ready to use i'm going to double click that i already have excel installed here and off the bat what you can see is we have a bunch of options and that we can fill in and here we have one example there and so i think that we could try to give this a go here but we're going to have to make sure we use the same principle username or here thing here so i'm just going to grab this here and let's just make a few people so we need our principal here we'll just say picard whoops did not like that i copy that and we'll do up here we'll say data card data card picard data data just see what else i need we're going to need a password i'm just going to use the same one here set that as no i don't think any of the other things there are needed we'll go ahead and save that and i'm just going to save this somewhere convenient okay and what i'm just going to do is pull this up off screen here i'm just looking for where i saved it just give me a moment and there it is i'm just going to drag it on in here i think i can drag it in like that maybe not as easy just go back to uh it's under my downloads here we go and we'll just hit submit we'll give that a moment shouldn't take too long you can probably view the status here if we click that there and now we're just under bulk operation results i'm just going to give it a refresh you can see that it's not completed yet i can click into it and we'll just have to wait a little bit here okay oh it looks like it just succeeded i'll give it a nice refresh it's going to tell me how did it turn out to success zero failures total requests let's click into that so we can see what it did that looks pretty darn good to me so that is pretty good we'll make our way back to our users and you can see that they're on mass imported so there you go let's take a look at how we can enable multifactor authentication for our users and so multifactor stands for mfa you're probably used to it where it's a secondary step to confirm your identity before logging in either via a phone or a hardware device such as an ubi key and so there's this button here this is multifactor authentication and it could be disabled for you it very likely is if you're on the uh the free version of your azure ad just going back up here one level you can tell based on what that is here so i'm using azure 80 premium p2 that's definitely going to allow me to have mfa so if you just don't want to turn on p2 or it's trying to charge you money don't worry about it just watch me do it here you pretty much learn what you need to know so we'll just go ahead and click that button it's going to bring us to a different screen here and here we have our users and you can see multifactor authentication is turned off for all of them so we can go ahead here and turn it on for a single one if i click enable and so that enables it for kevin then we have some other additional options like manage user settings and here you can see we have some extra options so require selected users to provide contact methods again delete all existing app passwords generated by the selected users restore multifactor authentication on all remembered devices so just some additional things another thing you will want to check out here it's not very obvious but we have a service settings button up here and this will give us more additional options to enforce for our users so first we have app passwords so this allows users to create app passwords to sign into nonbrowser apps so things that are not part of the website here and so you can white list some ip addresses for them uh so that they are trusted from those locations then for verification options this is something that's important we have some options like call to phone so it will actually to do the mfa will actually call your phone and and you'll tell you the letters and numbers that you enter in or it can send you a text message or it can notify you through the mobile application so you can install the companion app from the android or ios store or the windows store i imagine there's a windows store too um and then there is the verification code and mobile app on our hardware token so hardware tokens could be with an ub key right then you can say remember a multifactor authentication on trust devices for x amount of days that way they don't have to enter it all the time because it might get annoying for some users honestly for me i want everyone to enter it every single time because that's just how i am but let's make our way back here and see how we can do some bulk assignment so what i'll do is i want to enable for a few people here so if i go up to bulk update i can download a sample file and that's going to go ahead and download that and you can see i actually did this previously here so i'll just copy it and what we'll do is just paste that in there so we're just pasting in yeah that's fine we're just pasting in uh their their name right and the mfa status you can always get the name from here just back on the list there and so once we have saved that file we can go ahead and upload that they're the same here so it's not a big deal and we'll just wait a few seconds here this is pretty darn quick usually doesn't take minutes sometimes it takes seconds but we'll just give it a moment here and there we go so that was a long wait i don't know why but i verified the two there and so if we go next now those will be now enabled there's also this option to enforce so let's go there uh after multifactor authentication is forced users will need to create app passwords to use nonbrowser applications such as outlook or links and so that's a great option to have so i'll just go ahead and enable that as well so now that's enforced and so that's all you really need to know about mfa okay so we're back in our tenant here and what i want to do now is actually set some abilities for users to reset their password so they don't have to come and bother the administrator all the time so under your tenant on the left hand side what you want to look for is password reset and now we have a bunch of options here for resetting password so we're going to have to go ahead and enable it we can enable it based on a per group level such as developers but what i'm going to do is set it for everybody here which i think is a lot better option and then down below we have a lot of things we can do here so looking at authentication methods we can choose the number of methods required to reset so maybe they need to use two options in order to reset their password which is a lot more secure and then we can checkbox on possible methods they can use then we could require a certain number of questions uh required here so three and three you can see i was setting something earlier here you can create your own custom questions and then so we'd go ahead here and just add a new one so um you know what is the best um you know song we'll go ahead and hit add and the idea is that you can add uh them there there's also a bunch of predefined ones so if you have a hard time think of any you can definitely checkbox on some here right and so now we have a large pool of questions that can be used there we'll go ahead and save that then under registration we can say require users to register when signing in i think that's a great option to have there and then you can have number of days before users are asked to reconfirm their authentication method and that's defaulted to 180 days so i would probably leave that as on and we can notify users on password reset notify all admins when other admins reset their password very useful this was probably turned off so i'd go ahead and turn that on there you can customize a help desk link so the idea here is that uh when someone's having trouble uh they might say contact your administrator well how do you contact administrator will you provide that information either a url or an email so i've just provided my email there there's some onpremise integrations never used this stuff before because i'm never working with onprem but you know that stuff is pretty straightforward as well and you have usage and insights so you can actually see when people are resetting their passwords and things like that and what is being used um so that seems pretty darn good but that's pretty much it to it um yeah so there you go hey this is andrew brown from exam pro and we're looking at the azure active directory cheat sheet this one's three pages long so get ready for it and let's jump into it so active directory just active directory alone which is a d it's microsoft's identity and access management service helps your employees sign in and access resources then you have azure ad and this is the same thing except it's the cloudbased version and one term that you'll hear a lot with this is identity as a service which just means that it's like the serverless or hosted version of this so you don't have to think about it or manage servers azure directory comes into four editions you got the free one the office 365 apps premium 1 which is also known as p1 premium 2 which is p2 it's good to know the differences between what features you have here so you should know all the details in between here so i highlighted conditional access because that's an important one on the exam to know azure ad can authorize and authenticate to multiple sources so if you're doing onprem you're going to be using azure ad connect if it's for a web app you're using app registers registration sorry if you're using facebook or google you're using external identities if you're you can also connect to office 65 or azure microsoft okay for active directory terminologies uh when we're talking about a domain domain is an area of network organized by a single authentication database an active directory domain is a logical grouping of 80 objects on a network then you have domain controller commonly abbreviated as dc domain controller is a server that authenticates user identities and authorizes their access to resources you have a domain computer a computer that is registered with a central authentication database a domain computer would be an ad object then you have 80 objects this is the base element of active directory uh things like users groups printers etc you have a gpo this is a virtual collection of policy settings then you have organizational units this is a subdivision of your ad which you can place users groups computers etc in you have directory service such as active directory domain service adds which provides a method of storing directory data and making the data available to network users a directory service can run on a domain controller that's only page one we're on to page two so a tenant represents an organization a tenant is dedicated uh it's a dedicated azure 80 service instance a tenant is automatically created when you first sign up either with microsoft azure intune or 365. each ad tenant is distinct and separate from other tenants when you perform a lift and shift of 80 to azure not all the features are supported in that case you're going to be using adds so adds provides managed domain services such as domain joins group policies lightweight directory access uh and i never can say that word so i'm not going to try azure ad connect has the following features it has password hash synchronization pass through authentication federation integration synchronization health monitoring you've got the concept of users so that's an ad object remember so an identity of a person or employee in your domain a user has login credentials and can use them to log into the azure portal azure ad has two kinds of users we got a user that belongs to your organization and guest users a user that belongs to a different organization groups let resource owners assign a set of access permissions to all the members of the group instead of having to provide the rights one by one and the group contains owners and it contains members just a moment here there we go just want to make sure i'm recording the videos that i don't end up finding halfway i didn't do it so we got assignments so you can assign roles directly to a group and you can assign applications directly to a group we're on to the last slide here in the cheat sheet told you this one was long request to join groups the group owner can let users find their own their own group to join instead of assigning them the owner can also set up set the group to automatically accept all the users or join to require approval there are four ways to assign resource access rights to your users direct assignment group assignment rule based assignment external authority assignment know all four of these please and so yeah there we go we're at the end here and that's all there is so let's take a look at the type of azure roles and i was honestly first confused with azure roles because i'm from aws and uh we only have one kind of role but azure happens to have three clients and there's definitely good reason for it so let's break that down and the first one is classic subscription administrative roles and this is the original role system now it's not something you're going to be really using but you should know about it because it still does exist then you have azure roles this is the authorization authorization system known as rolebased access controls or rbac and it's built on top of azure's resource manager and then thirdly we have azure active directory also known as azure ad roles and these azure id roles are used to manage uh azure 80 resources in a directory so let's jump into it so let's first talk about access controls or iam so im stands for identity access management it allows you to create and assign roles to users and so this is the general interface for im in azure and so we're going to have azure roles which is part of that uh rolebased access system and so roles restrict access to resource actions known as operations and there's two types of roles for the azure role based access system you have builtin role so this is managed managed by microsoft and they are readonly and they're precreated roles for you to use and then we have custom roles and these are a role created by you with custom logic and so the place you're going to find it there is you're going to see it says roles right there and that's what we're looking at down below is a bunch of roles so you see owner contributor reader those are predefined rules for you and so then you have role assignment and this is when you apply a role to either a service principal a user group a user and so that's going to be under uh the tab there such as you applying a role to somebody and then you have deny assignment so these block users from performing specific actions even if a role assignment grants them access and the only way to apply a deny assignment is through azure blueprint so this is just something where you're just setting up like guard rails to make sure that certain things are never used regardless of a role is applied so now let's take a look at classic administrators and this is the original roll system and you honestly want to be using the new rbac system whenever possible uh but the idea here is that um you're gonna go uh it's in the same place that we just saw the access control and there's a tab called classic administrator and in there you can set up uh administrators here and so you have three types of roles you have the account administrator this is the billing owner of the subscription and has no access to the azure portal you have the service administrator so this is the same access of a user assigned the owner role at the subscription scope and they have full access to azure portal and then you have the code administrator so this is the same access of a user who is assigned to the owner role the subscription scope so it's those very three simple roles and we actually have one because uh when baco who's the other andrew set up our azure account i think it just sets you up one so even though you're not really going to be using it i do believe that it still sets you up one when you make your account so that's just what it is but there you go so let's take a closer look here at azure rolebased access controls because this is something we're definitely going to be using a lot and so role based access control helps you manage who has access to azure resources what they can do with those resources and what areas they have have access to and the idea is that you have a user and you want to assign them a role so you can use a role assignment and a role assignment's made up of three elements you have the security principle the role definition and the scope and we're going to look at those three things in a little bit more detail here in a second and there are four fundamental azure roles which we are going to learn and then azure rbac also includes over 70 builtin roles which we definitely do not need to go into great detail so let's take a look at one of those three elements first which is the security principle and this represents the identity requesting access to an azure resource and when we say identity that's just like a loose term for something and that something could be a user in your azure active directory a group which defines a set of users in your azure active directory a service principle so a security identity used by applications or services to access specified azure resources or a managed identity an identity in your active azure active directory that is automatically managed by azure so service principle basically in azure service and then managed identity is something in your azure active directory then we'll move on to scope and a scope is just a set of resources that access that assess the role of the role assignment applies to and so scope access controls at the management subscription or resource group level so what does that mean and we we have another slide on this i can't remember what section it's in but you have this breakdown of scope where you have management groups subscriptions resource groups or resources so when you're saying i'm setting a scope you're saying what is the scope is it on a management group is it on a particular resource is it a resource group and that's what we're trying to say there and then last the last element there is a role definition and this is a collection of permissions so a rule definition lists operations that can be performed such as read write and delete and roles can be high level like owner specific or like a virtual machine reader and so azure has uh builtin roles and we said there were four fundamental builtin roles and here they are it's owner contributor reader and user admin administrator so you want to know those four and then across the board you have those three operations read grant and then create update delete so you can see the owner can do everything the contributor can both read and create stuff they just can't grant access to other people the reader just has read only access and then a user access administrator is granting other users privileges but themselves are not creating anything all right so now let's take a look at the third type of roles and that's azure ad roles and so azure 80 roles are used to manage azure ad resources in the directory such as creating or editing users assigning administrative roles to others resetting user passwords managing user licenses and managing domains and so on the right hand side here what we can do is go into azure active directory in azure and under roles administrators we can see there is a bunch of roles predefined for us like application administrator application developer etc there's a lot in there and a few important builtin roles that you should know is the global administrator so this gives you full access to everything the user administrator full access to create and manage users the billing administrator makes purchases manage subscriptions and support tickets and i want you to know that you can create your own custom roles but for whatever reason you have to uh purchase this this to me is a bit unusual because in the aws world uh this is something you don't have to pay for but i guess azure active directory has been around for a long time and so they have this whole big tier in there and so if you want to be able to make custom roles you have to upgrade your azure ad to premium so premium could be p1 or p2 and i'm assuming the higher the number the more controls okay so let's take a look at the anatomy of an azure role and as you saw in the previous slide it does cost money uh to be able to have the privilege to create custom roles you'd need an um an active directory premium version one or two but it's still a good practice to go take a look at what the contents of an actual role is because you can actually look at the manage rules open them up and and see what they do and you should really look at them and not just take them based on the uh face value of the name of the role so i want you to know that uh azure rolled documents have two different syntaxes whether using powershell or cli and so for the example here on the right hand side is gonna be using powershell and it's very simple and it's just the property name is going to change so see where it says name uh if you were using powershell i have in parentheses would be role name okay so if you look one up and it's using json and the names are different you're going to have to do that translation all right so let's just quickly go through the properties here so the first thing is the name that's going to describe what the role is then you have the id this is not something that you're creating it's going to be autogenerated for you then you have is custom whether it's going to be custom or not the description that's selfexplanatory and then the actions this is what you care about because the actions tell you exactly what you're allowed to perform uh so that's a big list there it's pretty selfexplanatory but then you have not actions and this is basically you explicitly saying you're not allowed to do these things it's just a guardrail to make things safe do you really need and not actions i don't know that's just how they designed it then you have uh data actions this is an array of strings that specifies data operations that are allowed to be performed within the data within that object and then you have not data actions so things you're not allowed to do with data and then assignable scopes and this is an array of strings that specify the scope for the custom role and you can only define one management group for assignable scopes for a custom role i do want to just point out one other things with these uh these roles and really just has to do with these asterisks just if you're not familiar with them but that usually indicates a wild card permission it's like saying match everything and so when you're doing that it's going to match things like you can use it in actions not actions data actions and not dated actions and so again it just matches everything in that sub thing because you might have a bunch of options you can put there and so for example uh what it's doing like let's see see where it says management or cost management and it has action read write delete run action all that stuff in red would be matched with that asterisk and i know it doesn't match the examples on the right hand side but you just gotta imagine that there's other things there okay now let's just do a quick comparison between azure policies and azure roles uh just to make sure that we're very clear what the difference of these two things are so azure policies these are used to ensure compliance of resources and azure roles these are used to control access to azure resources so on the policy side these are going to evaluate the state by examining properties on the resources that are represented in resource manager and properties of some resource providers they do not restrict actions and we call this operations ensures that resource state is compliant to your business rules without concern for who made change or who has permission to make the change even if an individual has access to perform an action if the result is a noncompliant resource azure policy still blocks the create or update actions there okay on the roll side it's very simple it focuses on managing user actions at different scopes and it does restrictions it does apply restrictions on azure resources okay so azure resource or azure roles controls what you have access to azure policies ensures compliancy all right just another comparison here i want to compare azure 80 roles to the azure roles the rbac versions so ad roles they are used to control access to a.d resources and azure roles control access to azure resources and here's a good representation here where you have 80 roles on the lefthand side and azure roles on the righthand side and the idea on the lefthand side that actually represents like office 365 but you can see azure ad has some coverage both inside and outside of azure so an ad resource could be something like users groups billing licensing application registration or etc and then on the righthand side for azure resources this could be virtual machines databases cloud storage cloud networking and etc by default azure roles and eight azure 80 roles do not span azure and 8 and azure id and by default the global administrator doesn't have access to azure resources so that's just good to know and global administrators can gain access to azure resources if granted the user access administrator role so the azure role so there you go hey this is andrew brown from exam pro and we are looking at the azure roles cheat sheet and when we say azure roles cheat sheet we're actually talking about all the roles not just the azure rules okay because there's three types you've got the classic subscription administrator roles you have azure roles which is also known as role based access controls that's built on top of arm and then you have azure active directory roles okay so yeah you need to know the difference between the three when we're talking about identity access management iam that allows you to create and assign azure roles to users roles restrict access to resource actions also known as operations there's two types of roles you got builtin roles managed managed microsoft roles are readonly precreated rules for you to use and then you have custom rules a role created by you with your own custom logic for rule assignment is when you apply a rule to a user and a role assignment is composed of a security principle role definition and scope azure's four builtin rules are owner contributor reader and user access administrator i don't have the table here but you should know the difference between them but usually they're pretty selfexplanatory based on their name for classic administrators we have three types of roles we've got account administrator service administrator and coadministrator for important azure 80 roles you need to remember you have global administrator user administrator billing administrator and on the exam you might even see more kinds there so i would encourage you to go look up all the different kinds or i might end up making a video going in more in depth for some of them in this course you can create custom azure 80 rolls but you're going to need that p1 or p2 so there you go that's azure rolls hey this is andrew brown from exam pro and we are taking a look at azure key fault so this helps you safeguard cryptographic keys and other secrets used by cloud apps and services azure key vault focuses on three things so the first is certificate management so easily provision manage and deploy public and private ssl certificates for use with azure and internal connected resources key management so create and control the encryption keys used to encrypt your data secrets management store and tightly control access to tokens passwords certificates api keys and other secrets and certificates contain key pairs key and secret not to be confused with key management and secrets management i do have to list that out here because you'll notice here and i'm getting my pen tool out we have certificates keys and secrets but within a certificate it can contain keys and secrets but just understand that there are three isolate offerings that um azure key vault has if this was another provider such as aws these would all be isolate services but uh you know a common pattern with azure is they like to group a bunch of uh functionality under a single uh service and so you know these things which which should be three services or actually under all azure key vault but there you go let's talk about hardware security models and fips because this is important for azure key vault so hardware security modules also known as hsm is a piece of hardware designed to store encryption keys here's an example of one called gemalto i'm not sure how to pronounce it but the idea is that these are pieces of hardware that are extremely expensive but are extremely specialized for holding in keys in memory not even writing them to disk so that there's no chance of somebody stealing those keys if they were to take the device and then there's the standard of fips also known as the federal information processing standard so this is a u.s and canadian government standard that specifies the security requirements for cryptographic modules that protect sensitive information and so hsms are they can be multitenant uh and if they are this is where uh we will be using fips 140 hyphen 2 level 2 compliant so you have multiple customers virtually isolated on a hsm device that's kind of a way of sharing the cost of one of these very expensive machines and then you have hsms that are single tenants so this is uh where you if you need to maybe meet level three compliancy where you have a single custom customer dedicated on a hsm so there you go let's take a look at what a vault is for azure key vault so a vault stores secrets and keys that can be protected either by software or by fips 142 level 2 validated hsm so an actual hardware security module azure key vault provides two types of containers vaults so this supports software and hsm backed keys and then you have hsm pools so only support hsm back keys and to activate your hsm you will need to provide a minimum of three rsa key pairs up to a maximum of 10. specify the minimum number of keys required to decrypt the security domain called a quorum and one key thing you know is you do not choose the container on creation you choose between standard and premium when you choose premium and create rsa key pairs you'll be able to begin to create hsn pools because when i was going through the follow alongs i was like hey where are these hsn pools and so that's where i found out about these requirements we're not going to do that because it's just really expensive to do and it's out of the scope of the exam but you know if you are an organization and you're thinking about using hsms those are some of the requirements there okay all right let us quickly talk about the key vault api so it's a rest api for pragmatically managing key vault resources allowing you to perform operations such as creating a key a key or secret important key or secret revoking a key or secret deleting a key or secret authorizing authorizing user or apps to access its keys or secrets monitor and manage the key usage so here's kind of a diagram and the idea is that uh via the api you'd use the cli to access the api uh it would go into your vault and you know if you had a certificate there a certificate could contain metadata key and secret and you could operate on uh that kind of stuff there so the key uh the key vault rest api supports three different types of authentication manage identity so identities managed by azure id this is the recommended practice because it requires it introduces the least amount of risk service principle and certificate so this is where you use the certificate service principle and secret where you have a user and secret key so there you go so when setting up our vault we have some very interesting recovery options and so i just want to give this a little bit more emphasis of these interesting things and so here it is soft delete days to retain deleted vaults and purge protection so soft deletes allows you to recover or permit delete a key vault and secrets for the duration of the retention period it's enabled by default on creation there is a way to not have soft defaults and i think we discovered that during the follow along i just can't remember this moment here not that it's super important mandatory retention period and so the mandatory retention period prevents the permanent deletion of key vaults or secrets prior to the retention period elapse so just keep them around just in case then you have purge protection so when enabled prevents secrets to be purged by users or microsoft so yeah there you go let us take a look at pricing for azure key vault so it comes in two pricing tiers standard and premium so the key difference here is premium allows for both software and hms protected keys where standard does not have hms protected keys uh so let's just take a look at the pricing in particular so for rsa uh 2048 that's the size of the key but keys uh you're looking at a cost of .03 cents for 10 000 transactions and that's the same thing for hmhs protected keys if you have an enabled with exception that you have a dollar per month per key for advanced key types so we're looking at rsa 3072 4096 so longer ones or ecc uh keys we're looking at a a greater cost here so um 15 cents per transactions and you're going to still pay that 50 protractions but in the hsf protected you're going to see that it's going to have this kind of tiered thing so for the first 250 keys it's five bucks for the next range of keys from 251 to a thousand five hundred it's two dollars and fifty cents and it gets lower and lower with volume some more keys the more savings you'll have some other things here would be things like secret operations and this is this is the same for both software and hsm so there's no difference in cost here but the idea here is that there's a cost for secret operations at zero three cents for certificate renewal it's three dollars per request and then we have managed azure storage account key rotation one dollar per renewal manage hsn pools is dollars and twenty cents this might vary in terms of price based on your region uh but you know i'm just kind of want to show you that there is an additional uh cost for just the keys when you have hsn protected okay all right we're taking a look at keys for azure key vault one of the three core offerings one being keys secrets and certificates keys we use it to encrypt things uh utilizing with azure services so like if you need to encrypt azure disks or apply it in a variety of things where we have encryption at rest um you know it's useful for this because it'll just easily integrate with those azure services so when creating the key there are three options we have generate so azure will generate the key import import an existing rsi key restore backup so restore a key from backup for keys that are generated by azure you can either use rsa or ec sometimes we see people say ecc but uh in the ui they called it ec rsa is an algorithm and it's actually in a combining of three names of cryptologists or uh the people that made the algorithm and you'll see there's a few different lengths 2048 3072 4096 that's the amount of bits that's how long it is then you have ecliptic curve cryptography and then it says p hyphen and we have 256 384 5 5 2 1 two 256k i'm gonna assume that also has to do with length um i don't know exactly how it translates over to bit size but uh i'm sure that's similar for keys generated by azure you can set an activation expiration date probably a good practice because that forces you to rotate your keys after a period of time you can create new versions of keys so that's a cool benefit you can download backups of keys backups can only be restored within the same as your subscription and an azure key vault i think we do that in the follow along uh so for premium vaults you do have additional options for hsm and it's not that fancy it's just like rsa and ec are repeated here but with hyphen hsm to specify it's going to the hardware security module you can generate you can import for the hsm there are microsoft managed keys also known as mmk these are keys managed by microsoft that do not appear in your vault and in most cases are used by the default for many azure services so even if you don't think you're using keys you actually are and so the way if you're using like aws you actually do see them in a key management service but azure decides not to show them and just have it by default there for you customer managed keys cmk are keys you create in azure key vault you need to select a key from a vault for various services so sometimes a customer manage managed means that the customer has imported cryptographic material uh uh but when we're talking about azure when you generate or import a key they consider that a cmk and i'm pointing that out because you know i use other clouds like aws and gcp and for aws cmk indicates imported cryptographic material not um not exactly how azure describes it here in order to use a key as azure service needs an identity within azure id so for permission to access the key from the vault um for infrastructure encryption this is sometimes an option by default uh azure encrypts storage accounts data at rest infrastructure encryption adds a second layer of encryption to your storage accounts data and we'll talk about double encryption because i was like what's double encryption and so then i had to make some slides here for us okay all right let's talk about double encryption so if we're talking about storage accounts there's this option called infrastructure encryption so if we checkbox that on uh this isn't encrypt storage account data at rest infrastructure encryption adds a second layer of encryption to your storage accounts data we just said that in the last uh lecture here but i'm just trying to cement that this is for storage accounts for azure disks it's called double encryption and so double encryption is where two or more independent layers of encryption are enabled to protect against compromises of one layer or of encryption it's the same thing but for one they called infrastructure encryption and for the other they call it double encryption confusing i know but it's the same thing uh okay using two layers encryption mitigates threats that come with encryption data of course so i just feel like we're repeating ourselves a hundred times here but microsoft has twolayered approach for data rest and transit so for data rest you have disk encryption using customer managed keys and then you have the infrastructure encryption using platform managed keys data in transit the transit encryption uses tls 1.2 that is the latest standard there's a over the years it went between ssl tls it's back in tls's court with 1.2 being the most uptodate additional layer of encryption provided at the infrastructure layer so should you enable double encryption yes and always every time you have the opportunity utilize it okay all right let's take a look here at azure key vault secrets so one of the three offerings that azure key vault has so secrets keys and certificates azure key vault provides secure storage of generic secrets such as passwords and database connection strings so here's an example of the form we have uh the name of the secret the value um so pretty clear how that works key vault apis return and accept or accept and return secret values as strings internally key vault stores and manages secrets as a sequence of octets so 8bit bytes with a maximum size of 25k bytes each yes 25k not 256. key vault services doesn't provide semantics for secrets accepts the data encrypts it stores it and returns a secret id or sorry a secret identifier so an id for highly sensitive data clients should consider an additional layer of protection for data encrypting data using a separate protection key provided to storage in key vault is one example encryption data using separate protection key so i guess you could encrypt the data and then upload the encrypted material uh there so i suppose that might be what they're suggesting key vault also supports a content type field for secrets so clients may specify the content type of a secret to assist in interpreting the secret data when it's retrieved a maximum length of this field is 285 characters all secrets in your key vault are stored encrypted key vaults encrypts secrets at rest with a hierarchy of encryption keys all keys in that hierarchy are protected by modules that are fips 140 hyphen 2 compliant the encryption leaf key of the key hierarchy is unique to each key vault encryption root key of the key a hierarchy is unique to the security world protection levels varies between regions and we're talking about china here so china uses fips 140 hyphen 2 level 1 and all the regions uses level two or higher i apologize for that spelling mistake there we can apply secret attributes alongside our secrets um and so we have exp that's the expiration time we have nbf not before so default value is now the time before which the secret data should be retrieved enabled whether the secret data can be retrieved uh and we saw some of these options in the actual form itself uh there are also other readonly attributes such for created and uh created an update updated i think it was supposed to say updated there in order uh to access secrets within your application code you would use the azure sdk as an example so here we're importing uh the key vault secrets this looks like um c sharp this is c sharp code sure oh yeah it says over here net okay and so we import azure identity uh key vault secrets we're defining our our options here so delay max delay retries um i don't think that really matters too much but the idea is that we are providing a uri to the actual vault and then we create our credentials we create our client and this example is just getting the secret so we're not um we're not using this to uh insert a secret but just to kind of grab it programmatically you can also use the cli so we have a z key vault secrets show and i'm pretty sure that's what we are using during the follow along but yeah there you go that's secrets all right we're taking a look here at x 509 certificate so before we can talk about those we need to talk about what is public key infrastructure also known as pki so pki is a set of roles policies hardware software and procedures needed to create manage distribute use store and revoke digital certificates and manage public key encryption so if there's a cloud service that manages certificates it's a pki so what is x 509 certificates it is a standard defined by the international telecommunication union itu for public key certificates or certifications so x509 certificates are used in many internet protocols extremely commonly used with ssl tls for https signed and encrypted emails code signing and document signing so a certificate contains an identity so hostname organization or individual a public key so rsa dsa ecda etc and then there's the idea of a certificate authority so this is an entity that issues digital certificates a ca or certificate authority acts as a trusted third party trusted both by the subject owner of the certificate and by the party uh relying upon the certificate so there you go all right let's talk about uh chain of trust for 509 certificates so a certificate authority ca can issue multiple certificates in the form of a tree structure also known as a chain of trust so here is an example for the exam pro dot co website um and uh you know this was issued by amazon and so amazon is the root uh the root authority here and then they have their intermediate certificate which issues us the certificate that is used for our https tls communication to keep encryption and transit to the website let's talk about what a root certificate or root certificate authority is so the root ca is a selfsigned certificate and its private key is used to sign other certificates so i'm going to talk about selfsigned just means that they've generated a key and they've done it on their machine there and that's what they mean by selfsigned because they're not reaching out to anybody else in order to say that the certificate is valid it's important that the private key of the roots are protected uh so like these are the most important keys and to protect them that's why we have this intermediate certificate authority also we like to call these icas so intermediate certificates are signed by the root private key and act as entities that can issue certificates they protect the root certificate because the root certificate does not have to sign every issued certificate then you have your end entry certificate these are the ones that we're going to be using so a certificate issued by the ica used by the end entity the entity in the case of an ssl certificate for a website so ssl tls it's just common to say ssl even though tls 1.2 is the common standard when we were making the videos here but there you go all right so we're taking a look at the certificate format so here is the certificate information for the exampro.co certificate that we used for https for the data in transit to make sure that our information is secure if you open it up you can see there's a bunch of metadata and so let's talk about what's in here so we have the version number the version number is uh you know indicating that it's an x509 standard notice here it says three so we're using version three we have a serial number a unique serial number assigned to the certificate by the certificate authority so it's this big long thing here we have the certificate algorithm id used to sign the certificate uh you know could be rsa dsa in this case is suggesting that it is rsa 250 uh with sha 50 shaw 256 with rsa the issuer so the name of the civic authority the issued uh issued the certificate to so that's the issuer name up here but this one is saying organization amazon things like that so it doesn't say my name might be down below here somewhere uh yeah i don't see my name in here but you know there's something here uh validity period so how long is this certificate going to be valid for the start and end date the subject uh yeah that's what we want the issue would be amazon because that's what is i guess i was looking for the subject so the subject is who it is to be issued to so that's where we'd want to see exam pro but it gets cut off here so we just can't see it the subject public key so the public key that is meant to be authenticated by the certificate so this field is also named also names the algorithm used for the public key generation then we have other fields like issue unique identifiers subject unique identifier extensions which allow you to associate private information to a certificate but the clear thing to understand is that all this metadata is publicly readable so anyone can generally view it that's uh the purpose of that is so that we can have the means to validate the authenticity of these certificates so let's just talk about some things that can package along with a certificate so that there are ways of validating it so we have this metadata and this metadata gets produced in a hash a hash is a means of taking a bunch of information and basically turning it into a thing that looks like a bunch of random characters but it actually is a fingerprint because that hash if you take the same information it'll always produce the exact same patch so it is a fingerprint to identify the information so you know that's that there the that hash or fingerprint is going to then get signed with the private key the private key that is generated by the certificate authority and that way it guarantees the authenticity of that fingerprint and then the idea is that when you are signing when you are signing that hash it's basically creating a signature okay so that's that authenticity when you sign it you know it's valid so the certificate metadata the signature and then the public key provided by the uh the end user is packaged there and with all that information we have enough information in order to validate the office authenticity of the certificate okay all right let's take a look at um the extension name for digital certificates because it can be crt cr pamper der it there's so many varieties it's very confusing so let's see if we can unpack some of the complicated history here and make it a little bit more clear so we have the first is pem so privacy enhanced mail and the idea of this format is that it's base64 ascii you can open them up and they're very easy to read very easy to edit and it's the most common format for x 509 certificates uh and for certificate signing requests and cryptographic keys so the pem format normally have the following extensions crt pamser and key key specifically for private keys but there's no hard rule about it so just because something is pam doesn't mean it's going to end with dot pem then you have distinguished encoding rule so dur this is a binary encoding so if you open up the file it's a bunch of binary code you won't be able to make sense of it edit it do anything with it uh it's used for both x509 certificates and private keys der normally have the following extensions so dercr then there are certificate files that's where we have sir and crt these are base64 ascii so they're basically i think they're basically pen and sir and crts are interchangeable extensions so you can either have they basically stand for certificate file right so that's what it's saying so it's like hey cer is a short form certificate file crt is a shortform certificate file right um so that's where they get confusing so either cer or crt these are generic file extensions then you have the personal information exchange pfx this is a microsoft certificate format uh this is the successor so pkcs pound 12 is the successor pfx and so pkcs hyphen 12 is uh we'll use either dot p12 or pfx extensions so there is a lot of confusion about the variety of formats it's just because there was a lack of standardizations early on and so everybody was just trying to create their own formats or their own keys and they weren't being consistent and that's why we just end up with all these weird naming conventions so half the time you cannot tell what something is you have to open the contents of it you have to know what you're using it for and just you know just be aware that it is confusing even for me i've been doing it for like what 20 years and i still can't tell what's what so that's just what it is okay hey this is andrew brown from exam pro and we are taking a look at certificate signing requests also known as csr is a message sent from an application to a registration so a certificate authority of a private key infrastructure in order to apply for a digital identity certificate the most common time you would deal with a certificate signing request is when you need a ssl tls certificate so that your website has https so it's data in transit so the idea is you'd submit a certificate signing request to your certificate authority so like if you're getting your certificate from godaddy amazon i'm just i'm thinking godaddy actually has certificates but like geotrust site or something you know uh you know you have to submit a csr so a csr contains a public key and application information such as a fully qualified domain name because it like if you're getting a a csr for a domain name they're going to need to know the domain name and so that's the two things that get packaged together so the idea is you create your certificate signing request you request the certificate to the certificate authority the certificate authority will issue you a certificate if it deems that uh it likes your request but there you go hey this is andrew brown from exam pro and we are taking a look at certificates in azure key vault so that's one of the three keys secrets and certificates and so if you're wondering why we did all that x 509 certificate stuff it's specifically so that we are prepared to understand uh you know the purpose of certificates in azure key vault so azure key vault allows you to import generate and manage x 509 certificates so the idea here is you can uh you know generate one you can selfsign one you can uh you use one with a um a certificate authority but let's talk about some other things here so key vault partners with certificate issue providers for tls and ssl certificates so we have digicert and globalsign those being two very large companies you can generate selfsigned or through a certificate authority there's no need to manage the security of the private key key vault takes care of it for you it allows the certificate owner to create a policy that directs key vault to to the manage to the managed life cycle of a certificate this allows certificate owners to provide contact info for notification about lifecycle events for expiration renewal or certificate it supports automatic renewal with selected issuers so key vault partners 5x5 x 509 certificate providers or certificate authorities so there you go all right let's talk about the composition of a certificate with an azure key vault and so we use this uh graphic earlier to describe uh the rest api integra uh interacting with just the certificate part of azure key vault but what it did show is the three components that are part of certificate that it will be stored in azure key vault so there we have um azure key vault which allows the operations of keys okay then we have key vault secrets so allows the retrieval of the certificate value as a secret certificate metadata so x509 certificate data so if that's not clear let's just get our pen tool out here so we have the key we have the secret and then we have the metadata okay so those are the three components that are alongside the certificate and so here we have an example of a certificate that i selfsigned i generated in there and i just want to show you that you can download the certificate here and either sir format cer or pempfx format and so you see where i'm like is it ascii is it binary what's it going to be because you know when i downloaded it i didn't really get the exact result that i thought i would get but remember that the pem can be used as cer so the format format offering can be confused here but what can help us understand is what we set the content type when we generate our certificate so um like cer will be ascii format i believe um but like when we're doing with pemb or pfx the difference is going to be based on the content type that you choose there so if you choose the content type to be pkcs pound 12 it will use that format if you use if you choose pem that means that it will download in the pem format same thing with the cer so just realize it is a bit confusing and there's that additional option there and um azure really really should have not made these two buttons that confusing it should just be clear what they are but that's just one of those things that uh azure trips you up on okay all right we're taking a look here at certificate policies so typical policies allow you to set and configure your certificate such as the content type the lifetime of the certification the key type the key size and various other options but it's calling it an issuance policy so there's some inconsistency of naming by azure between their documentation and their ui which is no surprise but we have a bunch of options as we saw so the type of the authority whether it's selfsigned and things like that the subject the validity period the content type whether it's enabled the percentage lifetime which i'm not sure what that is we have advanced options where we can say the key type the key size things like that issuance policies only effect certificates that will be issued in the future so it won't modify the existing certificate but the ones that will be generated in the future okay hey this is andrew brown from exam pro and this fall along we're going to be learning all about azure vault so let's get to it so what i want you to do is go in the top here and type in key vault and here we'll have to go ahead and create ourselves a new vault and so from there we're going to create a new resource group i'm going to call this resource group my example vault and then we will make a vault key here so i'll just say my vault example which is kind of funny because this one's slightly different so you've seen i've done this before so i'm going to do my example vault as the name here and for the region usc is fine for pricing we'll keep it at standard soft delete is enabled and then there's the option for purge protection so we are going to enable purge protection and uh this is going to play into other follow alongs we'll explain that as it goes but purge protection does not allow you to purge things uh easily once it's enabled so what we'll do is go ahead and review and create and we'll go ahead and go review create and we'll give it a moment here and we'll just wait till it's done deploying okay all right so after a short little wait our vault is created and so what i want you to go to the resource and we're going to be using this vault a little bit in some of the follow alongs and in some cases not so much okay hey this is andrew brown and this fall along we're going to be doing some things with keys with an azure key vault so what i want you to do is make your way over to the keys blade on the left hand side here we're going to generate or slash import a new key we're going to choose the generate option in terms of naming we're going to call this my disk key and we're going to choose rsa2048 that seems totally fine to me everything else seems okay so go ahead and create that key so give it a moment to create doesn't take too long and then what we're going to do is go on the left hand side to im access controls and what we want we're going to want to do is add a new role assignment so we can go ahead and start using this key so what i want you to do is go and look for key vault administrator which is here and we'll go ahead and hit next and then for our user we will choose ourselves so under user i'm going to select the members i'm looking for the account i'm using there i am and you're around go ahead and select that there and so that is all we need to assign it so that we can actually uh work with that key so i think a good idea is to use a key to encrypt a disk so what we'll do is make our way over to disk encryption sets because before you can encrypt a disk you need to have an encryption set so go ahead and create ourselves a new encryption set we'll call we'll use the uh sorry the same um a resource group so it's very easy clean up afterwards we'll call this my disk encrypt set here and in terms of the encryption type we're going to use double encryption because that's a much better you have two keys that encrypted so that's a lot better we are going to choose our vault so we have my example vault there's only one option here and in terms of the key we'll just select my disk key in terms of the version uh we'll select the current version we'll go ahead and hit review create and then we'll go and create that and we'll give it a moment to create that encryption set shouldn't take too long here and after a short little wait uh our resource should be deployed only took about a minute for me if we go here it's going to have this message up here it's very small but it says dissociated disk image snapshot this disk encryption set you must grant permissions to key vault so all we have to do is click that alert and we'll grant permissions and so now we are able uh to use that key um or like to to we're going to have the permissions issues is solved so what we'll do is go to type and create a new disk and so we can apply this key to that encryption so go ahead and create we're going to choose the same resource group here i'm going to call this my example vault and or sorry my example disk because that's a little bit more clear than that and for the availability zone doesn't matter for the source type it doesn't matter as well in terms of the size we want this to be cheap we're not really using this for real so we'll use standard hdd and we'll say okay in terms of encryption this is where things get fun we go to double encryption we choose our key here we'll go ahead review and create and we'll just give it a moment for that to oh we'll hit create and we'll have to wait a little while here for the create that resource so we'll just wait until that is created okay and after a very short while the disk is ready so we'll go to that resource we'll go to the encryption tab to see that encryption is applied so that's all it takes to use a key to encrypt a disk so we are going to still use some of these accounts there's no cleanup yet i'll go back here and i'll see you in the next one hey this is andrew brown and this follow along we're going to learn about backup and restore key so what i want you to do is go back into the resource group that we just recently created and we're going to make our way over to keys so i'm just sorry we got to get into the vault first then we'll go over to keys and the idea is that we have this key here and so you can see that we have this current version so you can add additional versions but what's going to happen if we try to back this up so when you back this up you're going to get this file here and if you open up this file it's going to look like a bunch of gobbly goop so i'm just going to try to open it here i have it up off screen here so i'm just trying to open it up within uh visual studio code so i'm just going to open up visual studio code again doing this off screen here just give me a moment all right and so this is the file that we encrypted and you take a look here and it's it doesn't look like anything but the idea is that it is our backup of our keys so that we can reimport that and just taking a look at the key name this is what it looks like so this is my example vault my disk key then there's this date and that's key backup so just recognize that's the format and the date is very useful to indicate when you backed it up so let's go ahead and delete this key because the idea is we want to restore that backup and so we have deleted that key there and uh what we're going to do is we're going to attempt a restore so i'm going to go ahead and go occurred while restoring the key the key you're trying to restore already exists why would it throw that error we've clearly deleted it and the reason why is that we have purge protection on we did that in the first first part when we set up this actual vault here i'm going to see if we can find the settings wherever that purge protection is i'm trying to remember where it is purge protection is enabled so we can go here and once you enable it you cannot turn it off it's going to retain it for a certain amount of days and so all you can do is soft delete keys so this key is not actually deleted yet if you go to manage deleted keys you can see the key is over here and if you try to click on purge it is disabled because we cannot remove the key because we have purge protection on but we can recover the key so we'll go ahead and recover and so that will allow us to recover the key and if we refresh here it's going to take a little bit time for that key to restore so we'll just have to wait a little bit and then it will show up there's one other thing i wanted to show you was under policies because you know um if you go under where's policies here um or access policies if you look under our user here and we look at the key permissions there is an option to purge and we don't actually have that turned on right now but if we were to save this and we were to still go to that purge option it would still say the same thing so even if you have purge permissions it does not matter if purge protection is turned on it still will not let you purge but you would need a combination of those in order to uh you know be able to do things there so to really show you how to do that recovery i think what we should do i'm just going to delete our old key here because we don't care about it but we are going to well i guess we could try to import it into the other one so i'm just going to undo that for a second but we are going to go ahead and create ourselves another vault so i'm going to go and type in vault at the top here and we're going to be a little bit more careful when we create this vault so we'll go here and we will choose my example vault i'm going to say my vault no protect and the pricing tier will be standard one day we're going to leave it well 7 is the lowest and we'll say disable purge protection because we don't want to have that enabled and we'll see if we can import the key into another vault i'm not sure if we can do that worst case we'll make a new key download the key reupload it but i'm just curious what would happen if we tried to upload the same key as it's still in another vault i'm not exactly sure all right so this deployment is successful i'm going to go to this resource and go ahead to go to create and we're going to restore from backup and we're going to take this key and see if we can actually import it here so it looks like we can take a key and it can exist in multiple vaults i'm going to go ahead and delete this key and we're going to say are you sure you want to delete this key i'm going to say yes and if we go to manage keys and we refresh it takes a little bit of time here so we'll just wait a moment for this to persist and after a short little wait like about two minutes i refresh and the key is here so if i go here you'll notice the purges option is still not available we can obviously recover um but we don't have purge um protection on so if we go to access policies over here and we go ahead and scroll down and select purge and save our changes we can then go back to keys we'll give it a moment to save we go back to keys we'll refresh it we'll manage our keys and we'll go ahead and purge it and that will permanently purge it there so that's all it takes to do that so there you go hey this is andrew brown from exam pro and this follow along we are going to learn about key rotation so what i want you to do is make it back to the vault that we were just in the no protect vault and we'll go over to keys and we'll have to create a new key so i'm going to create a new key here called my new key and it's going to be rsa 2048 and we're going to go ahead and create that key so the idea of key rotation is that if you have uh if you think your key has been compromised or you have a policy like a company policy that you should rotate them for security purposes after a while you can easily rotate out keys so in order to rotate a key it's as simple as going to your rotation policy here and we can say rotate now and that will immediately rotate the key as you'll see there is now the current version and the older version so that was the key we had before this is the new key we can also set up a rotation policy so if we go to rotation policy here we can set the expiry time to let's put the lowest number in here one so this has to be at least 28 days and then we want to enable enable it and then we want to automatically renew after a certain amount of time so here if i put seven i'm trying to put the lowest number so it goes one and i try seven it says the value cannot be greater than 21 days because there's kind of a minimum and maximum both having a buffer i don't exactly understand the math but i know that um if we change it to something like i'm thinking here expiry time 40 days and there's a larger gap this should be less of a problem so it cannot be greater than that 21. i i knew the math here a moment ago i could have swore it was 28 and 7. so just well i'm not sure that this is broken let's go ahead and save that try 40 up here and hit save value cannot be greater than 33 days okay 33 oh boy so let me all right so the problem was i was in months and we'll have to go to days here so here if we put seven it works fine but there is a bit of work to figure this out and it will tell you so you'll have to figure it out we'll go ahead and save that um oh yeah it prompts again so the uh the time before expiry cannot be greater than 21 days cannot be greater than um okay so we put the expiry to 40 we save so there is a bit of work there i don't exactly understand all the math there but i guess it's a way so that you have less problems but that's key rotation in a nutshell so there you go hey this is andrew brown from exam pro and this follow along we're going to learn about secrets so go back to your previous fault the no protect fault and we'll go to secrets here and we're going to go ahead and generate a new secret and we're going to use this secret within an application so we're going to have manual we'll say my secret and make sure you spell right so my secret secret there we go and for the value i'm going to say hello mars it's a secret value that's the whole point of secrets for the content type we're going to say text plain so it knows that it's text and everything else looks fine so go ahead and save that and we will easily create our cells a secret here so the secret is now available now we just need to use it in some kind of application so what i'm going to recommend is that we go use a git pod template so we're going to say gitpod template.net core cli c sharp csharp there we go and from here we'll click into it and uh what we want to do is use this template and we will have it private it's totally fine i'm going to go to exam pro here and i'm going to call it i don't know azure vault secrets dot net oh did i use that before so we'll do new if you don't have to do new that's great i'm going to go ahead and create this as a private repository from a template so it's going to set us up with a base core.net application i think.net 6.0 um it's just going to make things a lot easier for us so you can click the button down below actually probably not because i'll go to the wrong repo but um what you'll want to do here is uh if you don't have the chrome extension installed all it's doing is it's just putting this in front of it to launch this get pod environment so get pod is has a free tier so it's very useful to use especially for net because we do have to do a bunch of setup for it so i'm not really good at net so this is the approach i like to take um and the idea is that we just close this and it cleans itself up so it's really really nice so we'll just wait for the image to build and for the environment to run and then we'll get to the next step okay all right so after a couple minutes this environment is ready and i'm just going to go ahead and hit the x there go to terminal and we're going to make sure this dot net app works we're going to do net run and this is just a basic hello world application so we can see that it prints out so we're going to need a couple packages we're going to use net add package to add these here so azure identity and then we will do dotnet dot net add package azure security key vault secrets and the idea is we'll need to modify this file in order to load some stuff so i can't remember if i wrote this myself or if i got it from an azure azure documentation but we'll write it out because it's not too much to write out as using azure core using azure identity using azure security key vault secrets and then under the program we want to modify our static void main so we're going to need a new string and this is going to be called secret name so my secret actually we have a bunch here and i think this is where we probably should set environment variables so key vault name and i'm just going to actually i don't know how to set environment variables in.net i don't want to look it up so i'm just going to write it out and hard code it because it's not going to really matter if we do that so i'm going to say my vault no protect that's what we called it my vault no protect yeah and for uh the key value uri we're going to do this so we'll say https colon forward slash forward slash my vault no protect dot vault dot azure dot net i think we need semicolons on the end here otherwise it complains a bunch um he vault name var secret client options options equals new secret client options parentheses and this i think is a function so we'll open this up here like that and we will try we'll have to set this to retry don't worry if you don't know what's going on because it honestly it doesn't really matter it's it's just like you know you just need to get some experience writing some code and eventually it starts to sink in because it really takes a lot of time to learn code you just have to spend a lot of time doing it so that we're just going through the motions without having to think about it too hard so from second 16 and i'll explain it once we write it all up here but yeah it's very good practice to just write out code max retries is five and we'll say mode equals retry mode dot exponential um and i feel like we have to put semicolons in dot net otherwise it complains so that is that part and then we'll need a client so we'll say new secret clients parentheses new uri key v uri comma new default azure credential parentheses options semicolon uh key vault secret secret equals client get secret parentheses secret name semicolon and down here we'll do console console dot write line get secret colon get secret plus secret value uh value there i didn't mean to spell vault capital v on the value it's giving me some trouble here semicolon and then the idea is we're going to want to enter the secret yeah i'm pretty sure i took this from uh the azure docs i just don't remember where because i know that this looks very familiar and uh but i think i reorganized it so that it was a little bit more streamlined i think there was a little bit more going on with theirs and so we'll say string secret value lowercase secret value equals console read line parentheses semicolon client set secret secret name secret value and we can do console right set secret semicolon on there we'll do another console log here this will be for the key so let's give this a couple spaces and we'll do plus secret uh name i can spell this one right and we'll do secret value we'll do another console right we're doing lots of these console rights here actually it'll just be pretty much the same lineup here so i'll just go grab this one here client start delete secret secret name console right line start delete secret start delete secret plus key vault name and then we'll go get it one more time so let's take a look at this i mean hopefully we didn't introduce any mistakes by typing this all up by hand but uh the idea and these are supposed to have semicolons on the end because i can see a little red mark there so the idea is that we are importing uh core identity and key vault secrets identity so that we can get access to stuff we're providing our secret the name of our vault the url that it needs to hit to do that um here it's going to get the secret client options and so it's going to have a bit of retries here if it doesn't work out properly down below we have the secret client so we're passing in the key value uri and then we need our credentials here we didn't type anything out for credentials so i'm a little bit surprised by that but maybe that's not an issue and so what i'm going to do here is i'm just going to go ahead and commit all this actually before we do that we're going to create ourselves a get pod yaml file so git pod init and it says it won't already exist so i'm just going to say no there but what we need to do here is install the cli the azure cli so i'm going to make a new task here called azure we'll say init and then here it will be curl hyphen s capital l https colon slash aka at ms aka is like azure's microsoft's short um shortener so we'll say azure cli deb just have install azure cli deb sudo bash okay so that should install it i'm just going to double check to make sure this i typed this out correctly and we will say allow and paste that in there yeah it's working so what we'll do is we'll commit our stuff here add that there so commit code and i guess we do want to run this line so i'm going to go here and run it because the way it's going to get the credentials it's going to grab it once we authenticate with the azure cli because it'll set something locally and that's why we didn't have to pass in a key or anything and maybe before we do that let's just double check our permissions so if we go to the im control here we go to our role assignment or actually um access policies we'll go to angie brown secrets get lists so we should be able to do it we have all the permissions to do it so now that that is done we're going to go a z login hyphen hyphen use device code and hit enter and so here we have this address here that will help us log in so we'll go here to this address we'll copy this code we'll enter the code in we'll say oops that's not what i wanted to do we'll paste that in again and we will hit uh sign in here and we'll say continue and that is all done there so now what we can do is try to run the application so we're going to type in net build to build this application see if we get any errors if we get errors i'm not really surprised because we had done so much typing here but we will go back to our code and take a look here so on line 38 um there's no semicolon a very small mistake so i don't mind that if that's all the mistakes we wrote that'll be great fix semicolon missing semicolon and we will commit nsync dotnet builds and all projects are up to date so that looks pretty good to me we'll do run and if it works that'll be great i don't know if we explained the whole program but we'll just run this here and it should run oh you know what it's prompted us probably no there we go so we'll type in hello world uh hit enter great so let's take a look at what this did so the idea is that we have a client and the id and we want to grab the secret so that's the secret name we have up there my secret and if we console log it out it gives us hello mars because that's what we set and then we are prompted to enter our own input and we use this to set the secret and then we immediately fetch and say what is the key what is the value now if you notice it says my secret here hello world and but when we go get the secret again um down below let me just see here so we have and the reason why it's not printing out as expected is because we don't have this is something i'm not a huge fan of in net but you actually have to add the um backslash end like that otherwise it gets a bit confusing here as you can see i'm a little bit confused about our output so we have set secret key value my secret hello world that's new value and then it goes down below and we print out the secret value again from the actual value object so these are the ones that are just locally that we set right but this is the actual object but the object is still saying hello mars but we updated up here and the reason why that happened is because it takes time for those changes to propagate so if we were to call it again it would have the right value but you know it has to replicate it and so that takes a bit of time so sometimes you have to wait uh for the stuff so we go ahead and we delete it and then we go start delete secret so we say that we are deleting it and then we go get the secret but we've deleted the secret so how can we see the secret if we delete it and again it just has to do with propagation if we were to go into our vaults and to go to secrets that secret should not be there anymore so just consider that you have to do that i think the original tutorial had like sleep commands i don't know what this loop command is in um in net but it's something probably like sleep you know dot net sleep right so they were running those in between that this that's like two seconds a thousand milliseconds and so they're doing that to make sure the change is there but i thought it was better to show you the fact that you have to wait a little while um but that's it there so we are all done here so we can go over to our resource group and we can delete uh this resource script and all the resources in it all right and we can close that off so there you go hey this is andrew brown from exam pro and we're taking a look at azure app configuration this allows you to centralize all your app settings in one location is ideal for multienvironment or multigeographic geography applications because it provides a dynamic way to change application settings without having to restart them but in general if you're building out any kind of application it is actually really useful it also integrates with azure key vault which stores application secrets which we'll talk about soon enough azure app configurations main benefits are a fully managed service that can be set up in minutes i would say seconds to be honest flexible key representations and mappings tagging with labels point in time replay of settings dedicate ui for feature flag management which is to me the best part of azure app configuration comparison of two sets of configurations on custom defined dimensions enhanced security through azure managed identities encryption of sensitive information at rest and in transit native integration with popular frameworks and when we say popular frameworks we're talking about things like net core asp.net net java spring other javascript node.js python azure functions.net core so no rails no larvael so no next to so i'm not sure what they're saying when they say popular frameworks quotations but you know you can still integrate them because they do have language support but there you go all right let's take a look at the tiers for azure app configuration and we got to here we got free and standards let's see what the difference is so the first is resources per subscription one for free unlimited for standard for storage per resource 10 megabytes for free versus 10 or sorry one gigabyte for standard revision history seven days versus 30 days for request quotas a thousand versus 30 000 per hour no sla on the free tier first security functionality this stuff is important you get encryption with microsoft managed keys uh hmac and azure ad so active directory authentication rolebased access control support managed identity service tags so a lot of stuff in the free tier but in the paid tier you get customer managed managed keys of cmks uh private link support and then for the cost it's free for the free tier of course and for standard it's 120 per resource with uh 0.06 per 10 000 requests with 200 000 requests are included in a daily charge so you do get you might like look at those requests and say oh that stacks up but you get that 200 000 uh free uh daily um and for soft release is not supported in the free tier pricing okay so soft delete mean like i deleted something and i didn't actually want to delete it so let's bring it back it's like putting in the trash can and pulling it right back out but there you go all right let's take a look at configuration explorer this is where you're going to be looking at your data for the most part that you've inserted and so the nice thing is that you go here you can see the key and the value even though it's hidden if you click values it will show you them all which for me doesn't feel super secure but i guess that's how it works again it just gives me a feeling of not being secure but it is secure um and the idea is that you'll be able to create a key value but actually it's either key value pair or a key vault reference and this is the way that you're able to reference uh secrets within your vault so there's not really much to it it's not super complicated this view is fine it does what it says it does but there you go all right let's take a look at feature manager for azure app configuration this is the meat and potatoes this is the thing that you really want to be using as your app configuration for it's for feature flags so what is a feature flag a feature flag provides an alternative to maintaining multiple feature branches in source code a condition within the code enables or disables a feature during runtime and this makes it easier to roll back or do a to b testing for new functionality so azure app configuration feature manager allows you to add featured flags which can be then accessed via code so the idea is you say enable a feature flag you name the flag whatever you want so i called flash cards assuming i need a flash card feature you can label them of course and then you are providing code now i didn't see much for things outside of net and or well csharp and java so i'm hoping that their sdks or there's some kind of library for ruby python node.js things like that but it might just be only for c sharp so net apps and java now let's take a talk let's talk about an advanced feature of feature manager which is feature filters this allows filtering of features so a feature filter consistently evaluates the state of a feature flag and a feature flag supports three types of builtin filters targeting time window and percentage which is confusing because when you go and turn it on it says targeting time window and custom so the thing is is that both targeting and percentage is within the targeting thing and then custom is its own thing so that is those little inconsistencies that microsoft likes to present you in their documentation so if we were looking at a time window and i think this is really cool but you could say okay this feature starts on this date and it's between this time and i guess starts on and ends on that time so it's like if you just want to run a feature for you know 10 days you could do that then there's a percentage thing so the idea is that you say okay i'm going to define two different groups and for group a they're going to get 50 of this feature in group b they'll get 50 of this feature so it'll change the amount the feature is available uh per per group like how many in that group will get it and then targeting would be like you could say i want it for these users so when they say targeting it literally just says user and you put a name in it's and it and it's just a a text value it's not like it's linked to an azure active directory user anything like that and then they have custom filters which can be created based on different factors such as device use types geography location etc when i clicked on it it wasn't very clear on how to do custom filters but the idea is that you can go beyond these ones if you want to uh you know have different values so there you go all right let's take a look here at the microsoft graph api which exposes rest apis and sdks to access data from the following microsoft cloud services the first is m35 core services where we have bookings calendar delve excel m365e discovery microsoft search onedrive onenote outlook exchange people planner sharepoint teams to do workspace analytics uh so basically a lot of services for enterprise mobility and security we have advanced threat and analytics threat protection which is atp azure azure 8d identity manager intune then we have dynamics 365 business central for windows 10 services we have activities devices notifications universal print and for support sdks we have it for android ios angular asp.net go javascript node.js java php powershell python and they had ruby but i i looked into it and um they it doesn't work anymore and they say they're going to make a new one but i'm still waiting for it so i always get the short stick when it comes to ruby and the data is accessible via the unified endpoint at graph.microsoft.com but how do we use this stuff let's take a look here so for node.js we would install the microsoft graph client and then here as an example we are sending an email so the idea is getting my pen tool out here we create a client we're defining uh some mail and then we're calling our endpoint um via the api that's basically how you're always going to be doing it post that message and that's how the graph api works so there you go all right let's take a look at microsoft graph connectors so connectors offers a simple way to bring external data into microsoft graph and enhance m365 intelligence experiences so you might want to build a custom connector to integrate with services that aren't available as connectors built by microsoft using microsoft graph connectors rest api and you can use the microsoft graph connector api to create and manage external data connections define and register the schemas of the external data types ingest external data items and microsoft graph sync external groups and microsoft search indexes all your m365 data to make it searchable for users with microsoft graph connectors so your organization can index thirdparty data so it appears in the microsoft search results so you're like okay where's we have microsoft graph connectors where are they and that's where i ended up on the connector gallery so microsoft has a gallery website containing a hundred plus connectors it's at microsoft.com for its microsoft search forward slash connectors and i was a bit confused at first but now i understand what it is but the idea is that they have a bunch of different connectors from a bunch of different thirdparty providers so microsoft has some but there's a lot from a bunch of different providers some example connectors could be amazon s3 by ba insights azure blob storage by accenture google cloud sql by ba insights ibm connections by raytheon google drive iba insights azure devops by microsoft and connectors might not be free and their instructions greatly vary based on the implementer so this is not like a unified thing it's you go there and it's like some other app that's used for it so um yeah it's kind of weird because it just doesn't feel very standardized but i guess it's just kind of a library of services that can connect things kind of like zapier but as if there was all these specialized services by individual thirdparty cloud vendors but there you go well let's take a look here at microsoft graph data connect which augments microsoft graph's transactional model with an intelligent way to access rich data at scale so they're just meaning about getting data out of microsoft and putting it into some kind of tool that can transform it do ai on it things like that so microsoft graph data connect uses azure data factory to copy n365 data to your application storage at your at configurable intervals and the way this is going to work is you're going to have to go to the microsoft graph data connect under your m365 admin center go to settings work settings and services and from there you will turn it on and the idea once it's turned on then you can go over to azure data factory and by adding it as a data set to your pipeline you now have the ability to access your data and if you know how data factory works then you know you can do a bunch of stuff with it store it in a blob storage send it to synapsis do whatever you want from there okay hey this is andrew brown from exam pro and we are taking a look at microsoft graph so this is a gateway service to programmatically access m365 windows 10 enterprise mobility plus security and so here's the big graphic but it's composed of three elements so microsoft graph api the microsoft graph connectors and the microsoft data connect so here in the middle we have graph api this is where you programmatically access various microsoft services you have connectors these are ways of getting external data in to microsoft graph and then you have graph data connect this is a way of storing data here so hopefully that makes it pretty clear i do find microsoft graph a little bit confusing because there's a lot going on here but hopefully as we work through these slides here it will make more sense okay hey this is andrew brown from exam pro and we are taking a look at azure front door which is a traffic manager traffic accelerator global load balancer and content distribution network cdn and yes it does all those things and i think that what creates a lot of confusion for people when they first look at azure front door uh but the idea here is azure front door is a modern application delivery network platform so the idea is that it can do all these things so provide a secure scalable cdn dynamic site acceleration global https load balancing for your global web application that's the definition that azure has but uh yeah there are going to be services that have overlap with azure front door like azure cdn and things like that but just understand that this is a very robust thing and something that you're going to want to use quite often for your applications so azure front door features caching like cdns with rules expiring policies resilience by distributing incoming traffic across multiple or different azure regions cookie based session affinity for restful applications when traffic needs to be redirected back to the same uh back end so that's for stateful applications health probes which are very common um at either the dns level or the load balancing level but since it is both here uh it has it so to determine uh the healthiest and closest back end to the client request it can have a waf attached to it so protecting your back ends from malicious attacks and vulnerabilities i believe that is using azure's waf policy so you're just basically attaching a policy to that there your url redirect so redirecting traffic based on a variety of things so protocols such as http or https the host name the path the query string not just that there's a lot of stuff in there url rewriting so with the powerful engine for rewriting incoming requests to a different backend request and so in the simplest sense this is what azure looks like you have a front you have frontends or domains where you're bringing stuff into azure front door and then you have your back ends so that we can filter stuff there there's actually a lot more components going on there we will talk about them or we'll have a diagram here to show you but this is the basis of azure front door well let's take a look at the core components for azure front door uh and the reason i specifically made this graphic was because there was no real uh representation or example that i could find and so i thought it was very important because when we start to talk about routing the path looks completely different from this it's similar but it's different so there is uh i don't know why there's inconsistencies there there is azure front door classic but i was using the latest version and so just understand that if you're confused i'm confused too but this is uh the visualization of what i think it works when you're actually using it within the gui okay so the idea is you have a profile and a profile as far as i can understand contains all the other azure front door components and then you have this idea of an end point and this is the pathway from the front end to the back end uh and within your endpoint you need to have some kind of origin so the origin is saying what to point at so what to point it at the back end and even though it says origin origin here it's really origin group so you actually assign origin groups to an endpoint and so it can rotate to multiple origins then you have a route and so the route is where you can apply your rules and rule sets to determine more routing things then we have the ability to have a waff policy before it reaches the back end and so hopefully that makes it a little bit more clear um but uh one thing that is interesting when we talk about routing is that when they talk about the order of data coming in they actually describe it as if the waf policies over here but then when you look at the interface and give me stuff they put it on the right side so whether it is um you know here or there i don't think really matters in practicality but uh you know i was trying to find that out um and the way data gets into azure front door is that it's going to uh hit a edge location and then that edge location is going to then uh trigger i don't know why it says trigger but the idea is to go to the front end it should be more like match uh but i have this kind of like expressway or highway to kind of emphasize that this is going to be very very fast to get to uh your azure resources so hopefully that makes it uh pretty clear about the components there but it'll be crystal clear when we do the follow along okay all right let's take a look at what tiers we have for azure front door because the different tier is going to determine the feature set that is available to us so azure offers two tiers for azure front door the first is standard or delivery optimized so the idea is you have content delivery optimize offering both static and dynamic contact acceleration we get our global load balancing our ssl offloading domain name domain and certificate certificate management enhanced traffic analytics basic security and capabilities and that sounds like a lot but with premium we get additional security optimized stuff so we get everything we get in standard uh but the idea here is we can uh have extensive security capabilities across our web application firewall you can have bot protection private link support which i believe is for internal routing of traffic keeping things within the azure network an integration with microsoft threat intelligence and security analytics uh standard is really good for most use cases but if you need that additional uh security that's where premium's gonna come into play okay all right let's talk about how routing works in azure front door because there are a lot of components and it's important to understand the flow and also the different types of routing that are available to us so routing is the path an https request uh from the user will take to reach a backend service configured in azure front door so at the start you have that http request from the user coming from their browser most likely and then the idea is it'll go over the internet and find the closest edge location edge location is just a a point of presence a data center or uh software hardware that is on on the edge of a network that allows you to enter the azure network and so from there you are now going to be in the azure network and so you will be matched with a azure front door profile from there it will then uh if you have waf rules it will evaluate the web application firewall rules and if everything's okay it will then match you to an azure front door route and then from that route it will then go through the engines rule to evaluate where to route things if there is cached information it will immediately return that information right away um but if there's not it will just proceed to the next step which is to send you to an origin group an origin group is just a grouping of origins and an origin is how you send a request to a back end so you know hopefully we'll see these components over and over again so it's very clear how they work together but let's talk about the four traffic routing methods that are available in azure front doors so the first is based on latency so requests are sent to the lowest latency backends acceptable with a sensitivity range you have priorities so requests are sent based on a userdefined number you have weighted so these are requests that are distributed to backends according to the weight coefficient and session affinity so requests from the same end user gets sent to the same back end so these are for state fullback and stateful applications this kind of functionality for routing like if you're using aws you'd see this functionality in rough d3 but in the case of azure we're seeing these within front door for this modern application delivery stack okay so you know hopefully that gives you an idea but yeah there you go we are looking at azure front door and it's time to take a closer look at the origin component so the origin is what azure front door will point at to serve up to the end user and origin is the end point that points to your back end so here's the example of the form of the information you'll need to fill out and the most important field here is the origin type because that is what's going to say what are the supported backends or supported origin so here we have azure blob storage azure storage for static website hosting cloud service yes there is a azure service called azure cloud service i cannot remember what it does but it is a service we have azure app services that's a place where we commonly deploy apps static web app api management application gateway public ip address so that would just be pointing to an ip not necessarily a service azure traffic manager azure spring cloud spring cloud is for java applications i believe azure container instances so aci or custom so you just provide a host name to other fields that are very important for origin is the priority and the weight so priority determines who to send traffic to first and this is going to be a number between one and five the lowest number is the higher priority back ends can have the same priority number for weights this allows you to determine the split of traffic distribution between origins of the same priority so if you had a priority one and somebody was 50 50 then they'd get 50 50 split between them but this number can between uh be between one to one thousand the default value is 50. and uh you know this is what it looks like when you add an origin uh to an origin group so you can see the status is enabled the priority is one the weight is one thousand uh but there you go so we just talked about origins for azure front door let's talk about origin groups so origin groups are a collection of origins and an origin has to belong to an urgent group so you can't just create an origin and let it float around in azure it's always going to be assigned to one and when you create an azure front door profile by default you will have an origin group called default origin group and so here it is and here you can see we have a default route for whatever we set up for this um azure front door so a few other things you need to know because there's more than just grouping it's the fact that you can apply things to that group and the two things you can apply are health probes to check the health of your origins and load balancing settings to balance uh determine the balancing of your origins and in order for inbound traffic to reach an origin group an endpoint needs to be associated to the origin group via a route so there's a button in the um in the interface so you'll see which is the associated endpoint to route it's that simple but we will explore health probes and load balancing settings a little bit more just understand that they belong to the origin group and those aren't just separate features that are floating around in the azure front door ui or gui there okay all right let's take a look here at health checks also known as health probes and that's what it will be called when you look at it in the interface but in the documentation they'll call them health checks and this is a feature found under origin groups and then it will apply to your origin so this allows you to ping a back end to determine if it has a healthy response if there is a healthy response which would be determined by status 200 okay things will get routed to it if it is not healthy or considered unhealthy then simply traffic will not be routed but instead rather to other healthy back ends if there are other origins configured so you might be asking well what is a http response code just in case you don't know what it is but the idea is that when a user sends an http request a http response is returned and so for http responses they will have a response code to communicate how backend servers interpreted the request and a response code is a number that coordinates to what has happened and there is a lot of them so but i'm just going to show you the most popular one so 200 for okay 403 for forbidden 404 for not found 500 for internal server error i'm not kidding there's like 100 of these but these are the most common ones that you've probably seen before but they all mean something okay all right let's take a look at load balancing settings this is another option that is available under the origin group so this allows you to define what sample set is needed to be used to call the backend as healthy or unhealthy so this is something you're going to be using alongside with health checks or health probes so the idea is you have the sample size so how many times uh or how many times do we need to consider how many of the sample size have to be successful and the latency betw uh between those okay so the latency sensitivity with the value of zero we're just talking about um this last value here that if you set it to zero it means that it will always send to the fastest available backend else front door will round robin traffic between the fastest and the next fastest back ends with the configured latency sensitivity so there you go so one thing you can do in azure front door is assign custom domain names and actually you can assign multiple custom domain names within azure front door to show you the form it's not too complicated but the idea is that for dns management you can either use azure managed dns or another provider you are specifying the dns zone name for the custom domain you're going to add that custom domain and you do have a couple options here for http where you can have afd or bring your own certificate for enabling https and you can apply either tls 1.0 or 1.2 of course 1.2 is newer so that's what you should be using but there's really not much to say about that other than you can attach custom domains very straightforward very simple okay all right we're taking a look here at endpoint manager so endpoint manager provides an overview of endpoints you've configured for your azure front door not to be confused with microsoft endpoint manager which is a security service specifically for managing external devices so devices to register your company for organizations that are using outside of your office so no relation to that product just very similar naming so this is an example of an endpoint that you will see in azure front door and the endpoint really means like the entry point and so that's going to be the domain name so if you do not have a custom domain azure provides you one azure fd.net it might vary based on um capacity and things like that but the idea is that it goes from domains to the origin group to the routes to security there so you know hopefully that is pretty darn uh clear endpoint manager lists how many instances of each element are created with an endpoint and the associated status of each element will also be displayed so there you go let's take a look at routes for azure front door so a route maps domains and matching path patterns to a specific origin group so here uh the idea is that we can define what domain that we want to use for this route we can say what patterns we're matching on so a wild card is going to match on everything is it going to be treated as a redirect to https because if you have http you probably want to go there and you're going to associate that with an origin group and you can provide an origin path and define the forwarding protocol routes can also have caching and compression applied so this is where you would uh start doing that and routes from rule sets can be associated to routes to apply intelligent routing so that's where you get that routing association but yeah there you go let's take a look at traffic acceleration for azure front door sometimes no also known as global acceleration depending on your closer provider but the idea here is without any requirements to make any modifications to your application code azure front door can speed up global delivery of your application so azure achieves this traffic acceleration by directing traffic to the nearest edge location to on ramp the azure to the into the azure network so the idea is you have somebody that wants to connect they're going to connect to that edge location and an education is just basically a data center a point of presence hardware on the edge of a network and then once you're in there everything from here to the uh origin to the uh to the back end is going to be through the azure network and it's going to be like being on an expressway so traffic that is following within the internal azure network travels at accelerated speed while also taking the most direct path think of it as an expressway and think of these edge locations as onramps so there you go let's take a look here at rule sets for azure front door and this allows you to customize how http requests get handled at the edge and provides more controlled behavior uh to your web application so the idea is that you have a rule set and within this you're going to define rules gonna get my pen tool out here so you name it and you can provide a condition and then an action and we have a lot of options here for things you can do so let's take a look at what those options are for conditions we have a lot of stuff device type http version cookies arg strings query strings a bunch of things we can do with the requests such as the body file name file extension etc so lots of conditions available to us then for operators we have equals contains less than greater than and a whole host of other even including regular expressions for actions uh this is where you really get a very interesting things but you can do cache expiration so the idea is that the action is to expire the cache you can have a cache key query string so the idea is you can cache every unique url so it's just a bit more complex caching you can modify the request of the header or modify the response of the header that might be useful for something to do you can rewrite urls uh so or sorry redirect urls so the idea is that you can send it to different types of redirects such as a temporary one or move like 301 or 307 or 308 you can rewrite urls you provide a source pattern and then a destination you can override the origin group there so a lot of stuff you can do within those rule sets hey this is andrew brown and this fall along we're going to be utilizing azure front door so before we do that we're going to need ourselves a storage account to set up some static website storage so what i want you to do is go to storage accounts and there is a static web app host hosting that we could use but we're going to do the old school way because it's always great to learn a few different ways to do things in azure and so there's probably another follow along where we use this static static site thing there so what we're going to do is create a new uh we're going to create a new um resource group i'm going to call this one my azure front door and we'll say ok and from here we'll have to name it something so we'll just say my azure front door or sorry we'll do static storage storage eight eight eight eight if you can't get the four eights because i'm using it then just give it a different um number sequence because these are treated like fully qualified domains choose like a us region so that we're all doing the same thing for performance we're going to stick with standard it doesn't matter if it's on georedundant we're not doing anything fancy there so what i want you to do is go ahead and hit review and create and then give it a moment and hit create and we're going to wait for that to finish deployment and from there we'll then have to enable static website so it doesn't take too long to deploy so we'll just give it a moment and while that's going we do need to create ourselves a index html file so you're going to need some kind of editor so i'm just opening up visual studio code on my computer just give it a moment there and i'm just creating a new file here this is taking it taking a moment to load okay so and what we need to do is just create an index.html file these are really basic you can find them anywhere online but this one's going to say hello mars hello mars very very simple so i'm going to go ahead and save this to my desktop so save as and show local maybe there we go that's a little bit more sane and we'll just say index.html and we'll now go to the resource and we're going to go to website static as a blade on left hand side so we can do a stack website stuff we're going to say index html i think we have to set that for it to work i cannot remember if that's the case we have a primary endpoint this is uh these matter for later but we'll come back to that and we're going to go over to our containers and we're going to have to create a new container so i'm just going to call that actually we don't because we're going to put it in web so when we turned on static website hosting it already gave us a blob or container a container for us to add our ad or upload our files there so we'll go to upload here and then i'm going to go ahead and select that index.html file and upload it in place we need to change our access level for this uh this uh container to um i think just blob so i'm just double checking here yeah i think it's blob access uh can only be anonymous request container data is not available so we'll say okay i believe that is correct okay uh sometimes you can like we're clicked into it but i think like in my instructions i do it a little bit different so i'm just going to double check to see what the settings are there just so you can see it if you're following my instructions so change access private blob yeah okay so that's fine um so blob access should be okay and so now what we want to do is just test that our our page is working so if we go back to static website here on the left hand side we can grab this primary endpoint and we should be able to post it anywhere here because they have the old tutorial here this is a moment ago so that's why it's a bit confusing but so hello mars is working but this is on static storage this isn't in front of or behind azure front door which is what we'll want to do next so what i want you to do is just close a couple of these tabs out we're going to type in azure front door our front door should be enough front there we go and we have a whole bunch of options we'll just hit create that's a front door we actually do use for the platform we have azure front door we have a quick create i always go custom i don't think i've ever done quick create i just i just have more trust in custom and from here we will choose front door and east us is fine i'm just wondering where our storage account is that's why we should always just kind of set them to be the same place so let's go to storage account i don't think it will matter but i'm just going to double check storage account and this one is in east us so we're going to be okay just make sure they match just so we have less problems and i'm going to call this one my azure front door now there are more functionalities in the premium but standard is fine for us we'll go next to secrets um this is if we want to add a certificate um like you could bring your own certificate um but uh we're not too worried about that we're going to have to add a new endpoint so we're going to say my end point here there's a lot of small steps in here so this should be fun hopefully we don't configure anything wrong here and then we need to add a route say my route and it's going to use the default domain that's totally fine we don't have to do anything it's going to just be on the forward slash asterisk there we do need to create an origin group so we'll go here and create a new origin group so my origin group and then from there we need to add an origin yes it is very squirrelly going through all this my origin the origin type is going to be azure or sorry storage static website and from here we need to choose the right one we called it static storage uh this is fine this is all fine we'll go ahead and hit add and down below we have some load balancing we don't care about any of that so you can see status is enabled all this should be okay the protocol here for the health probe should be http um actually let me double check because remember this is where i ran into some trouble i believe yeah http i think it's https okay i think i read somewhere that's like stack storage doesn't use http but clearly it does because we went to the link earlier it was https so i might have the screenshots wrong but i've corrected them in the actual instructions on my site we'll go ahead and save that i think it's https and then the the protocol here um we'll just leave it matching it's totally fine origin path is fine we'll go ahead and add that there and we'll go review create we'll give it a moment and we'll create all right so after waiting about a few minutes like two three minutes it looks like as your front door setup so we'll go to the resource i'm just hoping this works sometimes you have to play around with the settings but if you see the endpoint hostname we'll go ahead and grab that there paste it on in and we get a 404 so something's not working just right so we'll have to go do some debugging which i was hoping we didn't have to do so we go the front door manager this is the same setup we have here before there are like different ways to get to it but we'll have to just kind of go through and debug it so we do have my route my origin group so we can click into our origin group and then from here we'll expand it and we have our route so there's something that is not correct and that's what we have to figure out so i'm just going to double check my instructions because this was a bit tricky to figure out um and it really came down to like these protocols here so let me keep checking and i mean i think the probe is working correctly so if we go in the origin if this wasn't working correctly then it wouldn't it wouldn't show green so go back up a step here you know what maybe we should just wait a little bit because sometimes it takes time to propagate and i don't i don't 100 trust that it's not working because i feel like we configured it exactly right so i'll open up a new tab here it says 404 still my origin group that's fine we'll go here yeah so what i'm going to do is go back to our storage account i just want to double check what that string was for the static website i'm pretty sure it was https it is hps so there's no reason that should not work this is what i was trying to uh spare you the uh the debugging of this and it's very common it's not just azure anything that has a cdn sometimes it's it's difficult to figure that stuff out we'll go ahead and add that route and we'll just carefully look at what we have here so patterns match that is fine it's the correct endpoint both protocols are accepted that's totally fine redirect https that's totally fine match that doesn't matter so it should just work the only thing we didn't do was enable caching which i think wouldn't hurt to do because it is a cdn we don't take advantage of it if we don't turn it on so we'll do that and say ignore a query string i mean that's not going to fix this problem if the routing's messed up again i'm just hoping that maybe it just has to do with propagation i'll go back to the overview here and we will open this again oops grab this let's just make sure it's doing hps we're working to restore all services as possible right now what do you mean our services aren't available right now let's go look at azure status page here and we'll take a look at front door as your front door it's saying that it's fine u.s east there's no green beside it good so we go down here i guess it's a it's a nonregional service so if it's green it's green so you know i'm going to play around with this for a little oh 404 web so this is better right it's just saying it can't find the content well i mean i guess that makes sense because that was there so we'll just clear that out oh now it's loading so i think it what the issue is there was no issue it's just propagating to all the servers and it took some time so i think that's what really threw me for a loop um when i was originally doing it so just give it some patience and it will work eventually so we are all done here so we can go ahead and clean this up and i'm just looking for the resource group here we'll go ahead and delete it and there you go that's azure front door and i'll see you in the next one okay let's take a look here at application insights and this is an application performance management service so it's an apm and it's a sub service of azure monitor let's talk about what apms are so these are monitoring and they do monitoring management of performance and availability for software apps apm strives to detect diagnose complex application performance problems to maintain an expected level of performance so why use application insights will automatically detect performance anomalies and includes powerful analytics tools to help you diagnose issues and to understand what users do with your app designed to help you continuously improve performance and usability works for apps4.net node.js java python hosted on onpremise hybrid or any public cloud and i know that it works for other ones that are not part of the supported languages so there's one for ruby but the thing is is like azure's only providing official support for a bunch of languages but you might still have libraries out there for them and it integrates with your devops processes and can monitor analyze telemetry from mobile apps by integrating with visual studio app center and if you're running an app you definitely want to have an apm installed if you ever use data dog or skylight or new relic that's what apms are or what those providers offer but let's take a closer look here and this is kind of an example of an application and what you're going to see is that we have a front end of backend and workers just to kind of represent how you can instrument your applications and when we say instrument it just means install like this piece of code that runs on it to send data back to application insights and the idea is that when you instrument it you're just installing the instrument package sdk or in some cases you can just turn it on where it's supported so you don't necessarily have to install an app you might just press a button and it will install it within uh azure services and there's many ways that you can view your telemetry data so the idea is that the agents send that information to application insights and then you're gonna be able to leverage that in alerts power bi visual studio rest api continuous exports a lot of a lot of services can ingest application insights so apps can be instrumented from anywhere so if you're running on aws you can install it on your servers there when you set up application sites monitoring for your web apps you create an application site resource in microsoft azure so it's a physical resource and you open this resource in the azure portal in order to see an analyzed telemetry collected from your app and the resource is identified by the instrumentation key also known as the i key and i just got a big old list here just to tell you all the things you can do with it so what does application insight monitor well request rates response time failure rates dependency rates response times failure rates exceptions page views load performance ajax calls user and sessions count performance counters host diagnostics diagnostic trace logs custom events metrics uh so there you go that's a big old list and on the righthand side is where can i see my telemetry and we saw a little list but let's go through the big list and that's smart detection manual alerts application map profiler user usage analysis diagnostic search for instance data metrics explorer for aggregated data dashboards live metrics streams analytics visual studio snapshot debugger power bi rest api continuous export and there you go so you can see that it can collect and you can use it in a lot of places definitely definitely install if you're using are you running a web application let us quickly talk about open telemetry so open telemetry also known as otal is a collection of open source tools apis that's the case instrument generate collect and export telemetry data so otel standardizes the way telemetry data metrics logs choices are generated and collected it uses a thing called wire protocol which refers to a way of getting data from point to point um an application insight supports otel as an alternative to azure analytics sdk for instrumentation so open telemetry is very popular in the cloud native space and kubernetes um and it probably is the future of metrics so it's a standard and so we're seeing adoption top not not just by azure but by aws and google cloud so uh it is worth knowing about this alternative method will show up in the exam maybe not but i definitely want you to know what open telemetry is okay let's talk about instrumentation for application insights because if you don't do this you're not going to be getting data to your application insights so you instrument your application by adding the azure application insights sdk and implementing traces it supports a variety of languages.net java python node.js javascript no ruby i really like ruby support please so you do npm install application insight save that's the javascript example here i'm requiring it we're doing some setup and then from there what you can see with the client is we are tracking an event we're tracking an exception we're tracking a metric we're tracking a trace we're tracking dependency we're tracking requests so there's a variety of things we can do but you know there is auto instrumentation we'll talk about that uh soon but um you know this is the idea generally is that this is what instrumentation looks like okay so wouldn't it be nice if we didn't have to instrument our app that it would just work and that's where auto instrumentation comes in it allows you to enable application monitoring and application insights without changing your code and it's supported for a variety of different things so support's going to vary and you'll have to just come back and check if this is even true still i mean i did just get this table but you know azure moves really fast let's take a look at what i'm looking at right now so across the top we can see net.net core java node.js python and there is a term called onbd which means on by default so another thing is going to vary on based on service so for azure app services pretty good support across the board except for python which is surprising for linux not for net because it just doesn't let you have it and not for python for azure functions we only have it in public preview for java for azure spring cloud uh it's only for java again which makes sense because spring is a java thing so it's these will never get supported here they should really say n a azure kubernetes service so only for net core and maybe through the agent uh virtual machine windows onpremise virtual machine um windows they're the same here and then for standalone agent we have generally available for java so uh not a lot of support but i mean there is some so you know if you're in the dotnet world or in the java world looks like you're going to be in really good shape but there you go hey this is andrew brown and in this follow along what we're going to do is learn about using application insights so application insights allows us to get visible data about how our application is running like distributed tracing and so we're going to deploy some kind of workload and then apply application insights to it so what we'll do is go ahead and go to app services and deploy a new uh service so let's just type in the top your app services and we'll go here we'll go ahead and create ourselves a new app service and i'm going to make a new resource group here so it's easy for us to clean up later so my app insights and then from there we'll need a web name so we'll say my app insights 8888 if you cannot get this name just change the values to something that works for you because these are fully qualified domains it's like registering domain names so make sure it works for you we're going to publish code here today for the stack we're going to choose no just to make our lives super easy i'm choosing 16 lts if there's a newer version should work fine no does not change that much in terms of versions but you know depends how far in the future you are in terms of region uh canada uh sorry central u.s is fine it's always chooses by random i'm in canada but i'm going to stay in central u.s because i know that the things i'm going to be using will be available there in terms of the sku in sizes greatly affects the cost we'll click into here because we really do not want to pay much at all so we'll give it a moment to load it'll take a few seconds not well it usually doesn't but for right now it certainly does so we'll give it a moment to load i'm going to make my way over to dev test and we're going to choose the cheapest option here which is going to be f1 so you can see there's not a whole lot it's just memory storage you know we don't get custom domain we don't get manual scaling we don't need that stuff we need it nice and simple so we'll go ahead and choose free f1 that sounds good to me i like the word free we'll go to our next step which is deployment in fact i don't think anything has to be changed under deployment but we'll click it here not sure why azure is a bit slower today but that's just how it's going so we'll give it a moment to think and we will wait wow it is really really slow here right now okay so for deployment um i mean you could hook hiccup hook up github actions and things like that i don't think we need to do any of that so i'm going to make my way over to networking and we do want to inject oh sorry not networking monitoring because we don't need networking monitoring but we want to enable application insights this is going to set up some stuff for us um so it's going to set us an application insights what name space i think it'll set up something for us so we don't have to worry about that so we'll go ahead and hit review create and hit review and so now we just need to wait for that to deploy so i'll see you back here when that is done okay after a very short wait maybe like a minute or two that has been uh completed so we'll go ahead and go to that resource and i just want to show you uh over here in the configuration tab the environment variables that are going to get passed to our application so if you go here these are the environment variables that get passed along and notice we have one called application insights connection string instrumentation key and the reason this is important is because we don't have to um in our environment set it so like since we're using app services and we enabled application insights it's already going to set those values and the great thing about the sdk is it's going to pick up those values so we don't have to explicitly set them at least i don't think so if we do set them we'll use the environment variables i haven't decided as of yet for application insights um it set us up a i don't think they call them workspace name namespace i don't really care what the name is called but the idea is that we have an environment set up in order to gather information so what we're going to do is just go back here and notice down below it says node is in progress agent is enabled so just indicate that that is working as expected that's a good message to see not that we'd ever see any other message there and so now that uh we we know that everything is deployed or at least our environment is ready let's go ahead and create a new application so what i'm going to do is make my way over to github here and we are going to create ourselves a new repository now i don't know if i have an old one sticking around here so hopefully i don't but we'll see here i'm going to say my app insights would be something i would name it and so i already have one there so what i'm gonna do is i'm just gonna make a new tab here okay i'm just gonna go to uh anyone here this is my marketing website i'm gonna go my app insights i'm just going to rename the old one so it's out of the way so you can follow along exactly the same way as me here i'm just going to do settings we'll go down here we'll just say old all right and we'll rename that and i'll close that out and so what we'll do is go back here and this this is fine now i know that it says it's not but i know it is we're going to add our email file we're going to go down to get ignore and add node i might add i might have done this after the fact so i might have added the steps out of order here but uh that should improve that there i'm just going to do hyphen old i'm going to take that out there there we go and we'll go ahead and create this repository so um i want you to get getpod installed so gitpod is a really great great service for having a cloud developer environment it has a generous free tier so there's no reason not to sign up and really this is the future of cloud development so make sure to go get a get pod account but you really don't have to because when you launch this environment either launches it temporarily or you can connect your account and then persist that data but um anyway if you install the getpolychrome extension you'll get this button but you don't need to install the chrome extension all you have to do is you just need to put this in front of your address so this is the current address we're at like this is the current repository whoops this is the current repository link and all this button is doing uh copy link address is just putting this part in front here okay so this is going to open up a git pod environment this launches extremely fast and we're going to create a new node.js application and deploy it to app services so we'll give it a moment here doesn't take too long here we go and what we'll do is we're going to use the mpx uh express generator to generate a new express express application express.js generator so we'll type that in there we'll say yes and it's going to create us a skeleton app just say yes just say yes all the way down we'll get a bunch of stuff in here and then we'll do our npm install you can do npm installer npmi if you want to make it nice and short we'll give it a moment there okay and then we'll make sure this works we'll type in npm start this is going to prompt us to open our browser so we'll say open browser so express yes that application is working up this link up here so this is our application but it's not instrumented to send any data so that's what we need to do so the next thing we're going to do is just kill this control c uh yeah control c on my keyboard and we'll type in npm install i'm typing application insights hyphen hyphen save and what that's going to do is install that dependency for application insights so if i go over to my package.json here you can see that it is a dependency that is installed so now we need to configure that so what we'll do is go over to our um app.js file and so we got to insert this somewhere here um so what i'm going to do is i'm going to go all the way to the top that's probably a good place to put it and we'll say let i like how it says all var var is really old app insights equals require double quotations or single it doesn't matter it's not going to hurt either way application insights and that will define our variable here the next thing is we need to set it up so we'll say app insights dot setup and then we need our connection strings that's where we're going to put this thing here our connection string and then we need a bunch of other things so i'm not going to type these all out because there's a lot of them so we'll just say application insights instrumentation node.js and i'm sure the documentation is going to help us out there so go here i'm just scrolling for the usual configuration here it is that's all i've been typing here this entire time that's probably where i got it originally so we'll go ahead and we'll just paste that on in there allow take out the first line that way there's no spelling mistakes and so we have auto dependency correlation auto collect requests a bunch of interesting stuff here it's going to do distributed tracing we don't need the semicolon we'll just take it off there for fun and i'll save that and so now the next thing we need is the connection string in order to make sure this application works which is going to go here so we'll make our way back over to this tab and the connection string is actually in the configuration that were earlier here in this blade so give it a moment to load there we go and so we want the connection string so we'll click that i think before that you could use the instrumentation key but now connection string is what is preferred um i want to copy the value okay i just clicked into it to make it a bit easier for me and what i'm going to do this is really dirty but i'm going to just paste it in here and we're going to just do it in a few different steps just to make sure that this is working correctly before we switch this over to environment variables and one other thing that we need to do if we want to actually get any data is we'll have to turn send live metrics to true now if you are doing instrumentation you might not want live metrics i don't know if it costs more but for our development purposes it's not a big deal to turn this to true so that is now enabled and uh you know our dot get ignore should have a bunch of stuff in it like node module so we're not committing that stuff i'm going to commit this now it does have the connection string in here i'm not too worried about it because we are going to um delete it out later on so we'll just say um save my changes but i am pointing to pointing out to you that we are committing this string here which is bad practice right because if somebody got that then they could mess with this but again uh we're going to tear all this stuff down so it's not that big of a deal and so we should be okay so now that this is installed what we're going to do is do an npm start that's going to start up our application and then we're going to go open in the browser again so here's our application now we'll go back to application insights or not application sites but into our app service our actual application on the left hand side there's going to be application insights and from here what we can do is click the view application insights to go to the workspace the namespace whatever they want to call it and this is the way we're going to be able to see some information so live metrics on the left hand side will show us data in real time so now it says it's connected to your app so we'll go back here refresh so the app connects and we should get some data there we go so these are the requests we're making right here so i hit enter enter enter enter enter enter enter we go back here here's those requests coming in it's giving a 304 for the css um so yeah mostly yeah there's our stuff so we know that it's working because we can see information there so that's pretty darn straightforward now let's go back to our application set this up properly because we really shouldn't be setting our string like this this is pretty dirty so what i'm going to do is just delete this out here and i'm just going to type in um process.env and then we're going to say we'll match the name of what it's supposed to be called in our application so i'll click back to make it back to our actual application we'll go to configuration and from here i'm going to just go ahead and grab that string string name i just clicked into it i just don't want to type it wrong and we'll go ahead and paste that in there and so this is what we are going to set and just did ctrl c to kill there but this is what we're going to set as our environment variable so i'm doing gp gp stands for git pod it's a way of setting environment variables and we'll do double quotations always do double quotations especially with connection strings when setting environment variables so part of the string does not get cut off because it will cut off at the first forward slash and then you'll just end up with like that and that's not enough right so i've copied that i'm going to go back here paste it in we'll hit enter and so that's setting the key now if i do grep or sorry env so this show like this prints all the environment variables so env pipe pipe is just a vertical line grep and i type in connection any part of the string notice that it's not set yet so just because we said gpenv does not mean it's set uh what you have to do is restart the environment but we can temporarily set it export this is how you'd set it in bash you type export and you'd give it the the name here like this and you do equals double quotations and this will this will make the environment work but the only problem is that if we restart this uh workspace we'll lose that on the next time we restart it so this will definitely work but let's just do it let's just restart the environment anyway so we have process e and v up here but we have to import that as well so i'm going to say let process equals require process process is just a way of getting access to the process process being the application that's running to access the environment variables that are set which are getting passed along to the application by app services so that is good there and so i think we are in good shape the only other thing is i want to generate a git pod file so i'm going to just do gp for uh git pod and we'll just set some things up this is going to save us some time when we relaunch our environment so i'm just doing uh pipes here uh on the end here that allows us to do multiline even though we're only doing a single line we could probably emit the pipe if we wanted to but i'm just putting it in anyway and so we'll just indent there coloring usually tells us something's wrong and this starts up on port 3000. i want it in the browser i don't want to do preview that means opens in a little window here that's kind of gross and so this looks good i think i accidentally removed a line here because we'll say tasks up here and so now we have our get pod uh uh yaml file so we'll go ahead and add all these files here so um uh instrument i think that's everything we need to do there so we'll just go ahead and commit that we'll sync our changes and i'll do a get push just to double check double check that this stuff was synced and committed so if you go back to our actual uh get or github if you go context it just brings us back to the github repository that gitpod yaml file set so we know that our changes have been committed highlighting looks fine and this is green this is this kind of looks blue so hopefully that works but anyway what we're going to do is go ahead and open this again sorry we'll close this tab and we'll reopen up and get pod so this will launch a new workspace if you still have the old tab open it might ask you to physically press a button called new workspace but generally it opens a new workspace so it's setting up a new container completely for us so give it a moment it doesn't take too too long and what we'll do here is wait for the terminal to open okay notice it ran the server did npm install did everything for us i'm just going to open a new terminal here on the on the right hand side and bash whoops well i did not mean to split it but that's okay i don't know how to unsplit it so just drag it around that doesn't matter yours will look a little bit different than mine and so all i want to do is do env grep um con because i want to see that connection with set we'll do two ends so there it is application insights connection string so i'll just go ahead and close this one i'm getting a bit confused here with that open and our app should still work if we go on the left hand side here to remote explore it didn't give us the prompt here notice that this little red thing says all right you're blocking this popup continue blocking no always allow us to pop up on this and i'll just click the uh a little world button to see that it's running so it's still working we're going to go double check to make our instrumentations working with our new connection string that's being passed through as an environment variable so on the left hand side go back to application insights and we'll give it a moment here below we'll go back to viewing the data on the left hand side we're going to go to live metrics it says it needs to reconnect we'll hit refresh here if it connects then it must be working so refresh refresh refresh go back the data is coming in so therefore it is working so we are in good shape here so um our application is configured correctly but it's not actually deployed um because we are passing the invite like we're passing our own environment variables in so let's actually get it deployed to um app services so what we'll do is click back to get back to our app services which is here nope oh yeah this is it says app service that's good on the left hand side we want to deploy it so we'll go to deployment center and what we'll do is we'll choose the code and we'll go to github and from here we need to authenticate so omek's my personal account that's just the way i get to it and so i need to select an organization it's exam pro i need to select a repository this is going to be my app insights that's the new one we'll choose the main branch it knows what the runtime is and we'll go ahead and save that so as soon as we do that it should automatically start deploying it's using github actions as the build provider so if we go back to the repository here we go to actions we can see we have one queued up and so this should run on its own we don't need to do anything so all we're doing is waiting for this to deploy and once it's done then our application should be deployed to the our url and then we'll check to see if it works and if it works then we are in good shape and we're done the basics of application inside so i'll be back here in a little bit okay all right so it's done i was waiting here for quite a while but i think it was done in a few minutes you just have to hit the refresh here so you know don't be afraid to go up here and hit enter and and see if it's deployed but now that it says it's deployed i mean this url's on git pod so that's no good for us what we'll do is go back to our application all the way top to the overview tab or blade i should say we'll go to browse and see if it's deployed so we're hoping we see express js the first load might be a bit slow so we'll give it a moment to think it sure is thinking hard and we'll just close it again and reopen it we are using the free tier so i'm not sure if it's like spinning up the instance that's probably what it's doing it's probably um spinning up uh the whatever the underlying environment is to get it running so it is running here now the next question is is it tracking so we'll go over to application insights and we'll go back to viewing our insight data and we will go all the way down to alive metrics it says it's not connected we'll hit refresh give it a moment here see if it finds any data we'll hit enter there we go so we're having a couple requests in there so uh we have application sites working and it's deployed so we're in pretty good shape here we're going to keep this environment around because we need it for a few uh follow follow alongs here but if you want you can tear this down but you'd have to do all the setup again but you can see it doesn't take too darn long but there you go hey this is andrew brown and we're going to take a look at sampling with application insight so this is a very short follow along but very useful for you to know so we have this application and what i want you to do is open it up in git pod you should know by now how to open things up and get pot and we'll uh give it a moment here and we're going to configure this to do sampling so sampling is when you don't send every single trace to application and sites or every single request information and the reason why you'd want to do this is basically to save money because if you had all your data going there it just it's just too much information right so um and that cost will stack up a lot faster so you know applications might send only 50 requests only 22 percent of requests it's up to you to decide how much data you need to have something that's accurate but you definitely don't need all of it so i'm taking the start off the end of here and i'm just going to have app insights start here so it is set here and then what i'll do is do app insights app insights i think we should do before i think in my code in the code example i have it in the wrong order but i don't think it even matters but we'll do client config it might matter actually sampling um percentage equals 50. so we're saying only sample 50 of the time i'm just going to double check make sure that spell right app insights default client config sampling percentage equals 50 and so now if we if we set that there and i'm not going to uh i'm not going to test it in the albums go test it and get pod here but what we'll do is we'll go back to our application into the application sites tab view that we'll go to the um live metrics and if we go back over here and stop our application start it back up okay and we go here this should connect and so what i want to do i don't know i'd say it's not supported it's definitely supported we'll hit one two three four and i'll go back and notice that i hit four but i only have two requests so that's why we're only getting 50 of it because it's going to cut out 50 50 of the requests and i think yeah it should always do that so that's all i wanted to show you because that is something that is a very basic configuration that you should know for application insights uh but we're going to keep the environment around because we have a few more things to do but yeah there you go hey this is andrew brown and we are going to look at usage analysis with application insights so usage analysis allows you to get more rich information about the usage of your application so i'm just here in application insights for our app that we've been using if you go to the lefthand side and scroll on down you'll see this usage tab so we have users sessions all this interesting stuff it allows us to instantly create a cohorts which are extremely useful for visualizing information if you've never seen a cohort before like user cohort i don't think it matters what cohort we look at but i just want to show you an example of one so this is a cohort and the idea is that if you have an application you want to be able to see whether users are coming back every single day right so you go okay this person was here on day one and the same person came back or 23 of people came back in the second day uh 18 of the people came back in the third day and it gives you kind of a map of um of course very or like information about your user behavior so very very important very useful for your application and application insights allows you to collect that information now um what we're going to be doing here is we're going to be instrumenting uh the client side because uh we never we instrumented the server side when we installed it into the our back end server code but we never did in the front end so that's the first thing we're going to need to do so i just need the the code snippet to do that so um what i'm going to do i'm going to go back to our repository because we're going to need that open here so we'll go back to my my app insights i'm going to open that up and get pod while that's going i'm going to type in application insights insights instrumentation node.js and we should easily be able to find the code for this here i want a clientside though i'm just looking for it so because there's all sorts of pages here so it can be a bit well let's say usage how about application insights um usage usage analysis here's the page and that's the code we want so it's on this page here this is an analysis with application insights so i'm going to copy that code and that's going to go into i'm going to make a new workspace here that's going to go into the front end of our website so somewhere in the index.html or if they're called jade files and it'll go into there so it is a uh oops jade file it's just opening the browser for us here so what i'll do is go index jade is just this syntax language that is used by um this configuration of express.js i'm going to go ahead and paste this block in here and we actually have to format this a bit because jade is not going to like this so what we'll do is type in script here at the top this is how this language works for javascript tags we'll do parentheses type equals sql quotations text javascript okay and then from here what we'll do is just go ahead and select this okay and i'm just looking where we should put it so they probably want it in the head so we'll put it right here and um yeah so that's fine that looks good to me we're also going to need um to create some kind of cohort so what we'll do i'm going to just commit this add client side tracking okay and so we'll stop our app start it back up okay and back over here i just kind of want to show you where you can set up we're not going to be able to see a real cohort because we need multiple days to collect information but let's just kind of play around as if we do have the data so over here on the lefthand side we'll go to sessions and it should tell us how many sessions we have now there was no information showing it before but now we're seeing two okay so um i'm not sure if it's counting both the server side and the client side but we are getting data so that's good and if we went up here to create a cohort we have a template gallery so we could choose a variety of different ones so we might want to try a blank sessions cohort sessions are people that are connected to the application users are identified users um and so you know we can add parameters to filter things down but we don't have to do anything we can also like run the query to see what information we get back but we'll go ahead and just save this cohort so we'll just say my cohort whatever you want to name it doesn't matter because we're not going to see anything interesting anyway and so this is our cohort but you can see there's nothing interesting so if we want to see something that kind of looks like a cohort what we can do is make our way over to workbooks i think yeah over here i'm not sure if this is just the same thing uh but in a different place but if you go down below here they have usage like active users and things like that i'm looking for retention so maybe cohort analysis and so down below you can kind of start seeing a table here right of what that looks like so i find the workbooks a lot more useful than the usage tabs but yeah i just want you to know about this usage analysis because it's super useful that logins or application insights has that functionality so you know hopefully that is good enough there but that is all we're doing here so i'm just going to go home close these tabs and i'll see you in the next one okay hey this is andrew brown and this follow along we are going to do a bit more with application insights by putting in custom events which is very useful for tracking so what i want you to do is open up application insights for our project that we've been working with we'll also have to go back to our get github repository and open that up in git pod as uh you should know how to open git pod by now and we'll give it a moment there to load and we'll enter in some code so we do have server side clientside tracking we did the clientside tracking in the uh usage um the usage section there but if we expand this here we're going to want to put something in the index jade file and so here what we will do is add a new script tag so that's going to be script this is jade language i don't care if you learn it but you just got to follow along for this purpose coding is not the lesson here but we have text javascript that allows us to do some javascript there and i don't know why it's saying tab when i type that is not very useful for me but i'm just going to hit do space space and type in app insights track event we'll auto complete that there we'll do parentheses curlies name double quotations client count and so every time this page loads it's going to do track event with this name and that's going to count up it's called client count that's how we know it's on the client side so that's the index page there and so now we need to do server side so there's a few things we have to do here we've got to go to app.js and in here we need to set a variable that's going to get passed along so we'll go down below here and do app dot set parentheses app insights and this is specific to express.js so again i don't care if you remember how this works but just a way that we can pass a variable that we're going to access in our routes so we go app insights default client and i'm going to make sure we have that yeah it's up here so we're passing this in here so that's good we'll go over to our routes and in our route center index page we need to add app insights we'll do app insights and we're going to grab it from that app variable we just set so we do app request app get app insights and then we'll track it it looks exactly the same as the other one we are using javascript on the front a front end and the back end so it makes things a little bit easier and this one's going to be called name double quotations single quotations doesn't matter server count okay so now we have it serverside clientside i'm going to go up here just commit my code client and serverside tracking we'll commit that and what we will do is we'll do an npm start to restart up the application now that we have both sides of it we do have a problem here on line eight of index uh index index jade file so we'll go here take a look at what it's complaining about um does not like something so i'm just taking a look at my my code off screen here it's lining it it's this line it doesn't like so why not um hmm i mean i don't see a problem so what happens i'm just going to copy my my my oh you know what it's indentation see where it says block content it's supposed to be this level here that's probably the problem so we'll stop it we'll start it we'll give it a second it's still crying it's a big old crybaby here but that's okay we'll work through it now i hit tab it's actually tabbing properly because i wanted to do tab before i don't know why it was uh doing that still doesn't like it so i do have the code off screen here so i'm just going to copy paste it i mean you'll have if you're on my platform you'll have it so you just say allow oh there's a period on the end of course can barely tell that was there okay so we'll save that again i don't care if you learn jtemplating it's not a skill you have to remember so that is there i'm just going to keep hitting enter because i want to observe this custom event data so we go back over here into app insights and we should be able to see this somewhere um this would be under where this would be under somewhere i'm trying to find it it would be under which blade huh i didn't write it out of my instructions so we'll have to click around here to find it um i think well it's event data so it's probably under events yeah that's where it is now i'm remembering and we what we can do is filter this down and say any custom event here if we scroll on down we'll go view more insights i don't know why they have a little button there to get that information but we scroll on down we have server accounts the server account is definitely working we don't know why we don't see client count so what we'll do is go back here and we'll right click and see if there's any errors there is an error app insights is not defined and so it should be defined in the head because we have this script from before right you know what we didn't put the instrumentation key in so that's not good because in the last follow along we were supposed to do it but it didn't really matter because we didn't have any data to look at so that's why we didn't notice so we'll go back to our overview and we'll grab the instrumentation key here go back to our code and we will go up to app.js and this is client side so it's a bit harder to pass it in as environment variables we're not even going to try to do like to do that because it's the instrumentation key it doesn't matter and we're going to go back to the uh layout here and down below we'll just replace that okay we'll stop start it hit refresh right click to see if it's working now let's give it a refresh here still saying app insights is not defined which is not true but there is an error here we'll click and do it it's not showing me the line which would be nice okay so we'll go back here and there's something wrong here it doesn't like something you know what it's probably missing the period there we go i can't believe we never noticed that before so fixes i'm very confident that is the fix so we'll go back here inspect refresh already lost that let's do inspect again no errors here that's good we'll hit refresh refresh refresh refresh refresh refresh refresh we'll go back to application insights and we will go back to events if we can find it there it is and i'm going to go down to any custom event scroll on down more insights and it might just need some time to propagate because we're not getting any errors now so it must be working if it didn't track it it would give us an error client client count so it is showing up so it is counting now so that's how we do custom event tracking so there you go and for the most part i mean the only thing we didn't look at was applica application map take a quick look here this would show you like connections in between stuff so here we have like the web to the client if we set up other things like a database and stuff like that we would have something more interesting that's more of advanced thing and you know maybe i'll do that in another follow along but this one is done so let's go ahead and clean this all up you can delete this repository it's no big deal if you're worried about the connection string you can delete that whole yeah the repository but since we are tearing everything down it's not going to matter and i know we replaced it as an environment variable but remember it's in the git history so someone went back it got access to your github repository it went back in time they could find it but um what i'll do is i'll go to resource groups we'll look for our app insights here and we will hit delete resource group we'll say my app insights go ahead and delete delete and there's our cleanup so there you go that was application insights so azure monitor is uh quite a beefy service there's a lot going on in it and so it's a comprehensive solution for collecting analyzing acting on telemetry data from your cloud and onpremise environments so here's an example of one of the things it can do which is provide you a visual dashboard but it can create smart alerts automate actions log monitoring and a lot of different things and many azure services by default are already sending their data to azure monitor so you can use it right away now i just want to introduce you to the concept of the pillars of observability this isn't a thing specific to azure but it's something that in devops that you need to understand and it's just going to help you contextualize the offerings of azure monitor and other cloud service providers so what is observability that is the ability to measure and understand how internal systems work in order to answer questions regarding performance tolerance security and faults with a system and application and in order to obtain observability you need three things you need metrics logs and traces and you have to use them together using them in isolate does not gain you of observability so let's go define those three things the first thing is metrics and that is a number that is measured over a period of time if we measure the cpu usage and aggregate it over a period of time then we would have the average cpu metric then you have logs these are text files where each line contains event data about what happened at a certain time and then you have traces and this is a history of requests that is that is that travels through multiple apps and services so we can pinpoint performance or failures and i like to make the joke that it looks kind of like the triforce of observability once you've constructed it there at the end let's look at the anatomy of azure monitor which is a little bit complex but i'm sure we can work our way through it so the first thing is that we need uh sources of data so these this is what uh what data can be sent into azure so you probably want application data operating system data uh data from azure resources at your subscription level your tenant level which is going to be associated with active directory and custom sources and once you get those into azure monitor you have to store them somewhere you're going to put them in logs and monitors and these are just data stores that are within azure monitor that you're going to be able to work with and once you have that data in there you're going to be able to leverage different services and do different things and so we will call these functions so you can perform insights visualizations analysis response and integration so we'd look at insights uh we're getting insights into our virtual machines our containers our applications uh for visualization you might be making dashboards you could be using power bi or creating workbooks for analysis you might be using the log analysis or the metric analysis tools for responses you might want to create alerts or start auto scaling and for integrations you might want to use logic apps or export apis to connect things up so there you go so now let's take a closer look at all the different sources because this is going to really help us understand the utility at each level and i think that makes everything very crystal clear so remember that list of sources where we went to application to custom sources we're going to start at the top of that list and look at application code which is all about performance and functionality of your application and code so that's like collecting traces application logs and user telemetry and so here is the visual representation and so on the lefthand side we have our sources in the middle we have our storage and the righthand side we have our our services that we're going to be doing things with that data so looking on the left you're in your application you'll probably want to go ahead and install the instrumentation package this is going to allow us to collect data into application insights and application insights is a application for getting rich data about our applications then you have availability tests this tests the response responsiveness of your application from different locations on the public internet and that's really useful if you're saying like if you have your application you're running it in the east of canada and the west of the us and you want to make sure that the response times are the same or lower that's going to help you get there for your metrics this is going to be describing your performance and operate and operations uh and custom metrics of your application for your logs you're going to be storing operational data about your application such as page views application requests exceptions and traces and then the things you're going to be putting in storage is you're going to send the application data to your azure storage for archiving or what you'll do is you'll store your your availability tests if you need to go and analyze them further or you could create a debug snapshot of your data so that you can then go debug that at a later point okay so let's take a look at how we're going to monitor our operating systems and this is for the guest operating system not the host operating system uh so when you're dealing with virtualization um uh the vm has its own os but the actual underlying hardware has its own os and that's the host one and that's not what we're looking at you don't need to monitor that one that's up to azure to do or your cloud provider we're looking at the guest one the one we actually can control so the operating system is going to need a couple of tools installed so or agents so we'll want to install the log analytics agent so we can use that with log analytics we'll probably want to install the dependency agent that allows us to monitor processes on the machine so those are the programs that are running so like you know let's say mysql redis maybe you have a ruby on rails app whatever um and so that's that and i just want you to know that these agents can be installed anywhere so it can be installed on azure on premise or even on other cloud providers like aws if you want performance counters you're going to have to install the diagnostic extension which is i think a good thing to do and just to make note the log analytics agent is going to store that data into logs then you can use that log analytics later if you want to have the or store store the health state information then you're going to be using the azure diagnostic extension and they're going to put that in azure storage and if you want to use azure event hub so event hub is a way of connecting your app to other destinations you're going to stream it but you're going to need the diagnostic extension for that all right let's take a look at azure resources and how we're going to monitor those so um you're going to have resource logs and these provide insights to internal operations for azure resources and these logs are automatically created for you but you will have to set up diagnostic settings to specify destination for them to be collected for each resource for metrics you're going to get those automatically you don't have to do any additional work to configure them and you're going to be able to analyze those in metrics explorer for your log data you're going to use log analytics to look for trends and other analyses you can also copy your platform metrics to logs for analysis and uh your log resource logs are going to be archived with azure storage for longterm backup if you want to uh use um or or send or trigger uh or send your data to other destinations you can use event hubs that's generally what event hubs is for so there you go taking a very quick look at how we monitor our azure subscription this is going to be for uh the the service health of different resources that you're using like are they okay are they running and things about azure active directory so that's about it i'm going to take a look at our azure tenant and how we would monitor that and if you remember our azure ad section that's where we define tenant because tenant is highly coupled to active directory and so this is going to be for tenantwide services such as active directory so reporting that contains history of sign in activity audit trails of changes made within a particular tenant things like that so there you go so last on our list here is custom sources and basically if if none of the other previous categories fit where you want to collect data uh this is where you can just collect data using the azure monitor api and so basically you are using a rest client and then it's going to store it in analytics at log analytics or azure monitor so it's just really custom data custom storage so there's two fundamental types of data that we care about when working with azure monitor and that is logs and metrics and so azure has two services to deal with that one is called azure monitor logs and so this service collects and organizes log performance data for monitored resources data logs are consolidated from different sources into workspaces which we'll talk about platform logs from azure services is something you'll collect it'll collect also log performance data from vm agents the usage and performance data from applications also can be consolidated and these workspaces can be analyzed together together using a query language which we definitely cover in this course and you work with these log queries and the results interactively using a subservice called log analytics which is something we'll definitely be covering then there's azure monitor metrics so this is for the metric side so collect uh numeric data from monitor resources into time series database metrics are numerical values collected at regular intervals and describe some aspects of the system at a particular time uh lightweight and capable for of supporting near real realtime scenarios useful for alerting and fast detection of issues and you can analyze them interactively using metrics explorer and we cover that as well so really these two services are like the data stores or the databases for the respected types of data and then each of them have the sub services which is for exploring and some other additional services but let's get into that more detail now so when you use log analytics you're going to probably want a workspace and this is a unique environment for azure monitor log data each workspace has its own data repository and configuration and data sources and solutions that are configured to store their data in a particular workspace so it's really interesting because if you go over to azure monitor you can use uh log analytics without creating a workspace but i believe that if you want to isolate a lot of your data and install like maybe like collect it outside of azure services for other things you're going to need a workspace and it's going to also have a lot more robust options so that's something you're going to end up doing is creating workspaces and it's a good habit to do but there's not a lot to talk about there but let's move on to actual the query language which is really the meat of log analytics i want to quickly touch on log analytics and so this is a tool used to edit and run queries within azure monitor logs and so the idea is it kind of looks like something you use to connect to a database because it really is structured like a database with tables and columns and things like that and it has this its own query language called a kql and the idea is that you input your queries and it's going to output results for you and that's something we're going to look at in greater detail is that kql language so that we know how to use that panel let's go talk about workspaces let's take a closer look at custo and it's query language so azure monitor logs is based off of the azure data explorer and along came with it is the cousteau query language also known as kql and this is the way we're going to uh filter and uh sort and do things with our logs so kusto is based on a relational database management system and it supports entities such as database tables and columns there's also this thing called clusters and uh kql actually has a lot utility in azure because it's not just in monitor logs and data explorer you can use it in log analytics log alert rules workbooks dashboards logic apps powershell azure monitor log apis so it's definitely something you can be using across the board in azure and so they have some basic operators they have lots of operators that you can use so you can do calculated columns searching filterings on rows group by aggregates join functions and we're going to be looking at a lot of the operators in more detail after this slide here but anyway the queries execute in the context of a cousteau database that is attached to acousto cluster and we will talk about clusters database tables and columns up next let's take a look at what makes up something for cousteau and so we have a bunch of entities here clusters database tables columns and functions and so i have this nice visual to help us uh kind of see how they all work together so the top we have clusters and these are entities that hold multiple databases you can also have multiple clusters but it's just not being shown there in that graphic then you have the databases themselves these are named entities that hold tables and store functions you have tables these are named entities that hold data and a table has an ordered set of columns and zero or more rows of data each row holding one data value for each of the columns of the table then there are the columns themselves and these are named identities that have a scalar data type columns are referenced in the query relative to the tabular data stream and that is the context of the specific operator referencing them then we have stored functions and these are named entities that allow reuse of cousteau queries or query parts and then you got these external tables and these are tables that uh live outside of your cluster i think that uh you're referencing them from uh storage accounts and they're in blob storage so they pro i think there could be like csv files and stuff like that but these external tables are used for exporting data from cousteau to external storage so storage uh storage accounts as for querying external data without ingesting it actually into cousteau so hopefully that gives you an idea the lay of the land there let's take a look at the type of data types that we can use in cousteau but let's just define what scalar and data types are so scalars are quantities that are fully described by a magnitude or numerical value alone so the idea is that it just means like a single value then you have data types and a data type defines how a piece of data is interpreted so an integer number could be a data type if you've ever used programming languages like data types and stuff like that or an sql database you should be familiar with those so incusto data types are used for various things so they can be for columns or function parameters that expect specific data types uh that they want them to be and so let's just go through the quick list of data types because there are quite a few here and i've summarized them quickly so the first is billion and this represents a true false value then you've got date times your date and this represents a date and these are stored in ut utc time zones you got decimals these are numbers that like are 12.88 or something with a decimal in them you have integers which is like a whole number then you have longs which are also integers but with a greater range you have your guids or your uids and these are unique values so it's like a random hash there that you have and the idea is just so that you have unique values but they're not like one two three four five so people can't guess the size of your tables then you have reels these are double position floating point numbers so these are really really big num numbers something you probably use if you're doing um things with uh finance or numbers then you have strings and these are unicode strings and they're limited by default to one megabyte and the way you do them is you have them wrapped in quotations we have that hello world you have time spans these are interesting these are little time intervals represented so like 2d would equal 2 days 30 m would equal 30 minutes one tick would equal 100 nanoseconds and they have a variety of those then you have dynamic and this is a special type that can do a bunch of things so it can accept primitive scalar data types so the idea is it's kind of like a data type that can accept any value or it can accept an array of any value or it can accept a property bag which you know if you're from the javascript world looks like a json object so you have a key and value and it can even be nested and then last which is a data type but you're not setting these as your columns which is the null and that's a special value that represents a missing value and any of these data types can be a null value so like you look at boolean it can be true false or null and date time can be a date time or date or null and that goes for all of them let's talk about cousteau control commands this is really part of the uh kql language but it's a way of kind of working with the the databases and tables and stuff like that if you've ever used postgres and you've done like forward slash du or you have these commands that didn't really have to do necessarily querying but like managing the databases and tables and stuff that's what uh control commands are so i'll give an example so here we have a control command to create acousto table so they always start with a period so period create table logs and then it makes two columns and that when you're using the query um like log analytics and stuff like that you just type period and then start typing it and it'll show you all the list of control commands it was really hard to find the documentation a full list otherwise i would have picked some out for you but generally you can just kind of explore that way or the most common ones you're going to come across is all going to be in the documentation even though there's a very long list of control commands a very common one you're going to be using a lot is show so for example we can do dot show tables and then count the number of tables there are and so i just want to show you that if you type dot show it'll show you what it can take as a second parameter and there's a huge list there so just an idea like how you can work the control commands take a look here at custom functions and these are reusable queries or query parts and kusa supports several kinds of functions the first being stored functions which are userdefined functions and these are stored and managed as one kind of database schema entity then just to define user userdefined functions uh one step further they come in two categories we have scalar functions so these uh have input scalar data types and output scalar data types and then tabular functions so these uh take input tabular data and they output tabular data tabular data is just when you're working with multiple rows in a table then you have query defined functions which are also user defined functions and uh that are defined and used within the scope of a single query so very similar to stored functions but they're it's all based on scope okay uh then the last on our list here is builtin functions which are hardcoded and these are defined by cousteau and cannot be modified users and they just give you a lot of utility let's take a look at some of these builtin functions starting with special functions so select cousteau entities so you might want to say i want to select a cluster and then i want to select a database and then that table then you have aggregate functions which perform calculations on a set of values and return a single value so maybe you want to do a count so you see where it says count and parentheses then you have windows functions and these operate on multiple rows and records in a row in a row and so one really popular one is row number and so this could count the number of rows maybe you have to figure out the row numbers in relation to your query and so that's something you could do there let's take a look at some scalar operators and there's quite a few here and these are for working with scalar data types to do comparisons and there's a bunch of different categories so let's just uh quickly go through them here the first one one is bitewise operators that's where you're working with zeros and ones and flipping them around to do different things so you've got the binary end the not the or the shift left the shift right uh the xor and these make a lot of sense if you know how to work with binary math if you don't don't worry about them but i just want you to know if you do they're they're there for you use next are logical operators so this is what you're going to be more familiar with like equality so equals in equal and or or so this is pretty darn simple then you have date time and time span uh arithmetic and so we have add or subtract date times you can do that you can add subtract divide or multiple time span so add one day plus two day makes three days uh yeah then you have numerical operators and these work on inch longs and reals so you can add subtract multiply and divide you can do module where it's determining whether it's a whole number or not so or divisible by a whole number so like let's say you have 17 and you put it through there you say module 17 by like modulus 2 if it's not divisible by two like perfectly resolves into zero it will return either zero or one then you have less greater equal not equal uh less or equal than uh greater or equal so you got the idea there equals to one of the elements so that's in so you do the in it's very familiar if you're used to using sql then you have the opposite where you put an exclamation in front of it actually there's a lot of extra operators that just have the same thing with an explanation to mean the opposite especially with string operators so here's a bunch of string operators and they almost all have exclamations uh a variant with explanation in front of them then you have your between operators this matches the input that is inside an inclusive range so you can say between 1 and 10 or between these date times so there you go let's take a look at cousteau tabular operators and i really want you to pay attention to this one because this is where all the power of kql happens so these perform comparisons against a bunch of rows that's why it's called tabular operators and there's a lot of them and we're going to look at the most common ones because it would take me forever to go through all of them and you're not going to remember them all so let's just look at the ones we actually care about working with so the first is count this returns the count of rows in the table then you have take this returns up to a specified number of rows of data you have sort this uh will take the rows of the input into the order by one or more columns you see buy damage property descending so let's just sort then you have project returns a specific set of columns and where filter a table to subset of rows that satisfy a predicate before we move on the next slide i just want to point out like if you've worked with sql these are very familiar right like take is like limit sort is like order by pro project is like select okay moving on to the next one here we have top returns the first and records sorted by uh specified columns this is kind of like a shorthand i think it like makes uh take and sort and just abbreviates it to one line you have extend creates a new column by computing a value so notice that it says duration equals n time minus start time and then we're using duration somewhere else then you have summarize aggregate groups of rows that's kind of like group by in regular sql and then render so renders results as a graphical output and to me that's a really cool one moving on to metrics we're going to be looking at metrics explorer so message explorer is a subservice of azure monitor allows you to plot charts visualize correlating trends and investigates spikes and dips in metric values so the idea is you can create a really cool graph like that and you can make a variety of different ones based on how you want to chart it out let's talk about how would we define one of these metric visualizations in metric explorer so you got this cool bar and the idea is you got to fill it all the way to the end and then that will visualize it for us the first is the scope so you're going to open this up and it's going to allow you to um select resources so it'll show you like subscription and stuff like that and resource resource groups but always makes you select a resource at the at the end and some services you can select multiple resources and some you can only select a single instance so the storage account there it's only going to be a single one there so i had one called davestrom institute i made and then there you're going to choose a namespace so this is a specific group of metric data within a resource so notice like it makes sense for a storage account that would show account blob file queue table it's going to vary based on your service then you actually have the metric you care about so we have availability uh aggress ingress etc and a bunch of other things you're going to choose one of those and then you choose how you want to aggregate aggregate it so average min max etc and again this is going to be totally different based on what resources you choose but that's generally how it works let's take a look here at azure alerts and this helps us be notified when there are issues found within the infrastructure or application and this allows us to identify address issues before the users of your system notice them and so they come in three flavors we've got metric alerts log alerts and activity log alerts and when alert is triggered you can be notified or have it take action so here is kind of the anatomy of an alert and we have the alert rule this defines who we should monitor like the the service and uh uh like the the definition of when it is triggered which is going to be the next part here so a resource such as a virtual machine designated as the target resource will emit signal so it's going to be emitting a data payload and it could be of the following types could be a metric a log activity log application insights you can kind of see how that ties to the types of alerts then you have the criteria or logical tests this gets evaluated and determines are we in a triggered state it could be like percentage cpu greater than 70 percent then you have your action group which contains actions um and those actions uh will be uh performed when it is triggered and actions could be things like run uh run an automation run book use azure functions itsm logic app web hooks or secure web hook on the other side there we have um this box over here and this is all about the state of your alert and so we have monitor conditional alert state so monitor's condition is set by the system an alert state is set by the user but the idea that is there is so you can define where it is because you might want to have a history of saying okay i've resolved this issue so i'm marking this as closed and that'd be like an alert state there and so there you go this is probably not going to be on your exam but it's super cool to show and it's only one slide away so let's talk about it which are azure dashboards these are virtual workspaces to quickly launch tasks for data for daytoday operations and monitor resources and build custom dashboards based on projects tasks or user roles so the idea is you can go in here and you have like this little tile uh editor and you can drag stuff over so you can see i can build like a video in and a link to the help support and put a clock and some metrics i care about and some markdown so it's a really good way of building out these customized dashboards based on a user role to really help you focus on what you have to deal with within your infrastructure on azure let's take a look at azure workbooks just because this is in the scope of azure monitors so we should cover it so workbooks provide a flexible canvas for data analysis and the creation of rich visual reports within the azure portal to allow you to tap into multiple data sources from azure and combine them into a unified interactive experience and so the key word here is story it tells a story about the performance and availability about your applications and services so this thing kind of looks like a dashboard but it isn't and it's highly customizable but it's not very clear in here but really what it is it's like it's like the form of a document and the idea is that imagine that you have a document and then you can embed analytics in it that are realtime analytics so that you can visualize uh uh and and kind of investigate and discuss performance and availability and stuff like that you can kind of think of it like if you ever worked with jupiter notebooks but it's really for performance and monitoring um it's kind of like that okay and these things are highly customizable and really useful and uh if you ever use datadog databa dog has its own thing i think they call them notebooks as well they have workbooks they're called notebooks it's the same thing but it's a really great tool for really understanding your your performance hey this is andrew brown from exam pro and we're looking at azure monitor cheat sheet this one's a three parter so let's jump into it azure monitors comprehensive solution for collecting analyzing and acting as telemetry for your cloud onpremise environments and even though i don't have it in the slide here it is an umbrella service i mean there's a lot of things underneath this and that's why we have three pages of information and create visual dashboards smart alerts automated actions log monitoring to obtain observability you need to know metrics logs and traces you can have use them all together using them in isolate but that does not give you observability metrics is a number that is measured over a period of time logs as a text file that contains event data which was happening and traces is a history of requests that that travels through multiple apps and services so we can pinpoint performance or failures azure monitor collects two fundamental types of data from sources logs and metrics which kind of matches up with our theoretical information here right so azure monitor logs collects and organizes log and performance data for monitoring resources data logs are consolidated from different sources into workspaces platform logs and azure services logs and performance data and virtual machine agents usage and performance data for applications can be consolidated in a workspace you can be analyzed they can be analyzed together using a sophisticated query language which we'll talk about here in a moment or review i suppose um work with log queries and their results uh interactively using log analytics azure monitor metrics collects and that's the second part here right so the two fundamentals azure monitor metrics collects new numeric data from monitored uh resources in a time series database messages are new numerical values collected at regular intervals and describe some aspects of a system at a particular time lightweight and capable of supporting nearrealtime scenarios useful for learning and fast detection of issues you can analyze them in interactively via the metrics explorer onto page two log analytics is a tool in the azure portal used to edit and run log queries with data in the azure monitor logs login elixirs use the language called q kql log analytics uh i think that's cousteau right so log analytics workspace is a unique environment for the monitor logs data each workspace has its own data repository and configuration data sources and solutions are configured to store in their workplace and i really wish i put the word data like in here it's not important for uh the associate but for other exams it helps you understand if you think of it as a data lake azure monitor logs is based on the azure data explorer and log queries are written using the cousteau query language kql can be used in a log analytics log alert rules workbooks azure dashboards and all over the place kosovo is based on a relational database management system so you'll see database tables and columns some query operations include calculated columns searching and filtering on rows grouped by aggregates joint functions if you're used to using sql you know what i'm talking about cousteau queries execute in the context of some cousteau database that is attached to acousto cluster crystal is generally composed of the following entities clusters database tables columns functions let's talk about them quickly here clusters are entities that hold databases databases are are named entities that hold tables and stored functions stored functions are named entities that allow reuse queries or query parts tables are named entities that hold data columns are named entities that hold scalar data types and then you have external tables which are identities that reference data stored outside the cousteau database these both count as tables that's why it's not in the list up here but generally these are pretty selfexplanatory metric explorers is a subservice of azure monitor that allows you to plot charts visualize correlating trends and investigate spikes and dips in the metrics values to visualize the metric you need to define the scope the namespace the metric and the aggregation we're on to the last page here alerts and notify you when issues are found within your infrastructure or application they allow you to identify address issues before the users or system notice them azure has three kinds of alerts metrics logs and activity log alerts sorry there is a diagram if you remember our follow along i actually break down all the uh structure of an alert you should go review that i didn't put in the cheat sheet because it just would have been too much here azure dashboards are a virtual workspace to quickly launch tasks for daytoday operations and monitor resources azure workbooks provide a flexible canvas for data analysis and the creation of rich visual reports within the azure portal it tells it and just to highlight what azure workbooks are for it tells the story about the performance availability about your applications and services then we have application insights and this is an application performance management an apm service and it is a subservice of azure monitor that's why all these things are under azure monitor automatically detects performance anomalies includes powerful analytics tools to help you diagnose issues to understand what users do with your app designed to help you continuously improve performance and usability works for apps on the dotnet node.js java python hosted on premise hybrid and public cloud works everywhere basically integrates with your devops processes can monitor and analyze telemetry from mobile apps by integrating the visual studio app center to use application insights you need to instrument your application to instrument you need to install the instrument package sdk or enable application insights using application insight agents when supported apps can be instrumented from anywhere when you set up your application insights monitoring for your web app you create an application insight resource into azure monitor you open the resource in the azure portal to order in order to see an analyze telemetry uh collected from your app and last the resource is identified by the information instrumentation key the ik so there you go hey this is andrew brown from exam pro and we are taking a look at azure api management so this integrates existing backend services into modern api gateways and uh this service uh there's a lot to it so we're going to be doing quite a bit here maybe more than we have to but it is a very powerful service especially if you're trying to be a developer on azure so we're going to just make sure we spent a good amount of time with it so it follows the api approach of decoupling the front end and backend teams with the help of api mocking the azure api management handles the full management of your apis it centralizes the securing versioning documentation and compliance from your backend services in a single endpoint so very powerful tool but let's get to it so let's go over uh key concepts or key components of api management i do not have a fancy visual for this it's just very hard to visualize but uh we will learn all this stuff as we go through it so let's talk about what we have here so we have an api that represents a set of operations api operation which connects an api endpoint to its back end we have a product which is a logical grouping of apis a single or group of apis make up a product so this is how your apis are presented to developers so it can either be public or private we have a backend that represents backend services in your api there are groups these are used to manage the visibility of products to developers such as administrators who have full access to the api management developers for users with access to the developers portal with permissions to build applications guests users without access to the developer portal portal but with the reading permissions and some services there's the idea of developers so this belongs to a product group and each developer has a primary secondary key to call that products api there are policies uh configurations and validations that are applied in progress to incoming requests and a com responses which you will see in closer detail in the upcoming slides there are name values so these are key value pairs used with policies values can be a result of an expression uh there are gateways so this is where your api calls it and are received and policies are applied to incoming requests then there's the developer portal so this is where developers can access all the apis and products listed by your apim alongside the api's operations documentation developers can also request access to your apis from the developer portal but again uh we'll figure this out as we go through okay all right let's take a look here at the echo api service so when you create uh apim gateway that's the first thing you'll do you'll get by default uh this thing called echo api and i was like whoa what is this thing i wasn't sure what it was and i thought it was kind of interesting that's why i expand upon it here which i didn't really see much of it in the documentation so i had to piece things together but the idea is that echo api provides a bunch of existing endpoints and these are mocked endpoints to a nonproduction azure service used to test azure api management so you know if you don't have an api yet but you want to interact with it it's going to go to the service called echoapi.cloudapp.net forward slash api and again it's just a dummy a dummy application to test against so that's what that is so if you see it you don't need it you can absolutely delete it but you always get one when you create a gateway okay all right let's take a look at the feature comparison for apim because it's one of those services where there's a lot of features but they're not always available to you depending on what plan subscription what kind of user you are so let's take a look at how the features change based on uh what plan you're using so across the top we have consumption developer basics standard and premium red is what's not available for that particular plan green is what is so for azure ad we do not have it for consumption or basic for virtual network support it's just it's just not supported for consumption basic and standard for multiregion deployment availability zones we're only getting that for premium for multiple custom domains premium as well and also for developer developer portal the builtin cache and builtin analytics are not available in the consumption plan if you want a selfhosted gateway better be on the premium basically almost developer always says yes because you have to play around with things right um so tls settings is for everyone external caches for everyone client certificate authentication policies are for everyone which is very generous back in restore you're not going to get in the consumption model and that's not all we got a few more here so if you want management over get direct management api azure monitor logs and metrics static ip or web sockets api you're not getting those in the consumption model now graph api graph graphql api is available for all of them so there you go all right let's take a look at api authentication so in order to authenticate with their apis we configure those settings under our subscription setting so it's as simple as doing a checkbox there so if this description is required only developers with a valid access key can use it and so the idea here is we can configure where the api will receive those access keys which can be sent as the header or query string so there are some options there if this thing is not checked then that means anyone that is anonymous has request requests will be allowed so it's as simple as having that checkbox there and having a key so there you go all right so let's take a look here at groups for apim so groups are used to manage the visibility of products to developers and so we can break these down into some broad categories we have administrators so they manage api management service instances and create the api's operations and products that are used by developers then we have developers they are authenticated for the developer portal the users that build applications using uh your apis developers are granted access to the developer portal and build applications that call the operations of an api then you have guests so these are unauthenticated developer portal users such as prospective customers visiting the developer portal they can be granted certain readonly access such as the ability to view apis but not call them so administrators can also create custom groups or use external groups in an associated azure active directory tenant to give developers visibility and access to api products a user can belong to more than one group so there you go all right let's take a look here at front ends and backhand starting with front end so front ends to find the route or end point and the documentation configuration around that endpoint so the idea here is we have front end and we have a single method here uh which is post forward slash resource and if we were to open that up you can see that we have i'm just getting my pen tool out here we have a description for our documentation the display name the name of i can't remember what the difference for the name is but it's called create resource we can set its url that's method and then down below we have additional parameters so api does not host apis but it creates facades for your apis that's a key thing to remember api so api management it does not host apis it's created facade for your apis so let's take a look at the back end so for back ends you can set the following types you can set a custom url so point to a server where your service is running you can say to go to azure resources integrate directly with a resource such as azure functions app service container wrap logic app i'm just missing the p there but it's two p's so you have the idea up here where we can see custom url azure resource azure service fabric okay we have some additional options here so we can authorize credentials that present requests credentials to the backend service there are options like headers so that's http headers we can fetch from named values we can query based on query string you can fetch from name values there as well you have control of client certificates so x 509 certificate certificates which we do talk about in this course which are stored in azure key vault which is the section that we're talking about there so you're just seeing those options headers queries and client certificates but yeah there you go all right let's take a look here at policies for apim so api management policies allow you to change the behavior in multiple stages of your endpoints request lifecycle you can update any part of the request response message such as the headers bodies urls and there are four areas where policies can be applied we have inbound for incoming requests backend before requests reach your backend outbound before sending responses back to the client and error when a request encounters an error just to kind of visualize it we have our front end incoming processing backend outbound processing and so the idea is that here for the inbound processing that's where we have policies apply that's the one they're talking about right here and then we have for the back end so before requests reach your backend fbm processing and then uh error is not visualized because it would be whenever an error would occur here okay azure has a collection of policy groups which contain many policies you can apply and we got a big list here we have access restriction policies advanced authentication caching crossdomain transformation dapper integration validation policies graphqls there's a lot of great policies that we can already utilize when an error occurs no other policies are applied except the error policies however other policies were in effect prior to the error they will not be removed product level policies apply to all api operations within a product so now let's go take a look at an example of a policy okay all right let's take a quick look here at a policy example so you're going to see uh this policy here and we're doing it right on the echo api so this is for an outbound policy to cache the response in a get operation i believe that all those policies are in the documentation you can easily copy and paste them in uh we do do a follow along for policies that was something i definitely covered i just don't remember if it was part of the system or i mean it must be because it says here retrieve resource cash but anyway the idea here is that um this policy stuff is in an xml language um not super important to learn what it is but the fact is is that there is a lot of available policies available for us so we can pretty much copy paste and figure it out from there if that is clear and here is kind of another visualization that i just want to show you relating to our policies here uh just because it kind of maps up my little graphic where we have the front end the inbound processing the outbound processing here but if you notice here it says policies i know it's really small but you can see it says base cache lookup rewrite url and here it says base and then cash store so the idea is you can see what policies are being applied there so there you go all right so what we're going to do here is look at all the possible policy groups that are provided by azure uh it might say policy groups i mean the policies within those policy groups so you get an idea of what kind of policies you can do to transform manipulate filter during a transit of a request from a response through apim and we're not going to look at all the code examples but i'll pull out a couple per policy group here we're starting with access restriction policy so the first is check http header so enforce existence or value of http header then we have limit call rate by subscriptions that prevents api usage spikes by limiting call rate on a per prescription basis that's pretty cool when you look at other providers like aws where they have api gateway there's like a fixed limit on there i don't remember there being any kind of policy to do that so i like how you have the flexibility there to choose hopefully it's on there by default in that base i don't know limit call rate by keys or prevent api uses spikes by limiting call rate on per key basis restrict caller ip so filter so allow deny calls from specific api addresses or add an address range that could prove very useful set usage quota by subscription so allow use allows you to enforce a renewable or lifetime call volume and or bandwidth quota on a per subscription basis set usage quota by key so allows you to enforce a renewable or lifetime call volume and or ban with quota on a per key basis validate jwt so enforces existence and validity of a jwt javascript web token yeah extracted from either a specified http header or a specified query parameter gwt is very common for authentication of clientside applications validate client certificates so enforces that a certificate presented by a client to an api management instance matches specified validation rules and claims let's take a look at that restricted caller ip very very simple we define ipfilter we have an action the ip address that is allowed in this case or a range so that one is again very simple these are not hard to figure out we have validate jwt um so there there is some stuff there and uh it is what it is okay um so we'll go on to advanced policy so here we have control flow so conditionally applies policy statements based on the evaluation of bool expressions forward request so forwards the request to the backend service limit concurrency so prevents enclosed policies from executing by more than the specified number of requests at a time log to event hub so sends messages in the specified format to a message target defined by a logger entity emit metrics so send custom metrics to application insights and execution mock response so aborts pipeline execution returns a mocked response directly to the caller retry retries execution of the enclosed policy statements and if and until the condition is met execution will repeat at the specified time intervals and up to the specified retry count return response so aborts pipeline execution and returns the specified response directly to the caller let's take a look at limit concurrency that seems like a good one to have so here we have an inbound then we define our back end we set our limit concurrency it's setting the forward request to a timeout of 120 so preventing close policies from executing by more than specified number of request time we have mock response that one would be very very useful uh you know like you just want to you know you don't want to have a real response you just want to put whatever you want in there so it gives it back a status 200 for application json more more advanced policies here so send oneway request send a request to a specified url without waiting for response send request send a request to specify url send http proxy so allow you to route forward forward request via http proxy set variable persist a value in that name context variable for later access so here's an example one where we're setting that variable so we're saying is it mobile and we want to know is it mobile so the idea is it'll take that value from that agent and then we'll have that variable available for us to determine was the request mobile we have wait so wait for enclose send requests get value from cash or control policies to complete before proceeding set request method allow you to change the hp method for a request set status code changes http http status code to the specified value trace adds custom traces into the api inspector output application insight telemetries and resource blogs looking at authentication policies and i don't know why that's highlighting oh you know i probably have a graphic above it that's why um so we have authenticate with basics so authenticate with basic service using basic authentication very common for test environments authenticate with client certificates so authenticate with a back back end service using client certificate authenticate with managed identity very popular use case um for for azure services so authenticate the backend service using managed identity so there's an example basic auth that's a username password you've probably seen it if you've ever got a site where they just give you like kind of like an alert you have to enter those two in and for this one you can see that it's going to vault for the managed identity caching policies so get from cash perform cash look up and return a valid cash response when available probably very popular policy to be used store to cash cash response according to specified cash control configuration get value from cash retrieve a cash item by key store value in cash store an item in the cash by key remove value from cash remove an item in the cash by key so here the examples obviously very big for get from cash but here it says uh um vary by developer cash key lookup so i guess you're performing you're giving it these things now sometimes they're not always very clear when you're reading them but that's fine uh for cash store values the store value you provided a key there is the value how the duration i'm assuming is how long it lists for maybe the ttl crossdomain policies so we allow crossdomain calls this is something you probably really really would want to enable especially if you're building applications that are not on the same domain so i could see people using this quite a bit makes the api accessible from adobe flash microsoft's silverlight browserbased clients did not expect that as a description of course i was thinking of cores that's why ad cross origin resource sharing support and operation or api allow crossdomain calls i feel like this one's going to happen a lot i didn't know microsoft silverlight still existed if people don't know silverlight is a competitor adobe flash open i guess adobe flash probably does html5 now i'm just thinking of old fash flash players jsonp adds json with padding support to an operation for an api to allow crossdomain calls from javascript browserbased clients so that is the crossdomain one very simple there's cores and if you've ever seen cores this looks the same if you're on aws if you're anywhere else it's the same xml it looks the same so it looks big but it's not as scary and you'll come across it quite often especially with apis transformation policies convert json to xml converts requests or response bodies from json to xml convert xml to json you get it find and replace a string and a body mask url in the content set backend service set body these are very clear set http header set query string parameter if you want to read these you can but they're very straightforward rewrite url that's probably a very popular one but maybe the example is too large so i don't show it here transform xml and x xslt let's look at uh xml json because i thought that one was kind of interesting so you say xml to json and i guess it just turns it into json http header is something very common that you'll be doing you can just set a header for whatever values need to pass along we have zapper integration policies so send requests to a service send message to pub subtopic trigger output binding and there's an example of a trigger output binding validation policies this last group validate content validate parameters validate headers validate status code validate graphql request so we'll take a look at parameters so here's an example where we have a parameter and it says prevent prevent um detect detect uh prevent ignore ignore ignore so i guess the idea is it's just saying like if those parameters are present allow them if not ignore them validate status code i figured this would be like a little bit more interesting this one but it's not status code so prevent unspecified status code action so do not allow it to have an unspecified code action i i didn't know that that's possible i thought you all status codes are always returned but uh you know i guess you're learning something new every day but those are all the policy groups and hopefully you get an idea of policies you can apply because that's what that's the greatest power of api is those policies okay all right let's take a look at different ways we can define apis within apim so the idea here is that you're ready uh to create an api you have a gateway you want to create an api so you hit that add new api button you got a bunch of options here so you can define one manually the http endpoints web sockets graphql of course that's going to vary what you want sockets is great for real time stuff maybe you're making a video game maybe you're making a chat app graphql is becoming a popular alternative to rest based applications like standard endpoints so graphql allows you to define a query to query data um you know personally it's not my favorite but i like the fact that it's all rolled up under one api gateway in api in azure the next thing is we can define based on a schema standard for importing defining apis we have open api version three wadl wsdl and the the latter two i don't know much about and so um following this uh video we're gonna go talk about these three standards here um because i do think it's important to know all those three because that's the real way you're gonna really get uh apis into here you're not really gonna be doing it manually and the last one is really great if you want to spin up an api that you know you want to integrate right away very common ones would be app service actually i guess all three of them really to be honest logic app or app service or function app and i'm pretty sure in this we will show you with function app in the follow along there so those are some options there just to get quickly started but let's take a deeper look at these um open definition standards for defining apis and then importing them starting with open api all right let's take a look here at open api so here is an example of one written in yaml i have a feeling that it can also be done in json but just taking a quick look here you can kind of get an idea you have paths that is an endpoint or uh yeah a method if you want the method that to be used would be posts here you can provide a summary description the contents should it be application json schema information and what responses should come back so pretty straightforward it gets a little more complicated than that but that is the best snapshot i can provide to you of that language so open api specification oas defines a standard language agnostic interface to restful apis which allows both humans and computers to discover and understand the capabilities of the service without access to source code documentation or through network traffic inspection and open api is really the leading one this is what pretty much everybody uses nowadays no matter what cloud service provider you go to they're going to have an option for their api gateway to import open api swagger and open api used to be the same thing but as of version three of open api swagger and open api are two different things so open api is a specification swagger are tools for implementation for the specification if you want to use i think it's like swagger it's by bearsmart and bearsmart i believe is the one that came up with open api to begin with um and so open api can be represented as either json or yaml as i said earlier but there you go let's take a look at wadl and wsdl they're very similar that's why we're giving them one slide together so web application description language wadl and then web service description language wsdl these are specifications defined by the web consort consortism uh was it w3 w3g w3 something w3 w3c probably and i believe that wadl was originally made by sun microsystems i don't know who made wsdl but it definitely is under the w3c and here's an example of wsdl very very very heavy because it's all xml most people like json yaml that's why open api is so popular but just take a look here at wadl and wsdl the idea is that you will have some things that are similar so with wadl you define an application for wsdl the equivalent is definition here we have grammars here we have types here we have resource here we have interface method operation request response input output param element param simple type to me wadl looks more like a uh like how we would describe in terms of terminology i cannot remember which one is more advanced because one has more capabilities than the others but to be honest you're not going to be using any of these you're going to be using open api but i just wanted to show you them and the fact that you can translate their xml over to that there in this table so hopefully that gives you kind of an idea there you go let's take a look at the developer portal so the developer portal is an automatically generated fully customizable website with the documentation of your apis it's where api consumers can discover your apis learn how to use them request access to try them out so here is an example of the default one that you get to be honest i did not know how to use this i tried really hard because i thought it was a very cool idea this is actually the second iteration of the developer portal so there was an older one that was different from this but this is the new one and i like the idea but in execution i'm not sure exactly what to do here but the idea here is you need to publish you'll need to publish for the developer portal to be publicly viewable you can save revisions of the portal to quickly roll back to previous versions you can apply custom domain for your developer portal so idea here is uh you know there's on the portal overview to see it there's a link there that you click and that's how you view it not available in the consumer plan available in all the other plans i guess the idea behind the developer portal is like if you're trying to sell a cloud service but it's api driven and they just pay access the developer portal or i guess it's also to read the documentation for people that need to utilize it but yeah in theory i like the idea execution i don't know all right so uh there is authentication for the developer portal that you can set there's a few different ways to do it you can do active directory b2c identity providers like google microsoft facebook basic authentication which is the default version if you're not familiar with basic auth the idea is you can enter a username and password that gets prompted to you um so not super complicated there's also delegated authentication which allows you to use your own web app sign in sign up process product subscription instead of the builtin developer portal builtin functionality and just to figure out where that is under the developer portal you have identities you could add those you can see there's delegation down below so not super complicated but yeah there is authentication for the api and then there's authentication for the developer portal so just make sure you understand the distinction of those two because it does get a bit confusing let's talk about caching so we have a builtin cache and an external cache so api and operations in the api management can be configured via response caching we were looking at policies we had an indicator as to how caching works response caching can significantly reduce latency for api callers and backend load for api providers so we're gonna just provide a caching policy to the outbound or other places builtin cache is volatile and is shared by all units the same region in the api management service so for that reason you have the ability to set an external cache via redis so that would be using azure's reddit cache service so using external cache allows you to overcome the few limitations of builtin cache so avoid having your cache periodically cleared during api management updates have more control over your cache configuration cache more data than your api management tier allows using cache with the consumption tier of api management enable caching in the api management for selfhosted gateways you simply need to provide a connection string to your redis cache so there you go all right so after a really really really long wait like 40 minutes and i was able to do like so many other follow alongs while waiting um this is ready so very long time 30 40 minutes we'll go to the resource here and so we have this environment set up and we're gonna have to uh actually have something to route to and so the idea is that we want to set up a container app and so before we can uh go ahead and do that we're going to need to create a container app so what you'll do is open a new tab all right and we're going to search for containers and then we have contain oops container apps and so what we'll do is go ahead and open that and then from there we'll go ahead and create ourselves a new container application container app is a little bit better than container instances i don't know if this came afterwards it just serves a different purpose but container apps is pretty nice so what we'll do is say uh we'll create or actually we don't need to create one but we'll use the same namespace so my apim and we will call this one um my container doesn't really matter what we call it for the region of does this have a region or is this a global service feels like it should be a global service i don't think we chose the region so central u.s i'm just going to stick to the same thing just in case just a good way when you're doing labs and we're gonna start with a simple hello world container so we're just gonna make it super easy for us don't have to do any coding to get an application running we're gonna go ahead and do create and so now we just need to wait for that to deploy it shouldn't take too long so i'll see you back here in a moment all right so after waiting i don't know like four or five minutes it looks like our environment is set up or our container is deployed so we clicked into it and there should be uh there it is application url so we just open that a new tab you should see this and that is what we're going to try to route to for apim so what we'll do is make our way back over to apim we'll go ahead and create a container app and here we can go browse and select our container so to make it super super simple for us super easy as you can see i'm going to look at full for a second to see if there's anything interesting there nothing that i care about we'll go ahead and hit create so yeah they make it really really easy and so once the that's configured we just need to observe that this api actually works so we'll give it a moment okay there we go so after a few minutes there it is so let's just test that our api is working now this web app is an html page so it's not really what we'd want to return but it's the easiest example that we can set up here uh so i'm going to go to the get request here and from there i'm going to go to test and we are going to see it's going to send a request here notice that it's sending the subscription key so this is what we normally have to pass along which is over here we'll do that in a different follow along here we'll go ahead and hit send and we'll give it a moment and we get some data back so it's the html page it's what we're looking for welcome to azure container app so that is great so we're done here we'll keep this environment around for the next few follow alongs but there you go hey it's andrew brown and we are continuing on with apim and uh this time around what we want to do is actually work with some subscription keys because here we are just hitting the button that's not how we're going to use our api right so um let's go over and set up a new product before we do that i'll go over to subscriptions let's just show you that we do have some keys here and notice there's one called service we came with a product and unlimited one but the service one is actually being used uh in our apim so when we were testing that was the key that it was using so if we go back to apis and we could click into our here my container and we go to get somewhere here if we go around click around here settings test down below send i think gives us some information so we'll know what key it's using so here we can see the key somewhere or it should tell us i do know that's using the service key i can't remember how i know that but i know it okay and so um the idea is that we want to create our own key i guess at some point but we'll go for product and we'll make a new one i'm going to call this developers actually call it junior developers because originally was developers but i'm just going to do junior developers just so we're not conflicting we'll hit publish so that's okay and we need to add our api so here we'll choose my container we'll go ahead and create it so we'll just say for junior developers even though it's going to really have full access and this is the way that you can like um catalog out your apis or or assign them to different groups so it's a good way to organize your stuff is to create them in products and so if we click into here you can see that there's apis that's assigned we can assign policies if we want globally for everything we can change we have some settings for access controls and we have um subscriptions so we do already have the subscription key we don't have to go and create a new one um but we should probably look at access controls because right now it's just set up as administrator access we're gonna go ahead ahead here and add another one called developers i believe you can add or create your own groups but it's not super complicated so there's no point in really talking about it we do want to go get the primary key for this so here what i want you to do is click into here and oops actually that's not what i want there was nothing to change i don't know why i hit save there that was pointless but what we'll do is go back to junior developers here back in the products here we'll do the subscription side here because we need to see the key which is here junior developers so if we go here and say show keys now i can get the primary key so we need that key because we actually want to make an api request and this time we're going to use powershell for fun so i'm opening up cloud shell up here in the top left corner make sure it's set to powershell it always doesn't hurt to learn some powershell we could do it in bash but let's do powershell it's a bit easier to do that and we'll give it a moment to spin up if you're doing this for the first time it might ask you to make a storage account so you might have to press some buttons here so just press whatever buttons it wants to get to this part okay and it doesn't normally take that long to spin up there we go so the first thing we're going to set is a url i'm just waiting for it to give me some more information yep still going there we go so we're going to set a new variable whoops we'll type clear first we'll type a new variable and this needs to be well first we'll do a subscription key because we already have it in our clipboard subscription key double quotations paste that in it didn't actually grab it so that's fine i guess we'll go grab instead from apim the gateway url so we'll get the gateway url that's the uh url that we need to get to notices of the overview blade and their apim app in order for us to uh that's like the api endpoint you'd hit right so we'll type in url and we'll paste that in it really doesn't like the right uh that so we'll have to rightclick paste probably was working before for the subscription url we just probably had to right click so we'll do subscription url equals double quotations and we will grab this key here again so i'm just going to scroll on down here grab the primary key you can use the secondary key but you really really don't need it so let's just paste that in here so there we go that looks like a key so we'll hit enter and now we need to set some headers so i'm going to do at sign uh curly's o o p a p i m subscription key equals dollar sign subscription key you're wondering what is all this stuff here well um in powershell i believe this is called a dictionary another language they might call it a hash you might think of a json object but it's just a data structure that we need to utilize in order to pass this along i'm just double checking to make sure i named it all correctly we'll hit enter and now we'll use the powershell command called invoke web requests i'm hitting tab to autocomplete it's really nice that it does that it'll do that even for the flag so i'll do url here we'll do method i think we don't have to specify the method because it'll already be get but we'll do it anyway to be verbose we'll do headers here we'll do headers and we'll hit enter and we have access and i do the missing subscription key so here we have a small little problem uh so i just got to double check to see what the mistake is you know what i was staring at this thing like there's something wrong here it's ocp what ocp stands for i have no idea well we'll hit enter and we'll try again and maybe we'll just print out the headers to make sure it's correct there's no value in there so we'll print out subscription key to make sure that is correct oh because i wrote subscription url this is what happens when you don't type okay so not a big deal just learning a bit more as we go and i just want to set that again and then i'm going to print it out to make sure it's correct there it is and we'll invoke this and we should get back html with 200. so it says welcome to azure container app so that's how you can do requests of course that's with powershell traditionally you might use postman or you use your api like an api to do that uh but there you go so yeah that is how uh we work with a key we generate our own subscription key and work work with it to make a request not in the ui okay so i'll see you in the next one hey this is andrew brown and we are still working with apim and this time we are going to create a policy because policies are some of the most powerful things that api allows you to do so what i want you to do is on the left hand side in our our gateway that we created we'll go to apis we'll go over to my container and what we'll do is we'll just click onto any of them like get container actually i want to make a new one i'm going to call a new one called mock because we're going to do a mock request so do forward slash mock and if we go there and go ahead and create this so if we were to use this right now it shouldn't do anything if we test this what would we get back we got a 404 not found because there's no page there so what we'll do is we'll go here and we are going to go back to the design and we're going to add our own outbound process process so it doesn't matter what comes in we're going to always have a mock response come back we're going to do that by adding a policy so go add a policy you can see there's some basic ones like setting headers and things like that we're going to go to other policies i'm going our policy structure back and this is for the outbound i don't know why we click through that to get to this but the idea is that we're going to add a new policy here so what i'm going to do is type in mock response apim policy and i'm just looking for all the let or all the ones here so here's a bunch of policies and we're just looking for the mach one it's not a very uh um verbose or sorry like very flexible one just basically set the uh code but it does work so what we'll do is just paste that on in there and we'll say we want this to be a 200 and we can say whatever we want to get back so like if we want a different type i type like um i don't know uh csv file content type it'd be text csv it doesn't matter we can do whatever we want application ms word right like i'm not sure what would happen but let's do it for fun we could have just done javascript but no let's make it complicated and so now if we were to go to this mock request we're going to test it and we'd send it let's see we get back it says 200 mock and i mean it doesn't complain about the content type but let's just do this up in cloud shell just to make sure that we know what we're doing so we were doing that before so we're going to do it again okay it's good practice so we'll just test our new endpoint make sure it works this drag is sometimes weird so i clicked it and then it's just like it's still dragging it's very frustrating sometimes you have to freaking open it up like that so silly but um yeah what we'll do is we'll just do a bit of uh powershell again and uh get this working so i'm just double checking on how to do that so the first thing is we need the url what if we hit up oh we still have our stuff from before so yeah you want to set a url for the gateway you want to set a subscription key from before to then the last follow along and then we need the headers but it's slightly different here because the url needs to be forward slash mock and then what we can do is invoke a request we get back at 200 so no issues there so there you go uh that's all it takes to set a policy there's obviously more complex policies there but this is a simple example because you really got to go digging through these here and it's not really worth it to do that unless you actually do it for real but there you go so what we'll do is clean this up we're all done with apim look for that there delete this and there you go so that is apim and i'll see you in the next one something throughout this course that keeps getting mentioned at least for the az204 is distributed application runtime or also known as dapper and uh you know it's not going to show up on any of the exams but the idea is that uh it's good to know what it is it is a microsoft thing that they had built its distributed application runtime short for dapper provides an api that simplifies microservice connectivity so dapper is a portable event driven runtime that makes it easy for any developer to build resilient stateless and stateful applications that run on the cloud and edge and embrace the diversity of the language and developer frameworks and it's interesting they say uh diversity of languages because it doesn't support my favorite language which is ruby yet it supports php kind of explain that one to me but dapper provides basically a bunch of functionality so the idea is that you are going to write your application code up here like whatever language you want and then the idea is that you can communicate via a grpc or http http ai or api to a bunch of standard services so like services service invocation state management pub sub resource binding triggers actors observability secrets configuration and allows you to connect to edge infrastructure now basically these are just basically wrappers so to speak for um uh you know standard functionality that you'd want to have alongside a microservice application dapper is not taking off as you would think it is um i don't know anybody that's really like that interested in adopting it because it is like this thing is more in the cloud native space like like kubernetes cncf and the thing is is that unless there's buyin with that community and that this is truly agnostic um you know you're gonna have a hard time to see adoption unless it meets those things it just does not it's very interesting i think it'd be really cool to build an application on top of dapper for your micro service applications but i strongly doubt anyone would be using this because the future of this project is debatable but it is mentioned throughout this course so i just wanted to give you a little bit more attention there we're definitely not going to dig any code with dapper but there you go hey this is andrew brown from exam pro and we are taking a look at azure event grid which is a service that allows you to manage event routing from any source to any destination so event grid provides a simple and strong customizable event delivery process that allows you to manage at a minimum level which type of events will be received as well as which subscribers will receive those events so here is an example graphic uh you can see event grid is kind of sitting in the middle there where um events are being captured and being uh managed by event hub and it's routing to other places so event grid is ideal for event driven architectures where you can subscribe to azure resource events and publish them to an event handler or web hook you can also use custom topics to create custom events that will be published in your event grid and it supports event fan out with 24hour retry reliability to ensure that events are delivered it is a lowcost serverless product that supports dynamic scalability so there you go let's talk about event source and handlers so azure event grid is divided into two categories events sources of services that emit data and event handlers services that receive data and in between them that is where event grid is so let's talk about um what there is that can uh emit sources and this will just give you an idea of how much event grid can integrate with azure services so we have blob storage resource groups subscriptions event hub media service iot hub service bus azure maps azure container registry signal r azure app configuration azure machine learning azure communication services azure cash for redis cloud events azure policy and custom events so basically anything you want to get in there and i'm sure there's more data support for other services but as you can see we're limited for space but it can receive messages from a lot of places for event handlers the services that receive data it could be for serverless code so think azure app functions things like that we have workflow and integration so think service bus logic apps buffering and competing consumers so event hub storage queue other services and applications so i think hybrid connections like web sockets or web hooks or even automation so yeah there you go all right let's take a look at the key concepts for azure event grid so here we have this fancy diagram at the top here we have domains so these are used to group event grid topics that are related to the same application for easier management then you have topics these are the end points where events are going to be sent to there's different types of topics we have system topics these are builtin topics provided by azure services this is the most common kind you're going to be using because they're super easy to use we have custom topics for applications and thirdparty topics and for those thirdparty topics they emit their own partner events uh from thirdparty sasses to publish events so they're similar to system topics it's just that they're with third parties um but uh yeah there's some variation there we have events these are the actual event data that occurs within the service not visualized here but you get the idea uh publishers this is the service that published the event so you cannot see them on this diagram but the idea is imagine you have a publisher here and they are uh they are the ones where the event sources is coming from okay we have event sources this is where the event took place uh event subscriptions these are the mechanism that the routes uh that route the events sorry about that there so over here you can see subscriptions uh subscription expiration so this is where you set an expiration for the event subscription you have event handlers which is over here um you could say consumers if you'd like is the app or service that receives the events you have event delivery this is the delivery of events in batches or in single events so it just depends on how you want to send those messages you have batching so this is uh sending a group of events in a single request and there you go hey this is andrew brown from exam pro and we're going to take a look at event grid basics so let's get to it so the first thing we're going to want to do is we're going to search up subscriptions because we're going to need to make sure that like in order for us to use event grid that we have it turned on and so we'll go into our subscription and we're going to go under resource providers providers ah there it is and this is all the stuff that is registered all the providers that are registered so what we're doing is just make making sure that event grid is turned on because that one's not always turned on by default and just make sure that it's registered okay so you'll know that it's registered because it'll have a green check mark here and it'll say registered and so once that is done we can proceed to create a storage account because we're going to integrate a storage account into our event grid so what we'll do is create a new storage account and we're going to create a new resource group i'm going to call this resource group event grid basics and we're going to name our storage account event event grid basics all right and yes you can have hyphens you can't have anything else and we'll just make sure we spelt that right event event grid basics i can't remember if these are fully qualified domains if they are you might have to add some numbers on the end there but it is what it is we're doing uh usc i mean it just randomizes every time but this is where you should probably set it to we have standard premium we'll leave it as standard and everything all seems fine so we'll go ahead and create review and create and then it's going to allow us to review we'll go ahead and hit create and it's going to create the resource group here we'll just wait for it to finish uh deploying and then we'll go into the resource all right looks like it is finished deploying so we're going to go ahead and go to that resource we're going to go to containers and we're going to create a couple containers the first is the first one is going to be called basic we're going to leave it as private just make sure it's basic not basics and we'll create another container called basic alt the idea is that we're going to use an event grid in order to move one file from one storage account to another and that's going to be facilitated by log via a logic app because that's going to be the easiest way to use event grid so what i want you to do is search for logic apps up here and we're going to go ahead and add a new logic app and what we want to do is choose our resource group and we'll just go event grid basics here we are going to name this event grid basics um maybe we'll do lg to indicate that it is a logic app or lg maybe uh l yeah lg is fine like short for logic and from here we have a workflow or a docker container we're going to stick with a workflow just to knock on wood we're going to just put in the same region as our storage account so east us we have standard consumption i'd rather do consumption for this so you pay only as much as you use because we don't need enterprise level serverless application here we just need a consumption model we're going to leave this to disable so that is totally fine there's nothing else to do here so we'll go review plus create and we'll go ahead and create this logic app and we just have to wait for this to finish deploying all right so that should have been very very quick like under uh 10 seconds there so we've gone into the resource so just click go to the resource and so we have this very fun interface and so what we need to do is start with a common trigger there's a few different ways to get to it but there should be something on the front here i don't know if they redesigned this recently so i'm just gonna search start with a commentary oh yeah it's up here okay i'm being silly um and so what we want to do is uh because this is an event grid uh follow along we want to click on when an event grid occurs and so this is the designer where we can make things a lot easier for ourselves and so we're going to have to first sign in to authenticate so i'm this is my tenant example training inc so we'll go ahead and get connected there let's give it a moment we'll select andrew brown which is totally fine and now that is connected so that is great so once we are signed in we can click continue and we're going to go ahead and select our subscription here and we need to choose a resource here so i guess in this case it's going to be event grid event grid or hmm i could have swore yeah yeah i think that's what we want to do let me just double check here oh you know what it's just not for some reason i'm i'm searching it's not auto completing properly okay i just wasn't sure there and as far as i can remember this would be probably an event grid topic and then we need to give this a resource name so um let me just think about this for a moment okay all right so i think i understand where my confusion was it was because we click continue and i never i didn't see event quit anymore so i thought we had to configure it when it was already configured right so this is where we were so we're not we at this stage like event grid is already hooked up so it's ready to be triggered so this is the step that follows into it which is where we want to do our storage account so that's where i was getting confused so we'll choose our subscription here it's okay you know if you never get confused just step step back a couple steps and just double check what you're doing happens to me all the time so um what we want to do is actually connect storage uh storage accounts so we type in storage accounts here great uh we'll have that selected and then we need to select our storage account so this one's called event grid basics and then we're gonna have to enter in um some additional information event type so we want to have it happen when we add something to the container so the basic container so we'll do blob created and then from there we need to actually filter out the information so we need to add a new parameter and i think we'll have to do it on the prefix filter so a filter like whatever yeah so that's probably a good idea because then we could place it into a particular place and i believe that there are very specific filters that you can do for this because if i recall there's like standardized ones yeah see here like it's always going to be forward slash blob services default containers etc and you'll know that because you know if you read the documentation and you have to do that stuff you'll figure that out so i'm just going to type it by hand here blob services default containers and then we can put our container name so basic uh and i believe we have it without the s there so uh services see i don't trust my writing here so i'm just gonna copy paste it in okay and that looks good to me so i think that is what it needs to be so we'll go ahead and hit the next step so the idea is anything in that folder like when something's added to that folder then follow up with this operation right um and maybe before we do that we should probably um you know observe that this stuff works that's probably a good idea so what i'm going to do here is i'm going to make my way back over to our storage account so just close this tab here i'm going to open a new tab and we're going to make our way over to storage accounts and we'll go to event grid basics and we will go to containers and we'll click into our basic container because i want to just see that this is working and i'm going to need a file to upload so let me just go grab an image really quick all right so i just grabbed an image off the internet so i just have data here but before we upload we probably should save what we have because if we don't save it we're not going to be able to observe it so i went back to logic app and we just hit save in the top left corner so we'll give it a moment to save and it looks like it's saved now i'm just going to go back over to here if we can look at some of the code that gets executed i'm just trying to remember where it is because once it executes we want to um see what happened right so what i'll do is i'm going to go all the way back over here and i'm just going to go and drag or actually i'll hit the upload button so i don't trust that that there and i'll drag it onto here nope i still don't trust it so what i'm going to do is just click the files and i'm just going to grab it this way and say open and we'll do upload and so that is now uploaded and so there is somewhere where we can observe um where stuff has happened so i'm just trying to remember where it is um i mean we could run the trigger yeah we probably should run the trigger right run i think it's running so we'll just give it a moment okay you know i was thinking about it it doesn't make sense we shouldn't have to run it because it should just happen automatically i think it's on the overview page ah okay so if we look here we can see the run history and so and there's also trigger history of when the things are triggered so we could we could manually fire it but doesn't make sense so i think this is the run that we just did if we click into here yes this is what it is so here we can see what what has happened so if we expand it we can see the inputs right so it's we have a blob created um it might show some information so here we can see data yep and it's a webp file and so it's gotten this far through and so that's a great way to kind of like debug so you can uh logic run app you can do it each step but right now we are using um event grid to do that integration right we're just doing it through logic logic app because it's a lot easier so now that we have that what we should do is go back to our designer and we're going to have to add the followup step um so we have this oh yeah okay so that's the first step event grid doesn't show up there which is weird but um so we have this step here from our storage account and so the next step what we want to do is put it into another container so that will be the tricky part um so i'm just trying to remember what we do so we'll hit next step and um i think what we need to do is initialize a variable first because we're going to have to get some way to grab the name of the string because if we go back to our run over here just give it a moment here and we go into a run again here we need to extract some data to pass along because there are some limitations in terms of how json gets passed along or data gets passed along and so what we want is we just want this part of the name we want to say take this name as the identifier so that when we're copying stuff over it will work and so what we'll have to do is store that into an intermediate variable so we'll just type in variables here and i'm just seeing oh yeah so they look like this because i can remember they might be in the builtin yeah that looks a lot better and so we need a variable and it's initialize variable and we're going to name this file name and this is a string of course and now we need to insert the value so in here what we need to do is write an expression in order to extract that information out um so what we'll do is go to the expression tab and over here you can see we have all sorts of expressions that we can use so i'm going to type in last parentheses and then in there we'll do split parentheses and then what we're looking for is trigger body and then we'll do question mark square braces single quotations subject how did i know how to do that i looked it up i looked it up somewhere and you know i just don't feel like there's much reason to to teach this part because you mean if you really need to know you can go here um and learn all about it but a lot of times like if you need something you can just say i need this kind of function somebody's already done it right because there's so many common use cases so i probably search something like how do i get the name out of the the thing you know like for the blob and somebody had that there but it makes sense to me so let's hit okay here and it should turn purple because it is dime expression if you type it in here probably won't work correctly you have to type in here and then hit okay so it shows up like that uh but you notice we typed in like trigger body so if we go back over to our run here um this is the body here so when they say trigger body they're talking about here and then it was just grabbing that subject line there all right so that would be the second step and that gets it into a variable but the next part is we need to actually um get the blob content and then insert it and then create a new blob so we'll do is hit next step and we'll type in blob and see if we can find anything here and from here we need to get the get blob content using path version 2. so i'm just going to scroll down here and take a look for it there it is based on the path and we'll go down here and um i guess it would be access key oh because we're setting up a connection for the first time so enter name for connection um i know what the storage account is but what is the connection uh connection name i do not remember give me two seconds okay there wasn't much to help me here because what i remember before was that you click it and you'd authenticate it like the event grid but it's not doing that so maybe we just have to name it something so i don't know we'll just say azure storage account maybe it just wants a name maybe it doesn't really matter oh yeah like there's sign in that's what i want so connection name yeah so we'll say uh you know storage account event grid okay because if we can just single sign on let's do that that's super easy and we'll click that there okay so this is starting to look how i expect it to look and so we need the storage account name i don't know why it's not showing me any names here but that's okay we'll just go over back to our storage account here and it's called event grid basics so we'll type in event grid basics event grid basics uh that's custom value sure i mean that's what its name is i'm not sure why it's not auto completing but here what we need to do is we need to provide the path so it's going to be forward slash basic ah and so now there's our environment very or that variable so we'll just click that there so that will make it super super easy now notice that it is showing basic now so i just clicked here the folder we typed it in manually but we could have clicked uh here and then put the environment variable in or the uh this initialized variable in here but i did type that manually and it still did work correctly so we are okay here infer the content type sure why not um it doesn't matter if they do so this gets the content so now this gets the path and so the next thing is actually to create the blob you can't like do an easy clone you have to do at this um intermediate step that's just how it works um and so what we'll need to do is go to our builtin ones here we'll type in blob again maybe standard and this time we want to create a blob so there it is uh block bob no we just want a blob and so what we'll do is i guess we have to connect again i'm surprised it's not showing the name yeah it's just the name that's fine so we'll go back over here i just don't want to type it wrong so we'll just copy paste it in event grid basics grid basics enter custom value because it's giving us so much trouble for no particular reason make sure there's no space on the end there there now works fine um in this case what we want is basic alt and the blob name can be the file name which is totally fine and the blob content will be the file content and i don't think we need anything else so what we'll do is go ahead and click off and we will save alright so that's just the way we're going to have to do it so what we can do is go back to our overview and we'll go back to our basic folder and we'll delete data say ok and we'll go upload we'll select our file again we'll grab it we will upload and then we will make our way back over to our logic app close this tab here so we don't get too mixed up refresh the page and it failed so it failed for some reason so something has not been configured correctly it failed on the initialize variable so something's wrong there so unable to process the template language expression in the actions initialize variable uh inputs at line zero column zero templating function split is not defined as not valid so it's possible i just spelt it wrong so what we'll do is go back to our event grid we'll go back to our logic app designer here initialize variable we will click it and we probably just built it wrong spilt spilt what if we do lit spilt split so if that's wrong we'll just scroll on down and we'll just take a look i could have swore that it autocompleted for us oh you know what it is spelled wrong it should be spilit split all right and i'm just doublechecking to see if there's any other problems here nope looks fine to me so go ahead and say update we will save it in the top left corner we'll go back we'll delete our file here we'll say okay and we'll have to select a new file we'll click open let's double check make sure that's been saved it looks like it's been saved we'll hit upload we'll go back to our overview page it's already running super fast by the way and we'll click into it and we'll see if we get any other failure so there's another failure that's totally okay so we'll just expand it this request is not authorized to perform this operation using the permissions so it does not like the permissions i gave it totally fine so we will go back to our app designer we will go to this second step here even though it did select this properly so we'll change the connection i guess we'll add a new one so we did 80 integrated oh let's do managed identity you must enable managed identities in the logic app to use managed dna's authentication you must grant required access to the identity in the target resource okay there is an identity tab so we can go over there and take a look there quickly i don't remember it being that hard to do a system assigned managed identity is restricted to one person one person resource is tied to lifecycle you can grant permissions etc etc um can we just turn that on and hit save well it can be granted access resources protected by a z sure let's give it a go all right so um it seems like we have to assign some role stuff so we can try and assign a role um can we do the subscription level contributor okay so there's a few different ways you can authenticate so hopefully this will be the easiest way to do it we'll refresh here did it assign it and i don't think it said it all right so give me a moment and let me see what i can figure up okay you know just to make this easier i think what we should do is just do the access key because that seems like the easiest way to do it i was just hoping that we could have you know just did a simple sign in here but it's not a big deal so we'll hit change connection we're going to add a new connection just say storage account event event grid key and so this is going to want the azure storage account name so this one will be the name of the storage count if we can find it it's called event grid basics and then we need the azure storage account access key so there's probably a tab called keys yep and we will show the key and we will copy the key if i don't have to pass long keys i like to not do that please check your account info again storage account access key should be a correct base64 encoded string come on give me a break here i am doing what you asked me to do so we will try this again this thing just hates me today give me a second okay you know what it was really short so i really don't trust it so let's just do i just cleared it out there i didn't do anything else what we're going to do is go back here click the copy and then right click and paste that's so much longer okay that has to be the right key we'll hit create and we'll give it a moment okay great so that's for that one um but this has to have the right connection as well so what we'll do is just change the connection you have a few here eh um and the one we want is the one that's valid so we'll go this one down below as you can see a few attempts here and we'll save it and we'll go back to our overview here i'm just gonna close that tab out we're gonna close this out we're gonna go back into here we're gonna go into our containers we're gonna go into our basic we're gonna go ahead and delete this we'll say okay and we will upload a new file we will choose the new file we'll choose data upload it we'll go back over here and i want to see the latest run here we'll give this a refresh is it running it looks like it's running it's hard because this one looks like it just failed and and now the the messaging is getting really muddy here what is it doing so we'll click off here sometimes the portal is a bit funny is it just triggering over and over again did we make an infinite loop uh oh okay i think we have a problem here well if we go here is it basic or it's basic alt this one's basic so what's the problem we'll refresh failed why did it fail conflict another active upload session exists please retry after some time okay uh well let's just go take a look here go back it's here so it's it's here so it clearly has worked why it's triggering multiple times i don't know um don't particularly like that there we go let's go ahead and delete this one here and it's just it's just going over and over and over again so there's something wrong with my workflow so this looks fine to me that looks fine to me maybe it's triggering oh you know what the the parameters out of here so this is supposed to have a prefix here so what's happening is that it's triggering on any time a basic one is set up or a basic alt one and it's just stuck in an infinite loop which is really really bad um so we did do this earlier but for whatever reason um the changes still are not here so what we'll need to do is set up that prefix so what we'll do is type in blob services default containers basic because we really don't want to trigger it on any but that that container there and uh did it save it it doesn't look yeah i mean it should be there so what we'll do is go ahead like why is it not filtering oh i guess it wants a filter based on name but we gave it its name so i'm not sure what else we would have to type there okay um i'm just gonna put um dot it's the prefix filter so data i guess i don't know like it's not letting me save okay there we go we'll save that i just want to stop the infinite loop there for a moment so we'll go back over to the overview and we'll just make sure we're not running up our bill here and i'm just refreshing i just want to see that's not triggering anymore so it stopped triggering which is good and we'll go back over to here and we'll look at this prefix filter because i i remember having to do this so a filter like sample etc etc so we'll type it in again i guess blob services defaults containers basic it's very odd because like we typed it oh you know what i probably did i typed in the filter parameter here and we're supposed to add it then put it in there so it's just me getting confused by the ui silly me okay so what we'll do is go back to the overview and this time we just want to see trigger once so go back to basic all we'll go ahead and delete this we'll say okay and we'll go back to our event grid we'll go in or our event grid our basic our basic container we're going to go ahead and delete uh data here again and we're going to go upload one more time it's actually good that we had that problem because i got to show you uh why filters are so important um when we're dealing with uh the app logic there um or logic apps so we'll go ahead and hit upload we'll go make our way back over here we're gonna give this a refresh and now we have a new one and it only happened once and that's what we wanted to happen so we go back over to here and go to basic halt there it is so that's a means to which we can use event grid to integrate stuff you can see logic app is extremely useful for developers building all sorts of tools but we are all done here and what we'll do is make our way over to our resource group and we are going to just go ahead and clean up so we'll go into event grid basics and we'll go ahead and delete this resource group there we go and it's going to go ahead and delete there uh yeah and there you go hey this is andrew brown from exam pro and we are taking a look at azure event hub so event hub is an event in jet store that can consume millions of events from anywhere and process them in real time or micro batching thanks to the auto inflate feature which automatically scales the numbers of throughput to meet your changing needs so it provides an easy way to connect with your apache applications and clients here is kind of a overview illustration of event hubs so the idea over here is you have different protocols so hpsamqp kafka for your event producers to then uh enter into azure event hubs you have partitions within your hubs you have consumers uh and then there is the received events the event receivers so you know hopefully that gives you a quick overview let's go dive a bit deeper let us take a look at the pricing for azure event hub but before we do i just want to expand on a few initialisms so we know what they stand for we have capacity unit for cu processing unit for pu and throughput unit for tu so across here we have four different pricing plans basic standard premium dedicated lots of azure services like to have a whole lot of specialized plans uh built into their services uh but as you can see as uh you go right the more expensive it goes uh but there's some key differences so for azure event hub if you're on basic you do not get capturing capturing cost a bit more uh so i mean we'll cost something at standard at premium dedicated it's included if you want to use apache kafka you're not going to get it in basic if you want a schema registry you're not going to get that in basic if you need extended retention you do not get a basic or standard so hopefully that gives you an idea of the differences of these tiers let's take a look at the key concepts a lot of these will cover in future slides so this one is a bit text heavy so do not worry but azure event hub helps you build your big data pipeline to analyze logging anomalies use anomalies users and a device telemetry where you only pay for what you use so some of the key concepts here are name space so it is an endpoint for receiving and distributing events to the event hubs we have event hub where your events will be delivered event hub cluster which is a dedicated event hub with 99.99 sla event hub capture this allows you to automatically capture and save streaming events we have uh event hubs for apache kafka so this is compatibility with apache kafka's setup event publishers these are applications or services that publish events to event hub publisher policy this is a unique id used to identify publishers partitions are used to organize the sequence of events in event hub event consumers are applications or services that read events from event hub consumer group which are the enable consuming applications to each have a separate view of the event stream stream offset which holds the position of an event inside a partition checkpoint is the process of distinguishing between read and unread events but let's go look at these all in more detail let's talk about scaling because that's a very important concept in vent hub so the idea is that there's this feature called auto inflate it's basically just a check box here and will automatically scale up to the maximum to you so um throughput units based on the traffic demand it's not available for basic pricing as we saw over there we also this option of if we're in the premium tier you're going to notice there is no auto inflate because it's already there by default and the unit is different it's processing units so you'll just slide that left and right that's just kind of reflect what we saw in the pricing section okay let's talk about an actual event hub so what we do is go name our hub and we got a few options here we have partition count so partitions are our data organization mechanisms that relates to downstream parallelism required and consuming applications sounds very fancy but it's just a way of segmenting your data so that you have a faster concurrent reads you have retention message retention so this is the period for which your events will retain so that means that when they're past that period they are poof they are gone then you have capture so capture enables you to automatically deliver the streaming data in event hubs to azure blob storage or azure data lakes store account of your choice with the ad flexibility of specifying a time or size interval setting up capture is really fast there are no administrative costs to run it it scales automatically automatically with event hubs throughput units event hubs captures is the easiest way to load streaming data into azure and enables you to focus on data processing rather than on data capture so just a great way to offload into azure blob storage or azure data lake store there you go let's take a look at producers because that is what is going to emit events that will get into event hub so on the right hand example we have some code because the idea is that you will have to instrument your producer to send out events so here you can see we've created a client we'll have to have a connection string the event hub name so we know who we're talking to you would create a producer and then here what we're seeing is a batch job so they create a batch job and they push some data on there and then they send uh send uh send the batch and then when they're done they close the client if there are any errors we can get some exception handling this is a javascript example so a producer also known as a publisher emits data to the stream publishers can publish events using the following protocol so https so most azure sdks use https as this would probably use underneath you have amqp which is a very popular uh queuing protocol kafka protocol so if you're using kafka or you're using that protocol then that's another way to get in here and we just walked through that example you can publish events either one at a time or batches as we see here on the right hand side there's a limit of one megabyte regardless of whether it's a single event or a batch beyond one megabyte events will be rejected for automation or for authorization publishers use either azure ad with auth 2 issued jwt tokens or shared access signature sas which is something we see a lot in azure services let's just talk about some other things so https versus amqp for publishing events for amqk it requires the establishment of a persistent bidirectional socket in addition to tls or ssl mqamqp has higher network costs when initializing the session amqp has higher performance or frequent publishers and can achieve much lower latencies when used with asynchronous publishing code https requires additional tls overhead for every request so it's going to be up to you to decide whether you want to use https or amqp protocol for publisher policies event hub enables granular control over events publishers through publisher policies and they these publisher policies are run time features designed to facilitate large numbers of independent event publishers with publisher policies each publisher uses its own unique identifier when publishing events to an event hub using the following mechanisms and i guess there was supposed to be more there maybe but i think that's pretty much it because we don't really need to know much more about publisher policies to be honest uh if there's an opportunity if we do follow along that is opportunity maybe we will take a detour into that to see if it's worth our time but as far as i'm aware of i don't think it's that important there we go let us take a look at a consumer so here's an example of consumer code very similar kind of similar to the last one here so we are uh creating a consumer client and so here we need a connection uh connection string container name where we're gonna put this stuff looks like we're going to put in azure blob storage we're going to create that client and then we have the subscription so the idea is that it was going to listen for data to consume okay and then over here we can see that it updates a checkpoint we'll talk about checkpoints soon enough but let's just talk about consumers in general so consumer also known as a reader receives data to process from the stream all event hub consumers connect via the amqp 1.0 session and events are delivered through the session as they become available the client does not need to pull for data availability so that's that we'll next talk about consumer groups well let's talk about consumer groups here this is very tech savvy sorry but there's not much to show a consumer group is a view state position or offset of an entire event hub consumer groups enable multiple consuming applications to each have a separate view of the event stream and to read the stream independently at their own pace with their own offsets so in a stream processing architecture each downstream application equates to a consumer group there's always a default consumer group an event hub and you can create up to the maximum number of consumer groups for the corresponding pricing tier there can be at most five concurrent readers on a partition per consumer group however it's recommended there's only one active receiver on a partition per consumer group some clients offered by azure sdk are intelligent consumer agents that automatically manage the details of ensuring that each partition has a single reader and then all partitions of an event hub are being read from this allows your code to focus on processing the events being read from the event hub so it can ignore many of the details of the partition so there you go let's talk about offsets for azure event hub so an offset is the position of an event within a partition and so here's an example a very ugly graphic from the documentation but it makes our point clear the idea is you have a partition you have um you have an event within the partition there's all these little lines represent event and that's where our offset is so offsets enable a consumer or reader to specify a point in the event stream from which they want to begin reading events you can specify the offset as a time stamp or as an offset value consumers are responsible for storing their own offset values outside of the event hub service within a partition each event includes an offset so there you go all right let's talk about checkpointing uh there is really not a lot to say about it um or to visualize it but it's kind of part of offsets so checkpointing is a process by which readers mark or commit their position within a partition event sequence checkpointing is the responsibility of the consumer and occurs on a per partition basis with a consumer group responsibility means that for each consumer group each partition reader must keep track of its current position in the event stream and can inform the service when it considers the data stream complete hey this is andrew brown from exam pro we're taking a look at schema registry something that is uh not uncommon when we're looking at event buses there's something similar in aws um and the idea here is that you're able to enforce a particular schema of your data and this will help avoid um uh things breaking for your consumers and have a standard way of expectation of what something is so the idea here is skip registry provides a centralized repository for schemas this provides the flexibility for your producer consumer applications to exchange data without having to manage and share schemas between them and also evolve at different rates that's the key thing there evolve at different rates so here we have our schema group we have a schema we'll talk about schema group in a second and then you have schema versions and then there's the actual schema itself so schema groups are under your namespace and can be accessed by all topics event hubs under that namespace so here you can see we're just setting some options here and then here's the schema itself so here we have a very simple example where we have a type of record and name ordered and we have a couple of fields but the idea again is schema registry really does help you kind of enforce what you expect data to look like for your producers for your consumers and there you go hey this is andrew brown from exam pro and we are taking a look at apache kafka so the reason we're talking about it is just because event hub has a compatibility mode for it so apache kafka is an open source streaming platform to create high performance data pipelines streaming analytics data integrations and mission critical applications kafka was originally developed by linkedin in open source in 2011. kafka was written in scala and java so to use it you'll need to know how to write java and the idea is quite simple we have producers we have consumers we have topics there's a cluster there's partitions looks similar to event hub and other kind of streaming platforms and kafka data is stored in partitions of course that span multiple machines because it's for distributed computing you have producers via the kafka producer api you have consumers via the consumer api messages are organized into topics producers will push messages to topics consumers will listen on topics so let's talk about that kafka compatibility so event hub provides an endpoint compatible with apache kafka producers and consumers apis for version one and above so event hub kafka compatibility is alternative to running your own apache cluster so you know we said that they're very similar so eventhough calls them name space kafka calls them uh clusters so you can kind of see that there's that translation over but the idea is that when you enable this it will use all the stuff that kafka is used to seeing and then the idea there is that you don't have to run a kafka cluster you just run event hub if you've already written code that works with kafka that's the idea is that you can drop in event hub as a managed replacement now does event hub work exactly like kafka i'm not a hundred percent sure so there could be some feature differences um but in terms of uh protocols or endpoints being able to replace kafka with a vent hub if you need to um that's why they have this functionality here so there you go hey this is andrew brown from exam pro and we are looking at partitioning or partitions for event hub so vent hub organizes sequences events sent to an event hub into one or more partitions as new events arrive they're added to the end of the sequence this idea is that you have those partitions older's on the left mirrors on the right partitions hold the following data about the event the body of the event a userdefined property bag describing the event metadata such as its offset its partition its number in the stream sequence service side timestamps at which it was accepted partitioning allows for multiple parallel logs to be used for the same event hub and therefore multiplying the available raw input output throughput capacity you can use a partition key to map incoming event data into specific partitions for the purpose of data organization and the partition key is a sender supplied value passed into an event hub so there you go hey it's andrew brown from exam pro we're taking a look at event retention for event hub so published events are removed from an event hub based on a configurable time based retention policy and the default value and shortest possible retention period is one day so that's 24 hours for event hubs standard the maximum retention is seven days and we saw that um in the pricing page where retention can be different based on those things so for premium and dedicated the maximum retention is 90 days if you change the retention period it applies to all messages including messages that are already in event hub you cannot explicitly delete events the reason for event hubs limit on data retention based on time is to prevent large volumes of historical customer data getting trapped in a deep store that's only indexed by tam's timestamp and only allows for sequential access which doesn't sound too good if you need to archive events beyond the allowed retention period you can automatically have them stored in azure storage or data lake by turning on event hubs capture feature which we talked about earlier if you need to search or analyze such deep archives you can easily import them into azure synapse or other similar stores and analytics platforms so there you go hey this is andrew brown from exam pro and this follow along we are going to learn all about azure event hub uh so what i want you to do is go to the top here and type in event hub and we are going to create ourselves a new event hub namespace so we'll go here and hit create and we'll create a new resource group as we always do called my event hub and then for the namespace name we'll say my event hub if it doesn't let you do that you'll have to put some numbers on the end i'm just putting some numbers here because often these are taken up doesn't matter what location is just choose whichever one we're going with basic because there's not a huge difference between the pricing terms in terms of feature sets that we want to use today so go ahead and create this namespace so we'll give it a moment and we'll go ahead and hit create and then we'll just wait for this namespace to provision all right so after waiting about a couple of minutes there our namespace is deployed we're going to go ahead and create ourselves an event hub i'm going to call this one my hub we'll go ahead and hit review and create and we'll create and these crate very very quickly so we're not going to have to wait too long we need to set some shared access policies here so i'm going to go into the hub here i'm going to hit manage i'm going to call this my sas sas shared access policy and there we will now have the ability to have a primary key and connection key so we can actually connect to it so what we're going to do is go to github and i want to create a new repository and we'll go ahead here and we'll go down below go to exam pro we'll say my event hub that's already taken because i've done this before so for mine i'm going to call it new and we'll go down to private we're going to use node.js so let's type in node.js so that it ignores the node modules you're going to want to have getpod or a visual studio code installed on your computer the easiest way is to get pod because these environments are temporary and it's free to utilize it so if you can go get the chrome extension or if you don't want to install the chrome extension all you got to do is attach this to the end of the repo to launch a github or get pod environment so give that a moment to launch and there we go so i do have some code uh for this so i'm just looking for it off screen here and we're gonna need to have a couple files here we're going to need a new file called send.js and we're getting a new file here called receive.js i'm not typing the full word receive because i'm always really bad at spelling it so i'm just trying to save myself some trouble and we're also going to have to initialize a new package.json file so there we go and we're going to have to get a couple things installed yeah so we'll need mpm install azure event event hubs azure storage blob azure event hubs checkpoint store blob save dev and make our lives a bit easier it seems like i typed something wrong here i'll just hit up i forgot the forward slash here and so we'll just go event hub azure javascript because i believe yep i kind of use this one but i modified it to make it a little bit easier i think this is the one i was doing no i don't think so tutorial might be this one yeah it looks like this um so this is just the javascript there so for send we will grab this code here we're not going to do exactly the way they do it but pretty close and then there is a receive code so go down below and we'll grab this as you can see there's a lot going on here that will be our receive a couple things we need to properly set so these will be all environment variables so what we'll do is go to the top here and we'll just do constant process requires process that's going to allow us to import our environment variables so we'll just say copy let's save that we'll paste that in there as well and then this is where we need to replace all environment variables as you can see i always have things pinging up on me here so just close my teams out and just line these to make this a little bit faster so default will be what we'll keep here this will be our storage connection string say process env storage connection string then this one will be process env container name then we need process env event hub name and up here we'll have process env event hub connection string and then we'll go over to our send here we'll do something very similar so process env event hub connection string and then we'll have process env event hub name so we need to set all of these i'm just going to copy this for a moment here i'm going to make a new file file new file sure it doesn't really matter we're just using this as a quick scratch pad and so what i want to do here is just delete out this part and we're going to do export on the end here we'll take out this one here and the idea is that we'll just set them all here and then we will make our lives a little bit easier when we have to mask something set these so we do have the connection string because we saw it over here so we will grab the primary one it doesn't matter which one primary secondary and that is for the event hub we called the the event hub was called my hub i believe we'll just double check what the hub was called yeah it's called my hub up there we'll need a storage account so what i'm going to do push this thing we get out of here get out of here i'm not i'm not trying to save a freaking file there we go hit escape a bunch of times and we'll go back here and we'll create ourselves a new storage account i'll actually make this in a new tab so we can see what we're doing so we'll go over to storage accounts and we'll create ourselves a new storage account we'll create this storage account in the same name space so we'll go down to my event hub we'll just say my or we'll just say my event hub 8888 again you might have to change it based on your standards but um or like what is available to you we'll go ahead and go review create and for the container name we'll probably just call container maybe container one we just have to wait for this crate to grab that connection string so this usually doesn't take too long just a couple seconds okay so there we are we'll go over to access keys and we'll grab the connection string from here i believe this one should work uh let me just double check yeah i think this will work so what we'll do is go back over here if it doesn't we'll find out pretty soon and we'll just generate out a shared policy uh we'll go ahead and paste that on in here and just double checking that this is the correct one this looks identical to that one that can't be correct so we'll go back here this is the storage account so we'll go oh i have to actually hit the copy button that's what it didn't do we'll go ahead and paste that in and so theoretically this should work so we'll go ahead and copy these we'll drag our terminal up a bit we'll paste these in here and what i'll do is just double check that they're here so we'll say env grep event hub so those are both set and then we'll do storage that one is set and we'll do container that one is set so these are all in good shape for our storage account we still have to create the container so go here and create a new container say container one we'll go ahead and create that we'll make our way back over here and then instead of just having export we'll do gp env this is just in case we have to restart the environment for any reason so that these environment variables get exported twice so we'll paste that in there i believe those are all set i had to hit enter on the last one there and let's see if our code works so we'll do a send um actually we have to set up two scripts here so that we can actually call them so one called here is called send this will be node send js and we'll have receive so rec just because i always spell receive wrong and i just don't want to have to type it a thousand times we'll have that there so now what we'll do is do npm run send see if that works it says a batch of three events have been sent we'll go and confirm that over an event hub if it worked if we go to the overview it should show us some messages were received sometimes there's a bit of a delay so we'll just give it a teeny tiny amount of time we'll hit refresh here because we know we sent them but while we're waiting for those to kind of propagate what we'll go back here is just kind of look at the code because we didn't really look at it so the way it works is you are defining a client and it will be the producer client um and then down below so we say okay a producer is someone that produces events it's very common in a messaging system to have a producer and consumer we're going to create a batch job and we're going to add them all to the batch job then we're going to send them all at once and it's going to close and it'll enable to complete and if there are any errors it will alert us about it so we'll go back over here and we'll do a refresh so i want to see messages messages would normally show up here so since i don't trust it i'm just going to run it again i mean clearly worked because there was no errors and we'll go back over here and not here but we'll go back here we'll refresh and i'm just waiting to see something here processing data this is like something that's really powerful with them event hub here so still don't see the messages just give me a second to debug this i've done this lab like four times so it should work but uh you know sometimes sometimes it's trouble so just a moment all right so i literally did nothing and now it's actually showing up in the uh messaging queue so you know or the hub the funnel so that's just something you have to consider is that sometimes you just have to be a little bit patient let's see if we can go receive those messages now by running the other scripts so what i'm going to do here is do npm run receive and it should receive the messages as long as something isn't typed incorrectly so we'll go back over here we've seemed to introduce a little mistake so i'll go ahead and save that we'll hit up and it should receive the events so i should print the three out so there we go and so we are streaming this or the consumer is technically um storage storage accounts but if we go to the storage account there's nothing really uh um intelligible in terms of what's in here so like there are stuff in here checkpoints i guess it saved a checkpoint i personally don't know what i'm looking at so i'm not exactly sure what the point of doing that i guess it's just saying the checkpoint is like the last point it wrote but if we just take a look at the code here quickly you can see it's called consumer so we get a consumer client there's a blob checkpoint store then we have the consumer client we are subscribing so it's saying hey are there any events let's consume them if there are no events throw a console log so tell us about it iterate through them then update the checkpoints so move it to the next point um just say hey this is where it is now and that's pretty much all i want you to do we can go ahead and save this code we'll say eventhub code doesn't matter what you name it there we'll sync the changes and we'll go ahead and clean up so we'll go back to our resource groups we'll go to event hub and we will then go and delete this resource group and we'll go here and there you go hey this is andrew brown from exam pro and we are taking a look at azure notification hubs so the goal of the service is to send notifications to mobile apps in order to increase the app engagement and usage you can use tags to filter which apps or users that will receive notifications and those apps can be registered in azure notification hub in two different ways the first way is installation model so that's where the app sends an installation request to azure notification hub including all the information needed for the installation to be completed this model is easy to set up and maintain and then from there we have the registration model uh so this app sends a azure notification hub registration request with the pns handlers and tags and azure notification hub responds with the registration id uh it's great for native or generic templates that can be used in order to define the message so um you know azure notification hub in order for us to actually uh show off how to use this thing we'd actually have to set up a mobile app and it's not easy to do so this is something where we do not have a follow along for i don't think it's a big deal because i really didn't see it on the exam but it is part of the material so we definitely should know what it does so we'll just continue on here and learn a little bit more about azure notification hub okay all right let's talk about the supported platforms that azure notification hub supports this gives you an idea of how how it can be used with different uh mobile frameworks uh because you know we all don't use iphones there's androids and other things like that so for android we're gonna see support through firebase cloud messaging if you do not know what firebase is it's google's cloud's platform as a service it's a way of building out um applications similar to uh it was amplify or supabase but uh the component for messaging is through that sdk for cloud messaging this used to be called google cloud messaging so if you go through the documentation you might see this this does not exist anymore you have to use firebase cloud messaging okay for ios we have the apple push notification service apns and so through that it's going to be notification hubs for ios 13. and uh the way that works is through objective c sdk um i don't think there's support for swift at least i could not find it but this is only important if you are a native mobile application developer but basically the idea is the objective sdk is how you would interact with it there's also notification up for zamarin i always say zamarin wrong for ios applications samarin is i'm assuming the x is pronounced like an s i don't know why i just assume but um that is a uh a technology a framework built by uh microsoft that allows you to build out native applications for a variety of things like ios and android and things like that and windows phone of course for windows phone the way we're going to communicate is through the microsoft push notification service so mpns for windows because i suppose you can build a windows application not just a windows phone this is through the universal windows platform so uwp app uh and from there that is the windows push notification service of wns so you know this is just to give you an idea of the broad stuff and get you exposure to a lot of these terms again i don't think that's going to show up in the exam but good to know for azure notification hub okay all right let's take a look at how message flow works for azure notification hub so notice this tower here so this is um the platform notification service by your provider so you're using apple they have their own microsoft has their own et cetera et cetera and the idea is that notification hub is connected to that and that's how notification gets pushed out so notification i was just pushing to this tower and then this tower pushes out to tablets that are subscribed okay let's go through the flow here because there's some interesting things about handles we need to know so the idea is that your app says hey i want to subscribe to push to push notifications so it sends a request to your platform notification service and it's going to get back a handle that handle's going to vary so if you're using windows uh notification service you get back a url a uri and for apples you're going to get back a token so then you have to store that token into your application so it comes back to your phone your tablet whatever and then it goes okay to the back end and then stores that token here and then the idea is that you can then use that token that whenever your app wants to notify all devices that are uh registered for push notifications it's going to then push it out and with the handle it's going to go to the platform notification service and then that's going to roll it out and then all your apps are going to receive that information so hopefully that makes it uh pretty clear how that works okay all right let's take a look at some key concepts or components that are in azure notification hub to just kind of cement what we learned about the message flow and just the service in general so azure notification has these key concepts we have routing this is the process of selecting which registrations will receive the push notifications we have the id of broadcast all registrations will receive the push notification a tag registrations that contain the specific tag receive that push notification a great way to narrow down who you want to notify there are tag expressions so registrations that match the tag expression will receive a push notification so that is an even further way of uh narrowing down who we want to uh receive a push notification there's the notification hub name space so you have to create a group of one or more notification hubs in order to send stuff out tags are used to label applications so that they can be routed more efficient efficiently which is what we were talking about here just a moment ago but tag and tag expressions you have templates these define the format on how the client will receive the push notification and it's platform specific um okay each platform is this is platform independent so platform specific i think is probably a better word there each platform has its own platform notification service which is in charge of push notifications so the idea is that uh push notification hub or azure notification hub is just basically an adapter to all these a variety of services that are offered by these other uh telecom or notification companies okay hey this is andrew brown from exam pro and we are taking a look at queuing so both azure storage queue and azure service bus have overlapping responsibilities because they're they're both messaging or queuing as a service and so just before we jump into both of these services i wanted to distinguish the differences between these so you can kind of identify the use case for them so we have azure storage queue this is a simple message queuing services store large numbers of messages then you have azure service bus this is for broader messaging service that uh supports cueing pub sub and more advanced integration patterns so let's talk about the storage queue side of it so this can handle millions of messages there's no guarantee of the order of messages there's at least once delivery which is really nice 500 terabytes max q size 64 kilobyte message uh kilobyte yeah kilobyte message um and 48 kilobytes if you're doing base64 encoding so that's the size it can be after the encoding unlimited queues unlimited concurrent clients uh at least based access mode so we got 30 seconds to seven days uh set for the entire queue and then on the righthand side looking after service bus we it can offer first and first out which is uh very useful guarantees the order of messaging be being consumed that's what a first in first out means at least once or at most once delivery um we have one gigabyte to 80 gigabytes of max q size 256 kilobytes to one megabyte so a lot larger than azure storage queue we only have a maximum of 10 000 max queues but that is a lot of cues i don't know what you're doing with ten more than ten thousand five thousand concurrent clients uh lock based access mode and that is um with 60 seconds can change settings per message and it has dead letter uh support which is very useful state managed support message group support uh deduping support purging uh queue support and transaction supported so um you know the thing is is that it looks like service bus wins in all categories but again remember that storage queue is simple and but because it's simple by design it can handle millions of messages and if you don't care about first and first out it makes sense um so you really have to decide in your use case there probably is a pricing difference which we i think that we do cover in this course but you know hopefully you get an idea that these two services do the same thing but in different ways okay hey this is andrew brown from exam pro and we are taking a look at azure queue storage which is a simple uh message queue that allows you to exchange cloud messages it's used for application integration between services and it allows that via an authenticated https endpoint or protocol and it can accept messages of 64 kilobytes in size so the q storage is a service or a product underneath the azure storage account so you first have to create a storage account uh and then from there you create your queue but you'll be using the same access keys and connection strings like the rest of the other storage account resources there are three ways of handling messages for azure storage azure queue storage the first is peak and as it says it will retrieve or look at a message in the queue without removing or locking it you have delete so that will delete a message that is being processed and received so that is a message in the front of the queue uh but it gets locked temporarily while it is being used okay um and just to kind of give you a visual of how simple it is to use this queue you can uh in the ui in the azure portal you can open it up write a message set an expiry and just say okay but the way you're generally going to be using it is not through uh the ui because these are for application integration that means you're going to be writing either sdk code or utilizing the cli uh and i believe that azure queue storage is uh one of the ones where you can actually use cli we do cover that as the difference between um the other queueing service that you can use in azure um but here's an example of the azure sdk it's a python example so just getting my pen tool out here it is a bit brief but the idea is that we have our q client we have our connection string url which we're getting from our storage account we're putting the name of the queue and then we just send a message so it's that simple and that is a python example let's just talk about some of the components within a queue there's not a lot here so it's not too complicated but the idea is you have a queue this could be accessed by using a url format of the following so you'll have your storage account here qcor windows.net and then the queue name um would this show up in the exam probably not but it's generally good to know just in case because azure exams are very tricky and then we have also this url here as an example so i think it's reflecting the example on the right hand side here so the idea here is that we have my accounts this is my account and then we have images to download so images to download would be one particular queue okay so just to kind of hit home that url there we have a storage account so this is required in order for us to make the queue as we mentioned you have the queue itself this contains a set of messages the queue names must be in all lower case that's just one of those uh interesting um one of those interesting use cases or edge cases messages can be up to 64 kilobytes in format so it's very small so this is where you might want to use the other queueing service uh within azure which i can't remember but we do cover it when we get to that section uh and there is some um versioning that's a bit different so if you're using uh before 2017 the ttl on it is a max of seven days if you're using uh after 27 2017 um it can be any positive number or negative one indicating that they don't expire and it doesn't say um i guess it can be whatever number you want to be i don't know what the maximum is but i mean what's important to know is that the old one gives you seven days so you're going to be wanting to use the latest version to have the most flexibility the default time to live is seven days if the parameter is not specified which is totally fine let's take a look at some of the cli commands because uh you know what i'm making this course is specifically for the az204 uh which means developers so you're getting lots of handson stuff so you should really know these cli commands we do cover them in the follow along here but the idea is we have uh the az storage because we're using storage count message actions so that's going to be automatically for the queue so we have clear so delete all messages from the specified queue uh delete to delete a specific message get so retrieve one or remember messages from the front of the queue peak so retrieve one or more messages from the front of the queue but do not remove them do not change their visibility do not lock them put so add new messages to the back of the message queue update updates the visibility time of a message so these systems are very very very simple but they're also highly scalable which is really great um but yeah there you go hey this is andrew brown from exam pro and in this follow along we're going to learn all about storage queues so what i want you to do is log into the azure portal and at the top here type in q and so that's going to show you it's going to bring you to storage accounts because it is not a separate service it's uh underneath storage accounts so what we'll do here is we'll create ourselves a new storage account so we'll hit new or create there give it a moment to load and we're going to create ourselves a new namespace i'm going to call this one az204 um az204 azure storage account uh actually we'll just say um storage account queuing or queue we'll say okay there this will be called uh az204 storage account queue and it doesn't like the hyphens maybe there we go and we'll let it launch in whatever region it wants to here it's going to be e east u.s for performance we'll stick with standard we'll stick with a redundancy of georedundant storage it's not going to matter for those options for using the queue we'll go to advanced to see if there's anything interesting there nothing in particular i was just checking if there's anything for the queue so we'll go ahead and hit review and create we'll scroll on down and hit create and we'll give that a moment to create so uh in order to utilize the queue there isn't much in the ui that we can do in the portal and so we are going to need to utilize the azure cli so um you know there's a few ways we can go about this um we could launch up visual studio code but since it's all cli driven what i'm going to do is go ahead and actually open up cloud shell and that's the way we're going to interact with it now this is your first time spinning up cloud shell it might ask you to create a storage account so just let it do that and now this resource is complete we'll click into here and on the left hand side we're going to make our way over to cues and i'm going to create a new queue i'm going to call that my cue and we'll hit ok so for this you know there wasn't really a follow along in the microsoft documents so i just kind of made my own and so for that information i have a repository and you'll find this throughout the course which is call i'm just looking up here on the side here the developer associate so some of these are reworkings of microsoft ones or they're my own creations so this one in particular which is azure storage queue is a bit more unique and so the things that we're going to need to do is we're you're going to need to have a connection string in order for us via the cli to connect it because all the commands we're using like put peak get all these things are going to rely on it in terms of what we want to run i'm going to type in azure q message cli because this is mostly what you'll want to know how to use is how to interact with messages there is a bunch of different cli commands um for azure storage like working with the queue and stuff like that but i'd rather just work with messages because that's the only part that we can't do here and that's where we need to understand the difference between peak get put and things like that so the first thing we'll do is get a connection string url and i believe that is for the queue so we'll go into queue here click into the queue because sometimes it is the access key that is um at the storage account level and sometimes like if you go here sometimes it is so there's keys here sometimes it's the cue so i think it is uh the q key that we want uh and if it isn't we'll find out very shortly here if we made a mistake so what we'll do is we'll go back to our resources here we'll click into the queue and in the queue again and we'll look at access policies and so we can add a policy here nope you know what it was it absolutely is the storage account that we need so we go down here and we type in access and we have access keys so here it is and between different azure services sometimes this is called access keys and sometimes it's called something else we're going to discover that as we do other follows just understand this is not always consistent based on the ui but what we need is a connection string so we'll click show up here and what we'll do is grab this string and so what i want to do is type in export q connection string equals and then double quotations very important that you put those double quotations because if you don't this equals will terminate here and it will only think this is a part that is set here so i'm going to hit enter and so that's going to set us an environment variable uh for this so if i did env hyphen grep and put in this this is how we can check if it was set correctly okay if you've never t have you ever done this if you type in env that shows all of your environment variables and so all grep is doing is saying find the one that's called this i could even do it like this right partially and it would return it here so that's one thing the next thing is we need the queue name so we named it i believe my queue so if we go back to our storage accounts here and let's just click back on here such a pain sometimes clicking around here we'll go to cues and we named it my cue so i'm going to set export q name equals and we don't need double quotations here but i'm going to do it anyway my q will make sure that is set correctly so we'll type in grep cue name and there it is so some of the first commands we'll want to use is things like put because put will allow us to enter stuff in so we said that there are these messages here so we'll go to the put command here you can see there's a lot of options when you want to know what you need to put in all i do is just copy the base thing and paste it in and then it'll just tell us so i'll just give it a moment here and so complains it says we need um the queue name we need the content uh and i already know that we we need the connection string url even though it's not saying it there because it might say hey you need actually down here it says connection string account name things like that so um just to make this easier i'm gonna go open editor here and this is a terrible terrible editor but we're working with uh like just some text here it makes our lives a little bit easier so what i want to do is just type it up here before we paste it and of course this is all the repository but i'm just doing it by hand and i recommend that you do it by hand like i'm doing so that you have a better chance to commit to memory so we type in azure storage message message put i'm going to do a backslash so we can do multiline here we do connection string equals and we set that as an environment variable called uh q connection string when we do this dollar sign that allows us to access the environment variable that we said earlier and we did that is because we have to run a series of commands and it was just a lot easier to pass this in consistently as opposed to passing in that raw string every time so we need to provide the queue name so this would be q underscore name and then we need to provide it content so we'll say hyphen hyphen content equals and i'm gonna say hello world and so we will copy the contents there i'm going to just type clear here so i can see what i'm doing we'll go ahead and paste that on in there we'll hit enter and uh yeah so it is now in the queue now one thing that i was not able to figure out was how do you observe how many questions are in the queue i'm not exactly sure so if we go into here i'm not sure if we get any statistics oh well we have a message right here so we can see there is one in the queue but what i was trying to say is that when i was using the cli i was trying to find a cli command uh to find that and i just never did but it's nice to see that we can see it visible here in the actual console here i want to have more than one message here because i want to show you how visibility works and so what we'll do is just paste command v there is no if you notice right click there's no paste here which is kind of weird and so i'm going to do here hello mars as our second one and we'll copy this here copy it paste it on below hit enter and so now we have two queued up if we refresh we can see we have two messages in our cube we can even add a message here if we wanted to so we could say hello jupiter and hit uh maybe i have to scroll down here oh no it vanished i gotta do it again maybe bring it down all the way down here add message there we go hello jupiter we'll say okay and notice that we have some options there where we can set the expires in x amount of time or whether it never expires and things like that but anyway what we'll do is um now we should probably attempt to peak or get so what we'll do is go back to the top here because i think they list them out so we have guests so retrieves one or more message from the front of the queue and then you have peaks it retrieves one armor message from the front of the cube but does not alter the visibility of the message so when we say visibility the idea is that when somebody accesses a message they're now looking at it right and so nobody else can read it and the reason why that happens is to avoid people two people doing the same job by accident but if we use peak that means that um it doesn't become unvisible um it will just remain the same so let's do peak first and so what we'll do is we'll just copy this here and paste this in oh we have to control v there and we'll just get rid of the content line because we don't need to pass content we still need the cue we still need the connection string and i'm going to change this to peak and so what peak should do is it should return us what's on the front of the queue so this is the top up here we'll go ahead and copy it then we'll right click and paste down below we'll hit enter and so we get hello world if we were to hit up and run it again we would still get hello world because it has not been marked as uh invisible right it's still visible so now let's go ahead and change this to get oh we got to paste ctrl v so we do get here and copy can control c or command c depending on what you're using and we'll paste that in there and hit enter and so now it says hello world as it did before both times we ran hello world it was always returning the same one okay notice it says dq count is one so we'll go here refresh and notice that hello world is not appearing now right so it's because it's not visible it's still there it's just not visible right um and so the next thing we can do is if we hit enter again now types in hello mars okay and uh the next thing we can do here is uh clear the queue if we want so that's probably something that we should do so if we were to go here and copy this type in clear and hit enter that should clear the entire queue so hopefully that gives you an idea of how the queueing system works of course you can use the sdk so like if we were to type in azure storage queue documentation and we were to go to tutorials there's probably samples there's probably like examples i always go to javascript because it's the easiest one to read and if we go down to messages here send a message not really great that they don't have this in line but the way it works is you probably import the library at the top so you'd install with npm install azure storage queue you have the client the credentials and then down below here you're creating your message i guess you have to create the client queue first up here and then you pass along your message sent and then it says whether it's been sent or not here you can see that they're using peak right very similar receive message all very similar here so hopefully that gives you kind of an idea i think like forget or sorry like um notice like it's called receive message as opposed to get so in the cli uh we did get but this is really receive right so just so that translates over these queues are extremely simple but they're designed to be extremely simple when we look at service bus you'll see that it's a lot more complex but one of the key differences at least pragmatically with service bus and storage queue is that service bus allows you to use the cli to insert messages where when we do service bus we have to write code using the sdk and we will do that but yeah that's all there is really to this one that i want to show you what we'll do is make our way over to our resource groups i'm going to just close off our shell here we're going to look for the one we just created here so this is azure storage account cue we'll go ahead and delete that delete and it should all be good of course always double check five minutes from now make sure that stuff is deleted because azure is notorious for sometimes not deleting things or you think that things are being deleted but this won't show up immediately it will take some time to propagate through their servers but there you go hey this is andrew brown from exam pro we're looking at azure service bus so azure service bus is a fully managed enterprise message broker that allows you to publish and subscribe to topics and queue messages it can scale with your applications with asynchronous messages and builtin integration with azure services so here is an example of a service bus and so you know the way it works is you have producers you have consumers and so producers are sending messages or events into an event bus the event bus is designed to broker that information so pass it along to the ones that are subscribed so these applications which are consuming are subscribed to specific messages and they pull data to um get them into those and target end applications some things we need to know is you can handle single or batch messaging load balancing messages topic subscriptions manage uh managed sessions and transactions with guarantees that it compiles with industry standards and protocols like the advanced message queueing protocol so amqp java message services so jms and other protocols there so they're just saying like yeah if you're already using very particular protocols you can integrate with those let's talk about key concepts for service bus so the idea is that you have a namespace which works like a server with nqs and topics you have cues which contain the messages you have senders who send the messages receivers who receive the messages topic which is a queue with multiple receivers that work like a queue a subscription is a receiver in a topic a batch is a group of messages a safe batch validates if each message can be included in the batch sessions allow you to use first and first out and group your messages in a queue peak returns a message to the queue without removing it dead letter q a queue for messages that were unable to be delivered through the normal queue peak and lock so this retrieves a message from the queue without removing it and locks it so other receivers cannot receive it receive and delete so receives and deletes messages from the queue auto delete on idle so sets a time span to delete the queue if not used duplicate deletion detection history so checks if the message was not sent earlier before sending it a message so there's a lot of stuff here but don't worry we're going to kind of cover it the most important stuff here in the upcoming slides and in the fall long okay let us talk about namespaces for azure service but so simply put namespace is a container for all messaging components so cues and topics but there's uh you know a few options here i don't have a visual but there's some text we have to go through here to understand some of the things that namespaces do so a single namespace can contain multiple cues and topics and namespaces are often used as application containers so a service bus namespace is a capacity slice of a large cluster made up of a dozen of all active virtual machines that you control it may optionally span three azs or availability zones in azure you get all of the the benefits of running the message broker at massive scale in terms of availability and robustness a service bus is serverless messaging so you don't need to worry about the underlying complexity which is kind of interesting because they're mentioning about spanning across availability zones and virtual machines but uh you know it's just a component that's you know you don't have to worry about the management of it but you do need to know a little bit about how it works underneath but there you go hey this is andrew brown from exam pro and we are taking a look at cues for azure service bus so there's two types of uh messaging systems in azure service bus why they didn't make these two separate services i have no idea but one is for messaging and one is for pub sub and and the one that is for messaging is called cues so cues are used to send and receive messages messages are stored in queues until the receiving application is ready to accept and process them messages and cues are ordered and timestamped on arrival once accepted by the broker the message is always held durably in triple redundant storage across uh multiple azs if its namespace is zone enabled so if you don't enable zone enabled at the name space level you're not going to get that kind of durability or redundancy service bus never leaves messages in memory or volatile storage after they've been reported to the client is accepted and messages are delivered in pull mode only delivering messages when requested that's the key difference between a queue and pub sub cue uh the receiver you know the application has to pull say hey are there any messages in the queue and that's how they get them in a pub sub model they just get it sent to them like a subscription like a newsletter subscription to your door let's take a look at the queue configuration itself so you can see we can set the delete max delivery count things like that the queue size important things to note is the ttl so how long a message will stay in the cube before it's removed from the queue or removed or moved to dead letter q you can enable dead lettering down below you can enable partitioning here you can set the lock duration very similar to what we'll see for uh pub sub i think at the subscription level but there you go hey this is andrew brown and we are taking a look at topics so topics can be used to send and receive messages and the idea here is you have a queue which is often used to facilitate pointtopoint communication and topics are useful in publish to subscribe so onetomany communication so if you want onetoone communication use cues if you want pub sub then you use topics and both these options are available for azure service bus so topics are not available at the basic pricing tier you need to have standard or premium so if you're trying to find that option for pub sub that's where we're going to have it remember that we said that queues use a poll model well topics use a push model so you don't have to request it just gets pushed out to you because they're subscribed to it like having a newsletter delivered to your door so multiple independent subscriptions can be attached to a topic and work in the same way as cues from the receiver's side a subscriber to a topic can receive a copy of each message sent to that topic subscriptions are named as entities and so here's an example of sender and receiver you can define rules on a subscription a subscription rule has a filter that specifies a condition for a message to be copied into the subscription as well as an optional action that modifies message metadata so let's take a look at what the form is for topic so the idea is you name your topic you have a max size between one to five gigabytes you can have a time to live so after that period of time those those messages or event data is just goes poof so it's just gone you can avoid duplication by turning on enabled duplication or duplicate detection you can enable partitioning and partitioning is intended for scale so if you have a lot of messages you might need to partition based on your volume okay let's take a quick look at subscriptions for azure service bus so i just pulled up the form here because a subscription is just the um the application that is receiving that information so you just define one in the interface here but there are some values that i want to point out such as max delivery count which can be between one to two thousand uh this managed session so with sessions enabled the subscription can guarantee first in first out so fifo delivery messages so that is where that gets turned on you can also enable dead lettering here um and you can have a lock duration and adjust that there which you know i didn't highlight here but it's here there but we will see these things a little bit closer when we do the fall longs but yeah a subscription is you know an application that is receiving information getting pushed information to it that it will use okay let us explore the pricing for azure service bus so they have three different pricing tiers basic standard and premium the more expensive tiers provide more functionality which i would hope that would always be the case for most services but on the right hand side we have our available pricing uh and you can see it's based on a metric so like mu's per per month and things like that or operations per month and so here is the chart so let's take a look at what the differences are so i highlighted topics in yellow to show you that it is not basic it's not in the basic tier you have to have standard in order to utilize that and you know for your purposes as a person learning as a service bus you absolutely need to uh give topics and queues a go because qs is the classic messaging model and topics is pub sub and the only way you're going to do that is by using standard which we will do in the follow alongs but notice in basic we don't get topics transactions deduping sessions for to send via no resource isolation geo disaster recovery java messaging support az support and then some of those aren't supported in standard but then in premium we get basically everything and then notice like the big bump in terms of the message size between basic standard and premium so again the thing i really want to remember is that in basic there are no topics okay all right let's take a look here at dead letter q this is just a general concept that is common for messaging's pub sub systems and is enabled in azure service bus for both queues and pub sub but a dead letter queue is a service implementation to store messages that fail to deliver and common reasons for failed messages would be things like message that is sent to a queue that does not exist the queue length limit is exceeded the message length limit is exceeded the message is rejected by another queue exchange the message reaches a threshold read counter number because it's not consumed sometimes this is called a blackout queue or backup queue sorry the message expires due to per message ttl so time to live the message is not processed successfully so dead letter queues hold messages that uh that have failed to deliver a dead letter queue could be used for things like monitoring failed attempts requesting failed attack recueing failed attempts so try it again or trigger a followup action so maybe a remediation or a response so very useful feature but there you go let us talk about azure service bus command cli or cli commands because if you are doing the az204 the developer it's very important to know these these commands because they may ask you them so here i have a bunch of example commands so just quickly looking at this here we have creating a resource group creating a service bus namespace a queue an authorization rule and so it's pretty straightforward you know az service bus etc let's take a look at some commands we should know or sub commands i should say so when we say command the command is service bus and the sub command is what follows that so we have ones for geo recovery alias so we want to set the geo recovery for it we don't have an example here on the right hand side for migrations uh for namespace for queue for topic and so on the right hand side there we have one for namespace we have them one for creating a queue but you'll notice here that we don't have anything that kind of manipulates or adds messages to cues so the azure service bus does not have cli commands to send messages to the keyword topic unlike azure storage queue which i found out which was kind of interesting so to send messages to the queue you need to use an sdk like node.js so you'd install it and then you'd have to write some code and we do do that in the follow along but i just want you to remember that azure service bus you cannot send messages to the queue or topics via the cli and you know that kind of sucks because you know if you want to do some easy test uh it's unfortunate that functionality is not there but anyway that's the difference between that and azure storage queue so there you go hey this is andrew brown from exam pro and we are going to take a look at service bus so what i want you to do is type in service plus at the top here what's interesting is that this is the old icon they have a new icon so just realize that there's some inconsistencies there and that's not my fault that's azure's fault but the first thing we need to do is create a namespace because a service bus is kind of like you know storage account where you can have a variety of different kinds of storages well you can have uh more than one type of messaging system and so we have our traditional one uh like event messaging similar to storage cube but with first and first out functionality and we have pub sub via topic so what you'll do is create a new service bus namespace and i'm going to create a new resource group and call this the az204 um service bus and we'll say message or sorry cue because we're going to do a queue and then we're going to do a topic separately and for this i'm going to call the namespace service bus queue to keep it simple we'll let it launch wherever it wants to launch and notice there are multiple pricing tiers depending on the tier affects the functionality so if we do basic we're only going to have access to queue we're not going to have access to um topics and so this is totally safe and fine to do like even if we did premium it's fine because it's based on your consumption it's not based on um you know you're just having it holding around so we'll get the basic one here and we'll go to networking i don't think there's anything interesting there we'll go ahead and review and create and we'll let that create click create again and it's deploying as that is deploying which will not take too long what i've done is set up a private repository here you'll probably see me use this throughout the course it's literally an empty repository because i already have the code done i've been doing the followalongs and documenting them here in the free azure developer associate but when you're doing followons with me you should do them with uh do them from scratch and then if you need to you can reference the stuff here so um i have this separate repository i have a git pod account which has a free tier you can totally do this in your own visual studio code on your little local machine the reason i'm doing a git pod is i always want to show you how to set up the cli and those other tools and when you launch git pod it gives you a blank environment so i'm just going to launch that up there as that's going we'll go back here and take a look and see if this is ready just hit refresh here it is still going but we already have our environment and while that is going in the background i want to go install the azure cli so we don't even have a single file here i'm just going to say readme.md so i can see what is going on here maybe we'll just dump things in here as we go i'm going to go get the azure cli linux because this is running linux ubuntu here so something you should always check is like what linux version am i running if you're on windows of course this is going to be different but even windows using the windows subsystem linux is using ubuntu as well so what i'll do here i'll go to the first link nope that's not the one i want maybe the second one there's usually like a command here i can run uh which linux version am i running let's try this one here it's usually like so maybe it says here cat proc version it really does vary based on what you're using and so here i'm going to go file uh or sorry terminal new terminal we'll paste that in there hit enter and so here it says linux 513 ubuntu 11. uh 18 so i know that this is ubuntu i don't really like that one there um let's try this instead because this just doesn't read very well there we go we're running ubuntu 2004 i already knew that um but i just want to double check and the reason that matters is that when you're installing the cli it might matter what version you're using so we're going to go here and i'm going to go to linux and the instructions might vary this one says 16 18 20. so they're all the same here and we have this one liner here that we'll install and what i'll do here is drop it in here and hit enter not sure if this font is too small so while that's going i'm going to see if i can bump up our font here i'm looking for the terminal font size here terminal let's just say 20 here there we go and so the azure cli should be installed so i'll just type in clear so let's say azure or az to run it looks good to me so we'll type in azure login um i don't want to log in with um that way i want to log in with device so we'll do a z login device because if you're on your regular computer you can just click a button and go to the browser but i'm not gonna be able to do that so i'm gonna have to do a device login device well i'll have to do it the wrong way first to do it the right way so hit enter because the problem is if i go here it's going to go to localhost because it's trying to launch in my local machine so it does that and that's no good and so here it says do the az login use device code okay so that's the one i really wanted to use so hyphen typing use device code enter and that will give us a code so what we do is we will need uh this link here so i'll have to expand that to here and then i'm going to go ahead and grab that code continue let's now go back this will authenticate it'll just take a second here close the tab here there we go maybe i have to close the tab uh and so now i'm authenticated so i should be able to uh do whatever i want um what i need to do next is create ourselves a message queue so we'll go to the resource here and notice here the entities it always says q now if we had uh other than the basic the standard plan then we would see topics here we'll go click into q click create a new queue i'm going to call it my queue we have some options here the queue size can go up to five gigabytes the max delivery count so this is the maximum deliveries uh time to live that is how long they live in the queue before they are dropped out or they are dropped into a dead letter uh a system there we have lock duration so the set the amount of time a message is locked for other receivers you can enable partitioning uh that's pretty complicated but we'll go ahead and create our cue and so this should be pretty darn quick there is our q we'll click into it and you'll notice that there isn't really a way to view messages there's not a way to add messages uh here we have the service bus explorer which i guess technically you can send and receive here i had not noticed this before at least it was not working for me so i suppose we could send a message here saying like hello world this literally wasn't here last time i checked here um and we can go ahead and just hit send okay and notice here it says there's one active message and we can receive it say yes and so it says it received the message it's not showing us the answer so i guess there kind of is something here i guess they're still working on it but uh mostly what we're going to have to do is do things programmatically so that is why we have this account so what i want you to do is open a new tab here we're going to type in azure service bus documentation because we're going to grab some code there modify it make it our own so it's a bit easier to work with so here i'm in the service bus we'll go to tutorials i'm not sure if this one is the right one azure service bus documentation queue it's the same thing here but this doesn't look right what is service bus i mean it is the right page but it had a couple tutorials here uh that i had here so we'll type in like azure service bus uh tutorial topics sometimes things aren't where you think they're supposed to be okay i'll type in service bus seems like the same page again oh it was quick starts sorry so we have tutorials here and then we have quick start so under the quick starts this is where i was finding uh the example code that i thought was okay notice that we could do everything via the cli um that is not that fun but i mean this only does the creation of it doesn't necessarily do sending and receiving messages notice so that we only can use code so we'll use javascript because i think that will be the easiest to use uh so i already have node node comes preinstalled on git pod you'll have to figure that out for yourself on your own machine or you can just use git pod as well because it does have a generous free tier what i'll do is go ahead and paste on in this command it doesn't seem to want to paste today so i'll hit copy and then we'll go back here and go right click enter paste hit enter and so what that will do is install that library if you're not very familiar with node.js package.json is the package manager and this is showing that this requirement is there i want to install one other thing uh called env this will make our lives a lot easier um for node it comes for different things but i just wanted for javascript here so then we'll do npm install emv hyphen save that's just a safe way for us to pass along our environment variables and so now both of these are installed so what we'll do is we'll go back over to this code and we'll scroll on down and they have one called send and they have one called receive so what i'll do is create myself a couple files here so we'll have send.js and we'll have receive.js and then what we'll do is go ahead and copy this is the send code so we'll put this in the send js file and then down below we have the receive code and we will paste that on in there so i'll just make this bigger and we'll take a quick look here at what it's doing so what this does is it imports the sdk for service bus we need to set a connection string we need to set the queue name here is a bunch of messages that we are going to be passing along here we establish a service bus client very common in all sdks to set up a client first then we are creating a sender and then here we are doing uh create batch messages so it's a way of sending messages in batch very efficiently so we have a for loop here and uh so it says there's a batch and then it says try to add the message to the batch if it's not wait until it's ready then send the message okay so pretty straightforward for that code receive is going to be similar so connection string queue name create that client create a receiver and then from there we will set up a handler for the receiver an error and so then we'll subscribe and we will listen for the message and handler so even though we are doing cues it's called a subscription still so just don't get too mixed up with that what i want to do is just make sure that we're passing our environment variables in safely our configuration so this is pretty standard or good best practices when working with any language the idea is you don't want to hard code your values so i'm going to do process env and we'll do connection string here and then we'll do process env cue name this is the way you grab environment variables in javascript for every language it's a little bit different okay and i believe these are the same so i'm just going to go ahead and grab that there like this and i'll paste that on in here and i want to load environment variables so i'll make a new file here called env this is all part of that env dot thing then we're looking at emv dot i'm just pulling that up again here or dot env you're gonna get the right one because we need this line here require dot uh dot env config that will load the environment variables uh it'll load it from that env file so we will go above here and hit paste and then we'll go to the receive here and do this as well and in here we need to define these so i'm just going to copy this so i don't have to type it out by hand we will paste that on in here and so i just need the q q name and connection string we'll just say equals and then equals so our q is called my q and then we need to go grab the connection string so i'm just thinking here this is probably yeah it's at the name space level and we'll go to shared access policies notice it's called shared access policies remember when we're doing the storage key it was like called key access so it's totally different interface this is what i'm talking about where azure is inconsistent we're clicking on the root manage shared access key probably could create your own so it doesn't have full privileges but for this purpose we're just going to use this one on the left hand side we have a primary and secondary we're going to use the primary one and we will go back here and we will paste on that value in so i'll paste that in there notice we don't have to do double quotations here it should already escaping double quotations but we're doing the cli when we did the storage accounts that wasn't something that we could do um so we have these two values here so they should get loaded when we use them and this should all be good so we'll type in node send js and hopefully it just works fingers crossed and so it sent a batch of messages to the queue so we'll go back over to our queue here and see if we can see anything and we'll click into here i'm just trying to see so there's 10 active messages that are here right now and so what we'll do is we'll receive all those messages so we'll go up and hit node receive js and so this code is now receiving those messages from the service bus queue and we're just going to wait here because it takes time for whatever reason to finish here but we'll give it a little bit of time here to figure out that it's done still waiting there we go and so that's all there really is to it so that is q and we will do this again but next time with topics so what i want you to do is make your way over back to your resource groups we'll find the one that we just created which was uh this one here az204 service bus queue we'll delete this service group hit delete delete and there you go and as always double check to make sure that you've uh for sure delete that stuff and that's it for service bus q will do topics next okay hey this is andrew brown from exam pro and we are looking at service bus and this time we're looking at topics so what i want you to do just like before is go to the top we'll type in service bus and we'll go to the service bus service you can still see the old one is there it should be deleting that's how slow this thing is but we'll create a new one and we will create a new resource group and we will call it az204 service bus topic we'll say okay we'll name this service bus topic and this time it already exists if it does just dump a bunch of numbers here on the end because it's unique based on that so that's not having a domain name if somebody has it you're gonna have a problem and so here i'm going to go to standard because in order for us to use those additional features we will need to be on the standard plan so we'll go ahead and hit review and create and that'll take a little bit of time to create but while that's going i'm going to launch my environment here so this was the one i was just using a moment ago with github and so what i'm going to do is i'm just going to go back to our repo here i'm just doing it off screen because i don't want to expose all my stuff here and again if you want to you can do this in your local visual studio code i just want to show everything from scratch every time so here's my empty repo with git pod and so i'll just close this one and it will vanish all that code is now gone i'll let get pod this will launch a new environment it's trying to tell me to open the last one nope i'm gonna make a new one for this workspace here and we'll get going here in a moment so this namespace has been created so we'll go ahead and hit create and uh i guess it's still making i thought i already deployed it there but i guess i didn't okay while that's going we'll go ahead and install the azure cli so we'll type in azure cli linux because that is what we're using here today we'll go to linux here we'll scroll on down grab this one liner here to install it i'm going to open up my terminal your terminal might be somewhere else allow i'm going to go ahead and paste that on in there that's going to install our azure cli as that's installing it will be done here in a moment let's see if this is done this is still creating the namespace i think and this is still installing shouldn't take too long while this is going we can start grabbing the code for this so for this we'll type in azure um azure service bus documentation and this i found it under the quick start so we will go to quick start serve topics and services because there's some code here that i want under the javascript and we have one for send to topic so we'll just grab that name there make a new file sent to topic.js and we'll go down here to this other one this is receive from subscription you're going to notice this is very similar to doing a queue the difference is that you can have multiple subscriptions consume the same stuff our cli is done so we'll type in a z login um i can't remember what it is we'll hit enter here i know this is the wrong way but i just can't remember what it is i want use device code that's what it is so i'll go ahead and grab this and i'll just hit ctrl c to exit out of that and then paste that in there and then we'll do it the way that we actually want to do it so then we'll go ahead and grab this for device login we will provide the code as suggested here we will hit continue we will close this and it will say that we are authenticated so we are now authenticated we can use at az or the azure cli um i don't know if we need the cli well we have it anyway at least we had to authenticate so at least that was out of the way i'm not sure we're actually going to use the cli in this one um but what we'll do is go to the resource here and we want to create a topic this time around and i'm going to call this my topic we have a topic size between one to five you have a ttl you can do make sure there aren't duplicates we're gonna go ahead and create this topic okay so just like last time we need to install a couple things so if we go to the top here this should be like an npm install here it is npm install service bus so we'll paste that on in there we'll need our dot env so lookup.env again and we will just install npminstall.env for environment variables hit enter we'll create ourselves a new.env file and we will need to look at what environment variables we are going to need so uh looks like we didn't copy the send to topic content at least i didn't so we will go back over to here and go up here and get sent to topic paste that on in there we'll go all the way to the top and i'm looking for what we need here so here we have one which is connection string whoops do not mean to delete all that so we'll hit process env connection string and we have to spell it right or it's going to have a problem and then we have process env topic name i think in this one it actually also has subscriptions so we'll have to have a third one so i'm just going to copy these two over so i'm to type it 100 times and this one will be process env subscription name you're noticing i'm not having the um semicolon that is optional in javascript so it won't break anything if i don't have them there subscription name and so these are the three that we'll need i'm gonna just split this to make my life a little bit easier and i'm just gonna copy this here paste not exactly how i wanted to paste it but that's totally fine i'm gonna copy this i don't think we made a subscription yet but i know we're going to call it my subscription to make our lives easy so we'll say my topic my subscription and we'll grab this in a moment so we'll go back over to azure we got too many tabs open here while we're here we might as well go grab this code um before we go back to azure so i'm going to grab this dot env config that is going to load our configure configuration environments so we go to the top here paste that on in there for both files again best practice never to hard code your values always pass it in like that with environment variables we'll go back to our service bus the first thing we need well we made a topic we're going to need a subscription so if we go to topic here and click into it we can create ourselves a subscription so we'll go here we'll say my subscription notice we have max delivery account we have to set this between a value of 1 and two thousand i'm going to say 12 for fun we can set the idle notice here if we want to have first and first out we would check box enable sessions we're going to leave that alone does not matter too much for our demo so we'll go ahead and create that subscription it is created now what i need you to do is go back to the service bus topic namespace and from there on the left hand side we're going to shared access policies we're going to click into the root manage shared access we're going to grab the primary connection string key the secondary would work too it's just and second optional one because they always give you a two we'll paste that on in there and if this is all correct this should just work so now what we'll do is type in node um actually we did want the cli installed because i wanted to show you uh that there was stuff in the queue whereas with um storage queue uh when we're doing azure storage queue i couldn't show you because um i just didn't know of a command i believe at least i think that was the case but hey we'll take a look and see what we can see okay um so actually we didn't do it at any time i wonder which one i did that for let's just double check here i have um offscreen here somewhere my instructions because i run it for one of these maybe it was for the queue that we did it yeah i didn't do the last one but i did an azure service bus q show and the idea was to show you that there was a message count ten so you could see the queue but i think that since we saw it in the ui i just wasn't too worried about it now did i do it for this one i'm not sure yeah we do a topic show so we'll do that for fun but first we need to insert our messages which are part of the topic send i don't think we read through these so let's just quickly read they look very very similar to the last one so you have your messages that you want to send you create yourself a client you create yourself a sender you create a batch message send message like it's basically identical like i can't even tell the difference here except here we're supplying a topic name so um i mean i see it here ah here create sender the topic name is specified there so i just imagine that instead of providing a topic name you provide the queue name and that's how it knows the difference but anyway what we'll do is go ahead and execute this code we'll say node send to topic js okay so it sent the stuff now we didn't do this last time so let's do it this time around so we'll type in azure service bus topic show and here we'll need to set the resource group so this was called i don't know let's go take a look here what is our service group called it is called az204 bring that down a bit here az204 service bus topic we need to specify the namespace name so that's just called service bus topic hyphen 723 because we couldn't get the number we want and then we need to specify the name so i assume it's the name of the topic so the top is my topic and i'll hit enter and it says service business name is misspelled or not recognized by the service did you mean service bus yeah i gotta spell that right hit enter and that looks fine but i just want to specify it as output yaml just hit up on your keyboard if you want to go back to those previous commands let enter so this is a little bit easier to read and so what we're looking for here is just kind of like the message count does it show us here is the description count i don't see it so i guess it's not visible in the same way as uh the queue is like if we did this and we didn't do the last one it would we just thought like that message count there but let's take a look at what we can see in the cli to just see what information or a ui that our portal so we can see some information here so we have one subscription here we'll click into here um we have max size incoming request 12. um yeah i don't really see it message count 10 max delivery count 12. okay so i guess there was 10 and that's the 12. so i guess that's where it's being counted let's go run the other one to receive i'm going to just double check to make sure that we set those we did that's all good so we'll type in node receive if you're wondering how i'm auto complaining without typing that i just hit tab on my keyboard so it's receiving those good and that finish i'm going to go ahead refresh see if there's any difference here notice that the message count is zero so when the topics were there it was held in the subscription saying 10 is here and they've yet to be delivered when we ran it they were received and so that number cleared out that's all we really need to learn for um uh topics so we are done with topics so let's make our way over to resource groups and we'll go over to our service bus topic go ahead and delete this here there it is deleting we are all good to go um and you know just as always don't ever trust azure to delete these things go back and check in three four minutes make sure it's deleted so you just don't have things lingering around but yeah there you go all right let us compare event grid event hubs and service bus because these all are event driven services for application integration and use event buses as a means to work with event data but it gets confusing because i guess you could say they all have overlapping responsibilities so let's kind of clear that up so we have event grid event hubs and azure service bus so the idea is that event grid is a serverless event bus and i would say that you'd probably want it for azure service to service communications the idea is that you just have let's say you know you're like integrating from service to service so you're not deploying necessarily fully custom web applications uh or traditional applications onto virtual machines it's more for like uh cloud native cloud first kind of builds they're dynamically scalable they're low cost at least one once delivery of a message for event hubs this is streaming data so we get low latency can receive and process millions of events per second at least once per delivery of an event and streaming you know because of the nature of it is generally more expensive than traditional things like aq or a pub sub for web applications which is what a azure service bus is so reliable asynchronous message delivery that requires polling and i guess pushing technically for pub sub advanced messages features like first in first out batching sessions transactions dead lettering temporal control routing filtering uh duplicate uh detection at least once delivery of a message so it really sounds like and there's one option here or optional order delivery of message so it sounds like azure service bus has it all but really it's not it's not at the same scale as event hubs and it's not at the same like level of um like handsoff kind of approach that event grid gives you with serverless but you have all those options there and you will use them all but there you go hey this is andrew brown from exam pro and we're taking a look here at the introduction to redis so what is redis well redis is an open source inmemory database store and it acts as a caching layer or a very fast database it has a lot of utility but those are the two most common and one thing is that because it's in memory that data is highly volatile so there is a chance of a data loss so even though it is super fast within milliseconds like five milliseconds 10 milliseconds um you know it's not used as a primary database but you know there are tools out there that have made them more durable uh so it really is up to you uh but the other thing about redis is that it's a key value store so uh it's not like using a relational database um and so it has its own type of data structures data types so we have string sets sorted sets lists hashes bitmaps bit fields hyperlogs geospatial indexes streams uh and the ones in red i do want to go over with you because i think they're worth your time but the idea is that to use redis you're either going to use like a programming library they're pretty much available in every language um and or you could use the redis cli so getting my pen tool out here for a second here you can see we're logging to the right of cli we're doing a ping to the server it gives back a pong uh and here we're setting a simple key value so we say my key some value some value is the string we are setting and so it is that simple it's a very simple database but it's super super fast but let's take a look at some of those data structures or data types so we can see uh how we're going to interact with this database all right let's take a look at our different types of data types data structures that we have available to us in redis the first is strings the most basic one the most important one because everything is basically strings at the end of the day here in redis strings are binary safe so they can contain data such as jpeg image serialized ruby objects so they can represent different things that can represent numbers which is kind of odd and strings have a max length of 512 megabytes to get a lot of room there often you're doing a get in the set here so you say get uh you know get this uh key you get back that string set this key with that value so very simple to use um strings can represent numbers and so now you have these special functions for strings called atomic counters so you can do things like increment decrement uh increment by certain amounts so very very clear here so notice it is a string but we can increment it and now return 11. and it interprets it as an integer which is interesting there so the most common string commands would be things like get set append depend would just add something to the end of the string or check check if a string exists based on a key so it exists you have lists so lists are ordered collection of strings uh they're not going to give you a guarantee of unique uh unique strings or unique uh elements within that list there so you can have duplicates but um you do things like l push l range it's kind of like having an array uh in other languages here they're doing a range notice it does negative one but it actually returns all of them so that's kind of a trick there to uh get all the elements in a range but you could say like one to two and then it would just return one and two common commands for lists would be l pop r pop l push l uh uh pos was just for position so el pop would uh remove the last element and return it uh or on the left hand side so the first one our pop would be on the right hand side so if you did an l pop and return hello you did an rpop will return world and also remove it from the list l push would add a new thing on the end help uh pause is creating a position so you provide a string it'll tell you it's positioned in the list then you have sets so seems like they're like lists but they're unordered so there's no guarantee that if you request the stuff in the list it's going to be in the order that you expect it but the advantage though is that things are unique in the list so if you push the same thing you add the same thing to that set like world twice it'll only be there once that's when you need a unique amount of things in a collection there most common commands here is s add so add a member s members show us all the members s move move a member from a from one set to another s pop similar to um our pop uh like what we saw with um list so just remove the one or or multiple random members from a set we have hashes so hashes represent a mapping between string fields and string values um redis was originally coded in ruby so they're basically like ruby hashes but if you don't know ruby hashes there's json objects probably seen those before ruby hashes of course python dictionary so the idea is that you set a you say okay i'm making a hash i'm going to set a field which is just a key it's string field two uh and it's value there so um pretty straightforward common commands for hashes would be like h h get to get a value hdl to delete a value hm set to set multiple values hm get to set or to get multiple values um h valves to get only the values h key or probably keys to get uh only the the fields which are keys in this case here we have sorted sets so these are similar to um sets but um they have an associated score so when you add something you use a zed ad for this but you put it you give it a score like here one one two and uh these are really great for leaderboards um or where you need to score things and sort that way so common commands here would be things like zed ads to add something with a with a certain score z rem is for removing z range is when you want to return a range of elements in a sorted set score zed rank would be return the rank of a member in the sword set score uh uh set sorted so it's not giving you i said sort of stored but it's sorted so it's not giving you its score it's giving you where it's ranked within the list and then you have zscore and so that that you provide um the the value and it would tell you what its score is so those are all the data types that you should get some handson with there are some interesting ones like stream and things like that but uh this is pretty much the core stuff for redis redis is not hard to learn or use um so you just gotta jump in there and use it okay all right so we just took a look at redis now let's talk about azure cash for redis so azure cash for redis is based on the popular open source redis cash or data store it gives you access to secure dedicated redis cache that microsoft manages and that you can access from any azure application all major cloud service providers have a managed redis service or database because it's just super useful and we'll talk about the use cases here in a moment azure cache for redis is suited for high throughput and low latency requirements where the same data is often requested so remember those key three things to know when you're supposed to use reddit there are serverless databases um like nosql databases sorry that can operate extremely quickly like with a guarantee of a certain amount of time but redis is just like super fast it's faster than everything else out there because it's in memory azure cache for redis is commonly used for these scenarios and i really do want you to pay attention because these are very common very practical and as a developer you really should know these so the a very common use case is storing session data if you have a web application and it's deployed to uh multiple virtual machines um a hard part about having an application running across multiple virtual machines or containers is that um knowing what was the last like where the where the session is stored the actual uh state of a user and so you can't have it running on one virtual machine because then the other ones won't know about it so you need to store it somewhere else so redis is a very common use case for that storing cached html or json uh this very common use case to speed up response times another thing that we normally see would be mem uh memory dme mdb it's another caching solution specifically for html um but yeah this is something you put in front of your web application to speed up web pages or api calls another one is a job or message queueing system if you are in the ruby on rails world a very common queueing system is called sidekick and it uses redis or another thing again in ruby on rails if you want to use action cable which is for doing pub sub like real time like building a chat like a chat system or a game that's backed by redis um so those solutions aren't going to change you can usually use redis for those things another one is using it to be in front of a database to reduce recontention recontention means like too many reads are hitting the database so it's slowing everything down so you can put a cache in front of that to improve response times for fetching data to just take uh take the load off the database maybe you have a database that's expensive to call so it's going to save you money and so you can put this in front of cosmodb or azure sql very common cases or any kind of database really that returns data and so this is actually called the aside cache pattern something you want to kind of remember uh where it's going to sit in front of the database so if the date if the data is up to date um and it's there in the the redis store it'll take it from there if not it'll go to the database and the database can be proactive and send data to redis so that it doesn't have to do a hit or miss it can just have the data there knows that it's there just to kind of visualize this just to see where those are so i'm just gonna get my pen tool out here so the first is like where uh um redis or azure cache redis is sitting between your application and the request so maybe you're hosting on azure app services the idea is that before um a request makes it into your application it's going to check that cache and so that can reduce the load on your application improve the load times if if there's things that are computationally expensive then there's your database so over here between your application to your database it's going to hit the redis cache first if there's data it will return it if there's not it will go straight to the database and then the other common use case here is to have a fast database for realtime data maybe for queuing jobs um video games leaderboards things like that so uh yeah that's pretty much it for azure cashflow redis just gotta get some handson experience with it but very simple uh not too complicated very useful you
