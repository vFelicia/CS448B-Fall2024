With timestamps:

00:00 - Lang chain is a framework for developing
00:01 - applications powered by large language
00:03 - models like gp4 in this course you will
00:06 - learn Lang Chain by building six inin
00:09 - projects with the open AI API Google
00:12 - Gemini Pro and llama 2 you'll also learn
00:16 - how to deploy projects to the hugging
00:17 - face platform Chris neack developed this
00:20 - course considering this here is an
00:22 - amazing video of the entire Lang chain
00:25 - crash course along with this we'll be
00:27 - discussing six plus endtoend projects
00:30 - llm projects using different different
00:32 - llm models from open AI we'll be using
00:35 - Lama 2 from meta we'll also be using
00:38 - Google giny pro not only that we'll also
00:40 - be using hugging face and we'll also be
00:42 - seeing how we can deploy this in hugging
00:44 - space uh platform itself which is
00:47 - provided by hugging face itself right so
00:49 - there will be somewhere around six plus
00:51 - end to end projects we'll develop both
00:52 - the front end back end I hope you enjoy
00:55 - this entire Series this is a long video
00:58 - so please make sure that you devote some
00:59 - some amount of time every day and try to
01:03 - practice as much more you can right so
01:06 - in order to push this video to many
01:08 - people out there guys I'll keep the like
01:10 - Target to 2,000 please make sure that
01:12 - you hit like share with all the friends
01:14 - share your work in LinkedIn in different
01:17 - platforms tag me over there I'll be very
01:19 - happy to see what all things you
01:20 - specifically developing so let's go
01:22 - ahead and enjoy the series and I hope
01:25 - you practice along with me all the
01:26 - materials will be given in the
01:28 - description of this particular video so
01:30 - let's go ahead guys so finally finally
01:32 - finally here is the langin series and
01:34 - this is probably the first video where
01:37 - I'm probably going to discuss a very
01:39 - good one-hot video about Lang chain all
01:42 - the important components that you
01:44 - specifically need to know to build
01:47 - projects now why I'm stressing on langin
01:50 - is that because many people recently
01:53 - some of my students and some of
01:55 - experienced professional who switched
01:57 - into data science Industry they are
01:59 - getting work that are related to large
02:01 - language models and they're specifically
02:03 - using Lang chain and the community with
02:05 - respect to langin is also increasing so
02:08 - that is the reason I've created this
02:09 - dedicated playlist and I'm going to
02:12 - discuss a lot many things this video we
02:15 - will understand all the important
02:16 - competents of this Lang chain what you
02:18 - specifically need to know with respect
02:20 - to the Practical orientation and from
02:22 - the next video onwards lot of end to
02:25 - endend projects are probably going to
02:26 - come up uh in this video also I'll
02:28 - discuss about one Q&A project uh chatbot
02:32 - in short you know we'll try to use
02:33 - streamlet it's not like we only have to
02:36 - use streamlet you can also use flask you
02:37 - can use anything as you want right but
02:40 - streamlet it provides you a good UI you
02:42 - can also use gradio if you want right it
02:44 - is up to you so what all things we are
02:47 - specifically going to discuss first of
02:48 - all we'll try to understand the agenda
02:50 - and then we will try to uh do step by
02:53 - step each and every line of code will be
02:55 - done and as I said this is just like a
02:58 - kind of L chain one shot video just to
03:01 - give you an example that how we are
03:02 - probably going to create our end to
03:04 - endend application after we deploy that
03:06 - in a specific cloud like hugging phas
03:09 - right it provides your entire
03:10 - environment to probably host your
03:12 - application uh this is what we are going
03:14 - to create this chatbot see if I probably
03:16 - ask what is the capital of New oh sorry
03:21 - of India then you'll be able to see that
03:24 - I'm I'll be able to get the answer what
03:25 - is the capital of Australia many people
03:28 - get confused with the capital Australia
03:30 - please do comment down in the comment
03:32 - section what is the capital of Australia
03:33 - and let's see whether it is right or not
03:35 - so here you can see the capital of
03:37 - Australia scan so we are going to create
03:38 - this application at the end of the day
03:40 - but before that we really need to
03:42 - understand lot of important components
03:44 - in Lang chain so let's go ahead and
03:46 - let's start this particular session
03:48 - before I go ahead please make sure that
03:50 - you subscribe the channel press the Bell
03:51 - notification icon and share with many
03:53 - friend so that it will also be helpful
03:56 - for them and it will also be helpful for
03:59 - me so that you are able to provide this
04:01 - open source content to everyone out
04:03 - there many people require help and you
04:06 - should be the medium to provide this
04:08 - specific help by just sharing it right
04:10 - so what is the agenda we'll understand
04:13 - as I said this will be completely a
04:15 - practical oriented video it'll be a long
04:17 - video where I'll be talking more about
04:19 - practical things how you can Implement
04:21 - Lang chain how you can probably
04:22 - implement various important
04:24 - functionalities in Lang chain and use it
04:26 - probably to build an endtoend
04:27 - application so first of all we'll go
04:29 - ahead and do the environment setup this
04:30 - is commonly required you also require an
04:32 - open AI key API key so this also I will
04:35 - show you how it is done then we'll try
04:37 - to build simple applications uh with the
04:40 - help of llms three important things are
04:41 - there llms prompt templates and output
04:44 - parsel okay in llms you specifically
04:47 - have llms and chat models chat models
04:49 - are basically used in chat B to
04:51 - understand the context reference and all
04:54 - right I will also be discussing this
04:55 - practical promt templates can play a
04:58 - very important role and then coming to
05:00 - the third one that is called as output
05:02 - parser uh in short prompt template and
05:05 - llms and output parser gives a very good
05:07 - combination of output like it it'll give
05:09 - you a good output the output like you
05:12 - specifically want right so that is where
05:14 - output Parcels will also be used before
05:16 - to go ahead with what I will do is that
05:19 - uh first of all just go to the open aai
05:21 - uh website itself and get your API key
05:23 - how you should get it just go to this
05:25 - account and there you'll be able to see
05:26 - view API key and create the new C secret
05:30 - key right so once you create it give the
05:32 - key name and then copy it and keep it
05:34 - right I'm not going to share mine so
05:36 - that is the reason why I'm showing you
05:38 - this specific step now I will go to my
05:41 - vs code all my coding will be done in
05:43 - this VSS code itself here you can see it
05:45 - is completely blank right I've just
05:48 - opened a folder in the form of a project
05:51 - now all you have to do start your
05:52 - project over here now the first thing as
05:55 - usual what we need to do we need to
05:57 - create a new environment right so this
06:00 - is the first step that you specifically
06:03 - need to create with respect to an
06:05 - environment so don't miss this specific
06:07 - step it is important and probably
06:10 - whatever projects that we are going to
06:12 - discuss in the future we have to do the
06:14 - specific step so I will write pip
06:17 - install okay sorry cond create I'm
06:21 - creating an environment so let me just
06:24 - not open in Powers shelf instead I'll go
06:26 - and open in command prompt okay so here
06:28 - I will just right K create minus p v
06:35 - EnV V
06:37 - EnV
06:39 - python equal to 3.9 so 3.9 is the
06:43 - version that I'm going to specifically
06:44 - use and also going to give- Y so that it
06:47 - does not ask me for the permission
06:49 - instead start creating the specific
06:51 - environment the reason why I'm using
06:53 - this environment understand one thing
06:55 - that guys for every project that we
06:57 - probably create we have to create a new
07:00 - environment that actually helps us to
07:02 - just understand or just use only those
07:04 - libraries that are actually required for
07:06 - this particular project okay so this is
07:09 - the first step go ahead do it with me
07:11 - you know uh and do it in vs code because
07:14 - vs code is a good idea if you want to do
07:16 - it in pycharm then also you can do it
07:18 - but since you following this tutorial
07:20 - I'm actually going to do it in vs code I
07:22 - feel vs code is good I I've used both
07:25 - okay I've used different different IDs
07:26 - but I feel vs code is good okay so here
07:29 - it is now here you can probably see my V
07:32 - EnV environment uh the next thing we
07:35 - will go ahead and activate this V EnV
07:37 - environment so I will write cond
07:40 - activate
07:42 - cond activate V EnV Dash Okay so this is
07:46 - my first step done I'm good with it now
07:49 - the next thing what I will go to do is
07:51 - that I will just go ahead and write my
07:54 - requirement. thxt right because we need
07:57 - to install the library since inside this
07:59 - particular venv environment so I will go
08:02 - ahead and write all the list of
08:04 - libraries that I'm specifically going to
08:06 - use now what are libraries I'm going to
08:08 - use over here uh we will just write it
08:11 - down so that uh the first library that
08:13 - I'm going to use is Lang chain then open
08:16 - Ai and I think I'll be using hugging
08:19 - face also later on it is called as
08:21 - hugging face Hub that I will show you as
08:24 - we go ahead okay so these are the two
08:26 - libraries that I'm going to specifically
08:28 - use okay now the next thing what I will
08:30 - do I will go ahead and write pip install
08:33 - minus
08:34 - r okay so let me just hide my face so
08:38 - that you'll be able to see pip install
08:39 - minus r requirement. txt so once this
08:43 - installation will take place that
08:45 - basically means my requirement. txt is
08:48 - getting installed that basically means
08:50 - all the libraries that I actually
08:52 - require over here that is getting
08:53 - installed and for this I require Lang
08:55 - chain and open aai okay so once this
08:58 - installation will will be taken place
09:00 - like it will be done now one more
09:02 - Library I'll be required is called as
09:04 - IPI kernel because if I really want to
09:06 - run any Jupiter notebook over here I
09:09 - have to use that okay so let's wait and
09:12 - let's see once the installation is
09:14 - probably done and then we will continue
09:16 - the video so guys the all the libraries
09:19 - that were present in the requirement.
09:21 - txt has been installed now the next step
09:24 - that I'm probably going to do is also
09:26 - install IPI kernel which will be
09:29 - required to run my Jupiter notebook so I
09:32 - will go ahead and write pip install iy
09:35 - kernel Now understand one thing I'm not
09:38 - writing this library in requirement. txt
09:40 - because when we do the deployment in the
09:43 - cloud IPI kernel will not be required
09:45 - okay so that is the reason I'm
09:47 - installing separately pip install IPI
09:49 - kernel in the same VNV environment
09:52 - because VNV environment also we are not
09:54 - going to push it right so you can
09:56 - probably see over here downloading this
09:58 - this will happen and automatically the
10:00 - download will happen itself right now
10:02 - the next thing what I'm going to do I'm
10:05 - just going to write Lang chain dot
10:10 - ipynb okay py NB so this will basically
10:15 - be my jupyter notebook that I will
10:17 - specifically be using okay so right now
10:19 - it is detecting kernel once this
10:21 - installation will probably happen and
10:23 - then we will also be able to see the
10:25 - kernel okay so this is all the steps all
10:28 - the basic steps that you probably
10:30 - require from this you can start creating
10:32 - end to endend project but at least you
10:34 - require this uh along with this I'm also
10:36 - going to add two more steps one is about
10:39 - environment files EnV file okay so here
10:42 - what I will do I will write Dov file
10:46 - okay uh inside this EnV file the reason
10:48 - why I'm uh writing this EnV file because
10:51 - I need to probably use my open API key
10:54 - and probably mention the open API key
10:56 - over here so if I probably write it like
10:58 - this and and whatever API key that I'm
11:01 - probably getting from the website I can
11:02 - upload it over here and using right uh
11:06 - load environment function right we can
11:09 - load this API key uh as a variable so
11:12 - that we can actually call our API API
11:14 - key over there so I will update this
11:15 - later on as we go ahead okay so till
11:18 - here everything is done this is my Lang
11:20 - chin ipynb file I will go ahead and
11:23 - detect the environment this is what
11:25 - 3.9.0 so here it is so let's see whether
11:28 - it is working or not 1+ one it's working
11:32 - right so this is perfectly all right so
11:34 - everything over here is done with a
11:37 - respect to this and I'm very much Happy
11:38 - this is working uh which is really
11:40 - really good okay so this is done now
11:43 - what I am actually going to do over here
11:45 - is that we need to import some of the
11:48 - important libraries like open Ai and all
11:51 - right so for this I will go ahead and
11:53 - right from Lang
11:56 - chain. llms
11:59 - okay
12:01 - import open AI see there there lot many
12:05 - models like open llms and all first
12:07 - we'll start with open AI understand one
12:10 - thing guys the reason why I say this is
12:12 - completely practical oriented because
12:14 - you need to have the basic knowledge of
12:17 - machine learning deep learning and all
12:19 - but this specific libraries is used to
12:21 - build application uh fine tune your
12:24 - application with respect to models with
12:26 - respect to your own data set most of the
12:29 - things just with writing proper lines of
12:31 - code will be implemented in an easier
12:33 - way so you really need to focus on
12:35 - things how things are basically done
12:37 - okay now the next thing what I will do I
12:39 - will write import OS and I will say OS
12:42 - do Environ
12:45 - environment okay and here I will give my
12:51 - opencore API underscore key okay this is
12:56 - how you should basically write it down
12:59 - to import the API key now what I will do
13:02 - I will keep this hidden from you so just
13:05 - imagine I cannot show you the API key
13:07 - because I will be using my own personal
13:09 - API key itself so I will go ahead and
13:12 - probably pause my video and update the
13:14 - API key and I'll remove this specific
13:16 - code okay that is what I'm actually
13:18 - going to do so that none of you
13:20 - basically sees that and it is important
13:23 - you have to use your own API key so let
13:25 - me quickly go ahead and do that and let
13:27 - me come back so guys this is how you
13:31 - have to probably import your API key
13:33 - this will I've made some changes so if
13:35 - you also copy it is not going to work I
13:38 - made some internal changes in between
13:39 - changes so uh you just need to write os.
13:42 - environment open API key and this API
13:45 - key that you have specifically got okay
13:48 - so this is the initial step uh I have
13:50 - also imported open AI so that I will be
13:52 - able to call this particular uh open AI
13:55 - itself now this is done this is good
13:57 - everything is working Absol absolutely
13:59 - fine now what I'm actually going to do
14:00 - I'm going to create my llm model and go
14:03 - ahead and write my open AI let's see
14:06 - open AI open AI function is the called
14:09 - or not let's see okay open AI from L
14:11 - chin open Ai and inside this open AI
14:14 - what I'm actually going to do I'm going
14:16 - to basically call a variable which is
14:18 - called as temperature and temperature
14:20 - right now you can keep the value between
14:22 - 0 to 1 the more the value towards one
14:25 - the more different kind of creative
14:27 - answers you may get right if the value
14:30 - is towards zero then what kind of output
14:33 - you you are probably getting from the
14:34 - llm model is going to be almost same
14:36 - from the uh anytime you number anytime
14:39 - you probably execute so here I'm just
14:41 - going to keep it as 6 so this is
14:43 - basically my open a llm model okay now
14:47 - this is done my llm model is there so
14:49 - here you can see did not find an open AI
14:51 - key please add an environment variable
14:54 - open AI key now this is the error that
14:56 - you are specifically getting right so
14:59 - why this particular error is probably
15:01 - coming you should definitely understand
15:03 - okay without
15:05 - this understand that this kind of Errors
15:07 - can come to you the reason why I will
15:10 - not edit this particular error because I
15:11 - really want you all to understand it is
15:13 - saying did not find open API key now
15:16 - what you can probably do with respect to
15:18 - this okay there are two different things
15:20 - that you can probably do either you can
15:22 - take this API key save it in a constant
15:25 - variable and try to use that particular
15:26 - variable over here right so for that
15:29 - also you can directly do that uh I have
15:31 - also created this do EnV file what you
15:34 - can do you can load this environment
15:36 - variable and probably directly read it
15:38 - over there right but let me just go
15:41 - ahead with a simple way you know so you
15:43 - will be able to understand with respect
15:45 - to that also so here what I'm going to
15:47 - do here I will go ahead and probably say
15:50 - open API key and here I will going to
15:52 - write OS do
15:55 - environment okay and here I'm going to
15:57 - Define my open API key okay now let's
16:01 - see whether this will get executed or
16:03 - not I will show you much more better
16:04 - ways when we are probably executing our
16:07 - end to end application so let me go
16:09 - ahead uh open a key is not defined
16:12 - because I use double equal to perfect
16:15 - now you can see that it has got executed
16:18 - perfectly now when I am actually
16:20 - creating an end to end project I will
16:22 - show you a better way the most efficient
16:24 - way that we should specifically use when
16:26 - we are building an end to end project
16:27 - but right now I'll go to focus like this
16:30 - now understand one thing is that with
16:32 - respect to temperature variable right
16:35 - the temperature that we specifically
16:36 - used I will give a comment over here and
16:39 - you can probably see over here right so
16:42 - temperature value how creative we want
16:45 - our model to be zero means temperature
16:47 - it is it mean model is very safe it is
16:49 - not taking any bets it will risk it
16:51 - might generate wrong output may be
16:53 - creative so more the value towards one
16:57 - the more creative if the model becomes
17:00 - right it is going to take more risk to
17:02 - provide you some more better but again
17:04 - with respect to risk again there may be
17:05 - a problem you may get a wrong output
17:08 - perfect this is the step simple step
17:10 - that we have done at the end in this
17:13 - video only I'm going to probably do
17:16 - create an end to end project understand
17:18 - this will be very important for everyone
17:20 - because as we go ahead in the next
17:22 - videos the project difficulty will keep
17:24 - on increasing okay now this is done now
17:27 - what I will do quickly I will go ahead
17:29 - and write text let's say the text is
17:32 - what is the capital of India okay so
17:39 - here I will write print llm do predict
17:45 - and here I'm going to basically write my
17:47 - text so here you can see the capital of
17:50 - India is New Delhi so here what I have
17:52 - done is that this is my input and if we
17:54 - use lm. predict I'm going to probably
17:57 - get the text so if you like liking this
17:58 - video till here guys as I'm teaching you
18:01 - step by step I'm explaining you each and
18:03 - everything please make sure that you
18:05 - practice in a better way right if you
18:07 - like it please make sure that you
18:08 - subscribe the channel also okay so this
18:11 - is done lm. predict and we are able to
18:13 - probably get the output okay so
18:17 - understand what what all things we did
18:18 - we created an open AI model right but
18:21 - here in the open AI right you should
18:24 - also understand one important thing here
18:26 - there is a parameter which is called as
18:28 - model now what all model you can
18:31 - probably bring it over here what all
18:33 - model you can probably call so here I
18:35 - will go to my documentation page okay
18:38 - and uh if I probably click on the models
18:41 - now here are the set of models that you
18:43 - can probably use by default it is
18:46 - calling this GPT 3.5 turbo it is the
18:49 - most capable GPT 3.5 model and optimized
18:52 - for chat at the 110th cost of a Dy 003
18:56 - we'll be updated with our latest model
18:58 - iteration 2 weeks after it is released
19:00 - it's not like you only have to use this
19:01 - you can use this you can use this you
19:03 - can use this you can use this whatever
19:06 - models you want you can probably use it
19:08 - if you want to probably go with GPT 4
19:10 - you can use this you can use this you
19:12 - can use this but at the end of the day
19:14 - gp4 is the most uh amazing thing it is
19:17 - more capable than GPT 3.5 so this is
19:20 - what models is by default over there it
19:23 - is probably taking this specific model
19:25 - which you can probably use it now as we
19:27 - go ahead it is not like you can only
19:29 - call this model itself in hugging face
19:32 - you have open source models also right
19:34 - from Google from different companies you
19:36 - have that uh open source llm models you
19:38 - can also call that and I will also show
19:40 - you an example with respect to that okay
19:43 - so till here I hope you have understood
19:45 - it I think you should give a thumbs up
19:47 - if you're able to understand till here
19:49 - now let's go ahead and do one more thing
19:52 - I will also show you with respect to
19:53 - hugging face now okay now with respect
19:55 - to hugging face what I will do I will
19:57 - quickly go ahead head and write hugging
20:01 - facehub right I will probably install
20:04 - this library now let me go ahead and
20:06 - open my
20:07 - terminal okay so I will delete this and
20:11 - quickly we will go ahead and write pip
20:17 - install pip
20:18 - install minr requirement.
20:25 - txd done so this is getting executed and
20:28 - then I will probably show you with
20:29 - respect to hugging face also so hugging
20:32 - face this is done okay the installation
20:34 - is done so you have to use hugging face
20:37 - Hub now in case of hugging face also
20:39 - right you will specifically get
20:44 - a you will also be getting a because at
20:48 - the end of the day we'll also try to
20:49 - deploy it over here if you go to
20:51 - settings right and if you go to access
20:54 - tokens here you can probably see I have
20:57 - some kind of token right and with the
20:59 - help of this particular token only you
21:01 - will be able to call any models that are
21:04 - available over here so in this models if
21:06 - you probably go ahead and see there are
21:07 - lot of llm models that are available
21:09 - right so if you probably go over here
21:11 - natural language token classification
21:13 - question answer like uh let's say text
21:16 - to text generation this is basically one
21:17 - kind of llm model I will search for one
21:20 - name okay so the name is
21:22 - flan okay flan T5 base okay so this
21:26 - specific model or T5 large this is a
21:29 - text to text generation model right so
21:31 - this is also an llm model if I want I
21:34 - can use this also why I can use this
21:36 - because this is an open source okay if
21:38 - you want to probably see the answer
21:40 - right text to text generat false or not
21:42 - false or false is uh if I want to
21:44 - compute it right it'll give me some
21:46 - specific output okay false or false or
21:48 - false is the verb the verb right
21:50 - something like this it is getting an
21:51 - output right so I can also use my I can
21:55 - also create my chat models with the help
21:56 - of this kind of models directly by using
21:58 - this API right but how to do it let's go
22:01 - ahead and see it okay so here what I
22:03 - will do quickly I will first of all
22:06 - import one more important thing again in
22:09 - my environment
22:11 - variable os. environment and here I'm
22:14 - going to basically write in the case of
22:15 - hugging P I have to use something called
22:18 - as hugging oops I have to write in
22:21 - capital letter hugging
22:24 - face hubor API uncore to token okay so
22:30 - here is my token okay hugging face Hub _
22:34 - API _ token now with respect to this
22:37 - particular token I have to write my own
22:39 - token the token is basically given over
22:41 - here as I have already shown you if I
22:43 - probably click on show you'll also be
22:45 - able to see it so I'm not going to do
22:46 - the show part over here so what I will
22:48 - do I will just pause the video upload it
22:51 - execute it change the token and then
22:53 - come back to you okay so let's go ahead
22:55 - till then you can go ahead and create a
22:57 - hugging face account also so guys now I
23:00 - have set up my hugging face Hub API
23:03 - token with this specific token again
23:05 - I've made the changes so if you probably
23:06 - use this again it will not work okay but
23:09 - I have actually imported it now let's go
23:12 - ahead and probably see how we can
23:14 - probably call llm models with the help
23:16 - of hugging face so again and again I'll
23:18 - be using Lang chain Lang chin is just
23:20 - like a wrapper it can call open llm
23:22 - models it can call hugging face uh llm
23:25 - models anything you can probably call
23:27 - with it okay so that is the reason why
23:29 - I'm saying it is powerful so I will
23:31 - write from Lang chain okay
23:35 - import hugging face Hub okay and then
23:39 - here I'm going to probably Define
23:43 - my let me just execute it and then
23:45 - probably I will call it so here I will
23:48 - go and write hugging face
23:49 - Hub here first of all I need to give my
23:52 - repo ID now repo ID is nothing but uh
23:54 - whenever you search for any model this
23:56 - will be your repo ID Okay Google slash
23:59 - this okay so here I'm going to probably
24:01 - go ahead and Define
24:03 - it okay and this will basically be given
24:06 - in a sentence
24:08 - form okay so this is done now the next
24:11 - thing that I will have is model quarks
24:14 - okay and here I will go ahead and Define
24:16 - my
24:25 - temperature and then here I'll go ahead
24:28 - and write max
24:31 - length column 64 that basically means
24:34 - I'm giving the maximum string length to
24:36 - 64 okay now let's see whether this will
24:38 - execute or not I know it is going to
24:40 - give us an error let's see so it is not
24:43 - giving you an error it is probably
24:44 - executing it perfectly it has probably
24:47 - taken that particular key itself
24:49 - inference API is working perfectly
24:51 - everything is working so I will go ahead
24:52 - and Define my variable so here I'm going
24:55 - to basically write my l lmore hugging
25:00 - face okay is equal to this one right so
25:04 - this is done we have probably written
25:07 - this specific thing over here and then I
25:09 - will go ahead and write name or I will
25:12 - see
25:13 - output let's see the output and here I'm
25:16 - going to basically write llm do hugging
25:17 - face do predict and let's say I'll say
25:23 - can you tell me the capital of of Russia
25:29 - okay so let's see whether we will be
25:31 - able to get the output or not so here
25:33 - I'm going to write print output and
25:36 - let's see whether we are so here you can
25:38 - see the output is a simple word right
25:42 - the capital of Russia is Moscow right
25:46 - but here the previous output that we saw
25:48 - with respect to the llm models here it
25:50 - shows that capital of India is New Delhi
25:52 - it is giving you entire sentence and
25:54 - probably here it is just giving you a
25:55 - word and this is what the difference is
25:58 - with respect to an open source uh model
26:00 - itself and models like GPD 3.5 or 4
26:04 - right so here uh you can probably see
26:05 - this okay and I will also execute one
26:08 - more thing over here let's see uh can
26:10 - you write a
26:13 - poem can you write a
26:17 - poem about
26:20 - AI let's say I give this what kind of
26:23 - poem it
26:24 - gives okay it probably taking time to
26:28 - probably give the I love the way I look
26:29 - at the world I love the way I
26:32 - feel the way I think I feel I love the
26:34 - way I love see what kind of output
26:37 - you're specifically getting right now if
26:39 - I probably give the same output and
26:43 - probably write
26:44 - llm okay dot
26:48 - predict llm dot predict and if I
26:52 - probably give the same sentence let's
26:55 - see what is the output now you will be
26:57 - able to understand why we are
26:59 - specifically using
27:01 - this let's
27:03 - see it should give you a better output I
27:06 - think I love the way I look at the world
27:09 - oh so mysterious was so curious It's
27:12 - technology so advanced IT Evolution so
27:14 - enhance it's a tool that can assist um
27:17 - in making life much less hectic a force
27:20 - that can be used for good or a cause
27:23 - that's misunderstood it can make
27:25 - decisions so Swift and learn from the
27:27 - sto Swift tool that can be better see so
27:30 - how what an amazing poem it has
27:31 - basically written and by this you'll be
27:34 - able to understand the differences why
27:37 - I'm specifically using this open AI
27:39 - model and with respect to hugging face
27:41 - yes there are also some models which you
27:42 - can probably take paid one in hugging
27:44 - face that will give you better output
27:47 - but this is what happens with respect to
27:48 - open source uh models I think we should
27:51 - uh we have lot of Open Source models
27:53 - that are probably coming up Mr 7B and
27:55 - all that also we'll be seeing in this
27:57 - playlist as we go ahead but as I said
27:59 - this is just a basic thing to probably
28:00 - understand we will be focusing on
28:02 - understanding these things right so here
28:04 - uh till here I hope we have discussed so
28:06 - many things how to probably call open AI
28:09 - models with respect to open AI uh
28:11 - Library API itself and then hugging face
28:13 - API also llm models right and using Lang
28:15 - chain so here are what all things we
28:17 - have done now the next thing that we are
28:19 - probably going to discuss is about
28:20 - prompt templates prompt template is
28:23 - super amazing uh it will be very very
28:25 - handy when we are talking about about
28:27 - things with respect to prom template and
28:29 - all so that also we are going to discuss
28:31 - as we go ahead so guys now let's go
28:34 - ahead and discuss about prompt templates
28:36 - which is again a very handy component in
28:38 - the Lang chain even in open AI with the
28:41 - help of prompt templates you will be
28:43 - able to get efficient answers from the
28:46 - llm models itself right I'm not talking
28:48 - about prompt engineering that can get
28:50 - you three CR package okay I'm just
28:52 - talking about simple prompt templates
28:54 - and how prompt templates can be probably
28:56 - used okay now now let's go ahead and
28:58 - first of all import from Lang
29:02 - chain from Lang chain. prompts import
29:07 - prompt template okay uh what we are
29:10 - going to do is that whenever we call a
29:12 - specific llm model you know the llm
29:15 - model should know what kind of input it
29:17 - is probably expecting from the client or
29:20 - from the end user and what kind of
29:22 - output it should probably give it okay
29:25 - so what we do if you want to really
29:26 - Define how our input should be and how
29:28 - our output should be we can specifically
29:30 - use prompt template because understand
29:32 - if we directly use GPD 3.5 it can be
29:35 - used for various purposes right but here
29:37 - I want to restrict all those things
29:40 - within something right so with respect
29:42 - to the input and the output so here I'm
29:43 - going to probably Define my promt
29:45 - template so let me go ahead and Define
29:48 - my promt template now with respect to
29:50 - the promt template here first thing that
29:52 - we need to Define is our input variables
29:55 - so here I will go ahead and write write
29:57 - my input
29:59 - variables okay now in input variables we
30:03 - need to first of all say that what input
30:05 - we are specifically giving there will be
30:07 - a fixed template in that template I
30:10 - really need to give my input okay so
30:12 - let's say here I Define my input and
30:15 - here I'm just using one input let's say
30:17 - the capital or the country I'm just
30:20 - going to write it as country okay so
30:22 - this is my first parameter that I'm
30:23 - going to probably give in my template
30:25 - itself and this I will also store it in
30:28 - my variable which is called as promt
30:30 - template
30:31 - okay great now here I'm given country as
30:35 - my input variable okay here I will
30:38 - Define my
30:39 - template now inside this template what
30:42 - I'm going to say is that tell me the
30:46 - capital
30:48 - capital
30:50 - of
30:52 - this whatever template that I'm
30:55 - specifically giving that is country
30:57 - so this is just like my variable
31:00 - whenever I give the input to this
31:02 - variable it is going to replace it over
31:04 - here right so what will happen by this
31:07 - is that the open AI will be able to
31:10 - understand okay this is the question
31:11 - that is asked but this value is dynamic
31:15 - that I'm probably giving it during the
31:16 - run time something like that okay now
31:19 - here if I want to execute this again I
31:22 - can also write
31:25 - prompt uncore template
31:28 - okay prompt undor
31:32 - template uh dot format right so there is
31:36 - a function called as do format and here
31:38 - now instead of how should I give my
31:40 - input so here I will say
31:43 - country is equal to right whatever the
31:46 - variable name is country is equal to
31:47 - let's say
31:49 - India okay now here you can see that my
31:52 - entire prompt is generated in this way
31:55 - saying that tell me the capital of India
31:59 - okay so here you can probably see tell
32:00 - me the capital of India right now if I
32:03 - want to probably predict I will write
32:04 - llm do predict okay and here I will just
32:08 - say prompt template is equal to whatever
32:12 - my prompt template is defined so here
32:14 - I'm getting an error predict missing one
32:17 - required positional argument text now
32:20 - inside this I've given this prom
32:22 - template but it is expecting one text
32:25 - right what is that particular text that
32:27 - that is particular this particular value
32:28 - right so here what I will do I will go
32:31 - ahead and Define my second variable text
32:34 - and here I will write it as India let's
32:36 - see whether it will get executed still
32:38 - it is giving me an error so guys now you
32:40 - can see when I'm using this llm do
32:42 - predict and I've given my prompt
32:44 - template I've given my text also that
32:45 - was the error that was coming but still
32:47 - it is giving me an error saying that the
32:49 - prompt is expected to be a string
32:51 - instead found class list this this if
32:54 - you want to run the llm multiple prompts
32:56 - use gener instead okay something like
32:59 - this I will show you a way because that
33:01 - is the reason the reason I'm keeping
33:03 - this error over here there is a simple
33:05 - way of understanding things right
33:07 - because this is not the right way to
33:09 - call promt template along with the
33:11 - inputs itself so what I'm going to do
33:14 - quickly I will go ahead and import one
33:16 - important thing that is called as chains
33:20 - so I will say from Lang chain dot
33:24 - chains import llm chain understand one
33:29 - thing guys chain basically means combine
33:32 - multiple things and then probably
33:34 - execute I have my llm model I have my
33:36 - prompt I know I have to give my input
33:38 - based on that input I need to probably
33:40 - get the output so instead of executing
33:43 - directly by using llm do predict I'm
33:46 - going to use llm chain and inside that
33:48 - I'm going to give my llm model I'm going
33:50 - to give the prom template and I'm also
33:52 - going to give the input so this is what
33:54 - we are specifically going to do now this
33:56 - will get gots executed from Lang chain.
33:59 - chain import llm chain now I'm going to
34:01 - create my chain let's say the chain is
34:03 - equal to
34:05 - llm
34:07 - chain and here I'm going to basically
34:10 - write llm is equal to llm okay whatever
34:12 - is my llm model and my prompt is equal
34:14 - to my prompt template okay now this is
34:19 - there this is my chain not chain what it
34:21 - is doing it is combining the llm model
34:23 - it knows what is the prompt template
34:25 - right I'm going to use both of them and
34:28 - now in order to run it so I will write
34:30 - chain. run and here I will say India
34:34 - let's say I'm going to probably say
34:36 - India and I know what is the output it
34:40 - is going to get tell me the capital of
34:42 - India so if I write chain. run it is
34:44 - definitely going to give me the output
34:46 - the capital of India is New Delhi this
34:49 - is perfect right so I can also probably
34:51 - print it right see guys I'm not going to
34:55 - delete any of the errors I want you all
34:57 - to see the errors and then try to
34:59 - understand that is the best way of
35:02 - learning things okay other than this
35:04 - there is no other way right you need to
35:07 - find a way to learn things you don't
35:09 - worry about any errors that are probably
35:11 - coming up you just worry about okay fine
35:14 - you have got that error how you can
35:16 - probably fix that what is the alternate
35:18 - way of probably fixing it right so you
35:21 - can probably use all these techniques as
35:22 - we go ahead right so this is with
35:25 - respect to prompt template and here I'm
35:27 - going to also talk about and llm chain
35:30 - right so these are some important things
35:32 - because all these things we will
35:33 - probably be using in creating our end to
35:36 - end application now let's go ahead and
35:39 - probably discuss some more examples with
35:41 - respect to llm so guys now we are
35:44 - understanding one more important topic
35:46 - which is called as combining multiple
35:48 - chains using simple sequential chain
35:50 - till here we have understood about llm
35:52 - chain LM chain was able to combine an
35:54 - llm model along with the prompt template
35:57 - through which we can also give our input
35:58 - and get our specific output now if I
36:01 - have multiple chain let's say if I'm
36:03 - giving one input I want to use those
36:05 - input in both the chain or three chains
36:07 - four chains how can I specifically do it
36:09 - and for that I'm going to probably use
36:11 - Simple sequential chain okay so let us
36:14 - go ahead and let us probably see how
36:16 - this can probably be done so first of
36:18 - all I will probably say Capital template
36:22 - prompt Capital prompt okay so first like
36:25 - what is the capital of this specific
36:27 - country right so this will basically be
36:28 - my prompt so here I'm going to use my
36:31 - prompt template and here I'm going to
36:33 - basically use input variables is equal
36:38 - to and here I'm going to basically say
36:40 - country let's go to the
36:43 - next input the next input will basically
36:46 - be
36:46 - template and here I will say I want uh
36:51 - please tell me
36:54 - the capital of of the whatever input I'm
36:59 - specifically giving away as country so
37:01 - this becomes my first template right and
37:05 - what I will do I will create a chain the
37:07 - chain name will be Capital chain okay
37:10 - and here I'm going to probably use my
37:12 - llm chain and my llm model will be llm
37:16 - okay and then I will also be using
37:19 - my prom template is equal to as capital
37:25 - template capital
37:27 - template okay so this is done let's see
37:30 - Capital prompt what is capital prompt oh
37:34 - sorry Capital
37:36 - prompt Capital prompt is not defined
37:40 - why uh please tell me the capital of
37:44 - this uh
37:47 - template oh double equal to Let's it no
37:50 - worries uh two validation error for LM
37:53 - chain so first I've used an LM chain
37:57 - where prompt template is equal to this
37:59 - uh where it is capital prompt so guys
38:02 - after just checking the documentation
38:04 - this should be prompt itself okay
38:06 - because in llm chain we have used prompt
38:09 - and here is capital template here also
38:11 - I'm going to probably use Capital
38:12 - tempate now if I execute this this works
38:14 - absolutely fine uh one thing you can
38:17 - probably see over here that I've given
38:19 - my template name and then I've also
38:20 - given the capital chain right so if I
38:22 - want to probably execute it I can just
38:24 - give my chain. run and that particular
38:26 - parameter okay but now what I want is
38:29 - that I also want to create one more
38:31 - prompt template I want to give the same
38:33 - input to that chain also so here uh
38:36 - let's say I will write famous _ template
38:40 - and I will just say promp template and
38:44 - here again my input variable what is my
38:46 - input variable so my input variable will
38:50 - be whatever specific things that I'm
38:53 - trying to give right let's say please
38:55 - tell me the capital of uh India if I say
38:58 - right the capital whatever Capital I'm
38:59 - going to get that variable only I'm
39:01 - going to pass it over here so my input
39:03 - variable will basically be my capital
39:06 - okay and this will be my second one and
39:09 - here I'm going to probably sayest
39:11 - template and I'm going to probably ask a
39:13 - question suggest
39:16 - me some amazing
39:19 - things amazing some amazing
39:23 - places places to visit
39:27 - in that specific capital okay so this is
39:30 - what I'm probably telling right please
39:32 - tell me the capital of the country so I
39:34 - will have that capital information that
39:37 - will be my input variable to from this
39:39 - particular template uh in that specific
39:41 - chain okay so I will get two answers
39:43 - first of all I'll get the capital of
39:45 - that particular country and then what
39:47 - are the some amazing places to visit in
39:49 - that specific Capital place okay so
39:51 - these are all the information that I
39:53 - have put up okay so I hope this also
39:54 - works fine now now what I'm going to
39:58 - create I'm going to create an another
40:00 - chain which will be for this particular
40:02 - famous chain right so here I'll write
40:04 - famous chain okay is equal to and I'm
40:08 - going to probably use my llm chain oh
40:11 - llm chain and here I'm going to give my
40:14 - llm models but the second one that is my
40:17 - prompt is equal to uh whatever template
40:21 - that I'm going probably going to give
40:22 - the famous template okay so this is what
40:24 - I'm probably going to do uh and I've
40:27 - probably given this prompt also over
40:28 - there and this will basically be my
40:30 - chain so once I probably execute it both
40:32 - the chains are ready now I need to give
40:35 - one input it should go to one chain get
40:37 - the output from that particular chain
40:38 - and pass that output to the next chain
40:40 - okay so this is what I specifically want
40:42 - to do how can I do it so again from Lang
40:47 - chain do chains I'm going to import
40:51 - simple sequential chain I know guys uh
40:55 - here you may be thinking why I have to
40:57 - use this see you're passing one input to
41:00 - the get the other output from the one
41:01 - chain and pass that particular output to
41:03 - the other chain to get the just output
41:05 - itself right so this is quite amazing
41:06 - when you see an end to end application
41:08 - there you'll be able to understand these
41:10 - are some of the important components you
41:11 - should definitely know and try to
41:13 - understand okay so here finally what I'm
41:16 - going to do is that I'm going to
41:17 - probably create my chain is equal to and
41:19 - this will be my final chain and here I
41:21 - will probably say I'll import this okay
41:24 - so I get that so chain is equal to
41:28 - simple sequential
41:30 - chain capital letter simple sequential
41:33 - chain and inside the simple sequen chain
41:36 - I just have to name all my chains what
41:39 - all chains are specifically there in the
41:41 - form of list so the first chain uh that
41:43 - I have over here is nothing but Capital
41:47 - chain the second chain that I have is
41:49 - something called as famous chain okay so
41:52 - both the chains are ready now in order
41:55 - to run it all I have to do will write
41:56 - chain. run and here I'll specifically
41:59 - give India
42:01 - okay done let's see what kind of output
42:04 - I will probably
42:09 - get so it is running it is a bustling
42:13 - Metro Police and a great place to visit
42:16 - for historical site cultural this this
42:19 - this red Fort see most popular city the
42:22 - iconic monal is a multivisit who fought
42:25 - in World War one the 16th century mugal
42:28 - era Tom in UNESCO world heritage site
42:30 - everything it is probably giving it
42:32 - right so it did not give us the first
42:35 - answer with respect to the chain because
42:36 - it only provides the last input
42:38 - information okay uh if you want to
42:40 - probably display the entire chain I will
42:41 - show you a way how to do that for that
42:43 - we have to use buffer memory uh there
42:46 - will be something called as buffer
42:47 - memory but one amazing thing I gave one
42:50 - input I got the output and then probably
42:53 - I pass that particular output to my next
42:54 - chain and I able to to get one amazing
42:56 - out answer over here so definitely try
42:58 - it out from your side by using different
43:00 - different examples also now what I'm
43:02 - going to do is that I'm going to
43:04 - probably discuss one very important
43:05 - component about chat model open AI so
43:08 - that is also super important uh that is
43:10 - something related to chat models
43:11 - whenever you probably want to create a
43:13 - chart models you can have to use that
43:14 - okay so let's have a look onto that so
43:16 - guys one more additional thing that
43:17 - let's say I want to probably see the
43:19 - entire chain so here we will
43:21 - specifically use something called as
43:23 - sequential chain and let me just show
43:25 - you one example of that also uh it is
43:27 - not much to do with respect to that but
43:29 - you should definitely know this
43:31 - important video as said again I don't
43:35 - want to probably take more time with
43:37 - respect to this but it is good to know
43:39 - this okay sometime when you are
43:40 - developing things and that you'll
43:43 - probably be understanding once I start
43:44 - end to end project right today one end
43:46 - to end project will be done okay don't
43:48 - worry about this uh in this particular
43:50 - video it will be done uh but definitely
43:53 - I want to show this example also as we
43:55 - go ahead now now uh let's quickly go
43:57 - ahead and do the same thing I will copy
44:00 - this
44:01 - entirely okay I will paste it over here
44:05 - okay now along with llm promt template I
44:08 - will give my output key also so where I
44:11 - specifically want my output key so the
44:13 - output key will be nothing but it will
44:16 - be something called as capital okay so
44:18 - this is my Capital chain with this
44:20 - specific output okay so here I have
44:22 - created this now let's go ahead and
44:24 - probably create the next template
44:26 - uh that is this famous template
44:31 - okay so here also you can probably see
44:33 - the famous template uh suggest me some
44:36 - names of the capital and here I've
44:38 - probably created my template name uh and
44:41 - my chain is over here right so this will
44:44 - basically be my chain okay so the same
44:47 - name whatever output Keys over here I've
44:49 - given this as my input key and here uh I
44:52 - can also derive one output key like this
44:57 - output
44:59 - key places something like this okay so
45:02 - done this is done see two simple
45:04 - templates that I actually created uh s
45:07 - me some amazing places to visit in this
45:08 - particular Capital uh the capital is
45:11 - probably given from here so now the
45:13 - chain will probably be able to
45:15 - understand each and everything as we go
45:16 - ahead you know where the output is and
45:18 - all right so here now what I'm going to
45:20 - do I'm going to probably import from
45:22 - Lang
45:24 - chain do chain
45:26 - chains
45:28 - import sequential chain okay and then
45:31 - finally you can see I'll write chain is
45:33 - equal
45:35 - to
45:36 - simple okay I will let me just execute
45:40 - this because it is not giving me any
45:42 - suggestion so I will write chain is
45:44 - equal to
45:47 - sequential chain and now I'll Give All
45:50 - My Chains name so the first chain name
45:53 - is um to Capital chain
45:59 - D famous chain
46:02 - dang okay and uh after
46:06 - this you will basically be able to
46:09 - understand the input
46:10 - variables now the input variables that
46:13 - we specifically
46:18 - have input variables is nothing but
46:21 - whatever is my variable name what is the
46:23 - variable name in this case it is nothing
46:25 - but country
46:27 - okay and then my output variable I'll
46:31 - also create my output uncore
46:35 - variables so these are the two
46:37 - parameters see guys this this parameters
46:41 - is nothing but whatever parameters I'm
46:43 - specifically giving one is the
46:46 - capital and one is the
46:52 - places done so this is my entire chain
46:57 - now if I want to run any chain what I
46:59 - will do I'll basically write something
47:01 - like this and give what is my country
47:05 - name right so it should be given in the
47:06 - form of key value pairs so here is my
47:09 - country
47:12 - colon India right something like this
47:15 - now if I execute it I will now be able
47:17 - to see my entire chain right it'll take
47:20 - some time so what I have done over here
47:23 - I have in every llm chain that I'm
47:25 - probably creating I'm creating an output
47:27 - key uh two chains so two output key and
47:29 - here you can see chain country India
47:31 - country was India Capital the capital of
47:34 - India is New Delhi here are some amazing
47:37 - places to visit in New Delhi and all the
47:39 - information I have probably over here
47:42 - now let's go ahead and discuss chat
47:44 - models uh specifically in Lang chain and
47:47 - we also going to use one Library which
47:49 - is called as chat openi uh this is also
47:53 - very good if you want to probably create
47:55 - a conversational uh chat bot itself so
47:59 - in chat models with chat open AI first
48:01 - of all what we will do is that we will
48:03 - go ahead and import Lang chain uh do
48:06 - chatore
48:07 - models and I'm going to probably import
48:10 - chat open aai so we will quickly go
48:14 - ahead and import it now after I
48:16 - specifically import this in chat open AI
48:20 - there are three schemas that you really
48:22 - need to understand okay whenever a
48:25 - conversation basically happens if a
48:28 - human is probably giving some kind of
48:29 - input and expecting some response that
48:32 - basically becomes a human message right
48:34 - if by default when the when your chat
48:37 - bot is probably opening a default
48:39 - message will probably come right and
48:42 - that is something related to domain like
48:44 - what that specific chatbot does right so
48:47 - it can probably come up with a new
48:49 - schema which is called as system message
48:52 - and then there is also one more message
48:54 - which is called as AI message which is
48:56 - again a schema which probably gives the
48:58 - output right whatever the chatbot is
49:00 - giving an output uh the AI whatever
49:03 - models is specifically giving the output
49:05 - that is nothing but that is related to
49:07 - the schema that is called as AI message
49:09 - okay now from this here we what we are
49:12 - going to do we are going to import from
49:16 - langin do
49:20 - schema I'm going to
49:22 - import human message
49:26 - system message right as I said system
49:29 - message is also required and AI message
49:32 - here everything you'll be able to get
49:33 - this as an example because uh probably
49:36 - in the upcoming videos we'll create
49:37 - conversational U chat bot right at that
49:41 - point of time we'll be seeing all these
49:43 - things what we'll be using and how we
49:44 - will be using okay so here quickly we'll
49:47 - import this now uh obviously my llm
49:50 - model is there right by uh and while
49:53 - creating before the llm models how did
49:55 - we use it we basically use something
49:57 - called as uh open a right we used open
49:59 - AI now in this case I will probably copy
50:02 - the same thing
50:04 - okay and I will just past it over here
50:08 - and write chat llm okay and instead of
50:11 - writing open AI I'll use chat open AI
50:15 - right so this is what I'm specifically
50:17 - going to use chat open AI with some
50:19 - temperature in this and here I'm also
50:21 - going to give one model so let me go
50:23 - ahead and write my model name uh the
50:25 - model name that we are going to
50:27 - basically write from here is GPT
50:30 - 3.5 Das turbo right I showed you like
50:34 - what models we can specifically use so
50:37 - this is my chat llm model so here if I
50:39 - probably go ahead and write my chat llm
50:42 - so here you'll be able to see that it is
50:44 - a chat open AI uh and with all the
50:46 - information with so temperature what is
50:49 - the uh this and all open AI key I cannot
50:51 - show you so it shows you open AI key
50:53 - also over here so going to remove this
50:56 - okay so that you don't find the opening
50:58 - I key now let's use this three schema
51:02 - and then probably see how my output will
51:04 - look like let's say first of all I'll
51:07 - create the schema in the form of list
51:09 - okay first of all the system message
51:12 - right let's say system message I will go
51:14 - ahead and initialize and I'll write a
51:15 - content content one variable is there I
51:18 - say you are an you are a
51:24 - comedian
51:26 - AI assistant okay so this is the this is
51:30 - what I'm telling the chatbot to behave
51:33 - like right it is basically acting like a
51:35 - comedian AI assistant okay then in the
51:38 - next one I will say human message and
51:41 - here again I will go ahead and write the
51:44 - content and I will say
51:47 - please and this is
51:49 - what I will probably write as a human
51:51 - this is what the input that I am
51:54 - probably giving right
51:56 - so I'll say
51:57 - please make a
52:01 - comedy about
52:04 - or please provide some punch lines some
52:08 - comedy punch
52:11 - lines punch
52:13 - lines on okay AI
52:18 - okay so here you can probably see these
52:21 - are my two things these are the two
52:23 - information that I'm going to give to my
52:25 - chat llm models right and then let's see
52:28 - what is the output okay with respect to
52:30 - that now in order to give this input to
52:32 - my chat llm so I will write chat llm and
52:35 - here only I will open my
52:38 - brackets so it has two information first
52:40 - by default it knows the system is a
52:43 - comedian AI assistant and here as a
52:45 - human input what we are saying is that
52:48 - we are saying please provide some comedy
52:50 - punch lines on AI okay so if I execute
52:52 - this you will be able to see I will be
52:54 - able to get an output
52:55 - now this is how we are going to design
52:57 - later on in the end to endend project we
52:59 - are not going to give this as a
53:00 - hardcoded it'll be quite Dynamic so here
53:03 - you can see AI message see this is the
53:05 - output if I'm getting the output that
53:08 - basically becomes an AI message so this
53:09 - schema that we are able to see from the
53:11 - output of this particular chatbot the
53:14 - system message is basically telling that
53:17 - okay beforehand you have to act
53:19 - something like that we instructing the
53:21 - chatbot to probably act in that way
53:24 - right the human message is basically our
53:25 - input and AI message is what is the
53:27 - output so AI may be smart but it can
53:29 - tell me if your output makes look like a
53:32 - potato AI is a virtual therapist except
53:35 - it never judges you for eating an entire
53:37 - Pizza by yourself something like this so
53:39 - this is what comedy messages you can
53:41 - probably see right and I think this is
53:44 - quite amazing and you can probably set
53:46 - this up any number of time right you can
53:49 - probably say you can add this AI message
53:51 - over here and you can still build more
53:53 - conversational AI right so as AI also
53:57 - give the message you can probably store
53:58 - it inside this s let's say if I probably
54:00 - consider a list and I append this
54:02 - particular list with all this
54:03 - information it can act as a chat model
54:06 - as our as we go ahead right now guys we
54:09 - are also going to discuss about one
54:10 - topic and after this we are going to
54:12 - implement our project okay so over here
54:15 - we going to discuss about prom template
54:17 - plus llm plus output parcel now first of
54:20 - all we'll try to understand what exactly
54:22 - is output parcel now in order to make
54:25 - you understand about output plaster and
54:27 - how we can probably implement it I will
54:29 - use Lang chain again for this um as said
54:33 - guys langin is a powerful Library it has
54:35 - everything as a wrapper right so I will
54:37 - say from Lang chain okay from Lang
54:43 - chain.
54:44 - chatore models first of all I'm going to
54:50 - import chat open AI okay chat open AI I
54:55 - see there so many things Chad vertex AI
54:57 - chap open AI very powerful very powerful
55:00 - and the way it is getting developed
55:03 - right quite amazing right so from Lang
55:07 - chain dot prompts I'm also going to use
55:11 - some prompts and like how we have a
55:13 - prompt template when we use open AI
55:15 - right similarly in chat open AI we use
55:17 - uh prompts which is basically called as
55:19 - chat promate chat prompt template okay
55:23 - so I'm going to basically import
55:25 - chat prompt value no
55:29 - template chat prompt template so I'm
55:32 - also going to import this along with
55:33 - this as I said output parser right
55:37 - output parser is that if I want to
55:39 - modify any output of an llm model
55:41 - beforehand right so I can specifically
55:44 - use output parsers so for Lang chain I
55:47 - will also import this from Lang chain do
55:52 - schema import base output parel right so
55:58 - these are the three things I'm
55:59 - specifically importing and here I'll
56:01 - basically go ahead and write class let's
56:04 - say I am defining one output parser and
56:08 - I'll Define this in the form of class
56:10 - it'll inherit the base output class so
56:13 - let's say uh I will say comma separated
56:17 - output okay that basically means it is
56:20 - basically called as a comma separated
56:21 - output this is the class that I'm going
56:23 - to Define and uh
56:25 - even even in the documentation it is
56:28 - given in an amazing way okay so comma
56:30 - separated output and this will basically
56:33 - be inheriting the base output parcel
56:36 - okay now inheriting when I probably
56:39 - inherit right that basically means we
56:41 - are inheriting this base output par and
56:43 - we can call this along with an llm
56:45 - models here I will Define a parse method
56:48 - and inside this par I will take self as
56:51 - one keyword and whatever text the output
56:54 - that we have specifically getting which
56:56 - will be in the form of string format all
56:58 - we'll do we'll just write return text.
57:02 - strip
57:05 - dot dot split right and this will be a
57:10 - comma separated
57:11 - split understand one thing now this is
57:13 - what is the class that I've defined and
57:15 - this is just like an output parser by
57:17 - default the output parser is what you
57:19 - can probably see whenever I'm
57:22 - specifically using the chat models I'm
57:24 - I'm getting some kind of output right AI
57:26 - may be smart something it is giving in
57:27 - the form of sentence and it is adding a
57:29 - new line at the end but what I'm saying
57:31 - is that whatever output I'm getting I
57:33 - will take that output and divide all the
57:35 - words in comma separated okay something
57:38 - like that so for this again I will
57:39 - Define my template I will
57:45 - say you are
57:48 - a
57:50 - helpful assistant okay so this is my
57:52 - first message that is probably going as
57:55 - a template right so this becomes a
57:57 - system template the schema that we
57:59 - probably discussed right I will also
58:01 - give some
58:02 - information um let's
58:05 - say when the user gives any
58:12 - input okay you
58:15 - should generate five words okay in a
58:21 - comma separated list so this is what is
58:25 - my entire message okay the template I'm
58:27 - saying that whenever the user give any
58:29 - input you have to probably generate five
58:32 - words which should be comma separated
58:35 - okay so this is what I have specifically
58:36 - done okay now what will be the input
58:39 - what will be the text Will Define all
58:41 - those things okay and here I will say
58:43 - this will be my human template so what
58:45 - is the word that I'm going to probably
58:46 - give over here uh that will specifically
58:49 - defined over here right so here I will
58:51 - say Okay test uh you should generate
58:54 - five
58:55 - words synonym let's say synonym I'll
58:58 - just go ahead and write synonyms okay
59:01 - synonyms in comma separated so here will
59:04 - basically be my text whatever text I'm
59:06 - specifically given now I will go ahead
59:07 - and create my chat prompt now again from
59:09 - this chat prompt what I have to use I've
59:11 - already used chat prompt Pro template
59:14 - okay and inside this I will say do from
59:18 - message let's see that chat prompt
59:22 - template
59:23 - from from underscore messages okay now
59:28 - inside this from underscore messages I
59:30 - have to give two information okay
59:33 - whatever is the template right so first
59:34 - template is nothing but the system one
59:37 - so system information that I really want
59:39 - to give uh that system one is nothing
59:42 - but this normal template that I've
59:44 - defined and the second one will
59:46 - basically be my human template right
59:48 - whatever human message that I'm actually
59:49 - giving right and this will basically be
59:52 - defined as human uncore temp temp
59:55 - plates template right so once I execute
59:58 - it here you'll be able to see this is my
60:00 - chat prompt okay now in order to execute
60:04 - this obviously I have to use chain right
60:07 - because I have a prom template over here
60:09 - I have a human text I have this specific
60:11 - template also so how do I probably
60:14 - combine all these things that is what
60:16 - I'm actually going to show you over here
60:18 - so quickly first of all I will use this
60:20 - chat llm okay chat llm now see this is
60:25 - quite amazing and this is the best way
60:27 - of running chains so I will say chain is
60:29 - equal to whatever is my chat prompt so
60:33 - this is my chat
60:35 - prompt to this chat prompt I will give
60:38 - my
60:39 - API whatever API I'll write over here
60:42 - control V so I have to just give a or
60:45 - sign kind of thing right so this is
60:47 - getting chained up this symbol basically
60:49 - says that it is getting chained up and
60:51 - remember the order also okay or you can
60:54 - also initialize chat open AI over here
60:57 - now along with this I will also give my
60:59 - output parser the output parser will be
61:01 - the last one right so this will
61:03 - basically be my output parser comma
61:06 - separated output okay now see I've given
61:09 - each and everything over here one by one
61:11 - list by list right so here it is so once
61:14 - I probably execute it it will get
61:15 - executed so here what it is saying I'm
61:17 - giving this chat prompt the chat llm
61:19 - model is there and the output should be
61:22 - comma separated output which is getting
61:23 - derived from this particular class okay
61:26 - now here finally what I will do I will
61:28 - write chain do
61:29 - invoke and inside this I will again
61:32 - whenever I use chain I have to probably
61:34 - give it in a key value pair right colon
61:37 - something whatever the value is now in
61:40 - this case I will say the word is um
61:45 - intelligent let's say now I have to
61:48 - probably give in the form of text right
61:50 - so that is what I really have to give it
61:52 - right so this text is equal to in now
61:54 - let's see what is the output uh it is
61:57 - coming as okay there is some syntax
62:00 - issue that I have probably made because
62:01 - I have to close my dictionary over
62:05 - here now if I write chain. inor you can
62:08 - see that five words smart clever
62:11 - brilliant
62:12 - shop aute I don't know this specific
62:15 - word but here you'll be able to see
62:17 - whatever output that I'm probably
62:19 - getting right so if I probably remove
62:21 - this let's see okay this is how the
62:25 - output will look like okay AI message
62:27 - content this this this right but just by
62:30 - adding this output parsel you can see
62:32 - what an amazing message you're able to
62:34 - get and you're getting able to get the
62:35 - right thing that you specifically want
62:39 - this is what powerful a prompt template
62:42 - is all about right now this is done
62:45 - right and this is more than sufficient
62:46 - to know because more new things about
62:49 - PDF how to read PDF how to what is text
62:52 - iding and all we will discuss as we go
62:54 - ahead
62:54 - but now let us go ahead and try to
62:57 - create a simple chatbot okay simple
63:00 - chatbot I'll create an app.py and by
63:03 - using the simple chat bot we'll try to
63:05 - understand how things actually work and
63:08 - what all things we can basically do okay
63:10 - again here I'm going to probably use
63:12 - streamlet and I'll be writing the code
63:13 - line by line so let's go ahead and have
63:15 - a
63:16 - look so guys finally now we are going to
63:20 - develop our Q&A chatbot uh with all the
63:23 - concepts that we have probably learned
63:26 - I'm just not going to use all the
63:27 - concepts right now in this specific
63:28 - video itself because we should have
63:30 - probably uh 10 to 12 projects that are
63:33 - going to come up in the future in this
63:35 - series of playlist so there we are going
63:37 - to discuss about more projects as we go
63:39 - ahead right but now in this video I'm
63:42 - going to probably create a simple Q&A
63:45 - chatbot just with the help of open aai
63:48 - Lang chin and obviously use openi apis
63:51 - and llm models specifically to do that
63:54 - that here I'm also going to use
63:56 - streamlet okay so let's go ahead and
63:58 - let's see initially what all settings I
64:00 - need to do see this was Lang chain. iynb
64:03 - because I will be giving you this entire
64:05 - file uh again in the reference with
64:07 - respect to GitHub also so uh first of
64:10 - all in the requirement. txt I will be
64:13 - importing one more library because I
64:14 - need to install this Library this is the
64:16 - important Library itself which is
64:19 - python. EnV right so python d.v actually
64:23 - helps us to create or upload all our uh
64:27 - environment variables that we have
64:28 - created with respect to our application
64:31 - so here this is the library that I have
64:33 - to do it and just go ahead and install
64:34 - the requirement. txt I've already done
64:37 - that uh and this will be a specific task
64:39 - to you now we are starting over here so
64:42 - from lin. llms I have imported open aai
64:45 - then from EnV load load. EnV so as soon
64:50 - as I probably call this it will take all
64:52 - the environment variables from EMV file
64:54 - so here I've already created the
64:56 - environment variable I'm not going to
64:57 - show you again the environment variable
64:59 - because in short the environment
65:01 - variable will be something like this see
65:03 - I I I may have written like something
65:06 - like this open AP opencore API _ key is
65:09 - equal to this particular environment
65:10 - variable right so this is basically my
65:12 - open uh API key itself right so I'm
65:14 - going to probably use this uh in my
65:17 - application so here uh these are the
65:18 - basic steps that we will probably go
65:20 - ahead with now along with this what I'm
65:22 - actually going to do I'm also going to
65:24 - to import one more Library which is
65:26 - called as streamlet because we are going
65:28 - to use streamlet itself so let me go
65:31 - ahead and open my terminal and quickly
65:34 - let's go ahead and write pip
65:36 - install minus
65:39 - r minus r requirement. txt and then the
65:44 - installation will start taking place and
65:46 - the streamlet uh Library will also get
65:49 - installed Streamlight we are
65:50 - specifically using for a front-end
65:52 - application uh see it's it's not light
65:54 - only you have to use streamlet it'll be
65:55 - very much easy for me to probably create
65:57 - it and do the deployment because I'm
65:58 - also going to show you the deployment in
66:00 - the hugging face uh space itself right
66:03 - what is exactly hugging space uh space I
66:05 - will also discuss about all those things
66:07 - so quickly uh let's do this uh it'll
66:09 - probably take some time and then I will
66:12 - go to my app. Pui uh let me do one thing
66:15 - quickly uh let me go ahead and import
66:17 - streamlet also so I'll import
66:20 - streamlet as St okay so this will
66:23 - basically my streamlet itself okay so
66:26 - it'll probably take some time to
66:27 - download it let's see how much time it
66:29 - is going to take but again it depends on
66:31 - your internet speed and how fast your
66:34 - system is right my system is really
66:35 - really fast till it is taking
66:38 - time so for you it may probably take
66:40 - more time okay so let this installation
66:44 - take place till then I will go ahead and
66:46 - start creating our application now I
66:49 - will first of all create a
66:51 - function to load open AI model and get
66:56 - response okay get
66:59 - response so I will call this function
67:01 - something like definition um getor open
67:05 - AI response okay something like this and
67:09 - here I'm probably going to give since it
67:12 - is a Q&A chatbot so here I will have my
67:14 - question as my parameter which will be
67:16 - of a string type okay it can be a string
67:19 - type it can also be a numerical type so
67:21 - I will just keep it like this okay so
67:23 - this is done and here you can also see
67:25 - the installation is done so I will just
67:26 - close this now here as soon as I
67:29 - probably call this function what I
67:31 - really need to do I need to call my llm
67:33 - model so llm model I will say open AI
67:36 - okay open Ai and here I will go ahead
67:39 - and Define my
67:42 - model uh have I imported open AI yes I
67:45 - have imported model uh uh open itself so
67:48 - I will go ahead and write model uncore
67:50 - name is equal to and I will will Define
67:54 - my model I'll be using text Davin
67:58 - C uh this is one of the model that we
68:00 - have you can probably refer it so text
68:03 - Davy
68:04 - 003 and here I'm also going to Define my
68:09 - temperature temperature is equal to
68:11 - let's
68:12 - say5 Okay uh along with this uh I'll
68:15 - just go ahead and copy one more thing I
68:18 - will just set up my open API key also so
68:21 - I will set it up like by using this OS
68:24 - environment so here will be my first
68:26 - parameter okay so all this is done uh I
68:28 - think I need to also import OS okay so
68:32 - this is done in short what I'm doing is
68:34 - that I'm initializing my llm model OKAY
68:37 - in this code now the next thing is that
68:39 - I need to probably get my response so
68:43 - response will be nothing but llm
68:45 - directly how do I give a question over
68:47 - here I can probably give the question
68:49 - over here okay see I'm just creating a
68:53 - basic one
68:54 - then whatever things you really want to
68:56 - do from here you can probably do it try
68:58 - to create a own prom template try to use
69:00 - chain if you want try to do multiple
69:02 - things but just to start with I'm going
69:04 - to use a simple application where it is
69:06 - just taking an input and giving some
69:08 - kind of output it has no AI message set
69:11 - it has no human system message set no
69:13 - system message set also we have not
69:14 - given any prompt template also over here
69:17 - this just to give you an idea how things
69:19 - starts okay so now we will
69:22 - initialize in initialize our streamlet
69:28 - app okay now with respect to streamlet I
69:31 - will write STD do
69:34 - set underscore
69:37 - page underscore config so this is one of
69:40 - the function in streamlet which will
69:41 - actually help you to set the page title
69:45 - so I will just go ahead and write title
69:48 - is equal to I will say q& a demo okay Q
69:53 - a demo and this is done with respect to
69:56 - my uh and here I will set my another
69:59 - header the header will be something like
70:03 - Lang
70:04 - chain application something like this
70:08 - okay so I've given my header also with
70:10 - respect to this okay now I need to find
70:13 - out a way to get a user input okay uh if
70:18 - I get a user input then I should be able
70:21 - to submit the button and I should be
70:22 - able to get the text itself so first of
70:25 - all I will go ahead and create my submit
70:27 - button I will say St do
70:30 - button and here I will go ahead and
70:34 - write generate generate or ask the
70:38 - question something like this
70:42 - okay
70:44 - if ask button is
70:48 - clicked right if it is clicked that
70:51 - basically means if I write if submit
70:53 - okay if it is clicked this usually
70:56 - becomes true okay so if this is true it
70:58 - is probably going inside this particular
71:00 - function and here you'll be able to see
71:02 - s I'll just put a header and uh I'll say
71:08 - the
71:09 - response is okay and then I will
71:12 - probably write St dot right with respect
71:17 - to the response okay so this is what I
71:20 - am probably doing it okay I'm getting
71:23 - the response over here and with respect
71:25 - to this specific response I will
71:27 - probably this response is probably
71:28 - coming from here but still whatever is
71:31 - the input that input we are not able to
71:33 - capture it yet right because if we
71:35 - capture those input then only we'll be
71:37 - able to send that particular input
71:38 - somewhere right and for that also I may
71:41 - have to probably create another function
71:43 - so let's go ahead and handle the input
71:45 - part
71:48 - now so guys now what I am actually going
71:51 - to do over here is that first of all
71:53 - we'll go ahead and and capture our input
71:54 - text so let me go ahead and write over
71:57 - here input is equal to St do textor
72:01 - input because I'm going to use a text
72:03 - field over there and here I will
72:06 - probably be waiting for the response
72:10 - itself right so sorry from the request
72:12 - right so so here I will write input
72:14 - colon okay and I'll keep a space over
72:16 - here and I will just write key is equal
72:19 - to key is equal to input something like
72:22 - this so this will basically be my input
72:24 - itself okay now once I've done this okay
72:28 - once I've have done this I'm going to
72:30 - take this particular input and now call
72:33 - I hope you should know what we should
72:35 - call we should basically call this
72:36 - function right so this here I'm going to
72:39 - probably write uh not here itself uh so
72:42 - let me just write it over here and this
72:45 - input I'm actually giving it over
72:47 - here okay so this will basically be my
72:50 - input over here uh whatever input I'm
72:52 - probably getting it it'll just go ahead
72:54 - with respect to this particular question
72:55 - and I will probably get the response
72:57 - here I will just go ahead and write
72:59 - return
73:00 - response okay and then I will store this
73:04 - particular variable inside my response
73:06 - okay done see the way I probably got the
73:09 - input over here I sent this input to my
73:12 - get open AI response my open AI model
73:14 - has got probably loaded and then it is
73:17 - basically calling with respect to this
73:19 - llm you can either call predict message
73:21 - or predict functionality also uh you can
73:24 - also use chain you can use multiple
73:26 - things you can assign promt template in
73:27 - this particular function and all right
73:29 - and then finally you have S do Button as
73:31 - the button and if submit this is there
73:33 - okay now quickly let's go ahead and
73:36 - probably run it
73:40 - okay uh let me see whether if I directly
73:44 - call python app. Pui it will give us a
73:47 - error why because it is a stream L
73:49 - library right if it was flask I would
73:51 - have probably said okay it would have
73:53 - work ke now it says key error open API
73:56 - key okay so os. environment open API key
74:00 - load.
74:02 - EnV so guys one mistake that I have
74:05 - specifically done whenever I really want
74:06 - to call all the environment variables
74:08 - from EnV with the help of load uh this
74:12 - the specific library that is called as
74:13 - this this functionality which is load.
74:16 - EnV at that point of time I'll be using
74:18 - get EnV function and here I will just
74:21 - remove all the things brackets and
74:23 - probably call this function now I hope
74:25 - so it should work and I think we should
74:27 - not get any problem so Streamlight run
74:30 - app.py and here we have our entire
74:34 - application quickly it's running let's
74:37 - see um this is getting
74:40 - loaded and here we have right now
74:43 - probably I'll ask the question what is
74:46 - the capital of India right so I'll just
74:53 - ask the question over here the response
74:54 - is the capital of India is New
74:56 - Delhi um let's see what is generative AI
75:04 - right so I'll ask the question you'll be
75:07 - able to see that generative AI is a type
75:09 - of artificial intelligence that focuses
75:11 - on creating new data from existing data
75:13 - this this this is there still I'm I'm
75:15 - getting some kind of weird responses
75:17 - over here so that is the reason we'll
75:18 - also be using output parsers we'll make
75:21 - sure that we'll use conversation buffer
75:23 - memory we'll also Implement schemas like
75:25 - human message system uh human human
75:28 - system AI system um system messages all
75:31 - those things were there right all the
75:32 - schemas that part we probably discussed
75:35 - but this is a simple application that we
75:37 - are probably going to discuss with
75:39 - respect to this it is going to be quite
75:41 - amazing and uh you know this is just a
75:44 - basic Q&A chat bot uh wherein whatever
75:47 - questions you specifically ask like what
75:49 - is the
75:51 - please right write a
75:54 - poem
75:56 - on on please write a romantic poem I'll
76:00 - just give it as romantic
76:03 - poem on generative AI something like
76:06 - this because many people are now using
76:10 - this ask the
76:13 - question so here you can see gener way I
76:16 - knew love in my life your data driven
76:18 - hurt is perfect fit your algorithm so
76:21 - precise your knowledge is so wise so
76:22 - everything is over here now what I'm
76:24 - actually going to do is that I will go
76:26 - and show you the deployment part
76:28 - everything is working fine I will first
76:30 - of all login into the hugging face go to
76:32 - the spaces and create a new space
76:36 - because I'm going to probably do the
76:37 - deployment over here let's say I will
76:40 - say Lang chain Q&A chatbot okay I don't
76:45 - have to use license this will be a
76:47 - streamlet now in space Hardware like you
76:50 - have paid Hardwares also but I'm
76:52 - probably going to use a simple one CPU
76:55 - Basics 2v CPU 16GB and I will create
76:58 - this as public so that you can also
77:00 - refer it um let's see okay I will just
77:04 - remove this spaces please match okay
77:07 - this underscore is not there QA a
77:11 - chatbot I will go ahead and create the
77:12 - space now after creating the space uh
77:15 - there's couple of things that I'm
77:17 - probably going to do over here is that
77:20 - uh this is where this is just like a
77:21 - GitHub repository you know you if you
77:23 - probably go to the files you'll be able
77:24 - to see it now here I'm probably going to
77:27 - upload the file that I have okay uh but
77:31 - before that what I'm actually going to
77:32 - do I'll go to my settings okay and if I
77:35 - go down right so there will be something
77:37 - called as secret keys because this
77:39 - secret key I have to put it with respect
77:42 - to open AI so here no secrets are there
77:45 - so I will go and clear or click on uh
77:48 - new secret and you know that with
77:50 - respect to the new secret what I have to
77:52 - probably use I have to use open API key
77:54 - I will put it over here okay and now
77:58 - oops just a second open API key let me
78:02 - open
78:04 - this and I will put it over here okay
78:08 - and what I'm going to do I'm also going
78:10 - to upload the value okay so I'll not
78:12 - show you the value uh let me update this
78:15 - and let me come back and quickly and
78:17 - show you the next steps that we are
78:18 - probably going to
78:19 - do after adding the open AI API key uh
78:23 - you can see it over here in the secrets
78:25 - you'll be able to see this specific key
78:28 - now what I will do after updating that I
78:30 - will go to my app now again my entire
78:32 - application will start getting buil up
78:34 - now here you can see as soon as I add
78:37 - the open AI API you'll be able to see my
78:40 - application will start running now here
78:42 - I can probably ask question what is the
78:45 - capital of India okay and uh you can see
78:50 - that I will be able to get the response
78:52 - now clearly you'll be able to see uh
78:54 - I've been able to do the deployment in
78:57 - hugging pH spaces uh it was very much
78:59 - simple you can see the files over here
79:01 - itself on the app.py requirement. txt I
79:05 - had commented out all the codes with
79:06 - respect to EnV and all because soon as
79:09 - we add the secret variable as soon as
79:11 - this open a model is called it is going
79:13 - to take the open a API key from there
79:15 - and it is going to use it over here so
79:18 - yes this was with respect to the
79:19 - deployment and quickly we were able to
79:22 - also create a simple Q&A chatbot along
79:25 - with deployment so guys in this video
79:26 - we're going to create one Amazing llm
79:28 - Project which is nothing but PDF query
79:31 - with langen and cassendra DB uh
79:34 - cassendra DB will be probably creating
79:35 - in a platform which is called as data
79:37 - Stacks so if you have probably heard
79:39 - about this particular platform which is
79:41 - called as data Stacks which will
79:43 - actually help you to create cassendra DB
79:45 - in the cloud itself and why this
79:47 - platform is quite amazing because from
79:49 - this you will be able to perform Vector
79:52 - search and whenever we talk about this
79:54 - kind of documents or if you want to
79:56 - really create an Q&A applications from
79:58 - huge PDFs itself Vector search is the
80:01 - thing that you really need to implement
80:03 - now before I go ahead let's first of all
80:06 - understand the entire architecture we
80:08 - will be solving this entirely step by
80:10 - step what are the steps specifically
80:13 - that will be taken to probably complete
80:15 - this specific project that we really
80:17 - need to understand so let's begin with
80:19 - the architecture initially let's say you
80:21 - have a specific PDF this PDF can be of
80:24 - any size and any number of pages first
80:27 - of all we will read the documents and
80:30 - understand here we are going to use
80:31 - langin as I said because langin has some
80:35 - amazing amazing functionalities which
80:37 - will actually help you to perform all
80:39 - the necessary tasks to create this
80:40 - specific application now first we will
80:43 - go ahead and read the documents that is
80:46 - specifically the PDF and the first step
80:49 - usually when we whenever we work with
80:51 - this kind of data set is with respect to
80:53 - some kind of transformation we really
80:54 - need to do so after reading this
80:56 - documents we will convert this into
80:58 - various test chunks that basically means
81:01 - we'll split the data set into some kind
81:04 - of packets right so this text Chunk will
81:07 - be of some specific size based on the
81:10 - tokens that we are probably going to use
81:12 - so over here you can see some example
81:14 - reading the document and then we have
81:15 - divided this into some chunks then we
81:18 - will convert all this chunk into text
81:21 - embeddings now from here we will be
81:24 - specifically using open AI embeddings
81:26 - okay openai embeddings actually helps
81:29 - you to convert text into vectors now why
81:33 - you specifically require these vectors I
81:35 - hope you have heard about text embedding
81:37 - techniques in machine learning right
81:39 - there we have specifically used bag of
81:42 - words tfidf and many more things that is
81:44 - already present in my YouTube channel we
81:46 - have also used word to work average word
81:48 - to what are the main aim of all these
81:50 - techniques to convert text into vectors
81:54 - because once we probably convert into
81:56 - vectors we can perform various tasks
81:58 - like classification algorithms like
82:00 - similarity search algorithm and many
82:02 - more so that is the reason we will
82:04 - specifically be using openi embeddings
82:06 - which will be responsible in converting
82:08 - a text into vectors itself now once we
82:11 - convert every text into vectors we will
82:13 - also see this as text embeddings once we
82:16 - get this embeddings what we are
82:18 - specifically going to do now this will
82:19 - be quite amazing because understand if
82:21 - we have a huge PDF document right so
82:24 - definitely the vector size will keep on
82:26 - increasing so it is better we store
82:29 - entirely all this vectors into some kind
82:31 - of database and for this we are going to
82:34 - use Centra DB so in short what we are
82:37 - basically going to do is that we will
82:39 - take all the specific vectors and save
82:41 - it in a vector database here currently
82:44 - we are going to use cassendra DB now
82:46 - what exactly is cassendra DB so in order
82:48 - to understand about cassendra DB I have
82:51 - opened the entire documentation page
82:53 - over here cassendra aperture Apachi
82:56 - cassendra is an open source no SQL
82:59 - database and it can definitely be used
83:01 - for saving massive amount of data so it
83:04 - manages massive amount of data fast
83:06 - without losing sleep right so again
83:09 - understand this is a nosql database and
83:11 - for vectors kind of thing we definitely
83:13 - have to save it in this kind of database
83:15 - itself many bigger companies are
83:17 - basically using this cenda DB for this
83:19 - specific purpose so if you really want
83:21 - to read more about Apache cendra you can
83:24 - probably see over here Apache cendra is
83:25 - an open- Source nosql distributed
83:27 - database trusted by thousands of
83:29 - companies for scalability and high
83:31 - availability this is the most important
83:33 - point for scalability and high
83:35 - availability without compromising
83:37 - performance linear scalability and
83:39 - proven fall Tolerance on commodity
83:41 - Hardware or Cloud infrastructure make it
83:44 - as a making it as a perfect platform for
83:46 - Mission critical data now how we are
83:49 - going to probably create this specific
83:50 - database for that we will be using this
83:52 - data Stacks platform wherein it will
83:55 - actually help you to create this Vector
83:58 - cassendra DB so that you can store
84:00 - entirely all these vectors into this
84:02 - specific DB and at any point of time if
84:05 - a person is trying to query from this
84:06 - particular DB you will be able to get
84:09 - that specific response from that right
84:11 - and the most similar response that
84:12 - you'll be able to get it now that is the
84:14 - next step what we are basically going to
84:16 - do all these vectors we are going to
84:18 - save it in some kind of vector database
84:20 - as I said we going to use casser DB or
84:23 - we can also say astrab and this will be
84:25 - created in this data STS
84:28 - platform wherein you can actually
84:30 - perform Vector search now the next thing
84:33 - is that after you probably save entirely
84:35 - all your vectors in in the database
84:37 - itself then a human whenever a human
84:40 - tries to query anything that is related
84:43 - to that particular PDF document it is
84:45 - going to probably apply similarity
84:47 - search along with text iddings and is
84:49 - going to get that specific response so
84:52 - this this is the entire
84:53 - architecture that we are specifically
84:56 - going to perform in this specific
84:57 - project all the steps will be shown step
85:01 - by step everything will be explained in
85:03 - an amazing way along with the code and
85:05 - along with the explanation now let's go
85:07 - ahead and start our specific project for
85:10 - this PDF query with Lang chain and
85:12 - cassendra DB so guys now let's go ahead
85:15 - and implement this specific project I
85:17 - will be going step by step I will also
85:19 - be showing you how you can create the
85:20 - cassendra DB specifically in the data
85:24 - Stacks uh platform itself uh we'll be
85:26 - seeing step by step all the comment
85:29 - regarding this code and all is given
85:30 - over here I will also be providing you
85:32 - the code in the description of this
85:34 - particular video so first of all uh what
85:37 - exactly we are doing we are going to
85:39 - query PDF with astb and Lang chain uh it
85:43 - is basically powered and uh understand
85:45 - it is powered by Vector search so first
85:47 - of all you need to understand what
85:48 - exactly is Vector search so there is an
85:50 - amazing documentation that is given in
85:52 - the data stack documentation itself so
85:54 - Vector search enhances machine learning
85:56 - models by allowing similarity comparison
85:58 - of the embeddings embeddings basically
86:00 - means whatever text is basically
86:02 - converted into the vectors that is
86:04 - basically embedding right and over there
86:06 - you can definitely apply multiple
86:08 - algorithms right machine learning
86:09 - algorithms on the fly right as a
86:11 - capability of astrab vector search
86:13 - supports various large language models
86:15 - so large language models can be is very
86:18 - is supported in an amazing way in this
86:19 - the integration is very much smooth and
86:21 - easy right since this llm are stateless
86:24 - they rely on Vector database like Astro
86:26 - DB to store the embeddings see
86:28 - understand because uh when we say
86:30 - stateless that basically means what
86:32 - suppose if we have embeddings once we
86:34 - lose it we cannot again query it right
86:36 - so it is definitely require a database
86:38 - to probably store all these things and
86:40 - what you can do after that you can query
86:42 - any number of time so let us go step by
86:44 - step and let us see okay so first of all
86:47 - we need to create a database on astb so
86:50 - I will probably click this specific link
86:52 - everything is basically given over here
86:54 - for this we will be going to
86:57 - astra.com right so first of all it will
87:01 - probably ask you to sign in right and
87:04 - here you can probably sign it with your
87:06 - GitHub or with your Google account so
87:08 - here I'm going to go ahead and sign it
87:10 - with GitHub and probably once I probably
87:13 - sign in over here you will be able to
87:16 - see that uh I'll be providing you the
87:18 - link along with everything in the uh
87:21 - code itself right so it'll be very much
87:23 - easy for you so once you go to Astra
87:26 - data.com the next step is basically to
87:29 - create a database right so this database
87:32 - uh what kind of database we are going to
87:33 - probably create it will be serverless
87:35 - vector and this is specifically a
87:37 - cassendra DB okay so here I will
87:40 - probably give my database name let's say
87:42 - I want to do PDF query right so this
87:44 - will basically be my PDF query DB okay
87:47 - this will basically be my database name
87:49 - you can give anything as you want and
87:51 - here I'll be basically be giving Lang
87:53 - chain _ DB a key space name it should be
87:56 - unique the provider that you can
87:57 - specifically use you have multiple like
88:00 - Amazon web services Microsoft Azure but
88:02 - here I'm going to probably use Google
88:03 - Cloud which is the default that is
88:05 - selected in the next step we will go
88:07 - ahead and select the country region
88:09 - which is by default Us East one so as
88:12 - soon as you probably fill all this
88:14 - details and as you know that we are
88:16 - specifically going to use this Vector
88:18 - database itself because at the end of
88:20 - the day the algorithms that we probably
88:22 - going to apply it will be easy with
88:24 - respect to this kind of database right
88:26 - so finally we will go ahead and create
88:28 - the database now once how we create the
88:31 - database you will be able to see that my
88:33 - database is basically created over here
88:35 - right so this is what is my database
88:37 - that looks like right PDF query diving
88:41 - now if I probably go to my dashboard
88:42 - I've already created this kind of
88:44 - database a lot so let me consider one
88:46 - database which I have already created
88:48 - and over here some important information
88:50 - that you really need to take first of
88:52 - all I will go and click on connect okay
88:55 - so when I probably click on connect one
88:58 - some information you will definitely
89:00 - require one is generate token right and
89:02 - the other one is the DB ID so DB ID is
89:05 - basically present over here right the
89:07 - token is basically present over here now
89:09 - I'll talk about where this specific
89:11 - information will be required okay so
89:14 - here I will go with respect to my code
89:16 - now let's start our coding initially we
89:18 - will be requiring some of the important
89:21 - libraries like Casio data set langin
89:23 - open Tik toen so here I will go ahead
89:26 - and execute it and I will go ahead and
89:28 - install all the specific libraries so it
89:31 - will probably take some time right I
89:33 - have already done that installation so
89:35 - for me it has happened very much quickly
89:37 - now the next thing is that as we know
89:40 - that we are specifically going to use
89:41 - cass. DB so in Lang chain you have all
89:44 - these libraries which will actually help
89:45 - you to connect with cassendra DB and
89:47 - perform all the necessary tasks like
89:49 - text T meetings uh creating V vors and
89:52 - probably storing it in the database
89:53 - itself so here I'm going to probably
89:55 - import all these libraries from lin.
89:58 - Vector stores. cassendra I'm going to
90:00 - import cassendra along with this I'm
90:02 - also going to use this Vector store
90:03 - index wrapper it is going to wrap all
90:05 - those particular vectors in one specific
90:08 - package so that it can be used quickly
90:10 - then I'm also going to import open AI
90:12 - because open AI is the thing that we
90:14 - really need to use along with this we
90:15 - are also going to use open a embeddings
90:17 - which will be responsible for converting
90:20 - your text into vectors
90:22 - along with this if you want some kind of
90:23 - data set from hugging face you can also
90:25 - use this and one more important library
90:28 - that we are going to use is cashio now
90:30 - Casio actually helps you to uh probably
90:33 - integrate with the Astra DB right in
90:35 - Lang chain and it'll also help you to
90:38 - initialize the DB connection so all
90:39 - these libraries we are going to use I'm
90:41 - going to execute this step by step we
90:43 - going to probably see and this is the
90:44 - first step installing the libraries and
90:46 - initializing all the libraries that we
90:48 - are specifically going to use along with
90:50 - this what we are going to also import is
90:52 - one PDF which is called as Pi PDF 2 this
90:55 - will actually help you to read any PDF
90:58 - uh read the uh text inside the PDF
91:01 - itself so this is one amazing library to
91:03 - probably use okay so here I have
91:05 - basically used pip install Pi PDF 2 so
91:07 - let me just go ahead and execute it and
91:09 - inside this you will be able to see it
91:11 - shows requirement already satisfied
91:13 - because I've already installed over here
91:15 - then from PI PDF you're going to use PDF
91:17 - reader because this will be the
91:19 - functionality that will be used in order
91:21 - to to read the document here is the
91:24 - document that I have specifically taken
91:25 - so this is one budget speech PDF so this
91:28 - is the Indian budget that is probably of
91:31 - 2023 it's a big document with somewhere
91:34 - around 461 KB file it has around 30
91:37 - pages so I'm going to specifically read
91:39 - this PDF and then convert into vectors
91:42 - store it in the database itself and then
91:44 - query from the database anything that
91:46 - you have any information about that
91:48 - particular PDF now let's go ahead with
91:50 - the setup okay now with respect to the
91:53 - setup you require three important
91:55 - information one is the astrab
91:57 - application token one is the Astra DB ID
92:01 - okay so where you can probably get this
92:03 - two information so go to your vector
92:06 - database uh Vector database in the data
92:08 - Stacks so here uh is what you have
92:10 - specifically logged in okay as I said
92:14 - inside your DB just go and click on
92:16 - connect here you need to click on
92:18 - generate token as soon as you probably
92:20 - click on generate token then you will be
92:22 - getting some code which looks like this
92:24 - this token you will specifically go
92:26 - getting so this will be probably found
92:29 - in your token Json file so it'll
92:31 - probably show you a Json file which will
92:32 - have this information okay the first
92:35 - information that you have over here is
92:37 - the Astra DB application token so here
92:39 - you can probably see it starts with
92:42 - Astra CS so what you need to do just
92:44 - click on the generate token and you'll
92:45 - be able to see it this is the first
92:47 - information you just need to copy and
92:48 - paste it over here the second
92:50 - information is Astra DB ID right Astra
92:53 - DB ID is nothing but this specific
92:55 - information that is your database ID
92:57 - where do you get it you just need to
92:59 - copy it from here so this is the
93:00 - information with respect to your astrab
93:03 - ID so this two information once you do
93:05 - it you paste it over here I've already
93:07 - pasted it and then you can also see that
93:09 - I've also used some open API key and
93:11 - this specific API key don't use this
93:14 - only because Ive made some changes I've
93:15 - already executed the code also okay so
93:18 - I'm going to take this three information
93:20 - this two information is basically used
93:22 - to connect to your Astra DB right which
93:24 - has a cassendra DB hosted over there in
93:26 - the cloud right and the other
93:28 - information is basically to use the open
93:31 - AI API features right so all this
93:33 - information is basically there I'm going
93:35 - to probably execute this and then we
93:37 - will go ahead and read our budget speech
93:40 - PDF so this is the first step according
93:42 - to this we are reading the specific
93:44 - document before that we have initialized
93:45 - everything with respect to this okay so
93:48 - once we specifically do this I will
93:50 - probably be reading this
93:52 - now after reading as I said we are going
93:54 - to divide all our content into some kind
93:57 - of chunks right so here is what chunks
93:59 - we are basically going to do now first
94:01 - of all I will read all the raw text so
94:03 - for this I'm going to use from type
94:05 - extension using concatenate I'm going to
94:08 - read from each and every pages I will
94:10 - extract all the text so here you can
94:12 - probably see for I comma page in
94:15 - enumerate PDF reader. Pages page.
94:19 - extract text Will basically take out all
94:21 - the text from those pages and it will
94:23 - concatenate in this particular variable
94:25 - that is rawcore text so once I probably
94:27 - execute this what will happen is that
94:29 - you will be able to get all the text
94:33 - inside this particular variable so you
94:34 - can probably see over here rawcore text
94:37 - has all the entire text so this is the
94:40 - entire text from that specific PDF right
94:42 - slash in basically means new line so
94:44 - this step is basically done just imagine
94:46 - before if we did not had this specific
94:48 - Library it was very difficult to read a
94:49 - PDF right and we have actually done this
94:52 - just with writing four to five lines of
94:53 - code now the next step is that we will
94:57 - initialize the connection to your
94:58 - database I have all my database
95:00 - information right like uh token ID and
95:03 - the database ID I'm going to use that
95:05 - cashio cashio is basically used as a
95:07 - library over there for initializing of
95:09 - this particular database so cashier.
95:11 - init here I'll be giving one parameter
95:13 - which is called as token which will be
95:15 - nothing but astb application token and
95:17 - then your database ID which is nothing
95:19 - but astb ID right so I've taken this two
95:21 - information I will execute this you'll
95:23 - get some kind of warnings so don't worry
95:25 - about the warnings it is just like it is
95:27 - showing you some kind of warnings okay
95:29 - with respect to some drivers issues and
95:30 - all but this will basically get executed
95:33 - and now I have uh basically initialized
95:35 - my DB itself right now we are going to
95:38 - create the Lang chain embeddings L LM
95:41 - objects for letter use so for that I'm
95:43 - going to use I'm going to initialize
95:44 - open AI with my open AI key and
95:47 - embeddings also open AI embeddings with
95:48 - my open a key so I have my llm I have I
95:51 - have my embeddings okay now is the main
95:54 - step I need to create my lch Vector
95:57 - store so over here this is what we are
96:00 - basically going to create now right and
96:02 - for that you know we have initialized
96:04 - cendra right we have we have imported
96:06 - cendra now what will do is that in this
96:08 - cassendra we will provide three
96:10 - important information what is the kind
96:12 - of embeddings we are going to use what
96:14 - is the table name inside this particular
96:16 - database session none keyp space none so
96:18 - this is the default parameters we have
96:20 - specifically used QA mini demo is my
96:23 - table name okay just like a question
96:25 - answer table name and what kind of
96:27 - embeddings we are going to specifically
96:29 - use that basically means whenever we
96:30 - store any whenever we push any data in
96:33 - my cassendra DB in my Astra DB itself
96:35 - what it is going to do it is going to
96:37 - probably convert all the text using this
96:39 - embeddings into vectors right and this
96:41 - is the embeddings that we have
96:42 - initialized over here so here is the
96:44 - next step we will go ahead and execute
96:46 - this so this is my Astra Vector store
96:48 - but still I have not probably converted
96:51 - my text into vectors only when when I'm
96:53 - pushing my data inside my DB that time
96:56 - this entire embeddings will probably
96:58 - convert that particular data into
97:00 - vectors then what we are specifically
97:03 - going to do is that we will take this
97:05 - data and we will try to uh we'll take
97:08 - this entire data we'll convert into
97:10 - checks uh chunks and we'll also do the
97:12 - text embedding right text embedding
97:14 - while inserting right so here you first
97:16 - of all we are dividing the data or the
97:18 - entire data entire document into text
97:20 - Chunk so for this we are using character
97:22 - text splitter which is basically present
97:24 - in Lang chain. text splitter we need to
97:27 - split the text using character text
97:28 - splitter it should not increase the
97:30 - token size so here I've given character
97:33 - text splitter I'm saying use the
97:35 - separator slash in use chunk size some
97:37 - chunk size of 800 characters chunk
97:39 - overlap can be 200 and how much is the
97:41 - length with respect to that specific
97:42 - length you can probably provide it over
97:44 - here right and once I probably do this
97:47 - you can see text splitter. split text
97:49 - here you will be able to get all the
97:51 - text and if I see the top 50 text you
97:54 - can probably see that I'll be able to
97:56 - see all the top 50 text over here right
97:59 - all the data itself this is amazing
98:01 - right and this is basically from the PDF
98:04 - right all the data all the data right it
98:06 - is basically taking the top 50 right and
98:09 - understand the token size is basically
98:11 - over here as the chunk size is somewhere
98:13 - around 800 okay now this is done we have
98:16 - the text I'm going to just use top 50
98:18 - and probably store it in the vector
98:20 - database to see if everything is working
98:22 - fine now how to add this specific text
98:25 - now what will happen when I add this
98:26 - text inside my Cassandra DBC axtra
98:30 - Vector store what is this this is
98:31 - basically initialized with respect to
98:33 - the cendal library right so here you'll
98:35 - be able to see that I have used
98:37 - embeddings so now when I'm inserting
98:39 - inside the cassendra DB what it is going
98:41 - to do it is going to apply this specific
98:43 - embeddings also so that is the reason
98:45 - you'll be able to see that when we write
98:47 - extraor Vector uncore store. addore text
98:51 - and I'm taking the top 50 top 50 texts
98:54 - over there this will also perform
98:56 - embeddings so that basically means if I
98:58 - see over here it is going to perform
99:00 - this task and it is going to insert in
99:02 - the Astra DB which is having that
99:03 - cassendra over there right so it is
99:05 - going to do this both the steps with
99:07 - respect to this particular code so we
99:09 - are going to add this text and then we
99:11 - also going to wrap wrap this entire
99:14 - inside a wrapper okay so these are the
99:16 - information this is the index that we'll
99:18 - be getting with respect to those text so
99:20 - once I proba executed you'll be seeing
99:22 - that in the same database it is going to
99:25 - insert all this headlines okay now
99:28 - finally let's go ahead and tex it that
99:30 - basically mean I have my vectors inside
99:32 - my database now it's time that we just
99:34 - query and we ask some kind of questions
99:37 - now I have read this entire PDF guys I
99:39 - could find out some of the question like
99:40 - what is the current gbd how much
99:42 - agriculture Target will be increased and
99:44 - all so I will take this particular
99:45 - example and let's say I'm writing first
99:48 - question is equal to True while true if
99:49 - first question I'm just say that input
99:51 - okay it will just ask like what kind of
99:53 - question you want to type else uh it is
99:56 - just asking you to uh put more questions
99:59 - if I write quit it is going to break
100:01 - otherwise it is going to continue now
100:03 - see this is the most important as soon
100:04 - as I give my first question it will go
100:06 - ahead with v Astra Vector index and
100:09 - it'll query whatever query text we are
100:11 - specifically using and the llm models
100:13 - that we are specifically initialized and
100:15 - after that we will be getting the answer
100:18 - along with this we'll also be providing
100:19 - some information
100:21 - right like for Doc score or similarity
100:24 - Source score like some other information
100:26 - also right so let's go ahead and execute
100:28 - it and as I said I'm going to use this
100:30 - question okay how much is the
100:33 - agriculture Target to be increased and
100:36 - what focus it will be okay so I'm going
100:38 - to paste it over here I'm going to press
100:40 - enter so as soon as I press enter you
100:42 - can see that it is now taking the
100:44 - information see this um you can probably
100:47 - see over here we are quering this
100:49 - particular DB right and it's going to
100:50 - give me the top four results okay so
100:52 - here you can see that agriculture credit
100:54 - Target will be increased to 20 lakh CR
100:56 - with the focus on animal husbandry da
100:59 - and Fisheries right why it is giving
101:01 - only this much data because I've told
101:02 - that take the 84 characters or 84 words
101:06 - 84 characters text still there and
101:08 - probably give the results right if I
101:10 - increase this it'll give you more result
101:12 - along with this you can probably see
101:13 - that it is giving me stop K queries that
101:16 - is the four query Hyderabad will be
101:18 - supported as Center of Excellence some
101:20 - more information but the most suitable
101:22 - answer that you have specifically got is
101:24 - this one right and this is what probably
101:27 - if you go ahead and search in the PDF if
101:29 - you give the same question you will be
101:31 - able to see the same answer right along
101:33 - with this probably if I want to probably
101:35 - see what is the current GDP if this
101:37 - information is present over there it'll
101:39 - also be giving you that specific answer
101:41 - it'll just do the similarity search
101:43 - right so here you can current gbd is
101:45 - estimated to be 7% isn't it amazing now
101:48 - you can probably take any huge data
101:50 - because at the end of the day you
101:51 - specifically using DB right and finally
101:54 - if you want to quit it I will just go
101:55 - ahead and write quit and this is
101:57 - basically quit right so in short we have
101:59 - performed each and every step now this
102:01 - is what which is happening whenever a
102:02 - human is giving an text query text
102:04 - emings will happen and based on that
102:06 - similarity search and then you'll be
102:07 - probably get the output right and this
102:09 - is the entire steps We have basically
102:11 - done step by step so guys yet another
102:15 - amazing video on generative AI where I
102:18 - will be specifically discussing about
102:20 - llama 2
102:21 - uh Lama 2 is an open-source model uh
102:24 - again it has been created by Facebook or
102:27 - meta and you can use this specific model
102:30 - even for commercial purpose uh so this
102:32 - is quite amazing this is an
102:34 - open-source llm model altogether I will
102:37 - try to show you how we can use this
102:39 - create an end to endend project also in
102:42 - this specific video so there are many
102:44 - things that are going to happen and
102:45 - probably whatever topics that I teach
102:47 - going forward that is related to
102:49 - generative AI I will definitely follow
102:51 - this kind of approach so that you also
102:53 - get a brief idea about all these kind of
102:55 - models so what is the agenda of this
102:58 - particular video the agenda is that we
103:00 - will get to know about Lama 2 then we
103:03 - will go ahead and see the research paper
103:04 - where I will be talking about the key
103:06 - points uh about the Lama 2 model again
103:09 - since this is an open source and uh soon
103:12 - Lama 3 is also going to come up so that
103:14 - is a reason I'm going to create this
103:15 - particular video I really want to be in
103:17 - sync with all the open source llm models
103:19 - that are coming up right
103:21 - then we'll go and apply and download the
103:23 - Llama 2 model so we'll be seeing like
103:25 - how we can actually use this particular
103:27 - model in our project also so for that
103:29 - purpose I will be downloading this model
103:31 - you have to also apply this in the meta
103:33 - website itself and there is also one way
103:37 - how uh we can also use it directly from
103:39 - hugging face so I will also show you
103:41 - that and after that we will try to
103:43 - create an end to end llm project and
103:45 - this will be a Blog generation llm app
103:48 - uh all these topics I will be covering
103:50 - it I know it'll be a little longer video
103:53 - but every week one kind of this kind of
103:55 - video is necessary for you all and since
103:58 - 2024 I have the target I really need to
104:01 - teach you gener in a way that you can
104:04 - understand it and use it in your
104:05 - industries also so I will keep a Target
104:09 - so every video I'll keep a Target like
104:10 - this target for this particular video
104:12 - is, L likes not thousand lcks but
104:16 - thousand likes and comments please make
104:19 - sure that you write some comments and
104:21 - I'll keep the target to 100 okay
104:24 - so this will actually motivate me this
104:27 - will probably help this particular video
104:29 - to reach to many people through which
104:31 - they can actually use this and entirely
104:33 - this is completely for free which will
104:34 - also be beneficial for you and I my aim
104:37 - is to basically democratize the entire
104:39 - AI education okay so let's go ahead and
104:42 - let's first of all start with the first
104:44 - one that is introducing Lama 2 what
104:46 - exactly is Lama 2 Lama 2 is an again
104:49 - open source a large language model it
104:51 - can be it is used and it is uh available
104:54 - for free for research and commercial
104:57 - purpose you can actually use this in
104:59 - your companies in a startup wherever you
105:01 - want to use it okay now let's go ahead
105:03 - and read more about it so inside this
105:06 - model uh it has till now Lama 2 has
105:08 - released three different model size uh
105:11 - one is with 7 billion parameters the
105:13 - other one is 13 billion parameters and
105:15 - the the best one is somewhere around 70
105:17 - billion parameters uh pre-training
105:19 - tokens is taken somewhere around 2
105:21 - trillion context length is
105:24 - 4096 uh again when I say that if I
105:27 - probably compare most of the open source
105:29 - models I think Lama 2 is probably very
105:32 - good we'll be seeing all those metrics
105:34 - also so here you can see Lama 2
105:36 - pre-trained models are trained on two
105:37 - trillion tokens and have double the
105:39 - context length than Lama one it's fine
105:42 - tune models have been trained on over 1
105:45 - million human annotation okay and now
105:47 - let's go ahead and see The Benchmark and
105:49 - this is with respect to the the
105:50 - benchmarking with all the open source
105:52 - models so it is not comparing with chat
105:54 - GPT sorry GPT 3.5 GPT 4.0 or Palm 2 okay
105:58 - so all the open source models uh here
106:00 - you can probably see this is the three
106:02 - version 7 billion 13 billion 65 billion
106:04 - 70 billion right all Lama 2 right llama
106:07 - 1 was 65 billion one uh one model it had
106:10 - over there so if you see Lama 2 with
106:13 - respect to all the metrix is very good
106:15 - MML that is with respect to human level
106:18 - understanding Q&A all all the
106:20 - performance metrix is superb natural
106:22 - language processing gsmk human evalve in
106:25 - human evalve it is probably having a
106:27 - less when compared to the other other
106:29 - open source models so here you can see
106:31 - in human uh human Val human eval human
106:34 - eval basically means with respect to
106:36 - writing code code generation there it
106:38 - has a lot of problems so here you can
106:40 - see 12.8 18.3 it is less it is less when
106:43 - compared to all the other open source
106:46 - models over here and there are also some
106:48 - other parameters you can probably see
106:49 - over here with with respect to different
106:51 - different tasks you can see the
106:52 - performance metrics okay so this was
106:54 - more about the model now let's go ahead
106:56 - and
106:57 - probably and this is one very important
106:59 - statement that they have come up with we
107:01 - support an open Innovation approach to
107:03 - AI responsible and open Innovation give
107:06 - us all a stake in the AI development
107:08 - process so uh yes Facebook is again
107:11 - doing a very good work and then soon
107:13 - they also going to come up with the Lama
107:14 - 3 Model now let's go ahead and see the
107:17 - research paper so here is the research
107:18 - paper the entire research paper now see
107:21 - uh what you should really focus on a
107:23 - research paper you know in research
107:24 - paper they'll be talking about how they
107:27 - have actually trained the model what
107:29 - kind of data points they have they
107:30 - actually taken in order to train the
107:32 - model and all right so over here you can
107:34 - see that um in this work we developed
107:36 - and release Lama 2 a collection of
107:38 - pre-trained and fine-tune Lun language
107:40 - models ranging in scale from 7 billion
107:42 - to 70 billion parameters so if you talk
107:44 - about parameters it is somewhere around
107:45 - 7 billion to 70 billion our fine tune
107:48 - llms called Lama 2 chart are optimized
107:50 - for dialog use cases just like a chat
107:52 - bot and all right uh more information
107:54 - you can probably see over here what is
107:56 - the pre-training data see so they have
107:58 - told that our pre-training data includes
108:00 - a new mix of data from publicly
108:02 - available sources which does not include
108:04 - data from meta products or Services
108:06 - we've made an effort to remove data from
108:08 - certain sites known to contain a high
108:10 - volume of personal information about
108:12 - private individuals now this is where
108:14 - ethics comes into picture they really
108:16 - want to use this AI in a responsible way
108:18 - right so we trained on two trillion
108:20 - tokens uh and obviously for all these
108:23 - things you have to use Nvidia GPU okay I
108:25 - know guys it is boring to read the
108:27 - research paper but it is good to have
108:29 - all this specific knowledge so please
108:31 - keep your energy up watch this video
108:34 - till the end then only you'll be able to
108:36 - understand things right not only here
108:38 - because later on you'll be having other
108:40 - models like Mistral I'll probably create
108:42 - a video on Mistral also in the upcoming
108:44 - video right so everywhere with an end to
108:46 - end project everything I will take this
108:49 - format let me know know whether you're
108:50 - liking this format or not so training
108:52 - data we adopt most of the pre-training
108:54 - settings and model architecture from
108:56 - Lama one we use the standard Transformer
108:58 - architecture now you can understand how
109:00 - important Transformer is right most of
109:02 - the open source model are based on
109:04 - Transformer architectures itself right
109:06 - we trained using adamw Optimizer okay
109:09 - with so and so parameters we use consign
109:12 - learning rate schedule with so and so
109:14 - and here you could probably see with
109:15 - respect to the performance like how well
109:18 - it was training BPL process tokens how
109:20 - many tokens was actually done with
109:22 - respect to all the different varieties
109:23 - of llama model now this is basically the
109:26 - training loss you can probably see
109:27 - training loss for Lama 2 okay this is
109:30 - also important training hardware and
109:31 - carbon footprint it is basically saying
109:33 - that how much it is using they used
109:35 - Nvidia a100 I've seen this GPU it's
109:38 - quite amazing it's very huge okay and it
109:41 - is very fast also but again with such a
109:43 - huge amount of data it is also going to
109:44 - take time right so all these things are
109:46 - there you can also see time how much
109:48 - time it has basically taken how how many
109:50 - hours 70 billion this many number of
109:52 - hours power consumption this this all
109:55 - information is there right this is good
109:57 - to have right all all you should know
109:59 - like we just taking more energy and all
110:01 - right and here um with respect to the uh
110:05 - llama 2 you can probably see with
110:06 - respect to Common reasoning it is very
110:08 - good when compared to all the other
110:10 - models open source model World Knowledge
110:12 - reading comprehension math mlu math it
110:16 - is little bit less you can see over here
110:19 - when compared to the other model I think
110:20 - it is still 35 itself but remaining all
110:23 - it has basically come I think this 35 is
110:25 - also greater than all these things right
110:28 - MML is very much good it is able to
110:29 - achieve till 68.9 Google gini has said
110:32 - that it is reach to 90% okay but again
110:35 - this is the thing that you really need
110:36 - to know uh some more information fine
110:39 - tuning fine tuning also okay this is
110:41 - very much important guys it has it has
110:44 - used this uh reinforcement learning okay
110:47 - where uh and with human feedback so our
110:50 - R lhf basically means reinforcement
110:52 - learning with human feedback and this is
110:54 - what chat GPT is also trained with right
110:56 - so uh definitely I think as we go ahead
110:59 - as we go ahead and see Lama 3 and all it
111:00 - is going to give us very good accuracy I
111:03 - guess okay so superv fine tuning uh if
111:06 - you go ahead and just check how
111:08 - generative AI how llm models are trained
111:10 - you'll be able to get a video on this I
111:12 - created a dedicated video where I
111:14 - explained about supervised fine tuning
111:16 - how does supervised fine tuning happen
111:19 - what how does
111:20 - uh rhlf happens right reinforcement
111:23 - sorry R lhf human feedback happens all
111:26 - those things I've actually explained so
111:28 - here you can see some of the prompts
111:29 - right a poem to help me remember the
111:31 - first 10 elements on the periodic table
111:32 - hydrogen come first as the element one
111:34 - helium is second for balloons this this
111:36 - I want you to roast me now see this
111:38 - statement is also very important right
111:40 - so uh I want you to roast me I want you
111:43 - to make it particular brutal swearing at
111:46 - me so it is saying I'm sorry but I
111:47 - cannot comply with that request using
111:50 - language or intentional hurting someone
111:51 - feelings is never expectable so some
111:53 - kind of feelings they're trying to bring
111:55 - inside all these kind of models okay uh
111:58 - sft annotation is basically there you
111:59 - can probably read all these things this
112:01 - is good to have good to learn how this
112:03 - reinforcement learning with human
112:04 - feedback was done and all everything is
112:06 - given over here so uh this was all about
112:09 - the research paper still there are many
112:11 - papers to go ahead you can probably go
112:12 - ahead and check it out uh there is a
112:14 - concept of reward modeling also reward
112:16 - is also given right the parameters they
112:19 - have used two separate parameters over
112:20 - here and various kind of test is
112:22 - basically done so this was all the
112:24 - information about this now the next
112:26 - thing is that how you can go ahead and
112:27 - apply or download this specific model
112:30 - just click on download the model over
112:31 - here so the third part provide all the
112:35 - information over here and what all
112:36 - things you specifically required like
112:37 - Lama 2 and Lama chart code Lama Lam
112:39 - guard so go ahead and just put all this
112:42 - information and click on submit after
112:44 - submitting probably it'll take 30
112:46 - minutes and you will start getting this
112:48 - mail okay
112:50 - you all start to set building with code
112:52 - llama you will also be getting the
112:54 - access from Lama 2 see you'll be getting
112:56 - this entirely right model weights
112:59 - available all the models weight will be
113:00 - given to you in this specific link you
113:02 - can click and download it also if you
113:04 - want so that you can use it in your
113:06 - local or you can deploy it wherever you
113:07 - want okay so this kind of mail you'll be
113:09 - getting uh Lama 2 commercial license all
113:12 - the information with all the info over
113:14 - here and these all models it is
113:16 - specifically giving again I told you 70b
113:19 - 70b chat why these two models are there
113:21 - this is specifically for Q&A kind of
113:24 - application dialog flow application I
113:26 - can basically say uh remaining one can
113:28 - be used for any kind of task uh in a
113:30 - complex scenarios and all okay so once
113:33 - you do this the next thing is that you
113:34 - can also go to hugging face in hugging
113:36 - face you have this Lama 270b chat FF and
113:40 - there is the entire information that is
113:42 - probably given about the entire model
113:44 - itself you can probably read it from
113:46 - here with respect to this Lama 2 is a
113:48 - collection of pre-trained this this
113:49 - information is basically there you can
113:51 - also directly use it if you want the
113:53 - code with respect to Transformer you
113:54 - just click on using Transformer you'll
113:56 - be able to get this entire code where
113:58 - you can directly use this also okay what
114:00 - we are basically going to do I'm not
114:02 - going to use 70 billion parameters since
114:04 - I'm just doing it in my local machine
114:05 - with the CPU itself okay so what I will
114:08 - do I will be using a model which is
114:10 - basically uh it is basically a quantized
114:12 - model right with respect to this same
114:14 - llama model it is called as Lama to 7B
114:16 - chat gml so if you go ahead and see see
114:19 - this uh you'll be able to see that this
114:21 - particular model you'll be able to
114:23 - download it and you'll be able to use it
114:26 - it is just like a good version but uh
114:29 - less parameter versions right so when we
114:31 - say contage that basically means uh this
114:34 - model has been compressed and probably
114:36 - provided you in the form of weight so
114:37 - what you can do any of these models the
114:39 - recent model what you can do over here
114:41 - which is of 7.16 GB you will first off
114:43 - all download it so I've already
114:45 - downloaded it so I'm just going to
114:47 - cancel it over here okay because I've
114:50 - already downloaded it over here okay so
114:52 - I will do that specific download uh over
114:54 - here and then you can probably go ahead
114:57 - and start working on this and start uh
114:59 - using this and now how you can probably
115:01 - use it I Will Show You by creating an
115:04 - endtoend project so for creating an NN
115:07 - project what are the steps uh again the
115:09 - project name that I've already told is
115:11 - basically a b blog generation llm app
115:14 - here I'm going to specifically use this
115:16 - open-source llama Lama 2 model again I'm
115:19 - going to use the hugging face API also
115:21 - for that uh and let's see how the
115:23 - specific uh step by step how we'll be
115:25 - doing this specific project so let's go
115:27 - ahead and let's start this particular
115:28 - project okay guys now let's start our
115:31 - blog generation llm platform uh
115:33 - application so the model that I had
115:36 - actually generated over here you can
115:37 - probably see the model over here in the
115:39 - bin size and this is the size of the
115:41 - model is over here I'm going to
115:43 - specifically use in my local machine for
115:45 - my local inferencing and all so over
115:48 - here what I will do I will go quick
115:49 - quickly go ahead and open my VSS code so
115:52 - my vs code is ready over here okay now
115:57 - let's go ahead and do step by step
115:59 - things that we really need to do first
116:00 - of all I'm just going to create my
116:01 - requirement. txt file requirement. txt
116:05 - file and now I will go ahead and open my
116:09 - terminal so I will go ahead and open my
116:12 - command prompt and start my project okay
116:15 - so quickly I will clear the screen I
116:17 - will deactivate the default
116:20 - environment cond deactivate okay and
116:23 - we'll do it step by step so first step
116:26 - as usual go ahead and create my
116:28 - environment cond create minus P VNV
116:32 - environment I hope I've repeated this
116:34 - specific step lot many times so here I'm
116:36 - going to create cond create minus pvnv
116:39 - with python wal to
116:43 - 3.9 y okay so just to give you an idea
116:47 - what how exactly it is going to run run
116:50 - how things are basically going to happen
116:52 - uh step by step we'll understand so
116:55 - first of all we are creating the
116:57 - environment and then we will go ahead
116:59 - and fill our requirement. txt now in
117:02 - requirement. txt I'm going to
117:04 - specifically use some of the libraries
117:06 - like sentence Transformers C Transformer
117:10 - fast API if you want to specifically use
117:12 - fast API I I'll remove this fast API I
117:15 - think uh I will not require this IPI
117:17 - kernel so that I can play with Jupiter
117:20 - notebook if I want I can also remove
117:21 - this I don't want it langon I will
117:24 - specifically using and streamlet I'll be
117:26 - using okay so first of all I will go
117:28 - ahead and create cond activate Okay cond
117:33 - activate uh
117:35 - venv so we have activated the
117:37 - environment and the next thing is that I
117:39 - will go ahead and
117:41 - install all the requirement.
117:44 - txt okay and in this you don't require
117:48 - uh okay so okay I've not saved it so
117:51 - requirement. txt is not saved now in
117:54 - this you don't require any open AI key
117:56 - because I'm just going to use hugging
117:58 - face and from hugging face I'm going to
117:59 - probably call my model which is
118:01 - basically present in my local so here is
118:03 - the model that I am going to
118:04 - specifically call okay so once this
118:07 - installation will take place then we
118:09 - will go ahead and create my app.py and
118:13 - just give you an idea like uh I'm going
118:15 - to basically create the entire
118:18 - application in this specific
118:20 - file itself so quickly uh let's go ahead
118:24 - and import our streamlet so till the
118:28 - installation is basically happening I
118:30 - will go ahead and install
118:32 - streamlet Okay as
118:35 - St and then along with this I will also
118:38 - be installing Lang chain. prompts
118:40 - because I'm also going to use prompts
118:42 - over here just to give you an idea how
118:44 - things are going to happen it's going to
118:46 - be very much fun guys because open
118:48 - source right it it's going to be really
118:50 - amazing with respect to open source you
118:51 - don't require anything as such and then
118:54 - I'm going to basically write prompt
118:55 - template because we need to use this
118:57 - from Lang chain then I'll be also using
119:00 - from Lang
119:02 - chain Lang chain do llm I'm going to
119:06 - import C
119:09 - Transformer okay why this is used I will
119:12 - just let you know once I probably write
119:14 - the code for this okay so three three
119:16 - Transformers also I'm going to basically
119:18 - use over here so this is going to be
119:20 - from okay so C Transformers prom
119:23 - template and St for the streamlet I'm
119:26 - going to specifically use the first
119:28 - thing is that I will go ahead and write
119:30 - function to
119:32 - get
119:34 - response from my um llm uh llama model
119:38 - right Lama 2 model I'm going to
119:41 - basically use this okay still the
119:44 - installation is taking place guys it is
119:45 - going to take time because there are so
119:47 - many libraries I've been installing okay
119:49 - so I'll create a function over here
119:51 - let's create this particular function
119:53 - later on okay now after this what I'm
119:56 - actually going to do is that we'll go
119:57 - ahead and set our streamlet right setor
120:02 - pageor config see now many people will
120:05 - say streamlet or flask it does not
120:07 - matter guys anything you can
120:08 - specifically use streamlet why I'm
120:10 - specifically using is that it'll be very
120:12 - much easy for me to probably create all
120:14 - the things right the UI that I want so
120:18 - in a set page config I'm going to
120:21 - basically use page title generate blogs
120:23 - page icon I've taken this robot icon
120:25 - from the streamlet documentation layout
120:27 - will be Central and uh initial sidebar
120:30 - will be collapsed okay so I'm not going
120:33 - to open the sidebar in that specific
120:35 - page now I will keep my ht.
120:39 - header so ht. header in here I'm going
120:42 - to basically generate my blogs right so
120:48 - generate the blogs and I'll use the same
120:50 - logo if I want so it looks good okay so
120:54 - this is the next thing I will probably
120:56 - this will be my head over here first of
120:59 - all I will create my input text okay so
121:02 - input text field right and this will
121:06 - basically be my input text field and let
121:08 - me keep it as a um a text area or a text
121:13 - box whatever things is required so I
121:15 - will write go ahead and write St
121:18 - do
121:20 - st. input textor input okay so this will
121:24 - basically be my St so let's see
121:26 - everything is working fine why this is
121:28 - not coming in the color okay still the
121:30 - installation may be happening so over
121:32 - here I'll go ahead and write this I will
121:35 - say enter the blog topic right so if you
121:39 - just write the blog topic it will should
121:41 - be able to give you the entire blog
121:43 - itself with respect to anything that you
121:45 - want okay so done the installation is
121:47 - basically done over here you can
121:49 - probably see this good I will close this
121:52 - up now I'll continue my writing the code
121:54 - so I've created a input box now the
121:57 - other thing that I really want to create
121:58 - is that I'll try to create two more
122:00 - columns or two more Fields below this
122:02 - box okay one field I will say that how
122:05 - many words you specifically want for
122:07 - that blog okay so over here
122:11 - creating two more
122:13 - columns for additional two
122:17 - Fields additional two field okay so here
122:20 - first of all will be my column 1 let's
122:23 - say column 1 and column 2 I will just
122:25 - write it like
122:27 - this and here I will say St do
122:31 - columns and uh here I'll be using I'll
122:36 - be giving right what should probably be
122:38 - the width like let's say 5 comma 5 if
122:40 - I'm giving you'll be able to see that
122:42 - the width of the text box of width of
122:43 - the column that I specifically have I'll
122:45 - be able to see it okay I'm I'm just
122:47 - creating that width for that columns
122:49 - okay now I'll say with column one
122:53 - whenever I probably write anything in
122:55 - the column one or select in anything in
122:56 - the column one this will basically be my
122:59 - number of words okay number of words and
123:01 - for here I will be creating my St do
123:04 - text input and this text input will
123:07 - probably retrieve the details of number
123:11 - of words okay so here I have
123:14 - specifically number of words great now
123:18 - the next column that I specifically want
123:20 - the detail so for whom I am actually
123:23 - creating this particular blog I want to
123:25 - probably put that field also so with
123:27 - column 3 I will probably create
123:30 - something like this I will say okay fine
123:32 - um what blog style I will I'll create a
123:35 - field which is basically called as blog
123:37 - style okay now inside this blog style
123:41 - what I am actually going to do sorry not
123:43 - column 3 column two because I've created
123:45 - those variable over there okay so the
123:47 - blog style will be basically be a drop
123:49 - down so I will say St do select box okay
123:54 - and I will say what box this is
123:58 - specifically for so that first message I
124:00 - will say select
124:02 - write writing the
124:05 - blog
124:07 - for for okay so this I'm basically going
124:11 - to say that okay for whom I'm going to
124:12 - write this particular blog okay and with
124:15 - respect to this I can give all the
124:17 - options that I really want to give okay
124:19 - so for giving the options I will also be
124:21 - using this field so let's say the first
124:23 - option will be for researchers whether
124:25 - I'm writing that particular block for
124:27 - researchers or for data
124:30 - scientist okay data scientist or I am
124:34 - basically writing this block
124:38 - for for common people okay common people
124:42 - so this three information I really want
124:45 - over here and this will basically help
124:47 - me to put some Style filing in my blog
124:50 - okay that is the reason why I'm
124:51 - basically giving over here okay and by
124:54 - default since we need to select it in
124:56 - the first option so I will keep it as
124:58 - index as zero okay so here is all my
125:02 - stylings that I've have specifically
125:03 - used so if you want to probably make it
125:05 - in this way so you'll be able to
125:07 - understand this so this will be my
125:08 - column one and this is basically be my
125:10 - column two okay and then finally we will
125:13 - go ahead and write submit button submit
125:17 - will be St
125:19 - dot button and this will basically be my
125:23 - generate okay so I'm going to basically
125:27 - generate this entirely uh generate is
125:30 - just like a button which will basically
125:31 - Click by taking all this particular
125:33 - information so from here I'll be getting
125:35 - my input from here I'll be getting
125:38 - number of words from here I'll be
125:39 - getting my blog style okay all this
125:41 - three
125:41 - information now this will actually help
125:45 - me to get the final response here okay
125:48 - so I will say say if submit okay if
125:51 - submit I have to call one function right
125:54 - and what will be that specific function
125:56 - that function will return me some output
125:59 - okay and that output will be displayed
126:01 - over here now that function I really
126:03 - need to create it over here itself let's
126:04 - say I will say
126:06 - get
126:07 - llama response okay so this is basically
126:10 - my function and this I will create in my
126:14 - definition and what all parameters I
126:16 - specifically require over here right
126:18 - this three parameters right and uh if I
126:20 - probably call this function over here
126:23 - what are the parameters that I'm going
126:24 - to write over here is all these three
126:26 - parameters so first parameter is
126:28 - specifically my text input input
126:31 - text the second parameter that I'm
126:34 - actually going to give over here is
126:35 - number of
126:37 - words the third parameter that I really
126:40 - want to give is my block style so like
126:41 - what block style I really want okay so
126:44 - all this three information is over here
126:46 - so this will basically be my input text
126:49 - okay uh I'll write the same name no
126:52 - worries number of
126:54 - words and third parameter is basically
126:57 - my block style so all these materials
127:00 - will be given in the description if
127:01 - you're liking this video please make
127:03 - sure that you hit subscribe press the
127:04 - Bell notification icon hit like again
127:07 - just to motivate me okay if you motivate
127:09 - me a lot I will create multiple contents
127:11 - amazing content for you okay now here is
127:14 - what I will be calling my llama model
127:18 - right l Lama model Lama 2 model which I
127:21 - have actually downloaded in my local and
127:23 - for that only I will be specifically
127:25 - using this C
127:28 - Transformers right now if I probably go
127:31 - ahead and search in Lang chain Lang
127:34 - chain see whenever you have any problems
127:37 - related to anything as such
127:41 - right C Transformer C Transformer go and
127:45 - search in the documentation everything
127:46 - will be given to you so C Transformer
127:49 - what exactly it is it is it is over here
127:51 - it is given over here or not here let's
127:54 - see the documentation
127:56 - perfect so here you can see C
127:58 - Transformers the C trans Library
128:00 - provides python binding for ggm models
128:03 - so gml models the blog gml models
128:06 - whichever model is basically created you
128:07 - can directly call it from here let's say
128:09 - in the next class I want to call mistal
128:11 - so I can go ahead and write my model
128:13 - name over here as mist and it'll be able
128:14 - to call directly from the hugging phas
128:16 - okay um not only hugging face but at
128:19 - least in the local uh if you have the
128:21 - local if you want to call it from the
128:22 - hugging face then you have to probably
128:24 - use the hugging face API key but right
128:26 - now I don't want to use all those things
128:28 - so I want to make it quite simple so CC
128:30 - Transformers and here I'm going to
128:32 - basically write my
128:33 - model model is equal to and this should
128:36 - be my model path right which model path
128:39 - this one model slash this one right so
128:42 - here you can probably see this specific
128:44 - name V3 Q8 Z bin okay so I'm going to to
128:48 - probably copy this entire
128:50 - path and paste it over here okay so this
128:53 - will basically be my model and inside
128:56 - this what kind of model type I want
128:58 - there is also a parameter which is
129:00 - basically called as model type and in
129:02 - model type I'm going to basically say it
129:04 - is my llama model okay and you can also
129:08 - provide some config parameter if you
129:10 - want otherwise it will take the be
129:13 - default one so I'll say Max newcore
129:17 - tokens
129:18 - is equal to
129:22 - 256 and then the next one will basically
129:25 - be my
129:28 - temperature
129:30 - colon 0.01 let me keep the temperature
129:33 - point less only so I want to see
129:35 - different different answers okay so this
129:37 - is done uh this is my llm model that I'm
129:39 - basically going to call from here and it
129:41 - is going to load it okay now after my
129:43 - llm model is created I will go ahead and
129:46 - write my prompt template because I've
129:48 - Tak taken three three different
129:49 - information so template here I will go
129:52 - ahead and create this will be in three
129:55 - codes if you want to write it down
129:56 - because it is a multi-line statement and
129:58 - I will say write a
130:01 - blog write a blog for which style right
130:07 - blog style for whom for this specific
130:09 - blog style for researchers for freshers
130:12 - for anyone you can write right or I'll
130:15 - say job profiles I can for researcher
130:17 - job profile for fresher job profile for
130:19 - normal people job profile right so
130:21 - something like this job profile for a
130:25 - topic which topic I'm going to basically
130:28 - say this will be my number of words
130:30 - sorry not number of words this will be
130:32 - my
130:33 - input text so this is how we basically
130:36 - write
130:37 - prompts
130:39 - within how many words the number of
130:42 - words okay this many number of words I'm
130:45 - going to basically write this okay so
130:47 - this actually becomes my prompt template
130:50 - entirely okay this is my entire prompt
130:52 - template write a blog for so and so for
130:56 - block style this this this to make it
130:58 - look better what I will do I'll just
131:01 - press tab so that it'll look over here
131:05 - okay so this is my template that I'm
131:07 - probably going ahead with I've given the
131:09 - three information blog style input text
131:12 - number of words everything is given over
131:13 - here now finally I need to probably
131:15 - create the prompt template okay so for
131:18 - creating the prompt template I'm going
131:20 - to use prompt is equal to prompt
131:22 - template and here I'm going to basically
131:25 - give my input variables so input
131:29 - uncore variables and inside this I'm
131:33 - going to basically write first
131:34 - information that I want what kind of
131:36 - inputs I specifically want right uh
131:39 - whether I want um this block style so
131:41 - for block style I can just write style
131:44 - second one I can probably
131:46 - say text third one I can basically
131:50 - say ncore word so this will basically be
131:53 - my three information that I'm going to
131:55 - provide it when I'm giving in my prompt
131:58 - template okay and finally uh this is my
132:01 - input variable this next parameter that
132:03 - I can also go ahead with I can provide
132:04 - my template itself what template I want
132:06 - to give so this will be my template over
132:10 - here now finally we will generate the
132:14 - response from the Lama model OKAY Lama 2
132:19 - model which is from gml okay so here
132:23 - what I'm actually going to do I'm going
132:25 - to basically write llm and whatever
132:26 - things we have learned in Lang chain
132:28 - till now prompt
132:30 - dot prompt.
132:32 - format and here I'm going to basically
132:35 - use email sorry email what are the
132:38 - information that I really want to give
132:39 - over here prompt. format so the first
132:41 - thing is with respect to style the style
132:44 - will be given as block style so I'm
132:48 - going to basically write blog undor
132:51 - style okay the next information that I'm
132:54 - probably going to give is my input text
132:56 - input text is equal to not input text
133:00 - text is equal to input text I have to
133:01 - give text is equal to input undor text
133:05 - and the third parameter that I'm going
133:07 - to give is my ncore words which will
133:10 - basically be by number of words done so
133:15 - this is what I'm specifically giving
133:17 - with respect to my prompt uh and what
133:19 - llm will do it will try to give you the
133:21 - response for this and then we will go
133:25 - ahead and print this response and we
133:28 - will return this response
133:30 - also okay
133:34 - response response okay and what we'll do
133:37 - we will go ahead and return this
133:41 - response so step by step everything is
133:43 - done now I'm going to call this get Lama
133:47 - response over here here already is done
133:49 - now let's see if everything runs fine or
133:51 - not uh hope so at least one error will
133:54 - at least come let's
133:56 - see so I will delete this and let's go
134:00 - ahead and write over here to run the
134:01 - streamlet app all you have to do is just
134:04 - just write streamlet Run
134:07 - app.py Okay so once I probably execute
134:11 - this you'll be able to see this is what
134:14 - is my model but still I'm getting a
134:17 - model streamlet has attribute no head
134:20 - okay so let's see where I have
134:22 - specifically done the mistake because I
134:25 - think it should not be head it should be
134:27 - header okay I could see the error header
134:31 - okay fine no worries let's run it
134:37 - baby let's run this again stream L run
134:40 - app.py no I think it should
134:43 - run this looks good uh enter the block
134:46 - toping number of words researchers
134:48 - writing the researcher blog data
134:49 - scientist common people so let's go
134:51 - ahead and write about
134:53 - large language model so 300 words so
134:57 - number of words I will go ahead and
134:59 - write 300 I want to basically write it
135:02 - for common people and we will go ahead
135:04 - and generate it now see as soon as we
135:06 - click on generate it is going to take
135:07 - some time the reason it is probably
135:10 - going to take some time because uh we
135:12 - are using this particular in my local
135:15 - CPU but we got an error let's see key
135:17 - error block style it seems so I will go
135:20 - to my
135:21 - code block style block style block style
135:25 - so one minor mistake that I have
135:26 - specifically done over here so what I
135:28 - will do is that I'll give the same key
135:30 - name so that it does not give us any
135:33 - issue okay so this will be my input text
135:36 - and number of words the thing is that
135:39 - whatever things I give in that prompt
135:40 - template the input variables should be
135:42 - of that same name okay so that is a
135:43 - mistake I had done it's okay no worries
135:46 - so let's go ahead and and execute it now
135:49 - everything looks fine have assigned the
135:51 - same value over there number of words
135:53 - number of words so here also I'll go
135:56 - ahead and write number of words block
135:58 - style input
136:01 - text and this also should be block style
136:04 - the name I'm giving same right for both
136:07 - prom template and this okay so I think
136:09 - now it should work let's see so go ahead
136:14 - and write this and now my page is open
136:17 - opened now I'll go ahead and write large
136:22 - language models and it will probably
136:24 - create
136:25 - my words so this will be 300 I want to
136:29 - create it for common people let's
136:32 - generate it as I said that the output
136:35 - that I'm probably going to get is going
136:36 - to take some time because I'm running
136:38 - this in local CPU um let's say if you
136:41 - deploy this in the cloud uh with respect
136:43 - to if there are GPU features then you
136:45 - will get the response very much quickly
136:47 - so so just let's wait uh till then uh we
136:50 - get the output hardly but I think it is
136:52 - 5 to 10 seconds Max and since I've told
136:55 - 300 wordss it is again going to take
136:56 - time so let's see the output once it
136:58 - comes so guys it hardly tookes 15
137:01 - seconds to display the result so here
137:03 - you can see that large language models
137:04 - have become increasingly popular in
137:06 - recent year due to the in due to the
137:08 - ability to process and generate
137:09 - humanlike languages it looks like a good
137:11 - blog you can also create any number of
137:13 - words blog itself Now understand that I
137:16 - have a good amount of ram my CPU has lot
137:18 - of cores so I was able to get it in 15
137:20 - seconds for some of the people it may
137:22 - take 20 seconds it may take 30 seconds
137:25 - now you may be asking Kish how can you
137:26 - specifically reduce this time is very
137:29 - much simple guys we will probably do the
137:31 - deployment in AWS or any other Cloud
137:34 - Server itself which I will be probably
137:35 - showing you in the upcoming videos and
137:37 - there you'll be able to see that how
137:39 - with the help of gpus the inferencing
137:40 - also becomes very much easy not only
137:43 - that we'll also see how we can probably
137:45 - fine-tune all this data set with the OWN
137:47 - custom data itself guys yet another
137:50 - amazing llm project for you all now this
137:53 - llm project will be quite amazing
137:55 - because from this like this is just like
137:56 - a base you can probably create any kind
137:59 - of app on top of it you can create text
138:01 - summarizer you can create a quiz app or
138:03 - you can create any other app itself that
138:05 - is probably something related to text
138:07 - right so what is the main aim of this
138:10 - particular project is that from this
138:12 - project you will get all the guidance
138:14 - that is probably required to create that
138:16 - production grade application whenever
138:18 - you specifically work in the companies
138:21 - why because we are going to also include
138:23 - Vector search database and this is where
138:25 - you'll understand the power of the
138:27 - vector search
138:28 - database whenever you work in any NLP
138:31 - project something that is related to
138:33 - text you try to convert those text into
138:35 - embeddings or vectors if you have a huge
138:38 - vectors you you cannot just store it in
138:40 - your local machine you probably are
138:42 - requiring some kind of database and
138:44 - specifically with respect to vectors or
138:46 - embeddings V Vector DB is very super
138:49 - beneficial why because you can probably
138:51 - apply some of the important algorithms
138:53 - like similarity search or you can also
138:56 - uh apply any other algorithms that is
138:58 - related to text it can be text
139:00 - classification very much quickly by just
139:02 - squaring it from the vector database and
139:04 - getting the right kind of output so all
139:06 - these things we are basically going to
139:08 - cover it will probably be a longer video
139:10 - because every step by step I'll probably
139:12 - show you I will also write the code in
139:14 - code in front of you wherever any
139:16 - documentation is probably required I
139:18 - will also show you all those things so
139:21 - yes without wasting any time let's go
139:23 - ahead and probably see this project my
139:25 - main aim is basically to teach you in
139:27 - such a way that you get the right kind
139:29 - of knowledge and this you apply it in
139:31 - your company and nowadays many companies
139:33 - are specifically asking interviews
139:35 - regarding Vector DB they asking related
139:37 - to open a llm models and many more so
139:40 - let me go ahead and share my screen and
139:42 - as I said we will be doing completely
139:45 - from Basics right so here is my vs code
139:48 - I have a document over here budget
139:50 - speech. PF I am probably going to take
139:52 - this particular document upload it in my
139:55 - Vector DB right and then ask any kind of
139:58 - queries from it convert that into a quiz
140:01 - app right let's say this is a general
140:03 - knowledge book I can probably convert
140:06 - this into a quiz app with four options
140:08 - and get the right kind of answer from it
140:10 - right so this is what I'm planning to do
140:12 - other than this any idea you have you
140:14 - can probably do it on top of it right so
140:17 - first first thing first what is the
140:18 - first thing that we really need to do
140:20 - over here is that create a environment
140:22 - right and this is in every project I at
140:26 - least make you do this because it is
140:28 - super beneficial because at the end of
140:30 - the day with respect to every project
140:32 - you need to probably create a separate
140:34 - environment so in order to create an
140:36 - environment I will go ahead and write
140:37 - cond
140:38 - create cond create minus p v andv will
140:44 - be my uh environment name and then here
140:47 - I'll basically going to use Python 3.10
140:50 - right so once I execute it it'll ask me
140:52 - for an option whether I need to install
140:54 - or not I will just say why and go ahead
140:57 - with the installation so this is the
140:58 - first step that we should specifically
141:00 - do right and uh it is important step
141:03 - because at the end of the day you should
141:05 - definitely create a different
141:06 - environment don't always work in the
141:08 - same environment whenever you are
141:10 - actually working in this kind of
141:11 - projects the second thing that I'm
141:13 - probably going to do is that create my
141:15 - requirement. txt
141:17 - requirement. txt the reason because
141:22 - whenever I'm using an lmm model or
141:23 - anything as such I have to probably
141:26 - install a lot of packages so over here
141:29 - first of all I will go ahead and cond
141:30 - activate activate this specific
141:32 - environment V NV slash okay so this is
141:37 - done the environment is activated and we
141:41 - are ready to go right now from my
141:43 - requirement. txt what are things I'm
141:45 - basically going to use I'm going to to
141:47 - probably note down all the requirements
141:49 - over here the libraries that is
141:51 - unstructured Tik toen pine cone client P
141:54 - PDF open aai Lang chain pandas numai
141:58 - python. EnV and at the end of the day
142:01 - guys uh I would always suggest you to
142:03 - please understand about Lang chain Lang
142:05 - chain is an amazing Library it has lot
142:07 - of power lot of functionalities which
142:09 - you can specifically do the community is
142:11 - huge and many many companies are
142:13 - specifically using it okay so
142:15 - requirement. txt has been saved so I
142:17 - will quickly go ahead and install this
142:21 - requirement. txt so probably it may take
142:23 - some time and before that uh since it is
142:26 - probably installing what I will do I
142:28 - will also go ahead and create my EnV
142:31 - file right so in this ENB file what I'm
142:34 - actually going to do I'm going to put my
142:37 - open API key so quickly I'll create my
142:40 - open AP API key save it and start using
142:44 - it okay so this will basically be my
142:46 - open API key in the EnV so that I can
142:48 - basically load it so along with this
142:50 - python. EnV is also there so let's wait
142:53 - till the all the libraries has been
142:56 - installed okay so this is the initial
142:58 - steps that we should specifically do
143:00 - right our environment is ready we have
143:02 - installed all the libraries that is
143:03 - probably required we have also kept our
143:06 - open AI key because at the end of the
143:08 - day I'm specifically going to use Lang
143:09 - chin you can also do it with hugging
143:10 - face if you want but I will try it with
143:13 - open AI because the accuracy is pretty
143:15 - much better in this okay
143:17 - now the next step uh what I'm quickly
143:20 - going to do is that quickly create one
143:25 - file and here I will just show you test
143:29 - ipynb and this will basically be my
143:33 - ipynb file itself and here I'll be
143:35 - showing you the entire code later on you
143:37 - can probably convert this into an end
143:39 - to-end project you can probably create a
143:41 - streamlet app but here is the main thing
143:43 - that I'm probably going to show you by
143:45 - executing step by step and and what all
143:47 - things are specifically required to
143:48 - create this app again understand what I
143:51 - am planning to do right so I will just
143:54 - show you over here first of all let me
143:56 - just clean this screen and what is my
143:59 - entire agenda like what I really want to
144:01 - create as an llm application over here
144:04 - so I have a PDF okay I have a PDF so
144:08 - this is basically my PDF you can also
144:10 - say this is a data source it can be a GK
144:13 - book it can be a maths book it can be
144:15 - anything right I will will first of all
144:18 - load this document right once I load
144:22 - this document or read this
144:24 - document what I am going to do is that
144:26 - I'm going to convert this into chunks
144:29 - right because we cannot open AI hugging
144:31 - face models they have some restriction
144:33 - with respect to the Token size so I'm
144:36 - just probably going to create chunks and
144:38 - this is what we say it as text
144:42 - chunks right after this I'm going to use
144:45 - open AI embedding
144:48 - okay and this embeddings will be
144:51 - responsible in converting all this text
144:53 - CHS into
144:55 - vectors right so this will basically be
144:58 - my vectors I hope you know what exactly
145:00 - is vectors a numerical format for
145:02 - different different text right so this
145:05 - will specifically be my vectors and this
145:08 - I'm going to basically do with open
145:10 - embeddings further this vectors needs to
145:12 - be stored in some Vector search DB so
145:15 - here I will put all these vectors in
145:18 - some kind of vector surge DB now why
145:21 - this DB is required because at the end
145:23 - of the day whenever a human being
145:25 - queries any
145:26 - inputs
145:28 - right because of this Vector search DB
145:31 - here we can apply similarity
145:35 - search right and probably get any kind
145:39 - of info that I specifically want so this
145:41 - is my what my entire architecture of the
145:43 - specific project looks like right and
145:46 - here this Vector search DB I'm probably
145:48 - going to use something called as pine
145:50 - cone and they have lot of DBS I will
145:52 - talk about the advantage and
145:53 - disadvantage there is some amazing DBS
145:55 - called as data Stacks where they
145:57 - specifically use cassendra DB Pine clone
146:00 - is one right we'll see all the
146:02 - documentation page with respect to this
146:04 - okay so step by step I'm going to
146:06 - basically do this and you can actually
146:07 - do it for any number of pages one more
146:10 - thing that I'm probably going to install
146:12 - is IPI kernel since I'm going to work in
146:15 - my Jupiter notebook okay
146:17 - so this all steps are basically
146:18 - happening it is very much good and we
146:21 - are able to see this okay so let this
146:24 - installation happen and then I will set
146:26 - up my kernel okay so these are the
146:29 - initial steps that we really need to
146:31 - focus on and uh understand this project
146:34 - will be the base to create any kind of
146:36 - chatbot application mcqs quiz apps okay
146:40 - question answering chat Bots not only
146:43 - question answering chat Bots text
146:44 - summarizer anything that you probably
146:46 - want
146:47 - right or you can also basically say it
146:50 - as a chat B that gives you specific
146:51 - answer with respect to specific domain
146:53 - right with respect to the data that we
146:55 - have so all these things are done now
146:57 - I'm going to probably select the kernel
147:00 - V EnV python 3.0 right I'm going to save
147:03 - this perfect now the first step as usual
147:06 - I will go ahead and import start
147:09 - importing libraries and now we will do
147:11 - it completely step by step okay so what
147:15 - all things we basically require I'm
147:17 - going toire open a I'm going to import
147:21 - Lang
147:22 - chain uh apart from open and Lang chain
147:26 - what I'm going to also going to do go
147:27 - ahead with pine con I will talk about
147:30 - Pine con more when I probably show you
147:31 - the documentation apart from Pine con
147:34 - I'm going to basically go to Lang chain
147:37 - okay and I'm going to basically use
147:39 - something called as document
147:43 - loader document loaders will basically
147:45 - be responsible for for loading any kind
147:47 - of documents it can be a PDF file and
147:49 - all so for PDF we specifically use
147:52 - something called as P PDF I can also use
147:55 - directly Pi PDF loader but since my PDF
147:57 - is inside the directory I'm going to use
147:59 - Pi PDF directory loader okay so this is
148:02 - the next thing now from the next thing
148:05 - what we need to do is that as soon as we
148:06 - load any PDF we will get all the
148:08 - documents we have to basically do uh
148:11 - text splitting right because we really
148:13 - need to convert those into chunks we
148:15 - cannot take the entire token right there
148:17 - will be a restricted token size right
148:20 - like uh recently open has come up with
148:22 - the open 4. 4. o turbo right so I think
148:26 - it is GPD sorry GPD 4.0 turbo there 188
148:31 - 128k is the token size right so for that
148:34 - I'm going to basically use from Lang
148:36 - chain dot text spitter I'm going to
148:40 - probably import recursive character text
148:43 - spitter you can also use any other based
148:46 - on this right my always suggestion would
148:48 - be that go ahead and check out the
148:49 - documentation with respect to all the
148:51 - libraries that I'm probably uh uploading
148:54 - right now the next thing is that
148:55 - whenever I probably convert this into
148:58 - chunks right the next thing that I need
149:00 - to probably convert that into vectors
149:01 - and for that I'll be using some kind of
149:03 - embedding techniques so over here we are
149:06 - going to basically use do
149:09 - embeddings
149:11 - doop so it is embeddings do openai and
149:16 - we are going to import open AI embedding
149:18 - so open AI embeddings is a technique
149:20 - wherein it will probably convert any
149:22 - chunk into vectors right so this is the
149:25 - next step now the next step is basically
149:28 - also to uh import a library that will be
149:31 - responsible in creating a vector DB with
149:34 - respect to Pine call or we also say it
149:36 - as Vector store so here I'm going to
149:38 - basically use from Lang
149:41 - chain
149:43 - dot
149:45 - vector
149:48 - dot I think Lang chain dot it should be
149:51 - dot I'm writing comma I don't know why
149:54 - Vector oh spelling is mistaken do Vector
149:58 - stores okay and here I'm basically going
150:01 - to
150:02 - import I think it will be there pine
150:05 - cone right so I'm basically going to
150:07 - also use this pine cone pine cone will
150:09 - be our Vector store uh later on we can
150:12 - integrate this with our Vector DB that
150:14 - is present in Pines conone so
150:17 - the next thing is that I will also
150:18 - import our llm model because we will be
150:21 - requiring llm
150:23 - import open AI right so all these
150:27 - libraries I'm going to specifically use
150:29 - it I will quickly go ahead and execute
150:31 - it let's see if everything works fine
150:33 - you may get some kind of warning but
150:34 - it's okay right so this is your
150:38 - initial load that you are specifically
150:41 - doing now you know that I have an
150:42 - environment variables that is with
150:44 - respect to hugging pH so what I'm
150:45 - actually going to to do I'll go ahead
150:48 - and write from EnV
150:51 - import import load. EnV so this will
150:56 - specifically load all your environment
150:59 - variables okay so whatever environment
151:01 - variables that you have with respect to
151:02 - open API key or anything that is
151:04 - required you can basically do this right
151:07 - now I will also be importing OS over
151:09 - here right OS we can specifically use
151:11 - later on okay quickly now the first step
151:15 - as I said
151:17 - we need we have a PDF file we need to
151:20 - read it right so now I will write let's
151:24 - read the document okay now first step
151:29 - while reading the document I'll create a
151:30 - function so that I can reuse it and I'll
151:33 - write read Doc and here I will give my
151:35 - directory I can also give my file for
151:38 - that what library will be used Pi PDF
151:41 - loader right over here I'm using
151:43 - directory loader since I have to
151:44 - probably give my directory name and then
151:47 - I will basically write file _ loader and
151:50 - I will initialize my P PDF directory
151:52 - loader and basically give my directory
151:55 - path over here okay so directory path
151:59 - over here right so as soon as I give my
152:01 - directory path it will go to this
152:03 - specific directory and it will see
152:04 - whichever PDF is there it will start
152:06 - loading it okay and then I will go ahead
152:09 - and write file uncore loader dot load
152:14 - right now see why I'm showing you this
152:16 - step by step because everybody should
152:18 - understand what steps we are be doing it
152:20 - later on to convert this into a modular
152:22 - code it will be very much easy that is
152:24 - the reason I'm writing it in the form of
152:25 - functions all these functions will go
152:27 - into your utils.py file okay and finally
152:31 - you can see over here I'll get file
152:33 - loader _ load that basically means it is
152:35 - going to load all the documents and here
152:37 - I will basically be getting my documents
152:39 - right and finally we will return this
152:43 - documents done right now let's check
152:46 - check if everything is working fine so
152:48 - here I'm going to basically write read
152:51 - uncore Doc and this will basically be
152:53 - returning my document and here I will
152:57 - give my document folder so let me just
152:59 - go ahead and write
153:01 - documents in string right so this will
153:04 - basically be my directory
153:06 - path okay and now if I execute what is
153:09 - this doc it will probably read all the
153:12 - docs that are probably there now see
153:14 - every page by Page content this this is
153:16 - my first page second page third page
153:18 - fourth page fifth page like this I have
153:20 - 54 pages in my PDF right 54 Pages now if
153:24 - you also write length of Doc here also
153:26 - you'll be able to see it right so length
153:30 - of Doc I'm going to get 58 so that
153:32 - basically means we have done this first
153:34 - step right we have loaded we have read
153:37 - this particular PDF right now the next
153:39 - step is basically with respect to
153:42 - dividing these documents into text
153:43 - chunks okay so this is what we are
153:45 - probably going to do in our step two but
153:48 - till here everything is working
153:50 - fine so guys now we have finished
153:52 - reading the document uh now what we are
153:54 - basically going to do is that we are
153:56 - going to convert this into chunks right
153:59 - now for converting this into chunks what
154:02 - we are specifically going to do let's
154:04 - see so here I'm going to write the code
154:06 - here I'm going to basically say divide
154:08 - the
154:08 - docs into chunks and again because of
154:12 - the model restriction of the token size
154:14 - we really need to do this okay so here
154:16 - I'm going to basically use defin I'll
154:18 - create a function which is called as
154:20 - definition chunk data here first thing I
154:24 - will basically give my docs then I'm
154:27 - going to basically mention my chunk size
154:30 - right so let me go ahead and mention my
154:32 - chunk size my chunk size I'm going to
154:34 - mention it as 800 you can also mention
154:36 - it as 1,000 right don't keep a very huge
154:40 - value and then I can also say what about
154:43 - the chunk overlap Okay so so the chunk
154:46 - overlap like 50 characters can overlap
154:49 - with from one sentence to the other
154:50 - sentence right so here the next thing
154:53 - I'm going to basically create a text
154:55 - splitter and that is where we going to
154:57 - basically use this recursive character
155:00 - text splitter so first first thing first
155:02 - I going to mention my chunk size which
155:04 - will basically be my chunk size itself
155:08 - and along with this
155:10 - my chunk
155:12 - overlap which will be nothing but the
155:15 - chunk overlap that I have basically
155:17 - mentioned great uh so here I get my text
155:21 - splitter and now I'm going to basically
155:23 - up take this text splitter and split all
155:26 - the documents based on the kind of
155:29 - splitting that I've actually mentioned
155:31 - so here basically I'll provide my docs
155:33 - as my parameter and then I will convert
155:36 - this into and I'll return this docs
155:39 - perfect if you want to know more about
155:41 - this chunk uh recursive chunk splitter
155:43 - uh sorry recursive character splitter
155:46 - you can probably check this out
155:49 - documentation also I'm going to specify
155:51 - over here uh this documentation is good
155:53 - to understand what exactly it does and
155:55 - all right so perfect this is my chunk
155:59 - data function now what I'm actually
156:01 - going to do is that quickly use chunk
156:04 - uncore data and try it on my docs file
156:09 - right so here I'm going to basically
156:10 - mention my docs docs is this I've
156:14 - actually got this docs is equal to
156:18 - Doc okay and let's see so this will
156:22 - basically be my documents it is just
156:24 - going to apply all this right it is
156:26 - going to convert that entire document
156:27 - into a chunk size of 800 with the
156:29 - overlap of 50 okay and probably if I go
156:32 - and see my documents you'll be able to
156:34 - see it now see every document has now
156:37 - been properly mentioned right and the
156:40 - document that we are specifically
156:41 - reading is the Indian budget document
156:44 - right so any question that I proba asked
156:46 - related to Indian budget I'll be able to
156:48 - get the answer done this is good if I
156:52 - probably want to see the length of the
156:53 - documents also I can also see it okay
156:56 - just to give it get an idea like 58 is
156:58 - the length okay great so this is done uh
157:03 - now the next step what I'm actually
157:04 - going to do is that I'm going to also
157:06 - initialize my embedding technique so
157:09 - embedding technique of open AI right so
157:14 - we are going to initialize this so here
157:16 - I'm going to mention embeddings is equal
157:18 - to open Ai embeddings and here I'm going
157:22 - to use my API
157:24 - key OS do environment I can directly use
157:29 - os. environment and I can mention what
157:31 - is my API key so here I can say
157:35 - opencore open AI uncore API
157:41 - underscore key right so this is what is
157:44 - my embedding so let let me just
157:47 - quickly see what exactly is my
157:49 - embeddings so here is my embeddings that
157:51 - I'm going to probably use and this is
157:54 - what is basically used to convert that
157:56 - text into vectors right so quickly we
157:59 - have done this uh then I'm going to
158:01 - probably create my vectors let's say
158:03 - Let's test any vectors with this
158:05 - embeddings okay uh and there are various
158:07 - other embedding techniques like one h
158:09 - word to uh average word to and all but
158:12 - uh in open AI embeddings this provides
158:14 - you a much more advanced technique okay
158:17 - and over here also it will provide you
158:18 - some kind of vectors like there will be
158:20 - some Vector size also with respect to
158:22 - this okay like every every sentence will
158:25 - be provided with with respect to a
158:27 - vector size so here if I want to
158:29 - probably check and write embed
158:31 - underscore query and just test it with
158:34 - respect to anything like how are you
158:37 - right and let's see what kind of vectors
158:40 - we will probably get so this is my
158:42 - vectors that I'm actually getting see
158:44 - the entire text and if I want to
158:45 - probably check the length of these
158:47 - vectors it will also give you the length
158:49 - of this vectors okay so it is some
158:53 - something around 15 36 and this length
158:55 - will be super important because at the
158:56 - end of the day where I probably create
158:58 - my Vector database I have to specify
159:00 - this length okay for my problem
159:02 - statement now great now let's create our
159:06 - Vector search DB and pine code okay and
159:11 - now this step is very much important uh
159:13 - because after this particular step we
159:15 - will be able to see what kind of vector
159:17 - database we probably get okay Vector
159:19 - search DB in Pine con so let's go ahead
159:22 - and let's quickly create this Vector DB
159:24 - okay so guys here is the pine cone
159:27 - documentation you can probably check it
159:28 - out uh get started using pine con
159:30 - explore our examples this this is there
159:33 - what is the main important information
159:35 - about this Pine C is that it definitely
159:38 - helps you in semantic search in chat
159:40 - Bots also it helps you right where it
159:42 - probably helps you to store the entire
159:44 - Vector right and it provides you
159:46 - generative QA with open integration Lang
159:49 - CH leral
159:50 - argumentation uh rag also we basically
159:53 - say open integration and it has multiple
159:56 - uses okay so if I probably show you one
159:58 - of the document or guide right so if I
160:01 - probably go ahead and click on the guide
160:02 - right and this is where we really need
160:04 - to create the vector database I'll show
160:06 - you the steps of creating it right so
160:09 - Pine con makes it easy to provide
160:10 - long-term memory for high performance AI
160:12 - application it is managed Cloud native
160:14 - Vector database with a simple API no
160:16 - infrastructure has less pine cone serves
160:19 - fresh filtered query results with low
160:20 - hency at a scale of billions of vectors
160:23 - so if you have a huge data set you want
160:25 - to probably work with the vectors you
160:27 - can probably store it over here in the
160:28 - form of vector database um what is the
160:31 - importance Vector embedding provides
160:32 - long-term memory for AI Vector database
160:35 - stores and queries embedding quickly at
160:36 - a scale you know so anytime it is
160:38 - probably saved if you're quering it you
160:40 - will be able to get the response very
160:41 - much quickly now first thing first how
160:44 - you have to probably create this okay so
160:46 - if you once you log in once you log in
160:48 - over here or sign up you'll be able to
160:51 - see this okay so and here I've already
160:52 - created one index okay but this index
160:55 - will not work because see uh in the free
160:57 - tire right it allows you to just create
160:59 - one index so I will just delete this and
161:01 - show you how to probably create it
161:03 - completely from scratch so this is the
161:06 - name so I'm I'm going to delete this
161:08 - index because at the end of the day
161:10 - whatever vectors you are probably
161:11 - storing it will start indexing it okay
161:13 - so this is terminated now we will go
161:15 - ahead and create a new index so for
161:17 - creating a new index uh what I have to
161:19 - probably do is give a name so let's say
161:21 - I'm giving langin Vector now this is
161:23 - super important configure your index the
161:25 - dimensional metrix depends on a Model
161:26 - you select right now based on a Model if
161:29 - I probably uh see to it right so here
161:32 - you'll be able to see what is the length
161:34 - that I was able to get from my embedding
161:36 - 1536 so this is what is the dimension
161:39 - that I'm also going to give it over here
161:41 - and since I have basically doing cosine
161:43 - similarity kind of Stu or you can also
161:45 - do Dot product ukan but I'll stay to
161:47 - cosine similarity because at the end of
161:49 - the day the similarity search that is
161:50 - probably going to happen is with the
161:52 - help of cosine similarity and then
161:54 - finally we go ahead and create the index
161:56 - this is the main thing that you probably
161:58 - need to do this is basically getting
162:00 - terminated and this was the data that I
162:02 - had actually inserted but again we will
162:04 - do it okay so if I go to back to uh um
162:08 - back to probably a index let's see why
162:11 - it is not created or still it is showing
162:14 - terminating it should not not show
162:15 - terminating but at the end of the day
162:17 - because one I have already deleted it or
162:20 - I can just change the name if I want so
162:21 - but it is created over here okay Lang
162:23 - chin vector and it is a free Tire in the
162:26 - case of free tire they will provide you
162:28 - this thing right now from this there are
162:31 - some important information that you
162:32 - really need to retrieve one is the
162:34 - environment one is the database name
162:37 - okay so what sorry index name so I'll go
162:40 - back to my code here you'll be able to
162:43 - see Vector search DB and pine code let's
162:45 - let's initialize it I I've imported P
162:48 - cone I will say do in it okay so do in
162:51 - it basically does the initialization
162:53 - here two information are required API
162:56 - key okay API key with some information
163:00 - comma the next thing that I probably
163:02 - require is environment okay so
163:05 - environment something is required so
163:06 - let's retrieve this two information
163:09 - along with this what I can also do I
163:10 - have to also provide my index uncore
163:12 - name so here I will specifically say my
163:15 - IND Indore name index name I've already
163:17 - copied it from there so it is nothing
163:19 - but langin Vector let's go and see where
163:21 - is the API and environment so here I'm
163:24 - going to go back if I go and see there's
163:26 - an option of API keys so this is
163:28 - basically my API key I will copy it and
163:31 - I will paste it over here okay so Ive
163:34 - pasted my API key over here now the
163:36 - environment thing where will I get my
163:38 - environment so if I go to indexes and if
163:41 - I click this this is the environment
163:43 - that I'll be able to get it right so
163:45 - this two information is specifically
163:47 - required I will paste it over here right
163:50 - so this two information is done by
163:52 - executing it my Vector search DB will
163:54 - get initialized over there but at the
163:56 - end of the day I need to put all these
163:58 - embeddings specifically all these
164:00 - embeddings uh in my Vector DB right so
164:03 - over there I will again use pine cone
164:06 - pine cone which I have actually
164:07 - initialized and I'll say from
164:11 - documents and here I will give all my
164:14 - Docs so from documents I'm going to
164:16 - first of all give my doc parameter over
164:18 - here the dock which where which I need
164:20 - to probably store it in my Vector DB and
164:22 - then I will go ahead and write
164:23 - embeddings embeddings what kind of
164:25 - embeddings that I'm specifically giving
164:27 - is the same embeddings that we probably
164:29 - created and then I have my index name is
164:32 - equal to whatever index name I have
164:34 - basically initialized so as soon as I
164:36 - probably execute this you will be able
164:38 - to see that I will be able to get one
164:40 - index over here okay so let me just go
164:43 - ahead and execute it it'll probably take
164:45 - some amount of time because I have a
164:46 - huge data over there but you'll be able
164:49 - to see the changes once I probably go
164:51 - over here okay so if I go ahead and
164:54 - click it and if I refresh
164:58 - it let's see okay whether we'll be able
165:02 - to see everything or not whether the
165:05 - data part and all we will be able to see
165:07 - so here you can probably see query by
165:09 - vector and all all the data is there if
165:12 - I want to probably see the vector count
165:14 - it is 58 because the document size that
165:15 - you could probably see is 58 right and
165:19 - these are all the information you can
165:20 - see over here all the data has been
165:23 - basically stored and based on the
165:24 - metadata you can probably see it it has
165:26 - already done the indexing now any query
165:29 - that you probably hit it over here in
165:30 - the form of vectors you'll be able to
165:32 - get those specific response okay now to
165:35 - do the query part what I will do I will
165:37 - apply a cosine similarity so I will say
165:40 - cosine
165:42 - similarity retrieve results okay so I
165:45 - will try to retrieve the results over
165:47 - here so here let me just go ahead and
165:50 - write definition retrieve uncope
165:55 - query and here I will say query K is
165:58 - equal to 1 let's say k is equal to 2 I I
166:00 - will probably see the top two query now
166:03 - the second thing is that if I want to
166:05 - probably query I will get the matching
166:07 - results whatever matching results I'll
166:08 - use the same index dot whatever index is
166:11 - over here dot similarity
166:15 - let's see index
166:19 - dot
166:21 - similarity unor search right the
166:24 - similarity _ search what I can do is
166:28 - that let's see what function I've
166:31 - actually made for similarity _ search
166:35 - because I need to probably get those
166:36 - documents also right so basically uh I
166:41 - need to also create one more function
166:42 - over here right and that function will
166:44 - basically be my similarity underscore
166:46 - search that basically means what is my
166:49 - result that I'm probably going to get
166:51 - right inside this so index. similarity
166:54 - unders search is a function that is
166:57 - probably present inside this index
166:59 - itself right so similarity unders search
167:02 - and here I will probably give my query
167:05 - comma K is equal to some K value okay so
167:07 - what is the k k is equal to 2 whatever
167:08 - results I'm specifically getting and
167:11 - here I'm going to return my math
167:15 - matching results okay matching results
167:19 - so once I execute it so this is
167:21 - basically my retrieve query uh example
167:24 - so any query that I specifically give
167:27 - with respect to that particular PDF I'll
167:29 - be able to get the answer now this is
167:31 - fine this is the function to get the
167:34 - data from the database itself right so
167:36 - cosine similarity retrieve results from
167:38 - Vector DB I'll write it over here so
167:40 - that you will be able to understand okay
167:43 - so done this is the function that is
167:44 - basic Bally required now two important
167:46 - libraries that I'm going to probably use
167:48 - one is from langen Lang chin. change.
167:51 - question answering I'm using load
167:53 - question answering chain and along with
167:55 - this I'm also going to use open AI open
167:58 - AI will specifically be used to create
168:00 - my model and here my llm model will be
168:03 - created with the help of this so here I
168:04 - have written open a model name will be
168:06 - tax uh text Dy uh 003 and I've taken the
168:09 - temperature value as uh this one and
168:12 - then I'm also going to create my chain
168:14 - where I specific ially use this load QA
168:16 - chain and load Q chain actually helps
168:18 - you to create a question answer
168:20 - application and then here I will go
168:22 - ahead and write chain type is equal to
168:25 - stuff okay so my chain is ready my llm
168:28 - is ready everything is ready now all I
168:31 - need to do is that retrieve my queries
168:33 - right for retrieving the queries I will
168:36 - specifically be using this retrieve
168:37 - query function also so let's go ahead
168:40 - and write my definition I will go search
168:43 - answers from
168:46 - uh Vector database Vector DB okay and
168:50 - here I will basically
168:51 - write definition retrieve answers and
168:55 - whatever query I specifically give over
168:57 - here based on that I will should be able
168:59 - to get it right so if I probably see doc
169:02 - search then I will write doc search is
169:05 - equal to I'll call that same function
169:06 - retrieve query with my query that I'm
169:09 - actually going to give and then I will
169:11 - also print my doc search okay whatever
169:16 - print I'm basically getting now whenever
169:18 - I get that specific doc search I should
169:21 - also run this chain chain that I've
169:23 - actually created right to load QA chain
169:25 - so here I'm going to basically say
169:27 - chain. run and inside this what will
169:30 - basically be my input documents so I
169:32 - will say
169:37 - input input documents is equal to I will
169:41 - give my doc search right the document
169:44 - that I'm probably going to search and
169:45 - then the next will be my question which
169:47 - will be in the form of query right the
169:49 - query that I'm actually getting so once
169:51 - I do this my chain will basically be
169:53 - running and it will provide you some
169:54 - kind of response that it probably
169:57 - gets gets from the vector DB okay if it
170:01 - matches right and then we are going to
170:04 - return this specific
170:06 - response done now see the magic okay
170:10 - once I probably call this function what
170:11 - will happen so here I will write our
170:14 - query okay so the query will be I I saw
170:18 - something right I I basically so this
170:21 - will basically be my retrieve answer
170:23 - okay now see this I read the PDF I could
170:26 - find one question over there how much
170:28 - the agriculture Target will be increased
170:30 - by how many cores right how much the
170:33 - agriculture Target will be increased by
170:35 - how many cores I have just written this
170:36 - kind of statement now this retrieve
170:38 - query as soon as we call it will
170:40 - basically sorry retrieve answer as soon
170:42 - as we call okay so here I'll just make
170:44 - make this function change also the
170:46 - spelling should be right right
170:49 - retrieve and here also I'm going to
170:52 - probably make this to retrieve answers
170:55 - okay now as soon as I give my query over
170:57 - here it'll go to retrieve query it'll
170:59 - see from the index right it'll probably
171:02 - do the similarity search it'll give you
171:03 - the two results itself so let me just go
171:05 - ahead and see the answer over here now
171:09 - retrieve answer is not defined why okay
171:11 - I have to execute this sorry now it
171:13 - should be able to get the answer see so
171:17 - agriculture credit Target will be
171:18 - increased to 20 lakh CR with an
171:20 - investment of so and so information I
171:22 - I've just put right
171:26 - uh I can also write any other question
171:29 - how is the agriculture
171:32 - doing
171:34 - right so I may get some kind of question
171:37 - answer if it is not able to get
171:38 - something then it will say I don't know
171:40 - okay the government is promoting
171:41 - corporate based this this this this I'm
171:43 - getting that information from the entire
171:46 - PDF itself right so this is how
171:48 - beautifully you can probably get this
171:49 - and now it is probably asking being
171:51 - acting as a question answer application
171:54 - right now see this is what is the base
171:58 - you have a vector DB you're asking any
172:00 - question and you're getting some kind of
172:02 - response now on top of that you can also
172:04 - do a lot of prompt templating you can
172:06 - probably convert this into a quiz app
172:08 - you can convert it into a question
172:10 - answer uh chatbot you can convert this
172:12 - into a text summarizer you can do what
172:14 - whatever things you want right and this
172:17 - is what is the specific power over here
172:19 - right and this is really really good and
172:22 - that is what I'm probably going to show
172:23 - you in the next video on top of it like
172:26 - how can I probably do a custom prompt
172:29 - templating on my llm application this is
172:32 - what I'm probably going to show you in
172:33 - the next video but here i' shown you
172:36 - about what is Vector database and why it
172:39 - is very much important what exactly how
172:42 - the vectors are basically stored and
172:43 - here you can probably see see this is
172:45 - how your vector is stored if I probably
172:47 - search for any Vector over here I'll be
172:49 - able to get those kind of response over
172:50 - here right based on that uh search
172:53 - itself so start using this many
172:55 - companies are basically using this at
172:56 - the end of the day just for your
172:58 - practice sake it is completely for free
173:00 - right but if you have if you want more
173:01 - than one indexes two indexes then at
173:03 - that point of time you can probably take
173:05 - a paid tool paid version that
173:07 - specifically requires in a company
173:08 - itself so I hope you are able to
173:11 - understand this Amazing
173:12 - Project hello all my name is rush nyak
173:15 - and welcome to my YouTube channel so
173:17 - guys just a few days to end this
173:19 - specific year this month was amazing
173:23 - because I was able to upload many many
173:25 - videos related to generative AI many
173:28 - people had actually requested it uh the
173:30 - reason is very simple because all the
173:32 - students who have made successful career
173:34 - transition into the data science
173:36 - Industry they're working in different
173:38 - different llm projects so I hope you're
173:40 - liking all those videos that I'm
173:42 - specifically uploading um
173:45 - again as requested if you like this
173:47 - particular videos and all please do make
173:49 - sure that you subscribe the channel
173:50 - press the Bell notification icon hit
173:52 - like this will motivate me to upload
173:55 - more videos as we go ahead so it is a
173:58 - sincere request please do that and share
174:00 - with many people as uh many people as
174:03 - possible by you the reason is that all
174:05 - these videos is completely for free uh
174:08 - my main aim is basically to teach you in
174:10 - such a way that where you understand
174:12 - each and everything in depth Let It Be
174:14 - theoretical intuition practical
174:15 - intuition and many more right so what
174:17 - are we discussing in this specific video
174:19 - so I hope everybody has heard about um
174:24 - gini right gini API that was probably
174:26 - gini model that was launched from Google
174:29 - obviously the demo that they had
174:31 - actually shown on the website it was not
174:32 - true demo they had made made up that
174:35 - particular demo but now the gini pro
174:38 - model is available for you and you can
174:40 - also use it for your own demo purpose uh
174:43 - we'll understand we'll see a complete
174:44 - demo like how these models are these
174:46 - large language models are and how it is
174:49 - probably performing we'll be seeing
174:51 - various codes I'll be showing you that
174:53 - how you can probably create an API for
174:55 - this and the best thing is that with
174:57 - respect to this they have come up with
174:59 - this two different plan you know one is
175:01 - free for
175:03 - everyone the thing the model the gini
175:06 - pro model which is free for everyone it
175:08 - has a rate limit of 60 queries per
175:10 - minute that basically means within a
175:12 - minute you can actually just hit 60
175:14 - queries price is free Price output is
175:17 - also free input output is free and you
175:19 - can start using this uh to just get to
175:22 - know like how powerful the gini pro is
175:25 - and I probably used it it looks pretty
175:27 - much good you know um at least uh not
175:31 - like that fancy thing what it showed in
175:33 - the demo not like that but yes we can
175:35 - compare with chat GPT or we can compare
175:38 - with open AI apis which perform various
175:41 - tasks like test summarization uh text
175:44 - classification or let's say other tasks
175:47 - like Q&A and many more right so the next
175:50 - uh plan that is probably go uh that is
175:52 - going to come which can be specifically
175:53 - used in industries that is pay as you go
175:56 - and that basically starts for more than
175:58 - 60 queries per minute okay so here is is
176:01 - the entire information I will be
176:02 - providing the link with respect to the
176:04 - description in the description of this
176:05 - particular video now let's go ahead and
176:07 - check the documentation and one by one I
176:09 - will show you how you can probably
176:10 - create the API the API Keys how you can
176:13 - probably load it and start using it okay
176:16 - so first of all just go to the python
176:18 - section over here and I'm just going to
176:20 - click on this Google collapse so as soon
176:22 - as I probably clicked on this Google
176:23 - collap you can probably see over here
176:25 - I've got this entirely okay so we will
176:28 - go ahead step by step we'll understand
176:30 - each and every line of code we'll see
176:31 - multiple examples and then let's see how
176:35 - good the Gemini API is okay so as
176:38 - suggested as I told you already this is
176:40 - like Gemini has come up in three
176:42 - different version and gini Pro is right
176:44 - now available for you okay where you can
176:46 - probably try and uh just by seeing this
176:49 - things right I I definitely I've already
176:51 - tried it out I feel that yes it is good
176:54 - enough right um if I compare with any
176:57 - openai apis yes it is doing a fabulous
177:00 - task okay I know the demo was bad but
177:03 - here the things that I'm going to
177:04 - execute is very much good so to get this
177:07 - particular notebook file all you have to
177:09 - do is that click on python over here and
177:11 - just click on this okay so this is the
177:13 - entire information
177:14 - how you can probably create the API key
177:16 - and all I will be showing you step by
177:17 - step okay so let's start this video
177:20 - before that please do hit like as you
177:22 - know that I uh I I always thought that
177:25 - within this year right I may reach 10
177:27 - lakh subscribers but I could not but
177:29 - it's okay for the next upcoming 3 to
177:31 - four months I would definitely like to
177:34 - reach 10 lakh subscribers okay 10 lakh
177:37 - subscribers 10 lakh is a very good
177:39 - number right and just imagine how much
177:41 - beneficial it is for so many people out
177:43 - there so so let's go ahead and start
177:45 - this okay so we will go step by step I
177:48 - know please try to understand this
177:50 - things because I will be explaining you
177:52 - the code what exactly gini API is all
177:56 - about what this gini model why it is so
177:58 - good because this is a multi model right
178:01 - multimodel that basically means it is
178:02 - trained on both text and images so I'll
178:05 - show you if I probably give a image and
178:07 - it will be able to read that image and
178:08 - it'll be able to give you the
178:10 - information about that image that
178:11 - powerful the specific model is okay so
178:14 - uh we'll set up our development
178:15 - environment and get the API access to
178:17 - gini so this is the entire agenda we
178:19 - will generate text response from text
178:21 - inputs we will generate text responsive
178:24 - for multimodel inputs and then we'll use
178:26 - gmin for multi- turn
178:28 - conversation and we'll also see how
178:30 - different embeddings can be applied for
178:32 - large language model okay so first of
178:35 - all this generative AI entire there is a
178:39 - library which is called as Google
178:40 - generative AI with the help of API you
178:42 - can use various features that is
178:44 - available in this gini model right gini
178:46 - pro model so first of all I will go
178:47 - ahead and install this entire library
178:50 - that is Google generative AI okay with
178:52 - the help of the specific code it may
178:54 - take some time again it depends on your
178:55 - internet speed how good it is uh and
178:58 - then after we probably go ahead and
179:00 - install this so import the specific
179:02 - library now here are some of the
179:05 - important things that we are going to do
179:07 - okay so first of all you'll be able to
179:09 - see we are importing path lib text WP
179:11 - Google generative AI as geni so this is
179:14 - the allias that we are going to
179:16 - specifically use and this is the library
179:18 - that has almost all the features over
179:19 - there from google. collab you import
179:22 - user data inside user data you can store
179:25 - your API key okay I will show you how to
179:27 - how you can probably do it along with
179:29 - this I'm probably importing display and
179:31 - markdown to display the information and
179:34 - there is another method which is called
179:36 - as definition to underscore markdown
179:38 - that basically means whenever I get any
179:40 - kind of response I'm going to replace
179:42 - this dot by star because I think this
179:45 - generative a is gini pro gives some kind
179:47 - of response where it will be having this
179:49 - kind of output okay and then we are
179:52 - going to convert that entire data with
179:53 - respect to markdown okay so all these
179:55 - things we are specifically importing now
179:58 - let's go to our next step setting up
180:00 - your API key so for do doing that just
180:03 - click on this particular link okay so it
180:05 - will go to this makers suit google.com
180:08 - app API key you can also create Palm
180:10 - apis from here and all right so after
180:12 - going over here what you can do you can
180:14 - actually go ahead I've already created
180:16 - one particular API key over here which
180:19 - you can actually see if you want to
180:21 - create a new API key just go ahead and
180:23 - click on this as soon as you click on
180:25 - this you will be getting the API key
180:26 - just copy from there and here what I
180:29 - have done I have written the API key
180:30 - over here okay so don't worry that uh
180:32 - this API key is not right uh I've
180:35 - already executed or I've already stored
180:38 - that particular API key itself right so
180:40 - probably when I share this particular
180:42 - notebook with you you'll not be able to
180:44 - see this API key over there until then I
180:46 - may have also deleted it from here okay
180:49 - so the reason don't use this do it with
180:51 - your own because it is completely for
180:53 - free right 60 queries you can actually
180:55 - hit
180:56 - it now let's go ahead and do one thing
180:59 - first of all I'm creating an environment
181:01 - variable called as Google API key okay
181:06 - so this will basically be my API key
181:08 - okay and I will remove this I will not
181:11 - require this and then what I'm doing
181:13 - over here I'm I'm saying gen. configure
181:16 - API key and I'm using the environment
181:18 - variable and I'm setting that API key to
181:20 - this okay so once I do this done that
181:23 - basically mean my API is configured this
181:25 - is the simple steps that you
181:27 - specifically need to do now what all
181:30 - models you specifically get from Gemini
181:33 - API so over here there are two models
181:36 - one is Gemini Pro this is optimized for
181:38 - text only prompts so text summarization
181:41 - Q&A chat anything that you specific Al
181:43 - want to do you have to use this specific
181:47 - model that is Gemini Pro and then you
181:50 - have one more model which is called as
181:51 - gini provision this is optimized for
181:54 - text and image prompts okay so for text
181:58 - and image prompts specifically you can
182:00 - use gini pro version if you want to
182:02 - check how many different models are
182:04 - there just write for M gen. list uncore
182:08 - model list underscore models will give
182:09 - you all the models that it is basically
182:12 - having and it is saying that if
182:13 - generated content in M supported
182:15 - generation methods then it will show you
182:17 - all the both the model name so here you
182:20 - can probably see that as soon as you
182:22 - execute this you'll be getting that you
182:24 - have two models one is gini pro and then
182:26 - the other one is Gemini Pro Vision now
182:29 - let's go ahead and let's try something
182:31 - okay so first of all I'm going to use
182:35 - the gemin pro model this is specifically
182:37 - for text related task okay so here I'm
182:41 - going to use generative AI gen AI
182:44 - dot there is a method which is called as
182:46 - generative model and there I'll be
182:48 - giving my model name so once I execute
182:50 - this this basically becomes my model so
182:52 - if I probably go ahead and execute over
182:55 - here so here you'll be able to see that
182:58 - I am getting a model trust me guys this
183:00 - is good enough because if you don't have
183:02 - open API key also you can use this and
183:06 - when you use this you'll be able to
183:08 - understand more about the llm models
183:10 - okay so now let's go ahead and use one
183:14 - method which is called as generate
183:15 - content this generate content method can
183:18 - handle a variety of use cases including
183:20 - multi multi-t chat multimodel input
183:24 - depending on what the under underlying
183:26 - model supports so here I've used gin Pro
183:29 - so in this particular case I am going to
183:32 - specifically use it for text related
183:34 - task okay so now here we use model dot
183:37 - generate content and here I'm giving the
183:40 - message what is the meaning of life okay
183:43 - so this is the default message once I
183:45 - execute this you see the response time
183:47 - the response time with respect to this
183:49 - particular time it will go ahead and
183:51 - capture so uh and also remember guys uh
183:55 - when I was executing some some some time
183:58 - before you know this was really really
184:00 - fast right now it is a little bit slow
184:03 - is it a problem internet connection I
184:04 - don't know okay so but here you can see
184:06 - user time it tooks 120 millisecond
184:09 - system time was 10.5 millisecond total
184:11 - time was 131 seconds which is very good
184:15 - like open AI API speed I think it'll
184:18 - take more than this okay that I
184:19 - specifically know now in order to see
184:22 - the response I will use this 2core
184:24 - markdown and we will write response.
184:26 - text as soon as I write this you will
184:28 - now be able to see the output of this
184:32 - right what is the meaning of life so
184:33 - here you can see that the meaning of
184:35 - life is a philosophical question that
184:37 - has been posed by human for centuries so
184:40 - and so blah blah blah blah all the
184:43 - information is very much easily given
184:45 - and it looks good you did not have to do
184:48 - any other prompt template techniques to
184:51 - probably put it in this dotted point it
184:53 - has considered in a better way yes you
184:55 - can again use prom template also for
184:57 - this purpose okay so if the API and now
185:01 - this is the most important thing which I
185:03 - like about gini Pro okay if the API
185:06 - failed to return a result use generate
185:08 - content response. prompt feedback now
185:11 - you may be thinking why the API May Fail
185:14 - it may be because of some of the other
185:15 - reason to see it was blocked due to
185:18 - safety concerns okay because of safety
185:19 - concerns also it may not give you some
185:22 - response now what may be that safety
185:25 - concern okay so first of all we will go
185:28 - and see response. prompt feedback when I
185:31 - probably execute this I also have these
185:33 - all features that are probably coming up
185:35 - this information in the response safety
185:38 - rating category harm category sexually
185:41 - explicit probability neg eligible so you
185:44 - can see it is categorizing based on this
185:47 - four important categories whether it is
185:49 - a hate speech whether it is a harassment
185:52 - whether it is a dangerous content right
185:56 - and here you can probably see that it is
185:58 - also giving that particular feedback if
186:00 - any of the feedback is positive I think
186:02 - you may not get a response so let's go
186:05 - ahead and execute this particular code
186:07 - and let's see whether this works true or
186:10 - not okay I definitely want to check it
186:12 - out so I will go ahead and execute and I
186:15 - will say um I will say Okay I I did not
186:19 - I do not mean anything to write anything
186:22 - bad over here but uh I would just like
186:24 - to see how to insult someone let's
186:30 - see okay I'm just trying I'm not I I'm I
186:34 - don't mean to but let's see so here
186:38 - you'll be able to see that whether I'll
186:39 - be able to get a response or not and
186:42 - again over here now now it has becomes
186:44 - fast right so now let's go ahead and do
186:48 - this see the response part quicker ex
186:51 - only works for a single C but none were
186:53 - returned now I will go ahead and check
186:56 - this prompt feedback now this is where
186:59 - block reason safety see over here harm
187:02 - category harassment medium so this is
187:05 - what is so good in this ethics is
187:09 - definitely there right so it is not
187:11 - giving you any kind of resp resp right
187:14 - so I hope you have understood the
187:16 - importance of this okay now similarly G
187:20 - gini can also generate multiple possible
187:22 - responses for a single prompt okay it
187:24 - can also generate right so for that you
187:27 - just need to use this responses.
187:30 - candidates now what I will do I will not
187:32 - execute this but instead we will go
187:34 - ahead and execute this what is the
187:37 - meaning of life okay okay so let's
187:40 - execute this it's
187:42 - okay um or I I'll just go ahead and ask
187:46 - a different question can you let me
187:50 - know about
187:53 - the future
187:58 - of future
188:02 - of generative AI okay so I'm asking this
188:07 - specific question now let's see I'm not
188:10 - executing this now it should definitely
188:12 - give me some kind of
188:14 - response and this also depends on the
188:17 - kind of like amount of tokens that is
188:18 - probably coming from the llm model so
188:21 - here it has taken 122 milliseconds now
188:24 - let's see the prompt feedback I think
188:26 - this is absolutely fine okay and here I
188:30 - will also go ahead and see my text it'll
188:33 - give so here you can see the future of
188:34 - generative AI holds immense potential
188:36 - and Promises to revolutionize various
188:39 - style Gan gpt3 this this is there now if
188:42 - I go ahead and right response.
188:45 - candidates okay now here you can
188:47 - probably see that I'm getting the entire
188:49 - info this is so nice see so this is my
188:53 - first text right role model find this
188:55 - this this all the information is there
188:57 - with respect to all the information that
188:59 - you will be able to see that right all
189:01 - whatever thing we executed step by step
189:03 - we getting everything okay so this is
189:07 - very very good okay now by default the
189:10 - model returns a response after
189:11 - completing the entire generation process
189:13 - you can also stream the response as it
189:15 - is being generated and the model will
189:17 - return chunks of the response as soon as
189:18 - they are generated which is absolutely
189:21 - amazing again right so what we can also
189:24 - do is that you can stream the response
189:26 - okay no need to wait for the entire
189:27 - response so let's see okay so here I
189:30 - will use the same question let me see so
189:33 - I will so can you let me know the future
189:36 - of generative now I think for this it
189:39 - will not take much time when compared to
189:42 - the previous one one that we had done
189:43 - right so here I will go ahead and write
189:46 - all you have to do is that just write
189:48 - over here let me just
189:51 - show okay you just need to add a
189:54 - parameter which is called as stream is
189:56 - equal to True by that you'll be able to
189:58 - get uh the response as it is being
190:01 - generated so let me just go ahead and
190:03 - execute this okay now I'm not going to
190:07 - directly print response. text but I'm
190:10 - saying that for every Chunk in response
190:13 - print chunk. test text okay and then it
190:16 - will show
190:17 - 80 like dotted lines okay so we can also
190:20 - display this in this way so here you can
190:23 - probably see how fast it was able to
190:25 - chunk by chunk obviously when chunk by
190:27 - chunk is getting displayed it will be
190:29 - very very good right so here you can
190:31 - probably see all the information ethical
190:34 - this needed for skill Force long-term
190:36 - impact on society and all now one more
190:39 - thing is that when streaming some
190:41 - response attributes are not available
190:43 - until you have iterated through all the
190:45 - response CH this is demonstrated below
190:48 - so here if I say what is the meaning of
190:50 - life with streaming is equal to true now
190:54 - you'll be able to see
190:56 - that let's see let's see okay so once
190:58 - this gets executed now I have this
191:00 - response. prompt feedback in prompt
191:03 - feedback you'll be understanding whether
191:05 - it has some problems or not with respect
191:07 - to that thing now if I directly go ahead
191:09 - and write response.
191:11 - text okay
191:13 - it will automatically tell me please let
191:15 - the response complete iteration before
191:17 - accessing the final cumulated attribute
191:19 - okay so this is a very good problem
191:22 - statement and now based on your
191:24 - requirements what I feel when compared
191:26 - to open AI still Gemini uh API that we
191:30 - have right it may have more
191:32 - functionalities as we go ahead but till
191:34 - now it really looks promising it looks
191:36 - really good okay now we will try to
191:40 - generate and text from image and text
191:42 - text inputs okay so this is also good
191:45 - now let's download this image so by
191:46 - using a curl operation I am probably
191:49 - downloading the image if you see the
191:50 - image the image looks something like
191:53 - this okay or I may also use some more
191:56 - image right I will write cat playing
192:00 - image okay so cat playing image so here
192:03 - you will be able to see some images
192:05 - let's see let's see let's see let's see
192:09 - this looks like a complicated image I
192:11 - guess it should find some problem okay
192:14 - uh downloads okay downloads downloads
192:18 - here I will
192:20 - say upload that specific image
192:25 - okay I will save it let's rename this
192:29 - image I will say cat. jpj now if I
192:33 - execute this over here I will write cat.
192:36 - jpj
192:39 - okay now this is the image it looks
192:42 - something like this now what I'm going
192:44 - to do I'm going to give this image to my
192:47 - Gemini Pro Vision model and then we will
192:50 - see what it can understand from this
192:53 - image like if it is if I say that the
192:55 - Kay is playing with some cotton fur or
192:58 - Hol right this cotton holes now let's
193:01 - see what answer gini Pro Vision gives
193:03 - right so first of all we will go ahead
193:05 - and load gini provision okay and then we
193:10 - will go ahead and generate model.
193:11 - generate content image and then we'll
193:14 - say to markdown response. text okay so
193:18 - let's go and see the
193:19 - answer so it is going to take that image
193:22 - here directly we are giving the image
193:24 - now it is going to understand the
193:25 - specific image it is going to read and
193:27 - it's is going to understand it is going
193:29 - to give me a response in text okay now
193:33 - let's
193:35 - see so this looks amazing till here but
193:39 - let's see the response I really want to
193:40 - see the response of this and it is
193:42 - taking some amount of time I think when
193:44 - we go with respect to the paid API it
193:47 - will be little bit less but again it
193:49 - depends on the image size how we are
193:52 - specifically calling it and all right
193:54 - let's see I think as we go ahead the
193:56 - correct answer is play right so here you
194:00 - can probably see if I go ahead and see
194:03 - this image right it shows play now it
194:06 - was not that well but let's see the
194:08 - previous one okay so let's see the
194:11 - previous one over here okay I think with
194:14 - just play it is showing I think I'm not
194:17 - satisfied with that specific answer but
194:20 - we can also try with the previous one so
194:22 - let's say this is the image and then we
194:25 - will go ahead and execute this let's see
194:27 - what kind of response it gives us so
194:30 - here you have some tiffen box you have
194:32 - broccolis you have some food item right
194:35 - so let's see till then I will download
194:37 - some more images Okay
194:41 - um
194:45 - let's see let's take this image
194:48 - also images. jpj I will go ahead and
194:54 - upload it over
194:57 - here okay I just want to see the answer
195:01 - like what all different kind of answers
195:03 - we specifically get yes uh right now for
195:07 - this image to text I think we are taking
195:09 - some time but I don't know like in the
195:12 - future future it may have a good
195:15 - response time when compared to right now
195:17 - but here definitely some amount of time
195:20 - is basically happening right
195:23 - or these are meal prepared containers
195:26 - with chicken BR brown rice broccoli and
195:29 - Bell papers this also looks good so it
195:31 - is able to give minute details over here
195:35 - this is really good okay now I will go
195:38 - ahead and write play images. jpg
195:43 - let's
195:45 - see so this is the image I think this
195:48 - image is small so it should be able to
195:50 - give it should be able to check the
195:52 - image and generate some kind of text
195:54 - let's
195:56 - see it was an error post intern please
196:00 - try a report okay some some kind of
196:02 - error is basically coming up
196:05 - okay let's see I I think it was a
196:07 - timeout issue or I don't know but I
196:11 - think now we should be able to see the
196:13 - answer okay it is giving in some Chinese
196:18 - text okay so this I think works with
196:22 - some
196:23 - other other kind of images itself or
196:27 - where you have a detailed image but
196:29 - definitely some of the use cases is
196:31 - failing over here okay so I feel that it
196:35 - is failing over here okay now let's try
196:37 - this one okay so there is one more
196:39 - option I don't know like with respect to
196:41 - image do we have to give only this kind
196:44 - of images if I'm giving cat images
196:45 - playing cat playing
196:47 - images I don't know what kind of
196:49 - response that we are getting or do we
196:51 - have some options to probably change it
196:52 - we'll have a look okay now the next
196:54 - thing is that to provide both text and
196:56 - images in a prompt I'm saying that right
196:58 - a shot engaging blog post based on this
197:01 - picture it should include description
197:02 - this this I think for this uh it should
197:05 - be able to give right it should be able
197:08 - to give I think that is the reason it
197:09 - was not giving that well so now let's
197:12 - see I will go ahead and write cat. jpg I
197:15 - hope it was cat.jpg only okay so yes
197:18 - this was CAD
197:20 - jpg uh can you include the description
197:25 - of the
197:26 - photo of the
197:32 - of okay uh let's see can you write a
197:36 - short blog based on this picture okay it
197:40 - should includ a description of the
197:43 - photo okay now let's see how what kind
197:45 - of response we specifically get and here
197:48 - the second parameter I'm giving it as
197:49 - image and that is how we are going to
197:51 - probably get it and uh to provide both
197:54 - text and image pass a list containing
197:57 - the strings and the image so this is my
197:58 - string the first thing that I'm giving
198:00 - this along with this image so I'm saying
198:02 - that write a short engaging blog post
198:05 - based on this picture okay so now let's
198:07 - see what kind of response we will
198:09 - specifically get okay and quickly let's
198:13 - see but I did not find a good output
198:16 - something like this so here it says like
198:19 - that only right is designed to handle
198:22 - multimodel prompts and return a text
198:23 - output okay perfect so yes uh this is
198:27 - there now let's convert this into
198:32 - text the adorable Ginger kitten is
198:34 - having a blast playing with a colorful
198:36 - cat toy the kitten is batting at the toy
198:39 - with a spa this looks good right I'm
198:42 - really
198:43 - impressed by seeing the image it is
198:46 - providing you all the information okay
198:49 - now let's see one more okay
198:51 - sparton image I don't know let's see
198:55 - sparton group
199:00 - image I will put this image let's see it
199:02 - looks good I like this image
199:07 - spans jpg okay so I put this image let's
199:11 - upload this and let's see whether it'll
199:13 - be able to provide the description or
199:17 - not first is observing what all things
199:20 - are there in the image I think that is
199:22 - what gini provision actually gives you
199:24 - okay so here I will say
199:29 - Spartans do
199:31 - jpg not directory why spotten it is
199:35 - spotten spotten Spartan okay Spartan so
199:39 - this is my image which
199:41 - looks oops oops oops oops what is the
199:44 - error now
199:46 - Spartan okay it is
199:50 - jpeg okay so uh I'm getting this
199:54 - specific image of Spartan right now and
199:57 - let's see how it looks like so this is
199:59 - the entire image right now let me just
200:03 - go ahead and give this image over
200:06 - here okay I'm saying it to generate the
200:09 - content model. generate content let's
200:11 - see and I'm saying write a short
200:13 - engaging blog post based on this picture
200:15 - it should include a description of the
200:17 - photo okay so it is what I feel is that
200:21 - that Vision it was not able to just
200:25 - recognize everything but the text
200:27 - information that we getting over here
200:28 - it's good in this epic scene from the
200:30 - movie 300 Leon is the king of spot okay
200:33 - it is able to understand from the movies
200:36 - also completely right which is
200:38 - absolutely very very good okay so I hope
200:41 - you like this this particular video guys
200:43 - please go ahead and try it out and there
200:44 - is one more assignment that I really
200:46 - want to give in this go ahead and try
200:49 - there is something called as chat
200:51 - conversation so there are a couple of
200:53 - very simple by using Gemini Pro you can
200:56 - actually do it just go ahead and try
200:58 - this you just need to execute by
200:59 - appending this in the history all the
201:01 - information is given over here okay but
201:04 - yes I feel Gemini Gemini Pro looks
201:09 - promising and uh yeah it looks good
201:12 - in some computer vision right the images
201:14 - that I gave like cat and all it was it
201:16 - was just saying play so at least it was
201:19 - able to understand someone is playing
201:20 - over there okay so guys yet another
201:23 - amazing end to endend project for you
201:25 - and in this particular project we are
201:27 - probably going to create a multilanguage
201:30 - invoice
201:31 - extractor and we are going to use gini
201:34 - Pro API for this gini pro has been quite
201:38 - amazing you can definitely do a lot many
201:40 - things I have preped prepared more 10 to
201:42 - 15 different kind of projects that are
201:45 - related to Real World Industries and
201:47 - trust me all the specific projects are
201:49 - performing exceptionally well with
201:51 - respect to accuracy so over here we are
201:54 - going to focus on creating a multil
201:56 - language invoice extractor app we'll be
201:59 - using gemin Pro we will be writing all
202:01 - the codes step by step so please make
202:05 - sure you practice along with me and once
202:08 - we practice things right one then you
202:10 - get multiple ideas like what different
202:13 - kind of projects we can basically do
202:16 - okay so let's go ahead and first of all
202:19 - let me show you the agenda what all
202:21 - things we are basically going to focus
202:23 - on so here is the entire agenda so in
202:28 - this agenda what we are going to focus
202:30 - on first of all I will go ahead and show
202:32 - you the multil language in wased
202:35 - extractor app demo okay how the demo
202:37 - looks like later on we will start the
202:40 - process of creating the project project
202:42 - by creating the environment first of all
202:44 - then we will go ahead with the
202:45 - requirement. txt what all libraries we
202:47 - are specifically required and then we
202:50 - will start writing our code this will be
202:52 - an endtoend project code step by step
202:54 - we'll try to build this app again it
202:57 - will take some time let's say this
202:59 - project will probably take somewhere
203:00 - around 25 to 30 minutes and then in
203:03 - fifth point we'll also discuss about
203:05 - what more additional improvements you
203:07 - can specifically do so that you can also
203:09 - try it from your side and as usual guys
203:12 - I'm actually keeping Target likes for
203:14 - every video so let's target uh th000
203:18 - likes for this specific video because
203:19 - all these videos will be super
203:21 - beneficial for you in the
203:23 - companies so let me first of all
203:25 - complete the first one that is the demo
203:29 - okay so here you can probably see this
203:31 - is my entire app okay and here I have
203:36 - actually uploaded one of the uh invoice
203:40 - okay so this is the GST invoice and it
203:42 - is completely in Hindi okay the best
203:45 - thing is that if I ask any question
203:47 - related to this using gini Pro so here I
203:50 - have asked what is the address in the
203:51 - invoice so address basically means over
203:53 - here 1 2 3 uh SBC building DF state so
203:59 - this is just a common invoice I've taken
204:02 - from the internet itself so here you can
204:04 - probably see we are able to get the
204:06 - entire response so this is quite amazing
204:09 - not only this I've asked for different
204:10 - different questions what is the date
204:12 - let's say if I go ahead and say what is
204:15 - the
204:16 - date what is the date in the invoice and
204:21 - here you can see date basically means
204:23 - the knock so
204:24 - that usually this Google gini pro is
204:27 - able to understand those things okay so
204:30 - I think we will be getting the response
204:32 - so let me just go ahead and see it so
204:34 - it's
204:35 - running uh so here you can probably see
204:37 - 12 27 21 so all the information in this
204:41 - specific invoice you are able to extract
204:44 - just by putting a prompt over here now
204:46 - the best thing about this particular
204:48 - project is that it is very difficult to
204:50 - automate it because I'll tell you uh we
204:53 - have tried the specific project with the
204:54 - help of testra OCR and all right just
204:58 - imagine that Google Germany is able to
205:00 - perform exceptionally when well when
205:02 - compared to all those kind of tools okay
205:05 - so this was the demo now we will go
205:06 - ahead and probably develop this project
205:09 - completely end to end and we'll start
205:11 - from completely from scratch itself so
205:14 - uh let me go ahead and let me start this
205:16 - specific
205:17 - project so guys here is one of the
205:20 - project that I've started in my vs code
205:22 - itself so first of all just go to the
205:25 - terminal so I will show you all the
205:28 - steps what you should basically do as I
205:30 - suggested the first step is basically to
205:32 - create my virtual environment so for
205:36 - creating the virtual environment I've
205:38 - already created it so that it does not
205:39 - take much time because for creating the
205:41 - virtual environment also it takes some
205:43 - time so in order to create it just go
205:45 - ahead and write cond create minus P okay
205:50 - V andv your environment name okay and
205:53 - then you can also give python version
205:55 - and remember to give python version
205:57 - greater than 3.9 in this case because
205:59 - Gemini Pro is suitable for python
206:02 - version greater than 3.9 so here I'm
206:04 - going to basically use 3.10 and then you
206:07 - just give- y so that it does not ask you
206:10 - for any permission while doing the
206:12 - installation as soon as you probably
206:14 - press enter so this kind of V EnV
206:16 - environment will get created in the same
206:18 - project folder okay so I'm not going to
206:21 - repeat this thing and probably execute
206:24 - it because I've already done this okay
206:26 - so just do it from your site with the
206:28 - help of this specific command the second
206:30 - thing is that I will go into theb file
206:33 - and I'll create an API key which will be
206:35 - available from the Google okay so Google
206:38 - API key this is basically for gin Pro
206:41 - and and if you don't know gini pro gini
206:43 - pro is again an amazing model that is
206:45 - provided by Google which actually
206:48 - provides you in a free way so you can
206:50 - actually hit 60 queries per minute okay
206:53 - so here is the API key that I have got
206:55 - if you want to also create your API key
206:57 - go to this website okay maker suit.
207:01 - google.com/ API key and you can just
207:03 - click on this create API key new project
207:06 - okay so I have already created it so I
207:08 - don't want to create it again okay so
207:11 - these are the first two steps you really
207:13 - require the API key and you require the
207:16 - environment now after that you just
207:19 - activate like you just write cond
207:22 - activate
207:23 - venv Okay and just activate this
207:26 - specific environment okay once you
207:27 - activate it you will be able to see that
207:29 - you'll be in that same environment
207:32 - location now let's go to the next step
207:35 - in requirement. txt what all libraries
207:37 - we specifically require so here is
207:40 - streamlet then you you have Google
207:41 - generative AI then you have python. EnV
207:44 - then you have Lang chain you have P PDF
207:46 - P PDF is basically to load any PDF as
207:49 - suchar or read any PDF uh then you have
207:52 - chroma DB chroma DB is specifically for
207:54 - Vector embeddings so we will try to also
207:56 - do Vector stores or vector embedding
207:58 - will try to create it so uh these are
208:01 - the basic steps that we specifically
208:03 - require now let's go ahead and start my
208:08 - coding or creating this specific
208:10 - application uh I will start writing the
208:12 - code completely from end so first of all
208:14 - what I will do I will go ahead and load
208:17 - my environment variable so I will say
208:20 - load do from EnV
208:23 - import
208:25 - load uncore do EnV so the reason why I'm
208:29 - doing this is that so that I can upload
208:32 - my all I I can load all my environment
208:35 - Keys okay so if you remember we have
208:37 - also installed python. EnV so python.
208:40 - EnV is basically for all my environment
208:42 - variables now I will go ahead and write
208:44 - load doore EnV so this will what it'll
208:48 - do it will take it'll load all the
208:50 - environment
208:52 - variables all the
208:55 - environment variables
208:58 - fromb file okay so this is what it is
209:01 - specifically going to do we'll do step
209:03 - by step now you'll be able to understand
209:05 - and please make sure that you write the
209:07 - code along with me so that you'll be
209:09 - able to understand it now the next thing
209:11 - is that I will go ahead and use
209:12 - streamlet as streamlet is a better
209:15 - framework to
209:16 - quickly you know create an app and
209:19 - definitely I use chat GPT for taking out
209:22 - the code and all right so that is the
209:23 - reason you're able to see that I'm able
209:25 - to upload daily videos see not the
209:28 - entire project is created by chat GPT
209:30 - but how to use stream late how to create
209:32 - this uh website kind of app you know all
209:34 - those things I can usually use streamlet
209:37 - uh use chart GPD so then uh so this is
209:41 - is my streamlet the next thing is that I
209:42 - will go ahead and import
209:44 - OS uh OS will basically be useful by for
209:49 - picking up the environment variable
209:50 - assigning the environment variable from
209:52 - somewhere else okay now this is done uh
209:55 - the next thing that I want is from P I
209:59 - also import image okay I don't know
210:01 - whether I'll be using this but let's see
210:04 - the next thing I will also go ahead and
210:07 - import from Google
210:10 - Dot generative AI as gen AI okay so I'm
210:14 - also going to import this specific
210:17 - because gen AI will be my entire
210:20 - libraries that I'm going to access it
210:22 - okay so done this is done these are some
210:25 - of the basic uh things that we are
210:27 - specifically going to load it okay now
210:29 - usually when we start our any
210:32 - application using gin API so what we
210:34 - need to also do is that we need to
210:37 - configure uh the API key so here I'm
210:39 - going to write gen a configure API key
210:43 - is equal to
210:45 - os.
210:47 - getet OS
210:50 - dot get EnV okay and here I'm going to
210:54 - get my environment variable that is
210:56 - nothing but Google API key so here
210:59 - whatever environment variable is
211:00 - basically present over here we are going
211:02 - to take this okay so configure okay so
211:06 - gen. configure uh the API key with this
211:09 - okay now
211:11 - it's time that we will create our
211:14 - function to
211:18 - load
211:20 - gini gini provision since the invoice
211:24 - instructor is on top of an image so we
211:27 - have to use this gin provision okay so
211:30 - function to load or first of all I'll
211:32 - load the model so I'll say model dot gen
211:38 - dot generative
211:43 - generative
211:45 - model so I'm going to specifically use
211:47 - gen. generative model and here I'm going
211:50 - to basically give my model name so it
211:53 - will be Germany Pro Vision okay so once
211:57 - I do this that basically means we are
211:58 - going to use this specific model now I
212:01 - will go ahead and write definition get
212:04 - giny
212:07 - response
212:09 - input image
212:12 - prompt
212:15 - okay so let me go ahead and write model
212:18 - equal to gen
212:20 - AI sorry I'll not initialize the model
212:23 - again so what I will do I will go ahead
212:25 - and write see the thing is that here I'm
212:28 - going to give three parameters one is
212:30 - this specific input input basically
212:32 - means uh uh whatever input I really want
212:36 - okay with respect to all the images that
212:38 - I'm giving and I'll also talk about this
212:40 - specific thing okay okay the three
212:41 - important information this input is
212:44 - basically I'm telling what what I want
212:48 - the assistant to do okay if I say hey
212:50 - you need to act as an invoice extractor
212:53 - you need to act like an expertise who is
212:55 - very good at uh taking out details from
212:58 - the invoice right so that basically
213:00 - becomes my input okay this prompt is
213:04 - what message I want like what is the
213:05 - address I actually return this is
213:08 - basically the image that we are going to
213:10 - pass okay
213:11 - so all those information this three
213:13 - information what we can basically do
213:14 - I'll write response
213:16 - dot model
213:18 - dot
213:21 - generate content and here we are going
213:24 - to use this three information first of
213:27 - all is
213:28 - input then you have image of zero the
213:31 - second one and then you have the prompt
213:35 - okay so this three information basically
213:38 - when you're generating this content you
213:39 - can give this three information in this
213:41 - same way okay input image of zero and
213:43 - prompt okay so in gini Pro they take
213:48 - they take all the parameters in the in
213:49 - the form of a list okay and remember the
213:51 - first parameter is basically the kind of
213:55 - prompt that you're giving where your
213:57 - model needs to behave in that specific
213:59 - way so I will talk more about it as we
214:01 - go ahead and finally we are going to
214:02 - just return the response. text so this
214:05 - easy it is with the help of Gemini Pro
214:08 - okay and that is the reason I'm loving
214:10 - it when I probably compare with open Ai
214:12 - and the best thing is that I can also
214:14 - use uh this along with my Lang chain you
214:17 - can probably use it with different
214:19 - different things I will show you I've
214:20 - also created a project where you can
214:22 - chat with multiple documents okay so
214:24 - that will also be we'll be using Lang
214:26 - chain and all so this is the function
214:28 - that is specifically done now understand
214:30 - one thing guys um we will do our
214:33 - streamlit setup okay so streamlet setup
214:35 - what I will do so here I will go ahead
214:37 - and copy and paste like this so here I'm
214:40 - using st. set page config let's say that
214:42 - I'll go ahead and say over here
214:45 - multi-
214:47 - language
214:49 - invoice
214:50 - extractor okay multi- language invoice
214:53 - extractor now in this multi- language
214:55 - invoice extractor I will probably also
214:58 - give this information let's say okay now
215:02 - here I've given one input box this one
215:05 - input box is my input prompt okay and
215:08 - this is basically my upload file file
215:10 - upload so I'm saying that choose an
215:12 - image the image can be jpg jpg PNG this
215:15 - is the image of the invoice okay so let
215:18 - me go ahead and write this message of
215:20 - the invoice so once I specifically
215:23 - upload this specific file then we can do
215:25 - anything that we want okay now the next
215:29 - thing is that I will create an image
215:31 - variable I'll keep it blank initially
215:33 - and let me go ahead and write if
215:35 - uploaded
215:36 - file is not none so that basically means
215:39 - when I've when when I have uploaded some
215:42 - file then I will go ahead and write
215:45 - image and again I'll be using image.
215:48 - open and we will upload uh open this
215:51 - uploaded file okay now once we upload
215:54 - this so what we can also basically do is
215:57 - that we can uh write some kind of image
215:59 - and all I want to display the image also
216:03 - as soon as I upload it I probably want
216:05 - to display it so I can just try use this
216:07 - st. image functionality and I'll say
216:09 - caption uploaded image
216:11 - and we can use this properties Now
216:13 - understand that this this code right I
216:15 - have directly searched from uh uh chat
216:19 - GPT okay and uh I've just written okay
216:22 - just create me an image where to upload
216:25 - files and all right uh so very simple it
216:28 - is not like I am learning from somewhere
216:31 - I even not seeing the documentation chat
216:33 - GPT actually provides everything a
216:34 - Google Power provides everything that is
216:36 - basically required now this is my
216:38 - uploaded file now I will also go ahead
216:41 - and create my submit button so here I
216:43 - will go ahead and write St do button and
216:46 - I will talk about it saying that tell me
216:49 - about the image okay tell me about the
216:54 - invoice something so this is my message
216:57 - that I'm actually going to give in my
216:59 - submit button and finally I have to also
217:02 - design my input prompt now see this
217:05 - input that I'm actually going to give
217:07 - right so this basically becomes my input
217:08 - prompt I what how I want the germini pro
217:11 - llm model to behave so here I will go
217:14 - ahead and create my input prompt just
217:17 - see this okay this is important and this
217:19 - will also give you an idea like how
217:21 - improv prompt works okay how we can
217:24 - actually work with any kind of improv
217:26 - prompt I will say you are an
217:30 - expert
217:31 - okay in
217:34 - understanding invoices okay
217:39 - um
217:41 - we will
217:45 - upload we will upload a
217:49 - image image as invoices okay I'm just
217:53 - writing some messages
217:55 - and you will have to
218:01 - answer any
218:04 - questions based on
218:07 - the
218:09 - uploaded
218:10 - invoice image so this is just a basic
218:13 - prompt that I'm specifically using over
218:15 - here I'm telling this to do something
218:19 - related to this okay so this is my input
218:21 - prompt and all I've written it over here
218:24 - then let me go ahead and write if submit
218:27 - button is
218:29 - clicked if submit button is clicked so
218:32 - this is my default input prompt now what
218:34 - I will do is that I'll also create my
218:36 - prompt template itself and probably go
218:38 - ahead right if submit button okay okay
218:41 - is
218:42 - clicked now what will happen if I click
218:44 - the submit button so first of all I will
218:47 - go ahead and write if
218:49 - submit first I need to get my image data
218:52 - okay now understand over here as soon as
218:55 - we load the image but still we have to
218:57 - do some kind of image processing and
218:59 - convert those images into some bytes
219:02 - okay so for that again how do we do it
219:05 - so I will write definition input uncore
219:09 - image set up so for this I have just
219:12 - written in chat G saying that and here
219:16 - will be my uploaded file okay uh
219:19 - uploaded file uh okay uploaded file okay
219:22 - see now you may be thinking what I'm
219:24 - doing in this function in this function
219:27 - what we are writing is that it will take
219:28 - that uploaded file it will convert that
219:31 - into bytes and it will REM it will give
219:33 - all the image format all the image
219:35 - information in the bytes now I did not
219:37 - write this code I just went and searched
219:39 - in the chat GPT and this is the code
219:41 - that I specifically got okay and this
219:44 - code is quite amazing same way nothing I
219:47 - did not do anything see here if the
219:49 - uploaded file is not done so we first of
219:51 - all we are getting all the values then
219:53 - the image part what all things we
219:54 - basically require the type the data and
219:56 - byes data right and then we are
219:58 - returning the image Parts in these two
220:00 - format okay the M type and data and if
220:04 - the file is not uploaded this is that so
220:06 - this is completely I got it from chat
220:07 - GPT I'm not bragging anything about
220:09 - myself and all
220:10 - um again charit is already trained in
220:13 - internet data so it's just like writing
220:15 - an input prompt and I'm saying that okay
220:16 - I require this two specific information
220:18 - please give me that information now in
220:20 - this image data what we will basically
220:22 - do is that we will get all the image
220:23 - information so here I will go and write
220:27 - input image
220:29 - setup so let me do one thing input image
220:35 - details
220:36 - okay so I will call this give a good
220:39 - name okay and here I will give my
220:42 - uploaded
220:44 - file okay uploaded file so uploaded file
220:48 - whatever uploaded file I'm specifically
220:50 - getting I'm going to give that specific
220:52 - thing over here now by this I will be
220:55 - getting my image data now image data
220:58 - once I get it okay then I will go ahead
221:00 - and write my response and go ahead and
221:03 - call my get Gemini response so here I'm
221:06 - going to basically write my input input
221:08 - prompt first p parameter is this second
221:11 - parameter that I'm going to give is my
221:13 - image data as usual remember all the
221:15 - information will be coming in the form
221:17 - of list okay so image underscore data
221:20 - and this will basically be my input
221:23 - input and this input is nothing but
221:25 - whatever information I'm putting it over
221:27 - here all this information will go over
221:29 - here and you have all this information
221:31 - in this format right now after this I
221:34 - will get the response and now I will go
221:36 - ahead and write st. subheader
221:40 - I'm giving some kind of
221:42 - subheader and I will write
221:46 - the response
221:50 - is st.
221:53 - WR and I will just display the response
221:56 - okay whatever response we are
221:57 - specifically getting okay that response
222:00 - we going to get over
222:03 - here so all this information is done and
222:06 - this is really good now it's time that
222:08 - we can just run the code so guys now
222:10 - let's go ahead and run this uh we have
222:13 - completed almost everything that we
222:14 - really want to do now is the most
222:17 - amazing thing whether the project will
222:18 - run or not okay so if the project runs
222:21 - it is absolutely good because at the
222:22 - first time we have written the code and
222:24 - everything should work fine so here I'm
222:26 - going to write streamlit run
222:30 - app.py and let's go and execute
222:33 - this so it has opened let's see so I've
222:36 - have downloaded two invoices let's see
222:38 - what all things will be there first we
222:40 - will try with the normal invoice okay so
222:43 - here you can see all the
222:46 - information who is this
222:50 - invoice build to okay so I'm going to
222:54 - put this information over here and I'm
222:56 - going to click
222:58 - it tell me about the image tell me about
223:02 - the invoice on the all the information
223:04 - will be provided over here this is
223:07 - good so your client so all the
223:10 - information is over here your client
223:12 - this this this this even the number has
223:13 - been extracted which is quite amazing it
223:15 - is really a daunting process guys okay
223:18 - uh let's see I will just take a small
223:20 - one what is the deposit requested okay
223:22 - so I will just go ahead and write it
223:25 - what is the deposit requested this is
223:29 - good this giving an amazing response so
223:32 - I will go ahead and click tell me about
223:34 - the invoice over here and here you go
223:37 - let's
223:38 - see what it is going to get so tell me
223:43 - about the deposit requested it is just
223:44 - saying your
223:46 - company uh who is the deposit okay my
223:48 - prompt is wrong tell me so it is not
223:52 - able to understand the context obviously
223:54 - if you don't give the
223:56 - proper uh tell me how much was the
224:00 - deposit requested I'll give a good
224:08 - response Okay so I will go ahead and now
224:10 - click on it should give uh the proper
224:13 - answer I think now it is somewhere
224:15 - 169.99 5 it'll pick up that exact info
224:19 - and provide you all those
224:22 - information this is good so let's see
224:25 - 169.99 this is good guys this is trust
224:28 - me this is very very close uh what was
224:30 - the Consulting fees let me go ahead and
224:32 - write what was
224:34 - the Consulting
224:37 - fees now I think it should get confused
224:40 - with those two values what was the
224:45 - amount of the Consulting fees I think it
224:47 - should be able to give it let's see then
224:50 - we'll try with some other language
224:51 - invoice like Hindi and all
224:54 - okay let's see let's see let's see I
224:56 - think it should work fine but this is a
225:00 - good thing guys you can automate this
225:01 - entire process just imagine it is such a
225:04 - daunting process for with respect to
225:06 - invoice just see that whether you get an
225:09 - invoice all so the amount of the
225:10 - Consulting fees was
225:13 - $550 okay it is taking this
225:15 - information um okay there are some minor
225:18 - mistake but other than that I think
225:21 - let's see what is the total what was the
225:24 - total
225:30 - discount let's see discount is somewhere
225:32 - around
225:34 - 17.8 but if you give proper prompt I
225:38 - think you'll be able to get a good
225:39 - response response okay 179.1 4 okay so
225:43 - let's go ahead and try some other
225:45 - invoice this also looks good and uh let
225:47 - me go ahead and write this so here I
225:50 - will go ahead and write what is the HSN
225:52 - of Lenovo 51251 Lenovo in Hindi it is
225:56 - written in Lenovo so what is
226:01 - the HSN
226:04 - number
226:06 - of
226:08 - of
226:11 - of
226:12 - Lenovo okay I'm writing it in English
226:17 - 51251
226:20 - 5125 5125
226:23 - I let's see whether it'll be able to
226:25 - give or
226:30 - not see this small information also it
226:33 - will be able to take now over here the
226:35 - date is denak okay in Hindi we basically
226:37 - say it as dinak so here you can see
226:39 - 84713 01 0 amazing amazing just amazing
226:43 - okay so what is the date in the
226:47 - invoice and you can try anything you can
226:49 - try different different invoices if you
226:53 - want I downloaded this invoices from
226:55 - internet you can also do it okay so yes
227:00 - here let me see whether you're able to
227:02 - get it yes perfect so guys this was it
227:05 - for my side I hope you like this
227:06 - particular video if you like it please
227:07 - make sure that you subscribe the channel
227:09 - and all the information regarding this
227:10 - will be given in the description of this
227:12 - particular video this video we are going
227:13 - to see how we can actually build a
227:16 - conversational Q&A chat bot with the
227:18 - help of Gemini Pro API not only that
227:22 - we'll also try to save all this chat in
227:24 - the form of a history and will will also
227:26 - display all the results that we all had
227:28 - a conversation about that is the reason
227:30 - we are discussing about conversational
227:32 - Q&A chatbot so before I go ahead guys we
227:35 - will keep the light Target of this
227:37 - specific video to th000 so that I will
227:39 - definitely get motivated and I'll try to
227:41 - bring more similar kind of projects for
227:43 - every one of you out there now before I
227:46 - go ahead please let me go and go ahead
227:48 - and show you the demo so this is how the
227:50 - demo demo will look like so if I ask hi
227:54 - you can probably see I will be getting
227:56 - the answer the response how can I assist
227:58 - you today the chat history is Ive asked
228:01 - hi bot says hi hello how can I assist
228:03 - you so let's say if I go ahead and ask
228:06 - what is generative AI okay all all the
228:09 - previous information I really need to
228:11 - record it somewhere right so let's see
228:13 - after this the chat history right now is
228:15 - this much right so generative AI also
228:17 - known as so and so all the information
228:19 - is coming up and after that your chat
228:21 - history will also get updated and the
228:23 - best thing is that I'm streaming all the
228:26 - specific data okay so how streaming
228:27 - actually works and all we'll also be
228:30 - discussing about that so here you can
228:32 - see in the history I've asked what is
228:33 - generative Ai and this is specifically
228:35 - all the information now this is what we
228:38 - are going to implement step by step I'll
228:39 - show you each and everything again all
228:41 - the files regarding this all the code
228:43 - regarding this will be given in the
228:44 - description of this particular video so
228:46 - let me go ahead and let me solve this
228:48 - specific project so guys I have opened
228:52 - my VSS code over here and right now uh
228:56 - if you remember in my previous video
228:58 - I've discussed about all these things
229:00 - like vision. py how you can actually
229:02 - play with images with the help of gini
229:04 - API gini Pro API then we also discussed
229:07 - about some kind of simple Q&A chat B now
229:10 - in this video I am going to show you
229:12 - this entire code with respect to this
229:15 - qpy so here is what is my entire code
229:18 - I'm going to specifically write step by
229:20 - step I'll try to show you what all
229:21 - things is basically required as usual
229:23 - first of all we need to have an
229:25 - environment file with respect to Google
229:27 - API key this you can actually get it
229:29 - from makes. google.com so from there
229:32 - which will basically provide you all the
229:33 - features to create the API key for gini
229:36 - pro the first thing is EnV file once we
229:39 - create the EnV file then we will go
229:41 - ahead and start implementing our code if
229:44 - you have not seen all the videos in My
229:46 - Gin playist I would suggest go ahead and
229:48 - have a look so let me quickly write over
229:50 - there all the code so first of all I
229:52 - will go ahead and uh load all the
229:54 - environment variables so for this I will
229:56 - use
229:58 - forv um import
230:01 - loore EnV okay and then we will go ahead
230:05 - and initialize load. EnV and finally I'm
230:09 - going to import
230:12 - streamlet as
230:15 - STD so I'm going to use streamlit over
230:18 - here and remember one thing guys there
230:20 - are some important libraries that you
230:21 - need to install that is present in the
230:23 - requirement. txt everything will be
230:25 - provided in the description of this
230:27 - particular video the GitHub code right
230:29 - so I'm going to use streamlet Google
230:30 - generative a and python. EnV all this
230:33 - Library needs to be installed before
230:35 - ahead right so I'm importing stream.
230:37 - asst then I will go ahead and import OS
230:40 - because I will be requiring it and then
230:43 - I will say import Google do generative
230:47 - AI as gen okay so I'm going to use this
230:50 - Library only for doing all my task now
230:54 - initially whenever we load any
230:56 - environment key first of all we need to
230:58 - set this in my gen AI so for doing that
231:01 - I will write gen ai. configure configure
231:03 - is a method where it will be asking for
231:05 - the API key that I have and since I've
231:07 - already loaded that from my environment
231:09 - so you can see that os. getv I'm using
231:11 - this Google API key okay now the next
231:14 - thing over here I will try to create a
231:17 - function function to load gini pro model
231:22 - okay so here you can probably see gini
231:27 - pro gini pro model and get response okay
231:34 - so I'm doing this and here you have
231:36 - model.
231:37 - geni Dot
231:41 - generative model I'm going to initialize
231:43 - my generative model itself and in this
231:46 - case also I'm going to use gini Pro okay
231:49 - so gini pro and gini pro vision for
231:52 - conversational Q&A we will specifically
231:54 - be using this one that is called as
231:56 - Gemini Pro okay and then we will go
231:59 - ahead and execute it and write chat
232:02 - model dot startor chat and here I'm
232:06 - going to specifically use history
232:10 - okay so this history will also maintain
232:12 - all the things that we are probably
232:15 - going to uh have in our conversation but
232:18 - I will show you another way where I'll
232:19 - use the power of streamlit to store all
232:22 - the history in a form of a session later
232:24 - on you can also put that inside your DB
232:28 - or you can also use it from your session
232:29 - itself right uh so first of all let me
232:32 - go ahead and Define my definition so
232:34 - here I will basically write definition
232:37 - get gini under underscore
232:41 - response response and inside this
232:44 - response I will specifically give my
232:46 - question whatever question we have asked
232:49 - so this function should basically uh you
232:52 - know give me the response that I'm
232:54 - getting from the generative model itself
232:56 - like from the Gman Pro right so this
232:57 - question I will send it to my llm model
233:00 - and then I will specifically get the
233:01 - response so here I'm going to write
233:03 - response
233:05 - chat. sendor
233:07 - message and here I'm going to basically
233:10 - write question stream is equal to True
233:13 - okay so stream is equal to true because
233:16 - as the llm model is giving you the
233:18 - output we are going to stream it and we
233:19 - are going to display it right and then
233:22 - we will go ahead and return the response
233:24 - so once this response is basically
233:26 - coming this response is nothing but it
233:28 - is the output right when we specifically
233:31 - get response. text now we are going to
233:33 - initialize our stream late app for
233:36 - initializing it what we are going to do
233:37 - is that I'm going to use this three
233:39 - Fields one is the Q&A demo J Min L
233:42 - application page config and header it is
233:45 - very much common the most important
233:48 - thing is that if I really want to record
233:51 - the History part right the history of
233:53 - the conversation we will initialize
233:56 - session state so in stream late it
233:58 - provides you session States for chat
234:00 - history if it does not exist I'm saying
234:03 - if chatore history not in session State
234:06 - then we will go ahead and create a sess
234:07 - State and we'll right chatore history
234:10 - over here so right now it is blank as
234:12 - soon as we have any conversation later
234:15 - on we will try to record all those we'll
234:18 - try to put all those conversation inside
234:20 - this particular session state that is
234:22 - what we are going to look at then in the
234:25 - next step we are going to basically say
234:28 - input is equal to
234:31 - st. textor
234:34 - input and here I'm going to basically
234:36 - use
234:37 - input
234:41 - input and here we are basically going to
234:44 - write key is equal to input okay so
234:48 - input will be my variable name in short
234:50 - whatever text box I'm specifically using
234:52 - along with this I'm going to also use a
234:54 - submit button St do
234:57 - button and here I'm going to basically
235:00 - write
235:01 - ask the
235:03 - question okay so what I've actually done
235:06 - I've initialized my session okay session
235:08 - state so here that the name of the
235:10 - session state is chat history okay
235:14 - now if I click on submit or input right
235:19 - basically both the fields input should
235:22 - also be filled okay so if both this
235:25 - satisfies both this conditions satisfies
235:27 - then what I will do I will write I will
235:29 - go ahead and call my get Gemini response
235:31 - and here I'm going to basically give my
235:33 - input whatever input I am probably
235:36 - giving it as soon as it calls this
235:38 - function it is going going to get that
235:39 - message and it goes to get that response
235:41 - okay
235:43 - now the next thing is that add user
235:47 - query and response to session chat
235:52 - history now see as soon as I probably
235:55 - have created this input this is what is
235:57 - my input that user has given and this is
235:59 - what is the output that we have got
236:02 - right now what I'm actually going to do
236:05 - here for all this entire history right
236:08 - we going to save this in our chatore
236:11 - history session state so for that we
236:13 - will go ahead and write St do session
236:17 - State okay st. session State and here we
236:21 - going to basically use chatore history
236:26 - okay chatore history and I'm going to do
236:30 - append with this
236:33 - specific let's say I'm going to write
236:35 - you basically you who's sending the
236:38 - message
236:40 - and then I'm going to basically use
236:42 - input
236:43 - okay so I am storing all this session
236:47 - inside this particular U variable okay
236:50 - and finally I will get my response also
236:52 - so let me write over here St
236:55 - Dot subheader and here I will write a
236:58 - statement saying that the response
237:01 - is the
237:03 - responses and in short I have to
237:06 - probably display the response Now
237:09 - understand one thing we used something
237:11 - called as stream is equal to true that
237:13 - basically means this entire response can
237:16 - without getting the entire content can
237:18 - also be populated in whatever Pages we
237:20 - want to probably display right what what
237:22 - do I mean is that now my process did not
237:25 - have to wait for the entire content to
237:27 - come from llm so as llm is sending text
237:31 - Data whatever response it is basically
237:33 - sending it is going to display it in the
237:35 - front end screen okay so that is what we
237:37 - are basically looking at so now I'm
237:39 - going to write from Chunk in
237:41 - response see that is the reason when we
237:43 - write stream is equal to True okay that
237:46 - basically means we get the exess of all
237:49 - the streaming data and then we can write
237:51 - a for Loop and I'm saying from Chunk in
237:53 - response and here I write st. write and
237:57 - we will go ahead and write chunk.
238:00 - text so we are displaying the text part
238:03 - by part and along with this I will go
238:06 - ahead and append this entire
238:09 - response okay and here instead of
238:12 - writing you I will write bot okay and
238:16 - inside this instead of writing input I
238:18 - will go ahead and write my entire chunk.
238:23 - text so what I'm actually doing is that
238:25 - as soon as I get the response it is
238:28 - coming in the streaming manner we are
238:29 - displaying it and accordingly we are
238:31 - also appending in this chatore history
238:34 - okay now this is perfect this is done
238:36 - and finally what I will do is that I
238:38 - will just go ahead and create my history
238:41 - because I need to display all the
238:42 - history right so here I will go ahead
238:44 - and write the chat history is okay so
238:48 - this is what I am going to basically
238:50 - Implement
238:52 - and and here I will say for RO comma I
238:56 - will create two common variables in St
239:00 - dot session State and here I'm going to
239:04 - use my chatore history okay
239:09 - chatore history and whatever is there it
239:12 - is either in the ski value pairs U colon
239:15 - bot colon some kind of answers here I'm
239:18 - going to basically write st.
239:21 - write I'll use a f one over here and I
239:25 - will say
239:27 - roll
239:29 - colon and then here I will say text so
239:31 - in this specific format
239:34 - okay roll colum
239:36 - text now let's go ahead and see if
239:39 - everything works fine or not okay and I
239:42 - will go ahead and open my terminal I'll
239:44 - delete this let's see so we are now
239:48 - displaying perfectly all our details uh
239:52 - this VNV has got activated I've already
239:54 - shown you in my previous video how to
239:55 - activate V EnV environment also and here
239:58 - we will go ahead and write stream late
240:00 - run uh QA chat q a chat. py and let's
240:05 - see if everything works fine or not
240:09 - so it has got executed so here is my
240:13 - things I will say Hi how are
240:17 - you and then we will go ahead and ask
240:20 - the question let's see whether we'll get
240:22 - the response the response is I'm a
240:24 - conversational AI chat B this the chat
240:26 - history is having all the details okay
240:28 - and it is giving chunk by chunk you can
240:30 - see over here right uh let's ask some
240:33 - other question my name is krishn okay
240:38 - something
240:41 - krishn what is your name okay something
240:44 - like this and I will go ahead and ask
240:46 - the question let's see whether this is
240:48 - getting saved or not
240:50 - okay so here you can see how are you
240:53 - this my name is Krishna what is your
240:55 - name I'm a chat bot assistant I do not
240:57 - have a name bot designed to help users
240:59 - so and so so all the information that I
241:02 - probably type it over here along with
241:04 - the response it is getting recorded in
241:05 - the chat history now this is what I
241:08 - really wanted to show it to you and it
241:09 - was so much easy many people had asked
241:11 - this specific question in my upcoming
241:14 - video what I'm going to specifically do
241:16 - is that I'm going to create a PDF
241:19 - document with the help of gini API I'll
241:21 - show you different embedding techniques
241:23 - how we can convert a word into vectors
241:25 - and then how we can utilize for a normal
241:28 - document Q&A kind of thing with the help
241:30 - of Gemini API right so I hope you like
241:33 - this particular video this was it for my
241:34 - side all the information regarding this
241:36 - will be given in the description of this
241:37 - particular video thank you take care
241:39 - have a great day bye-bye

Cleaned transcript:

Lang chain is a framework for developing applications powered by large language models like gp4 in this course you will learn Lang Chain by building six inin projects with the open AI API Google Gemini Pro and llama 2 you'll also learn how to deploy projects to the hugging face platform Chris neack developed this course considering this here is an amazing video of the entire Lang chain crash course along with this we'll be discussing six plus endtoend projects llm projects using different different llm models from open AI we'll be using Lama 2 from meta we'll also be using Google giny pro not only that we'll also be using hugging face and we'll also be seeing how we can deploy this in hugging space uh platform itself which is provided by hugging face itself right so there will be somewhere around six plus end to end projects we'll develop both the front end back end I hope you enjoy this entire Series this is a long video so please make sure that you devote some some amount of time every day and try to practice as much more you can right so in order to push this video to many people out there guys I'll keep the like Target to 2,000 please make sure that you hit like share with all the friends share your work in LinkedIn in different platforms tag me over there I'll be very happy to see what all things you specifically developing so let's go ahead and enjoy the series and I hope you practice along with me all the materials will be given in the description of this particular video so let's go ahead guys so finally finally finally here is the langin series and this is probably the first video where I'm probably going to discuss a very good onehot video about Lang chain all the important components that you specifically need to know to build projects now why I'm stressing on langin is that because many people recently some of my students and some of experienced professional who switched into data science Industry they are getting work that are related to large language models and they're specifically using Lang chain and the community with respect to langin is also increasing so that is the reason I've created this dedicated playlist and I'm going to discuss a lot many things this video we will understand all the important competents of this Lang chain what you specifically need to know with respect to the Practical orientation and from the next video onwards lot of end to endend projects are probably going to come up uh in this video also I'll discuss about one Q&A project uh chatbot in short you know we'll try to use streamlet it's not like we only have to use streamlet you can also use flask you can use anything as you want right but streamlet it provides you a good UI you can also use gradio if you want right it is up to you so what all things we are specifically going to discuss first of all we'll try to understand the agenda and then we will try to uh do step by step each and every line of code will be done and as I said this is just like a kind of L chain one shot video just to give you an example that how we are probably going to create our end to endend application after we deploy that in a specific cloud like hugging phas right it provides your entire environment to probably host your application uh this is what we are going to create this chatbot see if I probably ask what is the capital of New oh sorry of India then you'll be able to see that I'm I'll be able to get the answer what is the capital of Australia many people get confused with the capital Australia please do comment down in the comment section what is the capital of Australia and let's see whether it is right or not so here you can see the capital of Australia scan so we are going to create this application at the end of the day but before that we really need to understand lot of important components in Lang chain so let's go ahead and let's start this particular session before I go ahead please make sure that you subscribe the channel press the Bell notification icon and share with many friend so that it will also be helpful for them and it will also be helpful for me so that you are able to provide this open source content to everyone out there many people require help and you should be the medium to provide this specific help by just sharing it right so what is the agenda we'll understand as I said this will be completely a practical oriented video it'll be a long video where I'll be talking more about practical things how you can Implement Lang chain how you can probably implement various important functionalities in Lang chain and use it probably to build an endtoend application so first of all we'll go ahead and do the environment setup this is commonly required you also require an open AI key API key so this also I will show you how it is done then we'll try to build simple applications uh with the help of llms three important things are there llms prompt templates and output parsel okay in llms you specifically have llms and chat models chat models are basically used in chat B to understand the context reference and all right I will also be discussing this practical promt templates can play a very important role and then coming to the third one that is called as output parser uh in short prompt template and llms and output parser gives a very good combination of output like it it'll give you a good output the output like you specifically want right so that is where output Parcels will also be used before to go ahead with what I will do is that uh first of all just go to the open aai uh website itself and get your API key how you should get it just go to this account and there you'll be able to see view API key and create the new C secret key right so once you create it give the key name and then copy it and keep it right I'm not going to share mine so that is the reason why I'm showing you this specific step now I will go to my vs code all my coding will be done in this VSS code itself here you can see it is completely blank right I've just opened a folder in the form of a project now all you have to do start your project over here now the first thing as usual what we need to do we need to create a new environment right so this is the first step that you specifically need to create with respect to an environment so don't miss this specific step it is important and probably whatever projects that we are going to discuss in the future we have to do the specific step so I will write pip install okay sorry cond create I'm creating an environment so let me just not open in Powers shelf instead I'll go and open in command prompt okay so here I will just right K create minus p v EnV V EnV python equal to 3.9 so 3.9 is the version that I'm going to specifically use and also going to give Y so that it does not ask me for the permission instead start creating the specific environment the reason why I'm using this environment understand one thing that guys for every project that we probably create we have to create a new environment that actually helps us to just understand or just use only those libraries that are actually required for this particular project okay so this is the first step go ahead do it with me you know uh and do it in vs code because vs code is a good idea if you want to do it in pycharm then also you can do it but since you following this tutorial I'm actually going to do it in vs code I feel vs code is good I I've used both okay I've used different different IDs but I feel vs code is good okay so here it is now here you can probably see my V EnV environment uh the next thing we will go ahead and activate this V EnV environment so I will write cond activate cond activate V EnV Dash Okay so this is my first step done I'm good with it now the next thing what I will go to do is that I will just go ahead and write my requirement. thxt right because we need to install the library since inside this particular venv environment so I will go ahead and write all the list of libraries that I'm specifically going to use now what are libraries I'm going to use over here uh we will just write it down so that uh the first library that I'm going to use is Lang chain then open Ai and I think I'll be using hugging face also later on it is called as hugging face Hub that I will show you as we go ahead okay so these are the two libraries that I'm going to specifically use okay now the next thing what I will do I will go ahead and write pip install minus r okay so let me just hide my face so that you'll be able to see pip install minus r requirement. txt so once this installation will take place that basically means my requirement. txt is getting installed that basically means all the libraries that I actually require over here that is getting installed and for this I require Lang chain and open aai okay so once this installation will will be taken place like it will be done now one more Library I'll be required is called as IPI kernel because if I really want to run any Jupiter notebook over here I have to use that okay so let's wait and let's see once the installation is probably done and then we will continue the video so guys the all the libraries that were present in the requirement. txt has been installed now the next step that I'm probably going to do is also install IPI kernel which will be required to run my Jupiter notebook so I will go ahead and write pip install iy kernel Now understand one thing I'm not writing this library in requirement. txt because when we do the deployment in the cloud IPI kernel will not be required okay so that is the reason I'm installing separately pip install IPI kernel in the same VNV environment because VNV environment also we are not going to push it right so you can probably see over here downloading this this will happen and automatically the download will happen itself right now the next thing what I'm going to do I'm just going to write Lang chain dot ipynb okay py NB so this will basically be my jupyter notebook that I will specifically be using okay so right now it is detecting kernel once this installation will probably happen and then we will also be able to see the kernel okay so this is all the steps all the basic steps that you probably require from this you can start creating end to endend project but at least you require this uh along with this I'm also going to add two more steps one is about environment files EnV file okay so here what I will do I will write Dov file okay uh inside this EnV file the reason why I'm uh writing this EnV file because I need to probably use my open API key and probably mention the open API key over here so if I probably write it like this and and whatever API key that I'm probably getting from the website I can upload it over here and using right uh load environment function right we can load this API key uh as a variable so that we can actually call our API API key over there so I will update this later on as we go ahead okay so till here everything is done this is my Lang chin ipynb file I will go ahead and detect the environment this is what 3.9.0 so here it is so let's see whether it is working or not 1+ one it's working right so this is perfectly all right so everything over here is done with a respect to this and I'm very much Happy this is working uh which is really really good okay so this is done now what I am actually going to do over here is that we need to import some of the important libraries like open Ai and all right so for this I will go ahead and right from Lang chain. llms okay import open AI see there there lot many models like open llms and all first we'll start with open AI understand one thing guys the reason why I say this is completely practical oriented because you need to have the basic knowledge of machine learning deep learning and all but this specific libraries is used to build application uh fine tune your application with respect to models with respect to your own data set most of the things just with writing proper lines of code will be implemented in an easier way so you really need to focus on things how things are basically done okay now the next thing what I will do I will write import OS and I will say OS do Environ environment okay and here I will give my opencore API underscore key okay this is how you should basically write it down to import the API key now what I will do I will keep this hidden from you so just imagine I cannot show you the API key because I will be using my own personal API key itself so I will go ahead and probably pause my video and update the API key and I'll remove this specific code okay that is what I'm actually going to do so that none of you basically sees that and it is important you have to use your own API key so let me quickly go ahead and do that and let me come back so guys this is how you have to probably import your API key this will I've made some changes so if you also copy it is not going to work I made some internal changes in between changes so uh you just need to write os. environment open API key and this API key that you have specifically got okay so this is the initial step uh I have also imported open AI so that I will be able to call this particular uh open AI itself now this is done this is good everything is working Absol absolutely fine now what I'm actually going to do I'm going to create my llm model and go ahead and write my open AI let's see open AI open AI function is the called or not let's see okay open AI from L chin open Ai and inside this open AI what I'm actually going to do I'm going to basically call a variable which is called as temperature and temperature right now you can keep the value between 0 to 1 the more the value towards one the more different kind of creative answers you may get right if the value is towards zero then what kind of output you you are probably getting from the llm model is going to be almost same from the uh anytime you number anytime you probably execute so here I'm just going to keep it as 6 so this is basically my open a llm model okay now this is done my llm model is there so here you can see did not find an open AI key please add an environment variable open AI key now this is the error that you are specifically getting right so why this particular error is probably coming you should definitely understand okay without this understand that this kind of Errors can come to you the reason why I will not edit this particular error because I really want you all to understand it is saying did not find open API key now what you can probably do with respect to this okay there are two different things that you can probably do either you can take this API key save it in a constant variable and try to use that particular variable over here right so for that also you can directly do that uh I have also created this do EnV file what you can do you can load this environment variable and probably directly read it over there right but let me just go ahead with a simple way you know so you will be able to understand with respect to that also so here what I'm going to do here I will go ahead and probably say open API key and here I will going to write OS do environment okay and here I'm going to Define my open API key okay now let's see whether this will get executed or not I will show you much more better ways when we are probably executing our end to end application so let me go ahead uh open a key is not defined because I use double equal to perfect now you can see that it has got executed perfectly now when I am actually creating an end to end project I will show you a better way the most efficient way that we should specifically use when we are building an end to end project but right now I'll go to focus like this now understand one thing is that with respect to temperature variable right the temperature that we specifically used I will give a comment over here and you can probably see over here right so temperature value how creative we want our model to be zero means temperature it is it mean model is very safe it is not taking any bets it will risk it might generate wrong output may be creative so more the value towards one the more creative if the model becomes right it is going to take more risk to provide you some more better but again with respect to risk again there may be a problem you may get a wrong output perfect this is the step simple step that we have done at the end in this video only I'm going to probably do create an end to end project understand this will be very important for everyone because as we go ahead in the next videos the project difficulty will keep on increasing okay now this is done now what I will do quickly I will go ahead and write text let's say the text is what is the capital of India okay so here I will write print llm do predict and here I'm going to basically write my text so here you can see the capital of India is New Delhi so here what I have done is that this is my input and if we use lm. predict I'm going to probably get the text so if you like liking this video till here guys as I'm teaching you step by step I'm explaining you each and everything please make sure that you practice in a better way right if you like it please make sure that you subscribe the channel also okay so this is done lm. predict and we are able to probably get the output okay so understand what what all things we did we created an open AI model right but here in the open AI right you should also understand one important thing here there is a parameter which is called as model now what all model you can probably bring it over here what all model you can probably call so here I will go to my documentation page okay and uh if I probably click on the models now here are the set of models that you can probably use by default it is calling this GPT 3.5 turbo it is the most capable GPT 3.5 model and optimized for chat at the 110th cost of a Dy 003 we'll be updated with our latest model iteration 2 weeks after it is released it's not like you only have to use this you can use this you can use this you can use this you can use this whatever models you want you can probably use it if you want to probably go with GPT 4 you can use this you can use this you can use this but at the end of the day gp4 is the most uh amazing thing it is more capable than GPT 3.5 so this is what models is by default over there it is probably taking this specific model which you can probably use it now as we go ahead it is not like you can only call this model itself in hugging face you have open source models also right from Google from different companies you have that uh open source llm models you can also call that and I will also show you an example with respect to that okay so till here I hope you have understood it I think you should give a thumbs up if you're able to understand till here now let's go ahead and do one more thing I will also show you with respect to hugging face now okay now with respect to hugging face what I will do I will quickly go ahead head and write hugging facehub right I will probably install this library now let me go ahead and open my terminal okay so I will delete this and quickly we will go ahead and write pip install pip install minr requirement. txd done so this is getting executed and then I will probably show you with respect to hugging face also so hugging face this is done okay the installation is done so you have to use hugging face Hub now in case of hugging face also right you will specifically get a you will also be getting a because at the end of the day we'll also try to deploy it over here if you go to settings right and if you go to access tokens here you can probably see I have some kind of token right and with the help of this particular token only you will be able to call any models that are available over here so in this models if you probably go ahead and see there are lot of llm models that are available right so if you probably go over here natural language token classification question answer like uh let's say text to text generation this is basically one kind of llm model I will search for one name okay so the name is flan okay flan T5 base okay so this specific model or T5 large this is a text to text generation model right so this is also an llm model if I want I can use this also why I can use this because this is an open source okay if you want to probably see the answer right text to text generat false or not false or false is uh if I want to compute it right it'll give me some specific output okay false or false or false is the verb the verb right something like this it is getting an output right so I can also use my I can also create my chat models with the help of this kind of models directly by using this API right but how to do it let's go ahead and see it okay so here what I will do quickly I will first of all import one more important thing again in my environment variable os. environment and here I'm going to basically write in the case of hugging P I have to use something called as hugging oops I have to write in capital letter hugging face hubor API uncore to token okay so here is my token okay hugging face Hub _ API _ token now with respect to this particular token I have to write my own token the token is basically given over here as I have already shown you if I probably click on show you'll also be able to see it so I'm not going to do the show part over here so what I will do I will just pause the video upload it execute it change the token and then come back to you okay so let's go ahead till then you can go ahead and create a hugging face account also so guys now I have set up my hugging face Hub API token with this specific token again I've made the changes so if you probably use this again it will not work okay but I have actually imported it now let's go ahead and probably see how we can probably call llm models with the help of hugging face so again and again I'll be using Lang chain Lang chin is just like a wrapper it can call open llm models it can call hugging face uh llm models anything you can probably call with it okay so that is the reason why I'm saying it is powerful so I will write from Lang chain okay import hugging face Hub okay and then here I'm going to probably Define my let me just execute it and then probably I will call it so here I will go and write hugging face Hub here first of all I need to give my repo ID now repo ID is nothing but uh whenever you search for any model this will be your repo ID Okay Google slash this okay so here I'm going to probably go ahead and Define it okay and this will basically be given in a sentence form okay so this is done now the next thing that I will have is model quarks okay and here I will go ahead and Define my temperature and then here I'll go ahead and write max length column 64 that basically means I'm giving the maximum string length to 64 okay now let's see whether this will execute or not I know it is going to give us an error let's see so it is not giving you an error it is probably executing it perfectly it has probably taken that particular key itself inference API is working perfectly everything is working so I will go ahead and Define my variable so here I'm going to basically write my l lmore hugging face okay is equal to this one right so this is done we have probably written this specific thing over here and then I will go ahead and write name or I will see output let's see the output and here I'm going to basically write llm do hugging face do predict and let's say I'll say can you tell me the capital of of Russia okay so let's see whether we will be able to get the output or not so here I'm going to write print output and let's see whether we are so here you can see the output is a simple word right the capital of Russia is Moscow right but here the previous output that we saw with respect to the llm models here it shows that capital of India is New Delhi it is giving you entire sentence and probably here it is just giving you a word and this is what the difference is with respect to an open source uh model itself and models like GPD 3.5 or 4 right so here uh you can probably see this okay and I will also execute one more thing over here let's see uh can you write a poem can you write a poem about AI let's say I give this what kind of poem it gives okay it probably taking time to probably give the I love the way I look at the world I love the way I feel the way I think I feel I love the way I love see what kind of output you're specifically getting right now if I probably give the same output and probably write llm okay dot predict llm dot predict and if I probably give the same sentence let's see what is the output now you will be able to understand why we are specifically using this let's see it should give you a better output I think I love the way I look at the world oh so mysterious was so curious It's technology so advanced IT Evolution so enhance it's a tool that can assist um in making life much less hectic a force that can be used for good or a cause that's misunderstood it can make decisions so Swift and learn from the sto Swift tool that can be better see so how what an amazing poem it has basically written and by this you'll be able to understand the differences why I'm specifically using this open AI model and with respect to hugging face yes there are also some models which you can probably take paid one in hugging face that will give you better output but this is what happens with respect to open source uh models I think we should uh we have lot of Open Source models that are probably coming up Mr 7B and all that also we'll be seeing in this playlist as we go ahead but as I said this is just a basic thing to probably understand we will be focusing on understanding these things right so here uh till here I hope we have discussed so many things how to probably call open AI models with respect to open AI uh Library API itself and then hugging face API also llm models right and using Lang chain so here are what all things we have done now the next thing that we are probably going to discuss is about prompt templates prompt template is super amazing uh it will be very very handy when we are talking about about things with respect to prom template and all so that also we are going to discuss as we go ahead so guys now let's go ahead and discuss about prompt templates which is again a very handy component in the Lang chain even in open AI with the help of prompt templates you will be able to get efficient answers from the llm models itself right I'm not talking about prompt engineering that can get you three CR package okay I'm just talking about simple prompt templates and how prompt templates can be probably used okay now now let's go ahead and first of all import from Lang chain from Lang chain. prompts import prompt template okay uh what we are going to do is that whenever we call a specific llm model you know the llm model should know what kind of input it is probably expecting from the client or from the end user and what kind of output it should probably give it okay so what we do if you want to really Define how our input should be and how our output should be we can specifically use prompt template because understand if we directly use GPD 3.5 it can be used for various purposes right but here I want to restrict all those things within something right so with respect to the input and the output so here I'm going to probably Define my promt template so let me go ahead and Define my promt template now with respect to the promt template here first thing that we need to Define is our input variables so here I will go ahead and write write my input variables okay now in input variables we need to first of all say that what input we are specifically giving there will be a fixed template in that template I really need to give my input okay so let's say here I Define my input and here I'm just using one input let's say the capital or the country I'm just going to write it as country okay so this is my first parameter that I'm going to probably give in my template itself and this I will also store it in my variable which is called as promt template okay great now here I'm given country as my input variable okay here I will Define my template now inside this template what I'm going to say is that tell me the capital capital of this whatever template that I'm specifically giving that is country so this is just like my variable whenever I give the input to this variable it is going to replace it over here right so what will happen by this is that the open AI will be able to understand okay this is the question that is asked but this value is dynamic that I'm probably giving it during the run time something like that okay now here if I want to execute this again I can also write prompt uncore template okay prompt undor template uh dot format right so there is a function called as do format and here now instead of how should I give my input so here I will say country is equal to right whatever the variable name is country is equal to let's say India okay now here you can see that my entire prompt is generated in this way saying that tell me the capital of India okay so here you can probably see tell me the capital of India right now if I want to probably predict I will write llm do predict okay and here I will just say prompt template is equal to whatever my prompt template is defined so here I'm getting an error predict missing one required positional argument text now inside this I've given this prom template but it is expecting one text right what is that particular text that that is particular this particular value right so here what I will do I will go ahead and Define my second variable text and here I will write it as India let's see whether it will get executed still it is giving me an error so guys now you can see when I'm using this llm do predict and I've given my prompt template I've given my text also that was the error that was coming but still it is giving me an error saying that the prompt is expected to be a string instead found class list this this if you want to run the llm multiple prompts use gener instead okay something like this I will show you a way because that is the reason the reason I'm keeping this error over here there is a simple way of understanding things right because this is not the right way to call promt template along with the inputs itself so what I'm going to do quickly I will go ahead and import one important thing that is called as chains so I will say from Lang chain dot chains import llm chain understand one thing guys chain basically means combine multiple things and then probably execute I have my llm model I have my prompt I know I have to give my input based on that input I need to probably get the output so instead of executing directly by using llm do predict I'm going to use llm chain and inside that I'm going to give my llm model I'm going to give the prom template and I'm also going to give the input so this is what we are specifically going to do now this will get gots executed from Lang chain. chain import llm chain now I'm going to create my chain let's say the chain is equal to llm chain and here I'm going to basically write llm is equal to llm okay whatever is my llm model and my prompt is equal to my prompt template okay now this is there this is my chain not chain what it is doing it is combining the llm model it knows what is the prompt template right I'm going to use both of them and now in order to run it so I will write chain. run and here I will say India let's say I'm going to probably say India and I know what is the output it is going to get tell me the capital of India so if I write chain. run it is definitely going to give me the output the capital of India is New Delhi this is perfect right so I can also probably print it right see guys I'm not going to delete any of the errors I want you all to see the errors and then try to understand that is the best way of learning things okay other than this there is no other way right you need to find a way to learn things you don't worry about any errors that are probably coming up you just worry about okay fine you have got that error how you can probably fix that what is the alternate way of probably fixing it right so you can probably use all these techniques as we go ahead right so this is with respect to prompt template and here I'm going to also talk about and llm chain right so these are some important things because all these things we will probably be using in creating our end to end application now let's go ahead and probably discuss some more examples with respect to llm so guys now we are understanding one more important topic which is called as combining multiple chains using simple sequential chain till here we have understood about llm chain LM chain was able to combine an llm model along with the prompt template through which we can also give our input and get our specific output now if I have multiple chain let's say if I'm giving one input I want to use those input in both the chain or three chains four chains how can I specifically do it and for that I'm going to probably use Simple sequential chain okay so let us go ahead and let us probably see how this can probably be done so first of all I will probably say Capital template prompt Capital prompt okay so first like what is the capital of this specific country right so this will basically be my prompt so here I'm going to use my prompt template and here I'm going to basically use input variables is equal to and here I'm going to basically say country let's go to the next input the next input will basically be template and here I will say I want uh please tell me the capital of of the whatever input I'm specifically giving away as country so this becomes my first template right and what I will do I will create a chain the chain name will be Capital chain okay and here I'm going to probably use my llm chain and my llm model will be llm okay and then I will also be using my prom template is equal to as capital template capital template okay so this is done let's see Capital prompt what is capital prompt oh sorry Capital prompt Capital prompt is not defined why uh please tell me the capital of this uh template oh double equal to Let's it no worries uh two validation error for LM chain so first I've used an LM chain where prompt template is equal to this uh where it is capital prompt so guys after just checking the documentation this should be prompt itself okay because in llm chain we have used prompt and here is capital template here also I'm going to probably use Capital tempate now if I execute this this works absolutely fine uh one thing you can probably see over here that I've given my template name and then I've also given the capital chain right so if I want to probably execute it I can just give my chain. run and that particular parameter okay but now what I want is that I also want to create one more prompt template I want to give the same input to that chain also so here uh let's say I will write famous _ template and I will just say promp template and here again my input variable what is my input variable so my input variable will be whatever specific things that I'm trying to give right let's say please tell me the capital of uh India if I say right the capital whatever Capital I'm going to get that variable only I'm going to pass it over here so my input variable will basically be my capital okay and this will be my second one and here I'm going to probably sayest template and I'm going to probably ask a question suggest me some amazing things amazing some amazing places places to visit in that specific capital okay so this is what I'm probably telling right please tell me the capital of the country so I will have that capital information that will be my input variable to from this particular template uh in that specific chain okay so I will get two answers first of all I'll get the capital of that particular country and then what are the some amazing places to visit in that specific Capital place okay so these are all the information that I have put up okay so I hope this also works fine now now what I'm going to create I'm going to create an another chain which will be for this particular famous chain right so here I'll write famous chain okay is equal to and I'm going to probably use my llm chain oh llm chain and here I'm going to give my llm models but the second one that is my prompt is equal to uh whatever template that I'm going probably going to give the famous template okay so this is what I'm probably going to do uh and I've probably given this prompt also over there and this will basically be my chain so once I probably execute it both the chains are ready now I need to give one input it should go to one chain get the output from that particular chain and pass that output to the next chain okay so this is what I specifically want to do how can I do it so again from Lang chain do chains I'm going to import simple sequential chain I know guys uh here you may be thinking why I have to use this see you're passing one input to the get the other output from the one chain and pass that particular output to the other chain to get the just output itself right so this is quite amazing when you see an end to end application there you'll be able to understand these are some of the important components you should definitely know and try to understand okay so here finally what I'm going to do is that I'm going to probably create my chain is equal to and this will be my final chain and here I will probably say I'll import this okay so I get that so chain is equal to simple sequential chain capital letter simple sequential chain and inside the simple sequen chain I just have to name all my chains what all chains are specifically there in the form of list so the first chain uh that I have over here is nothing but Capital chain the second chain that I have is something called as famous chain okay so both the chains are ready now in order to run it all I have to do will write chain. run and here I'll specifically give India okay done let's see what kind of output I will probably get so it is running it is a bustling Metro Police and a great place to visit for historical site cultural this this this red Fort see most popular city the iconic monal is a multivisit who fought in World War one the 16th century mugal era Tom in UNESCO world heritage site everything it is probably giving it right so it did not give us the first answer with respect to the chain because it only provides the last input information okay uh if you want to probably display the entire chain I will show you a way how to do that for that we have to use buffer memory uh there will be something called as buffer memory but one amazing thing I gave one input I got the output and then probably I pass that particular output to my next chain and I able to to get one amazing out answer over here so definitely try it out from your side by using different different examples also now what I'm going to do is that I'm going to probably discuss one very important component about chat model open AI so that is also super important uh that is something related to chat models whenever you probably want to create a chart models you can have to use that okay so let's have a look onto that so guys one more additional thing that let's say I want to probably see the entire chain so here we will specifically use something called as sequential chain and let me just show you one example of that also uh it is not much to do with respect to that but you should definitely know this important video as said again I don't want to probably take more time with respect to this but it is good to know this okay sometime when you are developing things and that you'll probably be understanding once I start end to end project right today one end to end project will be done okay don't worry about this uh in this particular video it will be done uh but definitely I want to show this example also as we go ahead now now uh let's quickly go ahead and do the same thing I will copy this entirely okay I will paste it over here okay now along with llm promt template I will give my output key also so where I specifically want my output key so the output key will be nothing but it will be something called as capital okay so this is my Capital chain with this specific output okay so here I have created this now let's go ahead and probably create the next template uh that is this famous template okay so here also you can probably see the famous template uh suggest me some names of the capital and here I've probably created my template name uh and my chain is over here right so this will basically be my chain okay so the same name whatever output Keys over here I've given this as my input key and here uh I can also derive one output key like this output key places something like this okay so done this is done see two simple templates that I actually created uh s me some amazing places to visit in this particular Capital uh the capital is probably given from here so now the chain will probably be able to understand each and everything as we go ahead you know where the output is and all right so here now what I'm going to do I'm going to probably import from Lang chain do chain chains import sequential chain okay and then finally you can see I'll write chain is equal to simple okay I will let me just execute this because it is not giving me any suggestion so I will write chain is equal to sequential chain and now I'll Give All My Chains name so the first chain name is um to Capital chain D famous chain dang okay and uh after this you will basically be able to understand the input variables now the input variables that we specifically have input variables is nothing but whatever is my variable name what is the variable name in this case it is nothing but country okay and then my output variable I'll also create my output uncore variables so these are the two parameters see guys this this parameters is nothing but whatever parameters I'm specifically giving one is the capital and one is the places done so this is my entire chain now if I want to run any chain what I will do I'll basically write something like this and give what is my country name right so it should be given in the form of key value pairs so here is my country colon India right something like this now if I execute it I will now be able to see my entire chain right it'll take some time so what I have done over here I have in every llm chain that I'm probably creating I'm creating an output key uh two chains so two output key and here you can see chain country India country was India Capital the capital of India is New Delhi here are some amazing places to visit in New Delhi and all the information I have probably over here now let's go ahead and discuss chat models uh specifically in Lang chain and we also going to use one Library which is called as chat openi uh this is also very good if you want to probably create a conversational uh chat bot itself so in chat models with chat open AI first of all what we will do is that we will go ahead and import Lang chain uh do chatore models and I'm going to probably import chat open aai so we will quickly go ahead and import it now after I specifically import this in chat open AI there are three schemas that you really need to understand okay whenever a conversation basically happens if a human is probably giving some kind of input and expecting some response that basically becomes a human message right if by default when the when your chat bot is probably opening a default message will probably come right and that is something related to domain like what that specific chatbot does right so it can probably come up with a new schema which is called as system message and then there is also one more message which is called as AI message which is again a schema which probably gives the output right whatever the chatbot is giving an output uh the AI whatever models is specifically giving the output that is nothing but that is related to the schema that is called as AI message okay now from this here we what we are going to do we are going to import from langin do schema I'm going to import human message system message right as I said system message is also required and AI message here everything you'll be able to get this as an example because uh probably in the upcoming videos we'll create conversational U chat bot right at that point of time we'll be seeing all these things what we'll be using and how we will be using okay so here quickly we'll import this now uh obviously my llm model is there right by uh and while creating before the llm models how did we use it we basically use something called as uh open a right we used open AI now in this case I will probably copy the same thing okay and I will just past it over here and write chat llm okay and instead of writing open AI I'll use chat open AI right so this is what I'm specifically going to use chat open AI with some temperature in this and here I'm also going to give one model so let me go ahead and write my model name uh the model name that we are going to basically write from here is GPT 3.5 Das turbo right I showed you like what models we can specifically use so this is my chat llm model so here if I probably go ahead and write my chat llm so here you'll be able to see that it is a chat open AI uh and with all the information with so temperature what is the uh this and all open AI key I cannot show you so it shows you open AI key also over here so going to remove this okay so that you don't find the opening I key now let's use this three schema and then probably see how my output will look like let's say first of all I'll create the schema in the form of list okay first of all the system message right let's say system message I will go ahead and initialize and I'll write a content content one variable is there I say you are an you are a comedian AI assistant okay so this is the this is what I'm telling the chatbot to behave like right it is basically acting like a comedian AI assistant okay then in the next one I will say human message and here again I will go ahead and write the content and I will say please and this is what I will probably write as a human this is what the input that I am probably giving right so I'll say please make a comedy about or please provide some punch lines some comedy punch lines punch lines on okay AI okay so here you can probably see these are my two things these are the two information that I'm going to give to my chat llm models right and then let's see what is the output okay with respect to that now in order to give this input to my chat llm so I will write chat llm and here only I will open my brackets so it has two information first by default it knows the system is a comedian AI assistant and here as a human input what we are saying is that we are saying please provide some comedy punch lines on AI okay so if I execute this you will be able to see I will be able to get an output now this is how we are going to design later on in the end to endend project we are not going to give this as a hardcoded it'll be quite Dynamic so here you can see AI message see this is the output if I'm getting the output that basically becomes an AI message so this schema that we are able to see from the output of this particular chatbot the system message is basically telling that okay beforehand you have to act something like that we instructing the chatbot to probably act in that way right the human message is basically our input and AI message is what is the output so AI may be smart but it can tell me if your output makes look like a potato AI is a virtual therapist except it never judges you for eating an entire Pizza by yourself something like this so this is what comedy messages you can probably see right and I think this is quite amazing and you can probably set this up any number of time right you can probably say you can add this AI message over here and you can still build more conversational AI right so as AI also give the message you can probably store it inside this s let's say if I probably consider a list and I append this particular list with all this information it can act as a chat model as our as we go ahead right now guys we are also going to discuss about one topic and after this we are going to implement our project okay so over here we going to discuss about prom template plus llm plus output parcel now first of all we'll try to understand what exactly is output parcel now in order to make you understand about output plaster and how we can probably implement it I will use Lang chain again for this um as said guys langin is a powerful Library it has everything as a wrapper right so I will say from Lang chain okay from Lang chain. chatore models first of all I'm going to import chat open AI okay chat open AI I see there so many things Chad vertex AI chap open AI very powerful very powerful and the way it is getting developed right quite amazing right so from Lang chain dot prompts I'm also going to use some prompts and like how we have a prompt template when we use open AI right similarly in chat open AI we use uh prompts which is basically called as chat promate chat prompt template okay so I'm going to basically import chat prompt value no template chat prompt template so I'm also going to import this along with this as I said output parser right output parser is that if I want to modify any output of an llm model beforehand right so I can specifically use output parsers so for Lang chain I will also import this from Lang chain do schema import base output parel right so these are the three things I'm specifically importing and here I'll basically go ahead and write class let's say I am defining one output parser and I'll Define this in the form of class it'll inherit the base output class so let's say uh I will say comma separated output okay that basically means it is basically called as a comma separated output this is the class that I'm going to Define and uh even even in the documentation it is given in an amazing way okay so comma separated output and this will basically be inheriting the base output parcel okay now inheriting when I probably inherit right that basically means we are inheriting this base output par and we can call this along with an llm models here I will Define a parse method and inside this par I will take self as one keyword and whatever text the output that we have specifically getting which will be in the form of string format all we'll do we'll just write return text. strip dot dot split right and this will be a comma separated split understand one thing now this is what is the class that I've defined and this is just like an output parser by default the output parser is what you can probably see whenever I'm specifically using the chat models I'm I'm getting some kind of output right AI may be smart something it is giving in the form of sentence and it is adding a new line at the end but what I'm saying is that whatever output I'm getting I will take that output and divide all the words in comma separated okay something like that so for this again I will Define my template I will say you are a helpful assistant okay so this is my first message that is probably going as a template right so this becomes a system template the schema that we probably discussed right I will also give some information um let's say when the user gives any input okay you should generate five words okay in a comma separated list so this is what is my entire message okay the template I'm saying that whenever the user give any input you have to probably generate five words which should be comma separated okay so this is what I have specifically done okay now what will be the input what will be the text Will Define all those things okay and here I will say this will be my human template so what is the word that I'm going to probably give over here uh that will specifically defined over here right so here I will say Okay test uh you should generate five words synonym let's say synonym I'll just go ahead and write synonyms okay synonyms in comma separated so here will basically be my text whatever text I'm specifically given now I will go ahead and create my chat prompt now again from this chat prompt what I have to use I've already used chat prompt Pro template okay and inside this I will say do from message let's see that chat prompt template from from underscore messages okay now inside this from underscore messages I have to give two information okay whatever is the template right so first template is nothing but the system one so system information that I really want to give uh that system one is nothing but this normal template that I've defined and the second one will basically be my human template right whatever human message that I'm actually giving right and this will basically be defined as human uncore temp temp plates template right so once I execute it here you'll be able to see this is my chat prompt okay now in order to execute this obviously I have to use chain right because I have a prom template over here I have a human text I have this specific template also so how do I probably combine all these things that is what I'm actually going to show you over here so quickly first of all I will use this chat llm okay chat llm now see this is quite amazing and this is the best way of running chains so I will say chain is equal to whatever is my chat prompt so this is my chat prompt to this chat prompt I will give my API whatever API I'll write over here control V so I have to just give a or sign kind of thing right so this is getting chained up this symbol basically says that it is getting chained up and remember the order also okay or you can also initialize chat open AI over here now along with this I will also give my output parser the output parser will be the last one right so this will basically be my output parser comma separated output okay now see I've given each and everything over here one by one list by list right so here it is so once I probably execute it it will get executed so here what it is saying I'm giving this chat prompt the chat llm model is there and the output should be comma separated output which is getting derived from this particular class okay now here finally what I will do I will write chain do invoke and inside this I will again whenever I use chain I have to probably give it in a key value pair right colon something whatever the value is now in this case I will say the word is um intelligent let's say now I have to probably give in the form of text right so that is what I really have to give it right so this text is equal to in now let's see what is the output uh it is coming as okay there is some syntax issue that I have probably made because I have to close my dictionary over here now if I write chain. inor you can see that five words smart clever brilliant shop aute I don't know this specific word but here you'll be able to see whatever output that I'm probably getting right so if I probably remove this let's see okay this is how the output will look like okay AI message content this this this right but just by adding this output parsel you can see what an amazing message you're able to get and you're getting able to get the right thing that you specifically want this is what powerful a prompt template is all about right now this is done right and this is more than sufficient to know because more new things about PDF how to read PDF how to what is text iding and all we will discuss as we go ahead but now let us go ahead and try to create a simple chatbot okay simple chatbot I'll create an app.py and by using the simple chat bot we'll try to understand how things actually work and what all things we can basically do okay again here I'm going to probably use streamlet and I'll be writing the code line by line so let's go ahead and have a look so guys finally now we are going to develop our Q&A chatbot uh with all the concepts that we have probably learned I'm just not going to use all the concepts right now in this specific video itself because we should have probably uh 10 to 12 projects that are going to come up in the future in this series of playlist so there we are going to discuss about more projects as we go ahead right but now in this video I'm going to probably create a simple Q&A chatbot just with the help of open aai Lang chin and obviously use openi apis and llm models specifically to do that that here I'm also going to use streamlet okay so let's go ahead and let's see initially what all settings I need to do see this was Lang chain. iynb because I will be giving you this entire file uh again in the reference with respect to GitHub also so uh first of all in the requirement. txt I will be importing one more library because I need to install this Library this is the important Library itself which is python. EnV right so python d.v actually helps us to create or upload all our uh environment variables that we have created with respect to our application so here this is the library that I have to do it and just go ahead and install the requirement. txt I've already done that uh and this will be a specific task to you now we are starting over here so from lin. llms I have imported open aai then from EnV load load. EnV so as soon as I probably call this it will take all the environment variables from EMV file so here I've already created the environment variable I'm not going to show you again the environment variable because in short the environment variable will be something like this see I I I may have written like something like this open AP opencore API _ key is equal to this particular environment variable right so this is basically my open uh API key itself right so I'm going to probably use this uh in my application so here uh these are the basic steps that we will probably go ahead with now along with this what I'm actually going to do I'm also going to to import one more Library which is called as streamlet because we are going to use streamlet itself so let me go ahead and open my terminal and quickly let's go ahead and write pip install minus r minus r requirement. txt and then the installation will start taking place and the streamlet uh Library will also get installed Streamlight we are specifically using for a frontend application uh see it's it's not light only you have to use streamlet it'll be very much easy for me to probably create it and do the deployment because I'm also going to show you the deployment in the hugging face uh space itself right what is exactly hugging space uh space I will also discuss about all those things so quickly uh let's do this uh it'll probably take some time and then I will go to my app. Pui uh let me do one thing quickly uh let me go ahead and import streamlet also so I'll import streamlet as St okay so this will basically my streamlet itself okay so it'll probably take some time to download it let's see how much time it is going to take but again it depends on your internet speed and how fast your system is right my system is really really fast till it is taking time so for you it may probably take more time okay so let this installation take place till then I will go ahead and start creating our application now I will first of all create a function to load open AI model and get response okay get response so I will call this function something like definition um getor open AI response okay something like this and here I'm probably going to give since it is a Q&A chatbot so here I will have my question as my parameter which will be of a string type okay it can be a string type it can also be a numerical type so I will just keep it like this okay so this is done and here you can also see the installation is done so I will just close this now here as soon as I probably call this function what I really need to do I need to call my llm model so llm model I will say open AI okay open Ai and here I will go ahead and Define my model uh have I imported open AI yes I have imported model uh uh open itself so I will go ahead and write model uncore name is equal to and I will will Define my model I'll be using text Davin C uh this is one of the model that we have you can probably refer it so text Davy 003 and here I'm also going to Define my temperature temperature is equal to let's say5 Okay uh along with this uh I'll just go ahead and copy one more thing I will just set up my open API key also so I will set it up like by using this OS environment so here will be my first parameter okay so all this is done uh I think I need to also import OS okay so this is done in short what I'm doing is that I'm initializing my llm model OKAY in this code now the next thing is that I need to probably get my response so response will be nothing but llm directly how do I give a question over here I can probably give the question over here okay see I'm just creating a basic one then whatever things you really want to do from here you can probably do it try to create a own prom template try to use chain if you want try to do multiple things but just to start with I'm going to use a simple application where it is just taking an input and giving some kind of output it has no AI message set it has no human system message set no system message set also we have not given any prompt template also over here this just to give you an idea how things starts okay so now we will initialize in initialize our streamlet app okay now with respect to streamlet I will write STD do set underscore page underscore config so this is one of the function in streamlet which will actually help you to set the page title so I will just go ahead and write title is equal to I will say q& a demo okay Q a demo and this is done with respect to my uh and here I will set my another header the header will be something like Lang chain application something like this okay so I've given my header also with respect to this okay now I need to find out a way to get a user input okay uh if I get a user input then I should be able to submit the button and I should be able to get the text itself so first of all I will go ahead and create my submit button I will say St do button and here I will go ahead and write generate generate or ask the question something like this okay if ask button is clicked right if it is clicked that basically means if I write if submit okay if it is clicked this usually becomes true okay so if this is true it is probably going inside this particular function and here you'll be able to see s I'll just put a header and uh I'll say the response is okay and then I will probably write St dot right with respect to the response okay so this is what I am probably doing it okay I'm getting the response over here and with respect to this specific response I will probably this response is probably coming from here but still whatever is the input that input we are not able to capture it yet right because if we capture those input then only we'll be able to send that particular input somewhere right and for that also I may have to probably create another function so let's go ahead and handle the input part now so guys now what I am actually going to do over here is that first of all we'll go ahead and and capture our input text so let me go ahead and write over here input is equal to St do textor input because I'm going to use a text field over there and here I will probably be waiting for the response itself right so sorry from the request right so so here I will write input colon okay and I'll keep a space over here and I will just write key is equal to key is equal to input something like this so this will basically be my input itself okay now once I've done this okay once I've have done this I'm going to take this particular input and now call I hope you should know what we should call we should basically call this function right so this here I'm going to probably write uh not here itself uh so let me just write it over here and this input I'm actually giving it over here okay so this will basically be my input over here uh whatever input I'm probably getting it it'll just go ahead with respect to this particular question and I will probably get the response here I will just go ahead and write return response okay and then I will store this particular variable inside my response okay done see the way I probably got the input over here I sent this input to my get open AI response my open AI model has got probably loaded and then it is basically calling with respect to this llm you can either call predict message or predict functionality also uh you can also use chain you can use multiple things you can assign promt template in this particular function and all right and then finally you have S do Button as the button and if submit this is there okay now quickly let's go ahead and probably run it okay uh let me see whether if I directly call python app. Pui it will give us a error why because it is a stream L library right if it was flask I would have probably said okay it would have work ke now it says key error open API key okay so os. environment open API key load. EnV so guys one mistake that I have specifically done whenever I really want to call all the environment variables from EnV with the help of load uh this the specific library that is called as this this functionality which is load. EnV at that point of time I'll be using get EnV function and here I will just remove all the things brackets and probably call this function now I hope so it should work and I think we should not get any problem so Streamlight run app.py and here we have our entire application quickly it's running let's see um this is getting loaded and here we have right now probably I'll ask the question what is the capital of India right so I'll just ask the question over here the response is the capital of India is New Delhi um let's see what is generative AI right so I'll ask the question you'll be able to see that generative AI is a type of artificial intelligence that focuses on creating new data from existing data this this this is there still I'm I'm getting some kind of weird responses over here so that is the reason we'll also be using output parsers we'll make sure that we'll use conversation buffer memory we'll also Implement schemas like human message system uh human human system AI system um system messages all those things were there right all the schemas that part we probably discussed but this is a simple application that we are probably going to discuss with respect to this it is going to be quite amazing and uh you know this is just a basic Q&A chat bot uh wherein whatever questions you specifically ask like what is the please right write a poem on on please write a romantic poem I'll just give it as romantic poem on generative AI something like this because many people are now using this ask the question so here you can see gener way I knew love in my life your data driven hurt is perfect fit your algorithm so precise your knowledge is so wise so everything is over here now what I'm actually going to do is that I will go and show you the deployment part everything is working fine I will first of all login into the hugging face go to the spaces and create a new space because I'm going to probably do the deployment over here let's say I will say Lang chain Q&A chatbot okay I don't have to use license this will be a streamlet now in space Hardware like you have paid Hardwares also but I'm probably going to use a simple one CPU Basics 2v CPU 16GB and I will create this as public so that you can also refer it um let's see okay I will just remove this spaces please match okay this underscore is not there QA a chatbot I will go ahead and create the space now after creating the space uh there's couple of things that I'm probably going to do over here is that uh this is where this is just like a GitHub repository you know you if you probably go to the files you'll be able to see it now here I'm probably going to upload the file that I have okay uh but before that what I'm actually going to do I'll go to my settings okay and if I go down right so there will be something called as secret keys because this secret key I have to put it with respect to open AI so here no secrets are there so I will go and clear or click on uh new secret and you know that with respect to the new secret what I have to probably use I have to use open API key I will put it over here okay and now oops just a second open API key let me open this and I will put it over here okay and what I'm going to do I'm also going to upload the value okay so I'll not show you the value uh let me update this and let me come back and quickly and show you the next steps that we are probably going to do after adding the open AI API key uh you can see it over here in the secrets you'll be able to see this specific key now what I will do after updating that I will go to my app now again my entire application will start getting buil up now here you can see as soon as I add the open AI API you'll be able to see my application will start running now here I can probably ask question what is the capital of India okay and uh you can see that I will be able to get the response now clearly you'll be able to see uh I've been able to do the deployment in hugging pH spaces uh it was very much simple you can see the files over here itself on the app.py requirement. txt I had commented out all the codes with respect to EnV and all because soon as we add the secret variable as soon as this open a model is called it is going to take the open a API key from there and it is going to use it over here so yes this was with respect to the deployment and quickly we were able to also create a simple Q&A chatbot along with deployment so guys in this video we're going to create one Amazing llm Project which is nothing but PDF query with langen and cassendra DB uh cassendra DB will be probably creating in a platform which is called as data Stacks so if you have probably heard about this particular platform which is called as data Stacks which will actually help you to create cassendra DB in the cloud itself and why this platform is quite amazing because from this you will be able to perform Vector search and whenever we talk about this kind of documents or if you want to really create an Q&A applications from huge PDFs itself Vector search is the thing that you really need to implement now before I go ahead let's first of all understand the entire architecture we will be solving this entirely step by step what are the steps specifically that will be taken to probably complete this specific project that we really need to understand so let's begin with the architecture initially let's say you have a specific PDF this PDF can be of any size and any number of pages first of all we will read the documents and understand here we are going to use langin as I said because langin has some amazing amazing functionalities which will actually help you to perform all the necessary tasks to create this specific application now first we will go ahead and read the documents that is specifically the PDF and the first step usually when we whenever we work with this kind of data set is with respect to some kind of transformation we really need to do so after reading this documents we will convert this into various test chunks that basically means we'll split the data set into some kind of packets right so this text Chunk will be of some specific size based on the tokens that we are probably going to use so over here you can see some example reading the document and then we have divided this into some chunks then we will convert all this chunk into text embeddings now from here we will be specifically using open AI embeddings okay openai embeddings actually helps you to convert text into vectors now why you specifically require these vectors I hope you have heard about text embedding techniques in machine learning right there we have specifically used bag of words tfidf and many more things that is already present in my YouTube channel we have also used word to work average word to what are the main aim of all these techniques to convert text into vectors because once we probably convert into vectors we can perform various tasks like classification algorithms like similarity search algorithm and many more so that is the reason we will specifically be using openi embeddings which will be responsible in converting a text into vectors itself now once we convert every text into vectors we will also see this as text embeddings once we get this embeddings what we are specifically going to do now this will be quite amazing because understand if we have a huge PDF document right so definitely the vector size will keep on increasing so it is better we store entirely all this vectors into some kind of database and for this we are going to use Centra DB so in short what we are basically going to do is that we will take all the specific vectors and save it in a vector database here currently we are going to use cassendra DB now what exactly is cassendra DB so in order to understand about cassendra DB I have opened the entire documentation page over here cassendra aperture Apachi cassendra is an open source no SQL database and it can definitely be used for saving massive amount of data so it manages massive amount of data fast without losing sleep right so again understand this is a nosql database and for vectors kind of thing we definitely have to save it in this kind of database itself many bigger companies are basically using this cenda DB for this specific purpose so if you really want to read more about Apache cendra you can probably see over here Apache cendra is an open Source nosql distributed database trusted by thousands of companies for scalability and high availability this is the most important point for scalability and high availability without compromising performance linear scalability and proven fall Tolerance on commodity Hardware or Cloud infrastructure make it as a making it as a perfect platform for Mission critical data now how we are going to probably create this specific database for that we will be using this data Stacks platform wherein it will actually help you to create this Vector cassendra DB so that you can store entirely all these vectors into this specific DB and at any point of time if a person is trying to query from this particular DB you will be able to get that specific response from that right and the most similar response that you'll be able to get it now that is the next step what we are basically going to do all these vectors we are going to save it in some kind of vector database as I said we going to use casser DB or we can also say astrab and this will be created in this data STS platform wherein you can actually perform Vector search now the next thing is that after you probably save entirely all your vectors in in the database itself then a human whenever a human tries to query anything that is related to that particular PDF document it is going to probably apply similarity search along with text iddings and is going to get that specific response so this this is the entire architecture that we are specifically going to perform in this specific project all the steps will be shown step by step everything will be explained in an amazing way along with the code and along with the explanation now let's go ahead and start our specific project for this PDF query with Lang chain and cassendra DB so guys now let's go ahead and implement this specific project I will be going step by step I will also be showing you how you can create the cassendra DB specifically in the data Stacks uh platform itself uh we'll be seeing step by step all the comment regarding this code and all is given over here I will also be providing you the code in the description of this particular video so first of all uh what exactly we are doing we are going to query PDF with astb and Lang chain uh it is basically powered and uh understand it is powered by Vector search so first of all you need to understand what exactly is Vector search so there is an amazing documentation that is given in the data stack documentation itself so Vector search enhances machine learning models by allowing similarity comparison of the embeddings embeddings basically means whatever text is basically converted into the vectors that is basically embedding right and over there you can definitely apply multiple algorithms right machine learning algorithms on the fly right as a capability of astrab vector search supports various large language models so large language models can be is very is supported in an amazing way in this the integration is very much smooth and easy right since this llm are stateless they rely on Vector database like Astro DB to store the embeddings see understand because uh when we say stateless that basically means what suppose if we have embeddings once we lose it we cannot again query it right so it is definitely require a database to probably store all these things and what you can do after that you can query any number of time so let us go step by step and let us see okay so first of all we need to create a database on astb so I will probably click this specific link everything is basically given over here for this we will be going to astra.com right so first of all it will probably ask you to sign in right and here you can probably sign it with your GitHub or with your Google account so here I'm going to go ahead and sign it with GitHub and probably once I probably sign in over here you will be able to see that uh I'll be providing you the link along with everything in the uh code itself right so it'll be very much easy for you so once you go to Astra data.com the next step is basically to create a database right so this database uh what kind of database we are going to probably create it will be serverless vector and this is specifically a cassendra DB okay so here I will probably give my database name let's say I want to do PDF query right so this will basically be my PDF query DB okay this will basically be my database name you can give anything as you want and here I'll be basically be giving Lang chain _ DB a key space name it should be unique the provider that you can specifically use you have multiple like Amazon web services Microsoft Azure but here I'm going to probably use Google Cloud which is the default that is selected in the next step we will go ahead and select the country region which is by default Us East one so as soon as you probably fill all this details and as you know that we are specifically going to use this Vector database itself because at the end of the day the algorithms that we probably going to apply it will be easy with respect to this kind of database right so finally we will go ahead and create the database now once how we create the database you will be able to see that my database is basically created over here right so this is what is my database that looks like right PDF query diving now if I probably go to my dashboard I've already created this kind of database a lot so let me consider one database which I have already created and over here some important information that you really need to take first of all I will go and click on connect okay so when I probably click on connect one some information you will definitely require one is generate token right and the other one is the DB ID so DB ID is basically present over here right the token is basically present over here now I'll talk about where this specific information will be required okay so here I will go with respect to my code now let's start our coding initially we will be requiring some of the important libraries like Casio data set langin open Tik toen so here I will go ahead and execute it and I will go ahead and install all the specific libraries so it will probably take some time right I have already done that installation so for me it has happened very much quickly now the next thing is that as we know that we are specifically going to use cass. DB so in Lang chain you have all these libraries which will actually help you to connect with cassendra DB and perform all the necessary tasks like text T meetings uh creating V vors and probably storing it in the database itself so here I'm going to probably import all these libraries from lin. Vector stores. cassendra I'm going to import cassendra along with this I'm also going to use this Vector store index wrapper it is going to wrap all those particular vectors in one specific package so that it can be used quickly then I'm also going to import open AI because open AI is the thing that we really need to use along with this we are also going to use open a embeddings which will be responsible for converting your text into vectors along with this if you want some kind of data set from hugging face you can also use this and one more important library that we are going to use is cashio now Casio actually helps you to uh probably integrate with the Astra DB right in Lang chain and it'll also help you to initialize the DB connection so all these libraries we are going to use I'm going to execute this step by step we going to probably see and this is the first step installing the libraries and initializing all the libraries that we are specifically going to use along with this what we are going to also import is one PDF which is called as Pi PDF 2 this will actually help you to read any PDF uh read the uh text inside the PDF itself so this is one amazing library to probably use okay so here I have basically used pip install Pi PDF 2 so let me just go ahead and execute it and inside this you will be able to see it shows requirement already satisfied because I've already installed over here then from PI PDF you're going to use PDF reader because this will be the functionality that will be used in order to to read the document here is the document that I have specifically taken so this is one budget speech PDF so this is the Indian budget that is probably of 2023 it's a big document with somewhere around 461 KB file it has around 30 pages so I'm going to specifically read this PDF and then convert into vectors store it in the database itself and then query from the database anything that you have any information about that particular PDF now let's go ahead with the setup okay now with respect to the setup you require three important information one is the astrab application token one is the Astra DB ID okay so where you can probably get this two information so go to your vector database uh Vector database in the data Stacks so here uh is what you have specifically logged in okay as I said inside your DB just go and click on connect here you need to click on generate token as soon as you probably click on generate token then you will be getting some code which looks like this this token you will specifically go getting so this will be probably found in your token Json file so it'll probably show you a Json file which will have this information okay the first information that you have over here is the Astra DB application token so here you can probably see it starts with Astra CS so what you need to do just click on the generate token and you'll be able to see it this is the first information you just need to copy and paste it over here the second information is Astra DB ID right Astra DB ID is nothing but this specific information that is your database ID where do you get it you just need to copy it from here so this is the information with respect to your astrab ID so this two information once you do it you paste it over here I've already pasted it and then you can also see that I've also used some open API key and this specific API key don't use this only because Ive made some changes I've already executed the code also okay so I'm going to take this three information this two information is basically used to connect to your Astra DB right which has a cassendra DB hosted over there in the cloud right and the other information is basically to use the open AI API features right so all this information is basically there I'm going to probably execute this and then we will go ahead and read our budget speech PDF so this is the first step according to this we are reading the specific document before that we have initialized everything with respect to this okay so once we specifically do this I will probably be reading this now after reading as I said we are going to divide all our content into some kind of chunks right so here is what chunks we are basically going to do now first of all I will read all the raw text so for this I'm going to use from type extension using concatenate I'm going to read from each and every pages I will extract all the text so here you can probably see for I comma page in enumerate PDF reader. Pages page. extract text Will basically take out all the text from those pages and it will concatenate in this particular variable that is rawcore text so once I probably execute this what will happen is that you will be able to get all the text inside this particular variable so you can probably see over here rawcore text has all the entire text so this is the entire text from that specific PDF right slash in basically means new line so this step is basically done just imagine before if we did not had this specific Library it was very difficult to read a PDF right and we have actually done this just with writing four to five lines of code now the next step is that we will initialize the connection to your database I have all my database information right like uh token ID and the database ID I'm going to use that cashio cashio is basically used as a library over there for initializing of this particular database so cashier. init here I'll be giving one parameter which is called as token which will be nothing but astb application token and then your database ID which is nothing but astb ID right so I've taken this two information I will execute this you'll get some kind of warnings so don't worry about the warnings it is just like it is showing you some kind of warnings okay with respect to some drivers issues and all but this will basically get executed and now I have uh basically initialized my DB itself right now we are going to create the Lang chain embeddings L LM objects for letter use so for that I'm going to use I'm going to initialize open AI with my open AI key and embeddings also open AI embeddings with my open a key so I have my llm I have I have my embeddings okay now is the main step I need to create my lch Vector store so over here this is what we are basically going to create now right and for that you know we have initialized cendra right we have we have imported cendra now what will do is that in this cassendra we will provide three important information what is the kind of embeddings we are going to use what is the table name inside this particular database session none keyp space none so this is the default parameters we have specifically used QA mini demo is my table name okay just like a question answer table name and what kind of embeddings we are going to specifically use that basically means whenever we store any whenever we push any data in my cassendra DB in my Astra DB itself what it is going to do it is going to probably convert all the text using this embeddings into vectors right and this is the embeddings that we have initialized over here so here is the next step we will go ahead and execute this so this is my Astra Vector store but still I have not probably converted my text into vectors only when when I'm pushing my data inside my DB that time this entire embeddings will probably convert that particular data into vectors then what we are specifically going to do is that we will take this data and we will try to uh we'll take this entire data we'll convert into checks uh chunks and we'll also do the text embedding right text embedding while inserting right so here you first of all we are dividing the data or the entire data entire document into text Chunk so for this we are using character text splitter which is basically present in Lang chain. text splitter we need to split the text using character text splitter it should not increase the token size so here I've given character text splitter I'm saying use the separator slash in use chunk size some chunk size of 800 characters chunk overlap can be 200 and how much is the length with respect to that specific length you can probably provide it over here right and once I probably do this you can see text splitter. split text here you will be able to get all the text and if I see the top 50 text you can probably see that I'll be able to see all the top 50 text over here right all the data itself this is amazing right and this is basically from the PDF right all the data all the data right it is basically taking the top 50 right and understand the token size is basically over here as the chunk size is somewhere around 800 okay now this is done we have the text I'm going to just use top 50 and probably store it in the vector database to see if everything is working fine now how to add this specific text now what will happen when I add this text inside my Cassandra DBC axtra Vector store what is this this is basically initialized with respect to the cendal library right so here you'll be able to see that I have used embeddings so now when I'm inserting inside the cassendra DB what it is going to do it is going to apply this specific embeddings also so that is the reason you'll be able to see that when we write extraor Vector uncore store. addore text and I'm taking the top 50 top 50 texts over there this will also perform embeddings so that basically means if I see over here it is going to perform this task and it is going to insert in the Astra DB which is having that cassendra over there right so it is going to do this both the steps with respect to this particular code so we are going to add this text and then we also going to wrap wrap this entire inside a wrapper okay so these are the information this is the index that we'll be getting with respect to those text so once I proba executed you'll be seeing that in the same database it is going to insert all this headlines okay now finally let's go ahead and tex it that basically mean I have my vectors inside my database now it's time that we just query and we ask some kind of questions now I have read this entire PDF guys I could find out some of the question like what is the current gbd how much agriculture Target will be increased and all so I will take this particular example and let's say I'm writing first question is equal to True while true if first question I'm just say that input okay it will just ask like what kind of question you want to type else uh it is just asking you to uh put more questions if I write quit it is going to break otherwise it is going to continue now see this is the most important as soon as I give my first question it will go ahead with v Astra Vector index and it'll query whatever query text we are specifically using and the llm models that we are specifically initialized and after that we will be getting the answer along with this we'll also be providing some information right like for Doc score or similarity Source score like some other information also right so let's go ahead and execute it and as I said I'm going to use this question okay how much is the agriculture Target to be increased and what focus it will be okay so I'm going to paste it over here I'm going to press enter so as soon as I press enter you can see that it is now taking the information see this um you can probably see over here we are quering this particular DB right and it's going to give me the top four results okay so here you can see that agriculture credit Target will be increased to 20 lakh CR with the focus on animal husbandry da and Fisheries right why it is giving only this much data because I've told that take the 84 characters or 84 words 84 characters text still there and probably give the results right if I increase this it'll give you more result along with this you can probably see that it is giving me stop K queries that is the four query Hyderabad will be supported as Center of Excellence some more information but the most suitable answer that you have specifically got is this one right and this is what probably if you go ahead and search in the PDF if you give the same question you will be able to see the same answer right along with this probably if I want to probably see what is the current GDP if this information is present over there it'll also be giving you that specific answer it'll just do the similarity search right so here you can current gbd is estimated to be 7% isn't it amazing now you can probably take any huge data because at the end of the day you specifically using DB right and finally if you want to quit it I will just go ahead and write quit and this is basically quit right so in short we have performed each and every step now this is what which is happening whenever a human is giving an text query text emings will happen and based on that similarity search and then you'll be probably get the output right and this is the entire steps We have basically done step by step so guys yet another amazing video on generative AI where I will be specifically discussing about llama 2 uh Lama 2 is an opensource model uh again it has been created by Facebook or meta and you can use this specific model even for commercial purpose uh so this is quite amazing this is an opensource llm model altogether I will try to show you how we can use this create an end to endend project also in this specific video so there are many things that are going to happen and probably whatever topics that I teach going forward that is related to generative AI I will definitely follow this kind of approach so that you also get a brief idea about all these kind of models so what is the agenda of this particular video the agenda is that we will get to know about Lama 2 then we will go ahead and see the research paper where I will be talking about the key points uh about the Lama 2 model again since this is an open source and uh soon Lama 3 is also going to come up so that is a reason I'm going to create this particular video I really want to be in sync with all the open source llm models that are coming up right then we'll go and apply and download the Llama 2 model so we'll be seeing like how we can actually use this particular model in our project also so for that purpose I will be downloading this model you have to also apply this in the meta website itself and there is also one way how uh we can also use it directly from hugging face so I will also show you that and after that we will try to create an end to end llm project and this will be a Blog generation llm app uh all these topics I will be covering it I know it'll be a little longer video but every week one kind of this kind of video is necessary for you all and since 2024 I have the target I really need to teach you gener in a way that you can understand it and use it in your industries also so I will keep a Target so every video I'll keep a Target like this target for this particular video is, L likes not thousand lcks but thousand likes and comments please make sure that you write some comments and I'll keep the target to 100 okay so this will actually motivate me this will probably help this particular video to reach to many people through which they can actually use this and entirely this is completely for free which will also be beneficial for you and I my aim is to basically democratize the entire AI education okay so let's go ahead and let's first of all start with the first one that is introducing Lama 2 what exactly is Lama 2 Lama 2 is an again open source a large language model it can be it is used and it is uh available for free for research and commercial purpose you can actually use this in your companies in a startup wherever you want to use it okay now let's go ahead and read more about it so inside this model uh it has till now Lama 2 has released three different model size uh one is with 7 billion parameters the other one is 13 billion parameters and the the best one is somewhere around 70 billion parameters uh pretraining tokens is taken somewhere around 2 trillion context length is 4096 uh again when I say that if I probably compare most of the open source models I think Lama 2 is probably very good we'll be seeing all those metrics also so here you can see Lama 2 pretrained models are trained on two trillion tokens and have double the context length than Lama one it's fine tune models have been trained on over 1 million human annotation okay and now let's go ahead and see The Benchmark and this is with respect to the the benchmarking with all the open source models so it is not comparing with chat GPT sorry GPT 3.5 GPT 4.0 or Palm 2 okay so all the open source models uh here you can probably see this is the three version 7 billion 13 billion 65 billion 70 billion right all Lama 2 right llama 1 was 65 billion one uh one model it had over there so if you see Lama 2 with respect to all the metrix is very good MML that is with respect to human level understanding Q&A all all the performance metrix is superb natural language processing gsmk human evalve in human evalve it is probably having a less when compared to the other other open source models so here you can see in human uh human Val human eval human eval basically means with respect to writing code code generation there it has a lot of problems so here you can see 12.8 18.3 it is less it is less when compared to all the other open source models over here and there are also some other parameters you can probably see over here with with respect to different different tasks you can see the performance metrics okay so this was more about the model now let's go ahead and probably and this is one very important statement that they have come up with we support an open Innovation approach to AI responsible and open Innovation give us all a stake in the AI development process so uh yes Facebook is again doing a very good work and then soon they also going to come up with the Lama 3 Model now let's go ahead and see the research paper so here is the research paper the entire research paper now see uh what you should really focus on a research paper you know in research paper they'll be talking about how they have actually trained the model what kind of data points they have they actually taken in order to train the model and all right so over here you can see that um in this work we developed and release Lama 2 a collection of pretrained and finetune Lun language models ranging in scale from 7 billion to 70 billion parameters so if you talk about parameters it is somewhere around 7 billion to 70 billion our fine tune llms called Lama 2 chart are optimized for dialog use cases just like a chat bot and all right uh more information you can probably see over here what is the pretraining data see so they have told that our pretraining data includes a new mix of data from publicly available sources which does not include data from meta products or Services we've made an effort to remove data from certain sites known to contain a high volume of personal information about private individuals now this is where ethics comes into picture they really want to use this AI in a responsible way right so we trained on two trillion tokens uh and obviously for all these things you have to use Nvidia GPU okay I know guys it is boring to read the research paper but it is good to have all this specific knowledge so please keep your energy up watch this video till the end then only you'll be able to understand things right not only here because later on you'll be having other models like Mistral I'll probably create a video on Mistral also in the upcoming video right so everywhere with an end to end project everything I will take this format let me know know whether you're liking this format or not so training data we adopt most of the pretraining settings and model architecture from Lama one we use the standard Transformer architecture now you can understand how important Transformer is right most of the open source model are based on Transformer architectures itself right we trained using adamw Optimizer okay with so and so parameters we use consign learning rate schedule with so and so and here you could probably see with respect to the performance like how well it was training BPL process tokens how many tokens was actually done with respect to all the different varieties of llama model now this is basically the training loss you can probably see training loss for Lama 2 okay this is also important training hardware and carbon footprint it is basically saying that how much it is using they used Nvidia a100 I've seen this GPU it's quite amazing it's very huge okay and it is very fast also but again with such a huge amount of data it is also going to take time right so all these things are there you can also see time how much time it has basically taken how how many hours 70 billion this many number of hours power consumption this this all information is there right this is good to have right all all you should know like we just taking more energy and all right and here um with respect to the uh llama 2 you can probably see with respect to Common reasoning it is very good when compared to all the other models open source model World Knowledge reading comprehension math mlu math it is little bit less you can see over here when compared to the other model I think it is still 35 itself but remaining all it has basically come I think this 35 is also greater than all these things right MML is very much good it is able to achieve till 68.9 Google gini has said that it is reach to 90% okay but again this is the thing that you really need to know uh some more information fine tuning fine tuning also okay this is very much important guys it has it has used this uh reinforcement learning okay where uh and with human feedback so our R lhf basically means reinforcement learning with human feedback and this is what chat GPT is also trained with right so uh definitely I think as we go ahead as we go ahead and see Lama 3 and all it is going to give us very good accuracy I guess okay so superv fine tuning uh if you go ahead and just check how generative AI how llm models are trained you'll be able to get a video on this I created a dedicated video where I explained about supervised fine tuning how does supervised fine tuning happen what how does uh rhlf happens right reinforcement sorry R lhf human feedback happens all those things I've actually explained so here you can see some of the prompts right a poem to help me remember the first 10 elements on the periodic table hydrogen come first as the element one helium is second for balloons this this I want you to roast me now see this statement is also very important right so uh I want you to roast me I want you to make it particular brutal swearing at me so it is saying I'm sorry but I cannot comply with that request using language or intentional hurting someone feelings is never expectable so some kind of feelings they're trying to bring inside all these kind of models okay uh sft annotation is basically there you can probably read all these things this is good to have good to learn how this reinforcement learning with human feedback was done and all everything is given over here so uh this was all about the research paper still there are many papers to go ahead you can probably go ahead and check it out uh there is a concept of reward modeling also reward is also given right the parameters they have used two separate parameters over here and various kind of test is basically done so this was all the information about this now the next thing is that how you can go ahead and apply or download this specific model just click on download the model over here so the third part provide all the information over here and what all things you specifically required like Lama 2 and Lama chart code Lama Lam guard so go ahead and just put all this information and click on submit after submitting probably it'll take 30 minutes and you will start getting this mail okay you all start to set building with code llama you will also be getting the access from Lama 2 see you'll be getting this entirely right model weights available all the models weight will be given to you in this specific link you can click and download it also if you want so that you can use it in your local or you can deploy it wherever you want okay so this kind of mail you'll be getting uh Lama 2 commercial license all the information with all the info over here and these all models it is specifically giving again I told you 70b 70b chat why these two models are there this is specifically for Q&A kind of application dialog flow application I can basically say uh remaining one can be used for any kind of task uh in a complex scenarios and all okay so once you do this the next thing is that you can also go to hugging face in hugging face you have this Lama 270b chat FF and there is the entire information that is probably given about the entire model itself you can probably read it from here with respect to this Lama 2 is a collection of pretrained this this information is basically there you can also directly use it if you want the code with respect to Transformer you just click on using Transformer you'll be able to get this entire code where you can directly use this also okay what we are basically going to do I'm not going to use 70 billion parameters since I'm just doing it in my local machine with the CPU itself okay so what I will do I will be using a model which is basically uh it is basically a quantized model right with respect to this same llama model it is called as Lama to 7B chat gml so if you go ahead and see see this uh you'll be able to see that this particular model you'll be able to download it and you'll be able to use it it is just like a good version but uh less parameter versions right so when we say contage that basically means uh this model has been compressed and probably provided you in the form of weight so what you can do any of these models the recent model what you can do over here which is of 7.16 GB you will first off all download it so I've already downloaded it so I'm just going to cancel it over here okay because I've already downloaded it over here okay so I will do that specific download uh over here and then you can probably go ahead and start working on this and start uh using this and now how you can probably use it I Will Show You by creating an endtoend project so for creating an NN project what are the steps uh again the project name that I've already told is basically a b blog generation llm app here I'm going to specifically use this opensource llama Lama 2 model again I'm going to use the hugging face API also for that uh and let's see how the specific uh step by step how we'll be doing this specific project so let's go ahead and let's start this particular project okay guys now let's start our blog generation llm platform uh application so the model that I had actually generated over here you can probably see the model over here in the bin size and this is the size of the model is over here I'm going to specifically use in my local machine for my local inferencing and all so over here what I will do I will go quick quickly go ahead and open my VSS code so my vs code is ready over here okay now let's go ahead and do step by step things that we really need to do first of all I'm just going to create my requirement. txt file requirement. txt file and now I will go ahead and open my terminal so I will go ahead and open my command prompt and start my project okay so quickly I will clear the screen I will deactivate the default environment cond deactivate okay and we'll do it step by step so first step as usual go ahead and create my environment cond create minus P VNV environment I hope I've repeated this specific step lot many times so here I'm going to create cond create minus pvnv with python wal to 3.9 y okay so just to give you an idea what how exactly it is going to run run how things are basically going to happen uh step by step we'll understand so first of all we are creating the environment and then we will go ahead and fill our requirement. txt now in requirement. txt I'm going to specifically use some of the libraries like sentence Transformers C Transformer fast API if you want to specifically use fast API I I'll remove this fast API I think uh I will not require this IPI kernel so that I can play with Jupiter notebook if I want I can also remove this I don't want it langon I will specifically using and streamlet I'll be using okay so first of all I will go ahead and create cond activate Okay cond activate uh venv so we have activated the environment and the next thing is that I will go ahead and install all the requirement. txt okay and in this you don't require uh okay so okay I've not saved it so requirement. txt is not saved now in this you don't require any open AI key because I'm just going to use hugging face and from hugging face I'm going to probably call my model which is basically present in my local so here is the model that I am going to specifically call okay so once this installation will take place then we will go ahead and create my app.py and just give you an idea like uh I'm going to basically create the entire application in this specific file itself so quickly uh let's go ahead and import our streamlet so till the installation is basically happening I will go ahead and install streamlet Okay as St and then along with this I will also be installing Lang chain. prompts because I'm also going to use prompts over here just to give you an idea how things are going to happen it's going to be very much fun guys because open source right it it's going to be really amazing with respect to open source you don't require anything as such and then I'm going to basically write prompt template because we need to use this from Lang chain then I'll be also using from Lang chain Lang chain do llm I'm going to import C Transformer okay why this is used I will just let you know once I probably write the code for this okay so three three Transformers also I'm going to basically use over here so this is going to be from okay so C Transformers prom template and St for the streamlet I'm going to specifically use the first thing is that I will go ahead and write function to get response from my um llm uh llama model right Lama 2 model I'm going to basically use this okay still the installation is taking place guys it is going to take time because there are so many libraries I've been installing okay so I'll create a function over here let's create this particular function later on okay now after this what I'm actually going to do is that we'll go ahead and set our streamlet right setor pageor config see now many people will say streamlet or flask it does not matter guys anything you can specifically use streamlet why I'm specifically using is that it'll be very much easy for me to probably create all the things right the UI that I want so in a set page config I'm going to basically use page title generate blogs page icon I've taken this robot icon from the streamlet documentation layout will be Central and uh initial sidebar will be collapsed okay so I'm not going to open the sidebar in that specific page now I will keep my ht. header so ht. header in here I'm going to basically generate my blogs right so generate the blogs and I'll use the same logo if I want so it looks good okay so this is the next thing I will probably this will be my head over here first of all I will create my input text okay so input text field right and this will basically be my input text field and let me keep it as a um a text area or a text box whatever things is required so I will write go ahead and write St do st. input textor input okay so this will basically be my St so let's see everything is working fine why this is not coming in the color okay still the installation may be happening so over here I'll go ahead and write this I will say enter the blog topic right so if you just write the blog topic it will should be able to give you the entire blog itself with respect to anything that you want okay so done the installation is basically done over here you can probably see this good I will close this up now I'll continue my writing the code so I've created a input box now the other thing that I really want to create is that I'll try to create two more columns or two more Fields below this box okay one field I will say that how many words you specifically want for that blog okay so over here creating two more columns for additional two Fields additional two field okay so here first of all will be my column 1 let's say column 1 and column 2 I will just write it like this and here I will say St do columns and uh here I'll be using I'll be giving right what should probably be the width like let's say 5 comma 5 if I'm giving you'll be able to see that the width of the text box of width of the column that I specifically have I'll be able to see it okay I'm I'm just creating that width for that columns okay now I'll say with column one whenever I probably write anything in the column one or select in anything in the column one this will basically be my number of words okay number of words and for here I will be creating my St do text input and this text input will probably retrieve the details of number of words okay so here I have specifically number of words great now the next column that I specifically want the detail so for whom I am actually creating this particular blog I want to probably put that field also so with column 3 I will probably create something like this I will say okay fine um what blog style I will I'll create a field which is basically called as blog style okay now inside this blog style what I am actually going to do sorry not column 3 column two because I've created those variable over there okay so the blog style will be basically be a drop down so I will say St do select box okay and I will say what box this is specifically for so that first message I will say select write writing the blog for for okay so this I'm basically going to say that okay for whom I'm going to write this particular blog okay and with respect to this I can give all the options that I really want to give okay so for giving the options I will also be using this field so let's say the first option will be for researchers whether I'm writing that particular block for researchers or for data scientist okay data scientist or I am basically writing this block for for common people okay common people so this three information I really want over here and this will basically help me to put some Style filing in my blog okay that is the reason why I'm basically giving over here okay and by default since we need to select it in the first option so I will keep it as index as zero okay so here is all my stylings that I've have specifically used so if you want to probably make it in this way so you'll be able to understand this so this will be my column one and this is basically be my column two okay and then finally we will go ahead and write submit button submit will be St dot button and this will basically be my generate okay so I'm going to basically generate this entirely uh generate is just like a button which will basically Click by taking all this particular information so from here I'll be getting my input from here I'll be getting number of words from here I'll be getting my blog style okay all this three information now this will actually help me to get the final response here okay so I will say say if submit okay if submit I have to call one function right and what will be that specific function that function will return me some output okay and that output will be displayed over here now that function I really need to create it over here itself let's say I will say get llama response okay so this is basically my function and this I will create in my definition and what all parameters I specifically require over here right this three parameters right and uh if I probably call this function over here what are the parameters that I'm going to write over here is all these three parameters so first parameter is specifically my text input input text the second parameter that I'm actually going to give over here is number of words the third parameter that I really want to give is my block style so like what block style I really want okay so all this three information is over here so this will basically be my input text okay uh I'll write the same name no worries number of words and third parameter is basically my block style so all these materials will be given in the description if you're liking this video please make sure that you hit subscribe press the Bell notification icon hit like again just to motivate me okay if you motivate me a lot I will create multiple contents amazing content for you okay now here is what I will be calling my llama model right l Lama model Lama 2 model which I have actually downloaded in my local and for that only I will be specifically using this C Transformers right now if I probably go ahead and search in Lang chain Lang chain see whenever you have any problems related to anything as such right C Transformer C Transformer go and search in the documentation everything will be given to you so C Transformer what exactly it is it is it is over here it is given over here or not here let's see the documentation perfect so here you can see C Transformers the C trans Library provides python binding for ggm models so gml models the blog gml models whichever model is basically created you can directly call it from here let's say in the next class I want to call mistal so I can go ahead and write my model name over here as mist and it'll be able to call directly from the hugging phas okay um not only hugging face but at least in the local uh if you have the local if you want to call it from the hugging face then you have to probably use the hugging face API key but right now I don't want to use all those things so I want to make it quite simple so CC Transformers and here I'm going to basically write my model model is equal to and this should be my model path right which model path this one model slash this one right so here you can probably see this specific name V3 Q8 Z bin okay so I'm going to to probably copy this entire path and paste it over here okay so this will basically be my model and inside this what kind of model type I want there is also a parameter which is basically called as model type and in model type I'm going to basically say it is my llama model okay and you can also provide some config parameter if you want otherwise it will take the be default one so I'll say Max newcore tokens is equal to 256 and then the next one will basically be my temperature colon 0.01 let me keep the temperature point less only so I want to see different different answers okay so this is done uh this is my llm model that I'm basically going to call from here and it is going to load it okay now after my llm model is created I will go ahead and write my prompt template because I've Tak taken three three different information so template here I will go ahead and create this will be in three codes if you want to write it down because it is a multiline statement and I will say write a blog write a blog for which style right blog style for whom for this specific blog style for researchers for freshers for anyone you can write right or I'll say job profiles I can for researcher job profile for fresher job profile for normal people job profile right so something like this job profile for a topic which topic I'm going to basically say this will be my number of words sorry not number of words this will be my input text so this is how we basically write prompts within how many words the number of words okay this many number of words I'm going to basically write this okay so this actually becomes my prompt template entirely okay this is my entire prompt template write a blog for so and so for block style this this this to make it look better what I will do I'll just press tab so that it'll look over here okay so this is my template that I'm probably going ahead with I've given the three information blog style input text number of words everything is given over here now finally I need to probably create the prompt template okay so for creating the prompt template I'm going to use prompt is equal to prompt template and here I'm going to basically give my input variables so input uncore variables and inside this I'm going to basically write first information that I want what kind of inputs I specifically want right uh whether I want um this block style so for block style I can just write style second one I can probably say text third one I can basically say ncore word so this will basically be my three information that I'm going to provide it when I'm giving in my prompt template okay and finally uh this is my input variable this next parameter that I can also go ahead with I can provide my template itself what template I want to give so this will be my template over here now finally we will generate the response from the Lama model OKAY Lama 2 model which is from gml okay so here what I'm actually going to do I'm going to basically write llm and whatever things we have learned in Lang chain till now prompt dot prompt. format and here I'm going to basically use email sorry email what are the information that I really want to give over here prompt. format so the first thing is with respect to style the style will be given as block style so I'm going to basically write blog undor style okay the next information that I'm probably going to give is my input text input text is equal to not input text text is equal to input text I have to give text is equal to input undor text and the third parameter that I'm going to give is my ncore words which will basically be by number of words done so this is what I'm specifically giving with respect to my prompt uh and what llm will do it will try to give you the response for this and then we will go ahead and print this response and we will return this response also okay response response okay and what we'll do we will go ahead and return this response so step by step everything is done now I'm going to call this get Lama response over here here already is done now let's see if everything runs fine or not uh hope so at least one error will at least come let's see so I will delete this and let's go ahead and write over here to run the streamlet app all you have to do is just just write streamlet Run app.py Okay so once I probably execute this you'll be able to see this is what is my model but still I'm getting a model streamlet has attribute no head okay so let's see where I have specifically done the mistake because I think it should not be head it should be header okay I could see the error header okay fine no worries let's run it baby let's run this again stream L run app.py no I think it should run this looks good uh enter the block toping number of words researchers writing the researcher blog data scientist common people so let's go ahead and write about large language model so 300 words so number of words I will go ahead and write 300 I want to basically write it for common people and we will go ahead and generate it now see as soon as we click on generate it is going to take some time the reason it is probably going to take some time because uh we are using this particular in my local CPU but we got an error let's see key error block style it seems so I will go to my code block style block style block style so one minor mistake that I have specifically done over here so what I will do is that I'll give the same key name so that it does not give us any issue okay so this will be my input text and number of words the thing is that whatever things I give in that prompt template the input variables should be of that same name okay so that is a mistake I had done it's okay no worries so let's go ahead and and execute it now everything looks fine have assigned the same value over there number of words number of words so here also I'll go ahead and write number of words block style input text and this also should be block style the name I'm giving same right for both prom template and this okay so I think now it should work let's see so go ahead and write this and now my page is open opened now I'll go ahead and write large language models and it will probably create my words so this will be 300 I want to create it for common people let's generate it as I said that the output that I'm probably going to get is going to take some time because I'm running this in local CPU um let's say if you deploy this in the cloud uh with respect to if there are GPU features then you will get the response very much quickly so so just let's wait uh till then uh we get the output hardly but I think it is 5 to 10 seconds Max and since I've told 300 wordss it is again going to take time so let's see the output once it comes so guys it hardly tookes 15 seconds to display the result so here you can see that large language models have become increasingly popular in recent year due to the in due to the ability to process and generate humanlike languages it looks like a good blog you can also create any number of words blog itself Now understand that I have a good amount of ram my CPU has lot of cores so I was able to get it in 15 seconds for some of the people it may take 20 seconds it may take 30 seconds now you may be asking Kish how can you specifically reduce this time is very much simple guys we will probably do the deployment in AWS or any other Cloud Server itself which I will be probably showing you in the upcoming videos and there you'll be able to see that how with the help of gpus the inferencing also becomes very much easy not only that we'll also see how we can probably finetune all this data set with the OWN custom data itself guys yet another amazing llm project for you all now this llm project will be quite amazing because from this like this is just like a base you can probably create any kind of app on top of it you can create text summarizer you can create a quiz app or you can create any other app itself that is probably something related to text right so what is the main aim of this particular project is that from this project you will get all the guidance that is probably required to create that production grade application whenever you specifically work in the companies why because we are going to also include Vector search database and this is where you'll understand the power of the vector search database whenever you work in any NLP project something that is related to text you try to convert those text into embeddings or vectors if you have a huge vectors you you cannot just store it in your local machine you probably are requiring some kind of database and specifically with respect to vectors or embeddings V Vector DB is very super beneficial why because you can probably apply some of the important algorithms like similarity search or you can also uh apply any other algorithms that is related to text it can be text classification very much quickly by just squaring it from the vector database and getting the right kind of output so all these things we are basically going to cover it will probably be a longer video because every step by step I'll probably show you I will also write the code in code in front of you wherever any documentation is probably required I will also show you all those things so yes without wasting any time let's go ahead and probably see this project my main aim is basically to teach you in such a way that you get the right kind of knowledge and this you apply it in your company and nowadays many companies are specifically asking interviews regarding Vector DB they asking related to open a llm models and many more so let me go ahead and share my screen and as I said we will be doing completely from Basics right so here is my vs code I have a document over here budget speech. PF I am probably going to take this particular document upload it in my Vector DB right and then ask any kind of queries from it convert that into a quiz app right let's say this is a general knowledge book I can probably convert this into a quiz app with four options and get the right kind of answer from it right so this is what I'm planning to do other than this any idea you have you can probably do it on top of it right so first first thing first what is the first thing that we really need to do over here is that create a environment right and this is in every project I at least make you do this because it is super beneficial because at the end of the day with respect to every project you need to probably create a separate environment so in order to create an environment I will go ahead and write cond create cond create minus p v andv will be my uh environment name and then here I'll basically going to use Python 3.10 right so once I execute it it'll ask me for an option whether I need to install or not I will just say why and go ahead with the installation so this is the first step that we should specifically do right and uh it is important step because at the end of the day you should definitely create a different environment don't always work in the same environment whenever you are actually working in this kind of projects the second thing that I'm probably going to do is that create my requirement. txt requirement. txt the reason because whenever I'm using an lmm model or anything as such I have to probably install a lot of packages so over here first of all I will go ahead and cond activate activate this specific environment V NV slash okay so this is done the environment is activated and we are ready to go right now from my requirement. txt what are things I'm basically going to use I'm going to to probably note down all the requirements over here the libraries that is unstructured Tik toen pine cone client P PDF open aai Lang chain pandas numai python. EnV and at the end of the day guys uh I would always suggest you to please understand about Lang chain Lang chain is an amazing Library it has lot of power lot of functionalities which you can specifically do the community is huge and many many companies are specifically using it okay so requirement. txt has been saved so I will quickly go ahead and install this requirement. txt so probably it may take some time and before that uh since it is probably installing what I will do I will also go ahead and create my EnV file right so in this ENB file what I'm actually going to do I'm going to put my open API key so quickly I'll create my open AP API key save it and start using it okay so this will basically be my open API key in the EnV so that I can basically load it so along with this python. EnV is also there so let's wait till the all the libraries has been installed okay so this is the initial steps that we should specifically do right our environment is ready we have installed all the libraries that is probably required we have also kept our open AI key because at the end of the day I'm specifically going to use Lang chin you can also do it with hugging face if you want but I will try it with open AI because the accuracy is pretty much better in this okay now the next step uh what I'm quickly going to do is that quickly create one file and here I will just show you test ipynb and this will basically be my ipynb file itself and here I'll be showing you the entire code later on you can probably convert this into an end toend project you can probably create a streamlet app but here is the main thing that I'm probably going to show you by executing step by step and and what all things are specifically required to create this app again understand what I am planning to do right so I will just show you over here first of all let me just clean this screen and what is my entire agenda like what I really want to create as an llm application over here so I have a PDF okay I have a PDF so this is basically my PDF you can also say this is a data source it can be a GK book it can be a maths book it can be anything right I will will first of all load this document right once I load this document or read this document what I am going to do is that I'm going to convert this into chunks right because we cannot open AI hugging face models they have some restriction with respect to the Token size so I'm just probably going to create chunks and this is what we say it as text chunks right after this I'm going to use open AI embedding okay and this embeddings will be responsible in converting all this text CHS into vectors right so this will basically be my vectors I hope you know what exactly is vectors a numerical format for different different text right so this will specifically be my vectors and this I'm going to basically do with open embeddings further this vectors needs to be stored in some Vector search DB so here I will put all these vectors in some kind of vector surge DB now why this DB is required because at the end of the day whenever a human being queries any inputs right because of this Vector search DB here we can apply similarity search right and probably get any kind of info that I specifically want so this is my what my entire architecture of the specific project looks like right and here this Vector search DB I'm probably going to use something called as pine cone and they have lot of DBS I will talk about the advantage and disadvantage there is some amazing DBS called as data Stacks where they specifically use cassendra DB Pine clone is one right we'll see all the documentation page with respect to this okay so step by step I'm going to basically do this and you can actually do it for any number of pages one more thing that I'm probably going to install is IPI kernel since I'm going to work in my Jupiter notebook okay so this all steps are basically happening it is very much good and we are able to see this okay so let this installation happen and then I will set up my kernel okay so these are the initial steps that we really need to focus on and uh understand this project will be the base to create any kind of chatbot application mcqs quiz apps okay question answering chat Bots not only question answering chat Bots text summarizer anything that you probably want right or you can also basically say it as a chat B that gives you specific answer with respect to specific domain right with respect to the data that we have so all these things are done now I'm going to probably select the kernel V EnV python 3.0 right I'm going to save this perfect now the first step as usual I will go ahead and import start importing libraries and now we will do it completely step by step okay so what all things we basically require I'm going toire open a I'm going to import Lang chain uh apart from open and Lang chain what I'm going to also going to do go ahead with pine con I will talk about Pine con more when I probably show you the documentation apart from Pine con I'm going to basically go to Lang chain okay and I'm going to basically use something called as document loader document loaders will basically be responsible for for loading any kind of documents it can be a PDF file and all so for PDF we specifically use something called as P PDF I can also use directly Pi PDF loader but since my PDF is inside the directory I'm going to use Pi PDF directory loader okay so this is the next thing now from the next thing what we need to do is that as soon as we load any PDF we will get all the documents we have to basically do uh text splitting right because we really need to convert those into chunks we cannot take the entire token right there will be a restricted token size right like uh recently open has come up with the open 4. 4. o turbo right so I think it is GPD sorry GPD 4.0 turbo there 188 128k is the token size right so for that I'm going to basically use from Lang chain dot text spitter I'm going to probably import recursive character text spitter you can also use any other based on this right my always suggestion would be that go ahead and check out the documentation with respect to all the libraries that I'm probably uh uploading right now the next thing is that whenever I probably convert this into chunks right the next thing that I need to probably convert that into vectors and for that I'll be using some kind of embedding techniques so over here we are going to basically use do embeddings doop so it is embeddings do openai and we are going to import open AI embedding so open AI embeddings is a technique wherein it will probably convert any chunk into vectors right so this is the next step now the next step is basically also to uh import a library that will be responsible in creating a vector DB with respect to Pine call or we also say it as Vector store so here I'm going to basically use from Lang chain dot vector dot I think Lang chain dot it should be dot I'm writing comma I don't know why Vector oh spelling is mistaken do Vector stores okay and here I'm basically going to import I think it will be there pine cone right so I'm basically going to also use this pine cone pine cone will be our Vector store uh later on we can integrate this with our Vector DB that is present in Pines conone so the next thing is that I will also import our llm model because we will be requiring llm import open AI right so all these libraries I'm going to specifically use it I will quickly go ahead and execute it let's see if everything works fine you may get some kind of warning but it's okay right so this is your initial load that you are specifically doing now you know that I have an environment variables that is with respect to hugging pH so what I'm actually going to to do I'll go ahead and write from EnV import import load. EnV so this will specifically load all your environment variables okay so whatever environment variables that you have with respect to open API key or anything that is required you can basically do this right now I will also be importing OS over here right OS we can specifically use later on okay quickly now the first step as I said we need we have a PDF file we need to read it right so now I will write let's read the document okay now first step while reading the document I'll create a function so that I can reuse it and I'll write read Doc and here I will give my directory I can also give my file for that what library will be used Pi PDF loader right over here I'm using directory loader since I have to probably give my directory name and then I will basically write file _ loader and I will initialize my P PDF directory loader and basically give my directory path over here okay so directory path over here right so as soon as I give my directory path it will go to this specific directory and it will see whichever PDF is there it will start loading it okay and then I will go ahead and write file uncore loader dot load right now see why I'm showing you this step by step because everybody should understand what steps we are be doing it later on to convert this into a modular code it will be very much easy that is the reason I'm writing it in the form of functions all these functions will go into your utils.py file okay and finally you can see over here I'll get file loader _ load that basically means it is going to load all the documents and here I will basically be getting my documents right and finally we will return this documents done right now let's check check if everything is working fine so here I'm going to basically write read uncore Doc and this will basically be returning my document and here I will give my document folder so let me just go ahead and write documents in string right so this will basically be my directory path okay and now if I execute what is this doc it will probably read all the docs that are probably there now see every page by Page content this this is my first page second page third page fourth page fifth page like this I have 54 pages in my PDF right 54 Pages now if you also write length of Doc here also you'll be able to see it right so length of Doc I'm going to get 58 so that basically means we have done this first step right we have loaded we have read this particular PDF right now the next step is basically with respect to dividing these documents into text chunks okay so this is what we are probably going to do in our step two but till here everything is working fine so guys now we have finished reading the document uh now what we are basically going to do is that we are going to convert this into chunks right now for converting this into chunks what we are specifically going to do let's see so here I'm going to write the code here I'm going to basically say divide the docs into chunks and again because of the model restriction of the token size we really need to do this okay so here I'm going to basically use defin I'll create a function which is called as definition chunk data here first thing I will basically give my docs then I'm going to basically mention my chunk size right so let me go ahead and mention my chunk size my chunk size I'm going to mention it as 800 you can also mention it as 1,000 right don't keep a very huge value and then I can also say what about the chunk overlap Okay so so the chunk overlap like 50 characters can overlap with from one sentence to the other sentence right so here the next thing I'm going to basically create a text splitter and that is where we going to basically use this recursive character text splitter so first first thing first I going to mention my chunk size which will basically be my chunk size itself and along with this my chunk overlap which will be nothing but the chunk overlap that I have basically mentioned great uh so here I get my text splitter and now I'm going to basically up take this text splitter and split all the documents based on the kind of splitting that I've actually mentioned so here basically I'll provide my docs as my parameter and then I will convert this into and I'll return this docs perfect if you want to know more about this chunk uh recursive chunk splitter uh sorry recursive character splitter you can probably check this out documentation also I'm going to specify over here uh this documentation is good to understand what exactly it does and all right so perfect this is my chunk data function now what I'm actually going to do is that quickly use chunk uncore data and try it on my docs file right so here I'm going to basically mention my docs docs is this I've actually got this docs is equal to Doc okay and let's see so this will basically be my documents it is just going to apply all this right it is going to convert that entire document into a chunk size of 800 with the overlap of 50 okay and probably if I go and see my documents you'll be able to see it now see every document has now been properly mentioned right and the document that we are specifically reading is the Indian budget document right so any question that I proba asked related to Indian budget I'll be able to get the answer done this is good if I probably want to see the length of the documents also I can also see it okay just to give it get an idea like 58 is the length okay great so this is done uh now the next step what I'm actually going to do is that I'm going to also initialize my embedding technique so embedding technique of open AI right so we are going to initialize this so here I'm going to mention embeddings is equal to open Ai embeddings and here I'm going to use my API key OS do environment I can directly use os. environment and I can mention what is my API key so here I can say opencore open AI uncore API underscore key right so this is what is my embedding so let let me just quickly see what exactly is my embeddings so here is my embeddings that I'm going to probably use and this is what is basically used to convert that text into vectors right so quickly we have done this uh then I'm going to probably create my vectors let's say Let's test any vectors with this embeddings okay uh and there are various other embedding techniques like one h word to uh average word to and all but uh in open AI embeddings this provides you a much more advanced technique okay and over here also it will provide you some kind of vectors like there will be some Vector size also with respect to this okay like every every sentence will be provided with with respect to a vector size so here if I want to probably check and write embed underscore query and just test it with respect to anything like how are you right and let's see what kind of vectors we will probably get so this is my vectors that I'm actually getting see the entire text and if I want to probably check the length of these vectors it will also give you the length of this vectors okay so it is some something around 15 36 and this length will be super important because at the end of the day where I probably create my Vector database I have to specify this length okay for my problem statement now great now let's create our Vector search DB and pine code okay and now this step is very much important uh because after this particular step we will be able to see what kind of vector database we probably get okay Vector search DB in Pine con so let's go ahead and let's quickly create this Vector DB okay so guys here is the pine cone documentation you can probably check it out uh get started using pine con explore our examples this this is there what is the main important information about this Pine C is that it definitely helps you in semantic search in chat Bots also it helps you right where it probably helps you to store the entire Vector right and it provides you generative QA with open integration Lang CH leral argumentation uh rag also we basically say open integration and it has multiple uses okay so if I probably show you one of the document or guide right so if I probably go ahead and click on the guide right and this is where we really need to create the vector database I'll show you the steps of creating it right so Pine con makes it easy to provide longterm memory for high performance AI application it is managed Cloud native Vector database with a simple API no infrastructure has less pine cone serves fresh filtered query results with low hency at a scale of billions of vectors so if you have a huge data set you want to probably work with the vectors you can probably store it over here in the form of vector database um what is the importance Vector embedding provides longterm memory for AI Vector database stores and queries embedding quickly at a scale you know so anytime it is probably saved if you're quering it you will be able to get the response very much quickly now first thing first how you have to probably create this okay so if you once you log in once you log in over here or sign up you'll be able to see this okay so and here I've already created one index okay but this index will not work because see uh in the free tire right it allows you to just create one index so I will just delete this and show you how to probably create it completely from scratch so this is the name so I'm I'm going to delete this index because at the end of the day whatever vectors you are probably storing it will start indexing it okay so this is terminated now we will go ahead and create a new index so for creating a new index uh what I have to probably do is give a name so let's say I'm giving langin Vector now this is super important configure your index the dimensional metrix depends on a Model you select right now based on a Model if I probably uh see to it right so here you'll be able to see what is the length that I was able to get from my embedding 1536 so this is what is the dimension that I'm also going to give it over here and since I have basically doing cosine similarity kind of Stu or you can also do Dot product ukan but I'll stay to cosine similarity because at the end of the day the similarity search that is probably going to happen is with the help of cosine similarity and then finally we go ahead and create the index this is the main thing that you probably need to do this is basically getting terminated and this was the data that I had actually inserted but again we will do it okay so if I go to back to uh um back to probably a index let's see why it is not created or still it is showing terminating it should not not show terminating but at the end of the day because one I have already deleted it or I can just change the name if I want so but it is created over here okay Lang chin vector and it is a free Tire in the case of free tire they will provide you this thing right now from this there are some important information that you really need to retrieve one is the environment one is the database name okay so what sorry index name so I'll go back to my code here you'll be able to see Vector search DB and pine code let's let's initialize it I I've imported P cone I will say do in it okay so do in it basically does the initialization here two information are required API key okay API key with some information comma the next thing that I probably require is environment okay so environment something is required so let's retrieve this two information along with this what I can also do I have to also provide my index uncore name so here I will specifically say my IND Indore name index name I've already copied it from there so it is nothing but langin Vector let's go and see where is the API and environment so here I'm going to go back if I go and see there's an option of API keys so this is basically my API key I will copy it and I will paste it over here okay so Ive pasted my API key over here now the environment thing where will I get my environment so if I go to indexes and if I click this this is the environment that I'll be able to get it right so this two information is specifically required I will paste it over here right so this two information is done by executing it my Vector search DB will get initialized over there but at the end of the day I need to put all these embeddings specifically all these embeddings uh in my Vector DB right so over there I will again use pine cone pine cone which I have actually initialized and I'll say from documents and here I will give all my Docs so from documents I'm going to first of all give my doc parameter over here the dock which where which I need to probably store it in my Vector DB and then I will go ahead and write embeddings embeddings what kind of embeddings that I'm specifically giving is the same embeddings that we probably created and then I have my index name is equal to whatever index name I have basically initialized so as soon as I probably execute this you will be able to see that I will be able to get one index over here okay so let me just go ahead and execute it it'll probably take some amount of time because I have a huge data over there but you'll be able to see the changes once I probably go over here okay so if I go ahead and click it and if I refresh it let's see okay whether we'll be able to see everything or not whether the data part and all we will be able to see so here you can probably see query by vector and all all the data is there if I want to probably see the vector count it is 58 because the document size that you could probably see is 58 right and these are all the information you can see over here all the data has been basically stored and based on the metadata you can probably see it it has already done the indexing now any query that you probably hit it over here in the form of vectors you'll be able to get those specific response okay now to do the query part what I will do I will apply a cosine similarity so I will say cosine similarity retrieve results okay so I will try to retrieve the results over here so here let me just go ahead and write definition retrieve uncope query and here I will say query K is equal to 1 let's say k is equal to 2 I I will probably see the top two query now the second thing is that if I want to probably query I will get the matching results whatever matching results I'll use the same index dot whatever index is over here dot similarity let's see index dot similarity unor search right the similarity _ search what I can do is that let's see what function I've actually made for similarity _ search because I need to probably get those documents also right so basically uh I need to also create one more function over here right and that function will basically be my similarity underscore search that basically means what is my result that I'm probably going to get right inside this so index. similarity unders search is a function that is probably present inside this index itself right so similarity unders search and here I will probably give my query comma K is equal to some K value okay so what is the k k is equal to 2 whatever results I'm specifically getting and here I'm going to return my math matching results okay matching results so once I execute it so this is basically my retrieve query uh example so any query that I specifically give with respect to that particular PDF I'll be able to get the answer now this is fine this is the function to get the data from the database itself right so cosine similarity retrieve results from Vector DB I'll write it over here so that you will be able to understand okay so done this is the function that is basic Bally required now two important libraries that I'm going to probably use one is from langen Lang chin. change. question answering I'm using load question answering chain and along with this I'm also going to use open AI open AI will specifically be used to create my model and here my llm model will be created with the help of this so here I have written open a model name will be tax uh text Dy uh 003 and I've taken the temperature value as uh this one and then I'm also going to create my chain where I specific ially use this load QA chain and load Q chain actually helps you to create a question answer application and then here I will go ahead and write chain type is equal to stuff okay so my chain is ready my llm is ready everything is ready now all I need to do is that retrieve my queries right for retrieving the queries I will specifically be using this retrieve query function also so let's go ahead and write my definition I will go search answers from uh Vector database Vector DB okay and here I will basically write definition retrieve answers and whatever query I specifically give over here based on that I will should be able to get it right so if I probably see doc search then I will write doc search is equal to I'll call that same function retrieve query with my query that I'm actually going to give and then I will also print my doc search okay whatever print I'm basically getting now whenever I get that specific doc search I should also run this chain chain that I've actually created right to load QA chain so here I'm going to basically say chain. run and inside this what will basically be my input documents so I will say input input documents is equal to I will give my doc search right the document that I'm probably going to search and then the next will be my question which will be in the form of query right the query that I'm actually getting so once I do this my chain will basically be running and it will provide you some kind of response that it probably gets gets from the vector DB okay if it matches right and then we are going to return this specific response done now see the magic okay once I probably call this function what will happen so here I will write our query okay so the query will be I I saw something right I I basically so this will basically be my retrieve answer okay now see this I read the PDF I could find one question over there how much the agriculture Target will be increased by how many cores right how much the agriculture Target will be increased by how many cores I have just written this kind of statement now this retrieve query as soon as we call it will basically sorry retrieve answer as soon as we call okay so here I'll just make make this function change also the spelling should be right right retrieve and here also I'm going to probably make this to retrieve answers okay now as soon as I give my query over here it'll go to retrieve query it'll see from the index right it'll probably do the similarity search it'll give you the two results itself so let me just go ahead and see the answer over here now retrieve answer is not defined why okay I have to execute this sorry now it should be able to get the answer see so agriculture credit Target will be increased to 20 lakh CR with an investment of so and so information I I've just put right uh I can also write any other question how is the agriculture doing right so I may get some kind of question answer if it is not able to get something then it will say I don't know okay the government is promoting corporate based this this this this I'm getting that information from the entire PDF itself right so this is how beautifully you can probably get this and now it is probably asking being acting as a question answer application right now see this is what is the base you have a vector DB you're asking any question and you're getting some kind of response now on top of that you can also do a lot of prompt templating you can probably convert this into a quiz app you can convert it into a question answer uh chatbot you can convert this into a text summarizer you can do what whatever things you want right and this is what is the specific power over here right and this is really really good and that is what I'm probably going to show you in the next video on top of it like how can I probably do a custom prompt templating on my llm application this is what I'm probably going to show you in the next video but here i' shown you about what is Vector database and why it is very much important what exactly how the vectors are basically stored and here you can probably see see this is how your vector is stored if I probably search for any Vector over here I'll be able to get those kind of response over here right based on that uh search itself so start using this many companies are basically using this at the end of the day just for your practice sake it is completely for free right but if you have if you want more than one indexes two indexes then at that point of time you can probably take a paid tool paid version that specifically requires in a company itself so I hope you are able to understand this Amazing Project hello all my name is rush nyak and welcome to my YouTube channel so guys just a few days to end this specific year this month was amazing because I was able to upload many many videos related to generative AI many people had actually requested it uh the reason is very simple because all the students who have made successful career transition into the data science Industry they're working in different different llm projects so I hope you're liking all those videos that I'm specifically uploading um again as requested if you like this particular videos and all please do make sure that you subscribe the channel press the Bell notification icon hit like this will motivate me to upload more videos as we go ahead so it is a sincere request please do that and share with many people as uh many people as possible by you the reason is that all these videos is completely for free uh my main aim is basically to teach you in such a way that where you understand each and everything in depth Let It Be theoretical intuition practical intuition and many more right so what are we discussing in this specific video so I hope everybody has heard about um gini right gini API that was probably gini model that was launched from Google obviously the demo that they had actually shown on the website it was not true demo they had made made up that particular demo but now the gini pro model is available for you and you can also use it for your own demo purpose uh we'll understand we'll see a complete demo like how these models are these large language models are and how it is probably performing we'll be seeing various codes I'll be showing you that how you can probably create an API for this and the best thing is that with respect to this they have come up with this two different plan you know one is free for everyone the thing the model the gini pro model which is free for everyone it has a rate limit of 60 queries per minute that basically means within a minute you can actually just hit 60 queries price is free Price output is also free input output is free and you can start using this uh to just get to know like how powerful the gini pro is and I probably used it it looks pretty much good you know um at least uh not like that fancy thing what it showed in the demo not like that but yes we can compare with chat GPT or we can compare with open AI apis which perform various tasks like test summarization uh text classification or let's say other tasks like Q&A and many more right so the next uh plan that is probably go uh that is going to come which can be specifically used in industries that is pay as you go and that basically starts for more than 60 queries per minute okay so here is is the entire information I will be providing the link with respect to the description in the description of this particular video now let's go ahead and check the documentation and one by one I will show you how you can probably create the API the API Keys how you can probably load it and start using it okay so first of all just go to the python section over here and I'm just going to click on this Google collapse so as soon as I probably clicked on this Google collap you can probably see over here I've got this entirely okay so we will go ahead step by step we'll understand each and every line of code we'll see multiple examples and then let's see how good the Gemini API is okay so as suggested as I told you already this is like Gemini has come up in three different version and gini Pro is right now available for you okay where you can probably try and uh just by seeing this things right I I definitely I've already tried it out I feel that yes it is good enough right um if I compare with any openai apis yes it is doing a fabulous task okay I know the demo was bad but here the things that I'm going to execute is very much good so to get this particular notebook file all you have to do is that click on python over here and just click on this okay so this is the entire information how you can probably create the API key and all I will be showing you step by step okay so let's start this video before that please do hit like as you know that I uh I I always thought that within this year right I may reach 10 lakh subscribers but I could not but it's okay for the next upcoming 3 to four months I would definitely like to reach 10 lakh subscribers okay 10 lakh subscribers 10 lakh is a very good number right and just imagine how much beneficial it is for so many people out there so so let's go ahead and start this okay so we will go step by step I know please try to understand this things because I will be explaining you the code what exactly gini API is all about what this gini model why it is so good because this is a multi model right multimodel that basically means it is trained on both text and images so I'll show you if I probably give a image and it will be able to read that image and it'll be able to give you the information about that image that powerful the specific model is okay so uh we'll set up our development environment and get the API access to gini so this is the entire agenda we will generate text response from text inputs we will generate text responsive for multimodel inputs and then we'll use gmin for multi turn conversation and we'll also see how different embeddings can be applied for large language model okay so first of all this generative AI entire there is a library which is called as Google generative AI with the help of API you can use various features that is available in this gini model right gini pro model so first of all I will go ahead and install this entire library that is Google generative AI okay with the help of the specific code it may take some time again it depends on your internet speed how good it is uh and then after we probably go ahead and install this so import the specific library now here are some of the important things that we are going to do okay so first of all you'll be able to see we are importing path lib text WP Google generative AI as geni so this is the allias that we are going to specifically use and this is the library that has almost all the features over there from google. collab you import user data inside user data you can store your API key okay I will show you how to how you can probably do it along with this I'm probably importing display and markdown to display the information and there is another method which is called as definition to underscore markdown that basically means whenever I get any kind of response I'm going to replace this dot by star because I think this generative a is gini pro gives some kind of response where it will be having this kind of output okay and then we are going to convert that entire data with respect to markdown okay so all these things we are specifically importing now let's go to our next step setting up your API key so for do doing that just click on this particular link okay so it will go to this makers suit google.com app API key you can also create Palm apis from here and all right so after going over here what you can do you can actually go ahead I've already created one particular API key over here which you can actually see if you want to create a new API key just go ahead and click on this as soon as you click on this you will be getting the API key just copy from there and here what I have done I have written the API key over here okay so don't worry that uh this API key is not right uh I've already executed or I've already stored that particular API key itself right so probably when I share this particular notebook with you you'll not be able to see this API key over there until then I may have also deleted it from here okay so the reason don't use this do it with your own because it is completely for free right 60 queries you can actually hit it now let's go ahead and do one thing first of all I'm creating an environment variable called as Google API key okay so this will basically be my API key okay and I will remove this I will not require this and then what I'm doing over here I'm I'm saying gen. configure API key and I'm using the environment variable and I'm setting that API key to this okay so once I do this done that basically mean my API is configured this is the simple steps that you specifically need to do now what all models you specifically get from Gemini API so over here there are two models one is Gemini Pro this is optimized for text only prompts so text summarization Q&A chat anything that you specific Al want to do you have to use this specific model that is Gemini Pro and then you have one more model which is called as gini provision this is optimized for text and image prompts okay so for text and image prompts specifically you can use gini pro version if you want to check how many different models are there just write for M gen. list uncore model list underscore models will give you all the models that it is basically having and it is saying that if generated content in M supported generation methods then it will show you all the both the model name so here you can probably see that as soon as you execute this you'll be getting that you have two models one is gini pro and then the other one is Gemini Pro Vision now let's go ahead and let's try something okay so first of all I'm going to use the gemin pro model this is specifically for text related task okay so here I'm going to use generative AI gen AI dot there is a method which is called as generative model and there I'll be giving my model name so once I execute this this basically becomes my model so if I probably go ahead and execute over here so here you'll be able to see that I am getting a model trust me guys this is good enough because if you don't have open API key also you can use this and when you use this you'll be able to understand more about the llm models okay so now let's go ahead and use one method which is called as generate content this generate content method can handle a variety of use cases including multi multit chat multimodel input depending on what the under underlying model supports so here I've used gin Pro so in this particular case I am going to specifically use it for text related task okay so now here we use model dot generate content and here I'm giving the message what is the meaning of life okay so this is the default message once I execute this you see the response time the response time with respect to this particular time it will go ahead and capture so uh and also remember guys uh when I was executing some some some time before you know this was really really fast right now it is a little bit slow is it a problem internet connection I don't know okay so but here you can see user time it tooks 120 millisecond system time was 10.5 millisecond total time was 131 seconds which is very good like open AI API speed I think it'll take more than this okay that I specifically know now in order to see the response I will use this 2core markdown and we will write response. text as soon as I write this you will now be able to see the output of this right what is the meaning of life so here you can see that the meaning of life is a philosophical question that has been posed by human for centuries so and so blah blah blah blah all the information is very much easily given and it looks good you did not have to do any other prompt template techniques to probably put it in this dotted point it has considered in a better way yes you can again use prom template also for this purpose okay so if the API and now this is the most important thing which I like about gini Pro okay if the API failed to return a result use generate content response. prompt feedback now you may be thinking why the API May Fail it may be because of some of the other reason to see it was blocked due to safety concerns okay because of safety concerns also it may not give you some response now what may be that safety concern okay so first of all we will go and see response. prompt feedback when I probably execute this I also have these all features that are probably coming up this information in the response safety rating category harm category sexually explicit probability neg eligible so you can see it is categorizing based on this four important categories whether it is a hate speech whether it is a harassment whether it is a dangerous content right and here you can probably see that it is also giving that particular feedback if any of the feedback is positive I think you may not get a response so let's go ahead and execute this particular code and let's see whether this works true or not okay I definitely want to check it out so I will go ahead and execute and I will say um I will say Okay I I did not I do not mean anything to write anything bad over here but uh I would just like to see how to insult someone let's see okay I'm just trying I'm not I I'm I don't mean to but let's see so here you'll be able to see that whether I'll be able to get a response or not and again over here now now it has becomes fast right so now let's go ahead and do this see the response part quicker ex only works for a single C but none were returned now I will go ahead and check this prompt feedback now this is where block reason safety see over here harm category harassment medium so this is what is so good in this ethics is definitely there right so it is not giving you any kind of resp resp right so I hope you have understood the importance of this okay now similarly G gini can also generate multiple possible responses for a single prompt okay it can also generate right so for that you just need to use this responses. candidates now what I will do I will not execute this but instead we will go ahead and execute this what is the meaning of life okay okay so let's execute this it's okay um or I I'll just go ahead and ask a different question can you let me know about the future of future of generative AI okay so I'm asking this specific question now let's see I'm not executing this now it should definitely give me some kind of response and this also depends on the kind of like amount of tokens that is probably coming from the llm model so here it has taken 122 milliseconds now let's see the prompt feedback I think this is absolutely fine okay and here I will also go ahead and see my text it'll give so here you can see the future of generative AI holds immense potential and Promises to revolutionize various style Gan gpt3 this this is there now if I go ahead and right response. candidates okay now here you can probably see that I'm getting the entire info this is so nice see so this is my first text right role model find this this this all the information is there with respect to all the information that you will be able to see that right all whatever thing we executed step by step we getting everything okay so this is very very good okay now by default the model returns a response after completing the entire generation process you can also stream the response as it is being generated and the model will return chunks of the response as soon as they are generated which is absolutely amazing again right so what we can also do is that you can stream the response okay no need to wait for the entire response so let's see okay so here I will use the same question let me see so I will so can you let me know the future of generative now I think for this it will not take much time when compared to the previous one one that we had done right so here I will go ahead and write all you have to do is that just write over here let me just show okay you just need to add a parameter which is called as stream is equal to True by that you'll be able to get uh the response as it is being generated so let me just go ahead and execute this okay now I'm not going to directly print response. text but I'm saying that for every Chunk in response print chunk. test text okay and then it will show 80 like dotted lines okay so we can also display this in this way so here you can probably see how fast it was able to chunk by chunk obviously when chunk by chunk is getting displayed it will be very very good right so here you can probably see all the information ethical this needed for skill Force longterm impact on society and all now one more thing is that when streaming some response attributes are not available until you have iterated through all the response CH this is demonstrated below so here if I say what is the meaning of life with streaming is equal to true now you'll be able to see that let's see let's see okay so once this gets executed now I have this response. prompt feedback in prompt feedback you'll be understanding whether it has some problems or not with respect to that thing now if I directly go ahead and write response. text okay it will automatically tell me please let the response complete iteration before accessing the final cumulated attribute okay so this is a very good problem statement and now based on your requirements what I feel when compared to open AI still Gemini uh API that we have right it may have more functionalities as we go ahead but till now it really looks promising it looks really good okay now we will try to generate and text from image and text text inputs okay so this is also good now let's download this image so by using a curl operation I am probably downloading the image if you see the image the image looks something like this okay or I may also use some more image right I will write cat playing image okay so cat playing image so here you will be able to see some images let's see let's see let's see let's see this looks like a complicated image I guess it should find some problem okay uh downloads okay downloads downloads here I will say upload that specific image okay I will save it let's rename this image I will say cat. jpj now if I execute this over here I will write cat. jpj okay now this is the image it looks something like this now what I'm going to do I'm going to give this image to my Gemini Pro Vision model and then we will see what it can understand from this image like if it is if I say that the Kay is playing with some cotton fur or Hol right this cotton holes now let's see what answer gini Pro Vision gives right so first of all we will go ahead and load gini provision okay and then we will go ahead and generate model. generate content image and then we'll say to markdown response. text okay so let's go and see the answer so it is going to take that image here directly we are giving the image now it is going to understand the specific image it is going to read and it's is going to understand it is going to give me a response in text okay now let's see so this looks amazing till here but let's see the response I really want to see the response of this and it is taking some amount of time I think when we go with respect to the paid API it will be little bit less but again it depends on the image size how we are specifically calling it and all right let's see I think as we go ahead the correct answer is play right so here you can probably see if I go ahead and see this image right it shows play now it was not that well but let's see the previous one okay so let's see the previous one over here okay I think with just play it is showing I think I'm not satisfied with that specific answer but we can also try with the previous one so let's say this is the image and then we will go ahead and execute this let's see what kind of response it gives us so here you have some tiffen box you have broccolis you have some food item right so let's see till then I will download some more images Okay um let's see let's take this image also images. jpj I will go ahead and upload it over here okay I just want to see the answer like what all different kind of answers we specifically get yes uh right now for this image to text I think we are taking some time but I don't know like in the future future it may have a good response time when compared to right now but here definitely some amount of time is basically happening right or these are meal prepared containers with chicken BR brown rice broccoli and Bell papers this also looks good so it is able to give minute details over here this is really good okay now I will go ahead and write play images. jpg let's see so this is the image I think this image is small so it should be able to give it should be able to check the image and generate some kind of text let's see it was an error post intern please try a report okay some some kind of error is basically coming up okay let's see I I think it was a timeout issue or I don't know but I think now we should be able to see the answer okay it is giving in some Chinese text okay so this I think works with some other other kind of images itself or where you have a detailed image but definitely some of the use cases is failing over here okay so I feel that it is failing over here okay now let's try this one okay so there is one more option I don't know like with respect to image do we have to give only this kind of images if I'm giving cat images playing cat playing images I don't know what kind of response that we are getting or do we have some options to probably change it we'll have a look okay now the next thing is that to provide both text and images in a prompt I'm saying that right a shot engaging blog post based on this picture it should include description this this I think for this uh it should be able to give right it should be able to give I think that is the reason it was not giving that well so now let's see I will go ahead and write cat. jpg I hope it was cat.jpg only okay so yes this was CAD jpg uh can you include the description of the photo of the of okay uh let's see can you write a short blog based on this picture okay it should includ a description of the photo okay now let's see how what kind of response we specifically get and here the second parameter I'm giving it as image and that is how we are going to probably get it and uh to provide both text and image pass a list containing the strings and the image so this is my string the first thing that I'm giving this along with this image so I'm saying that write a short engaging blog post based on this picture okay so now let's see what kind of response we will specifically get okay and quickly let's see but I did not find a good output something like this so here it says like that only right is designed to handle multimodel prompts and return a text output okay perfect so yes uh this is there now let's convert this into text the adorable Ginger kitten is having a blast playing with a colorful cat toy the kitten is batting at the toy with a spa this looks good right I'm really impressed by seeing the image it is providing you all the information okay now let's see one more okay sparton image I don't know let's see sparton group image I will put this image let's see it looks good I like this image spans jpg okay so I put this image let's upload this and let's see whether it'll be able to provide the description or not first is observing what all things are there in the image I think that is what gini provision actually gives you okay so here I will say Spartans do jpg not directory why spotten it is spotten spotten Spartan okay Spartan so this is my image which looks oops oops oops oops what is the error now Spartan okay it is jpeg okay so uh I'm getting this specific image of Spartan right now and let's see how it looks like so this is the entire image right now let me just go ahead and give this image over here okay I'm saying it to generate the content model. generate content let's see and I'm saying write a short engaging blog post based on this picture it should include a description of the photo okay so it is what I feel is that that Vision it was not able to just recognize everything but the text information that we getting over here it's good in this epic scene from the movie 300 Leon is the king of spot okay it is able to understand from the movies also completely right which is absolutely very very good okay so I hope you like this this particular video guys please go ahead and try it out and there is one more assignment that I really want to give in this go ahead and try there is something called as chat conversation so there are a couple of very simple by using Gemini Pro you can actually do it just go ahead and try this you just need to execute by appending this in the history all the information is given over here okay but yes I feel Gemini Gemini Pro looks promising and uh yeah it looks good in some computer vision right the images that I gave like cat and all it was it was just saying play so at least it was able to understand someone is playing over there okay so guys yet another amazing end to endend project for you and in this particular project we are probably going to create a multilanguage invoice extractor and we are going to use gini Pro API for this gini pro has been quite amazing you can definitely do a lot many things I have preped prepared more 10 to 15 different kind of projects that are related to Real World Industries and trust me all the specific projects are performing exceptionally well with respect to accuracy so over here we are going to focus on creating a multil language invoice extractor app we'll be using gemin Pro we will be writing all the codes step by step so please make sure you practice along with me and once we practice things right one then you get multiple ideas like what different kind of projects we can basically do okay so let's go ahead and first of all let me show you the agenda what all things we are basically going to focus on so here is the entire agenda so in this agenda what we are going to focus on first of all I will go ahead and show you the multil language in wased extractor app demo okay how the demo looks like later on we will start the process of creating the project project by creating the environment first of all then we will go ahead with the requirement. txt what all libraries we are specifically required and then we will start writing our code this will be an endtoend project code step by step we'll try to build this app again it will take some time let's say this project will probably take somewhere around 25 to 30 minutes and then in fifth point we'll also discuss about what more additional improvements you can specifically do so that you can also try it from your side and as usual guys I'm actually keeping Target likes for every video so let's target uh th000 likes for this specific video because all these videos will be super beneficial for you in the companies so let me first of all complete the first one that is the demo okay so here you can probably see this is my entire app okay and here I have actually uploaded one of the uh invoice okay so this is the GST invoice and it is completely in Hindi okay the best thing is that if I ask any question related to this using gini Pro so here I have asked what is the address in the invoice so address basically means over here 1 2 3 uh SBC building DF state so this is just a common invoice I've taken from the internet itself so here you can probably see we are able to get the entire response so this is quite amazing not only this I've asked for different different questions what is the date let's say if I go ahead and say what is the date what is the date in the invoice and here you can see date basically means the knock so that usually this Google gini pro is able to understand those things okay so I think we will be getting the response so let me just go ahead and see it so it's running uh so here you can probably see 12 27 21 so all the information in this specific invoice you are able to extract just by putting a prompt over here now the best thing about this particular project is that it is very difficult to automate it because I'll tell you uh we have tried the specific project with the help of testra OCR and all right just imagine that Google Germany is able to perform exceptionally when well when compared to all those kind of tools okay so this was the demo now we will go ahead and probably develop this project completely end to end and we'll start from completely from scratch itself so uh let me go ahead and let me start this specific project so guys here is one of the project that I've started in my vs code itself so first of all just go to the terminal so I will show you all the steps what you should basically do as I suggested the first step is basically to create my virtual environment so for creating the virtual environment I've already created it so that it does not take much time because for creating the virtual environment also it takes some time so in order to create it just go ahead and write cond create minus P okay V andv your environment name okay and then you can also give python version and remember to give python version greater than 3.9 in this case because Gemini Pro is suitable for python version greater than 3.9 so here I'm going to basically use 3.10 and then you just give y so that it does not ask you for any permission while doing the installation as soon as you probably press enter so this kind of V EnV environment will get created in the same project folder okay so I'm not going to repeat this thing and probably execute it because I've already done this okay so just do it from your site with the help of this specific command the second thing is that I will go into theb file and I'll create an API key which will be available from the Google okay so Google API key this is basically for gin Pro and and if you don't know gini pro gini pro is again an amazing model that is provided by Google which actually provides you in a free way so you can actually hit 60 queries per minute okay so here is the API key that I have got if you want to also create your API key go to this website okay maker suit. google.com/ API key and you can just click on this create API key new project okay so I have already created it so I don't want to create it again okay so these are the first two steps you really require the API key and you require the environment now after that you just activate like you just write cond activate venv Okay and just activate this specific environment okay once you activate it you will be able to see that you'll be in that same environment location now let's go to the next step in requirement. txt what all libraries we specifically require so here is streamlet then you you have Google generative AI then you have python. EnV then you have Lang chain you have P PDF P PDF is basically to load any PDF as suchar or read any PDF uh then you have chroma DB chroma DB is specifically for Vector embeddings so we will try to also do Vector stores or vector embedding will try to create it so uh these are the basic steps that we specifically require now let's go ahead and start my coding or creating this specific application uh I will start writing the code completely from end so first of all what I will do I will go ahead and load my environment variable so I will say load do from EnV import load uncore do EnV so the reason why I'm doing this is that so that I can upload my all I I can load all my environment Keys okay so if you remember we have also installed python. EnV so python. EnV is basically for all my environment variables now I will go ahead and write load doore EnV so this will what it'll do it will take it'll load all the environment variables all the environment variables fromb file okay so this is what it is specifically going to do we'll do step by step now you'll be able to understand and please make sure that you write the code along with me so that you'll be able to understand it now the next thing is that I will go ahead and use streamlet as streamlet is a better framework to quickly you know create an app and definitely I use chat GPT for taking out the code and all right so that is the reason you're able to see that I'm able to upload daily videos see not the entire project is created by chat GPT but how to use stream late how to create this uh website kind of app you know all those things I can usually use streamlet uh use chart GPD so then uh so this is is my streamlet the next thing is that I will go ahead and import OS uh OS will basically be useful by for picking up the environment variable assigning the environment variable from somewhere else okay now this is done uh the next thing that I want is from P I also import image okay I don't know whether I'll be using this but let's see the next thing I will also go ahead and import from Google Dot generative AI as gen AI okay so I'm also going to import this specific because gen AI will be my entire libraries that I'm going to access it okay so done this is done these are some of the basic uh things that we are specifically going to load it okay now usually when we start our any application using gin API so what we need to also do is that we need to configure uh the API key so here I'm going to write gen a configure API key is equal to os. getet OS dot get EnV okay and here I'm going to get my environment variable that is nothing but Google API key so here whatever environment variable is basically present over here we are going to take this okay so configure okay so gen. configure uh the API key with this okay now it's time that we will create our function to load gini gini provision since the invoice instructor is on top of an image so we have to use this gin provision okay so function to load or first of all I'll load the model so I'll say model dot gen dot generative generative model so I'm going to specifically use gen. generative model and here I'm going to basically give my model name so it will be Germany Pro Vision okay so once I do this that basically means we are going to use this specific model now I will go ahead and write definition get giny response input image prompt okay so let me go ahead and write model equal to gen AI sorry I'll not initialize the model again so what I will do I will go ahead and write see the thing is that here I'm going to give three parameters one is this specific input input basically means uh uh whatever input I really want okay with respect to all the images that I'm giving and I'll also talk about this specific thing okay okay the three important information this input is basically I'm telling what what I want the assistant to do okay if I say hey you need to act as an invoice extractor you need to act like an expertise who is very good at uh taking out details from the invoice right so that basically becomes my input okay this prompt is what message I want like what is the address I actually return this is basically the image that we are going to pass okay so all those information this three information what we can basically do I'll write response dot model dot generate content and here we are going to use this three information first of all is input then you have image of zero the second one and then you have the prompt okay so this three information basically when you're generating this content you can give this three information in this same way okay input image of zero and prompt okay so in gini Pro they take they take all the parameters in the in the form of a list okay and remember the first parameter is basically the kind of prompt that you're giving where your model needs to behave in that specific way so I will talk more about it as we go ahead and finally we are going to just return the response. text so this easy it is with the help of Gemini Pro okay and that is the reason I'm loving it when I probably compare with open Ai and the best thing is that I can also use uh this along with my Lang chain you can probably use it with different different things I will show you I've also created a project where you can chat with multiple documents okay so that will also be we'll be using Lang chain and all so this is the function that is specifically done now understand one thing guys um we will do our streamlit setup okay so streamlet setup what I will do so here I will go ahead and copy and paste like this so here I'm using st. set page config let's say that I'll go ahead and say over here multi language invoice extractor okay multi language invoice extractor now in this multi language invoice extractor I will probably also give this information let's say okay now here I've given one input box this one input box is my input prompt okay and this is basically my upload file file upload so I'm saying that choose an image the image can be jpg jpg PNG this is the image of the invoice okay so let me go ahead and write this message of the invoice so once I specifically upload this specific file then we can do anything that we want okay now the next thing is that I will create an image variable I'll keep it blank initially and let me go ahead and write if uploaded file is not none so that basically means when I've when when I have uploaded some file then I will go ahead and write image and again I'll be using image. open and we will upload uh open this uploaded file okay now once we upload this so what we can also basically do is that we can uh write some kind of image and all I want to display the image also as soon as I upload it I probably want to display it so I can just try use this st. image functionality and I'll say caption uploaded image and we can use this properties Now understand that this this code right I have directly searched from uh uh chat GPT okay and uh I've just written okay just create me an image where to upload files and all right uh so very simple it is not like I am learning from somewhere I even not seeing the documentation chat GPT actually provides everything a Google Power provides everything that is basically required now this is my uploaded file now I will also go ahead and create my submit button so here I will go ahead and write St do button and I will talk about it saying that tell me about the image okay tell me about the invoice something so this is my message that I'm actually going to give in my submit button and finally I have to also design my input prompt now see this input that I'm actually going to give right so this basically becomes my input prompt I what how I want the germini pro llm model to behave so here I will go ahead and create my input prompt just see this okay this is important and this will also give you an idea like how improv prompt works okay how we can actually work with any kind of improv prompt I will say you are an expert okay in understanding invoices okay um we will upload we will upload a image image as invoices okay I'm just writing some messages and you will have to answer any questions based on the uploaded invoice image so this is just a basic prompt that I'm specifically using over here I'm telling this to do something related to this okay so this is my input prompt and all I've written it over here then let me go ahead and write if submit button is clicked if submit button is clicked so this is my default input prompt now what I will do is that I'll also create my prompt template itself and probably go ahead right if submit button okay okay is clicked now what will happen if I click the submit button so first of all I will go ahead and write if submit first I need to get my image data okay now understand over here as soon as we load the image but still we have to do some kind of image processing and convert those images into some bytes okay so for that again how do we do it so I will write definition input uncore image set up so for this I have just written in chat G saying that and here will be my uploaded file okay uh uploaded file uh okay uploaded file okay see now you may be thinking what I'm doing in this function in this function what we are writing is that it will take that uploaded file it will convert that into bytes and it will REM it will give all the image format all the image information in the bytes now I did not write this code I just went and searched in the chat GPT and this is the code that I specifically got okay and this code is quite amazing same way nothing I did not do anything see here if the uploaded file is not done so we first of all we are getting all the values then the image part what all things we basically require the type the data and byes data right and then we are returning the image Parts in these two format okay the M type and data and if the file is not uploaded this is that so this is completely I got it from chat GPT I'm not bragging anything about myself and all um again charit is already trained in internet data so it's just like writing an input prompt and I'm saying that okay I require this two specific information please give me that information now in this image data what we will basically do is that we will get all the image information so here I will go and write input image setup so let me do one thing input image details okay so I will call this give a good name okay and here I will give my uploaded file okay uploaded file so uploaded file whatever uploaded file I'm specifically getting I'm going to give that specific thing over here now by this I will be getting my image data now image data once I get it okay then I will go ahead and write my response and go ahead and call my get Gemini response so here I'm going to basically write my input input prompt first p parameter is this second parameter that I'm going to give is my image data as usual remember all the information will be coming in the form of list okay so image underscore data and this will basically be my input input and this input is nothing but whatever information I'm putting it over here all this information will go over here and you have all this information in this format right now after this I will get the response and now I will go ahead and write st. subheader I'm giving some kind of subheader and I will write the response is st. WR and I will just display the response okay whatever response we are specifically getting okay that response we going to get over here so all this information is done and this is really good now it's time that we can just run the code so guys now let's go ahead and run this uh we have completed almost everything that we really want to do now is the most amazing thing whether the project will run or not okay so if the project runs it is absolutely good because at the first time we have written the code and everything should work fine so here I'm going to write streamlit run app.py and let's go and execute this so it has opened let's see so I've have downloaded two invoices let's see what all things will be there first we will try with the normal invoice okay so here you can see all the information who is this invoice build to okay so I'm going to put this information over here and I'm going to click it tell me about the image tell me about the invoice on the all the information will be provided over here this is good so your client so all the information is over here your client this this this this even the number has been extracted which is quite amazing it is really a daunting process guys okay uh let's see I will just take a small one what is the deposit requested okay so I will just go ahead and write it what is the deposit requested this is good this giving an amazing response so I will go ahead and click tell me about the invoice over here and here you go let's see what it is going to get so tell me about the deposit requested it is just saying your company uh who is the deposit okay my prompt is wrong tell me so it is not able to understand the context obviously if you don't give the proper uh tell me how much was the deposit requested I'll give a good response Okay so I will go ahead and now click on it should give uh the proper answer I think now it is somewhere 169.99 5 it'll pick up that exact info and provide you all those information this is good so let's see 169.99 this is good guys this is trust me this is very very close uh what was the Consulting fees let me go ahead and write what was the Consulting fees now I think it should get confused with those two values what was the amount of the Consulting fees I think it should be able to give it let's see then we'll try with some other language invoice like Hindi and all okay let's see let's see let's see I think it should work fine but this is a good thing guys you can automate this entire process just imagine it is such a daunting process for with respect to invoice just see that whether you get an invoice all so the amount of the Consulting fees was $550 okay it is taking this information um okay there are some minor mistake but other than that I think let's see what is the total what was the total discount let's see discount is somewhere around 17.8 but if you give proper prompt I think you'll be able to get a good response response okay 179.1 4 okay so let's go ahead and try some other invoice this also looks good and uh let me go ahead and write this so here I will go ahead and write what is the HSN of Lenovo 51251 Lenovo in Hindi it is written in Lenovo so what is the HSN number of of of Lenovo okay I'm writing it in English 51251 5125 5125 I let's see whether it'll be able to give or not see this small information also it will be able to take now over here the date is denak okay in Hindi we basically say it as dinak so here you can see 84713 01 0 amazing amazing just amazing okay so what is the date in the invoice and you can try anything you can try different different invoices if you want I downloaded this invoices from internet you can also do it okay so yes here let me see whether you're able to get it yes perfect so guys this was it for my side I hope you like this particular video if you like it please make sure that you subscribe the channel and all the information regarding this will be given in the description of this particular video this video we are going to see how we can actually build a conversational Q&A chat bot with the help of Gemini Pro API not only that we'll also try to save all this chat in the form of a history and will will also display all the results that we all had a conversation about that is the reason we are discussing about conversational Q&A chatbot so before I go ahead guys we will keep the light Target of this specific video to th000 so that I will definitely get motivated and I'll try to bring more similar kind of projects for every one of you out there now before I go ahead please let me go and go ahead and show you the demo so this is how the demo demo will look like so if I ask hi you can probably see I will be getting the answer the response how can I assist you today the chat history is Ive asked hi bot says hi hello how can I assist you so let's say if I go ahead and ask what is generative AI okay all all the previous information I really need to record it somewhere right so let's see after this the chat history right now is this much right so generative AI also known as so and so all the information is coming up and after that your chat history will also get updated and the best thing is that I'm streaming all the specific data okay so how streaming actually works and all we'll also be discussing about that so here you can see in the history I've asked what is generative Ai and this is specifically all the information now this is what we are going to implement step by step I'll show you each and everything again all the files regarding this all the code regarding this will be given in the description of this particular video so let me go ahead and let me solve this specific project so guys I have opened my VSS code over here and right now uh if you remember in my previous video I've discussed about all these things like vision. py how you can actually play with images with the help of gini API gini Pro API then we also discussed about some kind of simple Q&A chat B now in this video I am going to show you this entire code with respect to this qpy so here is what is my entire code I'm going to specifically write step by step I'll try to show you what all things is basically required as usual first of all we need to have an environment file with respect to Google API key this you can actually get it from makes. google.com so from there which will basically provide you all the features to create the API key for gini pro the first thing is EnV file once we create the EnV file then we will go ahead and start implementing our code if you have not seen all the videos in My Gin playist I would suggest go ahead and have a look so let me quickly write over there all the code so first of all I will go ahead and uh load all the environment variables so for this I will use forv um import loore EnV okay and then we will go ahead and initialize load. EnV and finally I'm going to import streamlet as STD so I'm going to use streamlit over here and remember one thing guys there are some important libraries that you need to install that is present in the requirement. txt everything will be provided in the description of this particular video the GitHub code right so I'm going to use streamlet Google generative a and python. EnV all this Library needs to be installed before ahead right so I'm importing stream. asst then I will go ahead and import OS because I will be requiring it and then I will say import Google do generative AI as gen okay so I'm going to use this Library only for doing all my task now initially whenever we load any environment key first of all we need to set this in my gen AI so for doing that I will write gen ai. configure configure is a method where it will be asking for the API key that I have and since I've already loaded that from my environment so you can see that os. getv I'm using this Google API key okay now the next thing over here I will try to create a function function to load gini pro model okay so here you can probably see gini pro gini pro model and get response okay so I'm doing this and here you have model. geni Dot generative model I'm going to initialize my generative model itself and in this case also I'm going to use gini Pro okay so gini pro and gini pro vision for conversational Q&A we will specifically be using this one that is called as Gemini Pro okay and then we will go ahead and execute it and write chat model dot startor chat and here I'm going to specifically use history okay so this history will also maintain all the things that we are probably going to uh have in our conversation but I will show you another way where I'll use the power of streamlit to store all the history in a form of a session later on you can also put that inside your DB or you can also use it from your session itself right uh so first of all let me go ahead and Define my definition so here I will basically write definition get gini under underscore response response and inside this response I will specifically give my question whatever question we have asked so this function should basically uh you know give me the response that I'm getting from the generative model itself like from the Gman Pro right so this question I will send it to my llm model and then I will specifically get the response so here I'm going to write response chat. sendor message and here I'm going to basically write question stream is equal to True okay so stream is equal to true because as the llm model is giving you the output we are going to stream it and we are going to display it right and then we will go ahead and return the response so once this response is basically coming this response is nothing but it is the output right when we specifically get response. text now we are going to initialize our stream late app for initializing it what we are going to do is that I'm going to use this three Fields one is the Q&A demo J Min L application page config and header it is very much common the most important thing is that if I really want to record the History part right the history of the conversation we will initialize session state so in stream late it provides you session States for chat history if it does not exist I'm saying if chatore history not in session State then we will go ahead and create a sess State and we'll right chatore history over here so right now it is blank as soon as we have any conversation later on we will try to record all those we'll try to put all those conversation inside this particular session state that is what we are going to look at then in the next step we are going to basically say input is equal to st. textor input and here I'm going to basically use input input and here we are basically going to write key is equal to input okay so input will be my variable name in short whatever text box I'm specifically using along with this I'm going to also use a submit button St do button and here I'm going to basically write ask the question okay so what I've actually done I've initialized my session okay session state so here that the name of the session state is chat history okay now if I click on submit or input right basically both the fields input should also be filled okay so if both this satisfies both this conditions satisfies then what I will do I will write I will go ahead and call my get Gemini response and here I'm going to basically give my input whatever input I am probably giving it as soon as it calls this function it is going going to get that message and it goes to get that response okay now the next thing is that add user query and response to session chat history now see as soon as I probably have created this input this is what is my input that user has given and this is what is the output that we have got right now what I'm actually going to do here for all this entire history right we going to save this in our chatore history session state so for that we will go ahead and write St do session State okay st. session State and here we going to basically use chatore history okay chatore history and I'm going to do append with this specific let's say I'm going to write you basically you who's sending the message and then I'm going to basically use input okay so I am storing all this session inside this particular U variable okay and finally I will get my response also so let me write over here St Dot subheader and here I will write a statement saying that the response is the responses and in short I have to probably display the response Now understand one thing we used something called as stream is equal to true that basically means this entire response can without getting the entire content can also be populated in whatever Pages we want to probably display right what what do I mean is that now my process did not have to wait for the entire content to come from llm so as llm is sending text Data whatever response it is basically sending it is going to display it in the front end screen okay so that is what we are basically looking at so now I'm going to write from Chunk in response see that is the reason when we write stream is equal to True okay that basically means we get the exess of all the streaming data and then we can write a for Loop and I'm saying from Chunk in response and here I write st. write and we will go ahead and write chunk. text so we are displaying the text part by part and along with this I will go ahead and append this entire response okay and here instead of writing you I will write bot okay and inside this instead of writing input I will go ahead and write my entire chunk. text so what I'm actually doing is that as soon as I get the response it is coming in the streaming manner we are displaying it and accordingly we are also appending in this chatore history okay now this is perfect this is done and finally what I will do is that I will just go ahead and create my history because I need to display all the history right so here I will go ahead and write the chat history is okay so this is what I am going to basically Implement and and here I will say for RO comma I will create two common variables in St dot session State and here I'm going to use my chatore history okay chatore history and whatever is there it is either in the ski value pairs U colon bot colon some kind of answers here I'm going to basically write st. write I'll use a f one over here and I will say roll colon and then here I will say text so in this specific format okay roll colum text now let's go ahead and see if everything works fine or not okay and I will go ahead and open my terminal I'll delete this let's see so we are now displaying perfectly all our details uh this VNV has got activated I've already shown you in my previous video how to activate V EnV environment also and here we will go ahead and write stream late run uh QA chat q a chat. py and let's see if everything works fine or not so it has got executed so here is my things I will say Hi how are you and then we will go ahead and ask the question let's see whether we'll get the response the response is I'm a conversational AI chat B this the chat history is having all the details okay and it is giving chunk by chunk you can see over here right uh let's ask some other question my name is krishn okay something krishn what is your name okay something like this and I will go ahead and ask the question let's see whether this is getting saved or not okay so here you can see how are you this my name is Krishna what is your name I'm a chat bot assistant I do not have a name bot designed to help users so and so so all the information that I probably type it over here along with the response it is getting recorded in the chat history now this is what I really wanted to show it to you and it was so much easy many people had asked this specific question in my upcoming video what I'm going to specifically do is that I'm going to create a PDF document with the help of gini API I'll show you different embedding techniques how we can convert a word into vectors and then how we can utilize for a normal document Q&A kind of thing with the help of Gemini API right so I hope you like this particular video this was it for my side all the information regarding this will be given in the description of this particular video thank you take care have a great day byebye
