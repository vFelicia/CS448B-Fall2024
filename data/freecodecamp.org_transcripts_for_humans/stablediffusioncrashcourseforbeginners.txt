With timestamps:

00:00 - learn how to use stable diffusion to
00:02 - create art and images in this full
00:05 - course you will learn how to train your
00:07 - own model how to use control net and how
00:10 - to use stable diffusion's API endpoint
00:12 - this course focuses on teaching you how
00:15 - to use stable diffusion as a tool
00:17 - instead of going into the technical
00:19 - details it's a perfect starting place
00:22 - for beginners Lin Zhang developed this
00:24 - course she is a software engineer at
00:27 - Salesforce and a free code Camp team
00:29 - member let's get started hey everyone
00:32 - I'm Lane I'm a software engineer and a
00:34 - hobbyist game developer and today I'm
00:37 - bringing you a new Hands-On coding
00:39 - tutorial
00:41 - in today's video we're gonna generate
00:43 - art using an AI tool called stable
00:45 - diffusion
00:47 - if you look up the definition of stable
00:49 - diffusion
00:51 - it is a deep learning text to image
00:53 - model released in 2022 based on
00:55 - diffusion techniques
00:57 - in today's video we're gonna focus on
00:59 - teaching you how to use stable diffusion
01:01 - as a tool instead of going into the
01:03 - technical details to understand some big
01:06 - terms like variational autoencoders or
01:09 - embeddings or diffusion techniques you
01:11 - will have to have some machine learning
01:13 - background and also do some research on
01:15 - your own
01:16 - there is some Hardware requirement for
01:18 - this course you will need to have access
01:20 - to some form of GPU either local or AWS
01:23 - or some other Cloud hosted ones to try
01:25 - out the course material
01:27 - because it will be hosting our own
01:29 - instance of stable diffusion
01:32 - it's very unfortunate uh this will not
01:34 - work for the free GPU environment that
01:38 - Google collab provides because Google
01:40 - collab bans the multiplication from
01:42 - running in their notebooks
01:44 - don't worry if you really don't have
01:46 - access to any GPU power
01:49 - it is still possible for you to try out
01:51 - some web hosted stable division
01:53 - instances I will include an extra part
01:56 - at the end of this video to show you how
01:59 - you can access these cloud-hosted
02:01 - environments
02:03 - so that said let's take a look at the
02:05 - topics we're going to cover today the
02:07 - first one will be how to use stable
02:09 - diffusion how to set it up locally the
02:11 - second how to train your own model for a
02:13 - specific character or art style we call
02:16 - these Laura models Third how to use
02:18 - control net a popular stable diffusion
02:20 - plugin and last but not least how to use
02:23 - stable diffusion's API endpoint
02:26 - at the end of the course you should have
02:28 - no problem generating images that look
02:30 - like these
02:31 - pretty impressive huh
02:35 - all right I know you're excited but
02:37 - before we get started uh disclaimer we
02:40 - respect the work of artists and
02:42 - acknowledge that AI generated art using
02:44 - stable diffusion is a tool that could be
02:46 - used to enhance creativity but it does
02:48 - not replace the value of human
02:50 - creativity
02:52 - all right and now we are ready to get
02:54 - started
02:56 - let's start by installing stable
02:58 - diffusion on your local
03:00 - we're gonna go to the skithub repository
03:02 - and install them based on the
03:05 - instructions
03:33 - I'm gonna be installing on a Linux
03:35 - machine
03:45 - the installation process may take a
03:48 - while so be patient
04:00 - all right and now everything has been
04:02 - installed
04:03 - let's open the directory that we just
04:06 - installed and inspect the contents
04:09 - we can see that there is a subdirectory
04:11 - for the models
04:14 - the instructions under model stable
04:16 - diffusion says to put stable diffusion
04:18 - checkpoint models here so let's go and
04:20 - download some models
04:25 - the website that we are going to be
04:27 - using is Civic AI
04:33 - Civic AI is a model hosting site and we
04:37 - can see there are lots of models
04:38 - uploaded by different users
04:44 - one of the models that I like to use is
04:47 - called counterfeit
04:49 - it generates anime-like images
04:55 - when we open the model page we can see
04:57 - some sample images generated by the
04:59 - model
05:01 - and we take a look at the files these
05:04 - are the uploaded models
05:07 - it looks like the maintainer of this
05:09 - model regularly updates the versions so
05:12 - we can go to their links to see the
05:15 - updated versions
05:23 - we see a bunch of files adding dot safe
05:26 - tensors and these will be the models
05:28 - that we're going to use we also see a
05:30 - DOT va.pt this is a variational
05:33 - autoencoder model that will make our
05:36 - images look better more saturated and
05:38 - clearer we're gonna download models and
05:41 - put the checkpoint models in model slash
05:44 - stable diffusion and the vaee model
05:46 - slash vaes if the vae folder doesn't
05:49 - exist just create a VA folder
06:06 - once we have downloaded our checkpoint
06:08 - and vae models we're almost ready to
06:11 - launch the web UI
06:12 - we can customize some settings in web UI
06:15 - user.shell
06:17 - when you open this file the export
06:19 - command line arcs should be an empty
06:21 - string for my command line arguments I
06:24 - have configured to share this means that
06:27 - when I launch the web UI it says
06:29 - localhost it will also expose a public
06:31 - accessible URL this way my friends can
06:34 - access my locally hosted web UI via this
06:36 - public URL
06:38 - here's a part that I didn't capture in
06:40 - the video for the web UI to use the VA
06:43 - by default you have to set the VA path
06:45 - in web UI user.shell here is my web UI
06:50 - user.shop it has lots of customizations
06:55 - now it's finally time to start the web
06:58 - UI using web ui.shell if you're a
07:00 - Windows user you will run web ui.bat
07:03 - starting the web UI is going to take
07:05 - some time so I will speed up the video
07:08 - from the log lines you can see that our
07:10 - vae has been loaded
07:13 - and we have both a localhost URL and a
07:17 - public URL
07:19 - let's try out the public URL
07:30 - so this is what the web user interface
07:32 - looks like
07:34 - we can see a big text box here for
07:37 - entering prompts to generate an image
07:44 - all right let's start typing something
07:45 - I'm gonna ask for a girl with a short
07:47 - brow hair and green eyes and with a
07:51 - simple background
07:59 - let's hit the big generate button and
08:01 - it's going to take a few minutes
08:03 - I'm gonna speed up the video for you
08:09 - alright it looks like we thought our
08:11 - girl with a short brow hair and a green
08:14 - eyes on a simple background
08:17 - all right so our tax prompt in the form
08:19 - of a long strand worked
08:21 - it is also possible to use keywords or
08:24 - text
08:26 - we're gonna look at this website that
08:28 - has some reference tags for example you
08:30 - can see like girl long hair short hair
08:33 - skirt
08:34 - most of the checkpoint models we're
08:36 - using for stable diffusion are indeed
08:38 - trained on these tags so the models will
08:40 - have no problem parsing those keywords
08:44 - let's rewrite our prompt using those
08:47 - keywords
09:04 - foreign
09:05 - our prompt we're gonna look at some of
09:08 - the parameters you can see when you
09:10 - hover over the parameters there's some
09:13 - explanation text
09:15 - we're going to increase the batch size
09:17 - so that we get multiple images from one
09:19 - generation
09:20 - and the turn on restore phase
09:24 - you can learn about these parameters by
09:26 - hovering over them and reading about
09:28 - them yourself
09:34 - and indeed we got some girls with brown
09:38 - hair green eyes round glasses on a very
09:41 - simple background
09:44 - I actually want stable diffusion to help
09:47 - generate some images of Lydia our
09:49 - protagonist from learntical RPG who has
09:51 - brown hair green eyes round glasses and
09:55 - also braids her hair between the back of
09:57 - her head
09:58 - so we're gonna add braided hair to see
10:00 - if the AI will pick it up
10:08 - if you're like me you might notice that
10:10 - the background is a little bit too green
10:12 - and
10:13 - we can adjust that we just need to put
10:16 - in a negative prompt for our green
10:17 - background and we will see different
10:19 - color backgrounds
10:26 - the good news is that the backgrounds
10:28 - are no longer green however the braided
10:30 - hair isn't exactly in the direction that
10:33 - we want it to be so I guess we should
10:36 - adjust that in the prompt
11:02 - I feel like it's also a good time to
11:04 - experiment with a different sampling
11:06 - method
11:08 - different Samplers can produce a pretty
11:11 - different art styles and you can see
11:14 - some comparison images if you do a
11:15 - Google search I will also try to link to
11:17 - some in my article
11:28 - looks like the first and second picture
11:31 - do look like Lydia's hairstyle
11:34 - all right we're just gonna keep playing
11:36 - with it and add some different prompts
11:58 - all right so we asked the AI to show our
12:00 - hands
12:01 - uh however the hands are pretty deformed
12:04 - and here we're gonna
12:06 - try to fix them by using embeddings
12:15 - we again go to counterfeit this model
12:18 - page and we see easy negative this is
12:23 - one of the textual inversion embeddings
12:25 - that we can use
12:27 - so the first row is with easy negative
12:29 - and the second row is without looks like
12:32 - easy negative does enhance the image
12:35 - quality and make better hands
12:40 - all right we're gonna grab those easy
12:42 - negative dot safe tensors and put it in
12:46 - our embeddings directory
12:55 - all right going back to our web UI we're
12:57 - gonna add easy negative to the negative
13:00 - prompts
13:03 - click on this button that shows a
13:05 - portrait and under textual inversion we
13:08 - have easy Necto clicking on it will
13:10 - bring it into the negative prompt
13:13 - all right let's do some more Generations
13:18 - so what do you think do you think the
13:20 - images look better with the easy
13:22 - negative I think so
13:31 - it looks like we got the basics of text
13:34 - to image now it's time to try out image
13:36 - to image
13:41 - let's save this image for our use
13:56 - go to the image to image Tab and upload
14:00 - our image here
14:04 - suppose we want a similar post but
14:07 - instead of a brown hair girl we want the
14:10 - pink hair girl
14:17 - image to image also have batch size and
14:21 - the restore faces and all the other
14:23 - settings we've seen in text to image
14:27 - let's hit generate
14:35 - we can see that the generated images all
14:37 - have similar poses to our original image
14:39 - but the hair color has changed to Pink
14:49 - let's add our easy negative embeddings
14:52 - and try some other prompts
15:02 - for example this time instead of a white
15:05 - background I would like to add some
15:07 - detailed backgrounds
15:15 - and hit generate
15:21 - all right similar poses no glosses and
15:25 - with some books in the background
15:35 - for other image to image options besides
15:38 - uploading a picture you can also do
15:40 - sketch or in paint to repair or restore
15:44 - by repainting obliterated areas
15:51 - this is pretty much it for the basic
15:54 - usage of stable diffusion next we're
15:57 - gonna cover how to train a model for a
16:00 - specific character or specific art style
16:03 - these models are also known as Laura
16:05 - models
16:10 - the internet definition for Laura is a
16:14 - low rank adaptation it is a new
16:16 - technique for fine-tuning deep learning
16:18 - models that works by reducing the number
16:20 - of trainable parameters and enables
16:22 - efficient tile switching
16:25 - so essentially for stable diffusion we
16:28 - are patching the checkpoint models so
16:31 - that the generated images will look more
16:33 - like our character or art style
16:35 - for this part of the tutorial we won't
16:38 - need to use your logo we will be doing
16:39 - everything in Google collab
16:42 - we're going to use this amazing tutorial
16:44 - on Civic AI to train our Laura
16:47 - it's worth noting that since Google
16:49 - collab is an online collaborative
16:51 - service and the maintainer might need to
16:54 - modify or update some of the code inside
16:57 - the notebook
16:58 - it will be perfectly normal if your
17:00 - notebook doesn't look like mine or you
17:02 - run into some errors I didn't run into
17:04 - during recording this tutorial
17:07 - in that case reaching out to this
17:10 - maintainer on the Civic AI post might
17:12 - help
17:14 - cool time to take a look at the data set
17:17 - requirements
17:18 - for you to be able to train your Laura
17:21 - you will need
17:22 - anything between 20 up to a thousand
17:25 - images of your desired character or art
17:28 - style
17:33 - also your images should have some
17:35 - diversity if all of them are close-up
17:38 - shots of the face your model will have a
17:40 - hard time generating the whole body
17:47 - feel free to read the tutorial
17:49 - thoroughly in your free time but I'm
17:51 - gonna just jump straight to copying The
17:53 - Notebook
18:04 - here is The Notebook that we just copied
18:06 - I'm gonna put down Lydia as the product
18:08 - name because I want to be able to
18:10 - generate a model specifically for learn
18:13 - to call RPG's protagonist Lydia
18:16 - and when we run this Cell It's Gonna
18:18 - prompt us to connect to our Google Drive
18:20 - so that it could read our training
18:22 - images
18:37 - and in the cell you can decide whether
18:40 - to grab images from the Internet or to
18:43 - upload them to your Google Drive so you
18:45 - can read them in from your Google home
18:46 - app
18:48 - here in your Google Drive you can see
18:51 - that the first step of the Google collab
18:53 - notebook created a Laura folder and
18:56 - inside it there is your project
18:59 - Lydia and we're gonna upload our
19:02 - training images to this data set folder
19:06 - I have 13 or 14 images generated by Ai
19:09 - and when I was creating a tutorial
19:11 - Quincy asked whether I think there will
19:13 - be an in-breeding effect if
19:16 - the AI is consuming images generated
19:18 - also by AI but I think we'll be okay
19:23 - I'm just gonna give it a few minutes for
19:26 - the upload to be complete and now we're
19:28 - ready to curate our data set
19:36 - here is a cell titled cure your images
19:39 - it will help you eliminate duplicates
19:41 - from your training set however I know
19:43 - that my training set doesn't have any
19:45 - duplicates so I will skip this part
19:48 - all right this next cell will use some
19:50 - AI tools to help us tag our images in
19:54 - other words to generate those keywords
19:55 - we'll be using as the text prompt
19:59 - so like for those Lydia's images we
20:02 - might expect tags like brown hair short
20:04 - hair green eyes one girl solo face
20:08 - focused and so on
20:13 - once this AI Auto tagging is complete
20:16 - the next step will help us curate our
20:18 - attacks
20:19 - This Global activation tag will help
20:22 - identify our lower model when we are
20:24 - using a base stable diffusion model
20:27 - when we're putting the activation tag in
20:29 - the text prompt the stable diffusion
20:31 - based model will know to generate
20:33 - characters or art style that's specific
20:36 - to our Laura
20:42 - let's wait for these cells to finish and
20:45 - take a look at the results
20:52 - here are the top 50 tags from our
20:55 - training set so we see one girl solo
20:57 - brown hair green eyes all those tags
21:00 - that we expect
21:04 - now we run this carrier TAG cell to add
21:07 - this Global activation tag to our many
21:09 - text
21:16 - we are basically ready to go when we
21:18 - look at the extras we can analyze the
21:20 - text and if we look at it we should be
21:22 - able to see our Global activation tags
21:24 - as well as the other tags we identified
21:26 - earlier
21:52 - just like for the last notebook in the
21:55 - project name we're gonna put in Lydia
21:59 - and we can inspect some of the training
22:02 - parameters for example you get a base
22:04 - training model but you can also use your
22:06 - own if you don't like the default
22:09 - and there is activation tag which is the
22:12 - global activation tag that we put in
22:14 - earlier
22:20 - but before we can run this notebook we
22:24 - need to close the other notebook and not
22:25 - just close we need to delete the runtime
22:27 - too this is because Google collab
22:29 - wouldn't let you run more than one
22:31 - runtime if you are using the free
22:33 - version
22:37 - so we go back to our data set maker
22:40 - notebook interrupt any execution that
22:42 - might still be happening and manage
22:44 - runtime and delete this one
22:54 - now we can return to our training
22:55 - notebook there is one additional
22:57 - parameters that we need to think about
22:59 - which is the training steps
23:02 - this will determine how long you will
23:04 - train your model for
23:06 - let's look at some guidelines from the
23:08 - original stupid AI post
23:18 - the guide is saying that too few steps
23:20 - will undercut The Lure and make it
23:22 - useless and too many will overcook it
23:23 - and distort your images so we're gonna
23:26 - find a balance between these
23:32 - in this notebook It also says that your
23:35 - image will repeat this number of times
23:37 - during training I recommend your images
23:38 - multiplied the by their repeat is
23:40 - between 200 and 400 so since we have
23:43 - like 13 images
23:45 - we're gonna slightly increase the number
23:47 - of repeats
24:00 - and instead of saving every airport
24:02 - we're gonna save every two epochs
24:17 - let's leave all the other parameters as
24:20 - default and start running the training
24:48 - training might take some time depending
24:50 - on your configurations my model took
24:53 - like 20 something minutes to train and I
24:55 - will speed up the process just for you
25:07 - all right now we see step 100 done our
25:10 - model is fully trained
25:13 - inside Lydia output we see multiple
25:16 - models ending in safe tensors
25:19 - the number indicates the number of
25:20 - epochs the model has been training for
25:25 - now comes the fun part of evaluating our
25:28 - model by generating some images
25:33 - let me take some time to explain the
25:35 - customizations I put into web UI
25:38 - user.show
25:43 - as we mentioned before generates a
25:46 - public URL exformers according to the
25:49 - docs will increase the performance on
25:51 - certain Hardware
25:52 - clean dark because I prefer Dark theme
25:55 - to The Light theme
25:57 - and the vae path and also no half VA is
26:00 - to prevent some floating Point errors
26:10 - all right now we launch the web UI and
26:12 - wait for it to give us the public URL
26:57 - now the web UI is already as usual we
27:00 - put in our easy negative embeddings
27:03 - then let's first try the Laura that has
27:05 - been trained for eight epochs
27:16 - all right the results are looking pretty
27:18 - decent without any other prompts the
27:21 - Laura has captured Lydia's some
27:24 - character traits for example
27:27 - um short brown hair
27:28 - and uh green eyes
27:32 - however we can do better as you may
27:35 - remember from our training we're putting
27:37 - an activation keyword and now let's put
27:40 - in the activation keyboard to see if
27:42 - that help guides the lower model even
27:43 - better
27:53 - all right now we got the results you
27:56 - might notice that there are a lot of
27:59 - half body Images instead of the more
28:01 - diverse full body ones we saw with the
28:03 - previous prompt before we added the FCC
28:06 - Lydia keyword this might be because our
28:08 - training set has a lot of half body
28:10 - images and that's what this model
28:12 - selected for
28:15 - if you are doing your own character
28:17 - remember to have some diversity in your
28:19 - 20 set poses
28:23 - all right now let's try the model that
28:26 - has been trained for 10 epochs to see if
28:28 - there is any major difference
28:38 - the results are not significantly
28:40 - different but do notice that in one of
28:43 - the images we get some hint of some
28:46 - glasses without us having to enter the
28:49 - glasses as part of the tags
28:52 - now let's try adding a lot more text and
28:55 - being more specific with the model to
28:57 - see if it generates more detailed images
29:07 - all right now the results have a lot
29:10 - more resemblance to Lydia
29:14 - well in lurgical RPG Lydia worked at a
29:17 - cafe as a barista while she was also
29:20 - learning how to code on the site so
29:23 - let's change up the background to see if
29:25 - we can make Lydia barista
29:28 - so Cafe and Barista outfit and the April
29:47 - all right looks like we might be missing
29:49 - the classes so let's try again
30:02 - we can also change our base model so
30:05 - that we get a completely different base
30:07 - art style I have another one and we can
30:10 - give that one a try too
30:14 - all right now the model is fully loaded
30:17 - let's click the generate button
30:26 - well this open EG model is supposed to
30:29 - mimic the style of an easy Journey which
30:31 - is used by mid-journey so it looks like
30:33 - it has a more vibrant art style which
30:35 - people might enjoy better
30:38 - feel free to experiment with different
30:39 - base models and see which one you like
30:42 - the most
30:48 - next we can try adding more loros and
30:52 - generate more complex images
31:02 - navigate back to the Civic website that
31:04 - we have open and we can see that besides
31:07 - those checkpoint models there are some
31:08 - lower models and if we click on those
31:10 - sample images they have the generation
31:12 - data there they have the text prompt
31:14 - negative prompts and simpler number of
31:17 - steps Etc these generation data are a
31:20 - really good reference and especially if
31:22 - you copy the seed to you should be able
31:25 - to get an image very similar to the
31:27 - sample images
31:28 - it's no guarantee that they're going to
31:30 - be exactly the same but they should be
31:32 - very similar
31:34 - alright let's find some Laura to use
31:36 - with the lower models that we have
31:38 - already trained
31:41 - for example I like this Laura model
31:43 - which is trained on black and white
31:45 - manga images and they should give you
31:48 - images a manga like style if you copy
31:51 - their trigger words
31:53 - let's download it and put it in our
31:55 - models Laura folder
32:02 - now on my web Eli if I refresh I should
32:05 - be able to see the anime line art manga
32:07 - like
32:07 - model
32:09 - and let's add it to our prompt
32:14 - we need to add the trigger words too
32:16 - which are line art and monochrome
32:30 - alright we can see that now our images
32:32 - look like black and white Manga style
32:35 - that's pretty cool
32:38 - in the next part we are gonna cover how
32:40 - to use control net to gain more fine
32:42 - grain control of your image generation
32:54 - here is the GitHub page of the control
32:56 - net web UI plugin
33:00 - control net actually gives you
33:02 - fine-tuning power over your images you
33:05 - can fill in your line art with AI
33:07 - generated colors you could just scribble
33:09 - and let AI fill in the rest for you or
33:12 - you could even control the pose of
33:15 - characters in your picture
33:18 - let's scroll through this page and get
33:22 - to the installation parts
33:28 - follow the installation steps and if you
33:31 - run into any security warnings you might
33:34 - need to add enable insecure extension
33:36 - access to your webuser.show and then
33:39 - restart web UI and try the installation
33:57 - after installation just follow these
34:00 - steps and download the models you need
34:01 - and put them in the correct folder
34:12 - once you've done this it's time to go
34:15 - back to the web UI and try out the
34:16 - control net plugin
34:18 - alright so when you expanded the control
34:20 - net plugin looks like this there's an
34:22 - area from two drag and drop images and
34:25 - there are several parameters that you
34:26 - could set
34:27 - so I'm going to turn on low vram and
34:30 - also just enable the control net plugin
34:33 - Pixel Perfect
34:35 - and I have two models I have a scribble
34:39 - and a line art
34:47 - let's redo our text prompt both the text
34:50 - prompt and the control net
34:53 - parameter setting will affect how the
34:56 - images turn out
35:07 - cool so I'm gonna connect to my tablet
35:10 - and just draw something really random on
35:13 - those canvas and see what we can
35:16 - generate from there
35:20 - oh don't play too much bets on my
35:23 - drawing skill
35:26 - I'm just gonna roughly draw a person
35:30 - it's really hard to use the provided pen
35:33 - tool so
35:44 - yeah this is the head those are the eyes
35:49 - and the body
35:53 - oh I hope control net and the amazing AI
35:57 - is going to help me fix my drawing
36:14 - well I'm being totally unnecessary here
36:17 - but I'm gonna draw some ribbons
36:20 - and they of course don't even look like
36:23 - ribbons
36:32 - well we can see that the generated
36:35 - images basically followed all the line
36:37 - artwork I did
36:39 - and building some pretty amazing looking
36:42 - colors
36:44 - it's actually pretty impressive and they
36:46 - even got the ribbons right
36:51 - well that was for the scribble model now
36:54 - let's use some real line art and try the
36:57 - lineup model as well
36:59 - so I do have some promotional artwork I
37:02 - did for learn to go RPG so let's try it
37:14 - now let's configure our text prompt
37:16 - we'll do the regular easy negative for
37:19 - the negative prompts
37:25 - then pick a sampling method
37:30 - and we want the same to be a cafe
37:34 - what the line to be a softer
37:37 - and detailed
37:44 - you might have noticed that we actually
37:47 - didn't put in any request for people to
37:50 - be in the image we'll see what a general
37:52 - image actually has people in it or not
37:58 - and this time we do the lineup model
38:08 - so because we didn't ask for any people
38:11 - to be in the artwork we are only getting
38:13 - images of the cafe background looks like
38:17 - I guess we need to explicitly put in two
38:19 - girls in a prompt
39:00 - and it looks like this time we are
39:02 - getting girls in the images
39:04 - looks like AI has filled in colors for
39:07 - our line art
39:23 - so that was the result of the lidar
39:25 - model the colors are a little bit weird
39:27 - so let's give the AI some more freedom
39:29 - and use the scribble one instead
39:40 - well now the AI has full freedom to
39:43 - imagine the faces of the two girls and
39:46 - looks like the results are pretty decent
39:49 - the images are in square but our line
39:51 - art
39:53 - original picture is in the Square so
39:55 - let's fix up the
39:57 - aspect ratio
40:13 - again with the scribble model let's
40:16 - generate some more images
40:25 - so yeah on the left is my original line
40:28 - art and looks like the AI have generated
40:32 - something that looks pretty vibrant
40:35 - color
40:37 - these actually look pretty good I'm very
40:40 - impressed
40:45 - all right besides control net there are
40:48 - some other really powerful plugins or
40:50 - extensions maintained by other open
40:52 - source contributors
40:53 - I'm gonna show you where to find those
40:55 - so on the Wiki page of the stable
40:58 - diffusion UI get repository we see a
41:02 - page for extensions
41:06 - for example this one looks like it works
41:09 - with control net
41:10 - and this vram estimator will allow you
41:13 - to
41:14 - increase dimensions of your image and
41:17 - batch size until you run out of memory
41:20 - so you can maximize your vram usage
41:31 - this one will allow you to draw some
41:33 - poses
41:36 - this one says it will Target an area to
41:38 - selectively enhance details
41:44 - this one I've seen a lot of people use
41:46 - to generate videos
41:49 - and it works with the control net
41:50 - extension tube
41:56 - this one will help you fine-tune where
41:58 - your Laura is acting on in the image
42:12 - this theme seems to be showing you a
42:15 - custom thumbnail view of your Laura
42:17 - model
42:21 - this one is for localization
42:30 - this one is for editing posts I'm just
42:33 - scrolling through it's a long list just
42:35 - to see what kind of core effects we can
42:37 - have
42:45 - this one converts your images into pixel
42:48 - art style looks pretty cool
42:57 - this one use Transformer models to
43:00 - generate text prompts for you
43:05 - another pixel art
43:08 - and other effects too
43:19 - another prompt generator
43:30 - this one saves intermediate images while
43:32 - the AI is trying hard to generate them
43:54 - there's even this one that will
43:56 - automatically remove backgrounds and
43:57 - leave only the central figures
44:02 - yeah so there are lots and lots of those
44:04 - extensions out there there's a lot for
44:06 - you to explore we can't possibly cover
44:08 - everything in a single course
44:10 - and people also create their own plugins
44:13 - and extensions as they see their new
44:16 - device
44:18 - moving on let's talk about generating
44:21 - images using the stable diffusion API
44:28 - there is this API Doc Page on the stable
44:31 - diffusion repository website
44:37 - and to enable the API we need to turn on
44:40 - dash dash API in our web UI user.shell
44:52 - we can see that there are several
44:55 - endpoints for example the text to image
44:57 - API endpoint image to image API endpoint
45:02 - and when we scroll down we see sample
45:04 - payload
45:07 - we're just put in our prompt and number
45:10 - of steps and other parameters in the
45:12 - payload and it will be exactly the same
45:14 - as if we were using the web UI itself
45:25 - so the idea is that we send the
45:27 - parameter payload to the web UI API
45:31 - endpoint
45:32 - using a post method and then we will
45:35 - retrieve some bytes that we can decode
45:38 - into our image
45:44 - below the developers have provided us
45:47 - with some simple python called snippet
45:49 - that we can use to query the endpoint
45:51 - and get our images and save it as the
45:54 - output.png
45:57 - all right let's grab those code snippet
45:59 - and run it locally
46:50 - all right for now I'm just going to copy
46:52 - paste it and run it directly and I will
46:55 - talk through the code in a little bit
46:57 - once we see the output
47:04 - just know for now that our payload is a
47:07 - simple puppy dog
47:23 - if we don't have some of the modules
47:24 - installed that we have to import we need
47:27 - to install them using pip
47:37 - and just run python3 sd.py and wait for
47:40 - it to finish now it looks like it has
47:43 - finished and we have an output.png file
47:47 - which is a puppy
48:04 - all right let's try some different props
48:33 - foreign we can add negative prompts into
48:36 - the payload too
49:21 - we're on the file again and see what we
49:23 - are gonna get
49:32 - oh it looks pretty futuristic and really
49:34 - cool
49:50 - of course we could append our alarm
49:53 - modeling to it and see if we can get
49:56 - Lydia generated
50:52 - foreign
50:59 - because we have increased the number of
51:01 - steps
51:07 - and we have a picture of Lydia
51:14 - the last five minutes of the video is
51:16 - like a bonus part
51:18 - we're gonna use PostNet which is an app
51:21 - for us to test out API endpoints
51:25 - we are going to look at how the images
51:27 - is represented in the API response
51:32 - and we'll also take a deep dive into the
51:34 - python code and walk through it line by
51:36 - line
51:38 - this is the postman app we're going to
51:41 - put in our request URL there
51:44 - which is the text to image endpoint and
51:47 - we're going to use the post method
51:49 - make sure the body is Json Raw
52:02 - and click
52:04 - send
52:13 - so we get this long string representing
52:15 - our images
52:27 - and then in the response we could see
52:29 - some other parameters that we don't need
52:31 - to pay too much attention to
52:42 - I searched for some base64 to image
52:45 - decoder online
52:46 - and we'll see what it works or not
52:50 - all right
52:52 - remove the quotes
52:55 - remove the starting quilt two
53:03 - and here's our image
53:13 - now let's walk through this python file
53:15 - line by line
53:22 - so here we're doing a bunch of imports
53:24 - we imported Json we imported requests
53:27 - for making a post request and we import
53:30 - a base64 for decoding and we imported
53:33 - some image libraries
53:34 - and we set our URL to be the public
53:38 - instance that our web UI is running on
53:43 - and we create our payload just like this
53:46 - and then we post to our URL and the
53:51 - specific API endpoint which is text to
53:52 - image given the payload and I get the
53:56 - response convert it to Json
53:58 - and remember the images field that we
54:01 - saw that's our image string so we decode
54:04 - it into an image
54:07 - this part here is retrieving some
54:09 - metadata for the PNG for example what
54:12 - parameters are used to generate a PNG
54:14 - Etc and finally we save this image
54:17 - alongside with this PNG info it's pretty
54:20 - straightforward
54:22 - feel free to play with the code on your
54:24 - own you can also make API requests in
54:26 - other programming languages of your
54:27 - choice
54:33 - all right as promised in case you don't
54:35 - have access to a GPU here are some ways
54:39 - you can run stable diffusion on some
54:41 - free online platforms
54:43 - of course there is going to be
54:44 - restrictions like you won't have access
54:47 - to all the models you want you won't be
54:49 - able to upload your custom model and the
54:51 - other people on the internet are also
54:53 - using the servers so you could be in the
54:55 - queue for a long time before your images
54:57 - are generated
55:01 - here we are on hugging face and we will
55:04 - go through spaces
55:10 - and in the search bar we type in stable
55:13 - diffusion
55:15 - and let's sort by most number of likes
55:23 - so looks like the most popular one is
55:26 - the one by stability AI which happened
55:28 - to be the original one so this will be
55:30 - the best option however it shows that it
55:32 - is passed so I don't think you'll be
55:34 - able to use it
55:36 - let's take a look at the other ones that
55:38 - we have access to
56:01 - of course these online models might be
56:03 - limited in that it doesn't have the base
56:05 - model you want it doesn't have the style
56:06 - that you want but this is the best we
56:08 - can do if we don't have access to a
56:10 - local GPU
56:19 - foreign
56:33 - s are a little distorted so maybe the
56:37 - space model isn't great for generating
56:38 - anime images
56:40 - and that's one of its limitations
56:50 - looking through the list we see some
56:52 - image to image some in paintings some
56:55 - photo generic one I think we'll be using
56:58 - the photo generic one
57:00 - let's give it a shot
57:06 - now this one looks like the stable
57:09 - diffusion standard web UI that we're
57:11 - used to
57:12 - and we can see that it's currently live
57:15 - and running on an online GPU
57:18 - they say the model is a protogen
57:29 - which should be a photorealism model
57:39 - foreign
58:09 - looks like they are restraining the
58:11 - number of images we can generate
58:15 - but let's do it
58:19 - and also we will have to wait in the
58:21 - queue
58:25 - so yeah
58:27 - if you don't like to wait or if you
58:30 - really need to use your custom models
58:32 - you should consider getting your own GPU
58:40 - all right finally after waiting for
58:42 - almost five minutes I got my result back
58:44 - it looks pretty good
58:47 - and with that we'll conclude today's
58:49 - tutorial I hope you enjoyed it and I
58:52 - hope to see you in the next video

Cleaned transcript:

learn how to use stable diffusion to create art and images in this full course you will learn how to train your own model how to use control net and how to use stable diffusion's API endpoint this course focuses on teaching you how to use stable diffusion as a tool instead of going into the technical details it's a perfect starting place for beginners Lin Zhang developed this course she is a software engineer at Salesforce and a free code Camp team member let's get started hey everyone I'm Lane I'm a software engineer and a hobbyist game developer and today I'm bringing you a new HandsOn coding tutorial in today's video we're gonna generate art using an AI tool called stable diffusion if you look up the definition of stable diffusion it is a deep learning text to image model released in 2022 based on diffusion techniques in today's video we're gonna focus on teaching you how to use stable diffusion as a tool instead of going into the technical details to understand some big terms like variational autoencoders or embeddings or diffusion techniques you will have to have some machine learning background and also do some research on your own there is some Hardware requirement for this course you will need to have access to some form of GPU either local or AWS or some other Cloud hosted ones to try out the course material because it will be hosting our own instance of stable diffusion it's very unfortunate uh this will not work for the free GPU environment that Google collab provides because Google collab bans the multiplication from running in their notebooks don't worry if you really don't have access to any GPU power it is still possible for you to try out some web hosted stable division instances I will include an extra part at the end of this video to show you how you can access these cloudhosted environments so that said let's take a look at the topics we're going to cover today the first one will be how to use stable diffusion how to set it up locally the second how to train your own model for a specific character or art style we call these Laura models Third how to use control net a popular stable diffusion plugin and last but not least how to use stable diffusion's API endpoint at the end of the course you should have no problem generating images that look like these pretty impressive huh all right I know you're excited but before we get started uh disclaimer we respect the work of artists and acknowledge that AI generated art using stable diffusion is a tool that could be used to enhance creativity but it does not replace the value of human creativity all right and now we are ready to get started let's start by installing stable diffusion on your local we're gonna go to the skithub repository and install them based on the instructions I'm gonna be installing on a Linux machine the installation process may take a while so be patient all right and now everything has been installed let's open the directory that we just installed and inspect the contents we can see that there is a subdirectory for the models the instructions under model stable diffusion says to put stable diffusion checkpoint models here so let's go and download some models the website that we are going to be using is Civic AI Civic AI is a model hosting site and we can see there are lots of models uploaded by different users one of the models that I like to use is called counterfeit it generates animelike images when we open the model page we can see some sample images generated by the model and we take a look at the files these are the uploaded models it looks like the maintainer of this model regularly updates the versions so we can go to their links to see the updated versions we see a bunch of files adding dot safe tensors and these will be the models that we're going to use we also see a DOT va.pt this is a variational autoencoder model that will make our images look better more saturated and clearer we're gonna download models and put the checkpoint models in model slash stable diffusion and the vaee model slash vaes if the vae folder doesn't exist just create a VA folder once we have downloaded our checkpoint and vae models we're almost ready to launch the web UI we can customize some settings in web UI user.shell when you open this file the export command line arcs should be an empty string for my command line arguments I have configured to share this means that when I launch the web UI it says localhost it will also expose a public accessible URL this way my friends can access my locally hosted web UI via this public URL here's a part that I didn't capture in the video for the web UI to use the VA by default you have to set the VA path in web UI user.shell here is my web UI user.shop it has lots of customizations now it's finally time to start the web UI using web ui.shell if you're a Windows user you will run web ui.bat starting the web UI is going to take some time so I will speed up the video from the log lines you can see that our vae has been loaded and we have both a localhost URL and a public URL let's try out the public URL so this is what the web user interface looks like we can see a big text box here for entering prompts to generate an image all right let's start typing something I'm gonna ask for a girl with a short brow hair and green eyes and with a simple background let's hit the big generate button and it's going to take a few minutes I'm gonna speed up the video for you alright it looks like we thought our girl with a short brow hair and a green eyes on a simple background all right so our tax prompt in the form of a long strand worked it is also possible to use keywords or text we're gonna look at this website that has some reference tags for example you can see like girl long hair short hair skirt most of the checkpoint models we're using for stable diffusion are indeed trained on these tags so the models will have no problem parsing those keywords let's rewrite our prompt using those keywords foreign our prompt we're gonna look at some of the parameters you can see when you hover over the parameters there's some explanation text we're going to increase the batch size so that we get multiple images from one generation and the turn on restore phase you can learn about these parameters by hovering over them and reading about them yourself and indeed we got some girls with brown hair green eyes round glasses on a very simple background I actually want stable diffusion to help generate some images of Lydia our protagonist from learntical RPG who has brown hair green eyes round glasses and also braids her hair between the back of her head so we're gonna add braided hair to see if the AI will pick it up if you're like me you might notice that the background is a little bit too green and we can adjust that we just need to put in a negative prompt for our green background and we will see different color backgrounds the good news is that the backgrounds are no longer green however the braided hair isn't exactly in the direction that we want it to be so I guess we should adjust that in the prompt I feel like it's also a good time to experiment with a different sampling method different Samplers can produce a pretty different art styles and you can see some comparison images if you do a Google search I will also try to link to some in my article looks like the first and second picture do look like Lydia's hairstyle all right we're just gonna keep playing with it and add some different prompts all right so we asked the AI to show our hands uh however the hands are pretty deformed and here we're gonna try to fix them by using embeddings we again go to counterfeit this model page and we see easy negative this is one of the textual inversion embeddings that we can use so the first row is with easy negative and the second row is without looks like easy negative does enhance the image quality and make better hands all right we're gonna grab those easy negative dot safe tensors and put it in our embeddings directory all right going back to our web UI we're gonna add easy negative to the negative prompts click on this button that shows a portrait and under textual inversion we have easy Necto clicking on it will bring it into the negative prompt all right let's do some more Generations so what do you think do you think the images look better with the easy negative I think so it looks like we got the basics of text to image now it's time to try out image to image let's save this image for our use go to the image to image Tab and upload our image here suppose we want a similar post but instead of a brown hair girl we want the pink hair girl image to image also have batch size and the restore faces and all the other settings we've seen in text to image let's hit generate we can see that the generated images all have similar poses to our original image but the hair color has changed to Pink let's add our easy negative embeddings and try some other prompts for example this time instead of a white background I would like to add some detailed backgrounds and hit generate all right similar poses no glosses and with some books in the background for other image to image options besides uploading a picture you can also do sketch or in paint to repair or restore by repainting obliterated areas this is pretty much it for the basic usage of stable diffusion next we're gonna cover how to train a model for a specific character or specific art style these models are also known as Laura models the internet definition for Laura is a low rank adaptation it is a new technique for finetuning deep learning models that works by reducing the number of trainable parameters and enables efficient tile switching so essentially for stable diffusion we are patching the checkpoint models so that the generated images will look more like our character or art style for this part of the tutorial we won't need to use your logo we will be doing everything in Google collab we're going to use this amazing tutorial on Civic AI to train our Laura it's worth noting that since Google collab is an online collaborative service and the maintainer might need to modify or update some of the code inside the notebook it will be perfectly normal if your notebook doesn't look like mine or you run into some errors I didn't run into during recording this tutorial in that case reaching out to this maintainer on the Civic AI post might help cool time to take a look at the data set requirements for you to be able to train your Laura you will need anything between 20 up to a thousand images of your desired character or art style also your images should have some diversity if all of them are closeup shots of the face your model will have a hard time generating the whole body feel free to read the tutorial thoroughly in your free time but I'm gonna just jump straight to copying The Notebook here is The Notebook that we just copied I'm gonna put down Lydia as the product name because I want to be able to generate a model specifically for learn to call RPG's protagonist Lydia and when we run this Cell It's Gonna prompt us to connect to our Google Drive so that it could read our training images and in the cell you can decide whether to grab images from the Internet or to upload them to your Google Drive so you can read them in from your Google home app here in your Google Drive you can see that the first step of the Google collab notebook created a Laura folder and inside it there is your project Lydia and we're gonna upload our training images to this data set folder I have 13 or 14 images generated by Ai and when I was creating a tutorial Quincy asked whether I think there will be an inbreeding effect if the AI is consuming images generated also by AI but I think we'll be okay I'm just gonna give it a few minutes for the upload to be complete and now we're ready to curate our data set here is a cell titled cure your images it will help you eliminate duplicates from your training set however I know that my training set doesn't have any duplicates so I will skip this part all right this next cell will use some AI tools to help us tag our images in other words to generate those keywords we'll be using as the text prompt so like for those Lydia's images we might expect tags like brown hair short hair green eyes one girl solo face focused and so on once this AI Auto tagging is complete the next step will help us curate our attacks This Global activation tag will help identify our lower model when we are using a base stable diffusion model when we're putting the activation tag in the text prompt the stable diffusion based model will know to generate characters or art style that's specific to our Laura let's wait for these cells to finish and take a look at the results here are the top 50 tags from our training set so we see one girl solo brown hair green eyes all those tags that we expect now we run this carrier TAG cell to add this Global activation tag to our many text we are basically ready to go when we look at the extras we can analyze the text and if we look at it we should be able to see our Global activation tags as well as the other tags we identified earlier just like for the last notebook in the project name we're gonna put in Lydia and we can inspect some of the training parameters for example you get a base training model but you can also use your own if you don't like the default and there is activation tag which is the global activation tag that we put in earlier but before we can run this notebook we need to close the other notebook and not just close we need to delete the runtime too this is because Google collab wouldn't let you run more than one runtime if you are using the free version so we go back to our data set maker notebook interrupt any execution that might still be happening and manage runtime and delete this one now we can return to our training notebook there is one additional parameters that we need to think about which is the training steps this will determine how long you will train your model for let's look at some guidelines from the original stupid AI post the guide is saying that too few steps will undercut The Lure and make it useless and too many will overcook it and distort your images so we're gonna find a balance between these in this notebook It also says that your image will repeat this number of times during training I recommend your images multiplied the by their repeat is between 200 and 400 so since we have like 13 images we're gonna slightly increase the number of repeats and instead of saving every airport we're gonna save every two epochs let's leave all the other parameters as default and start running the training training might take some time depending on your configurations my model took like 20 something minutes to train and I will speed up the process just for you all right now we see step 100 done our model is fully trained inside Lydia output we see multiple models ending in safe tensors the number indicates the number of epochs the model has been training for now comes the fun part of evaluating our model by generating some images let me take some time to explain the customizations I put into web UI user.show as we mentioned before generates a public URL exformers according to the docs will increase the performance on certain Hardware clean dark because I prefer Dark theme to The Light theme and the vae path and also no half VA is to prevent some floating Point errors all right now we launch the web UI and wait for it to give us the public URL now the web UI is already as usual we put in our easy negative embeddings then let's first try the Laura that has been trained for eight epochs all right the results are looking pretty decent without any other prompts the Laura has captured Lydia's some character traits for example um short brown hair and uh green eyes however we can do better as you may remember from our training we're putting an activation keyword and now let's put in the activation keyboard to see if that help guides the lower model even better all right now we got the results you might notice that there are a lot of half body Images instead of the more diverse full body ones we saw with the previous prompt before we added the FCC Lydia keyword this might be because our training set has a lot of half body images and that's what this model selected for if you are doing your own character remember to have some diversity in your 20 set poses all right now let's try the model that has been trained for 10 epochs to see if there is any major difference the results are not significantly different but do notice that in one of the images we get some hint of some glasses without us having to enter the glasses as part of the tags now let's try adding a lot more text and being more specific with the model to see if it generates more detailed images all right now the results have a lot more resemblance to Lydia well in lurgical RPG Lydia worked at a cafe as a barista while she was also learning how to code on the site so let's change up the background to see if we can make Lydia barista so Cafe and Barista outfit and the April all right looks like we might be missing the classes so let's try again we can also change our base model so that we get a completely different base art style I have another one and we can give that one a try too all right now the model is fully loaded let's click the generate button well this open EG model is supposed to mimic the style of an easy Journey which is used by midjourney so it looks like it has a more vibrant art style which people might enjoy better feel free to experiment with different base models and see which one you like the most next we can try adding more loros and generate more complex images navigate back to the Civic website that we have open and we can see that besides those checkpoint models there are some lower models and if we click on those sample images they have the generation data there they have the text prompt negative prompts and simpler number of steps Etc these generation data are a really good reference and especially if you copy the seed to you should be able to get an image very similar to the sample images it's no guarantee that they're going to be exactly the same but they should be very similar alright let's find some Laura to use with the lower models that we have already trained for example I like this Laura model which is trained on black and white manga images and they should give you images a manga like style if you copy their trigger words let's download it and put it in our models Laura folder now on my web Eli if I refresh I should be able to see the anime line art manga like model and let's add it to our prompt we need to add the trigger words too which are line art and monochrome alright we can see that now our images look like black and white Manga style that's pretty cool in the next part we are gonna cover how to use control net to gain more fine grain control of your image generation here is the GitHub page of the control net web UI plugin control net actually gives you finetuning power over your images you can fill in your line art with AI generated colors you could just scribble and let AI fill in the rest for you or you could even control the pose of characters in your picture let's scroll through this page and get to the installation parts follow the installation steps and if you run into any security warnings you might need to add enable insecure extension access to your webuser.show and then restart web UI and try the installation after installation just follow these steps and download the models you need and put them in the correct folder once you've done this it's time to go back to the web UI and try out the control net plugin alright so when you expanded the control net plugin looks like this there's an area from two drag and drop images and there are several parameters that you could set so I'm going to turn on low vram and also just enable the control net plugin Pixel Perfect and I have two models I have a scribble and a line art let's redo our text prompt both the text prompt and the control net parameter setting will affect how the images turn out cool so I'm gonna connect to my tablet and just draw something really random on those canvas and see what we can generate from there oh don't play too much bets on my drawing skill I'm just gonna roughly draw a person it's really hard to use the provided pen tool so yeah this is the head those are the eyes and the body oh I hope control net and the amazing AI is going to help me fix my drawing well I'm being totally unnecessary here but I'm gonna draw some ribbons and they of course don't even look like ribbons well we can see that the generated images basically followed all the line artwork I did and building some pretty amazing looking colors it's actually pretty impressive and they even got the ribbons right well that was for the scribble model now let's use some real line art and try the lineup model as well so I do have some promotional artwork I did for learn to go RPG so let's try it now let's configure our text prompt we'll do the regular easy negative for the negative prompts then pick a sampling method and we want the same to be a cafe what the line to be a softer and detailed you might have noticed that we actually didn't put in any request for people to be in the image we'll see what a general image actually has people in it or not and this time we do the lineup model so because we didn't ask for any people to be in the artwork we are only getting images of the cafe background looks like I guess we need to explicitly put in two girls in a prompt and it looks like this time we are getting girls in the images looks like AI has filled in colors for our line art so that was the result of the lidar model the colors are a little bit weird so let's give the AI some more freedom and use the scribble one instead well now the AI has full freedom to imagine the faces of the two girls and looks like the results are pretty decent the images are in square but our line art original picture is in the Square so let's fix up the aspect ratio again with the scribble model let's generate some more images so yeah on the left is my original line art and looks like the AI have generated something that looks pretty vibrant color these actually look pretty good I'm very impressed all right besides control net there are some other really powerful plugins or extensions maintained by other open source contributors I'm gonna show you where to find those so on the Wiki page of the stable diffusion UI get repository we see a page for extensions for example this one looks like it works with control net and this vram estimator will allow you to increase dimensions of your image and batch size until you run out of memory so you can maximize your vram usage this one will allow you to draw some poses this one says it will Target an area to selectively enhance details this one I've seen a lot of people use to generate videos and it works with the control net extension tube this one will help you finetune where your Laura is acting on in the image this theme seems to be showing you a custom thumbnail view of your Laura model this one is for localization this one is for editing posts I'm just scrolling through it's a long list just to see what kind of core effects we can have this one converts your images into pixel art style looks pretty cool this one use Transformer models to generate text prompts for you another pixel art and other effects too another prompt generator this one saves intermediate images while the AI is trying hard to generate them there's even this one that will automatically remove backgrounds and leave only the central figures yeah so there are lots and lots of those extensions out there there's a lot for you to explore we can't possibly cover everything in a single course and people also create their own plugins and extensions as they see their new device moving on let's talk about generating images using the stable diffusion API there is this API Doc Page on the stable diffusion repository website and to enable the API we need to turn on dash dash API in our web UI user.shell we can see that there are several endpoints for example the text to image API endpoint image to image API endpoint and when we scroll down we see sample payload we're just put in our prompt and number of steps and other parameters in the payload and it will be exactly the same as if we were using the web UI itself so the idea is that we send the parameter payload to the web UI API endpoint using a post method and then we will retrieve some bytes that we can decode into our image below the developers have provided us with some simple python called snippet that we can use to query the endpoint and get our images and save it as the output.png all right let's grab those code snippet and run it locally all right for now I'm just going to copy paste it and run it directly and I will talk through the code in a little bit once we see the output just know for now that our payload is a simple puppy dog if we don't have some of the modules installed that we have to import we need to install them using pip and just run python3 sd.py and wait for it to finish now it looks like it has finished and we have an output.png file which is a puppy all right let's try some different props foreign we can add negative prompts into the payload too we're on the file again and see what we are gonna get oh it looks pretty futuristic and really cool of course we could append our alarm modeling to it and see if we can get Lydia generated foreign because we have increased the number of steps and we have a picture of Lydia the last five minutes of the video is like a bonus part we're gonna use PostNet which is an app for us to test out API endpoints we are going to look at how the images is represented in the API response and we'll also take a deep dive into the python code and walk through it line by line this is the postman app we're going to put in our request URL there which is the text to image endpoint and we're going to use the post method make sure the body is Json Raw and click send so we get this long string representing our images and then in the response we could see some other parameters that we don't need to pay too much attention to I searched for some base64 to image decoder online and we'll see what it works or not all right remove the quotes remove the starting quilt two and here's our image now let's walk through this python file line by line so here we're doing a bunch of imports we imported Json we imported requests for making a post request and we import a base64 for decoding and we imported some image libraries and we set our URL to be the public instance that our web UI is running on and we create our payload just like this and then we post to our URL and the specific API endpoint which is text to image given the payload and I get the response convert it to Json and remember the images field that we saw that's our image string so we decode it into an image this part here is retrieving some metadata for the PNG for example what parameters are used to generate a PNG Etc and finally we save this image alongside with this PNG info it's pretty straightforward feel free to play with the code on your own you can also make API requests in other programming languages of your choice all right as promised in case you don't have access to a GPU here are some ways you can run stable diffusion on some free online platforms of course there is going to be restrictions like you won't have access to all the models you want you won't be able to upload your custom model and the other people on the internet are also using the servers so you could be in the queue for a long time before your images are generated here we are on hugging face and we will go through spaces and in the search bar we type in stable diffusion and let's sort by most number of likes so looks like the most popular one is the one by stability AI which happened to be the original one so this will be the best option however it shows that it is passed so I don't think you'll be able to use it let's take a look at the other ones that we have access to of course these online models might be limited in that it doesn't have the base model you want it doesn't have the style that you want but this is the best we can do if we don't have access to a local GPU foreign s are a little distorted so maybe the space model isn't great for generating anime images and that's one of its limitations looking through the list we see some image to image some in paintings some photo generic one I think we'll be using the photo generic one let's give it a shot now this one looks like the stable diffusion standard web UI that we're used to and we can see that it's currently live and running on an online GPU they say the model is a protogen which should be a photorealism model foreign looks like they are restraining the number of images we can generate but let's do it and also we will have to wait in the queue so yeah if you don't like to wait or if you really need to use your custom models you should consider getting your own GPU all right finally after waiting for almost five minutes I got my result back it looks pretty good and with that we'll conclude today's tutorial I hope you enjoyed it and I hope to see you in the next video
