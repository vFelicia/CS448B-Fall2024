With timestamps:

00:00 - in this course you'll dive into
00:01 - deploying expressjs and node.js
00:04 - applications on AWS Lambda using neon
00:07 - serverless postgress and the serverless
00:09 - framework by the end of this course
00:11 - you'll also learn how to adapt your
00:13 - deployment for versel alongside
00:16 - mastering database setup migrations
00:18 - GitHub actions and more Justin Mitchell
00:20 - developed this course Justin has created
00:23 - many excellent courses on his coding for
00:25 - entrepreneurs course platform and his
00:27 - YouTube channel neon provided Grant to
00:30 - make this course possible hello and
00:32 - welcome to the serverless nodejs API
00:35 - course in this one we are going to learn
00:38 - about expressjs to build what's called a
00:40 - rest API now we're really going to be
00:43 - building a JavaScript application that
00:46 - runs on the back end in a serverless
00:48 - environment which really means that we
00:50 - just focus on our code we don't have to
00:51 - focus on the servers at all now
00:54 - traditionally speaking when it comes to
00:55 - seress a big challenge that comes up is
00:58 - what database to use in and how do we
01:00 - actually connect to it now for that
01:01 - we're going to be using neon and servess
01:04 - postgres neon is actually Paving the way
01:06 - for so many applications to leverage
01:09 - serverless databases as well and of
01:11 - course they are a partner in this course
01:13 - but the idea here is we need an
01:15 - application that we can scale up or
01:17 - scale down as needed and we'll do that
01:20 - using AWS Lambda serverless functions
01:23 - but then we will also need our database
01:25 - to match that scaling up and scaling
01:27 - down and that's where neon comes in but
01:29 - it gets better better than that what we
01:30 - can do is we can also have multiple
01:33 - versions of our application deployed so
01:35 - we can do these iterations share them
01:38 - with other people and test out little
01:40 - pieces of it while also simultaneously
01:43 - having our database made and ready for
01:46 - that neon does this really cool thing to
01:48 - Branch data at any given time you can
01:50 - look at a point in time and Branch it so
01:52 - you can work on multiple versions of
01:54 - your application at once and allow
01:56 - people to use it without having major
01:59 - overhead to make all of this work it's
02:01 - fantastic it's a really straightforward
02:03 - way to do all of this now the idea here
02:05 - is really going back into focusing on
02:07 - the code and being able to iterate as
02:09 - much and as fast as we possibly can so
02:12 - sure we are going to be deploying an API
02:14 - service itself but that's really just
02:16 - for a foundational piece so we can
02:18 - actually focus on getting our code
02:19 - better and better and better over time
02:22 - so this is really about setting that
02:24 - foundation and serverless is the perfect
02:26 - way to do it so let's talk a little bit
02:28 - more about why serverless is important
02:30 - and why you should care about it let's
02:32 - talk about what servus means for your
02:35 - application and why you should consider
02:37 - using it first and foremost it's all
02:39 - about focusing just on the code and
02:42 - effectively running that code now the
02:44 - best analogy I have is lights like quite
02:47 - literally the lights in your room right
02:49 - so when you walk into your room you turn
02:50 - your lights on when you walk out you
02:52 - turn them off right but what if you
02:54 - forget to turn them off well they just
02:55 - keep running whether or not you're using
02:57 - those lights or not servus is just like
03:00 - that now a normal server application you
03:02 - turn the server on when you want it to
03:04 - run and you could leave it running
03:05 - forever that doesn't mean it's being
03:07 - used all the time but it does mean that
03:09 - it's running forever now of course
03:10 - there's a lot of cost Associated to that
03:12 - to have it run forever because computers
03:15 - just have a bunch of teeny little lights
03:16 - that really that's how it works right
03:18 - but the idea being that if you leave it
03:20 - on it's going to continue to run always
03:23 - but there's another part of this is like
03:25 - what if you have just a small little
03:26 - light turned on maybe it's just your
03:28 - desk light and that's the light that you
03:29 - have on and then when you leave that
03:32 - desk light is still on but then a 100
03:34 - people walk into the room that desk
03:36 - light is not going to light up the room
03:38 - you'll need a lot more lights to do that
03:39 - and therefore you would have to scale
03:41 - your lights up now it's not any
03:43 - different in a server environment when
03:45 - you have one server yes you can have a
03:47 - tiny server that maybe costs a few bucks
03:49 - a month to actually run your application
03:51 - but as soon as you get thousands of
03:53 - people trying to access that server well
03:56 - you're going to need to scale up you're
03:57 - going to need to load balance across all
03:59 - of these different other servers which
04:01 - means that you'll have to actually
04:02 - understand the infrastructure behind it
04:04 - you'll have to learn how to scale those
04:06 - things up now don't get me wrong I find
04:08 - that stuff really fascinating it's
04:10 - something that I really like doing it's
04:12 - a fun orchestration but the thing is not
04:14 - everyone's ready for that or is
04:15 - interested in doing that especially with
04:17 - business critical applications maybe you
04:19 - don't want to manage those servers and
04:21 - so that's where these serverless
04:23 - Services come in servus is really just
04:25 - focusing on the code you deploy your
04:27 - code and then somebody else makes sure
04:29 - that all the systems scale up to meet
04:32 - the demand and all that scaling happens
04:34 - automatically but when you use something
04:36 - like AWS Lambda you just focus on making
04:39 - sure your code runs and then once it
04:41 - does run you can deploy it to Lambda
04:43 - Lambda will scale up and scale down to
04:45 - meet request demands now Lambda isn't
04:48 - the only servoless service that's out
04:50 - there there are other servess options
04:52 - but Lambda is a great one it's one that
04:54 - helped Pioneer so much of what we take
04:56 - for granted with serverless today so
04:59 - it's actually really great but actually
05:00 - deploying to Lambda presents a few
05:03 - problems for us number one deploying
05:05 - directly to Lambda with code is tricky
05:07 - there's a lot of different things that
05:08 - we need to consider with doing that and
05:10 - if you don't know AWS it becomes even
05:13 - more tricky because there's a lot to AWS
05:16 - and that's where we use something like
05:17 - the serverless framework we really just
05:18 - use the open source framework to help
05:21 - orchestrate the deployment to Lambda so
05:24 - that it's only a few commands instead of
05:25 - trying to figure out a bunch of commands
05:27 - in there now there are things that we
05:29 - will discuss in terms of I am or access
05:32 - policies and all that with servis and
05:35 - AWS um but you know it actually is
05:37 - fairly straightforward with servess I've
05:39 - I've seen a lot of different Services
05:40 - over the years that deploy to AWS Lambda
05:43 - I can honestly say that servess is one
05:45 - of the best if not the best way to do it
05:47 - specifically for Lambda and what we're
05:49 - trying to do with nodejs now the other
05:52 - part that comes as a challenge when it
05:54 - does deploy a serverless application the
05:57 - serverless application typically doesn't
05:59 - interact well with databases because
06:01 - databases themselves historically are
06:04 - always on that's because they pretty
06:06 - much always need to be available to grab
06:08 - that data so traditionally speaking
06:10 - that's what they do and the reason we're
06:12 - using neon and well neon is pioneering
06:15 - this serverless for databases and
06:18 - specifically for SQL databases and
06:20 - postgres postgres is probably the most
06:22 - used database in the world and so a
06:24 - serverless postes really just unlocks
06:26 - this for your database as well so in
06:29 - addition to your code application your
06:31 - database is going to do that as well now
06:34 - there's a huge advantag as to having
06:36 - serverless databases as well is this
06:37 - point in time branching so what's going
06:40 - to happen is whenever you need a copy of
06:43 - your database with neon you can just
06:45 - Branch it just like you would with your
06:46 - code and it will actually make a point
06:48 - in time copy of that data so then you
06:51 - can actually stage your application many
06:54 - many different times so if you're
06:55 - working on version 10 or version one and
06:58 - all of the versions in between between
06:59 - you can actually create different
07:01 - branches of that in your database do the
07:03 - migrations make the changes that you
07:05 - need for your database all right there
07:07 - and all of this is possible because of
07:09 - how they approach serverless and how
07:11 - easy they make it for your applications
07:13 - to run and consume all of this stuff so
07:16 - that's really what we're going to be
07:17 - doing is orchestrating all these things
07:19 - together now I realize it might sound
07:21 - complicated but it's actually fairly
07:23 - straightforward thanks to both AWS
07:26 - Lambda and neon really you just focus on
07:29 - code uh and you just pass in in the case
07:32 - of neon you're going to have a database
07:34 - URL string that Express will consume
07:38 - using the neon servess package that
07:41 - works with JavaScript so it's actually
07:43 - fairly straightforward there are some
07:45 - moving parts so yeah you probably want
07:46 - to know some JavaScript to get this
07:48 - going but we'll talk about that in just
07:49 - a moment but that's a high level
07:51 - understanding of why Servo is in the
07:53 - first place it's far more effective it's
07:55 - far more efficient than managing a
07:58 - server yourself now of course if you're
08:00 - not ready to really scale up your
08:02 - application and you're not ready to
08:03 - build into this environment you can
08:05 - still bring around the expressjs
08:07 - application and your neon application or
08:10 - your neon database with you to wherever
08:12 - you want to deploy it doesn't have to be
08:14 - servess and so we actually do take a
08:16 - look at how to deploy this on for sale
08:18 - for that very reason is so you can see
08:20 - how portable it can be by leveraging a
08:23 - manage database service in this case a
08:25 - serverless one that ends up costing
08:27 - significantly less so I'm really excited
08:29 - get into this one so let's go ahead and
08:30 - take a look at this course let's talk
08:32 - about the requirements you'll need to do
08:34 - well in this course and then we'll also
08:36 - do a highle overview of what it is we're
08:38 - going to be using what technology we're
08:40 - going to be using now first and foremost
08:42 - this is a JavaScript heavy Series so
08:44 - that means that you need to know stuff
08:45 - like functions classes objects arrays
08:49 - string substitution handling promises
08:52 - and callbacks maybe even Json once you
08:55 - know that stuff in JavaScript then you
08:57 - can move to the next part which is using
08:59 - node.js or serers side JavaScript so be
09:02 - sure to install node.js grab 20 LTS so
09:06 - version 20 is the version we're going to
09:08 - be using but whatever version you end up
09:10 - using make sure it's an LTS one unless
09:12 - of course you know what you're doing but
09:14 - the idea here is we're going to be using
09:15 - nodejs to actually run our JavaScript so
09:18 - we'll create a Javascript file and then
09:19 - use node to run it now if you're from
09:22 - the browser world of JavaScript as in
09:24 - writing stuff like react or vanilla JS
09:27 - node.js is just on the server side your
09:29 - web browser won't run your JavaScript
09:31 - node.js will although they work
09:33 - basically the same which is one of the
09:35 - Magics of node.js so once we actually
09:37 - have our application running locally
09:40 - we'll start to deploy it to a serverless
09:43 - function called AWS Lambda now at a high
09:45 - level serverless just means that it runs
09:47 - when it needs to that's what we're going
09:49 - to be using Ed Lambda for and that's how
09:51 - they can offer things like 1 million
09:53 - requests free per month on their free
09:56 - tier which is amazing that's so many
09:58 - requests to pay $ Z for so the next part
10:01 - of this of course is not just running
10:04 - the actual code itself but Al also
10:05 - storing the data that we need to store
10:08 - and for that we're going to be using
10:09 - serverless postres for this thanks to
10:11 - neon neon is really pioneering the next
10:14 - generation of what post CR can be by
10:17 - implementing things like servess and
10:19 - many other features to make it a very
10:21 - very powerful offering for your
10:23 - applications this is very very
10:25 - straightforward as we'll see but the
10:27 - idea here is these are going to work int
10:29 - to have a fully seress application from
10:32 - the actual node.js side of it itself as
10:35 - well as our database is fantastic then
10:38 - to actually build and deploy the
10:39 - application we're going to be using
10:40 - GitHub and more specifically GitHub
10:42 - actions to have a serverless part of
10:45 - that as well that's going to handle so
10:47 - much of that so if you don't know git
10:49 - definitely brush up on that as well now
10:52 - finally the main part of AWS that we're
10:54 - going to be using really is
10:57 - awsam this is because of the the tools
10:59 - that we'll use we really just have to
11:01 - focus how to make the right policies to
11:03 - do the deployments itself we're not
11:05 - going to have to manually deploy to AWS
11:07 - Lambda because that's kind of tricky and
11:09 - it takes a long time to configure when
11:11 - we can really just automate the process
11:13 - and really only focus on the I am access
11:16 - the policies all of that that's one of
11:18 - the things that we'll be doing for sure
11:20 - so make sure that you already have an
11:21 - account on AWS and also that you already
11:23 - have an account on neon and all of those
11:26 - links are in the description of course
11:27 - and then definitely sign up for GitHub
11:29 - as well now a couple optional things
11:31 - that we'll do towards the end are
11:33 - deploying to versel so deploying to
11:35 - versel is going to basically take
11:37 - everything we did for AWS Lambda and
11:39 - then deploy it on versel and that's
11:41 - really where we're going to wrap things
11:43 - up to really just see how portable we
11:45 - can make this application and of course
11:46 - when we deploy to versale we will still
11:48 - be using neon our serverless database so
11:51 - that's going to be very very straight
11:53 - forward so we can see how we can really
11:54 - move the code around without doing much
11:56 - to our database which is really nice to
11:58 - to keep that Integrity now the final
12:00 - thing that's optional is actually using
12:03 - visual studio code or vs code this is
12:05 - the text editor I like using the most to
12:08 - me it's by far the best one that is also
12:10 - free so if you have one that you prefer
12:13 - by all means go ahead and use that one
12:14 - I'm going to be using visual studio code
12:17 - so with all those requirements out of
12:18 - the way let's go ahead and jump into
12:20 - actually building out our project of
12:22 - course with node.js already installed
12:25 - which we'll verify when we build out
12:26 - that project as well now when it comes
12:28 - to building for AWS Lambda one of the
12:31 - key things we have to think about is how
12:33 - do we actually even test or develop
12:35 - locally for a serverless function and
12:38 - the way we're going to do this is by
12:39 - using the serverless framework from
12:41 - serv.com now this framework is going to
12:44 - make it easy for us to test locally as
12:46 - well as deploy into production but
12:48 - before we even get there we need to jump
12:50 - into the AWS console to just verify
12:53 - which version of node.js we should even
12:55 - use I know I mentioned that we should
12:57 - download and install the LTS one in the
13:00 - last part but we want to verify that
13:02 - that LTS is even available in Lambda
13:04 - itself so if we go into AWS into Lambda
13:07 - you can do a quick search for Lambda as
13:09 - well we're going to go ahead and hit
13:11 - create function and in here we just want
13:13 - to look at the runtime and the run times
13:16 - that are available right now as we see
13:19 - there are a number of them on there I
13:21 - would say node and python are probably
13:22 - among the most popular ones there but
13:25 - the idea is node 20 is available and
13:27 - therefore the one that is the current
13:29 - LTS is also available so download what's
13:32 - available there so we can actually test
13:34 - it out locally and so in my case I've
13:36 - got the node version 20 There It Is I've
13:39 - got mpm 10 which comes in with node
13:42 - version 20 and then I've got the
13:43 - serverless framework is what I want to
13:45 - use to actually test out AWS Lambda
13:49 - we're also going to use serverless to
13:52 - create our project that's what we want
13:54 - to do now is we're going to use that
13:55 - package to create a project so to do
13:57 - this we're going to do NP PM install DG
14:01 - serverless and this will install that
14:04 - serverless package for us in my case I
14:06 - already actually have it installed so
14:07 - it's actually pretty fast but the idea
14:10 - then is once we have serverless there's
14:12 - a lot of different Frameworks that we
14:14 - can use from it so it has a lot of
14:16 - different features in here as well many
14:18 - of which we are not going to cover at
14:20 - all but this is what we're going to do
14:21 - we're going to create a new serverless
14:23 - project and then we're going to navigate
14:24 - into that directory and then we'll
14:26 - actually bring it into our vs code
14:29 - instance so to do this I'm going to
14:32 - navigate into a location where I like to
14:34 - store my projects which in my case is in
14:37 - the dev folder and we'll call serverless
14:40 - right here hit enter okay and so what we
14:43 - should see now is different kinds of
14:45 - serverless functions we might want to
14:47 - build out and so what I'm going to be
14:49 - doing is the express API one and this is
14:52 - just to get some pure expressjs built
14:55 - out for us or the package for us I'm
14:57 - going to go ahead and save that and I'm
14:59 - going to call this my serverless nodejs
15:03 - API okay and so it's going off of the
15:06 - node Express and all that I'm not going
15:09 - to register a login for the serverless
15:11 - framework I don't need to do that and
15:13 - I'm also not going to deploy right now
15:15 - in fact I don't really have a way to
15:17 - deploy yet and so we will worry about
15:19 - that when we get there uh but for now
15:21 - what I want to do is I want to navigate
15:23 - into this server list no. JS API and
15:27 - we're going to go ahead and open this up
15:29 - into vs code just like this and so I'll
15:32 - go ahead and start off and save this as
15:34 - a workspace and then I'll also do my get
15:37 - in it definitely want to make sure that
15:39 - I have that notice there's already a g
15:41 - ignore file in here so it's not
15:42 - something I need to worry about just yet
15:45 - next thing what I want to do is prepare
15:47 - this to work with the correct postgres
15:50 - so when it comes to postgres databases
15:53 - you need to have a client of some kind
15:56 - to connect to that postc database so
15:58 - traditionally speaking you might use
16:00 - something like node postgres node
16:02 - postgres is a fantastic package that
16:04 - makes it really easy for something like
16:07 - expressjs to connect to a postres
16:09 - database which works really well when
16:12 - the node application is continuously
16:14 - running even if it's updating and it has
16:17 - some downtime overall it would be
16:19 - continuously running right and so when
16:21 - it comes to something like AWS Lambda
16:24 - it's doing the opposite of that it's
16:26 - only running when it needs to run and
16:28 - therefore it's going to be down a lot
16:31 - and so that's where something like node
16:33 - postgres isn't well positioned to make
16:36 - that happen maybe at some point it will
16:38 - be but the thing is neon actually
16:41 - pioneered neon Serv list this actual
16:43 - database which is ideal for serous and
16:45 - Edge deployments using htps and
16:48 - websockets so that persistent connection
16:50 - can be interrupted on a regular basis
16:53 - because of these two things and
16:55 - therefore is very well suited for any
16:58 - kind of Ser lless including AWS Lambda
17:01 - and it also can use persistent
17:02 - connections because it's a drop-in
17:04 - replacement for that node postr package
17:07 - and it's also what this is based on but
17:09 - again it's designed for this modern
17:11 - serverless architecture that AWS Lambda
17:14 - has that we've seen in a lot of
17:15 - different places as well and so that's
17:17 - the actual package we'll end up using to
17:19 - connect to our database and of course
17:21 - our database is also provided by Neon
17:24 - but the idea here is we definitely need
17:25 - to use this postres package or this
17:28 - postes driver and client to be able to
17:31 - connect our expressjs application to it
17:34 - so let's go ahead and install this as
17:35 - well I'll go ahead and use the mpm
17:37 - install command as well so we'll install
17:40 - this into our application and while
17:41 - we're at it we're also going to go ahead
17:43 - and do mpm install DG and it's going to
17:45 - be the neon command line tool so we'll
17:48 - use this to use all sorts of things
17:50 - related to Neon and so with that
17:52 - installed I'll also go ahead and do neon
17:54 - CTL and off so I can actually log in to
17:58 - neon and verify my local machine on this
18:01 - to do all of these sorts of things which
18:03 - in my case I'll go ahead and authorize
18:05 - you can also use an API token which is
18:08 - something we will do when we get into
18:09 - the GitHub actions of it all and we can
18:11 - see where those credentials are saved
18:13 - right there okay so we now have our
18:16 - Baseline project set up right it's ready
18:18 - to go and overall there are still a few
18:21 - other installations that I want to bring
18:23 - in and those are related to how we
18:26 - connect to our database and how we
18:27 - manage databases schema so this is going
18:31 - to be done using drizzle o and drizzle
18:34 - kit both of these things will be
18:35 - installed right now so let's go ahead
18:37 - and install drizzle kit
18:39 - omm and and then we'll go ahead and
18:42 - install drizzle kit and to do this we'll
18:44 - do mpm install D capital D drizzle dkit
18:49 - and go ahead and install that as well
18:51 - which it might have actually done with
18:52 - Drizzle it did not so drizzle kit will
18:55 - be used as well to help us with our
18:57 - migrations the drizzle omm will help us
19:00 - design out our schema once we get to
19:03 - that portion but these are the main
19:04 - packages we're going to be using if we
19:06 - need to add any later we will uh but
19:08 - overall for right now this is what we're
19:10 - going to start with and we really need
19:12 - to see how to do a Hello World with this
19:15 - serverless package and see exactly what
19:18 - that's going to look like in conjunction
19:20 - with our neon database when we get there
19:22 - in a standard server environment the way
19:24 - you run expressjs is by modifying what
19:27 - we have here by using app. listen and
19:31 - then we can pass in some sort of Port
19:33 - value here and this port value then can
19:35 - have a callback function once it
19:37 - actually starts running and you can do
19:39 - something like console log and then
19:41 - running at HTTP col
19:45 - localhost
19:46 - 3000 right and so what this will allow
19:49 - us to do then is to open up our terminal
19:52 - and we can now run node
19:54 - index.js now before I even look at that
19:56 - I'll close it out and turn that off I'll
19:58 - comment that out again and run it again
20:01 - and we see it goes away this is actually
20:03 - a good way to think of serverless right
20:06 - so the first version this is going to
20:08 - continuously run no matter what the
20:11 - second version is running when we tell
20:13 - it to run and the way we restart the
20:15 - first version is well quite literally
20:18 - restarting the application itself and
20:20 - then making sure it runs so now we've
20:23 - got that let's go ahead and take a look
20:24 - at that homepage and we can do that by
20:26 - going into our web browser we could o up
20:28 - open up a new terminal window and we
20:31 - could just run curl HTTP col
20:35 - localhost and 3000 and that will give us
20:38 - the same response back great so that's
20:41 - showing us that expressjs is working and
20:43 - in some sense it's actually a hello
20:45 - world from expressjs but of course I
20:48 - want to make sure that my serverless app
20:50 - is actually working and I can use it
20:52 - locally so the way we do that of course
20:54 - is by commenting out this app. listen
20:57 - this is for like a server
21:00 - full app which we are not using right
21:03 - and so let's go ahead and close out this
21:04 - node
21:05 - index.js so what we want to do is we
21:07 - need to update our local environment to
21:10 - handle the offline mode from the
21:13 - serverless framework to do this we're
21:15 - going to go ahead and use serverless
21:18 - plugin install dashin serverless Das
21:23 - offline okay so once we install that it
21:26 - should open up and install something
21:28 - into package.json related to the
21:31 - serverless offline package and again
21:33 - this is for development mode so we can
21:36 - emulate what the actual AWS lamb
21:40 - serverless would end up doing and so to
21:42 - actually run this we can use server list
21:45 - offline
21:46 - now and that will trigger the exact same
21:49 - application and it's running at the same
21:51 - port in this case so if I went ahead and
21:54 - run this again I get the exact same
21:56 - value except now if it's going based off
21:58 - of an AWS Lambda event or it's emulating
22:02 - exactly what that is it even shows you
22:04 - the build duration for this request now
22:07 - another thing that's cool about this is
22:09 - it can also emulate different regions so
22:12 - if we wanted to run that with a
22:13 - different region so like region and US
22:16 - West one you could do that and that
22:19 - would actually show you this different
22:20 - region we can also do different
22:22 - different stages which allows us for a
22:24 - lot of options here so if I do region
22:27 - and or rather stage and prod and hit
22:31 - enter I have just another stage that I
22:34 - can work off of and of course we're
22:36 - going to build this out quite a bit more
22:37 - throughout this course but the idea now
22:40 - is I have a really quick and easy way to
22:43 - test this serverless application itself
22:46 - but there's still some things that I
22:47 - need to modify the first one is going to
22:49 - be package.json what I want in here is
22:52 - the scripts de definition so we can
22:55 - actually call out those scripts that we
22:58 - might want like Dev and this is going to
23:01 - be just simply serverless offline and
23:03 - you can add in the stage of let's go
23:06 - ahead and say Dev for example the region
23:08 - in this case doesn't really matter that
23:10 - much uh but I'll go ahead and leave it
23:12 - like that that way I can do mpm run Dev
23:15 - and then it's still the same thing and
23:17 - it's still working just as it was before
23:20 - but then I don't have to remember all of
23:22 - the commands related to this serverless
23:23 - offline that's a big part of the reason
23:25 - I use scripts like this there are other
23:27 - reasons as well but again we're just
23:29 - sort of preparing for development mode
23:31 - in this at this point so the next thing
23:33 - is going into serverless yaml so what
23:36 - you might see right off the bat is well
23:39 - yes it added this plugins here perhaps
23:41 - you looked at this file perhaps not if
23:43 - you haven't looked at this file we see
23:45 - this functions in here this function
23:46 - Handler we'll talk about that in a
23:48 - second but then I also have this big red
23:50 - flag which is this right here that is
23:52 - showing us the runtime node.js
23:54 - 18 this is not the runtime we want now
23:57 - now to remember what run times we have
23:59 - available to us we can log into the
24:02 - console on AWS and we can navigate into
24:05 - Lambda and then go into create function
24:09 - and going down to what's listed here
24:11 - right so we have other options on here
24:14 - now we've already talked about it but
24:16 - 20. X should be the runtime we are
24:19 - targeting here so we're going to go
24:21 - ahead and save that we'll run this again
24:23 - in this case it doesn't actually change
24:25 - anything it doesn't change anything
24:27 - technically from how this is running
24:29 - because node 18 and node 20 aren't a
24:31 - whole lot different but at the at the
24:33 - point of this is to make sure that our
24:35 - runtime is matching our development mode
24:37 - and our production mode that's really
24:38 - the point there the next one I want to
24:40 - do is change how my API Handler is
24:43 - working so the API Handler is really
24:46 - just looking for this index. Handler
24:49 - which in our case is coming from the
24:51 - servis application itself and then this
24:54 - module right here that's exporting
24:57 - Handler that is what we're end up using
25:00 - so we've got is index. Handler if I
25:02 - change this to app. Handler I would need
25:04 - to change index to
25:06 - app.js right and you might end up doing
25:09 - that now in my case what I usually do is
25:11 - I create a file called SRC so SRC a
25:15 - folder there and then I move my app
25:17 - inside of SRC now it's probably a good
25:20 - idea to just keep it in as just index.js
25:23 - our application itself is not going to
25:25 - get that complicated so we're going to
25:26 - leave it as index. JS but with this it
25:29 - actually separates my application code
25:31 - from a lot of the configuration for the
25:33 - project itself which is what I always
25:36 - always do and so back into this server.
25:38 - yaml we can now just change this Handler
25:41 - then into SRC do index. Handler right
25:46 - and so that's going based off of the
25:48 - folders themselves so let's go a and
25:50 - save that and let's try that again with
25:52 - this run Dev in this case I can go ahead
25:54 - and run it again and now I get a problem
25:57 - in here so let's go ahead and change the
25:59 - the actual handle and Handler to
26:01 - actually doing not DOT notation but
26:04 - using the folder itself and so we'll run
26:06 - it again this time it works correctly
26:09 - okay so that dot notation is the
26:11 - intuition I would typically use to run
26:13 - some sort of Handler like this but it's
26:15 - important to be able to test these
26:17 - things out try them out and see exactly
26:19 - how to configure this so now we can
26:21 - actually start developing with this
26:23 - serverless offline framework it's really
26:26 - just that simple I don't think there's a
26:27 - whole lot more we need to talk about
26:29 - this in terms of getting the serverless
26:31 - offline working now I will say that you
26:34 - could probably experiment with a lot of
26:36 - things related to just doing your server
26:38 - full app that's probably going to be
26:40 - okay and realistically I wonder if we
26:42 - could have that server full app in here
26:44 - with the Handler itself if that causes
26:47 - any issues and it doesn't appear so
26:50 - mostly because of the export itself the
26:52 - actual name space for that export is
26:54 - what's being used it's not actually
26:56 - calling index JS like this app Listen
26:59 - would end up doing so basically we have
27:01 - support for either one which is also
27:04 - pretty nice okay so now that we've got
27:05 - this let's go ahead and see how we can
27:07 - load in environment variables into our
27:09 - application right off the bat at this
27:11 - point in your coding Journey you
27:12 - probably realize that it's not a good
27:14 - idea to hardcode API Keys database
27:17 - strings or really any secret data right
27:20 - inside of the code itself it's much
27:22 - better to inject that when the code runs
27:25 - so have it done in the environment at
27:27 - itself and the way we're going to do
27:29 - that is by using a EnV file this is very
27:32 - very common to do in all sorts of
27:34 - programming languages but the idea here
27:36 - is in index.js if I did something like
27:38 - database URL we don't want to have the
27:40 - actual database URL string in here we
27:43 - want that to be abstracted away and so
27:46 - the way we're going to do this is by
27:47 - using process.env so what we have here
27:50 - is process. EnV do and then the
27:54 - environment variable we want to use in
27:55 - our case it's going to be database URL
27:58 - so if I save that as is and then run our
28:01 - serverless application our offline and
28:04 - then curl to it what I'll get is this
28:06 - hello world here looks like maybe I
28:08 - didn't save anything so let me try that
28:09 - again with that curl I'm still not
28:11 - getting anything for this so let's go
28:13 - ahead and actually change this a little
28:15 - bit more and then go ahead and say
28:17 - something like not here okay so now
28:21 - we've got if it is there it will be set
28:23 - otherwise we'll just have it as not here
28:25 - we'll go ahead and re run our lless
28:27 - application and now I get this database
28:30 - URL not here of course this is just a
28:32 - fallback value not something we'll
28:34 - necessarily use for much longer but the
28:36 - idea is that we want to actually have
28:38 - this work and so that's what we'll do
28:40 - now now traditionally speaking you might
28:42 - use something like theb package this is
28:44 - a fantastic package and is used a ton so
28:48 - this is something you would absolutely
28:49 - consider using when it comes to well non
28:53 - AWS Lambda Services we want to prepare
28:56 - it for aw Lambda and the way we're going
28:59 - to do that is by using the server list.
29:01 - EnV plugin it's a fairly straightforward
29:03 - plugin and it allows for a lot of
29:05 - different support as we'll see right now
29:08 - so the idea is we want this database URL
29:09 - to work and the way we can do that is by
29:12 - coming into our server. gaml and we can
29:15 - Define our environment and so in this
29:17 - environment I can add in our database
29:20 - URL here and I could set it equal to EnV
29:23 - colon as in the environment colon and
29:26 - then the actual value value we want to
29:28 - set from the EnV file like that I can
29:33 - also have a
29:36 - fallback like that okay great so this is
29:39 - sort of configuring it but we still need
29:40 - to add in the plugin itself so let's go
29:43 - ahead and close out this server and now
29:44 - we're going to go ahead and run
29:46 - serverless plugin install dasin
29:49 - serverless
29:51 - dv- plugin hit enter and this will
29:55 - install something in our serverless Di
29:57 - gaml
29:57 - as well as package.json will be this
30:00 - right here and so now that we've got
30:02 - that we can come back into Ser yaml we
30:04 - see that this is one of the plugins
30:06 - that's available to us which of course
30:08 - will be very useful when we actually
30:10 - push this into production then I also
30:12 - want to add in use. EnV being true so
30:16 - we'll go ahead and save that and I just
30:17 - want to see if the fallback value shows
30:19 - up if the EnV value shows up or if the
30:23 - inline code value shows up so let's go
30:25 - ahead and try this out by running it
30:27 - again I don't need to reinstall I'll
30:29 - just go ahead and run mpm run Dev and
30:32 - then we'll run our curl command and we
30:35 - got something else in here so what's
30:37 - happening in in our serverless DL it's
30:40 - fallback value is actually not being
30:43 - used but rather the EnV value is being
30:45 - used so at this time in my get ignore I
30:48 - want to actually add in EnV here with a
30:50 - star at the end because what I can also
30:53 - do is EnV let's say for instance DOD as
30:57 - in my development mode this is another
31:00 - value that I might have so we'll go
31:02 - ahead and say Dev DB right and so then
31:05 - if I wanted to use this Dev mode here I
31:07 - could go in package.json and quite
31:10 - literally change the stage to Dev mode
31:12 - which I still have it's actually on that
31:14 - stage so this Dev right here if I called
31:17 - it Dev ABC I would want to change this
31:19 - EnV to Dev ABC type of thing right so
31:23 - that stage is how I can actually modify
31:25 - this a little bit which which is also
31:28 - super nice and it actually will help
31:30 - prepare us and for our local environment
31:33 - for our staging environment and then
31:35 - finally for our production environment
31:36 - we could quite literally have all of
31:38 - those environment variables in here and
31:40 - then just call the different stage to
31:41 - run the different environment variable
31:43 - which we can test out really quickly
31:45 - right now what we see is that database
31:48 - URL coming through fantastic so the next
31:51 - thing is like maybe we want to add in
31:53 - another environment variable I'm going
31:54 - to go ahead and say debug and we're
31:55 - going to set this equal to one so this
31:58 - is now in myv back into serverless iaml
32:01 - I'm going to go ahead and do something
32:02 - similar um and this time I'll go ahead
32:05 - and say debug and we'll go ahead and
32:07 - come in here this time I'm to give the
32:09 - fallback value of zero and so now in
32:12 - index.js I'll go ahead and add that in
32:15 - as well and
32:18 - debug and this time I'll just go ahead
32:20 - and leave it like that save it and
32:25 - actually I'll go ahead and say it's
32:26 - equal to one so debug being equal to one
32:29 - based off
32:30 - ofv let's go ah and save that we'll
32:32 - restart everything and let's run it
32:35 - again this time I'll curl again and what
32:38 - we've got is false right so EnV is not
32:42 - equal to one let's go ahead and see if
32:44 - it's equal to or what it is equal to
32:47 - let's just console log
32:49 - it like that I'll go ahead and restart
32:52 - the application itself we'll run it and
32:55 - my console log is one
32:57 - what the what just happened here well
32:59 - let's actually try it or equal to the
33:02 - string of one and let's see what that
33:05 - looks like so go ahead and just put it
33:07 - in as a string value here and maybe we
33:10 - wrap this whole thing in a string as
33:13 - well now let's go ahead and try this
33:15 - again rerun this and run that again and
33:18 - now we get true so the actual values
33:21 - themselves are often going to be treated
33:23 - as a string itself so you need to be
33:26 - aware that when you're going through and
33:28 - using environment variables inside of
33:30 - your express application um now there
33:32 - are more advanced usages of the actual
33:36 - uh you know plug-in itself which I do
33:38 - encourage you to go into the serverless
33:40 - Frameworks plug-in documentation for
33:42 - that and you can see all of the advanced
33:44 - usage that's in here but generally
33:46 - speaking what we want to know about are
33:48 - three main things one is we have to
33:50 - define the environment variables in ser.
33:52 - yl that we're going to use in our
33:54 - application two we can use different EnV
33:58 - files to actually load in those
33:59 - environment variables and those EnV
34:01 - files can be as simple as just the
34:03 - standard EnV or of course based off of
34:07 - Any Given stage that we might want to
34:09 - have Dev test prod V1 V2 and then each
34:13 - stage can still inherit from that base.
34:16 - EnV which means that we don't have to
34:18 - rewrite all of the same variables over
34:20 - and over and over again like in the case
34:22 - of debug maybe we just want it always to
34:24 - be one so that all of the other stat PES
34:27 - can just go based off of one CU we're in
34:29 - our local development environment and
34:31 - especially because these EMV files would
34:32 - never be committed to the git repo and
34:35 - then of course in the final one our code
34:36 - itself needs to be aware of when the
34:39 - actual data types don't work as you
34:41 - might expect so that's another thing
34:43 - that's just to be aware of when it comes
34:45 - to using those environment variables and
34:48 - so at this point we now have a way to
34:50 - test out our application altogether
34:52 - notice that when I did change the EMV
34:55 - Dev the actual stage environment
34:56 - variable then the actual value came
34:59 - through so this actually overrode what
35:01 - was in the first one the EnV itself uh
35:04 - which Al is also pretty nice great so
35:06 - now that we've got this we have we're
35:09 - really just ready to start actually
35:10 - using that database URL we start the
35:13 - integration process with our neon
35:15 - postgres database so let's start that
35:17 - process now now we're going to take a
35:19 - look at neon serverless postres now what
35:22 - I want to show you here is the console
35:24 - on neon so we can really highlight what
35:26 - branching is all about it's that point
35:29 - in time snapshot of our database that we
35:32 - can essentially use a brand new copy of
35:34 - it isolated from the original one which
35:37 - is great for production applications and
35:39 - then staging instances or versions of
35:42 - our application and it happens really
35:44 - really quickly so of course we get all
35:46 - the benefits of serverless as well so it
35:48 - actually scales down to zero and then
35:50 - it'll scale up to meet demand as well
35:52 - our application needs it now the latency
35:55 - to actually scale from 0 to 1 one is
35:57 - surprisingly incredibly fast with neon
36:00 - as well so go ahead and go to neon.pdf
36:26 - versions as well I'm going to go ahead
36:28 - and use the default database name but
36:30 - feel free to change this as you see fit
36:32 - the last most important part is the
36:34 - region itself this region should be as
36:36 - near to your application as possible now
36:39 - in our case we're going to be using the
36:41 - default which is Us East Ohio at least
36:43 - that's my default and the reason for
36:45 - this is because it's going to be where
36:47 - we deploy our application on AWS and of
36:50 - course that region of Ohio is where
36:52 - we're deploying to us East to in AWS
36:56 - which will come back back to once we
36:57 - actually deploy that but the idea here
36:59 - is we want to start off with our
37:01 - database being in that region now it's
37:03 - actually really easy to create a new
37:05 - project with a new region so if you have
37:07 - to you could always just use whatever
37:09 - region is physically closest to you
37:11 - right now and then then when you
37:13 - actually start deploying your
37:14 - application you'll go ahead and change
37:16 - the region then okay so at this point
37:18 - let's go ahead and create that project
37:20 - and what we get right out of the gates
37:21 - is a connection string this is a
37:23 - postrest connection string that gives us
37:25 - our username our password our host our
37:28 - database and a few other configuration
37:30 - items now the other thing that's really
37:32 - cool about this when you first start out
37:34 - is you've got a lot of options for
37:35 - integration in here right and there is
37:37 - one for node.js but we are actually not
37:39 - going to use this integration option we
37:41 - don't need to for a number of reasons
37:43 - one of them being that we will just use
37:45 - that connection string that's what we
37:46 - want so basically what we'll do is copy
37:49 - this string whatever it ends up being
37:51 - you can click copy right here and then
37:53 - you'll bring it back into your local
37:54 - project into the environment vers
37:56 - variables when we did the EnV stuff and
37:58 - you'll paste this in and this is what
38:00 - we'll end up loading in once we get
38:02 - there for now I'm going to leave it just
38:03 - as a you know the original D Dev DB
38:06 - thing uh but the idea being that
38:08 - eventually we'll use this string for now
38:10 - though what I want to do is I actually
38:12 - want to play around with branching
38:14 - inside of our project so let's go ahead
38:16 - and close out this window here of course
38:19 - we can still get all of that data right
38:20 - in the dashboard also but what we want
38:23 - to do is we want to actually insert some
38:25 - data into our database now of course if
38:27 - we look at our tables here we have no
38:29 - tables yet there's really nothing in our
38:31 - database so let's go ahead and add some
38:33 - just by using the SQL editor that just
38:35 - gives us some default code that we can
38:38 - just run that will actually insert some
38:40 - data in for us and so what we'll see is
38:42 - if I go back into the tables tab I now
38:44 - see there's a table in here in which I
38:46 - can select and actually see the data
38:47 - that's coming through from that and so
38:50 - we could also just run that a few more
38:51 - times by going into our history here I
38:54 - can just run a couple lines here by
38:55 - selecting it I'm going to run that run
38:57 - it again run it again just so we have
39:00 - some more data that we are playing with
39:02 - just like that so I've got 40 rows in
39:04 - here right now so let's take a look at
39:06 - the magic of branching so if I go in
39:08 - this branches area here I'm going to go
39:10 - ahead and create a new branch and I'm
39:12 - going to call this my Dev Branch so
39:15 - current point and time that's what we're
39:16 - going to go off of but notice we've got
39:18 - other options in here we can also select
39:20 - which branch we want to be the parent
39:22 - the one that we're basically copying
39:24 - from I'm going to go ahead and create
39:25 - this new Branch from my main branch cuz
39:27 - that's the only one I have and now I get
39:29 - another connection string this is a
39:32 - different version of the exact same data
39:35 - but it's a brand new and unique
39:37 - connection string that I can use in my
39:39 - project specifically for like a Dev
39:41 - branch that has a lot of the same data
39:44 - so let's actually just verify that real
39:45 - quick by going back into that SQL editor
39:48 - and now what we want to do is just
39:49 - switch our branches to our Dev Branch
39:52 - I'm going to goe and run those two lines
39:54 - once again and now I'm going to go run
39:57 - it and let's run it again and what we'll
40:00 - see is if I go into my tables once again
40:03 - I'll change the brancher make sure that
40:04 - I'm on a new Branch I'll select the
40:06 - table that I just was playing with I'll
40:08 - scroll to the bottom and notice that I
40:10 - have 60 items in here now and if I were
40:12 - to go back into the main branch which I
40:14 - can do that quickly select it again and
40:18 - now we see once again the main branch
40:20 - Stills back at that 40 amount of items
40:23 - or 40 rows in there um and it happens
40:25 - just that quickly but the other thing
40:27 - that's great about this is if at any
40:29 - time we feel like deleting this I'll
40:31 - just go ahead and copy the name go ahead
40:33 - and delete this delete it now that
40:35 - project is completely deleted so what we
40:37 - can do of course is just bring it right
40:39 - back now of course this is going to be
40:40 - an empty project with empty databases
40:43 - but overall it's the same configuration
40:45 - just it doesn't have the data and I can
40:47 - spin it back up that quickly so of
40:49 - course actually bringing this back to
40:51 - what we just looked at I would just need
40:52 - to go back into that SQL editor and
40:54 - create those tables of course before I
40:57 - created tables I could just verify that
40:58 - yeah it actually is an empty database
41:00 - which no surprise it is you know like
41:03 - that kind of makes sense but the idea
41:05 - being that if I come back into the SQL
41:06 - editor I were to run it again and maybe
41:09 - again and let's say for instance I
41:11 - wanted to create a new Branch once again
41:13 - I'll go into branches create a new
41:15 - Branch Dev Branch from the main branch I
41:17 - go and create that and then we'll go
41:19 - ahead and go into well let's create
41:21 - another Branch we'll create one from the
41:23 - dev Branch we'll go ahead and do Dev V2
41:26 - right you can create as many branches as
41:29 - you really need I mean my project in
41:31 - this case has a limit of 10 but you're
41:33 - probably never going to go past 10 that
41:35 - would be surprising if you ever did but
41:37 - the idea of course is that we have all
41:38 - these different branches so I can go
41:40 - back into the SQL editor once again and
41:42 - run a few commands for the various
41:45 - branches that I see fit and it just is
41:47 - that fast it's it's so cool uh and it
41:49 - makes things really really easy and
41:51 - granted there's a lot of other
41:52 - configurations we might consider like
41:54 - new roles new database is these things
41:57 - get a little bit more advanced on the
41:58 - postgres side itself but overall there
42:01 - are other options that we can Implement
42:03 - now one of the main ones that we want to
42:05 - implement is well basically doing these
42:08 - things in an automated fashion and we
42:10 - will do that using the command line tool
42:12 - later the neon command line tool that
42:14 - will actually build up our branches it
42:17 - will do the branching for us it won't
42:19 - necessarily run SQL directly with the
42:21 - command line tool because that's not
42:23 - what we're going to be using it for
42:24 - instead you'll use a connection string
42:26 - to run the various SQL commands itself
42:29 - and then what we will also do is we'll
42:31 - implement the API keys so inside of
42:33 - developer settings here we're going to
42:35 - generate a new API and then use the
42:37 - command line tool specifically for that
42:39 - so that so much of what we just did will
42:41 - be automated you can also even automate
42:43 - the project itself which is something
42:45 - we're not going to do but we will
42:47 - automate the process of branching
42:49 - because it is a fundamental part that
42:51 - really just unleashes a lot of value
42:54 - that we can provide to to our
42:56 - development process right so whenever I
42:58 - need to actually have a Dev version or
43:01 - Dev V2 I can still use production data
43:05 - if I want to or I can just build out
43:08 - data that I need just for local testing
43:11 - and that data could be in its own branch
43:13 - of our original database and so then
43:15 - when we run migrations we can migrate
43:17 - each database as we see fits as in
43:19 - change the structure of the database
43:22 - whenever we need to on each branch as
43:24 - well so that parts also pretty cool but
43:27 - overall the idea here is if you actually
43:29 - let it sit for a little bit longer
43:31 - what's going to happen is these are not
43:32 - going to be active anymore and so that
43:35 - the actual databases will be ready to be
43:36 - used but they'll just become inactive
43:39 - and then actually reusing them or making
43:41 - any sort of call on these databases
43:43 - we'll spin them back up to meet the
43:45 - demand that you need as we'll see as
43:47 - well once we actually start implementing
43:50 - this database uh into our project so at
43:52 - this point go ahead and clean up the
43:53 - branches delete the ones that you don't
43:55 - need I'm going to be going just off of
43:56 - the main branch and we'll start
43:58 - integrating that into our project here
44:00 - in just a little bit let's take a look
44:01 - at how to use the neon command line tool
44:03 - to automate generating branches as well
44:06 - as getting our connection strings and
44:08 - then of course deleting branches when we
44:10 - need so the way we're going to do this
44:12 - is by starting from scratch we're going
44:14 - to create a brand new project I already
44:15 - deleted that project and then of course
44:17 - we're going to use the command line tool
44:19 - which there's a lot of documentation on
44:21 - it we're only really scratching the
44:23 - surface here and doing a few of the
44:24 - primary actions that I've been using
44:26 - using so to do this we want to make sure
44:28 - that the neon CTL is installed in my
44:31 - case it is and then I can also use just
44:33 - straight up neon and if you remember we
44:35 - used this mpm install DG neon CTL that
44:39 - was actually how I went about installing
44:41 - it now at one point I will use something
44:43 - called JQ which really just helps me
44:46 - parse Json data as you'll see and to
44:49 - install that it's Brew install JQ this
44:52 - is absolutely optional but it's one more
44:55 - thing that you could can potentially use
44:57 - to automate okay so once you actually
44:59 - have it installed you definitely want to
45:01 - run through neon CTL o now I already
45:03 - actually authenticated this I did all of
45:05 - this earlier so if you haven't done that
45:07 - go ahead and do that now and make sure
45:09 - that you have access to your local
45:12 - project now in my case if I actually go
45:14 - into neon CCL projects and list hit
45:17 - enter what I will see is absolutely
45:19 - nothing I actually deleted my project
45:22 - and it's really simple so to create one
45:24 - let's go ahead and create one real quick
45:25 - with a name here and I'm going to call
45:27 - this serverless and node js- API now
45:33 - this name flag there are it's pretty
45:35 - straightforward as far as how to create
45:37 - something I go ahead and create it and
45:39 - there we go I get all of the data that I
45:41 - might want including our connection URI
45:43 - or connection string that we can use to
45:46 - actually connect to this we're not going
45:47 - to use that yet instead what we're going
45:49 - to do is just delete this project I'm
45:51 - going to go aad and copy this uh
45:53 - Restless rice here and we'll go ahead
45:55 - and do neon CTL projects delete and use
45:59 - that ID and that will actually delete
46:01 - that project itself so then when I go to
46:03 - list everything out again I get nothing
46:05 - right and so that's really that simple
46:08 - to create a project and delete a project
46:10 - being that it is that simple we need to
46:12 - be aware that yes you could absolutely
46:15 - accidentally D delete a project that you
46:17 - didn't want to delete but in our case
46:19 - we're going to stick with just this for
46:21 - now and so I have my project created and
46:23 - I already have this connection URI now
46:26 - what I can do is I can actually run neon
46:28 - CTL and connection string hit enter and
46:33 - this will give me that same connection
46:35 - string right and so I can use this
46:37 - within the psql the actual local client
46:41 - that I have I can use that connection
46:43 - string and it's psql not
46:47 - pqsl but we can use that connection
46:49 - string just like that and it will allow
46:52 - me to jump into this like something like
46:54 - select now
46:57 - and I can run that and it'll give me the
46:58 - exact time that it is and we can quit
47:01 - out of here in a number of different
47:03 - ways you can write out quit you can
47:05 - write out exit it's actually pretty
47:06 - flexible as to how postgres works these
47:09 - days so that's the connection string so
47:12 - let's go back up there it is and so I
47:14 - can use it in many different ways now
47:16 - the other idea is if we needed another
47:18 - Branch right so if we were going to
47:20 - create a branch let's take a look at
47:21 - that so neon CTL and then branches G
47:26 - and actually let's go ahead and do
47:28 - branches list first we can see all of
47:30 - these things so to me the format works
47:33 - much like kubernetes if you're familiar
47:35 - with that but the idea is you get the
47:37 - neon CTL or just neon the actual
47:40 - resource that you're looking for and
47:42 - then the action you want to do right so
47:44 - it's going to be get list delete right
47:48 - or
47:49 - create and then in this case so we've
47:51 - got branches and if we do get let's go
47:55 - and grab this ID here and paste it in we
47:59 - get that particular Branch if we go neon
48:02 - CTL branches create well let's go and
48:06 - create a new one I'll go ahead and just
48:08 - write out Dev unknown command Dev now
48:11 - just like with our project we use D-
48:13 - name and hit Dev and then we get a name
48:16 - so the name and the ID are not the same
48:18 - right so we've got the ID here which
48:20 - what do you know that ID is also
48:22 - corresponding to the actual connection
48:25 - string itself but once again if I go to
48:28 - list out all of the branches I can see
48:31 - the various branches in here if I do
48:33 - neon and CTL
48:36 - branches git let's say Dev once again
48:40 - now I can actually call Dev right so
48:42 - when I go to create it I need to pass in
48:44 - the name argument if I don't create it I
48:47 - don't have to do that the other thing is
48:49 - when we create one let's go ahead and
48:50 - just not have a name argument at all and
48:53 - what do you know it actually creates one
48:55 - for us so it'll actually come up with
48:57 - our its own name and we can list all of
48:59 - those things out just like that so then
49:02 - if we want the connection string we can
49:03 - use neon CTL
49:06 - connection string and then pass in the
49:09 - branch that we're looking for so we can
49:11 - use the autogenerated one that the name
49:13 - and the ID look to correspond or we can
49:17 - use our own let's go ahead and use Dev
49:19 - in this case that Branch should be
49:21 - different than the main branch right and
49:25 - enough it is right so the user that's
49:28 - being created is not different and we
49:31 - could absolutely create a different
49:32 - users to actually run this connection
49:35 - which you can just verify inside of here
49:37 - as well there's these different roles in
49:39 - here that you can create for any given
49:41 - user that takes some extra that we'd
49:44 - have to do in the post side as far as
49:46 - their permissions and whatnot whereas
49:48 - this user has full permission to the
49:50 - database itself um but the idea here is
49:53 - now we can create branches really
49:55 - quickly we can create connection strings
49:57 - really quickly and then of course we can
49:59 - actually use those connection strings so
50:01 - in the case of Linux and Mac you can
50:04 - export let's say for instance our
50:07 - database URL we can set this equal to
50:10 - that neon command so neon CTL connection
50:15 - string and then you could do our branch
50:18 - of
50:19 - Dev something like that you hit enter
50:23 - that will run that command now I can use
50:25 - this connection string with the psql
50:28 - like
50:32 - this and let's make sure that we have
50:34 - this in double quotes actually not
50:36 - single
50:38 - quotes and there we go so now I'm
50:40 - actually able to connect and hide the
50:43 - database URL as that is one of those key
50:46 - things that you would want to do and so
50:49 - when it comes to postes itself the
50:50 - different clients actually downloading
50:53 - installing psql is something that I I
50:55 - recommend you do if you want to learn
50:57 - more about postgres itself so using that
51:00 - SQL editor like we saw before we had all
51:02 - of this stuff I can absolutely do that
51:04 - inside of postes SQL right so I'd go
51:07 - line by line to just make it simple
51:10 - we've got this example here we created
51:12 - that table and then I can go ahead and
51:14 - bring in some of those values again once
51:17 - again it's doing that and then I can
51:19 - select all and see all of those values
51:21 - right here so that's really how you can
51:24 - do the second part of what we were doing
51:26 - within the console itself of the SQL
51:28 - editor you would actually have to use a
51:30 - postgres client of some kind now we can
51:33 - do a lot of these same sort of things
51:35 - within the actual JavaScript itself we
51:37 - could call SQL queries inside of the
51:40 - JavaScript itself as we'll see here very
51:42 - shortly but the idea overall is that we
51:45 - have the ability to automate so much of
51:47 - this being able to create branches on a
51:49 - whim is really great because then when
51:52 - we are actually committing our code we
51:54 - could create those branches get the
51:55 - connection strings and deploy it
51:57 - wherever we need to with that stage and
52:00 - all that so it just really lends really
52:02 - well for Automation and staging purposes
52:05 - for many many different stages which is
52:07 - why I wanted to show you this now again
52:09 - this is foundational for what we'll do
52:10 - later but overall I recommend playing
52:13 - around with this and being comfortable
52:14 - with just deleting a assets whenever you
52:16 - feel like it so in the case of our
52:18 - project neon projects right so we can do
52:21 - neon projects list rather and we can go
52:24 - ahead and delete this project and then
52:25 - start to get right so neon project
52:27 - delete and you get that ID now in this
52:30 - case I only have the ability to create
52:33 - one project let's go ahead and just try
52:34 - create hit enter notice it creates one
52:37 - for me I try to create another one I got
52:39 - my limit exceeded right so I can really
52:42 - only create one project but within that
52:44 - project you can create multiple
52:45 - databases um as you could you know
52:48 - experiment with on your own in there as
52:50 - well so if you wanted a multiple
52:52 - databases what you would with multiple
52:54 - brand brch is what you would do is you
52:56 - would have multiple branches each branch
52:59 - can have its own database itself so the
53:01 - project had can have multiple branches
53:04 - each branch can have multiple databases
53:05 - or they can just have all the same sort
53:07 - of things uh so that's pretty cool as
53:09 - well so yeah if you have any questions
53:11 - on the neon command line tool definitely
53:13 - consult this reference here there's a
53:15 - lot that it's just a matter of playing
53:17 - around with it uh to get the most out of
53:19 - it and then of course downloading and
53:22 - installing some sort of post client so
53:24 - you can really get your hand diry with a
53:26 - SQL if that's what you're wanting to do
53:28 - and if you're not wanting to do that if
53:29 - you're wanting to get using JavaScript
53:32 - that is what we need to do now it's time
53:34 - to actually integrate our neon uh
53:36 - connection string that database URL and
53:39 - also the neon seress package into our
53:41 - nodejs application so we can do a lot of
53:43 - these things with now heavy JavaScript
53:46 - so let's take a look at how to do that
53:47 - now we're going to go ahead and
53:48 - integrate our node.js application with
53:50 - our neon postgres database the key part
53:53 - of all of this is the fact that we are
53:55 - using serverless so the way we approach
53:58 - it has to be from a serverless
53:59 - perspective and that's how we have to
54:01 - configure things which means that we're
54:03 - going to be using the very specific
54:05 - package of neon database serverless
54:08 - there are a lot of postes packages out
54:10 - there to integrate with node.js
54:12 - specifically no- postes could be one
54:15 - that you would end up using but the
54:16 - important part for us as we've said
54:18 - before is that we are using seress and
54:20 - we need to use the neon version of that
54:22 - seress so the first thing that I want to
54:24 - do is I want grab that database URL
54:26 - wherever that might be so in my case you
54:29 - can go into the neon console grab the
54:31 - database URL that's right here or we can
54:33 - use the neon CTL connection string as
54:37 - well right so you could always create a
54:39 - new project if you don't already have
54:40 - one I'm going to go ahead and grab this
54:42 - connection string right here and bring
54:44 - it into my EnV Dev file not justv but
54:48 - rather just dodev so with this in mind
54:51 - I'll go back into index.js and now I
54:53 - need to start configuring fing out my
54:56 - database client so I'm going to go and
54:58 - create a function called DB client and
55:00 - what we really want to do is we want to
55:02 - return for the moment just the database
55:05 - URL itself from the process so I'll go a
55:08 - and return that and so now what we want
55:10 - is our DB client is basically equal to
55:14 - that so const um let's go ahead and call
55:17 - this our DB is equal to that database
55:20 - client that's roughly speaking where
55:21 - we're going with this so to actually
55:23 - make this work we need to start
55:25 - integrating the neon database serverless
55:28 - so this package itself and so at this
55:31 - point I already have it installed and if
55:32 - you don't just go ahead and run that npm
55:34 - install if you once again we already
55:37 - have that database URL we just set that
55:39 - up next we need to use it so this is how
55:42 - we're going to use it don't worry about
55:43 - the actual SQL command that's right here
55:45 - we're not going to look at that one
55:46 - instead we're going to go ahead and use
55:48 - these two items right here so let's
55:50 - paste this into
55:52 - index.js and before we go any further
55:54 - I'm I'm going to go ahead and use this
55:56 - down in the DB client instead so that's
55:59 - really what I want to return is that SQL
56:01 - command from neon and that's the point
56:04 - of having that so this DB client then I
56:06 - can run different commands directly from
56:08 - it itself so we could do something like
56:11 - con results equals to DB with these back
56:14 - ticks it's really cool that you can use
56:16 - these back ticks and you can do select
56:18 - and now just like we've seen before this
56:21 - should give us some sort of result that
56:23 - we can then pass in to our API request
56:28 - okay so one thing that's important to
56:31 - note is this right here let's go ahead
56:34 - and actually try and run this if you
56:36 - recall we have mpm
56:38 - runev inside of our package.json to go
56:41 - with our serverless offline specifically
56:44 - with the stage of Dev which is why I put
56:46 - it into EnV dodev so let's go a and run
56:48 - that again and so what we'll get when we
56:51 - actually go to the endpoint of Local
56:54 - Host 300 000 is we cannot use an import
56:57 - statement outside of a module so by
57:00 - default we do not have a standard module
57:04 - we have commonjs so it's just a slight
57:07 - difference which means that we just need
57:08 - to change this from importing to being a
57:11 - constant and using
57:16 - require and if you wanted to convert it
57:18 - to a model module you could but that
57:21 - takes extra configuration that we're
57:22 - just not going to cover at this time
57:24 - instead we're going to go ahead and use
57:26 - the commonjs which is what is
57:28 - traditionally been used by expressjs in
57:30 - a lot of these node.js applications but
57:33 - now that we've got that we should be
57:34 - able to use this just fine so I'll go
57:36 - ahead and refresh in here and now I get
57:39 - something it's not necessarily exactly
57:41 - what I wanted but it is something now
57:44 - why is it not actually giving me what I
57:45 - want to see this is because we need this
57:48 - to be all asynchronous so the database
57:50 - client itself we want to change this to
57:52 - being async which which means that down
57:55 - here we need to change our call back
57:57 - Handler for the route itself that also
58:00 - needs to be in a sync right so if you
58:03 - think of how any given route Works
58:05 - inside of expressjs it is also a
58:08 - function so we can say like our index
58:11 - route and by default it comes in with
58:13 - the request the response next and this
58:17 - function itself could then handle all of
58:20 - the things from that function it could
58:22 - go like this and
58:25 - like that right so we can absolutely
58:27 - separate these things out like this
58:28 - that's just not a common pattern to use
58:31 - with the nodejs application itself you
58:34 - usually just put the call back right in
58:35 - there and so putting a sync in front of
58:37 - it is the same thing as putting async in
58:39 - front of a normal function itself now if
58:42 - you didn't know that that's what's going
58:43 - on because now that it's asynchronous we
58:46 - can now await our database client and
58:49 - then within our database client to
58:51 - actually use it we can await it as well
58:53 - and to really follow follow a little bit
58:55 - closer to the convention you could also
58:57 - just call this SQL as
59:00 - well and so that kind of fits a little
59:02 - bit more with what you'll see throughout
59:04 - the documentation on the SQL client
59:06 - itself directly from the neon database
59:09 - server list which is what we do see in
59:11 - the docs itself that's this right here
59:13 - so as you can see it says await and
59:15 - that's how we're actually able to do
59:17 - that is by making everything a
59:18 - synchronous and so now we should
59:20 - actually see some sort of result that
59:21 - goes into the actual database itself so
59:24 - back into our Local Host we rerun this
59:27 - and we should see something that is
59:29 - coming from our database now the first
59:31 - time we run it it might take a moment
59:32 - because our actual database on the neon
59:34 - console has to be active for it to work
59:37 - and so that first little bit of latency
59:40 - does make a difference now I'm also
59:42 - using a database that's in Us East it's
59:45 - quite literally on the other side of the
59:47 - country than me so how fast it's gone is
59:50 - really really great so it should in the
59:53 - speed should improve quite a bit when I
59:55 - actually deploy this application into
59:57 - AWS Lambda and so we've got here as
60:00 - results that actually is an array here
60:02 - with some data and that that's just what
60:04 - happens from this particular command so
60:07 - we can actually unpack the results like
60:09 - this if I refresh that that should give
60:11 - me a dictionary value I might need to
60:15 - refresh the server itself um or what we
60:18 - could do in here let's go ahead and run
60:21 - that again refresh in here now now it's
60:24 - a dictionary value now I can actually
60:27 - just do results.
60:29 - now and we might have to refresh the
60:32 - server once again so let's go ahead and
60:34 - try that
60:36 - out and I refresh and there are my
60:39 - results this is now a timestamp that's
60:41 - coming directly from the database like
60:43 - we saw when we were in the SQL editor
60:45 - itself and we ran you know select now
60:49 - just like that it's the exact same
60:51 - command going to the exact same database
60:54 - and return R turning roughly the same
60:55 - value of course it's changing just for
60:58 - however long I need to actually look at
60:59 - this um so yeah now we are fully
61:02 - integrated with our neon database inside
61:05 - of our node.js application so that means
61:08 - we can start doing a lot of more
61:09 - interesting things before we do that I
61:11 - want to point out this right here is raw
61:14 - SQL so this command what you can do with
61:16 - a weight SQL now is you could write out
61:19 - your own raw SQL so if you wanted to
61:22 - experiment with how you know SQL itself
61:26 - you could create an endpoint that would
61:28 - in theory accept data to run various SQL
61:31 - commands in other words you can create
61:33 - an endpoint that handles a form very
61:36 - similar to what we see inside of the
61:38 - console itself that's not something I
61:39 - want to do I want to make sure that the
61:41 - commands I am running are exactly to the
61:44 - structure that I want them to be I don't
61:47 - want to have pure SQL commands running
61:49 - through in my application at least
61:52 - that's not how I want to design this one
61:54 - right so now that we've got this all
61:56 - integrated there is one more thing that
61:58 - I want to add in to the configuration
62:00 - for neon and this just has to do with
62:03 - the connection itself when we go into
62:06 - our deployed serverless environment I
62:09 - want to just change the configuration to
62:12 - use an experimental feature called Fetch
62:15 - connections cache and we're going to set
62:17 - that being equal to true this is for
62:21 - HTTP connections and nonp pooling right
62:24 - that's really what's going on here feel
62:26 - free to look at the documentation
62:27 - themselves but we're not going to have
62:29 - pooling in a serverless environment
62:31 - because that server is going to be
62:32 - scaled down so it's it's a little bit
62:34 - more difficult to pull connections when
62:36 - they aren't persistent it's going to
62:38 - cause a lot of havoc and issues so
62:40 - that's why we're going to be using HTTP
62:41 - connections and that's also why I put
62:43 - that configuration just to make sure
62:45 - that that's in there well before we
62:47 - actually go into production itself
62:50 - pretty great now we're going to do our
62:51 - first deployment of our node.js
62:53 - application of course using the
62:55 - serverless framework to make sure that
62:56 - it runs into AWS Lambda now to make this
63:00 - work I'm going to go ahead and use this
63:01 - command seress but instead of saying
63:03 - serverless offline I want to do
63:05 - something different so we'll give it a
63:07 - new script name of deploy and then we'll
63:09 - go ahead and use serverless deploy and
63:13 - then the stage we want to deploy and of
63:16 - course that stage is going to work very
63:18 - similar to the offline where it has
63:20 - maybe in its own environment variable so
63:22 - let's actually change the stage to
63:24 - Simply prod and so now to run this I'm
63:27 - going to go ahead and run npm run deploy
63:30 - and of course it's going to run this
63:31 - command in here one of the big benefits
63:33 - of writing the script in here like this
63:35 - is that you don't have to remember what
63:38 - different flags you're going to be
63:39 - adding for different deployment options
63:42 - in this case the stage of prod will
63:44 - always be the stage whenever I run npm
63:46 - run deploy and so what we've got here is
63:49 - we've got loading environment variables
63:51 - from EnV so it's going to look into the
63:55 - EnV here and then it would look into
63:57 - dodev if that was the stage so if we
64:00 - wanted to add in ourr stage we could do
64:03 - that just like that and so now let's go
64:06 - ahead and add in just another key in
64:08 - here so hello world equal to this
64:12 - something like that we go ahead and run
64:14 - the deplo deploy again and we can see
64:16 - the different environment variables that
64:18 - are coming through in here and so of
64:20 - course it fails to deploy it for a
64:22 - number of reasons one of them the
64:24 - security token is invalid so at this
64:27 - point I have credentials on my local
64:29 - machine but they're not the valid ones
64:32 - and so I also have this Us East one in
64:34 - here so two kind of major issues that we
64:37 - want to address we want to verify that
64:39 - this is the correct region and in
64:41 - package.json I can actually bring in
64:44 - that flag of region and in this case
64:46 - I'll leave it in as us East one now the
64:49 - question of course is going to be how do
64:50 - we know which region we want to use well
64:53 - it has everything to do with our
64:54 - database we want to make sure that the
64:56 - region is as close as it as it can
64:58 - possibly be and so to do that we can use
65:00 - neon CTL and we can go ahead and do
65:03 - projects list and we can see the project
65:06 - that we're using and the the region ID
65:09 - which is Us East 2 we can ignore AWS
65:13 - Dash because Us East 2 is the AWS region
65:17 - itself so we can update this region to
65:19 - being that region itself and then we can
65:21 - run deploy again and this time it's
65:23 - going to attempt to do it in the correct
65:25 - region which is exactly what we were
65:27 - hoping for so the security credentials
65:29 - we need to address but before we do that
65:31 - I want to bring your attention to
65:33 - something important and interesting so
65:36 - yes it is deploying using
65:38 - this.r right so it's going to use all of
65:40 - these environment variables and it's
65:42 - going to build something and go and do
65:44 - what exactly well what it's happening is
65:46 - it's building this dot serverless folder
65:49 - inside of our local repo so it builds
65:52 - this folder out and then creates this
65:54 - zip file here so what we could do is we
65:56 - could actually unzip this ZIP file and
65:59 - we could take a look at what happens so
66:01 - inside of my project here I can actually
66:02 - see what do you know it's actually
66:04 - everything related to my repo my main
66:07 - code in itself is zipped up and brought
66:10 - into here it's actually Mi missing the
66:13 - EnV files so it doesn't actually have
66:15 - those things in there which is pretty
66:16 - nice and fairly important right and it
66:19 - also doesn't have the doget ignore so it
66:21 - is missing some things but it does have
66:22 - all of the node modules in there so it's
66:25 - kind of like a point in time instance of
66:27 - the application itself but it's also
66:29 - showing me something in this state file
66:32 - and that is all of my environment
66:34 - variables now on one hand this is great
66:36 - it actually uses
66:38 - my.pro and it's just going to go ahead
66:39 - and do that and I can just deploy this
66:41 - application and in theory this will just
66:43 - run which is nice but the thing is this
66:47 - state file is well it has our database
66:50 - URL directly in the state file so this
66:53 - is a security con concern for sure
66:55 - because we will show you where this is
66:56 - going to end up in just a moment so to
66:58 - make it end up where it needs to we need
67:00 - to actually update our credentials so if
67:03 - you've used the AWS CLI before you'll
67:05 - know that most of the time you'll log in
67:08 - there and it'll create something inside
67:10 - of a. AWS credentials in your
67:13 - application and you'll see something
67:15 - like this now these credentials in my
67:17 - case are no longer valid in general but
67:19 - they're also not valid to actually do
67:21 - the deployment they're not valid to
67:22 - actually use serverless serverless
67:24 - itself needs its own policies and its
67:28 - own credentials to be able to deploy
67:29 - everything well maybe it doesn't need
67:31 - its own credentials but ideally with
67:33 - each project you attach different
67:35 - policies and then create credentials for
67:38 - them now if you're not familiar with
67:39 - this process I'm definitely going to
67:41 - show you but before I show you I want to
67:43 - just show you this reference that we
67:44 - have here for this I am policy so this
67:47 - is a policy we're going to be using for
67:49 - the serverless framework it's pretty
67:50 - extensive one of the things that's
67:52 - important is when you you actually use
67:54 - this policy you want to change the AWS
67:57 - ID to the correct one for your account
68:00 - which is something we will do in just a
68:02 - moment but the idea is once you actually
68:04 - have those the permissions in place then
68:07 - in your EnV for example you'll have
68:09 - something called AWS uh access key ID
68:14 - and then AWS
68:17 - secret access key cool so we need to set
68:21 - these up let's go ahead and create these
68:23 - now to do this I'm going to go into IIM
68:26 - and I'm going to create a user group
68:28 - this group I'm going to just call it
68:31 - serverless
68:32 - framework okay just naming it calist
68:35 - framework and then we'll go ahead and
68:37 - create this group Next I'm going to go
68:39 - and create a user and I'll go ahead and
68:41 - create this user as
68:43 - serverless framework user realistically
68:47 - I would actually probably want to give
68:48 - it the name of my project so serverless
68:51 - nodejs API just like that
68:54 - we do not need to give them access to
68:55 - the Management console but what we do
68:58 - need to give them access to is the
69:00 - permissions that will'll add to this
69:01 - group so I'm going to go ahead and add
69:03 - into the serverless framework I'm going
69:04 - to add this user into that group Next
69:07 - what I want to do is we can review and
69:10 - create this user so let's go ahead and
69:11 - create them so all I did there was
69:13 - create a new user and add it to the
69:15 - group that I just created at this point
69:17 - they have no permissions and they also
69:19 - don't have any security credentials so
69:22 - going back into our user we go into
69:24 - security credentials and we'll go ahead
69:26 - and create a brand new access key go
69:29 - ahead and create this key we'll hit
69:30 - other and then I'll go ahead and say
69:33 - next and this is going to be our uh we
69:36 - don't actually need a description tag it
69:37 - unless you really want to here are those
69:40 - access keys I'm going to go ahead and
69:41 - copy this one uh the first one is the
69:43 - access key ID the next one is the secret
69:46 - and we'll paste that in there the names
69:48 - of these have to be exactly this the
69:51 - actual keys are going to be different of
69:52 - course but the actual AWS access key ID
69:55 - it has to be in this format it has to be
69:57 - written just like this now you could use
69:59 - it inside of credentials as well but the
70:02 - reason I'm using it into. EnV is so it
70:04 - doesn't conflict with my system at large
70:07 - and so I want to once again run my mpm
70:11 - run deploy and hit enter so this time
70:14 - what it's going to do is it's going to
70:15 - attempt to use these environment
70:17 - variables credentials but one of the
70:19 - problems is it's actually loading these
70:20 - into my environment as well this is
70:23 - definitely going to be causing an issue
70:25 - very very soon but now our new error is
70:27 - a little bit different it says that it's
70:29 - not authorized to perform this data so
70:31 - the user I just created is right here so
70:35 - that is the user we want to give
70:37 - permission to be able to perform all of
70:39 - the things that serverless wants to do
70:41 - and the reason this is working is
70:43 - because it's in the EnV itself this
70:45 - right here so now we need to give those
70:48 - permissions we actually need to create
70:49 - that policy so again in the actual
70:52 - GitHub repo itself you can go through
70:54 - and see this exact same policy I'm going
70:56 - to go ahead and copy it the repo is
70:58 - right here um and it will be linked in
71:01 - the description as well and so in my
71:02 - case I'll go ahead and create this
71:04 - policy reference and it's policy. Json
71:07 - I'm going to paste it in here I'm
71:09 - actually going to add this to get ignore
71:11 - because I do not need to commit this so
71:13 - I'll add it to get ignore and what we
71:16 - want to do is we want to change this AWS
71:18 - ID here so I'll do a command F or
71:21 - control F and we want to do a finding
71:23 - replace and I'll do a colon in front and
71:27 - back of it so we want to change that all
71:30 - across the board and the way we're going
71:31 - to do that is by finding the actual AWS
71:34 - ID for our account inside of I am so if
71:38 - I go ahead and close this out for the
71:39 - user go into the IM am dashboard we can
71:43 - see the account ID right here I'll go
71:45 - ahead and copy that and then we'll go
71:47 - ahead and replace it inside of this
71:49 - policy and we'll just do replace all
71:51 - just to make sure that our account ID
71:53 - ideas in here now you could in theory
71:56 - use just a star here but I like
71:58 - narrowing this down to specifically the
72:00 - account in the case of this particular
72:02 - policy so that if I were to reuse it
72:05 - again in the future I actually review it
72:07 - before I use it um and that's the idea
72:10 - okay so I will say that the permissions
72:12 - themselves if you were wanting to remove
72:15 - permissions you totally could but that
72:17 - might cause problems when it comes to
72:19 - the serverless framework itself that
72:21 - you'd have to look into even more but
72:23 - now that we've got this policy let's go
72:25 - ahead and create it by going back into
72:27 - the I am into policies here create
72:30 - policy and I'm going to go ahead and
72:32 - click on the Json and we'll go ahead and
72:35 - paste this in and so now I've got that
72:37 - policy in there with I think everything
72:39 - that we'll need I did test this a number
72:41 - of times so let's go ahead and create
72:43 - this policy now and I'll just call this
72:45 - the serverless framework uh permissions
72:49 - policy and then notice that it has very
72:52 - limited access I'll goad and create this
72:54 - policy and we want to attach this to
72:56 - either our user or our group I'll go
72:58 - ahead and attach it to the group click
73:00 - on the group itself click on permissions
73:02 - click on ADD permissions attach policies
73:06 - and do a quick search for the policy we
73:08 - made so the serverless framework policy
73:11 - I'll go ahead and attach that now the
73:13 - important thing about using permissions
73:15 - and policies within aw is you can attach
73:19 - too much permission right so the admin
73:22 - access gives quite literally all access
73:24 - to all AWS Services which could get you
73:26 - into trouble you could accidentally
73:29 - provision way too many things and cause
73:31 - a lot of Havoc within your account and
73:34 - charges as well and of course that also
73:36 - means that you maybe accidentally
73:38 - exposed your access keys and all that so
73:41 - the policy we're using is really limited
73:43 - to what the serverless framework needs
73:45 - to do it doesn't have unlimited policies
73:47 - just a few which is why we created our
73:49 - own and it's why it's working in this
73:51 - way so at this point we now have all the
73:55 - policies in place to run what we need to
73:57 - run within our serverless framework but
73:59 - before we do I will say the policy
74:01 - itself if for some reason the policy is
74:03 - not working we have inside of the gist
74:06 - itself for the policy reference we do
74:10 - have a gist available for you uh that
74:12 - will have the latest up-to-date
74:14 - information related to this policy and
74:17 - the servess framework so be sure to
74:18 - check that out that's going to be linked
74:20 - in a lot of different places uh but the
74:22 - idea being that the we use needs to
74:24 - actually work for everything we're
74:26 - trying to do here so with that out of
74:28 - the way let's go ahead and actually run
74:31 - our deployment here right so we go ahead
74:33 - and do mpm run deploy hit enter and so
74:37 - this will attempt to deploy this now
74:39 - with all those policies in place and so
74:42 - since it's going to deploy or attempt to
74:44 - I'm also going to go ahead and create
74:45 - another script in here and we're going
74:47 - to call this remove so deploying and
74:50 - removing are basically two sides of the
74:52 - same coin it's really just that simple
74:54 - to deploy it and then turn around and
74:56 - remove it I'm going to let this finish
74:58 - it might take a minute or a little bit
74:59 - more depending on our speed and
75:01 - everything that's going on in the AWS
75:04 - configuration uh but I'll go ahead and
75:05 - let this finish and then we'll come back
75:07 - and actually test out that deployment
75:09 - it's not quite done yet but it did say
75:11 - just a moment ago that it was uploading
75:13 - to S3 so since that just finished I want
75:16 - to actually look at S3 to see what that
75:19 - means exactly so if I do a quick search
75:21 - in here for S3 inside AWS we go click on
75:24 - S3 and then in here the buckets
75:27 - themselves I want to search for
75:28 - serverless and what we see here is our
75:30 - serverless application right there so
75:33 - this is going to be autogenerated for us
75:35 - that's what the serverless framework
75:36 - does it will autogenerate these buckets
75:38 - for us yes we could designate our own
75:40 - buckets but that configuration is a
75:43 - number of steps that we're just not
75:44 - going to cover so the idea here is we
75:46 - can click on this bucket we can click on
75:48 - serverless we can look at our project
75:50 - the stage that we're deploying the
75:52 - actual commit of the stage and then all
75:55 - of that same sort of data the Json State
75:57 - and all this so if I click on this state
76:00 - once again if I go ahead and open this I
76:02 - will see that there's my database URL
76:05 - and I also have my AWS access keys in
76:07 - here that's not great either so all of
76:09 - these things we we definitely want to
76:11 - move towards getting rid of and so we
76:13 - actually might have an error in our
76:14 - deployment because of our AWS access
76:16 - keys and sure enough by the time I said
76:19 - that we do so we definitely do not want
76:21 - to have these access keys inside of our
76:23 - deployment now I wanted to show you this
76:25 - error because it's related to ourv so
76:29 - we're kind of at this weird spot it's
76:30 - like well I need to use these keys but I
76:33 - also don't want to deploy them so how do
76:34 - I actually handle that well luckily for
76:36 - us serverless diagel has the ability to
76:39 - do this so what we can do is we can
76:41 - actually customize what environment
76:43 - variables will be submitted to our
76:46 - backend so if we do custom and
76:50 - then and then exclude
76:53 - we can actually put a list of enir
76:55 - environment variables that we don't want
76:57 - so like the AWS access key here and the
77:01 - secret key here we can do all of that
77:04 - and it's really just that simple every
77:05 - once in a while we might have to use
77:07 - something like this AWS session token
77:10 - and you know we also will get to a point
77:11 - where we want to remove the database URL
77:14 - we're not quite there yet but we will
77:15 - have that for now so with this in mind I
77:19 - have some changes that I made I'm going
77:20 - to go ahead and run this deploy again uh
77:23 - notice that these are still coming up so
77:24 - it looks like maybe I did something
77:26 - wrong and this should say exclude so I
77:28 - canel that out and we'll write it again
77:31 - now those environment variables are gone
77:32 - the ones that we just simply don't want
77:34 - anymore great so this is exactly what we
77:37 - want so once again it's uploading and we
77:40 - can see once it's done that progress so
77:43 - we go into prod here it's going to show
77:45 - us a new folder based off of that and
77:47 - it's also based off of the time itself
77:49 - so this one looks to be a little bit
77:50 - later so I'd imagine this is the more
77:52 - recent recent one and sure enough it
77:55 - it's already uploaded I can verify the
77:57 - state once again open this up and once
78:00 - again the things that I'm okay with
78:02 - exposing for now are exposed um and the
78:05 - other ones are not and so once this
78:08 - actually finishes we will actually test
78:09 - out this Lambda function um and we can
78:12 - also review the function itself inside
78:14 - of Lambda which is something I'll leave
78:15 - for you to just explore on your own uh
78:18 - but for now I'll I'll wait for this to
78:20 - finish and there it is it's fully
78:22 - deployed so if I go to this URL here
78:25 - let's open it up in the web browser just
78:26 - to make it a little bit easier to see it
78:29 - might take a moment to spin up but once
78:31 - it does it's going to be really really
78:33 - fast as we see here so there it is and
78:36 - of course we can also curl this out and
78:38 - get the same results from it it has the
78:41 - timestamp that's coming directly from
78:44 - the database itself which is what this
78:46 - is right and it also is using the
78:48 - environment variable for that database
78:50 - URL which of course is not as secure as
78:52 - we want so what we need to do is move
78:54 - this towards being a lot more secure
78:56 - than it currently is and that's
78:58 - definitely something we will do um in
79:00 - the next part but overall what we see
79:02 - here is we removed ourv variables we can
79:07 - deploy really simply using basically the
79:09 - same things we've been doing with the
79:11 - offline then our policy this is the part
79:14 - that's going to be the trickiest to get
79:15 - configured and set up and working
79:17 - correctly if at any point you need to
79:19 - change things you need to test what
79:21 - policy problems are happen happening you
79:23 - can always go into I am and go into the
79:27 - policy
79:28 - directly and let's go ahead and search
79:30 - for it Ser a list click on this since we
79:35 - have Json in here now if I go to edit
79:37 - this I can navigate to any block section
79:40 - so let's say for instance our S3 isn't
79:42 - working correctly like this put but
79:44 - bucket tagging let's say for instance
79:46 - that wasn't on there and it showed you
79:48 - there was some issue with that what you
79:50 - could do is go into that one single
79:52 - block notice that over here it says edit
79:54 - statement now I can actually look for
79:57 - various statements so like put um tag or
80:00 - let's go ahead and just type out tag and
80:02 - we can scroll down here and we can see
80:05 - put job tagging put bucket tagging put
80:07 - object tagging we can just add these
80:09 - things in really simply like that into
80:11 - this one statement and then it would
80:14 - actually adjust as as it needs to so
80:16 - this is a good way that you could really
80:18 - experiment with what's absolutely
80:20 - necessary when you go through and start
80:22 - building things out so for example let's
80:25 - just try this out I'm going to go ahead
80:26 - and get rid of this describe right here
80:28 - so I want to just refresh real quick um
80:31 - and I'm going to go ahead and edit out
80:32 - just one single permission so we can see
80:35 - what that error might look like right so
80:37 - we're going to come in here it looks
80:38 - like I'm still editing I'm going to go
80:40 - ahead and get rid of this describe star
80:42 - I'll go a and cut that out for the cloud
80:44 - formation stack itself we'll hit next
80:47 - and we'll go ahead and hit save changes
80:49 - I now removed a pretty major permission
80:52 - so going back in into my local project
80:53 - here I'll now run mpm run remove which
80:57 - will of course attempt to remove this
81:00 - and right off the bat I get not
81:02 - authorized to perform this describe
81:04 - stack resource so I can't even do
81:06 - anything here as far as deleting it so
81:08 - that alone will stop it in its tracks
81:11 - for any given user and so I can actually
81:13 - go ahead and come back and change that
81:16 - once again by going back into cloud
81:18 - formation and bringing that back in and
81:21 - that one single permission allows me to
81:23 - do describe which we could also verify
81:25 - by going into the statement itself we
81:27 - can go to cloud formation and describe
81:30 - as soon as you put describe there it
81:32 - will describe all of these things so
81:34 - describe star does all of these actions
81:37 - right here which is why they're
81:38 - highlighted and selected because of this
81:40 - if I remove that and come back and type
81:43 - out describe I can see that none of them
81:45 - are selected so it's actually kind of
81:47 - nice that I can I can go about doing
81:49 - that whenever I need to okay so we
81:52 - definitely want to bring that back in I
81:53 - don't need to do it manually I'll just
81:55 - write in that one single change save
81:58 - everything and now we'll go ahead and
82:00 - attempt to remove this and it might take
82:03 - a moment for it to be fully propagated
82:05 - as far as the description but right a
82:07 - bit right away it it happens really
82:09 - really quick basically and so getting in
82:11 - the habit of deploying and removing is
82:14 - perfectly okay because our code itself
82:16 - is being tracked by git so we don't have
82:18 - to really worry about our code our
82:20 - actual database itself is not being
82:23 - managed by serus at all the only thing
82:25 - that's being managed by serus in
82:27 - relation to the database at this time is
82:29 - the database URL but very soon that is
82:32 - going to be abstracted away to something
82:34 - a lot more secure but overall the idea
82:36 - here is being able to deploy and remove
82:39 - really really quickly at a moment's
82:41 - notice and having the correct
82:42 - permissions to be able to do that for
82:44 - your AWS user and so you could also
82:47 - practice right now just take a pause
82:49 - practice right now and actually remove
82:52 - access to this particular user and see
82:54 - if you can recreate access because it's
82:56 - fairly straightforward um but if you
82:58 - don't know how to do it then this is a
83:00 - good time to practice that before we
83:01 - move into securing that database URL a
83:04 - little bit more so let's take a look at
83:06 - that now if you're ready now we're going
83:08 - to go ahead and configure our database
83:10 - URL to be more secure by using the AWS
83:14 - systems manager so if you do a quick
83:16 - search for systems manager inside of AWS
83:20 - we're going to go ahead and configure
83:21 - this to work with our application and
83:23 - more specifically through the parameter
83:25 - store of the AWS systems manager now
83:28 - this is not the only place that you can
83:29 - store Secrets or variables that you want
83:32 - to encrypt but it is one of the best
83:34 - ways to do it so we're going to go ahead
83:35 - and create this parameter here and we're
83:36 - going to start off with Slash I'm going
83:38 - to say project name slash stage and then
83:41 - SLV name something like that that's kind
83:44 - of the format how I think about creating
83:46 - these parameters themselves so what I'm
83:48 - going to do is this is going to be
83:49 - called the serverless and nodejs API the
83:53 - stage itself we'll just use prod and
83:55 - then I'll go ahead and do database URL
83:58 - that's it so this path right here is
84:00 - something we will use in the future so
84:02 - let's go ahead and grab it into
84:04 - index.js I'm going to go ahead and do
84:06 - const and this is going to be my um
84:09 - database URL SSM
84:14 - pram and we'll pass that in okay so
84:17 - that's just the name that we'll have
84:18 - next up we need the database URL itself
84:20 - so I'll go ahead and use the Neon
84:22 - command line tool for the connection
84:23 - string and we'll go ahead and place that
84:26 - in here I'll copy that and we will then
84:29 - place that inside of our parameter
84:32 - itself so we've got our name we can use
84:34 - this standard tier we're going to use a
84:36 - secure string so it encrypts this data
84:39 - and then we're going to go ahead and use
84:40 - our current account we don't need to
84:41 - change any of these things you can use
84:44 - all of the defaults the next thing is
84:46 - just going to be the value that we want
84:47 - to encrypt and then I'm going to go
84:48 - ahead and create this parameter so it's
84:51 - encrypted at rest so if I go go and look
84:52 - at it it's going to be stored in here
84:54 - crypted until I decrypt it so this is
84:56 - decrypting it right there I'm just
84:58 - toggling that value and then of course
85:00 - if I go to edit it I can have it
85:02 - decrypted as well okay so it is stored
85:05 - encrypted and so what that will allow us
85:07 - to do is well a couple things number one
85:10 - we could abstract this to an environment
85:12 - variable now I'm going to leave it in
85:14 - just as a standard variable but since
85:16 - you now know how to create environment
85:18 - variables that would be a good place to
85:20 - potentially put it in your environment
85:22 - is actually using that database URL SSM
85:25 - peram and it's called SSM that's just
85:27 - the default name for it for the AWS
85:30 - systems manager that's the name of it as
85:32 - we'll see when we actually start using
85:34 - the package itself inside of our
85:36 - application so the main thing here is
85:38 - then I want to actually decrypt this
85:40 - data I want to be able to look it up and
85:41 - grab it so before I can look it up and
85:43 - grab it I want to grab the database URL
85:46 - out of the environment variables and so
85:48 - inside of myv I can leave it there but
85:51 - inside of the Ser list yaml I'll go
85:53 - ahead and just add it into exclude just
85:56 - so it never goes in and so at this point
85:58 - I wouldn't have access to my database
86:01 - client at all and that's what we want to
86:03 - change so to change it we're going to be
86:05 - using the AWS um client for JavaScript
86:09 - which is the AWS SDK and so we want to
86:12 - install this I'll go ahead and copy the
86:14 - install call here and then we'll just go
86:16 - ahead and install it onto our project
86:19 - it's really important that we do have it
86:20 - in our project as a dependency because
86:22 - it is an actual dependency for our
86:25 - runtime so the way it works is inside of
86:27 - index.js we can bring this in and I'll
86:30 - do const AWS equals to require and this
86:33 - is going to be the AWS SDK no surprise
86:37 - there then we initialize a SSM
86:40 - connection so there's going to be SSM
86:42 - equals to AWS SSM and then the region
86:46 - itself so region we will pass in here as
86:49 - well and once again I'll go ahead and
86:51 - add another con in here as AWS region
86:54 - and we'll go ahead and use Us East to
86:58 - once again you can have this as an
86:59 - environment variable that you might want
87:00 - to change when you need to the region
87:04 - and the parameter work in tandem
87:06 - together okay so now this is going to
87:09 - get all of the parameters or at least
87:12 - allow for me to get all of the available
87:14 - parameters inside of that particular
87:17 - region right so the parameter store in
87:19 - that region it'll show all of them that
87:20 - are in here this is a going to allow me
87:23 - to do that so the next thing is our
87:25 - actual database URL so now I'm going to
87:28 - go ahead and say const DB URL equals to
87:31 - well what exactly well to do this I'm
87:34 - going to go ahead and bring in the data
87:37 - itself the data from the pram store so
87:40 - let's just call it pram store data and
87:42 - this is going to be await SSM the
87:45 - initialized class right here and we also
87:48 - want this to say new because we are
87:50 - creating a new instance of that class so
87:53 - await SSM doget
87:57 - parameter and we want to pass in some
87:59 - arguments here the first argument is
88:01 - going to be the name this is the name
88:04 - right here the second argument will be
88:06 - with decryption and we want to say true
88:09 - we stored it as a secure string if it
88:12 - just says string we would not need to
88:13 - use with Des decryption it would just
88:16 - work then if I do promise I can turn
88:18 - this into an asynchronous call which is
88:21 - exactly what I want after I've got that
88:24 - I can actually use this neon Connection
88:25 - in here with the parameter store data
88:28 - Dot and this is going to be do
88:30 - parameter.
88:32 - value just like that and so this should
88:35 - actually now allow for us to make this
88:37 - work um and so obviously we need to make
88:40 - sure that this is actually working at
88:42 - least locally before we even push it
88:44 - into production because there is
88:46 - something else we'll have to change with
88:47 - our production environment so let's go
88:49 - ahead and do mpm
88:51 - runev and once again we see that debug
88:54 - is in here now that database URL is not
88:56 - in here and I'm not even using it anyway
88:58 - so there's nowhere in my application
89:00 - that has that database URL for the
89:02 - client itself so what I want to do then
89:04 - is open up the local host and let's just
89:06 - take a look let's make sure that I can
89:08 - still connect and still get that data
89:10 - and sure enough I am it's still giving
89:12 - me the exact data that we were expecting
89:15 - from the database itself in other words
89:17 - this parameter store data is actually
89:19 - working so we can console log this out
89:22 - and see that parameter store data as
89:24 - well uh just like
89:26 - this and let's go ahead and just refresh
89:29 - our
89:30 - application and reload the page and we
89:33 - refresh in there there it comes through
89:35 - and there is that data coming directly
89:37 - from the parameter store great so this
89:41 - same package could be used to set the
89:42 - parameter as well that that might be
89:44 - something we do later uh but for now we
89:46 - have the ability to use a production
89:49 - ready parameter for any given stage so
89:52 - naturally we could also come in here and
89:54 - do our constant and say stage and this
89:58 - is going to be equal to
89:59 - process. env. Stage or just simply prod
90:05 - right and so then we would come in here
90:07 - and just change this to being something
90:09 - along those lines and it should actually
90:12 - be right
90:16 - here and that will give us at least the
90:19 - ability to modify the stage once again
90:21 - this could be just its own environment
90:23 - variable itself so now that we've got
90:26 - this what I need to do is I need to
90:28 - prepare this whole thing for production
90:31 - to be able to just use this so one of
90:33 - the things about AWS Services is they
90:36 - can communicate with other services
90:39 - without a secret key or an access key
90:43 - they can just do it together and so in
90:45 - our local environment we have those keys
90:48 - in EnV now the only reason I can
90:51 - actually communicate to SSM with these
90:54 - environment variables has to do with
90:56 - what we did in the last part which was
90:58 - in the Imam so if we go into the I am
91:01 - and into our user groups into our
91:04 - particular policy that we've got on here
91:07 - we can see something important to
91:10 - understand why this user can actually
91:13 - access that data it has to do with this
91:15 - right here SSM and star now generally
91:17 - speaking you probably don't want to give
91:19 - every resource every access to this
91:21 - parameter store I did this as a quick
91:24 - and easy way to have access locally yes
91:27 - it would be something I would want you
91:28 - to change as well but I knew I was going
91:29 - to point it out so here it is I'm
91:31 - pointing it out and this is where we'd
91:32 - want to change it right here now so that
91:35 - any different project just doesn't have
91:37 - access to all of your parameters on your
91:40 - entire application but the thing is when
91:43 - we deploy this to Lambda it doesn't have
91:46 - permission it won't necessarily have
91:48 - default permission in here just like
91:50 - this so we need to actually add in the
91:53 - permission that it needs we need to give
91:55 - it the permission now one of the ways to
91:57 - do that is by inside of ser. yaml what
92:00 - we could do is inside of this provider
92:02 - here we can add in additional permission
92:05 - and the way we do that is by creating
92:07 - Imam and then we're going to create a
92:09 - new role this role will give it a
92:12 - specific name that starts with
92:13 - serverless I'll show you why in a moment
92:15 - as well and I'll give this serverless
92:17 - SSM roll and then the statements we need
92:21 - for this is effect and we want to
92:26 - allow resource and we're going to go
92:29 - ahead and say all and then I'll go ahead
92:31 - and write out all of the things all the
92:33 - actions I want to allow and so I'll just
92:37 - copy and paste them and we'll talk about
92:38 - them right now and where you can
92:40 - diagnose these things SSM get parameter
92:43 - what do you know we just did get
92:44 - parameter get parameters well that would
92:46 - be a multiple parameters by path and so
92:48 - on right so these aren't all of the
92:50 - permissions that we necessarily need
92:51 - need but it is all also opening up the
92:53 - resource everywhere so this role will
92:56 - play the same exact role as this effect
92:59 - right here and so if we wanted to get
93:01 - more specific with that we can go into
93:03 - edit here into our policy or any policy
93:06 - this is how we can start learning about
93:07 - those various permissions like we've
93:09 - talked about before we've got SSM here
93:11 - which is this systems manager if I get
93:14 - rid of this all let's get rid of that
93:16 - all call right there and just search for
93:18 - systems manager oops get rid of that
93:21 - comma
93:22 - and then look for systems manager as far
93:24 - as the service is concerned we can start
93:26 - seeing all of the various commands that
93:28 - we can use in here and this of course is
93:30 - going to correspond to the SDK itself
93:33 - how the SDK can be used so each one of
93:36 - these policies is important so get Pam
93:39 - we see that there what if we do create
93:42 - and we've got a bunch of different
93:43 - create in here right create resource
93:45 - right well we also might want put
93:49 - Pam right there that would be adding in
93:51 - one right uh and then maybe
93:54 - delete and so all of these different
93:56 - parameters that you might want to have
93:58 - for a resource is well this is where
94:00 - you'd find them this is the easiest way
94:02 - to find them in my opinion now of course
94:04 - we could just do all of them here but
94:06 - our Lambda roll certainly does not need
94:08 - all of that data the actual serverless
94:11 - role is mostly just for our user so
94:13 - going back to these serverless
94:14 - permissions I'm going to go ahead and
94:16 - abort all the saves and changes these
94:18 - permissions right here have nothing to
94:20 - do with our Lambda project itself right
94:23 - that's what this role does the only way
94:25 - it has something to do with that Lambda
94:27 - project is so that my local environment
94:29 - or really this particular user right
94:32 - here has access to do what it needs to
94:34 - do so these Keys these access keys and
94:37 - secret keys are not on the AWS Lambda by
94:40 - default which means that this code would
94:42 - not be able to work unless of course I
94:44 - were to add that actual roll on here and
94:47 - the reason we have this serverless Dash
94:49 - roll on here is so that we can can
94:51 - actually manage this so inside of our
94:53 - Json data inside of our policy here if
94:56 - we scroll down a bit and look for our
94:58 - role let's do a quick command and we
95:02 - look down a little bit for I
95:05 - am and we will be able to see that we've
95:08 - got some roles here so create roll put
95:11 - roll and so on notice the name of it
95:14 - right so we've got serverless Dash is
95:16 - how it starts which is exactly why I
95:18 - call this serverless Dash and then the
95:21 - SSM Ro right or I could actually say to
95:24 - be even more safe I could say my SSM
95:27 - rooll or project SSM Ro if if it does
95:31 - conflict with anything else that the
95:33 - serverless framework gives us but the
95:35 - idea is if I went in here and called it
95:37 - my SSM rooll like that I would have to
95:39 - update this policy for this particular
95:41 - user to adjust for that like it wouldn't
95:44 - be able to create it this is only able
95:46 - to create if the actual name of it
95:49 - starts with serverless great so now that
95:52 - we've got this let's go ahead and try
95:54 - and deploy this here so we'll go ahead
95:56 - and do mpm run
95:59 - deploy and what we should see is two
96:02 - environment variables working that's
96:04 - great I don't have any errors so far
96:05 - about the role itself uh but the main
96:08 - thing that we want to verify is the fact
96:11 - that the application once it's all said
96:13 - done is going to go based off of this
96:14 - database parameter that will end up
96:17 - coming through so I'll let this finish
96:18 - and then we'll check that out and so now
96:20 - I get this error of I am tag roll action
96:23 - I I don't have the ability to tag this
96:25 - role so it's not performed the tag roll
96:28 - so this actually gives us the
96:30 - opportunity to see how we can adjust our
96:32 - policy to make that work so back into
96:35 - our seress framework permissions policy
96:37 - we're going to go ahead and edit it I'm
96:39 - going to navigate down all the way to
96:41 - theam stuff and so what we've got here
96:44 - is get roll create roll and so on so in
96:47 - here I'll select it we'll do the
96:49 - included service and we we'll go ahead
96:51 - and look for the put and I think it was
96:54 - what put or no tag roll so we'll go
96:56 - ahead and do tag roll and there is the
97:00 - tag roll and untag roll so I now add
97:03 - both of those just so I have the ability
97:04 - to do that tagging and so we'll once
97:07 - again save this so we hit next here and
97:11 - save
97:12 - changes and then of course now I'll go
97:14 - ahead and run that deploy once again and
97:17 - of course that's how it goes this is how
97:19 - I was able to build out that policy in
97:20 - general and these things modify every
97:23 - once in a while there's little things
97:24 - that the serverless framework adds on
97:26 - that it needs access to do like taging a
97:29 - roll I don't think is so critical that
97:31 - it needs to have it happen but there's
97:33 - also things behind the scenes that I
97:35 - don't know that's going on that might
97:37 - have a good reason to be able to tag
97:39 - that role or have that particular
97:40 - permission um so yeah I'll let this
97:42 - finish and then we'll come back and here
97:44 - we go we got this success that it was
97:46 - able to deploy so I'll go ahead and grab
97:48 - the URL itself we'll just do a quick
97:50 - curl call it might take a moment for it
97:52 - to boot up completely all across the
97:54 - board but once it does it actually gives
97:56 - us the results that we were expecting so
97:59 - now what I want to do is simulate the
98:01 - problem with something on Lambda itself
98:04 - like if there was some issue on Lambda
98:06 - let me just go ahead and use you I'm
98:09 - just getting rid of the L there
98:11 - altogether right and the only reason for
98:13 - that is just to see what's going to
98:15 - happen with these results here and I
98:17 - also want to bring in something else and
98:19 - that is I'm going to grab this right
98:21 - here here I'm going to just go ahead and
98:22 - say now and then I want to go ahead and
98:25 - declare a Delta and this is going to be
98:28 - equal to dates. now minus now.
98:32 - now.
98:35 - time and then we're going to divide this
98:37 - by a th000 I want to see how long it
98:40 - takes for the application which is what
98:42 - this will do to actually reach the
98:44 - database and come back and that's what's
98:47 - going to happen with this new Delta here
98:49 - just so I can see what exactly is
98:50 - happened happening within the database
98:52 - so we can once we actually get
98:54 - everything working we will see a Delta
98:55 - so it'll be something that changed at
98:57 - least a little bit and so uh this is
98:59 - going to be our now. now or maybe our um
99:02 - you know DB now result let's just make
99:06 - it a little bit more of aose so it's not
99:08 - too confusing here and there we go okay
99:11 - so now that we've got this let's go
99:12 - ahead and do another deploy so I don't
99:15 - need to push it or anything like that I
99:16 - just run npm run deploy and it will
99:19 - start that process so what's going to
99:21 - happen is it's going to fail right so
99:23 - it's not going to find the parameter and
99:24 - it's going to be a major system error so
99:27 - what we're going to need to do is look
99:28 - for AWS Lambda we'll actually go into
99:31 - Lambda itself we don't need to type out
99:33 - AWS actually uh so we'll go into Lambda
99:35 - itself we'll click on here and we will
99:38 - see very soon where we will be able to
99:40 - navigate the problem with this and it
99:42 - comes into monitor this tab right here
99:45 - we can go to view cloudwatch logs and
99:48 - this will allow us to see some of the
99:50 - errors that might come through for this
99:53 - particular project from these log
99:54 - streams and we can see other logs as
99:56 - well like the things actually working
99:58 - but the idea is this log stream is how
100:00 - you might be able to diagnose errors
100:02 - with your particular project so in my
100:04 - case that that error is going to be this
100:06 - parameter is invalid and so the
100:09 - application itself won't attempt to use
100:11 - that parameter until it attempts to use
100:13 - the database client at least at this
100:15 - point right so realistically I think
100:17 - when I wanted to boot up the application
100:19 - I might then want to initialize the
100:21 - database client right away instead of
100:24 - always in the endpoint itself but those
100:26 - are optimizations that I don't really
100:28 - need to worry about because they're not
100:29 - really that necessary at this point but
100:31 - the idea being that we've got an error
100:33 - coming through and I just want to see if
100:35 - it's done and here we go we've got it
100:37 - done I'll go ahead and do a curl here um
100:39 - and we should get an error back invalid
100:41 - or internal server error so let's go
100:44 - back into our production and we've got
100:45 - two new streams here I'll go to the
100:47 - newest one click on that and we're going
100:50 - to see that we've got a runtime error
100:52 - right here and then we'll go parameter
100:54 - not found right so that's the actual
100:58 - error itself and this obviously means
101:00 - that a parameter name was not found I
101:02 - don't see anything else in here for
101:04 - catching the error or handling the error
101:06 - as in right here we might want to handle
101:09 - that error or down here we might want to
101:11 - handle that error in the actual
101:12 - application itself U but I wanted to
101:14 - simulate what would happen if there were
101:16 - an error with the parameters themselves
101:19 - so we could actually update and make
101:21 - sure that it's working correctly on our
101:23 - next deploy right and so this would be
101:25 - something you'd also want to test
101:26 - locally and make sure that all of that
101:28 - stuff's working uh but it is important
101:30 - to be able to also see the logs when you
101:32 - need to inside of a Lambda function and
101:35 - just a quick recap as to how to get
101:37 - there you go to the Lambda function you
101:39 - go to Monitor and you go to view
101:41 - cloudwatch logs and then You' be able to
101:43 - see all of the various logs in there
101:45 - including the metrics that are happening
101:47 - for the application itself and how
101:49 - quickly all of that's working
101:51 - and so once this actually finishes I do
101:53 - want to see that updated Delta I want to
101:56 - see what that value is because it's
101:57 - pretty impressive so while that's still
101:59 - building I'm going to go ahead and
102:01 - create a new uh we'll do mpm run Dev and
102:05 - then I'll go ahead and go back up I want
102:08 - to compare and contrast the production
102:12 - version versus the local one so if I do
102:13 - curl HTP Local Host 3000 I should get
102:18 - that Delta and it gives me a pretty fast
102:21 - Delta right this is local and then if I
102:23 - go into
102:25 - production do the same thing and it will
102:28 - take a moment to boot so the first one
102:30 - might be a little bit longer but then
102:31 - it's going to be just as fast or a lot
102:34 - faster uh it really depends on where you
102:36 - are located in the world and how quickly
102:38 - the Delta ends up working as well uh but
102:41 - overall that's it we are able to now
102:43 - check is how fast we can actually get
102:46 - the database call back to us a return
102:48 - call back to us have the database spun
102:50 - up and then also have our application
102:53 - now live and you know out there so we
102:56 - can we can use it in many different ways
102:59 - uh which is great so from here on out we
103:00 - just need to build on top of this
103:02 - because realistically we have the
103:04 - foundation we would need to build
103:06 - full-fledged applications we've got our
103:07 - database we've got a way to deploy it
103:09 - and we've got a way to secure it with
103:11 - the secur data itself um it's fantastic
103:14 - now one of the things that we're not
103:16 - going to cover are custom domain names
103:18 - so adding a custom domain name to this
103:20 - system it's not negatively supported by
103:22 - serus because there's a lot of different
103:24 - ways to put custom domain names on AWS
103:26 - Lambda there are some support out there
103:29 - for custom domain names but that's just
103:31 - not something we're going to cover at
103:32 - this time that would be the only thing I
103:34 - would want to improve about this project
103:37 - for fully production consumer facing
103:39 - projects if it's not consumer facing
103:41 - projects which is kind of what we're
103:43 - assuming this could go anywhere now we
103:45 - could actually use it anywhere just by
103:48 - using that endpoint itself and then
103:50 - making updates as we see fit now we're
103:52 - going to go ahead and decouple our
103:54 - application so it's a little bit more
103:56 - concise on how it works and it's just
103:57 - cleaned up then we're also going to
103:59 - update our AWS SDK we want to upgrade it
104:02 - to a newer version now this older
104:04 - version is still valid and it still
104:06 - works and it'll probably work for years
104:07 - to come but overall we want to start
104:09 - adopting the newer stuff because the
104:11 - package itself is probably smaller as
104:13 - well and maybe just a little bit easier
104:15 - to use than what we've got here with
104:17 - this promise and all that that's kind of
104:18 - a mess so let's go ahead and use the new
104:21 - package which is aws-sdk
104:24 - client SSM so we'll go ahead and install
104:27 - this into our package since we're
104:30 - installing that I'll go ahead and do mpm
104:32 - uninstall the AWS SDK I want to get rid
104:35 - of that one right away just to make sure
104:37 - I don't have that any longer so with
104:40 - this in mind I want to just change how
104:41 - I'm going to grab that parameter itself
104:44 - so to do this I'm going to go ahead and
104:45 - create a new module with lib secrets. Js
104:50 - and I'm gonna quite literally copy
104:51 - everything from this DB client stuff up
104:54 - and so this is where we'll change
104:56 - everything so I'm G to get rid of all of
104:57 - the expressjs things that we just simply
105:00 - don't need in here and this will be just
105:02 - be get database URL now and it's going
105:05 - to be roughly the same values here I do
105:08 - not need the neon stuff just yet we will
105:10 - decouple that one as well um but overall
105:13 - if I were to keep the old version this
105:16 - is how I would decouple it right just
105:18 - like that and then I would update where
105:20 - I need to use this database URL with the
105:22 - Imports as we'll see in a little bit as
105:24 - well okay so with these things in mind I
105:26 - want to go ahead and use that new SDK so
105:29 - what we can do is we can look in the
105:30 - documentation and scroll down a bit you
105:32 - can see the installation process we've
105:34 - already done that you can also see the
105:36 - import process is basically this right
105:38 - here so I'm going to go ahead and copy
105:40 - this and we'll go ahead and paste this
105:41 - up here now and so I'm going to get rid
105:43 - of this AWS here and to create the
105:46 - client we'll go ahead and come down here
105:48 - and say const client equals to well
105:51 - let's see how they do it in the
105:52 - documentation it says new and SSM
105:57 - client hey what do you know is very
106:00 - similar to what we saw before in fact we
106:02 - can pass the region in like we saw
106:04 - before as well the exact same way so
106:07 - that modification was already really
106:09 - simple so we can get rid of that and
106:11 - it's not a whole lot different going
106:13 - throughout so next one is going to be
106:15 - the actual pram data so I'll leave that
106:17 - in but I'll get rid of this git SSM
106:19 - parameter way but rather just turn this
106:22 - into an object itself then we need to
106:24 - actually create this as a command so we
106:26 - say con
106:28 - command and this is going to be equal to
106:30 - well what we've see in the documentation
106:32 - is this list associations command which
106:35 - is not what we want we want something
106:37 - different and there are a lot of
106:39 - different commands that you can run in
106:41 - here if you scroll down you will see all
106:44 - of them the one we need is of course the
106:47 - git param command so G parameters is in
106:50 - here so let's go ahead and grab that and
106:52 - I'll just search by it unfortunately it
106:55 - doesn't give me the actual command that
106:57 - I want by searching in here if you go
106:59 - into the API reference you will see it
107:02 - but what I know is I can just go ahead
107:03 - and change this to just get parameter
107:06 - command it's really just that simple so
107:08 - the command itself will then be new of
107:11 - that command with these arguments in
107:13 - here and then we want to grab the result
107:15 - from it with a wait and this is going to
107:17 - be client. send and that command and
107:22 - then the final data is just simply that
107:24 - results parameter. value and that's it
107:26 - just a SM a small modification to how
107:29 - this ends up working and then we'll go
107:30 - ahead and do module. exorts and the git
107:34 - database URL is equal to that right
107:36 - there great so what we could do now of
107:39 - course is change our DB client to work
107:42 - off these secrets now I could do it in
107:43 - the index.js or I can follow along this
107:46 - pattern of decoupling and decouple the
107:49 - DB client to its own location so that's
107:51 - what I'll do I'll do DB clients I'll use
107:55 - the S in here for a reason as you'll see
107:57 - once we get to the drizzle stuff now
107:59 - I'll go ahead and copy all this once
108:01 - again just because it's already working
108:03 - mostly okay and I'll get rid of the
108:05 - things I don't need which is serverless
108:07 - HTTP and express I also won't need this
108:10 - AWS part here instead what I'll do is WR
108:13 - under the neon import I'll go ahead and
108:15 - do con Secrets equals to require and
108:19 - then my new libr library for secrets and
108:23 - that's the one I'll use so I'll go ahead
108:25 - and get rid of all of the SSM stuff in
108:27 - here now and so really this pram data
108:29 - store is no longer going to be that
108:31 - instead we'll get rid of all these
108:33 - comments even I'll just do con DB URL
108:36 - equals to
108:38 - secrets. database URL and of course this
108:41 - is an asynchronous function so we go
108:43 - ahead and do await those secrets and now
108:45 - I can actually just use that database
108:47 - URL just like that so it's cleaned up a
108:50 - lot in both places if you ask me the
108:53 - database URL SSM pram I might put in
108:55 - here or maybe I have it in as an
108:57 - argument itself if I needed to change it
108:59 - and support other things uh but overall
109:02 - it's definitely cleaned up a bit so last
109:04 - thing of course is just to bring in our
109:05 - database client into the index itself so
109:07 - to do that we'll do module exports and
109:10 - the database client is going to be that
109:12 - database client let's go ahead and stick
109:14 - with that same naming convention we just
109:16 - did with the git database URL to get DB
109:19 - client
109:20 - and just change it ever so slightly and
109:23 - now in index.js here all we need to do
109:26 - is bring that one in as well this time
109:28 - I'll just go ahead and import the
109:30 - function itself and so we'll do const
109:32 - getdb client and this is going to be
109:34 - require and then do/ DB and
109:39 - clients just like that and of course the
109:42 - index file itself is not inside of Any
109:44 - Given folder which is why we can just do
109:46 - that so now that we've got that here is
109:49 - the old DB client which I'll go ahead
109:50 - and get rid of and now SQL itself can be
109:53 - just a wait get DB client right there
109:56 - and I can get rid of all of the old AWS
109:59 - client stuff as well as Neon right
110:01 - inside of this
110:03 - index.js because now it's imported in
110:05 - places that it makes a lot more sense so
110:08 - of course the last thing to test here is
110:11 - that this is actually working so let's
110:13 - go ahead and do mpm run Dev here and so
110:16 - there is my Dev server running and let's
110:19 - just do a quick curl call and we'll go
110:22 - ahead and do curl and after a moment or
110:25 - two because everything needs to spin up
110:28 - it go it does that exact call great so
110:32 - there's one last thing that I want to
110:33 - fix and that is this Delta here this
110:36 - Delta I don't think is actually accurate
110:38 - so what we want here is we want to grab
110:40 - now which is going to be date now we
110:43 - want to actually call this before we
110:45 - call our SQL command this will now be in
110:48 - the past where this should be in the
110:49 - present hopefully we're going to go
110:51 - ahead and change the ordering of this
110:54 - just
110:55 - slightly and I'll go ahead and get rid
110:57 - of
110:59 - the divided by a th000 here we'll save
111:02 - that and I want to just refresh real
111:05 - quick and run this and just see how
111:08 - quickly I can grab this
111:11 - data okay so let's take a look at the
111:13 - Delta now it should be a different
111:15 - number so this is still in milliseconds
111:18 - so we can put that
111:21 - back of SL
111:24 - a000 just to make it a little bit more
111:26 - accurate as to seconds uh but overall we
111:29 - just wanted to see something a little
111:31 - bit closer to how long it actually takes
111:34 - so I'm making this request from the
111:35 - other side of the world and it actually
111:37 - is still going pretty fast which is
111:39 - pretty nice um but with this in mind
111:41 - we're going to go ahead and do mpm run
111:43 - deploy I did say the other side of the
111:45 - world I meant the other side of the
111:46 - country um that's all so I'm going to go
111:50 - ahead and deploy it let that run there's
111:51 - one more thing that I might want to have
111:53 - in my scripts in here which is just
111:55 - going to be simply info so every once a
111:58 - while when you run deploy it's not
111:59 - necessarily going to always show the
112:01 - endpoint that you are using so using
112:03 - just info will give us that endpoint so
112:06 - we can see that by running it in here
112:08 - with mpm run info this will give us that
112:11 - data from the current one that's being
112:13 - deployed which is going to be this
112:14 - endpoint right and so the endpoint will
112:18 - or could change if you took it down and
112:20 - then brought it back up that endpoint
112:22 - will might most likely be different uh
112:23 - but I'm going to let this finish and
112:25 - then we'll just verify everything is
112:26 - working with our stuff decoupled and all
112:29 - that looks like it finished it was
112:30 - pretty fast this time so we'll go ahead
112:32 - and curl it out and I'll run that and
112:34 - there we go so the Delta is really
112:37 - really small right that's the point of
112:39 - what we see there and so the actual
112:42 - commands themselves the time stamp it's
112:44 - a little bit tricky to get an accurate
112:46 - representation of the time stamp that's
112:48 - happening but over overall we do see
112:50 - that the actual Delta is not really that
112:53 - different it's like almost instantaneous
112:56 - because of how close it is whereas when
112:57 - I was when it was on my machine it was a
113:00 - little bit longer and that would make
113:01 - sense because of how far away it is uh
113:04 - but it is very fast cool so now that
113:06 - we've got this decoupled and we're using
113:08 - the new AWS client it's time to actually
113:10 - start building out our schemas so we can
113:12 - actually get to the process of actually
113:14 - putting stuff in to our database now
113:17 - we're going to go ahead and implement
113:18 - the drizzle omm now OMS are great
113:21 - because they allow us to basically use
113:23 - our runtime language to manage our
113:25 - database in this case our runtime
113:27 - language is Javascript and it's going to
113:29 - be managing our postgress database and
113:31 - drizzle omm is going to handle that
113:33 - interaction for us so we can just write
113:36 - a bunch of JavaScript instead of writing
113:37 - a bunch of SQL and what it's going to do
113:39 - is it's going to allow us to insert data
113:41 - like you would with SQL it's going to
113:43 - allow us to retrieve data delete data
113:45 - anything you would need to query it but
113:47 - it also treats the data with the the
113:49 - same data type in other words if in the
113:52 - database it's an integer javascript's
113:54 - also going to treat it as an integer
113:56 - this is also true with time stamps and
113:57 - we've already seen that so using just
113:59 - the neon database client we actually saw
114:03 - this select now doesn't come back as a
114:06 - string it comes back as a date object
114:08 - itself which is how we're just able to
114:10 - do this in other words I don't have to
114:11 - parse it I don't have to like parse the
114:13 - date from here and build out some
114:15 - complicated parsers just to use the
114:18 - value from the database itself
114:20 - and that's the key here is we want to
114:21 - make it so it's super easy to use values
114:24 - directly from the database without
114:26 - making any major changes so let's go
114:28 - ahead and create our first schema so
114:31 - inside of DB here we'll go ahead and do
114:32 - schema. JS or let's call it schemas
114:36 - actually so schema. JS is where we'll
114:38 - Define our tables so this is what what
114:41 - we're going to do is we're going to do
114:42 - const lead table and for now I'm going
114:45 - to go ahead and just put a dictionary
114:46 - here and really think out what I want in
114:49 - a lead table table well I'm going to go
114:51 - ahead and say I want an email uh perhaps
114:54 - I want to have a timestamp right like a
114:57 - created timestamp and maybe I want a
114:59 - description maybe a source right maybe
115:02 - the first name maybe last name right and
115:05 - so these are all of the columns that I
115:07 - might want to store in my database now
115:10 - to keep things simple I'm just going to
115:12 - go ahead and just do email and timestamp
115:14 - for now but instead of calling it
115:16 - timestamp I'll just call it created at
115:19 - so the these are going to be the fields
115:20 - we will want to use and so we need to
115:23 - actually design these in a way that
115:24 - makes sense within the drizzle way of
115:27 - doing things so let's go ahead and
115:29 - comment it out for a moment and let's
115:32 - take a look at the drizzle documentation
115:33 - and how this is done so inside of
115:36 - drizzles you know om. drizzle. team in
115:40 - the docs if you scroll down here we've
115:41 - got these column types here for managing
115:43 - the schema this is where I like to start
115:46 - because well I want this to manage my
115:48 - database so we need to start with
115:51 - designing the tables we're going to
115:52 - design and of course you could reverse
115:54 - engineer this too if there were already
115:56 - existing tables but that's outside the
115:58 - scope of what I want to accomplish
116:00 - instead we want to just create the ones
116:02 - that we want to create so in here we're
116:04 - going to go ahead and look for a column
116:06 - type called email and we're going to be
116:09 - severely disappointed there is no column
116:12 - type email now this is for good reason
116:15 - that's because email is really just a
116:17 - string of data right it's just like abc
116:21 - abc.com that's just data that's just a
116:24 - string of data so validating that it's
116:26 - email is going to be something we'll do
116:28 - later for now we just need to treat it
116:30 - as if it was data and that is going to
116:32 - be text right so it's really just text
116:35 - itself so we're going to go ahead and
116:37 - follow this line of thinking right now
116:39 - to build out our lead table so I'll go
116:42 - ahead and copy and paste it in and we
116:45 - can't do it in this fashion instead we
116:47 - have to change it to const and then
116:49 - equals to require and that's how we
116:51 - actually do our Imports with this es5 or
116:54 - the Comm common.js module um and so the
116:57 - next thing is going to be our table so
117:00 - the the const table the actual variable
117:02 - name itself or the constant name in this
117:04 - case we actually want to keep it in as
117:06 - lead table the actual database table
117:08 - itself I'm going to call it leads so
117:11 - this is going to be my JavaScript name
117:13 - for the table this is going to be the
117:14 - actual database name for the table and
117:16 - what do you know this is also the same
117:18 - so the first thing is going to be our
117:20 - email we want to leave it in as email
117:22 - this is going to be the text data type
117:24 - or the text column and inside of there
117:27 - we're going to go ahead and leave the
117:28 - database as email as well so they are
117:30 - corresponded they're the same you don't
117:32 - always have to do that but that's one
117:34 - way to do it next we're going to go
117:35 - ahead and do our
117:37 - created at now within JavaScript I
117:40 - typically use this camel case and I
117:43 - think that's the best practice within
117:44 - JavaScript but in the database I might
117:47 - want to change it a little bit
117:48 - differently and use use snake case which
117:51 - is common inside of databases but you
117:53 - could use camel cases if you want as
117:55 - well so what I got here is a leads table
117:58 - but hopefully you have a little bit of a
118:00 - red flag going off or a little bell
118:02 - going off is like should I treat
118:04 - something like created as or created at
118:07 - as text well no there's another data
118:11 - type and we've already seen it we've
118:12 - already used it which is related to a
118:14 - timestamp itself so going back into our
118:17 - column types we can scroll down and we
118:19 - can see there's one called timestamp
118:21 - there's one called time there's one
118:22 - called date so I can actually use a
118:25 - timestamp itself and that's what I want
118:27 - so I'll go ahead and come in into my
118:29 - schemas and use a timestamp in here and
118:32 - so I'll go ahead and use that instead so
118:35 - we can also set a default here as to
118:37 - default now so default now works on
118:40 - timestamps it doesn't work everywhere
118:42 - else so if you wanted to set a default
118:44 - let's say for instance for a description
118:47 - let's do that now we'll just add in that
118:49 - new column I can add in a default here
118:51 - with just simply default and say this is
118:54 - my comment or something like that right
118:58 - we can really build that out on how we
118:59 - want to do it based off of the data T
119:02 - tables that we have here and so another
119:05 - advantage of using specific integer
119:07 - types has to do with doing calculations
119:10 - or running functions inside of the
119:14 - database itself so if I were to jump
119:16 - back into the neon console here I want
119:18 - to show you this in action so first off
119:20 - this select here I can run this and this
119:22 - will give me a bunch of numbers right
119:24 - it's going to generate series it's going
119:25 - to generate numbers from well in theory
119:29 - 0 to 100 but it actually goes into 11
119:33 - 101 rows so it does from 0 to 100 into
119:36 - however many rows that takes which is
119:38 - what that generate series does and now
119:40 - we can also count how many rows there
119:41 - are by selecting and running that and
119:44 - this is a database function to count up
119:46 - all those rows we can also use a
119:48 - database function to sum up all of those
119:50 - rows there's many different functions
119:52 - that you can do and you can also see how
119:54 - this would affect maybe different rows
119:56 - in there as well so if you did a 100 and
119:58 - you multiply each row by 1.2 that would
120:01 - give us that sum whatever that ends up
120:03 - being right and of course you can also
120:05 - do much bigger rows as well so this is
120:07 - where databases start to really outshine
120:09 - doing something directly in JavaScript
120:12 - you would actually do the calculation
120:14 - something this big you would do it on
120:16 - the actual database side and then you
120:19 - would bring it into JavaScript and so
120:21 - that's where these data types have get
120:23 - another reason to use them as the
120:25 - correct data type right so in the case
120:27 - of the time stamp this is great because
120:29 - then I could use the JavaScript stuff to
120:31 - calculate how long it's been basically
120:33 - when I want to display it but also if I
120:35 - wanted to do like the number um you know
120:38 - of entries I could do something like
120:41 - that and then that way I could then grab
120:44 - maybe an in a integer type so we come up
120:48 - here we've got an integer I could bring
120:50 - in an integer in here as to what that
120:53 - would end up being right and we could
120:56 - have this default being zero now the
120:59 - integer itself we would also want to
121:00 - import
121:03 - it okay so there's a lot of different
121:05 - ways on how we can think about writing
121:07 - out these actual tables themselves I'm
121:10 - actually not going to do that number of
121:11 - entries this way because there's a way
121:13 - to count it in the database as well with
121:15 - the email being the same uh so that's
121:18 - outside the scope of this one but these
121:19 - are generally the fields that I want to
121:21 - have but there is one more field I want
121:23 - to keep as well and that's going to be
121:25 - an ID field so every time I do an entry
121:29 - I want to have a new ID field in here
121:31 - and the way we do that is by using
121:33 - something called serial and we can set
121:35 - this as our primary key just like this
121:39 - and this will be our primary key for the
121:41 - database which helps with indexing and
121:43 - speed and all sorts of stuff but overall
121:46 - the primary key here and we'll go ahead
121:48 - and say not null in fact wherever I
121:50 - don't want this to be empty in the
121:51 - database I will go ahead and say not
121:53 - null the description itself it could be
121:56 - empty that's okay and so now I've got my
121:58 - actual schema that I want to use for my
122:01 - entire project itself I've got something
122:04 - a table with ID email description and so
122:07 - on so it's time to actually bring this
122:09 - in to our table or our database itself
122:13 - but the thing is I don't have a way to
122:15 - bring it in just yet so let's go ah and
122:17 - do module. exorts and Lead table and so
122:21 - what we've got here now is the starting
122:24 - of creating an actual table in the
122:27 - database but we don't actually have the
122:29 - SQL behind it so we need to create the
122:31 - SQL behind it and then we need to run or
122:34 - migrate that SQL and actually execute
122:36 - that SQL and so that's what we'll do in
122:38 - the next few now we're going to convert
122:40 - this JavaScript schema into an actual
122:43 - SQL migration file so to do this I want
122:46 - to start off with creating a folder
122:48 - called migrations now I'm putting it
122:50 - inside of the SRC because I really just
122:52 - want this all bundled in one place
122:54 - sometimes it's recommended that you just
122:55 - put it in your main project itself so
122:58 - with it in here with this folder
123:00 - existing and my schemas existing I'm
123:03 - going to go ahead and create a new file
123:04 - called
123:05 - drizzle. config.js so this configuration
123:10 - itself is going to be fairly
123:13 - straightforward we declare a constant
123:15 - called config and we set this equal to
123:19 - well our schema the location as to where
123:22 - it is and then where we want to Output
123:25 - this schema as in that migrations folder
123:28 - and then we're going to go ahead and
123:29 - Export this as a default so export
123:32 - default and the config itself so the
123:36 - reason I can do this is because the way
123:38 - I'm going to actually use this file is
123:40 - going to be right inside of package.json
123:42 - I'm going to go ahead and come below
123:45 - here and do something called generate
123:47 - and this is where we'll use use drizzle
123:50 - kit itself so drizzle kit will allow me
123:53 - to do
123:54 - drizzle kit generate colon PG as in
123:59 - postres and then config equals to
124:03 - drizzle. config.js or that file name and
124:07 - that file path so in this case the file
124:10 - path for drizzle. config.js is right
124:13 - next to package.json if you had it into
124:15 - the SRC folder you would do something
124:18 - like that okay so with this in mind I
124:20 - now have a way to call this file but I
124:22 - don't have the configuration correct so
124:25 - what I'm going to do is I'm going to
124:26 - copy this relative path here bring it in
124:29 - and what do you know then I'm going to
124:30 - go ahead and copy the relative path to
124:32 - migrations also bring it in and there we
124:34 - go so we can use slash if we need to uh
124:38 - but the idea being that it's a relative
124:40 - path to wherever drizzle config is and
124:43 - once again if it was in SRC we would get
124:45 - rid of SRC no surprise there okay so
124:48 - with this let's go ahead and run this
124:49 - mpm run generate and hit enter and all
124:53 - this is going to do is create our SQL
124:56 - file this is quite literally the SQL
124:58 - that will run to match this schema right
125:01 - here and if we were to change anything
125:03 - let's say for instance we got rid of
125:05 - this description here I want to generate
125:07 - that migration file and once again it
125:10 - shows me exactly what I need to run in
125:13 - order for me to match my schema to my
125:16 - database right so it's going to do each
125:18 - one of these things and that's it that's
125:21 - really just that simple and if I were to
125:23 - add a whole another table let's go ahead
125:25 - and add in a whole another table I'll
125:26 - call this lead table 2 and leads to
125:29 - whatever right let's go ahead and Export
125:31 - it just make sure you export it as well
125:35 - and then I run the schemas again now
125:38 - it's going to go ahead and show me hey
125:39 - what do you know it's creating a new
125:41 - table in here so since I haven't
125:43 - actually touched the database yet I can
125:45 - actually feel comfortable about deleting
125:48 - this folder
125:49 - if I try to run it again it's going to
125:51 - probably give me an error or it's going
125:53 - to actually do the entire thing itself
125:56 - right so the actual migrations folder
125:57 - was created this time uh but I've had it
125:59 - where it didn't create but what we see
126:01 - is it actually will now just combine
126:03 - those two commands into one and so this
126:06 - is where you can really learn a lot
126:07 - about SQL itself because well we're
126:10 - using JavaScript to generate that SQL so
126:12 - in my case I want to get rid of all that
126:14 - and I really just want that one single
126:17 - uh migration file itself
126:19 - and that's where we're going to leave it
126:20 - is just like that now there's one other
126:22 - thing to note is this meta folder in
126:24 - here is giving a snapshot of each change
126:28 - that's happening so it's really keeping
126:30 - track of the changes between these SQL
126:32 - files themselves so when we go ahead and
126:34 - run it that's how it knows not to do
126:37 - anything it didn't actually change
126:39 - anything because we didn't change
126:40 - anything in our migrations so that's
126:42 - actually pretty nice that it has a
126:44 - history of these changes the other thing
126:46 - about this that's really great is I can
126:48 - check this into G so I can keep those
126:50 - changes throughout and I can see all of
126:53 - the schem of changes that might happen
126:55 - so if at any time I need to roll back
126:57 - those changes I could and also I could
127:00 - run these migrations on well maybe a
127:03 - branched version of the database and
127:05 - make sure everything's working correctly
127:07 - before I actually run those migrations
127:08 - on the main version of the database
127:11 - which is yet another thing to think
127:12 - about when it comes to developing with
127:15 - the various schemas and of course
127:17 - drizzle om so at this point now that we
127:19 - can create or generate our migrations we
127:22 - actually need to perform those
127:23 - migrations and to do that I'm going to
127:25 - make my own CLI tool to actually make
127:28 - that happen just because it automates it
127:30 - just a little bit to work with our
127:32 - serous environment that we have and more
127:35 - specifically with our neon database and
127:38 - all of our neon client related things
127:40 - itself because as it stands right here
127:43 - this actually won't do the migrations
127:45 - for us we need to do a slight variation
127:47 - on this so let's go aead and and take a
127:48 - look at how to do that now now there's a
127:50 - few ways to perform these migrations one
127:52 - of them is you could just copy and paste
127:54 - it and just run it wherever you run SQL
127:56 - like for example in index.js we could
127:59 - run it like this we could go into the
128:01 - console run it there but those things
128:03 - aren't scalable and they're prone to a
128:05 - lot of Errors instead we want to
128:07 - automate them so to automate them I'm
128:09 - going to create a new SRC file called
128:11 - CLI SL
128:14 - migrator JS so this right here is is
128:18 - going to use TSX to be called so TSX SRC
128:23 - CLI migrator JS so we want to call it
128:28 - this way and the way it's called is by
128:30 - doing if require domain equals to the
128:34 - module itself we're going to go ahead
128:36 - and console log run this exclamation
128:40 - mark so obviously we need to fill out
128:42 - all of these things but before I do I
128:43 - want to make sure that TSX is even
128:45 - working so inside of my my package.json
128:49 - I want to go ahead and add in my Dev
128:51 - dependencies two things so mpm install
128:55 - D- saav dev. EnV and then also TSX so
129:01 - EnV is going to play an important role
129:03 - as well as we'll see in just a moment
129:05 - but in my migrator here let's go ahead
129:07 - and just run this I'll go ahead and use
129:09 - this command itself into package.json
129:11 - and we're just going to call this
129:15 - migrate there we go so MP M run migrate
129:19 - hit enter and run this great so that's
129:22 - exactly what we wanted to see the next
129:24 - thing I want to check is the environment
129:26 - variables access key and secret access
129:29 - key we just need one of them really so
129:31 - in my migrator here I'll go ahead and do
129:34 - console log and
129:38 - process. env. AWS access key write it
129:42 - again ooh undefined now this is why we
129:46 - use the EnV so if I just run require
129:49 - and. EnV then config run that and run it
129:55 - again now we can see that environment
129:57 - variable and it's coming directly from
129:59 - EnV so just like with our serverless
130:01 - package we have EnV being able to be
130:04 - used our now CLI package can also load
130:07 - in those environment variables like that
130:10 - now why is this important well hopefully
130:12 - you realize it's important because well
130:14 - we need to grab our secret we need to
130:17 - get that database your L and so back
130:19 - into my migrator here let's go ahead and
130:21 - import that database URL by doing cons
130:24 - secrets and this is going to be equal to
130:26 - require and dot dot slash and this going
130:29 - to be lib secrets and then now I'll go
130:32 - ahead and do my secret itself so how am
130:36 - I going to go ahead and call this secret
130:37 - because it's asynchronous well I'm going
130:39 - to go ahead and do async function and
130:42 - we're going to call this perform
130:45 - migration and then I'll go ahead and do
130:47 - the const DB URL equals to A wait
130:51 - secrets. git database URL and then I'll
130:54 - just console log that DB URL okay so
130:58 - then in this main module here I can just
131:00 - do the promise handling and we'll say
131:03 - Val equals
131:07 - to and something like this and then
131:12 - catch
131:13 - error
131:15 - and let's go ahead and console log the
131:19 - error okay and so we can exit the
131:21 - process with process. Exit 0 this is
131:25 - successful we can also do
131:27 - process. one as in
131:30 - unsuccessful okay so um there we go now
131:33 - I want to just see this database URL
131:35 - let's go ahead and run it again and our
131:37 - database URL shows up so this at the
131:40 - very minimum is how I can actually use
131:42 - those environment variables now we might
131:44 - need to change them we might need to use
131:46 - different ones but it's just important
131:47 - to realize that if I did actually use my
131:50 - environment variables let's go ahead and
131:52 - change it just slightly and do export
131:54 - the access key to something like ABC now
131:57 - it's in the environment I'm going to go
131:59 - ahead and comment out to this EnV here
132:02 - and I'll go ahead and run this again and
132:04 - now it just shows me ABC and I get an
132:06 - error right so that is why we actually
132:09 - use this required. EMV but it is
132:11 - actually not required as long as the
132:13 - environment variables are actually in
132:15 - the environment it will use them uh but
132:17 - if if are not in the environment which
132:19 - in our case they will not be we will go
132:21 - ahead and add in that that way can we
132:23 - can actually get the correct data itself
132:26 - which of course caused this error still
132:28 - so let's go ahead and just refresh that
132:29 - terminal itself and create a new one and
132:32 - then npm run migrate like that and there
132:35 - we go so now we've got that data Okay
132:37 - cool so now we need to build out and use
132:39 - that database URL and to do this we need
132:42 - to use a neon pool or a postgres pool uh
132:45 - but the neon seris pool
132:48 - now the idea for this is to have
132:50 - multiple levels of a transaction that
132:52 - might need to happen which is exactly
132:54 - why we're doing the serous pool and so
132:56 - the repo itself for this package
132:59 - actually has some good documentation on
133:01 - how to do exactly what we need which is
133:04 - doing our pool with a websocket itself
133:06 - so this is what we're going to be doing
133:08 - I'm going to go ahead and copy all of
133:09 - this and we're going to bring it on into
133:12 - our code itself for this command line so
133:15 - the first thing is I need to uh you know
133:16 - take out the actual import statements
133:19 - and change them so they are es5 and
133:21 - we'll do
133:22 - require and like that and then same with
133:27 - this and here we
133:33 - go great so let's go ahead and tab this
133:37 - in a little bit so first and foremost
133:39 - the connection string well what do you
133:41 - know it's to this database URL it's no
133:43 - longer process EMV it's just to that
133:45 - database URL okay so we would have a
133:48 - console if there's an error with the
133:49 - pool itself that's something that well
133:51 - basically like we would want to just
133:53 - reconnect or try something new so the
133:56 - next parts of this are how we actually
133:59 - run the query itself and so the things
134:03 - that are important that we're really
134:05 - going to change is this right here right
134:08 - and so that's where we need to actually
134:09 - grab drizzle and change how we use
134:12 - drizzle itself so inside of here is
134:15 - where we're going to run our migration
134:17 - so at the top we're going to go ahead
134:18 - and first off do const and
134:21 - drizzle and this is going to be require
134:25 - and it's drizzle o SL neon
134:30 - serverless and then I'll go ahead and
134:32 - also we're going to go ahead and
134:33 - implement this down here so we'll do
134:36 - const and DB equals to a weight drizzle
134:41 - of this
134:43 - client and our schema so we need to also
134:47 - Define where our schema is going to be
134:49 - so let's go ahead and grab the entire
134:51 - module itself so right above secrets
134:54 - I'll go ahead and do schema and this is
134:57 - going to be our DB and
135:04 - schemas okay there's our schemas there
135:07 - next what we want to do is bring in
135:09 - migrate so I'll go ahead and do const
135:11 - migrate and this is equal to require and
135:14 - this is drizzle o
135:16 - rm/ poost
135:18 - scjs SL
135:22 - migrator okay so this is a manual way to
135:24 - call migrate and then the next part is
135:27 - going to just be
135:28 - await and calling migrate on the
135:31 - database itself and then using the
135:34 - migrations folder which in our case is
135:36 - in SRC migrations
135:42 - right right here so grabbing that
135:44 - relative path we'll put that in there
135:47 - just like that
135:48 - so that's it that's how we will actually
135:50 - run those migrations at the very end
135:52 - here we'll await
135:54 - pool. end just to close out that
135:57 - connection after all of that stuff is
135:59 - done so this is definitely in a lot of
136:02 - different places for the documentation
136:04 - uh but the key thing here is that we got
136:06 - our database and if of course if there's
136:08 - not a database so we'll go ahead and say
136:10 - if not DB URL then we'll just go ahead
136:12 - and return something along those lines
136:15 - uh but now we can actually go ahead and
136:16 - run this migration again and so let's go
136:19 - ahead and run it and at this time what
136:21 - it should do we should get rid of these
136:23 - console logs but overall what it should
136:24 - do is actually update our database and
136:27 - so we'll see that with I'll just console
136:30 - log in here
136:33 - like migrations done and we'll say
136:37 - running or run migrations get rid of
136:40 - this and then migrations
136:44 - error great okay so with this in mind
136:47 - let's go ahead ahead and run it again no
136:49 - big deal running migrations migrations
136:52 - done great so to verify let's go back
136:54 - into our neon console into our tables
136:57 - here and what we should see is our leads
136:59 - table inside of that leads table if we
137:01 - wanted to look at some of the data about
137:02 - it we see the different columns and
137:04 - their data types as we see here great
137:09 - fantastic so let's go ahead and take a
137:11 - look at our actual schemas themselves uh
137:13 - notice the ID is down here as well em
137:18 - created that let's go ahead and bring in
137:20 - the description and just do an endtoend
137:21 - example where we go ahead and run our
137:26 - generate and then we run our
137:31 - migrate okay and done refresh in here
137:35 - leads look at the description is right
137:38 - there great so now we have a way to
137:41 - change data in our schema change the
137:44 - design of our schema based off of
137:46 - different columns that we we might want
137:47 - to have then we have a way to actually
137:50 - migrate that schema now the client the
137:53 - pool itself can do a lot of different
137:55 - transactions at once which is why we use
137:57 - the pool itself and it does it using
137:59 - websockets so you're not going to do
138:01 - this in a serverless environment that's
138:03 - why we needed to create a command line
138:05 - interface for it you would not run these
138:07 - migrations once you deploy it to service
138:10 - you run it beforehand and so that's even
138:12 - more reason to actually get the
138:14 - production database URL as well or
138:16 - whatever one you're wanting to deploy to
138:19 - which could add in some additional
138:21 - arguments that you might want to have
138:23 - for that maybe you always have the the
138:26 - production database URL as a part of
138:28 - your secrets so in here maybe you would
138:31 - want to grab this and go get prod
138:36 - database URL and change this stage in
138:38 - here just to be simply prod now I'm not
138:41 - going to do that I'm going to leave it
138:42 - as is uh because that's really the only
138:44 - string that we are using as the
138:46 - production but you could also change the
138:48 - stage itself in the environment if you
138:50 - wanted to change it to something
138:51 - different getting that database URL
138:53 - assuming that it is available uh and if
138:55 - it's not it'll just go ahead and say you
138:57 - know it'll return it and not try to
138:59 - migrate anything so yeah uh the pool is
139:02 - great you can do a lot of Big Time
139:03 - queries with this pool if you were not
139:06 - deploying to serverless this is probably
139:07 - the method you'd want to use for your
139:10 - database in general for some of those
139:11 - bigger transactions that might take a
139:13 - little bit longer so cool now that we've
139:15 - got this we just need to start inserting
139:18 - data into our database using crud
139:20 - methods and then we're going to need to
139:22 - validate that data to make sure it's
139:24 - valid data uh but that's really the main
139:26 - things we're going to do as far as the
139:28 - actual uh you know schema and database
139:31 - related queries are concerned with our
139:33 - schema created our migrations done we
139:35 - are now ready to start actually creating
139:38 - and retrieving data from our database
139:40 - using drizzle and neon so to do this
139:43 - we're going to actually Implement a curl
139:45 - function so we use Curl HTTP col SL
139:48 - localhost 3000 and then leads that will
139:52 - be how we get that data which in this
139:54 - case it's not found currently then we'll
139:57 - go ahead and also use the same exact
139:59 - idea with grabbing post data so to do
140:03 - Post data with curl we
140:06 - do-x say post we're going to go ahead
140:08 - and pass in a header this particular
140:11 - header is going to be content type of
140:15 - application Json which is very common
140:18 - for data that's using an API of some
140:21 - kind and then we pass in data in this
140:24 - case I'm going to go ahead and put
140:25 - single quotes on the outside of it and
140:27 - then double quotes on the inside so
140:28 - we'll say something like email and test
140:32 - test.com and then we'll use our endpoint
140:35 - right there so those are the two
140:36 - endpoints we need to configure so the
140:38 - first step of this is to allow our
140:41 - actual data from the expressjs
140:44 - application to work so the first one is
140:47 - going to go into index.js we're going to
140:50 - go ahead and grab this git path here and
140:53 - we're going to go ahead and just say
140:54 - leads so leads this will allow us to get
140:58 - that data so if we were to run that curl
141:00 - command after we restart our server here
141:04 - we can actually see that data come
141:06 - through or in this case it's just saying
141:08 - hello world now the post data is not any
141:10 - different than this we're going to go
141:11 - ahead and grab this and just do post so
141:14 - instead of dogit it's just poost and
141:17 - that's it so with that in mind we can go
141:19 - ahead and curl it again actually we will
141:21 - go ahead and restart the server one time
141:24 - and then I'll go ahead and press up a
141:25 - few times to that curl post and once
141:28 - again it says hello from path and so
141:30 - both methods are now working HTTP post
141:33 - and HTP git typically speaking you use
141:35 - the post method to you know create data
141:39 - or update it sometimes you use a
141:41 - different method than update but we'll
141:43 - just leave it in as create data so
141:45 - that's what we want to do here and the
141:47 - way we actually can create data is first
141:49 - and foremost we're going to change the
141:51 - Callback Handler to asynchronous and
141:54 - then we're going to go ahead and say
141:55 - that we want to grab the actual data
141:57 - that's coming through from this request
142:00 - itself so that means we're going to go
142:02 - ahead and say the email data is going to
142:05 - be equal to await requestbody we'll just
142:09 - start with request. body or R eq. body
142:13 - so what we want to see here is just
142:14 - echoing back maybe the body data in here
142:17 - and we'll just go ahead and use it's
142:19 - something like that so once again we'll
142:21 - go ahead and restart our server here and
142:24 - then I'll go ahead and do another curl
142:26 - command run this and there's our body
142:28 - data coming back but it looks kind of
142:30 - weird it says buffer so the reason for
142:33 - this is because natively this data is
142:35 - not going to just automatically be
142:37 - turned into Json data so to change it
142:40 - into Json data and basically to accept
142:43 - this application Json content type
142:45 - header we just change change our Express
142:48 - app just a little bit by doing
142:50 - app.use and express. Json just like that
142:55 - so with this in place again restarting
142:58 - our server what we'll be able to do is
143:01 - we'll be able to run this call again and
143:03 - now it'll actually Echo back that data
143:05 - to us which means we can now start to
143:08 - actually bring this into our database
143:10 - which of course is going to require us
143:12 - to do that and so the actual body we can
143:14 - go ahead and now say that's the data and
143:16 - then we can go ahead and say the email
143:18 - is inside of that data itself and now I
143:21 - can just Echo back that email great so
143:23 - here is where we need to insert data to
143:26 - the database so how are we're going to
143:28 - go about doing this now first and
143:30 - foremost in the DB folder here I'm going
143:32 - to go ahead and create a file called
143:34 - crud DJs now what we're going to be
143:36 - focusing is the C and the r of crud that
143:40 - is creating data and retrieving data
143:43 - that's it we're not doing anything more
143:44 - than that at this point but once you
143:46 - have this found in place you'll be able
143:48 - to use it wherever you need to so the
143:51 - main thing here is we want to go ahead
143:52 - and create another asynchronous function
143:55 - so async function and this function
143:57 - itself we're going to just go ahead and
143:59 - call it add lead or maybe new lead new
144:02 - lead might be
144:03 - better and then in here we just want to
144:05 - bring in the email itself and then I'll
144:08 - go ahead and console log the email okay
144:11 - so that's that function then we'll go
144:12 - ahead and create another function and
144:14 - we'll call this list leads and once
144:17 - again we don't necessarily need an
144:19 - argument here and I won't console log
144:21 - anything at this point and then we'll go
144:23 - ahead and do our module exports and do
144:27 - new lead equals to new
144:29 - [Music]
144:30 - lead and then
144:32 - module do exports and list leads equals
144:37 - to list leads now no surprise here to
144:41 - actually use this data back into our
144:44 - expressjs app we can go ahead and grab
144:46 - it so what I'm going to do here is I'm
144:48 - going to say const and we'll just call
144:49 - it crud and this is equal to require and
144:52 - this is going to be DB and
144:54 - crud okay so the reason for that is then
144:57 - in my leads here I'll be able to grab my
145:00 - data so let's go ahead and just say that
145:02 - our results are equal to a wait cr. list
145:07 - leads and then our results down here for
145:11 - the data itself which will be a single
145:14 - result this is going to be uh new
145:17 - lead and then we would pass in this data
145:20 - right here now my git command I need to
145:22 - change this to be an asynchronous
145:24 - callback as well and so that's the goal
145:26 - that we're working towards here
145:28 - basically just have a quick and easy way
145:30 - to add a new lead in and all that so of
145:33 - course this shouldn't be anything new
145:35 - nothing about this so far is really that
145:38 - challenging to do at this point
145:39 - hopefully if it is challenging then yeah
145:43 - this might be a little bit too advanced
145:44 - for you but overall what we want to do
145:46 - now is we want to use this schema to do
145:49 - those two things getting new leads in
145:51 - our database and listing things out now
145:53 - of course the first thing I actually
145:55 - need to do is add data in there before I
145:57 - can list any of that data out so how do
145:59 - we do this well if we jump into our
146:01 - client we have a database client but we
146:04 - don't have anything related to drizzle
146:06 - drizzle has its own database clients
146:08 - itself so if we actually go into access
146:11 - your data in query what we see here is
146:14 - this drizzle Command right here where it
146:15 - takes in a client and then it also can
146:18 - take in a schema so what we need is just
146:21 - the drizzle portion of this and
146:23 - basically we want to have a drizzle
146:25 - client itself so now what I'm going to
146:28 - do is change a new client function
146:30 - inside of my clients for my database and
146:33 - this is now going to be git drizzle DB
146:36 - client just like that roughly speaking
146:39 - and then the SQL itself is going to go
146:41 - from the other normal git DB client cuz
146:45 - we definitely need to use that one
146:47 - and all I need to do is I'm going to go
146:49 - ahead and grab in
146:51 - drizzle and it's going to be equal to
146:54 - require and drizzle
146:57 - omm and what we want within drizzle omm
147:01 - is going to be related to the database
147:04 - itself in our case the database is neon
147:07 - so we'll do neon HTTP this is coming
147:10 - directly from their docs and so we'll go
147:12 - ahead and wrap this SQL client into
147:15 - drizzle itself
147:17 - and so now we'll go ahead and Export
147:19 - that get drizzle DB client in here just
147:23 - like that great so that's the first step
147:25 - back into crud here now we're going to
147:27 - go ahead and do con client equals to
147:31 - require and this is going to be
147:35 - ourclients or we'll go ahead and say
147:37 - clients in here and now I'll do con DB
147:40 - equals to await clients. G drizzle DB
147:45 - client great so this is the drizzle DB
147:48 - portion and so what we see in the
147:50 - documentation is we now are at this part
147:53 - so we're close to doing something like
147:54 - this and the documentation in drizzle
147:57 - documentation for the querying is vest
148:01 - there's a lot that you could do in here
148:02 - so we're really just scratching the
148:04 - surface of what you can do but the idea
148:06 - is something not too dissimilar to this
148:08 - once we actually get into listing out
148:10 - our leads but before we get there I
148:13 - actually want to insert new leads and
148:15 - the way I'm going to do that is by doing
148:16 - const and schemas equals to
148:20 - require and/ schemas and so now with
148:24 - that schema what I'll do is I want to go
148:26 - ahead and grab in a result and this is
148:29 - going to be equals to await dp. insert
148:34 - where we're going to insert it in is
148:36 - related to the schema and the lead table
148:39 - all right so we're inserting it into
148:41 - that specific schema this again is from
148:44 - the documentation so if you go into the
148:45 - documentation under under accessing your
148:47 - data you can see this insert here and we
148:50 - see that it says DB insert this users
148:53 - corresponds to what we've been calling
148:55 - lead table right I just made it this way
148:58 - so it's a little bit easier for me to
148:59 - understand what's going on and to unpack
149:01 - at any time but then we see thist values
149:04 - here and that's the key of actually
149:06 - inserting the data is values and then
149:09 - what data we want to insert so in our
149:12 - case our schema only has really one
149:14 - thing that we're wanting to insert in
149:17 - here and that's the email but if we had
149:19 - other fields in here this is where we
149:21 - would do it we would actually bring in
149:22 - those other fields so what I want to do
149:24 - here is then this is going to be an
149:26 - actual object we'll pass in here that
149:28 - object we will unpack and then go ahead
149:31 - and bring it in to the values as well so
149:34 - email being email in other words once we
149:38 - have this data Maybe we're going to go
149:40 - ahead and say new lead
149:42 - object and then you would pass that in
149:45 - something like that uh but for now I'll
149:47 - just leave it very verbose so it's going
149:49 - to be the actual email itself so what's
149:53 - happening here is it's going to insert
149:54 - that data but then it's just going to be
149:56 - done so if we actually wanted to return
149:58 - data we can do returning just like that
150:01 - so that will actually return back the
150:03 - result that's come through in here and
150:05 - we can narrow this down even more if we
150:07 - wanted to to let's say for instance the
150:09 - time stamp or uh let's go ahead and do
150:12 - that we'll just go ahead and bring back
150:14 - timestamp and this is going to be the
150:16 - data based off of the schema again and
150:19 - the data that's being inserted here so
150:21 - timestamp and then dot uh let's go ahead
150:23 - and grab what the schema value is in our
150:26 - case it's created at so there we go so
150:29 - this allows me to return back that
150:31 - timestamp as some sort of result so
150:33 - we'll go ahead and return back that
150:35 - result and see what ends up happening
150:37 - there okay so now it already is in place
150:40 - in that actual location so I'm just
150:41 - going to go ahead and console well let's
150:43 - go ahead and Echo back the results I
150:45 - think I have something close to that I'm
150:47 - going to go ahead and just do results
150:49 - and results so just changing it slightly
150:52 - we're not echoing back the data but
150:54 - rather echoing back what goes into the
150:56 - database so with this in mind we want to
150:58 - actually call that that one curl command
151:01 - that we were doing just a moment ago
151:02 - with the post and then I'll go ahead and
151:04 - run that and of course it might take a
151:07 - moment for it to fully boot up and I
151:08 - actually might have to restart my server
151:10 - alt together uh so let's go ahead and
151:12 - try that once again and we'll go ahead
151:15 - and email put that email in there we get
151:17 - an empty reply from server and so if we
151:20 - come back in here I get this n column
151:22 - value okay so part of the problem that
151:24 - I'm currently facing has to do with
151:27 - validation right so is this valid data
151:30 - the other problem is how I'm actually
151:31 - passing the data into my new lead I just
151:34 - talked about it I actually turned it
151:35 - into an object and then I unpacked that
151:37 - object right here so we actually want to
151:40 - just pass in the Raw data right here for
151:42 - now and then we'll go ahead and undo
151:44 - that so now the request body goes
151:46 - directly into the new lead but we still
151:49 - need that validation of some kind so
151:52 - while this is working with this we'll go
151:54 - ahead and just run it and see if we get
151:56 - anything back from the server in this
151:58 - case I do get something back I get that
152:00 - time stamp back or at least what I've
152:02 - been calling a time stamp and notice
152:04 - that I get an actual array back and a
152:06 - list of items back so what's happening
152:08 - here is inside of this result it's
152:11 - inserting it into the database and then
152:13 - returning back a list of items so
152:15 - basically we will say then if the
152:18 - result. length is equal to let's say one
152:22 - then we're going to go ahead and return
152:23 - back the result zero right and then
152:27 - otherwise we'll go ahead and return back
152:28 - the whole list of results themselves or
152:30 - we could say null or just an empty list
152:32 - or something like that but the idea
152:34 - being that if this is a list then we'll
152:36 - just return back that first the index
152:39 - value in that list and so returning back
152:41 - a specific time step we can also return
152:43 - back so so uh new email or the actual
152:48 - inserted email and then just do
152:50 - something like this right and that will
152:52 - actually change back what we return as
152:54 - we can see we might have to restart the
152:56 - server but if I go ahead and try it
152:57 - again uh yeah let's go ahead and restart
152:59 - the server here and we'll go ahead and
153:02 - run this one more time and when we get
153:05 - back should be that email or new email
153:08 - basically as the object that's coming
153:10 - back to the you know expressjs
153:14 - application so great so we now have a
153:16 - way to insert data now in this case I
153:18 - actually don't want just one single
153:20 - thing I want all of the data the entire
153:22 - schema to come back to me and that's
153:24 - what's going to go into the front end
153:26 - but we could narrow that down if we were
153:28 - interested in doing that so now if I do
153:30 - it again let's go ahead and restart that
153:31 - server again and we'll go ahead and run
153:34 - this and what we get of course is all of
153:37 - the data including the ID in here so at
153:39 - this point it now seems that I have
153:41 - seven items in here based off of how
153:43 - many times I submitted it to the backend
153:46 - and notice that the ID is auto
153:47 - incrementing the created timestamp is
153:50 - changing as well the description has
153:52 - that default text that we had in here uh
153:55 - right there so the last thing that we
153:57 - would need to really do with this crud
153:59 - is to well build out this list leads so
154:02 - it's actually very similar to what we've
154:04 - got here except it's just using
154:06 - different database commands so when you
154:09 - think about listing things out you're
154:10 - not inserting in any data but you're
154:13 - going to be using the same schema so
154:15 - instead of inserting we're going to go
154:17 - ahead and use db. select and then
154:19 - instead of insert it will be just from
154:22 - and that's roughly all we need to do so
154:24 - I can go ahead and get rid of this
154:26 - values here this time we're just going
154:28 - to leave it as results and I'll get rid
154:30 - of this all together and so I'll go
154:32 - ahead and put that in there just like
154:34 - that okay very simple very
154:36 - straightforward and then we can also do
154:38 - something called limit we can call limit
154:40 - on here on the number that we want to
154:42 - send back and so now with this in mind
154:45 - we've got got our leads here there's our
154:47 - results I'll go ahead and give out
154:49 - results here as the results I probably
154:52 - don't need that message in there any
154:53 - longer on either one so let's go ahead
154:56 - and save that and this time I'm going to
154:58 - go ahead and just curl the one single
155:02 - leads so we'll just run curl which will
155:04 - be a get request by default I'm going to
155:07 - go ahead and restart that
155:10 - server and then I'll go ahead and run
155:13 - that curl command and what I should see
155:15 - is all of those leads coming back in the
155:18 - item of results right and then again if
155:20 - I were to submit some more I can curl it
155:24 - again and once again I'm getting more
155:26 - results so back into crud what we could
155:29 - do is a lot of different things to just
155:31 - make this a little bit more organized
155:33 - for example so I'm going to just add in
155:35 - one little quick element here and we're
155:37 - going to do uh const and this is going
155:39 - to be descending and we're going to say
155:42 - equals to require and this going to be
155:44 - drizzle or in okay so I'm going to just
155:48 - reorder this and then do just order by
155:52 - and this is a camel case here and what
155:55 - we want is the sending value of the
155:58 - schema data so we'll go ahead and grab
156:01 - this schema data
156:03 - here and then I'll grab the field I want
156:06 - it to be ordered by so in this case we
156:08 - have created at so descending value of
156:11 - created at and let's go ahead and
156:14 - restart that server again
156:16 - and then I'll go ahead and refresh this
156:19 - request and so what we should see is a
156:22 - reordering of it so ID of nine is coming
156:25 - first instead of three and so if we
156:28 - wanted to see this a little bit further
156:30 - we can also do like maybe get lead so
156:32 - git lead would be an individual item and
156:35 - maybe we're just going to grab it by the
156:36 - ID and so once again we're going to
156:38 - select that same table but now we can
156:41 - use a condition in here called where and
156:44 - within where we can actually import
156:46 - something from drizzle called EQ as an
156:48 - equal so we would say where EQ and this
156:52 - is going to be our leads
156:54 - table. ID is equal to the argument of ID
156:59 - and then we'll go ahead and Export this
157:01 - as well so get lead get lead get lead
157:04 - and these results should actually be a
157:06 - single result now similar to like what
157:09 - we saw with our new lead and once again
157:11 - I'll go ahead and grab this single
157:13 - result if it exists otherwise well go
157:16 - ahead and say null right so if it's not
157:18 - a single result then null now this
157:21 - should be a single result because of the
157:23 - ID itself and so back into our index
157:26 - what we could do is just try this out
157:27 - with Git lead and then some result that
157:30 - we are pretty sure is going to happen
157:31 - which is the ID of nine and so let's go
157:34 - ahead and restart our server and then
157:36 - we'll rerun our call our curl call and
157:40 - then this should give us that one single
157:42 - result back and sure enough it does
157:46 - right so this wear Clause is also very
157:49 - powerful because it can filter things
157:50 - down in this case it filtered it down to
157:52 - just one thing where it's equal to this
157:54 - and this if you're familiar with SQL
157:56 - this is a very SQL like command that's
157:59 - happening here and of course if we only
158:01 - wanted let's say for instance the ID in
158:03 - here we could do table. ID and put this
158:07 - in as ID and like that so this is
158:12 - selecting the ID from that value and
158:14 - then we can curl it again let's go ahead
158:16 - and restart that server again and run
158:20 - that one more time and so that would
158:21 - actually give us just that data and that
158:23 - would be true whether or not it's an
158:25 - individual item or a listing of items
158:28 - and so drizzle gives us this really
158:29 - powerful way to perform SQL without
158:32 - writing a bunch of SQL commands but
158:34 - they're SQL like so it's getting closer
158:37 - to what you would need to do to write
158:38 - the SQL commands yourself and so while
158:41 - you're learning drizzle you're also
158:42 - learning a lot about SQL without writing
158:45 - a bunch of of it and it's really just
158:47 - that straightforward so at this point I
158:50 - will challenge you to go into the
158:51 - documentation and just find how you can
158:55 - play around with this data more because
158:57 - there are so many different queries that
158:59 - you could write and then of course you
159:00 - would want to update data and delete
159:02 - data as well but with all three of these
159:05 - we have a foundation to how you might
159:08 - end up doing all of those things uh so I
159:10 - do encourage you to read more about the
159:12 - database itself but at this point it's
159:14 - actually pretty powerful where we're at
159:16 - we can definitely improve it but as far
159:18 - as an API is concerned to store data and
159:21 - more specifically email data this is it
159:24 - we can absolutely do it but we are
159:25 - leaving one critical piece out and that
159:28 - is of course this data validation so
159:30 - that's something we need to do now let's
159:31 - take a look at Zod now we're going to do
159:34 - some very basic validation using Zod now
159:37 - you can absolutely make this more robust
159:40 - by using drizzle Zod which actually
159:42 - corresponds really closely to our
159:45 - schemas that we've already built and
159:46 - then even more robust
159:51 - is now we're going to go ahead and use Z
159:53 - as a way to validate the data we have
159:56 - coming through now eventually you can
159:59 - make
160:07 - this now we're going to go ahead and
160:09 - validate our data that's coming through
160:11 - from our request so I'm actually going
160:13 - to change this data to post data in
160:15 - instead and then we'll go ahead and grab
160:17 - the data itself from somewhere else
160:20 - which will be a wait and we'll call it
160:22 - validators do let's say validate uh
160:26 - lead and then we'll pass in the post
160:29 - data there so naturally we need to bring
160:31 - these things in and actually make them
160:33 - happen so to do that we're going to go
160:34 - into our DB and we're going to create
160:36 - something called validators JS and then
160:39 - I'm going to go ahead and generate some
160:41 - sort of validation function so we'll go
160:43 - ahead and do async and we wanted to call
160:45 - call it the function of validate lead so
160:49 - I'll go ahead and do that function
160:50 - validate lead and it's going to take in
160:52 - post data and then it's going to return
160:54 - some stuff and we'll do our module
160:56 - exports and validate lead equals to
160:59 - validate lead so the things we want to
161:01 - return is for sure the data itself the
161:03 - actual validated data right that's a key
161:06 - part of this now I also want to see if
161:09 - there's an error so I'll go and say has
161:10 - error and then maybe a message now the
161:13 - reason for that is then I can change the
161:15 - respon response based off of those two
161:16 - values so in other words if the has
161:19 - error is true then we're going to go
161:21 - ahead and return one type of response
161:24 - and then if the otherwise we'll go ahead
161:26 - and say else
161:29 - if the has error is equal to undefined
161:34 - then we'll go ahead and do another
161:35 - response now if it's undefined I'll go
161:37 - ahead and just give a message of server
161:41 - error and has error the actual status
161:44 - code will be 500
161:46 - and then if it does have an error in
161:47 - general we'll just go ahead and say 400
161:49 - and I'll give it a message based off of
161:51 - the message that's going to come back or
161:54 - we'll go ahead and say something like
161:57 - invalid request please try again okay
162:01 - and then down here with the new lead
162:02 - itself assuming that that actually
162:04 - worked and we didn't have to do a tri
162:05 - Block in there uh we'll go ahead and
162:07 - change the result to being 2011 because
162:09 - that's typically what happens when you
162:11 - create data the status code will be 2011
162:14 - so with this in place we need to build
162:15 - out this validate lead function so the
162:19 - first things is are how we're going to
162:21 - respond back and that's going to be has
162:22 - error true and then message just being
162:26 - an empty string so the way we're going
162:28 - to validate this is by using a package
162:30 - called Zod now Zod itself is very
162:33 - popular and it also has a lot of other
162:37 - features to it as well as Integrations
162:39 - to drizzle and then maybe even more
162:41 - robust validation messages with this Zod
162:44 - validation error but what we're doing is
162:46 - something very straightforward we're
162:48 - just validating the email we just want
162:50 - to make sure that that's working so when
162:51 - we do insert data into our database it's
162:54 - valid data and it's not invalid data so
162:57 - to do this I'm going to go ahead and
162:59 - import or actually install Zod so we'll
163:01 - do mpm install Zod and then inside of my
163:05 - validators here we'll do const and z and
163:08 - this is going to be equal to require and
163:10 - simply Zod so what I would normally do
163:13 - is I would probably put this schema the
163:16 - actual validator and the schema together
163:18 - i' put them right next to each other
163:19 - because they're based off of each other
163:21 - but in this case I'm just going to go
163:22 - ahead and put it in this validate lead
163:24 - function and so that means I'm going to
163:25 - go ahead and say my lead object
163:29 - validation something like that or just
163:31 - simply lead equals to Z doob and then we
163:35 - can start to design or format what we
163:38 - want to validate in here and so what
163:41 - it's going to what's going to happen is
163:42 - we can put any sort of data in here we
163:45 - we can put a name we can put an email we
163:46 - can put a description and then in here
163:48 - we can put those things that correspond
163:50 - so like name email
163:53 - description we can fill these out more
163:55 - but the idea is what you put in here
163:58 - might correspond directly to what's
163:59 - inside of the lead itself so in our case
164:01 - it's just going to be the email itself
164:04 - so instead of name and description we're
164:06 - just going to use email and the way we
164:08 - validate that is by having an actual
164:10 - dictionary an actual Z object in here
164:13 - and that means z. string so we
164:15 - definitely want it to be a string in
164:16 - here so it's not going to be a number
164:17 - that's for sure and then we can use
164:19 - email as well and so using email is
164:23 - another method that's available to us
164:26 - because of Zod so if we look in Zod and
164:28 - do a quick email search in here we can
164:30 - actually see some validation that's
164:32 - going on and there are a lot more places
164:35 - where we can actually call email and you
164:37 - can actually see that in the Baseline
164:40 - validations so we can also validate the
164:42 - size of it we can make it bigger or
164:44 - smaller right we can validate the length
164:46 - of it which is similar to that uh we can
164:48 - do what it starts with what it ends with
164:50 - if we wanted a certain domain name um
164:52 - there's a lot of different things on how
164:53 - we could go about doing the validation
164:55 - itself which is beyond the scope of what
164:57 - we're trying to do here I really just
164:59 - want some simple validation and so this
165:01 - is really designing the way we're going
165:04 - to validate it then when we actually
165:06 - want to validate it we will use lead.
165:10 - part so let's go ahead and say
165:12 - valid let's do valid data equals to
165:15 - lead. parse and now what we're parsing
165:18 - is we're actually going to pass in an
165:19 - object of some kind so this needs to be
165:21 - an object which in our case it is it's
165:24 - the request post body object itself and
165:27 - of course if this is incorrect it's
165:28 - going to run a validation error itself
165:31 - so I'm going to go ahead and run catch
165:32 - and
165:33 - error so we want to put it into this
165:36 - catch statement like that and basically
165:39 - what we would do then is say let has
165:42 - error right so we'll leave it as
165:44 - undefined
165:45 - and then I'll go ahead and say has error
165:47 - equals to false and then has error
165:49 - equals to True right so if it doesn't
165:52 - have an error then we will you know set
165:55 - it accordingly so the error is only
165:57 - going to come when we try to parse out
165:59 - this data so this valid data really is
166:02 - actually going to be let valid
166:05 - data and I'm going to set this equal to
166:07 - just an empty dictionary and basically
166:10 - that's what's happening here this parse
166:13 - call is going to parse out all of the
166:16 - data that I actually set inside of here
166:18 - so if the post data had a name for
166:20 - example it's only going to validate the
166:22 - things that I declare and it will only
166:24 - return the things that I declare so that
166:27 - now that I've got this valid data here I
166:28 - can actually pass it in and then we'll
166:30 - go ahead and just do let
166:33 - message and then I'll give it a message
166:35 - of equaling to an empty string here and
166:38 - then the message being invalid data
166:42 - invalid email please try
166:46 - again okay so now we've got our
166:48 - validations and our various validation
166:51 - errors so let's go ahead and try to run
166:54 - this now so mpm runev and then I'll go
166:58 - ahead and do a curl call in here just
167:01 - like we've seen I'll go ahead and run
167:03 - this I get an empty reply from the
167:04 - server so let's go ahead and see what's
167:06 - going on my validators is not defined so
167:09 - let's go back into this page here and
167:12 - we'll go ahead and valid or actually
167:14 - bring in our
167:17 - validators the entire module let's make
167:20 - sure we exported it and it looks like we
167:22 - did so let's go ahead and try that again
167:23 - we'll rerun the server and then I'll go
167:26 - ahead and trigger a call and I get
167:28 - invalid request please try again so that
167:31 - invalid request please try again is
167:33 - coming from this right here so the
167:36 - message doesn't seem to be coming back
167:38 - from this validate lead so let's go
167:40 - ahead and see why let me bring this back
167:42 - here so we got our message and message
167:44 - ah there there it is right here I need
167:46 - to go ahead and pass in that message so
167:48 - we'll save that and let's go ahead and
167:50 - refresh that server again and then we'll
167:53 - go ahead and come back in here run it
167:55 - again and now I get invalid email please
167:58 - try again so let's go ahead and do an
167:59 - actual valid email by just putting this
168:01 - at the at sign hit enter and now what we
168:05 - should see is of course the database is
168:07 - going to have to spin up and then we see
168:10 - this so the nice thing about this
168:11 - validation is it's actually not going to
168:14 - hit our database until it's actually
168:16 - valid data which is another reason to do
168:18 - this and of course this is sort of the
168:20 - basic level of doing this kind of
168:22 - validation the more robust and more
168:24 - advanced level would be using drizzle
168:26 - omm because you can build out schemas
168:29 - based off of other schemas and what I
168:31 - would actually do from here is to modify
168:34 - my schema of validation that I've been
168:36 - doing and do something along these lines
168:38 - where insert user schema and you could
168:41 - import that in this validate lead if
168:43 - you'd like or you could just just use
168:45 - the schema directly in the view as well
168:48 - but the reason I like doing this
168:49 - validate lead here is I can actually
168:51 - have a little bit more robust methods
168:52 - but of course there's nothing stopping
168:54 - you from actually putting in the post
168:55 - data itself uh so yeah that's how we can
168:58 - actually validate this data and we do it
169:00 - using Zod feel free to make this more
169:03 - powerful and also research a little bit
169:05 - more about Zod itself on how you can
169:07 - just do that much more validation to me
169:10 - the validation is as good as you can
169:12 - actually use it as as you can actually
169:14 - implement it right so I could definitely
169:17 - spend a lot of time trying to figure out
169:19 - how to give a much better message than
169:21 - invalid email as in the validator itself
169:25 - actually will give you better validation
169:27 - errors than just invalid email right so
169:30 - I can actually see what those messages
169:32 - would be directly from the validator
169:34 - itself so let me just show you real
169:36 - quick if I restart this and we do an
169:39 - invalid data again I run that again it
169:42 - says invalid email if I go back into the
169:44 - node I can see just exactly what
169:46 - different validation errors are going so
169:49 - yeah we can absolutely make the messages
169:52 - themselves much more robust than what
169:54 - we've got here but to me it's like we
169:56 - better just validate the data and then
169:59 - provide some sort of message that makes
170:01 - sense as well so another thing that you
170:03 - could consider doing in addition to
170:05 - validating the data is you could also
170:08 - check to see if it's already exists in
170:10 - your database right if it's already
170:13 - there and the way you would do that is
170:14 - very similar to like what we were
170:15 - talking about with crud where it's like
170:17 - listing out the leads but more
170:19 - specifically looking where the email
170:22 - matches the requested email and if that
170:24 - exists then we run another sort of
170:26 - validation error which is something
170:28 - that's outside the scope of this but
170:30 - we're working that direction the key of
170:33 - all of this with whenever it comes to
170:35 - inserting data from the Internet is you
170:37 - almost always want to clean that data
170:40 - and validate it before you want to send
170:42 - it into the database the database itself
170:44 - will help with the data types whereas
170:47 - the validator itself will help us with
170:49 - these kinds of messages and prevent as
170:51 - many database server errors that they
170:54 - could possibly be because of poorly
170:56 - formed data uh so yeah validators are
170:59 - great and fairly important so yeah
171:02 - hopefully you start using them now uh
171:04 - it's pretty simple and of course it can
171:06 - be a lot more robust thanks to zods and
171:08 - its powerful tooling so let's keep going
171:11 - now we're going to go ahead and automate
171:12 - the process of staging our application
171:15 - so like the dev stage for example when I
171:17 - make some changes with the code I want
171:19 - to be able to deploy it to GitHub and
171:21 - then GitHub automate the process of
171:23 - deploying the application on AWS Lambda
171:26 - but also branching our database at that
171:28 - point in time so we can work off of that
171:31 - and the way we're going to do this is by
171:32 - of course updating our secrets this is
171:35 - the key part we actually need to update
171:37 - our secrets for that stage so if you
171:40 - think about this in seress diagel we
171:42 - have our production environment going
171:44 - through and this database URL well we
171:46 - actually don't use that anymore so we're
171:48 - going to be gone altogether with that
171:49 - one so what we need instead is really
171:52 - the stage itself and so this is going to
171:54 - be based off of an environment variable
171:57 - that we will then later set but the
171:59 - default stage is going to be prod we're
172:01 - going to use the production stage as
172:03 - default now the reason for this of
172:05 - course is going into our secrets here
172:07 - where our database URL is actually
172:09 - stored that's where that stage is being
172:11 - set right so then in my get database URL
172:15 - I can actually modify what stage is
172:17 - going through so in other words in my
172:19 - Dev stage I can add in stage of Dev and
172:22 - get rid of this database URL right here
172:25 - and get rid of it basically anywhere so
172:27 - we don't accidentally expose it when we
172:29 - don't need to so of course our AWS
172:33 - access keys have the ability to get that
172:35 - database URL that's stored in this
172:37 - parameter of course one of the things
172:39 - that you're thinking about hopefully is
172:41 - the fact that we set this parameter
172:43 - manually so we actually went into AWS to
172:46 - set it manually that isn't automated
172:49 - it's quite literally the opposite so
172:51 - what we need to do is we need to
172:52 - automate the process of not just the
172:54 - stage itself but also setting in that
172:58 - database URL now luckily for us actually
173:01 - using the neon CTL it's really easy to
173:04 - Branch the database so if we look at the
173:06 - branches that are available uh we can
173:08 - see that with neon CTL branches and list
173:13 - we can see all the branches that are
173:14 - available and right now it's just the
173:15 - main branch so if we want to create a
173:17 - new one we could do it like this create
173:20 - name and then like Dev right so that
173:22 - gives us back this connection string
173:24 - here and of course if I wanted to use
173:26 - that connection string or just the
173:28 - connection string I can use neon CTL and
173:31 - connection string and then the branch
173:34 - itself of Dev and so that's going to be
173:37 - the exact same connection string as we
173:39 - see and so this one we want to treat as
173:41 - ephemeral we want to be able to delete
173:43 - it at any time and then Branch it later
173:46 - at any time both things we want to be
173:48 - true but that presents A new challenge
173:51 - with our production application and when
173:54 - I say production I mean one that's
173:56 - running on AWS Lambda regardless of the
173:58 - current stage itself so we need to be
174:01 - able to automate the process of updating
174:04 - our parameter store and luckily for us
174:07 - it's actually not a whole lot different
174:08 - than getting data so we'll go ahead and
174:10 - do that now and let's go ahead and put
174:13 - data we'll do put
174:15 - and parameter command and that's the
174:17 - command we're going to use now it's
174:19 - almost identical to this git database
174:21 - URL so I'll go ahead and paste that over
174:24 - and copy it and so the idea then is the
174:27 - param path needs to change based off an
174:30 - argument so we'll go ahead and do the
174:32 - put database URL and there's going to be
174:34 - a stage that we'll pass in here and I'll
174:36 - just call this cons Pam stage which is
174:39 - going to be equal to the stage that's
174:40 - coming through and otherwise it's going
174:43 - to just be simply Dev what I want to
174:46 - check right away is if the Pam stage is
174:48 - equal to the prod then I'll just go
174:52 - ahead and return I don't want to change
174:55 - the pro the production stage ever right
174:58 - I don't want to change this
174:59 - automatically that's something I'm happy
175:01 - to go in manually and change if I need
175:03 - to and so this pram stage is now going
175:05 - to be what the actual path is for this
175:08 - item or really the database URL name
175:11 - which of course comes down here so we go
175:13 - based off of that for am stage and now
175:15 - we'll be able to modify it in just a
175:17 - moment so this put URL database URL we
175:20 - actually need to have the DB URL value
175:23 - as well and so that's going to be part
175:25 - of the arguments we'll end up using
175:27 - everything else is pretty much the same
175:29 - of course if we wanted to change the
175:30 - region based off of our environment
175:32 - variables we could totally do that I'm
175:34 - going to leave it in as just the default
175:35 - one that we've been using uh so because
175:38 - I'm not really going to be changing
175:39 - anything else but the idea here is the
175:41 - other parameters that we want is the
175:43 - value
175:45 - so we use value here and we'll pass in
175:47 - that database value now once again if
175:49 - that value does not exist then we'll go
175:52 - ahead and return this element here and
175:54 - basically it's incorrect it's not going
175:55 - to work um so we still want to use this
175:58 - decryption but the way we do that is
176:00 - instead of with decryption we just
176:02 - declare the type itself and this is
176:05 - going to be secure string and of course
176:07 - we need to put that in the string itself
176:10 - so the idea of secure string is of
176:12 - course coming directly back into the
176:14 - console itself when we did create a new
176:16 - parameter here we have these three type
176:19 - options here I'm just using secure
176:21 - string and all of the other default
176:23 - options in there yes there are ways to
176:25 - modify this as well so feel free to look
176:27 - at the AWS SDK documentation for that
176:31 - next up the final thing is overwrite and
176:34 - I'm going to go ahead and say true so
176:36 - basically whenever I want to put a new
176:38 - stage database URL I want to overwrite
176:41 - the value that's currently in there just
176:43 - to make sure that that it is my latest
176:45 - value of course there could be issues
176:47 - and errors with this process which is
176:50 - why we don't do it with our production
176:52 - database we only do it with everything
176:54 - that's not production everything that's
176:56 - not the main stage that we're using so
176:58 - of course the final commands are
176:59 - basically the same but instead of get
177:01 - parameter command we'll use put
177:04 - parameter command in there and then I'll
177:06 - just return the result we'll save it
177:08 - just like that so before I go any
177:10 - further I can I will tell you now we can
177:13 - also do something called delete
177:14 - parameter command where we can actually
177:17 - delete and basically undo everything we
177:19 - just did and it's a little bit closer to
177:21 - this git URL where you just basically
177:23 - use the name itself but the reason I'm
177:26 - actually not going to be deleting the
177:27 - command itself is it makes it a little
177:29 - bit more complicated but also since we
177:32 - are going to be overwriting the database
177:33 - on a regular basis it doesn't really
177:35 - matter if that's the correct connection
177:36 - string or not so I'll go ahead and just
177:39 - leave it in as just put next up we'll go
177:42 - ahead and do module and and export. put
177:45 - database URL we'll go ahead and Export
177:47 - that out so we can start the process of
177:50 - automating it now the way we're going to
177:52 - automate it this is by using a command
177:55 - line tool we're going to make our own
177:56 - command line tool so we're going to copy
177:57 - this migrator one and I'll go ahead and
177:59 - just call this put
178:01 - secret.
178:03 - JS now this is actually more like put
178:05 - database secret so maybe you want to
178:08 - change the name of it uh but I'm not
178:10 - going to change the name now first off
178:12 - the migration command we don't need this
178:14 - function anymore we already have a
178:15 - function we just created that function
178:17 - on what we're going to end up doing and
178:19 - so I'm going to go ahead and get rid of
178:20 - all of the things related to my gr now
178:22 - the main reason I actually copied that
178:25 - is to remember I definitely do need the
178:28 - EnV configuration because remember in
178:30 - order for me to use the AWS SDK itself I
178:34 - need to have the environment variables
178:36 - for the AWS secret access key and the ID
178:41 - both of those things I need in there
178:43 - which is exactly why we've got this
178:44 - command line tool going and why I like
178:47 - copying these other command line tools I
178:48 - use the same sort of stuff um and so now
178:51 - the question of course is like what are
178:53 - we going to put in here so before I do
178:55 - that what I want to do is I want to
178:57 - declare args and there's going to be
178:59 - process.
179:01 - argv and then we'll go ahead and do
179:03 - slice of two now the reason I have slice
179:06 - of two is because the way this works is
179:08 - going to be the same as the migrator but
179:09 - of course instead of it being the
179:10 - migrator it's going to be put secrets
179:12 - and then we'll go ahead and do something
179:14 - like stage and then the DB URL so this
179:18 - is going to allow me to get everything
179:20 - after like basically these two items
179:22 - here and so all I want to do then is
179:24 - just check if the rs. length is equal to
179:27 - two then we're going to go ahead and do
179:30 - process. exit one that's it so we just
179:34 - want to exit it and say something like
179:36 - along the lines of console log and usage
179:40 - is that simple enough okay so now let's
179:44 - go ahead and try and run this one I'll
179:46 - copy the relative path here or basically
179:49 - run TSX so I'm gon to go aead and clear
179:51 - this out we'll do npx and then TSX run
179:56 - all of that I'll start out with hitting
179:58 - enter and it'll say run migrations so
180:01 - this is a problem this should be not
180:04 - equal to two let's try that again we run
180:06 - it again and now it's giving me the
180:09 - usage itself so we'll go ahead and try
180:11 - Dev and then sum DB your L hit enter and
180:15 - now it says run migrations which of
180:16 - course should be you know update
180:19 - secret right something like that there
180:22 - we go so how are we going to update the
180:24 - secret well we're going to grab the
180:25 - arguments themselves and the way we do
180:28 - that is by doing con stage and DB URL
180:32 - and it's going to be based off of those
180:34 - args so this is of course is unpacking
180:36 - it based off of the position it's in
180:38 - which means that if we accidentally do
180:41 - the incorrect position we've got some
180:43 - issues here here so this might be where
180:45 - you want to check if the stage is equal
180:47 - to like Dev or something along those
180:51 - lines and then go ahead and run
180:55 - everything else I'm not going to do that
180:57 - I'm just going to assume that we can
180:58 - have any stage we want in any database
181:00 - URL we want so we'll go ahead and set
181:03 - the actual Secrets now from our new
181:07 - Secrets function so to do this I'll go
181:09 - ahead and say const Secrets or Secrets
181:12 - equals to
181:14 - require and lib and then Secrets okay
181:18 - and we'll do secrets. put database URL
181:22 - what do you know we can put in our stage
181:23 - and our DB URL here so the idea here is
181:27 - it is an asynchronous function so you
181:29 - want to do a wait but the nice thing is
181:32 - with asynchronous functions is we can
181:34 - actually treat them as promises so we
181:37 - can just do
181:38 - then and we'll go ahead and say value
181:41 - and we'll do console log secret set and
181:46 - with that value whatever that might be
181:49 - so we go ahead and do value and then put
181:52 - these into back
181:53 - ticks and then process and
181:57 - exit being zero because it's successful
182:01 - and then we'll go ahead and do catch and
182:03 - in this case it's going to be
182:05 - error and once again we'll go ahead and
182:07 - run something similar to this but now
182:11 - secret not set and the error and we'll
182:15 - go ahead and exit at one okay cool so
182:19 - now that we've got this let's go ahead
182:20 - and set a secret I'll just go ahead and
182:22 - run this value here hit enter and we've
182:26 - got update secret secret set object.
182:28 - object so what is the object that's
182:30 - actually being logged in here so we'll
182:32 - go ahead and do console log value and
182:34 - then we'll go ahead and do Secret set
182:37 - and let's just call this updating uh you
182:40 - know database URL let's try that again
182:43 - again and we run that and here's the
182:45 - value that comes back now this is the
182:48 - default result from the actual put
182:51 - database URL that's what this is this is
182:54 - right here so we can actually see that
182:56 - the status code is 200 which is
182:58 - successful right of course we could also
183:01 - then get the result we could quite
183:03 - literally use something very similar to
183:04 - what we did with this git database we
183:06 - could quite literally go and grab the
183:08 - result itself to get the actual value
183:10 - but it's not necessary really we just
183:11 - need to know that the status code is 200
183:14 - and I'll let you unpack that and handle
183:16 - those errors later um but overall we
183:19 - just want to verify that that secret is
183:21 - actually in our parameter store now so
183:23 - in this the correct region that I set it
183:25 - in I'm going to go in here and what do
183:27 - you know there it is and so I'll go
183:29 - ahead and scroll down here and debug the
183:31 - value or decrypt the value and we've got
183:33 - our sum DB URL here so it's definitely
183:36 - working it's actually putting that
183:38 - database in there and every time I run
183:40 - it it creates a brand new version of it
183:42 - so I can refresh in here and we you can
183:43 - see that version and that thing is
183:46 - actually changing so if I change it
183:48 - again and refresh in here we can see
183:51 - what that is so actually the nice thing
183:53 - about this is there is a way to get
183:56 - different versions of it as well so if
183:58 - for some reason in the future you want
183:59 - to be able to change this to different
184:01 - versions you could totally do that uh
184:03 - which is pretty cool so now we actually
184:05 - need to put the correct database in
184:08 - there right so again using the same idea
184:11 - we're going to now do our NE on CTL and
184:15 - this is going to be our Branch or rather
184:17 - connection string and then we want to
184:21 - get the branch of Dev okay so this is
184:24 - going to be that connection string the
184:26 - way I'm going to automate this is by
184:27 - putting it into a export command now if
184:30 - you're on Windows this right here is not
184:32 - going to work for you until you go into
184:34 - GitHub actions there's a way to do it
184:36 - this is just not it anyways so we're
184:38 - going to export this as our uh we'll go
184:40 - ahead and do this our database what's
184:43 - call this our stage DB URL and this is
184:47 - going to be grabbing it just like that
184:50 - okay so now I can use MPX right here Dev
184:54 - and then use dollar sign of that and
184:57 - oops not dollar sign equal dollar sign
184:59 - let's get rid of that dollar sign equal
185:00 - dollar sign and now we've got the
185:03 - updated database version six let's go
185:05 - and refresh in here and so now we should
185:07 - actually see that database URL and it's
185:09 - there so of course we just need to
185:11 - verify that that part is actually
185:13 - working and we can do that thanks to of
185:15 - course this process. env. stage and our
185:18 - EnV dev has that stage as well so let's
185:22 - go ahead and run our
185:24 - local mpm run Dev our local application
185:27 - version our offline version notice
185:29 - that's that stage is coming in there so
185:31 - one of the things we might also want to
185:33 - check maybe is on our homepage is
185:35 - putting that stage out there so let's go
185:37 - ahead and grab that argument like this
185:40 - and we'll go ahead and put this on our
185:41 - homepage just to make make sure that we
185:44 - actually are using the correct stage in
185:47 - that environment so let's go ahead and
185:48 - restart that run it again so I'll go
185:50 - ahead and open up local host and so what
185:53 - we've got is that stage of Dev so what
185:56 - we can see then is we can actually start
185:58 - adding in some data now of course inside
186:00 - of the neon console we can verify this
186:02 - as well so going into the tables we see
186:04 - that we have our main branch and our Dev
186:06 - Branch so let's go ahead and add a few
186:07 - leads in here and of course the way I'm
186:09 - going to do this is by using that curl
186:11 - command so let's go ahead and I'm going
186:13 - to copy and paste what we used in that
186:15 - last part uh but there we go I'm going
186:17 - to run this a few times and then I'll
186:19 - jump back into my neon console here and
186:24 - we should see all of those new leads and
186:27 - we do so if I go back and change the
186:29 - branch to main something like that and
186:32 - also see those new leads they are not
186:34 - there so it is now working on that
186:37 - branch which is great so of course the
186:39 - last few things that we would really
186:40 - need to do is to push this into
186:42 - production but one of the things that I
186:44 - also want to make sure that I'm doing
186:46 - when I actually run this is inside of
186:48 - package.json perhaps this is where I'm
186:50 - going to want to run that command just
186:52 - to remember it um but the thing is since
186:55 - I'm going to be automating it what I'm
186:56 - really going to be doing is using this
186:58 - inside of GitHub actions itself so this
187:00 - is going to be a GitHub actions CLI
187:03 - command that will run when we get there
187:05 - uh but overall this is fairly
187:07 - straightforward on how we can set the
187:08 - staged URL and set the branch so of
187:11 - course the last thing that we could
187:12 - really test here now is to delete that
187:15 - branch and do it all over again so we go
187:17 - ahead and do that with neon CTL branches
187:21 - and list right so let's just make sure
187:23 - it's in there I'll go ahead and do
187:24 - branches and delete then the name of Dev
187:29 - all right and maybe just
187:32 - Dev try that again there we go and so
187:36 - we've got our deleted Branch now so
187:37 - let's go ahead and create a new one neon
187:39 - CTL branches and create the name of
187:44 - Dev okay so I'm going to go ahead and
187:47 - scroll up a few times to get that
187:50 - database string again to our environment
187:52 - variables there and then I'm going to go
187:54 - ahead and use it once again like that so
187:57 - I'm putting that secret on there updated
187:59 - it's number seven now we can verify that
188:02 - it is a different branch let's go ahead
188:03 - and take a look in the parameter store
188:07 - and we'll go ahead and refresh in here
188:09 - and so I'm going to do a quick search we
188:11 - got EP soft Glade
188:13 - and we got EP soft Glade great so let's
188:16 - go ahead and refresh our node again and
188:19 - let's go ahead and add one more so if I
188:23 - remember correctly off the top of my
188:24 - head this is going to have 13 or 14
188:27 - items so this should be item
188:30 - 13 and we see that with ID of 13 we do
188:33 - it one more time we got the ID of 14 and
188:35 - so now that same exact branch that we
188:38 - originally tested on is gone and if we
188:41 - look at the new Dev Branch we we can see
188:43 - that it's down to 14 and of course the
188:45 - time and all of that has changed great
188:48 - this is awesome so the next part of this
188:50 - of course is now turning this into a
188:53 - GitHub action that it does all of that
188:55 - for us so we don't actually have to
188:56 - touch it anymore uh so let's go ahead
188:58 - and take a look at how we're going to do
188:59 - that now now we're going to go ahead and
189:01 - use GitHub actions to automate our
189:03 - deployment stages first we'll automate
189:06 - the production deployment and see how
189:08 - that works then we're going to go a
189:10 - little bit more advanced and use a
189:13 - different git Branch to deploy a
189:15 - different stage the dev stage itself
189:18 - once it's deployed then we will go ahead
189:20 - and submit a poll request to the
189:22 - production Branch to have that run then
189:24 - at that point so I actually already
189:27 - started with some of the workflows
189:29 - themselves so if you were interested in
189:32 - going from what we have now just go into
189:33 - the workflows on GitHub and we're going
189:35 - to go through these things right now so
189:38 - the idea behind GitHub actions is it's a
189:40 - serverless way to just run some code the
189:43 - code we run is really up to what we
189:45 - Define in these GitHub actions so in our
189:47 - case what's happening in this production
189:49 - one is it's going to go ahead and grab a
189:50 - copy of our code it's going to set up
189:52 - no. JS and specifically version TW 20
189:55 - and then it's going to go ahead and run
189:56 - our mpm install and then run our mpm run
189:58 - deploy which of course is this Command
190:01 - right here and so that's it that's the
190:03 - production stage itself now one of the
190:05 - dependencies this production stage has
190:06 - is the actual environment variables from
190:09 - AWS so let's go ahead and start this off
190:12 - right now let's go ahead and put that
190:13 - into our GitHub actions repo so inside
190:16 - of our settings here there's a couple
190:17 - things that we want to do the very first
190:19 - thing is under GitHub actions we're
190:20 - going to go into General we want to go
190:22 - ahead and scroll down and allow for
190:25 - workflow permissions to read and write
190:26 - permissions as well as allow GitHub
190:29 - actions to create and improve poll
190:31 - requests those things are important for
190:33 - what we'll do in the staging process the
190:35 - next thing is we want to scroll to the
190:37 - secrets and variables in the GitHub
190:40 - actions um section and then we'll go
190:42 - ahead ahead and scroll down to
190:44 - repository Secrets this is where we're
190:45 - going to put in our environment
190:47 - variables okay so what we want to do is
190:49 - these two in variables here so the first
190:52 - one is going to be the access key ID I
190:54 - actually want to create a brand new
190:56 - access key ID specifically for GitHub
190:58 - actions so inside of I am I'm going to
191:01 - navigate to my user here into security
191:03 - credentials go into access Keys create a
191:05 - brand new one we'll go ahead and say
191:07 - other next up we'll go a and give it a
191:10 - actual description this time so GitHub
191:12 - actions
191:13 - secret basically or GitHub actions
191:15 - workflow basically and we'll go ahead
191:17 - and hit create we're going to grab that
191:19 - access key there and we're going to
191:20 - bring it into our GitHub action secret
191:23 - we'll go ahead and create that one next
191:25 - we're going to go ahead and create the
191:27 - secret access key itself and we'll
191:29 - scroll down and paste this in here and
191:32 - then I'll go ahead and copy this secret
191:34 - access key and paste that just like that
191:36 - okay great so now our GitHub action
191:38 - should have everything necessary to
191:40 - deploy our production application so
191:42 - I'll go ahead and run this workflow now
191:45 - the nice thing about these workflows is
191:47 - they are fast and efficient and they
191:50 - have really good logs to figure out
191:52 - what's going on and what might be wrong
191:54 - with them so one of the things that
191:56 - would have been wrong is if I didn't
191:57 - actually fill out these secrets now
192:00 - something that's cool about how the
192:01 - environment variables work in the
192:03 - serverless application is that it will
192:06 - use the AWS access key and secret key
192:09 - instead of using EnV so that's actually
192:12 - a really nice feature of the seress
192:14 - framework so the next thing I actually
192:15 - want to do is I want to put the stage
192:17 - here and I'm going to go ahead and say
192:18 - it's prod now the reason for that is
192:20 - because in my Dev stage I also want that
192:22 - to be Dev which I already have in there
192:25 - but I just want to make sure that I do
192:26 - have that in the production one that
192:28 - also means that since I'm declaring the
192:30 - stage in more places I want to make sure
192:32 - that when I'm grabbing my database
192:34 - secret that stage has an environment
192:36 - variable one right so it's certainly
192:38 - possible that you're just like oh I'm
192:39 - just going to use the production one but
192:41 - hopefully you didn't do that based off
192:42 - of a a number of things that we did with
192:44 - the last part of actually setting and
192:46 - automating the secrets themselves so
192:48 - with that stage in mind I also want to
192:50 - update serverless.yml and I want to
192:53 - change the I Ro here to also match that
192:56 - stage because originally it was just for
192:58 - the production stage we only had one
192:59 - stage really creating a role now we're
193:01 - going to have multiple stages creating
193:03 - roles so we just want to update that as
193:05 - well so we're going to save everything
193:07 - and then in my GitHub I'll go and update
193:10 - these and we'll go ahead and say prepare
193:12 - for stages and we'll hit commit and we
193:15 - can sync those changes at this time okay
193:19 - so back in GitHub notice that my
193:20 - deployment actually did happen deploy
193:23 - our production app and it happened in 43
193:25 - seconds we can click on this deploy
193:27 - section here and in here we actually see
193:29 - our endpoint come through so I can open
193:31 - that up and sure enough this should
193:33 - actually work for me and then if I go
193:34 - into leads I think I just get one lead
193:36 - back based off of some of the
193:38 - configuration we set up which of course
193:40 - we could verify in our code itself for
193:43 - leads the actual default page we
193:45 - actually have G lead in here instead of
193:47 - get leads okay so now that with this
193:50 - change it's a minor change that maybe I
193:52 - want to be in Dev first in to the dev
193:55 - stage first so what we want to do is in
193:58 - our prod stage yaml I want to change
194:00 - this a little bit to being based off of
194:02 - poll requests so in other words my Dev
194:05 - stage workflow is going to have to
194:06 - generate that poll request so that's
194:09 - going to take a number of steps and it
194:10 - gets a little bit more complicated than
194:12 - what we just did so if you want things
194:14 - really simple and you just want to run
194:16 - just straight to production which I
194:17 - don't advise but you definitely could
194:19 - you could automate this with a push
194:23 - request and basically change this to the
194:26 - branch that you're going to be using for
194:27 - your main branch or your primary branch
194:29 - on GitHub which would just be Main and
194:31 - then if you had this active it would
194:34 - automatically run that workflow that we
194:36 - just did workflow dispatch okay so I'm
194:39 - going to go ahead and just leave only
194:40 - workflow dispatch for now and work on
194:42 - the dev stage and really figure out how
194:45 - what we're going to be doing here now we
194:47 - already have a few of the environment
194:48 - variables in place we do not have the
194:50 - neon API key which we'll need in a
194:52 - moment we have our stage here we also
194:55 - already have a built-in GitHub token
194:57 - this is provided to us by GitHub actions
195:00 - it actually we don't need to set it we
195:02 - don't need to find it anywhere which is
195:04 - really nice for later when we use the uh
195:06 - GitHub CLI which actually comes in by
195:10 - default which we'll see and moment as
195:13 - well so again we'll we actually check
195:15 - out the code itself we grab all versions
195:18 - of the code mostly for the poll request
195:20 - a uh action that we'll do very soon once
195:23 - again we set up node.js we do the
195:25 - installations the default project
195:27 - installations then I want to make sure
195:29 - that I have the neon command line tool
195:30 - and TSX this of course is so that we can
195:33 - automate creating the branches like
195:35 - we've seen before as well as actually
195:37 - getting the connection string and then
195:39 - outputting that into our AWS sec secrets
195:42 - that parameter store so all of this
195:44 - stuff is mostly things that we've
195:46 - already seen before something that's a
195:47 - little bit new now is this API key call
195:50 - so basically anywhere we need to run a
195:53 - neon CTL command inside of GitHub
195:55 - actions workflow we just pass in an API
195:58 - key in there and that's how we're able
195:59 - to do all of these things so how I
196:02 - designed this workflow was to delete the
196:04 - previous Dev Branch so that I can take a
196:07 - point in time version of the branch or
196:09 - of the actual database itself to
196:12 - actually create a new version of the dev
196:14 - Branch at that time when I run this
196:16 - workflow so that then my staged Dev
196:20 - deployment actually has a new Branch
196:23 - from the database itself it actually is
196:25 - working off a new instance of the
196:27 - database having more of that data that's
196:29 - been stored from the production Branch
196:31 - now of course that's optional you don't
196:33 - have to do that I just want to show you
196:34 - how it's done next of course is getting
196:36 - the actual connection string itself from
196:38 - that branch and then putting that into
196:40 - our secrets and then finally we're going
196:42 - to go ahead and grab info about the
196:44 - branch itself so this is the one I want
196:46 - to run so in order for this to work
196:48 - correctly we need to bring in the neon
196:50 - API key so let's goe and do that now I'm
196:52 - going to go into my production app here
196:54 - we'll go into settings I'm going to go
196:56 - down into those Secrets again and we're
196:58 - going to go and create a new repository
197:00 - secret for the neon API key now where we
197:03 - find this is in the neon console we go
197:06 - into our user account go into the
197:08 - profile go into developer settings
197:11 - generate a new key here and I'll call
197:13 - this GitHub actions and then we'll go
197:15 - ahead and grab this and bring it into
197:18 - GitHub actions now do be aware that this
197:20 - key right here has access to our neon
197:22 - project so it could quite literally
197:24 - delete all of our databases which might
197:26 - be something you want to have happened
197:28 - but it also might not want you also
197:29 - might not want that but just keep that
197:31 - in mind when you do actually use that
197:32 - API key itself okay so with this in mind
197:35 - now we're going to go ahead and run that
197:38 - GitHub action so I want to bring that
197:39 - Dev stage in here so we'll go ahead and
197:41 - do get status
197:42 - and I'm just going to go ahead and say
197:45 - updated all for Dev we're going to
197:48 - commit it and we'll go ahead and sync
197:51 - those changes and then back into GI up
197:55 - actions I'll go ahead and deploy this
197:56 - Dev stage now so the key thing that I've
197:59 - been doing so far is actually just
198:01 - working on the single branch of main
198:04 - here so what we actually want to do is
198:06 - work on a different branch we want to
198:08 - work on a Dev Branch specifically with
198:11 - Git the dev Branch will will correlate
198:13 - to the dev application the dev stage of
198:16 - the application as well as the dev
198:18 - branch in neon so it's going to be Dev
198:21 - all across the board just to keep things
198:23 - consistent and so I know exactly what's
198:25 - going on now the way we do this locally
198:28 - is by doing get checkout dasb as in
198:32 - Branch Dev and this switches us over to
198:35 - the dev Branch so all of the changes we
198:37 - do in here will only go into this de
198:40 - Branch until we merge it into the other
198:43 - Branch itself so that's into the main
198:45 - branch that is so for example if I
198:47 - change this right here I just change
198:49 - that one single push on this workflow
198:51 - we'll go and say updated Dev
198:54 - Branch go ahead and Commit
198:57 - This and I'll go ahead and P publish
198:59 - this branch and push it into GitHub so
199:02 - now back in GitHub if I refresh in here
199:05 - um what I should see is something
199:07 - related to my Dev Branch so I have this
199:09 - Branch here and I have the ability to
199:12 - now bring in and merge these two
199:14 - branches together right so I can
199:16 - actually merge the dev Branch with the
199:19 - main branch itself so it's one commit
199:21 - ahead as we see here by just changing it
199:23 - we can go ahead and create a manual poll
199:25 - request and go ahead and hit create poll
199:27 - request here and then now I've got a
199:29 - poll request right so in my case it says
199:31 - 11 just because I did a bunch of testing
199:34 - yours hasn't or yours probably will say
199:36 - one maybe and then we can just go ahead
199:38 - and merge this and go ahead and confirm
199:40 - that merge and then we can also delete
199:42 - that Branch if we wanted to and so all
199:45 - of that did was now in our main branch
199:47 - we still only have one branch in here
199:49 - and then in my workflow we can double
199:51 - check in this Dev stage here that that
199:53 - Branch was actually pushed it actually
199:55 - did something in here so it actually did
199:57 - make a change into that workflow so
199:59 - let's go ahead and actually do this once
200:01 - again and still on the dev branch and
200:03 - still doing it manually before we
200:05 - automate it so the way we're going to do
200:07 - this is by going into
200:09 - index.js this time I'm going to go ahead
200:10 - and get rid of this homepage P message
200:13 - right I just want the Delta and that's
200:14 - it so I'll go ahead and save that and
200:16 - we'll do get status get add get commit
200:21 - updated
200:22 - homepage and then get
200:24 - push okay so once again it pushes it and
200:27 - then we can go right here and click to
200:30 - create a new poll request and do that
200:32 - right here now before I actually create
200:35 - that poll request I want to modify my
200:37 - production one just a little bit so back
200:39 - into the production workflow here I've
200:41 - got this poll request and we can do this
200:43 - types of closed so when the poll request
200:46 - is closed it triggers a GitHub event for
200:49 - that poll request and we can verify if
200:52 - it's merged here so if it is merged then
200:55 - we could run all of the production
200:57 - deployment stuff so we'll save that and
201:01 - now we'll go ahead and do get status get
201:03 - add Dall get commit
201:06 - prepare for
201:09 - auto deploy via
201:12 - merged PR go ahead and
201:15 - push okay so now I refreshed it
201:19 - everything should be up to par and I'm
201:21 - going to go ahead and now create this
201:22 - poll request I could have created the
201:24 - poll request before but now I have it no
201:27 - descriptions in here yet but this poll
201:29 - request now when I actually merge it
201:31 - what should happen is it should actually
201:33 - start a deployment process but since I
201:35 - just pushed some code into the dev stage
201:37 - itself I might want to actually wait for
201:40 - this to finish before I run anything
201:42 - else with serverless or maybe not I
201:44 - don't know let's go ahead and give it a
201:45 - shot I'm going to go ahead and merge
201:48 - this poll request and delete the branch
201:50 - so we goe merge confirm merge and delete
201:54 - branch and it's deleted going back into
201:56 - our GI up actions now we've got this
201:58 - deploy production app it's starting to
202:01 - deploy it based off of that merged poll
202:03 - request cool so if we wanted to automate
202:07 - this it's just one more step and it's a
202:10 - sort of major step inside of the Dev
202:12 - stage but not really that major and all
202:14 - it is is taking this Branch information
202:17 - here and then using the GitHub actions
202:20 - PR create command which is the GitHub
202:22 - command line tool to actually create a
202:24 - poll request for US based off of this
202:27 - actual branch and the branch that we're
202:29 - going to so what we want to do then is
202:32 - actually run the command here so I'm
202:34 - going to go ahead and say title and
202:36 - we'll give the title of automated PR
202:39 - from
202:40 - Dev right or really from Dev
202:43 - stage okay so that's the title we'll
202:45 - give it and then I'll go ahead and pass
202:47 - in a body and coming soon for
202:52 - now and then I'll go ahead and pass in
202:55 - the actual uh base branch which is going
202:57 - to be our default
203:01 - branch and then the pr branch is going
203:04 - to be the next one and that's going to
203:06 - be head
203:09 - branch and there we go
203:12 - and then we can also do our GitHub repo
203:15 - in here and just say repo and the
203:17 - repository okay so there we go we've got
203:20 - our branches that we're going to do here
203:23 - and let's go ahead and see how this ends
203:24 - up working once again I'm still in that
203:26 - Dev Branch here so I'll go ahead and do
203:28 - get status get add get commit and we'll
203:31 - do updated Dev workflow for auto
203:36 - PRS get
203:38 - push okay and that's going to take a
203:40 - moment for that to finish so going back
203:42 - into get up actions we see that the
203:45 - production app did was successful
203:47 - because it's really not that different
203:49 - than what we've been doing the poll
203:51 - request itself I should have none in
203:53 - here if I have any Dev poll requests in
203:55 - here this GitHub action will fail you
203:58 - can't have more than one for any given
204:00 - Branch now there are more advanced ways
204:02 - to have multiple branches and stuff like
204:04 - that um I'm not doing those Advanced
204:06 - ways I'm just going to be doing this
204:07 - really hopefully simple and
204:09 - straightforward one okay and so we've
204:11 - got this brand info here which I need to
204:12 - change the name of it but it does do the
204:14 - pr looks like everything was successful
204:17 - itself and then we can see the branch is
204:19 - the pr branch is the dev and then the
204:22 - main branch is right there for the
204:23 - default Branch great so now we can go
204:25 - into our poll requests here and there it
204:27 - is our automated PR from the dev stage
204:30 - the actual body itself needs to change
204:32 - we'll change that in a moment but
204:33 - overall now I can actually merge this
204:35 - data and again I can delete the branch
204:38 - if I want to it's really up to you on if
204:40 - you keep the branch or not um because
204:42 - locally I can still work on that branch
204:44 - and then push it again so the last part
204:46 - of this is really going to be do our uh
204:49 - poll
204:51 - request so it's going to be our Dev
204:53 - stage poll request and what we want in
204:57 - the body data itself instead of coming
204:59 - soon we want to actually get the info
205:02 - from our deployment right so the
205:05 - deployment info itself uh which is going
205:07 - to be in here so we need to update that
205:10 - as well and so let's see our Dev stage
205:13 - looks like our Dev stage has been
205:14 - running our deployment in here as well
205:17 - no our Dev stage is actually missing the
205:19 - deployment and the info so that was a
205:22 - key part that is missing but let's go
205:24 - back into package.json and make the
205:26 - deployment happen for our Dev stage as
205:27 - well but overall the GitHub action stuff
205:31 - uh is working correctly it's just there
205:33 - was one piece that was missing which is
205:35 - deploy Dev stage and then of course this
205:37 - is going to change it to just simply Dev
205:40 - and all that next up is we're going to
205:42 - go ahead and get info Dev stage and info
205:46 - now the reason for this is going back
205:47 - into the dev stage itself we'll go ahead
205:49 - and just grab one more name here and
205:51 - we'll go ahead and do deploy Dev stage
205:54 - and then we'll go ahead and run the you
205:56 - know mpm run deploy Dev stage and then
206:01 - that's going to take as long as it takes
206:03 - and then down here we're going to go
206:04 - ahead and get export the dev
206:08 - stage info equals to the dollar sign of
206:13 - mpm repo and now info Dev stage and then
206:17 - that is what's going to be inside of
206:18 - this body right here we're going to go
206:20 - ahead and pass that in just like that
206:22 - now we've got a new body stage and let's
206:24 - go ahead and try this out with get
206:26 - status get add-- all get commit and
206:30 - updated put the message in here updated
206:33 - for full Dev
206:35 - deployment stage get
206:39 - push and of course we'll let that go
206:42 - through I need to make sure that my poll
206:43 - request is empty looks like it is of
206:46 - course if I were to cancel any of the
206:48 - poll requests in here or deny them the
206:50 - production stage won't run it will only
206:52 - run if they are merged and that's it
206:55 - great so there's obviously a lot of
206:57 - options on how you could actually go
206:58 - about running those things as well now
207:01 - I'll just go ahead and let this finish
207:02 - so we can just kind of polish off this
207:04 - GitHub actions workflow and there we go
207:06 - the workflow looks to be successful so
207:08 - let's go into our poll requests here now
207:10 - we can see the automated poll request
207:12 - from the dev stage and what do you know
207:14 - there is the endpoint URL so so much of
207:17 - this poll request was reliant on this so
207:20 - that somebody could then go in and
207:22 - review some of those changes right so we
207:24 - would actually a be able to open up that
207:26 - page into like a new tab and review
207:29 - what's going on I made something change
207:31 - to the leads and I get this internal
207:33 - server error so clearly that's something
207:35 - I do not want to actually have go into
207:37 - production so then let's go back into
207:39 - our local environment here and see
207:42 - what's going on with that problem and
207:45 - then we'll go ahead and just change and
207:47 - see how we might fix it so of course the
207:50 - leads itself is having a problem get
207:52 - leads is simply not a function so let's
207:55 - go back into our actual uh DB crud and
208:00 - it's list leads there it is okay so we
208:03 - found the error itself go back into our
208:05 - index page fix this error in our Dev
208:08 - stage still get status get add get
208:12 - commit updated to list leads and then
208:16 - get push and then again you know that's
208:18 - going to go ahead and run through and
208:20 - finish and so this is why we actually
208:22 - have those PRS in place is to just do
208:25 - some checks to make sure everything is
208:26 - good in this case we do not want this
208:29 - we're just going to close this poll
208:30 - request uh and we will delete that
208:33 - Branch as well and that's it so we
208:35 - actually do not want that one of course
208:36 - we could put comments and stuff like
208:38 - that uh but once you close it what will
208:40 - not happen
208:41 - is the deployment actually running right
208:43 - so this won't build it it won't go
208:45 - through and the reason of course is it
208:47 - was skipped because of the way we set it
208:50 - up and that nothing merged nothing
208:52 - changed and so that is also showing that
208:55 - this is working correctly as well so
208:58 - that's actually pretty exciting to me I
209:00 - think is we can now see the dev stage in
209:03 - its full capacity we can do all sorts of
209:06 - tests there um in the sense that I could
209:09 - even share this with developers and
209:10 - stuff like that to they go through the
209:12 - different tests that they might need for
209:14 - this particular API and so yeah granted
209:16 - this is beyond the scope of really
209:18 - building out a expressjs application
209:21 - because the GitHub action itself could
209:22 - be used in many different places but the
209:25 - thing that's really nice is our Dev
209:27 - stage itself has a lot of features built
209:30 - into it that are really only possible by
209:32 - using neon's database service and as
209:35 - well as using AWS Lambda and of course
209:37 - is because when I'm not using the
209:39 - database or the aw with Lambda it's not
209:42 - charging me it's practically zero cost
209:45 - to be able to have this deployment stage
209:47 - or this Dev stage that I can then use
209:49 - when I need it to use and then that's
209:51 - when the cost will start to incur but
209:53 - only for a few minutes or so of course I
209:55 - get this little error here and I think
209:57 - it's related to actually deleting out
209:59 - that Branch uh too far in advance so I'm
210:01 - going to go ahead and just make another
210:02 - little change I'll just add a line here
210:04 - and do get status get add Dall get
210:08 - commit and minor fix and then I'll go
210:12 - ahead and push this just to verify that
210:14 - the actual GitHub action is working
210:17 - before we call this part all said and
210:18 - done and there we are with that minor
210:20 - fix that should be in there so I'm going
210:22 - to go ahead and now merge this poll
210:23 - request and again we can test this out
210:26 - as we see fit and I'll go ahead and run
210:28 - this and confirm that merge and I'm
210:31 - going to leave the branch now you could
210:33 - absolutely delete it if you needed to um
210:36 - but I think one of the problems that we
210:37 - saw with that previous minor fix or at
210:40 - least the up updated it to list leads
210:42 - the problem was that well when I went to
210:44 - merge it um I think it deleted it during
210:47 - the GitHub action workflow running which
210:49 - caused that issue um but now we've got a
210:52 - brand new one where it's going to go
210:53 - ahead and go into production and all
210:55 - that now the other thing about this is
210:57 - the actual production workflow does not
211:00 - have to actually go into using servus at
211:02 - all we can now pick wherever we want is
211:05 - far as when this thing gets merged and
211:07 - all that and of course the actual source
211:08 - code itself is going to change based off
211:11 - of that being merged and there's our
211:12 - list leads in there like that and also
211:14 - that just little extra space in there to
211:16 - make sure that that is all working as
211:18 - well uh so it's fairly straightforward
211:20 - on how we go about going from here uh
211:23 - the most important part of all of this
211:25 - was making sure that we could automate
211:27 - the dev stage itself so now at any time
211:30 - when I need to make a change to my code
211:31 - I work in that Dev branch and then you
211:33 - know commit it and do all that let the
211:35 - dev stage actually do its thing so then
211:38 - my main branch can actually work based
211:40 - off of that when whenever I need to so I
211:41 - basically won't need to work off of the
211:43 - main branch any longer unless there's
211:45 - some serious reason that I really need
211:47 - to but of course if I were working
211:49 - through this Dev branch and running
211:51 - through this Dev stage I should be
211:53 - pretty good from here on out yeah GitHub
211:55 - actions is pretty great for a lot of
211:57 - reasons let's take a look at another
211:59 - place to deploy this application than
212:02 - just AWS Lambda but still working in
212:04 - everything that we've been working
212:06 - with now as you may know expressjs was
212:08 - not really designed for the serverless
212:10 - world but the reason we can actually run
212:12 - it in servus is number one it's a
212:14 - minimal framework there's not a lot of
212:16 - overhead to run Express itself number
212:18 - two is that we have servess environments
212:20 - out there that we can run it on like AWS
212:23 - Lambda it has no JS and it can actually
212:25 - run Express for us but almost as
212:27 - important if not more important is
212:29 - actually our database so as soon as
212:32 - expressjs needs a database something
212:34 - like neon in servess post CR is how
212:37 - we're actually able to run and execute
212:39 - all of these things effectively without
212:41 - overloading our database and being ready
212:43 - to scale up if we need to now of course
212:45 - a better option for Ser list is probably
212:48 - nexts nextjs was designed for front-end
212:51 - and backend operations those backend
212:53 - operations are designed to be running on
212:56 - serverless they're like small little
212:58 - functions that can just run on
212:59 - serverless and it's really really fast
213:01 - but the thing is you actually have to
213:03 - learn react to really get the most out
213:05 - of xjs and maybe you're not ready to do
213:06 - that yet but what we will do is we're
213:09 - going to go ahead and integrate every
213:10 - everything we just did with our project
213:12 - and bring it right into reactor where it
213:15 - feels very native to our nextjs projects
213:18 - if you start building them out and this
213:19 - is thanks to versell of course versell
213:21 - manages nextjs but also they made it
213:24 - really simple to have some rewriting of
213:26 - rules for our API routes as we'll see in
213:29 - just a moment now generally speaking
213:31 - expresss is not going to be deployed
213:33 - directly to versell but I want to show
213:35 - you a way to do that as well and that's
213:37 - something we'll do in a moment but first
213:39 - let's go ahead and see how we can
213:41 - actually integrate nextjs with our
213:44 - entire ecosystem that we've already
213:46 - created in our serverless expressjs
213:48 - application so I actually created a repo
213:50 - specifically for this one and the only
213:53 - thing that you need to change for this
213:54 - repo is in versel Json this rewrites
213:58 - here that's all we need to change to
214:00 - make all of this work and more
214:02 - specifically the actual destination like
214:04 - we've got here now you can simply clone
214:07 - this project or you could just go to
214:08 - your local computer run n PX create next
214:12 - app at latest run through all the
214:15 - default options and then you just want
214:17 - to bring in this versell Json here and
214:19 - so what we want to do is we want to have
214:21 - these rewrites in here because of how
214:24 - nextjs handles API calls so inside of
214:27 - nextjs you can actually add in an a
214:29 - place to run backend API calls which is
214:33 - why we're actually doing this rewrite so
214:35 - basically we can treat our backend that
214:37 - we already have in in existence as one
214:40 - of the routes that you use for the back
214:42 - end of your nextjs application so that's
214:45 - what we want to implement now to do this
214:47 - I want to jump back into my serverless
214:49 - node.js API application so in here I'll
214:52 - go ahead and grab that with my
214:53 - serverless and info and then we'll go
214:56 - ahead and grab the you know stage being
214:59 - pra and then the region being the region
215:02 - we've been using which is Us East 2 hit
215:05 - enter and this will give me the actual
215:07 - endpoint I want to use so go ahead and
215:10 - cop Cy this endpoint and we'll paste it
215:12 - in just like that okay so I'm going to
215:15 - leave it as is and then what I want to
215:17 - do also is I want to make sure that
215:19 - versel is installed with mpm install DG
215:23 - versell at latest the reason for that is
215:26 - so we can actually use versel to well
215:30 - run these rewrites traditionally nextjs
215:33 - does not need to run those rewrites it
215:35 - won't actually do it by default so using
215:37 - versell will allow for that to happen so
215:39 - we'll go ahead and just call cell to
215:41 - start out it's going to ask us to set up
215:43 - a project I'm going to select all of the
215:45 - defaults in here I don't need to set up
215:47 - an existing project I'm going to use the
215:50 - you know directory's name I'm going to
215:52 - keep it in the local directory itself
215:54 - I'm not going to modify any settings and
215:56 - now it's going to go ahead and push this
215:57 - into versell itself and maybe even make
216:00 - a production version of this so we can
216:02 - verify this by going into our versell
216:04 - account which of course I'm in a free
216:06 - account here there is my application
216:09 - right and so we of course want to have a
216:11 - production build um and we'll do that in
216:14 - just a moment but before I do that I'm
216:16 - going to go ahead and create a new item
216:18 - here and we'll just call it versell and
216:20 - Dev so this is the versell development
216:22 - environment that's running our nextjs
216:24 - application with a port value which in
216:27 - this case is giving us Local Host 3,000
216:30 - now my other application that we've been
216:31 - working on is not running at all right
216:34 - so that's another part of this it's not
216:35 - running through local it's going to
216:37 - quite literally go to our production
216:39 - endpoint here so I'll go ahe and open up
216:41 - Local Host now and I'll go into SL API
216:46 - leads what I get is not found but this
216:48 - is coming directly from expressjs so
216:52 - quite literally is calling expressjs our
216:55 - actual endpoint directly from a versell
216:58 - managed API right so this this actual
217:01 - inpoint is from versell so just having
217:05 - this alone can allow me to create a
217:07 - custom domain so right in versell I
217:09 - could just add in a customer domain
217:11 - right now and that will allow me to have
217:13 - this/ API leads so in other words if I
217:16 - wanted my example.com
217:19 - SL API leads I could quite literally
217:21 - just deploy it as is now of course I
217:24 - need to update my actual API itself to
217:26 - manage things a little bit differently
217:28 - which we'll see in a moment um but the
217:29 - idea is using versell with this rewrite
217:32 - is just a a quick and easy way to sort
217:35 - of do a proxy service itself uh which
217:38 - gives us another Advantage as well okay
217:41 - so going back into API and SL leads I
217:45 - actually want to not have it not found
217:47 - but actually go to those leads
217:48 - themselves so as it stands right now the
217:51 - API is at slash leads that's that's how
217:53 - we designed it we didn't create a path
217:54 - for API so we can actually change that
217:57 - destination just like that we can run it
217:59 - again and then I'll refresh in here and
218:01 - now it actually should do an API request
218:03 - to my actual endpoint on AWS Lambda with
218:06 - that neon database and getting back that
218:08 - neon data and of course I can also post
218:10 - dat as well but overall What's Happening
218:12 - Here is it's actually now treating this
218:15 - as just another path an API path in
218:18 - nextjs so if you're familiar with nextjs
218:20 - you'll know that typically speaking the
218:22 - API itself is going to be that backend
218:24 - part of things so I'm not going to go
218:26 - into the nextjs stuff of it just this
218:28 - right here that part is great so we can
218:30 - also test this so inside of page.js I'm
218:33 - going to go ahead and do a quick little
218:35 - test for this page.js is just the
218:37 - homepage of our nextjs application
218:41 - and it's all in react so if you're not
218:43 - familiar with react this might be a
218:44 - little confusing so just bear with me if
218:47 - it is I'm going to go ahead and get rid
218:48 - of all of the stuff in between main save
218:51 - that and I'll refresh in here notice
218:53 - everything's gone if I put H1 of hello
218:55 - world save that refresh in here and now
218:58 - it says hello world great so all I
219:00 - really want to do here is have a button
219:02 - of some kind that's going to emulate you
219:04 - know press me that's going to emulate
219:07 - actually calling our API there's that
219:09 - button right right there great okay so
219:12 - this is all Tailwind CSS classes in here
219:14 - so I can do BG green like 500 or 400 uh
219:19 - to actually have a background again
219:20 - that's another thing that's outside the
219:21 - scope of this but nextjs has a lot of
219:23 - features that come into it by default
219:26 - which can make it a little overwhelming
219:27 - to start out with but the idea here is
219:29 - we want to actually press this and
219:31 - actually do an API call that's really
219:32 - what I want to see and to make this
219:34 - happen inside of next we're going to go
219:35 - ahead and get rid of this import here
219:37 - and I'm going to go ahead and pass in
219:38 - the string of use CL client at the top
219:41 - to basically treat this page as a
219:43 - front-end page then I'm going to go
219:45 - ahead and bring in something from react
219:47 - itself we go ahead and import use State
219:51 - and this is going to be from react
219:53 - itself and then I'll go ahead and do
219:55 - const and new or let's go ahead and just
219:58 - say data and then set data equals to use
220:01 - State and there going to be just an
220:04 - empty string for the moment and then I
220:06 - want to actually have a click event in
220:08 - here so I'll go and do const handle
220:11 - click which is an event and we'll go
220:14 - ahead and console log that event and so
220:17 - we can add in a click Handler on the
220:19 - button itself so we'll do onclick and
220:22 - that's equal to that function itself now
220:24 - this is straight up react if you don't
220:26 - know this stuff it's okay I just wanted
220:28 - to do one simple thing with the fetch
220:31 - call on this onclick here and then the
220:33 - data itself we're going to go ahead and
220:35 - just say if the data exists we'll go
220:37 - ahead and do json. stringify of that
220:40 - data we really just want to see that it
220:41 - is working as we see fit so the way
220:44 - we're going to do that is by doing Fetch
220:47 - and then we want to fetch to/ API leads
220:50 - what do you know it's going to the
220:51 - rewrite itself I do not have to put that
220:54 - entire URL in there just the destination
220:57 - can be left inside of R sell. Json so
221:00 - what that does is it allows us to treat
221:03 - this like you normally would in a
221:05 - versell application um so or an xjs
221:08 - application in versel I don't really
221:10 - have to think about how to actually
221:11 - handle this I don't need to think about
221:13 - what URL I'm using or anything like that
221:15 - but realistically it's not a whole lot
221:17 - different than calling any normal API
221:19 - itself but in this case it's actually
221:20 - going to augment it in a way that's
221:22 - really cool as we'll see so now that
221:25 - we've got this let's go ahead and
221:26 - actually get the data back from this
221:28 - which I'll just say response equals to a
221:30 - weight fetch which means I need to turn
221:32 - this handle click into an asynchronous
221:35 - function and then I'll go ahead and do
221:36 - set data being a weight response uh
221:40 - response. Json like that this of course
221:44 - could have errors itself which we're not
221:46 - going to handle right now but overall I
221:48 - got a now click event that I can do in
221:50 - here let's just make sure everything's
221:52 - running I'll refresh in my create app
221:54 - and I'll hit press me and what should
221:57 - happen maybe at first it doesn't
221:58 - necessarily come through uh because it
222:00 - needs to spin up and all that uh but
222:02 - after I press it it will actually come
222:04 - through in there which we could console
222:06 - log the event each time it's happening
222:09 - and I'll go and press it now and you can
222:11 - see that it's clicking and it actually
222:13 - is doing that call for us and it's
222:15 - getting that data from our API in a way
222:18 - that's very nextjs based in other words
222:21 - the developer the nextjs developer if
222:23 - you were working with somebody else they
222:25 - could just work with their normal
222:27 - development process altogether and of
222:29 - course we could post data in here as
222:31 - well so for example to post that data we
222:34 - could just change this method being post
222:37 - and then the header uh is going to be
222:40 - our you know our content
222:44 - type and this going to be application
222:47 - Json and then our body
222:50 - data is going to be uh json.stringify a
222:54 - dictionary here and that's going to be
222:56 - email abc1 123@gmail.com
223:00 - and again it will still have a response
223:02 - so this is how I can post data and of
223:04 - course if you know react you will be
223:05 - able to change this as UC see fit as
223:08 - well so let's go ahead and try that out
223:10 - out I'll post out some data here and
223:12 - there is the result back from it right
223:14 - so this is coming directly from my API
223:16 - itself uh and there we go so it's now
223:18 - almost fully integrated here so that's
223:21 - pretty cool so now at this point what we
223:23 - could do is we could actually add this
223:24 - into our repo which I will so I go get
223:27 - add and we'll do get uh you know first
223:30 - prod
223:33 - deploy and we'll go ahead and push this
223:35 - into production and so what's going to
223:37 - happen here is it's going to go into
223:39 - vers sell uh it's going to go to the
223:41 - repo itself and it's going to do all of
223:43 - the build that it needs to to make sure
223:44 - that this is working properly uh which
223:47 - might take a moment or so uh but overall
223:49 - the idea being that once this is ready
223:52 - the production application version will
223:54 - be able to use API leads right in there
223:58 - uh and we already can but of course we
223:59 - could then customize our domain and do
224:01 - all that stuff but once it actually gets
224:03 - fully ready it will work the way we
224:05 - needed to and so the way I would
224:07 - actually change the r API itself is I
224:10 - would change this to being API leads and
224:13 - run off of that so that process would be
224:15 - then get status get add get commit and
224:18 - then updated
224:19 - versel inpoint for you know nextjs API
224:24 - so I'm basically going backwards now and
224:27 - so in my actual serverless node API
224:29 - itself I would then need to add in these
224:31 - paths here which I already did off the
224:33 - video and then I would actually have to
224:35 - go back into that application itself uh
224:38 - into serverless no jsapi look at those
224:41 - poll requests and then we would want to
224:43 - actually commit these things right so
224:46 - this is of course assuming that the dev
224:48 - stage was correct I'm not actually going
224:49 - to go through that process but the idea
224:51 - being that we now release the production
224:54 - version a new version of the API itself
224:56 - and really just kind of separating these
224:58 - two things out so realistically what our
225:01 - node.js application is doing is just
225:02 - consuming a pre-existing API it's not
225:05 - actually deploying our expressjs
225:07 - application on versell uh but it is
225:09 - getting close to that it's getting close
225:10 - to what we might want to do with our
225:13 - application here and so every time you
225:15 - run a release it should actually start
225:17 - building out a production version of it
225:19 - uh assuming that we are in the right you
225:22 - know branch and all that and it actually
225:24 - does go into our repo look looks like it
225:27 - has and so that would take just a moment
225:29 - for the deployments to start filling out
225:32 - uh right on here so it looks like my
225:34 - deployment is not actually filling out
225:35 - so let's go into our settings here and
225:37 - into G and oh yes we need to connect our
225:41 - git repository so I'll go in GitHub here
225:43 - and then let's go ahead and connect the
225:45 - serverless uh one that we have which is
225:47 - the API next so I'll go ahead and
225:49 - connect that one
225:51 - now and I might have to run another
225:54 - production push but there's our main
225:56 - here um and so now we have our project
225:59 - connected to get so it actually should
226:00 - do that deployment when I do make those
226:02 - changes so let's just do a quick change
226:04 - here and just call it a day and this one
226:07 - I'll go ahead and say Hello World V2
226:10 - and we'll go ahead and do a commit again
226:13 - and push it again so that we can trigger
226:16 - a deployment automatically and so that
226:19 - should actually happen now that I
226:20 - actually did that push and there it goes
226:22 - so it's now going to build that next
226:23 - version and by now hopefully our API
226:26 - leads is working correctly again it's
226:29 - still working correctly based off of
226:30 - that new rewrite that we just did right
226:33 - here uh and let's go ahead and just
226:35 - refresh that server just to make sure
226:37 - that that did happen and so I can
226:39 - refresh in here and all of that happened
226:41 - because of everything we set up with our
226:43 - seress framework and all of that
226:45 - serverless automation within there our
226:47 - production build takes a minute to
226:49 - actually deploy a new version of the API
226:51 - whereas the nextjs application might
226:53 - take a little bit longer than that uh
226:55 - depends on how big our nextjs
226:56 - application ends up being but overall
226:59 - once we actually deploy this we'll have
227:00 - a new version right on versell as we see
227:02 - here and now we can visit this and
227:05 - here's my production version on verell
227:07 - and there it is it's actually committing
227:09 - and making new data in my API backend
227:12 - and you know API leads is definitely
227:15 - showing that backend as well so great
227:18 - this is actually showing us how easy and
227:20 - how powerful can be to start leveraging
227:23 - that API into our projects into our xjs
227:25 - projects the other huge benefit of this
227:27 - is the developer who knows nextjs might
227:29 - not know how you built out this back end
227:31 - and there it is I want to point out also
227:34 - I never actually put any AWS keys in
227:36 - here I never put anything related to
227:38 - Neon in here I could quite literally
227:40 - just share this back end this API as I
227:43 - see fit and build down to my front end
227:45 - so like if you're working with another
227:47 - developer that's not necessarily on your
227:48 - team but you want them to build out a
227:50 - feature this is how this is one of the
227:52 - ways you could do it you could just send
227:53 - them whether it's your production API or
227:55 - your development API you could send them
227:57 - this link and say hey this is how you're
227:59 - going to do it and now you can actually
228:00 - build out what I need and then we can go
228:02 - from there of course you can still use
228:04 - wild cards there's a lot of different
228:05 - things you can do with these rewrites so
228:06 - just check the ver sale documentation
228:08 - for that um but this is the methodology
228:10 - I would take to leverage everything
228:12 - we've done to then start actually
228:14 - learning and building out an xjs
228:16 - application and then maybe modifying and
228:18 - migrating over to nextjs because a lot
228:21 - of the code itself can migrate not
228:23 - exactly because of how it's written
228:25 - we've got all this require here you have
228:27 - to change some of the import statements
228:29 - and all that but overall it's like now
228:31 - going towards what you would want to do
228:34 - within a nextjs application which is
228:36 - really exciting the final piece that I
228:38 - want to do is actually deploy our
228:39 - serverless node.js
228:41 - API itself this pure API itself directly
228:45 - to versell as well and just see what
228:47 - that looks like and how well it's not
228:49 - actually a whole lot different than what
228:50 - we just did but it's not going to give
228:52 - us the functionality of the reactjs
228:54 - framework we'd have to worry about that
228:56 - ourselves so let's take a look at how to
228:57 - do that now now we're going to take a
228:59 - look at how to deploy our expressjs
229:01 - application directly into versell now
229:04 - this can be combined with what we did
229:05 - with nextjs but I wouldn't recommend it
229:08 - generally speaking you probably don't
229:10 - want to deploy expressjs to versell even
229:12 - if it's technically possible nextjs is a
229:15 - much better alternative specifically to
229:17 - leverage a lot of the things that
229:18 - versell does really well and part of the
229:20 - reason I showed you the previous one
229:22 - first is because that's a better method
229:24 - to start leveraging your expressjs
229:25 - application and building off of that so
229:29 - the first thing that I want to do inside
229:30 - of here is I want to go ahead and add a
229:32 - new project this project is going to
229:34 - take a few things in here so let's go
229:36 - ahead and grab this serverless nodejs
229:38 - API the actual repo itself that we've
229:41 - been using and I'll go ahead and just
229:43 - deploy this as is now the problem with
229:46 - this of course is it's not going to work
229:48 - it's not going to work at all because of
229:50 - what versel attempts to do so we need to
229:53 - make it work and we need to change a few
229:55 - things on our project so the first thing
229:57 - that I want to change is I'm going to go
229:58 - ahead and add in versel
230:01 - Json and let's go ahead and change that
230:04 - to actually Json and I'm going to bring
230:06 - in that rewrites again so this time the
230:09 - destination is a little bit different
230:10 - and it's going to be a catchall and it's
230:12 - going to go into the destination of just
230:14 - simply SL API so in order for this to
230:18 - work we need to create a folder called
230:20 - simply API and in this folder we're
230:22 - going to go ahead and do
230:25 - index.js and this we need to import
230:28 - using the actual es6 modules from dot
230:33 - dot dot source and in our case it's
230:37 - Index right there so what we're
230:39 - importing here is the expressjs
230:41 - application which is defined right there
230:45 - and we also export it or we need to
230:47 - export it with module. exports and app
230:51 - equaling to app just make sure that it
230:53 - is exported so then it can be imported
230:56 - and then we'll go ahead and do export
230:58 - default of app okay so far so good so
231:02 - we've did that same rewrite we just did
231:04 - we also are adding in the index itself
231:06 - the actual app itself inside of the API
231:09 - and that rewrite is going to just return
231:11 - everything from that app to the API
231:14 - itself okay next up of course is going
231:16 - to be that we need a public directory so
231:20 - forel is really good at leveraging the
231:22 - public directory to serve the project so
231:26 - when it comes to nextjs it actually
231:27 - generates a public directory for you
231:30 - that will then manage all of the
231:32 - frontend side of things for sales
231:34 - optimized for front end and serverless
231:36 - in the back end and we're trying to
231:37 - emulate that here a little bit with just
231:40 - Express so the first thing I'm going to
231:41 - do is put get keep in here this is if I
231:44 - didn't want to actually put a front end
231:45 - of any kind but I'll also go ahead and
231:47 - add in
231:49 - index.html and then I'll just put in
231:50 - poorly formatted HTML here just to have
231:53 - a Hello World call of some kind okay so
231:57 - I didn't actually change this in terms
231:58 - of the expressjs application itself it's
232:01 - still using this SL API paths here like
232:04 - we saw before so we might need to change
232:07 - that as well uh but the reason I changed
232:08 - those in the first place was really for
232:11 - that integration with the xjs
232:12 - application based off of a rewrite okay
232:16 - so with this m let's go ahead and change
232:17 - it we'll do get status get add Dall get
232:21 - commit and we're going to prepare for
232:24 - versel and then we go ahead and do get
232:27 - push so what vercel is going to do is
232:29 - it's going to wait until this is on the
232:30 - main branch right now we're working on
232:32 - that Dev Branch so what versel will also
232:35 - do is well it doesn't really care about
232:37 - this production stage at all it's going
232:39 - to do its own thing it's going to
232:40 - basically do a similar item of this to
232:43 - deploy it which means we need one more
232:46 - step and that is our package.json versel
232:48 - is actually looking for something called
232:50 - build or versel we'll do versel build
232:54 - it's also looking for a script called
232:56 - build itself in this case we're just
232:58 - going to go ahead and do Echo the word
233:00 - of Simply hello and that's it in other
233:04 - words our build command is not going to
233:06 - do anything and the reason for this is
233:08 - that we maybe we add in another build
233:10 - script in here of some kind but ver cell
233:12 - build is going to attempt to build
233:14 - something and if this command does not
233:16 - exist then it's going to fail so during
233:18 - the versell build command it does
233:20 - something like our mpm run deploy but
233:23 - instead it's going to be versell build
233:25 - if that's available so we'll go ahead
233:27 - and leave that as is and then I need to
233:29 - redeploy this again but to do that I
233:32 - need to go back into GitHub uh and
233:34 - specifically into this repo itself so we
233:37 - go ahead and look for that repo and here
233:39 - it is right here and so what I want to
233:41 - do is just accept the pr that I just did
233:45 - um and commit pull and
233:48 - confirm okay and so there we go notice
233:51 - that versel actually looked at that as
233:53 - well so I'll go ahead and do another one
233:55 - with get status and get add and then get
233:58 - commit and then we'll go ahead and do
234:00 - updated script for versell and we'll go
234:04 - ahead and push that okay so it's going
234:06 - to take it a moment to build out so back
234:09 - in for sale we actually need to update
234:11 - this a little bit more too right so back
234:13 - into our project we've got our project
234:15 - right here well the problem with our
234:17 - project as it stands is the fact that
234:19 - well it doesn't have our environment
234:20 - variables that we need so our
234:23 - environment variables are going to be
234:24 - related to AWS right so the reason we
234:27 - need that of course is for that runtime
234:30 - so let's think about our production
234:31 - stage itself we've got en.pr which might
234:35 - have the stage variable in here for prod
234:39 - it also might need well not might it
234:41 - definitely will need various things
234:43 - related to our secrets so in our secret
234:45 - Library here we've got the AWS client
234:48 - that's coming through now when we deploy
234:50 - it to AWS those variables are going to
234:53 - be in there as in the AWS access key ID
234:55 - and the secret access key and of course
234:58 - it also has a role in here that we
235:00 - created specifically for the different
235:02 - stages and all that but we aren't
235:04 - pushing that into production anymore so
235:06 - we need to make sure verell has AWS
235:08 - access Keys itself so of course what we
235:11 - could do is we could go into AWS and
235:13 - create some new ones I'm just going to
235:14 - go ahead and use these local ones that
235:16 - I've got here just so I can emulate or
235:19 - show this example working correctly and
235:22 - there we
235:25 - go and I'll go ahead and save this the
235:28 - environment itself I don't necessarily
235:29 - need a bunch of different environments
235:31 - but I'll go ah and save those variables
235:33 - for all of those environments and there
235:35 - we go so one of the things I don't need
235:37 - in this version of my expressjs
235:39 - application is a neon API of any kind or
235:43 - my database URL because of course we've
235:45 - already done this but the idea being
235:47 - that my secrets are now stored on AWS so
235:51 - if you wanted to get rid of that
235:52 - functionality You' have to readjust how
235:55 - you're doing this and you'd probably be
235:57 - read addding your database URL as an
235:59 - environment variable but again this
236:01 - isn't a deployment that I recommend but
236:04 - it is one that you could do the question
236:06 - of course is whether or not you should
236:08 - so now with this deployed I see that it
236:10 - does say hello world so it looks like
236:13 - I'm on the right track that hello world
236:14 - of course is coming from the public
236:17 - index.html here so let's go ahead and
236:19 - visit this and at least the front end is
236:22 - going to work here so we've got the
236:23 - front end working which is kind of cool
236:26 - um I I want to visit the actual main the
236:29 - deployment as well so let's go back into
236:30 - the project and grab the actual
236:32 - deployment uh URL which is this one
236:34 - right here and so there we go and now
236:37 - I'll go ahead and jump into the API SL
236:39 - leads and if I did everything correctly
236:42 - it will work but of course the problem
236:45 - that it is facing right now is merely
236:48 - the fact that I didn't have those
236:49 - environment variables when I deployed
236:52 - this version so let's go ahead and just
236:53 - create them now I'm going to go ahead
236:55 - and update my index page and we'll go
236:57 - ahead and do p and EnV
237:00 - working try this again and actually one
237:03 - thing I might need to do also is just
237:05 - verify any poll requests that I have in
237:07 - here so this will also trigger another
237:10 - deployment so I'll go ahead and run that
237:13 - what this will actually do is solve that
237:14 - environment variable thing so I probably
237:16 - don't even need to submit this yet so
237:18 - I'm not going to do that yet let's go ah
237:19 - and see if this recent deployment solves
237:22 - the environment variable thing that we
237:23 - had with versel so get going back in
237:26 - here we'll just go ahead and take a look
237:27 - at the deployments that are coming
237:28 - through and of course it's building one
237:30 - notice it doesn't take very long that's
237:32 - not surprising because it's only
237:33 - installing a few things here it's not
237:35 - using the cache in the same way that AWS
237:38 - Lambda is it's literally installing all
237:40 - of the things in our package.json all
237:42 - over again and there we go we've got a
237:44 - new production version so I'll go ahead
237:45 - and open up my app again I can actually
237:48 - just go back into this URL here and if I
237:50 - refresh now everything's working and
237:52 - there is my expressjs application
237:54 - running on versel so what we could do is
237:57 - yes we could keep this I don't recommend
237:59 - it but we could keep it and then build
238:01 - out your front end using something
238:03 - different so expresss has a lot of
238:06 - different front-end support as well in
238:08 - other words the
238:09 - index.html could then bring in something
238:12 - like pug or some other JavaScript that
238:14 - you might want to have in there that
238:16 - then calls the API itself uh but that
238:18 - starts to get way too complicated then
238:20 - it's like oh you might as well use for
238:22 - yourself you're going to start building
238:23 - out the front end um itself forell and
238:25 - xjs uh to build it out and make it even
238:28 - more powerful but I really wanted to
238:30 - show you how those two different
238:31 - approaches worked so that if you wanted
238:33 - to use something like versell to deploy
238:35 - your applications you totally could now
238:38 - all all of this is possible because of
238:40 - how easy it is to be flexible with our
238:42 - deployment options neon is well suited
238:45 - to do all of that for us right so the
238:47 - fact that our database we didn't have to
238:49 - touch our database at all for those last
238:50 - few deployments was so nice and that's
238:54 - because of how we configured everything
238:55 - to work and in this like automated
238:58 - fashion so when it comes to deploying to
239:00 - versel the other part of this that you
239:02 - could consider is you don't always have
239:04 - to have your database URL exposed inside
239:07 - of the application itself
239:09 - so even if you were using nextjs you
239:12 - could still use the parameter store that
239:14 - we did and then in your workflows you
239:16 - could still do the dev stage where you
239:18 - would basically stop uh as far as the
239:21 - dev stage is concerned you would stop
239:23 - you would create a branch maybe the
239:24 - deployment Branch you could then also
239:27 - create that uh pole stage actually
239:29 - everything in here with maybe the
239:30 - exception of deploying a development
239:33 - stage on AWS could still work in a
239:36 - nextjs application and nextjs itself can
239:39 - also do a bunch of things with that and
239:40 - of course if we wanted to deploy our
239:42 - development version on versell well we
239:44 - could go back into versell into our
239:46 - project here the first thing that I need
239:48 - to do for this project is jump into the
239:50 - settings go into my environment
239:52 - variables here and we want to add one
239:54 - specifically for a preview environment
239:57 - I'll go ahead and grab the branch itself
239:58 - which will be our Dev environment I want
240:00 - to go ahead and add the stage of Dev
240:03 - right and then we'll go ahead and save
240:05 - that so now we've got a preview
240:07 - environment on that stage right so it's
240:09 - going to give us that Dev there so the
240:12 - next thing that I would want to do is
240:14 - jump into our deployments here and we're
240:16 - going to go ahe and create a new
240:17 - deployment so it's going to be commit or
240:20 - Branch reference this is going to be Dev
240:23 - so let's do a quick search for the dev
240:25 - Branch there it is right here it was
240:27 - happened just a little bit ago I'm going
240:29 - to create a deployment for that and this
240:31 - should work right so it's going to be
240:34 - basically the same thing just using the
240:36 - dev Branch itself so in other words the
240:39 - branch itself should also have the same
240:41 - data that our production one has because
240:44 - well we've been deleting and recreating
240:46 - those branches uh but overall what what
240:49 - I really want to see here is that the
240:50 - dev Branch actually works and it is
240:52 - something that's different and again
240:54 - it's just another way to just Branch out
240:56 - our database do some tests and preview
240:58 - environments and all that so here it is
241:00 - there's our preview environment I think
241:02 - it's working so let's go ahead and give
241:03 - this a shot by looking at the Domain
241:06 - itself um and so there's our world let's
241:08 - go into our API just make sure that
241:10 - that's working so API leads and what we
241:13 - should have in here is something maybe
241:15 - slightly different than the other one
241:17 - this has 18 values in here um and so
241:20 - once again I could just go back into
241:22 - this one let's go ahead and just create
241:24 - a preview or uh you know a stag version
241:28 - I'll go ahead and update this and go
241:29 - ahead and do staged version and we'll go
241:32 - ahead and push it okay so of course one
241:35 - of the challenges with pushing this is
241:37 - the fact that I have my Dev stage here
241:40 - is going to quite literally create and
241:41 - delete a branch which could cause Havoc
241:44 - or issues with staging this version uh
241:47 - so it's not necessarily something I
241:49 - recommend doing but we could we could
241:51 - just try it out just to see that you can
241:53 - absolutely deploy different versions of
241:55 - your application with different stages
241:57 - and different branch databases at that
241:59 - time as well um which in our case the
242:02 - stage version uh it just came out 20
242:04 - seconds ago so we can go ahead and take
242:06 - a look I'm not I'm not confident that
242:08 - the API is going to work here but we can
242:10 - give it a try at least we've got some
242:12 - you know other text in there so let's go
242:15 - ahead and grab in the SL API and leads
242:19 - and I I would imagine that it's either
242:20 - going to work or it's not in this case
242:22 - it actually did work uh which is great
242:24 - so we have the ability to now also have
242:27 - those different stages as well on
242:29 - versell and each stage could have it a
242:32 - different domain as well as the main
242:34 - production one can also have its own
242:35 - custom domain which is maybe one of the
242:37 - main reasons you end up deploying to
242:39 - versell as well is because it's quite a
242:41 - bit easier to put a custom domain on
242:44 - versell than it is on AWS and the
242:46 - servess environment so yeah that's
242:48 - pretty cool I think for sale is a great
242:50 - tool and now I would recommend that you
242:53 - start modifying converting all this
242:55 - stuff to nextjs which is really the the
242:58 - next part of so much of this but even if
243:00 - you didn't we now have a really solid
243:02 - foundation for an API service that we
243:04 - can start building out a lot more
243:06 - endpoints if we needed to hey thanks so
243:08 - much for watching hopefully you got a
243:10 - lot out of this now I want to leave you
243:11 - with two challenges one is how do you
243:14 - actually Implement authentication
243:16 - whether it's users or other applications
243:18 - how do you actually secure our API more
243:21 - now that will be a really good challenge
243:22 - to undertake at this point if you were
243:25 - able to get this far the other one is
243:27 - well how do you actually convert this
243:29 - expressjs application into a nextjs
243:31 - application those are the two challenges
243:33 - I want to leave you with they are fairly
243:35 - straightforward especially because
243:37 - there's a lot of thir third party
243:38 - support out there but I think those are
243:40 - really good challenges that you can
243:41 - Undertake and guess what they can feed
243:43 - into each other as well so thanks again
243:45 - for watching my name is Justin Mitchell
243:47 - I really enjoyed sharing this with you
243:49 - because I think servus is one of the
243:50 - most fantastic things that you can learn
243:52 - about and Implement because it really
243:54 - just helps us focus on just our code we
243:57 - don't have to worry about the
243:58 - infrastructure that infrastructure is
244:00 - taken care of by the different services
244:01 - that we leveraged right so AWS Lambda
244:04 - will scale up if we get a lot of users
244:06 - coming in so we our on database so would
244:09 - versel if we went there it's really
244:11 - really great to just focus on the code
244:12 - itself and not really worry about the
244:14 - infrastructure that is one of the keys
244:16 - of serus and that's why I wanted to
244:18 - create this course for you so thanks
244:20 - again for watching and I look forward to
244:21 - seeing you again in the near future take
244:23 - care

Cleaned transcript:

in this course you'll dive into deploying expressjs and node.js applications on AWS Lambda using neon serverless postgress and the serverless framework by the end of this course you'll also learn how to adapt your deployment for versel alongside mastering database setup migrations GitHub actions and more Justin Mitchell developed this course Justin has created many excellent courses on his coding for entrepreneurs course platform and his YouTube channel neon provided Grant to make this course possible hello and welcome to the serverless nodejs API course in this one we are going to learn about expressjs to build what's called a rest API now we're really going to be building a JavaScript application that runs on the back end in a serverless environment which really means that we just focus on our code we don't have to focus on the servers at all now traditionally speaking when it comes to seress a big challenge that comes up is what database to use in and how do we actually connect to it now for that we're going to be using neon and servess postgres neon is actually Paving the way for so many applications to leverage serverless databases as well and of course they are a partner in this course but the idea here is we need an application that we can scale up or scale down as needed and we'll do that using AWS Lambda serverless functions but then we will also need our database to match that scaling up and scaling down and that's where neon comes in but it gets better better than that what we can do is we can also have multiple versions of our application deployed so we can do these iterations share them with other people and test out little pieces of it while also simultaneously having our database made and ready for that neon does this really cool thing to Branch data at any given time you can look at a point in time and Branch it so you can work on multiple versions of your application at once and allow people to use it without having major overhead to make all of this work it's fantastic it's a really straightforward way to do all of this now the idea here is really going back into focusing on the code and being able to iterate as much and as fast as we possibly can so sure we are going to be deploying an API service itself but that's really just for a foundational piece so we can actually focus on getting our code better and better and better over time so this is really about setting that foundation and serverless is the perfect way to do it so let's talk a little bit more about why serverless is important and why you should care about it let's talk about what servus means for your application and why you should consider using it first and foremost it's all about focusing just on the code and effectively running that code now the best analogy I have is lights like quite literally the lights in your room right so when you walk into your room you turn your lights on when you walk out you turn them off right but what if you forget to turn them off well they just keep running whether or not you're using those lights or not servus is just like that now a normal server application you turn the server on when you want it to run and you could leave it running forever that doesn't mean it's being used all the time but it does mean that it's running forever now of course there's a lot of cost Associated to that to have it run forever because computers just have a bunch of teeny little lights that really that's how it works right but the idea being that if you leave it on it's going to continue to run always but there's another part of this is like what if you have just a small little light turned on maybe it's just your desk light and that's the light that you have on and then when you leave that desk light is still on but then a 100 people walk into the room that desk light is not going to light up the room you'll need a lot more lights to do that and therefore you would have to scale your lights up now it's not any different in a server environment when you have one server yes you can have a tiny server that maybe costs a few bucks a month to actually run your application but as soon as you get thousands of people trying to access that server well you're going to need to scale up you're going to need to load balance across all of these different other servers which means that you'll have to actually understand the infrastructure behind it you'll have to learn how to scale those things up now don't get me wrong I find that stuff really fascinating it's something that I really like doing it's a fun orchestration but the thing is not everyone's ready for that or is interested in doing that especially with business critical applications maybe you don't want to manage those servers and so that's where these serverless Services come in servus is really just focusing on the code you deploy your code and then somebody else makes sure that all the systems scale up to meet the demand and all that scaling happens automatically but when you use something like AWS Lambda you just focus on making sure your code runs and then once it does run you can deploy it to Lambda Lambda will scale up and scale down to meet request demands now Lambda isn't the only servoless service that's out there there are other servess options but Lambda is a great one it's one that helped Pioneer so much of what we take for granted with serverless today so it's actually really great but actually deploying to Lambda presents a few problems for us number one deploying directly to Lambda with code is tricky there's a lot of different things that we need to consider with doing that and if you don't know AWS it becomes even more tricky because there's a lot to AWS and that's where we use something like the serverless framework we really just use the open source framework to help orchestrate the deployment to Lambda so that it's only a few commands instead of trying to figure out a bunch of commands in there now there are things that we will discuss in terms of I am or access policies and all that with servis and AWS um but you know it actually is fairly straightforward with servess I've I've seen a lot of different Services over the years that deploy to AWS Lambda I can honestly say that servess is one of the best if not the best way to do it specifically for Lambda and what we're trying to do with nodejs now the other part that comes as a challenge when it does deploy a serverless application the serverless application typically doesn't interact well with databases because databases themselves historically are always on that's because they pretty much always need to be available to grab that data so traditionally speaking that's what they do and the reason we're using neon and well neon is pioneering this serverless for databases and specifically for SQL databases and postgres postgres is probably the most used database in the world and so a serverless postes really just unlocks this for your database as well so in addition to your code application your database is going to do that as well now there's a huge advantag as to having serverless databases as well is this point in time branching so what's going to happen is whenever you need a copy of your database with neon you can just Branch it just like you would with your code and it will actually make a point in time copy of that data so then you can actually stage your application many many different times so if you're working on version 10 or version one and all of the versions in between between you can actually create different branches of that in your database do the migrations make the changes that you need for your database all right there and all of this is possible because of how they approach serverless and how easy they make it for your applications to run and consume all of this stuff so that's really what we're going to be doing is orchestrating all these things together now I realize it might sound complicated but it's actually fairly straightforward thanks to both AWS Lambda and neon really you just focus on code uh and you just pass in in the case of neon you're going to have a database URL string that Express will consume using the neon servess package that works with JavaScript so it's actually fairly straightforward there are some moving parts so yeah you probably want to know some JavaScript to get this going but we'll talk about that in just a moment but that's a high level understanding of why Servo is in the first place it's far more effective it's far more efficient than managing a server yourself now of course if you're not ready to really scale up your application and you're not ready to build into this environment you can still bring around the expressjs application and your neon application or your neon database with you to wherever you want to deploy it doesn't have to be servess and so we actually do take a look at how to deploy this on for sale for that very reason is so you can see how portable it can be by leveraging a manage database service in this case a serverless one that ends up costing significantly less so I'm really excited get into this one so let's go ahead and take a look at this course let's talk about the requirements you'll need to do well in this course and then we'll also do a highle overview of what it is we're going to be using what technology we're going to be using now first and foremost this is a JavaScript heavy Series so that means that you need to know stuff like functions classes objects arrays string substitution handling promises and callbacks maybe even Json once you know that stuff in JavaScript then you can move to the next part which is using node.js or serers side JavaScript so be sure to install node.js grab 20 LTS so version 20 is the version we're going to be using but whatever version you end up using make sure it's an LTS one unless of course you know what you're doing but the idea here is we're going to be using nodejs to actually run our JavaScript so we'll create a Javascript file and then use node to run it now if you're from the browser world of JavaScript as in writing stuff like react or vanilla JS node.js is just on the server side your web browser won't run your JavaScript node.js will although they work basically the same which is one of the Magics of node.js so once we actually have our application running locally we'll start to deploy it to a serverless function called AWS Lambda now at a high level serverless just means that it runs when it needs to that's what we're going to be using Ed Lambda for and that's how they can offer things like 1 million requests free per month on their free tier which is amazing that's so many requests to pay $ Z for so the next part of this of course is not just running the actual code itself but Al also storing the data that we need to store and for that we're going to be using serverless postres for this thanks to neon neon is really pioneering the next generation of what post CR can be by implementing things like servess and many other features to make it a very very powerful offering for your applications this is very very straightforward as we'll see but the idea here is these are going to work int to have a fully seress application from the actual node.js side of it itself as well as our database is fantastic then to actually build and deploy the application we're going to be using GitHub and more specifically GitHub actions to have a serverless part of that as well that's going to handle so much of that so if you don't know git definitely brush up on that as well now finally the main part of AWS that we're going to be using really is awsam this is because of the the tools that we'll use we really just have to focus how to make the right policies to do the deployments itself we're not going to have to manually deploy to AWS Lambda because that's kind of tricky and it takes a long time to configure when we can really just automate the process and really only focus on the I am access the policies all of that that's one of the things that we'll be doing for sure so make sure that you already have an account on AWS and also that you already have an account on neon and all of those links are in the description of course and then definitely sign up for GitHub as well now a couple optional things that we'll do towards the end are deploying to versel so deploying to versel is going to basically take everything we did for AWS Lambda and then deploy it on versel and that's really where we're going to wrap things up to really just see how portable we can make this application and of course when we deploy to versale we will still be using neon our serverless database so that's going to be very very straight forward so we can see how we can really move the code around without doing much to our database which is really nice to to keep that Integrity now the final thing that's optional is actually using visual studio code or vs code this is the text editor I like using the most to me it's by far the best one that is also free so if you have one that you prefer by all means go ahead and use that one I'm going to be using visual studio code so with all those requirements out of the way let's go ahead and jump into actually building out our project of course with node.js already installed which we'll verify when we build out that project as well now when it comes to building for AWS Lambda one of the key things we have to think about is how do we actually even test or develop locally for a serverless function and the way we're going to do this is by using the serverless framework from serv.com now this framework is going to make it easy for us to test locally as well as deploy into production but before we even get there we need to jump into the AWS console to just verify which version of node.js we should even use I know I mentioned that we should download and install the LTS one in the last part but we want to verify that that LTS is even available in Lambda itself so if we go into AWS into Lambda you can do a quick search for Lambda as well we're going to go ahead and hit create function and in here we just want to look at the runtime and the run times that are available right now as we see there are a number of them on there I would say node and python are probably among the most popular ones there but the idea is node 20 is available and therefore the one that is the current LTS is also available so download what's available there so we can actually test it out locally and so in my case I've got the node version 20 There It Is I've got mpm 10 which comes in with node version 20 and then I've got the serverless framework is what I want to use to actually test out AWS Lambda we're also going to use serverless to create our project that's what we want to do now is we're going to use that package to create a project so to do this we're going to do NP PM install DG serverless and this will install that serverless package for us in my case I already actually have it installed so it's actually pretty fast but the idea then is once we have serverless there's a lot of different Frameworks that we can use from it so it has a lot of different features in here as well many of which we are not going to cover at all but this is what we're going to do we're going to create a new serverless project and then we're going to navigate into that directory and then we'll actually bring it into our vs code instance so to do this I'm going to navigate into a location where I like to store my projects which in my case is in the dev folder and we'll call serverless right here hit enter okay and so what we should see now is different kinds of serverless functions we might want to build out and so what I'm going to be doing is the express API one and this is just to get some pure expressjs built out for us or the package for us I'm going to go ahead and save that and I'm going to call this my serverless nodejs API okay and so it's going off of the node Express and all that I'm not going to register a login for the serverless framework I don't need to do that and I'm also not going to deploy right now in fact I don't really have a way to deploy yet and so we will worry about that when we get there uh but for now what I want to do is I want to navigate into this server list no. JS API and we're going to go ahead and open this up into vs code just like this and so I'll go ahead and start off and save this as a workspace and then I'll also do my get in it definitely want to make sure that I have that notice there's already a g ignore file in here so it's not something I need to worry about just yet next thing what I want to do is prepare this to work with the correct postgres so when it comes to postgres databases you need to have a client of some kind to connect to that postc database so traditionally speaking you might use something like node postgres node postgres is a fantastic package that makes it really easy for something like expressjs to connect to a postres database which works really well when the node application is continuously running even if it's updating and it has some downtime overall it would be continuously running right and so when it comes to something like AWS Lambda it's doing the opposite of that it's only running when it needs to run and therefore it's going to be down a lot and so that's where something like node postgres isn't well positioned to make that happen maybe at some point it will be but the thing is neon actually pioneered neon Serv list this actual database which is ideal for serous and Edge deployments using htps and websockets so that persistent connection can be interrupted on a regular basis because of these two things and therefore is very well suited for any kind of Ser lless including AWS Lambda and it also can use persistent connections because it's a dropin replacement for that node postr package and it's also what this is based on but again it's designed for this modern serverless architecture that AWS Lambda has that we've seen in a lot of different places as well and so that's the actual package we'll end up using to connect to our database and of course our database is also provided by Neon but the idea here is we definitely need to use this postres package or this postes driver and client to be able to connect our expressjs application to it so let's go ahead and install this as well I'll go ahead and use the mpm install command as well so we'll install this into our application and while we're at it we're also going to go ahead and do mpm install DG and it's going to be the neon command line tool so we'll use this to use all sorts of things related to Neon and so with that installed I'll also go ahead and do neon CTL and off so I can actually log in to neon and verify my local machine on this to do all of these sorts of things which in my case I'll go ahead and authorize you can also use an API token which is something we will do when we get into the GitHub actions of it all and we can see where those credentials are saved right there okay so we now have our Baseline project set up right it's ready to go and overall there are still a few other installations that I want to bring in and those are related to how we connect to our database and how we manage databases schema so this is going to be done using drizzle o and drizzle kit both of these things will be installed right now so let's go ahead and install drizzle kit omm and and then we'll go ahead and install drizzle kit and to do this we'll do mpm install D capital D drizzle dkit and go ahead and install that as well which it might have actually done with Drizzle it did not so drizzle kit will be used as well to help us with our migrations the drizzle omm will help us design out our schema once we get to that portion but these are the main packages we're going to be using if we need to add any later we will uh but overall for right now this is what we're going to start with and we really need to see how to do a Hello World with this serverless package and see exactly what that's going to look like in conjunction with our neon database when we get there in a standard server environment the way you run expressjs is by modifying what we have here by using app. listen and then we can pass in some sort of Port value here and this port value then can have a callback function once it actually starts running and you can do something like console log and then running at HTTP col localhost 3000 right and so what this will allow us to do then is to open up our terminal and we can now run node index.js now before I even look at that I'll close it out and turn that off I'll comment that out again and run it again and we see it goes away this is actually a good way to think of serverless right so the first version this is going to continuously run no matter what the second version is running when we tell it to run and the way we restart the first version is well quite literally restarting the application itself and then making sure it runs so now we've got that let's go ahead and take a look at that homepage and we can do that by going into our web browser we could o up open up a new terminal window and we could just run curl HTTP col localhost and 3000 and that will give us the same response back great so that's showing us that expressjs is working and in some sense it's actually a hello world from expressjs but of course I want to make sure that my serverless app is actually working and I can use it locally so the way we do that of course is by commenting out this app. listen this is for like a server full app which we are not using right and so let's go ahead and close out this node index.js so what we want to do is we need to update our local environment to handle the offline mode from the serverless framework to do this we're going to go ahead and use serverless plugin install dashin serverless Das offline okay so once we install that it should open up and install something into package.json related to the serverless offline package and again this is for development mode so we can emulate what the actual AWS lamb serverless would end up doing and so to actually run this we can use server list offline now and that will trigger the exact same application and it's running at the same port in this case so if I went ahead and run this again I get the exact same value except now if it's going based off of an AWS Lambda event or it's emulating exactly what that is it even shows you the build duration for this request now another thing that's cool about this is it can also emulate different regions so if we wanted to run that with a different region so like region and US West one you could do that and that would actually show you this different region we can also do different different stages which allows us for a lot of options here so if I do region and or rather stage and prod and hit enter I have just another stage that I can work off of and of course we're going to build this out quite a bit more throughout this course but the idea now is I have a really quick and easy way to test this serverless application itself but there's still some things that I need to modify the first one is going to be package.json what I want in here is the scripts de definition so we can actually call out those scripts that we might want like Dev and this is going to be just simply serverless offline and you can add in the stage of let's go ahead and say Dev for example the region in this case doesn't really matter that much uh but I'll go ahead and leave it like that that way I can do mpm run Dev and then it's still the same thing and it's still working just as it was before but then I don't have to remember all of the commands related to this serverless offline that's a big part of the reason I use scripts like this there are other reasons as well but again we're just sort of preparing for development mode in this at this point so the next thing is going into serverless yaml so what you might see right off the bat is well yes it added this plugins here perhaps you looked at this file perhaps not if you haven't looked at this file we see this functions in here this function Handler we'll talk about that in a second but then I also have this big red flag which is this right here that is showing us the runtime node.js 18 this is not the runtime we want now now to remember what run times we have available to us we can log into the console on AWS and we can navigate into Lambda and then go into create function and going down to what's listed here right so we have other options on here now we've already talked about it but 20. X should be the runtime we are targeting here so we're going to go ahead and save that we'll run this again in this case it doesn't actually change anything it doesn't change anything technically from how this is running because node 18 and node 20 aren't a whole lot different but at the at the point of this is to make sure that our runtime is matching our development mode and our production mode that's really the point there the next one I want to do is change how my API Handler is working so the API Handler is really just looking for this index. Handler which in our case is coming from the servis application itself and then this module right here that's exporting Handler that is what we're end up using so we've got is index. Handler if I change this to app. Handler I would need to change index to app.js right and you might end up doing that now in my case what I usually do is I create a file called SRC so SRC a folder there and then I move my app inside of SRC now it's probably a good idea to just keep it in as just index.js our application itself is not going to get that complicated so we're going to leave it as index. JS but with this it actually separates my application code from a lot of the configuration for the project itself which is what I always always do and so back into this server. yaml we can now just change this Handler then into SRC do index. Handler right and so that's going based off of the folders themselves so let's go a and save that and let's try that again with this run Dev in this case I can go ahead and run it again and now I get a problem in here so let's go ahead and change the the actual handle and Handler to actually doing not DOT notation but using the folder itself and so we'll run it again this time it works correctly okay so that dot notation is the intuition I would typically use to run some sort of Handler like this but it's important to be able to test these things out try them out and see exactly how to configure this so now we can actually start developing with this serverless offline framework it's really just that simple I don't think there's a whole lot more we need to talk about this in terms of getting the serverless offline working now I will say that you could probably experiment with a lot of things related to just doing your server full app that's probably going to be okay and realistically I wonder if we could have that server full app in here with the Handler itself if that causes any issues and it doesn't appear so mostly because of the export itself the actual name space for that export is what's being used it's not actually calling index JS like this app Listen would end up doing so basically we have support for either one which is also pretty nice okay so now that we've got this let's go ahead and see how we can load in environment variables into our application right off the bat at this point in your coding Journey you probably realize that it's not a good idea to hardcode API Keys database strings or really any secret data right inside of the code itself it's much better to inject that when the code runs so have it done in the environment at itself and the way we're going to do that is by using a EnV file this is very very common to do in all sorts of programming languages but the idea here is in index.js if I did something like database URL we don't want to have the actual database URL string in here we want that to be abstracted away and so the way we're going to do this is by using process.env so what we have here is process. EnV do and then the environment variable we want to use in our case it's going to be database URL so if I save that as is and then run our serverless application our offline and then curl to it what I'll get is this hello world here looks like maybe I didn't save anything so let me try that again with that curl I'm still not getting anything for this so let's go ahead and actually change this a little bit more and then go ahead and say something like not here okay so now we've got if it is there it will be set otherwise we'll just have it as not here we'll go ahead and re run our lless application and now I get this database URL not here of course this is just a fallback value not something we'll necessarily use for much longer but the idea is that we want to actually have this work and so that's what we'll do now now traditionally speaking you might use something like theb package this is a fantastic package and is used a ton so this is something you would absolutely consider using when it comes to well non AWS Lambda Services we want to prepare it for aw Lambda and the way we're going to do that is by using the server list. EnV plugin it's a fairly straightforward plugin and it allows for a lot of different support as we'll see right now so the idea is we want this database URL to work and the way we can do that is by coming into our server. gaml and we can Define our environment and so in this environment I can add in our database URL here and I could set it equal to EnV colon as in the environment colon and then the actual value value we want to set from the EnV file like that I can also have a fallback like that okay great so this is sort of configuring it but we still need to add in the plugin itself so let's go ahead and close out this server and now we're going to go ahead and run serverless plugin install dasin serverless dv plugin hit enter and this will install something in our serverless Di gaml as well as package.json will be this right here and so now that we've got that we can come back into Ser yaml we see that this is one of the plugins that's available to us which of course will be very useful when we actually push this into production then I also want to add in use. EnV being true so we'll go ahead and save that and I just want to see if the fallback value shows up if the EnV value shows up or if the inline code value shows up so let's go ahead and try this out by running it again I don't need to reinstall I'll just go ahead and run mpm run Dev and then we'll run our curl command and we got something else in here so what's happening in in our serverless DL it's fallback value is actually not being used but rather the EnV value is being used so at this time in my get ignore I want to actually add in EnV here with a star at the end because what I can also do is EnV let's say for instance DOD as in my development mode this is another value that I might have so we'll go ahead and say Dev DB right and so then if I wanted to use this Dev mode here I could go in package.json and quite literally change the stage to Dev mode which I still have it's actually on that stage so this Dev right here if I called it Dev ABC I would want to change this EnV to Dev ABC type of thing right so that stage is how I can actually modify this a little bit which which is also super nice and it actually will help prepare us and for our local environment for our staging environment and then finally for our production environment we could quite literally have all of those environment variables in here and then just call the different stage to run the different environment variable which we can test out really quickly right now what we see is that database URL coming through fantastic so the next thing is like maybe we want to add in another environment variable I'm going to go ahead and say debug and we're going to set this equal to one so this is now in myv back into serverless iaml I'm going to go ahead and do something similar um and this time I'll go ahead and say debug and we'll go ahead and come in here this time I'm to give the fallback value of zero and so now in index.js I'll go ahead and add that in as well and debug and this time I'll just go ahead and leave it like that save it and actually I'll go ahead and say it's equal to one so debug being equal to one based off ofv let's go ah and save that we'll restart everything and let's run it again this time I'll curl again and what we've got is false right so EnV is not equal to one let's go ahead and see if it's equal to or what it is equal to let's just console log it like that I'll go ahead and restart the application itself we'll run it and my console log is one what the what just happened here well let's actually try it or equal to the string of one and let's see what that looks like so go ahead and just put it in as a string value here and maybe we wrap this whole thing in a string as well now let's go ahead and try this again rerun this and run that again and now we get true so the actual values themselves are often going to be treated as a string itself so you need to be aware that when you're going through and using environment variables inside of your express application um now there are more advanced usages of the actual uh you know plugin itself which I do encourage you to go into the serverless Frameworks plugin documentation for that and you can see all of the advanced usage that's in here but generally speaking what we want to know about are three main things one is we have to define the environment variables in ser. yl that we're going to use in our application two we can use different EnV files to actually load in those environment variables and those EnV files can be as simple as just the standard EnV or of course based off of Any Given stage that we might want to have Dev test prod V1 V2 and then each stage can still inherit from that base. EnV which means that we don't have to rewrite all of the same variables over and over and over again like in the case of debug maybe we just want it always to be one so that all of the other stat PES can just go based off of one CU we're in our local development environment and especially because these EMV files would never be committed to the git repo and then of course in the final one our code itself needs to be aware of when the actual data types don't work as you might expect so that's another thing that's just to be aware of when it comes to using those environment variables and so at this point we now have a way to test out our application altogether notice that when I did change the EMV Dev the actual stage environment variable then the actual value came through so this actually overrode what was in the first one the EnV itself uh which Al is also pretty nice great so now that we've got this we have we're really just ready to start actually using that database URL we start the integration process with our neon postgres database so let's start that process now now we're going to take a look at neon serverless postres now what I want to show you here is the console on neon so we can really highlight what branching is all about it's that point in time snapshot of our database that we can essentially use a brand new copy of it isolated from the original one which is great for production applications and then staging instances or versions of our application and it happens really really quickly so of course we get all the benefits of serverless as well so it actually scales down to zero and then it'll scale up to meet demand as well our application needs it now the latency to actually scale from 0 to 1 one is surprisingly incredibly fast with neon as well so go ahead and go to neon.pdf versions as well I'm going to go ahead and use the default database name but feel free to change this as you see fit the last most important part is the region itself this region should be as near to your application as possible now in our case we're going to be using the default which is Us East Ohio at least that's my default and the reason for this is because it's going to be where we deploy our application on AWS and of course that region of Ohio is where we're deploying to us East to in AWS which will come back back to once we actually deploy that but the idea here is we want to start off with our database being in that region now it's actually really easy to create a new project with a new region so if you have to you could always just use whatever region is physically closest to you right now and then then when you actually start deploying your application you'll go ahead and change the region then okay so at this point let's go ahead and create that project and what we get right out of the gates is a connection string this is a postrest connection string that gives us our username our password our host our database and a few other configuration items now the other thing that's really cool about this when you first start out is you've got a lot of options for integration in here right and there is one for node.js but we are actually not going to use this integration option we don't need to for a number of reasons one of them being that we will just use that connection string that's what we want so basically what we'll do is copy this string whatever it ends up being you can click copy right here and then you'll bring it back into your local project into the environment vers variables when we did the EnV stuff and you'll paste this in and this is what we'll end up loading in once we get there for now I'm going to leave it just as a you know the original D Dev DB thing uh but the idea being that eventually we'll use this string for now though what I want to do is I actually want to play around with branching inside of our project so let's go ahead and close out this window here of course we can still get all of that data right in the dashboard also but what we want to do is we want to actually insert some data into our database now of course if we look at our tables here we have no tables yet there's really nothing in our database so let's go ahead and add some just by using the SQL editor that just gives us some default code that we can just run that will actually insert some data in for us and so what we'll see is if I go back into the tables tab I now see there's a table in here in which I can select and actually see the data that's coming through from that and so we could also just run that a few more times by going into our history here I can just run a couple lines here by selecting it I'm going to run that run it again run it again just so we have some more data that we are playing with just like that so I've got 40 rows in here right now so let's take a look at the magic of branching so if I go in this branches area here I'm going to go ahead and create a new branch and I'm going to call this my Dev Branch so current point and time that's what we're going to go off of but notice we've got other options in here we can also select which branch we want to be the parent the one that we're basically copying from I'm going to go ahead and create this new Branch from my main branch cuz that's the only one I have and now I get another connection string this is a different version of the exact same data but it's a brand new and unique connection string that I can use in my project specifically for like a Dev branch that has a lot of the same data so let's actually just verify that real quick by going back into that SQL editor and now what we want to do is just switch our branches to our Dev Branch I'm going to goe and run those two lines once again and now I'm going to go run it and let's run it again and what we'll see is if I go into my tables once again I'll change the brancher make sure that I'm on a new Branch I'll select the table that I just was playing with I'll scroll to the bottom and notice that I have 60 items in here now and if I were to go back into the main branch which I can do that quickly select it again and now we see once again the main branch Stills back at that 40 amount of items or 40 rows in there um and it happens just that quickly but the other thing that's great about this is if at any time we feel like deleting this I'll just go ahead and copy the name go ahead and delete this delete it now that project is completely deleted so what we can do of course is just bring it right back now of course this is going to be an empty project with empty databases but overall it's the same configuration just it doesn't have the data and I can spin it back up that quickly so of course actually bringing this back to what we just looked at I would just need to go back into that SQL editor and create those tables of course before I created tables I could just verify that yeah it actually is an empty database which no surprise it is you know like that kind of makes sense but the idea being that if I come back into the SQL editor I were to run it again and maybe again and let's say for instance I wanted to create a new Branch once again I'll go into branches create a new Branch Dev Branch from the main branch I go and create that and then we'll go ahead and go into well let's create another Branch we'll create one from the dev Branch we'll go ahead and do Dev V2 right you can create as many branches as you really need I mean my project in this case has a limit of 10 but you're probably never going to go past 10 that would be surprising if you ever did but the idea of course is that we have all these different branches so I can go back into the SQL editor once again and run a few commands for the various branches that I see fit and it just is that fast it's it's so cool uh and it makes things really really easy and granted there's a lot of other configurations we might consider like new roles new database is these things get a little bit more advanced on the postgres side itself but overall there are other options that we can Implement now one of the main ones that we want to implement is well basically doing these things in an automated fashion and we will do that using the command line tool later the neon command line tool that will actually build up our branches it will do the branching for us it won't necessarily run SQL directly with the command line tool because that's not what we're going to be using it for instead you'll use a connection string to run the various SQL commands itself and then what we will also do is we'll implement the API keys so inside of developer settings here we're going to generate a new API and then use the command line tool specifically for that so that so much of what we just did will be automated you can also even automate the project itself which is something we're not going to do but we will automate the process of branching because it is a fundamental part that really just unleashes a lot of value that we can provide to to our development process right so whenever I need to actually have a Dev version or Dev V2 I can still use production data if I want to or I can just build out data that I need just for local testing and that data could be in its own branch of our original database and so then when we run migrations we can migrate each database as we see fits as in change the structure of the database whenever we need to on each branch as well so that parts also pretty cool but overall the idea here is if you actually let it sit for a little bit longer what's going to happen is these are not going to be active anymore and so that the actual databases will be ready to be used but they'll just become inactive and then actually reusing them or making any sort of call on these databases we'll spin them back up to meet the demand that you need as we'll see as well once we actually start implementing this database uh into our project so at this point go ahead and clean up the branches delete the ones that you don't need I'm going to be going just off of the main branch and we'll start integrating that into our project here in just a little bit let's take a look at how to use the neon command line tool to automate generating branches as well as getting our connection strings and then of course deleting branches when we need so the way we're going to do this is by starting from scratch we're going to create a brand new project I already deleted that project and then of course we're going to use the command line tool which there's a lot of documentation on it we're only really scratching the surface here and doing a few of the primary actions that I've been using using so to do this we want to make sure that the neon CTL is installed in my case it is and then I can also use just straight up neon and if you remember we used this mpm install DG neon CTL that was actually how I went about installing it now at one point I will use something called JQ which really just helps me parse Json data as you'll see and to install that it's Brew install JQ this is absolutely optional but it's one more thing that you could can potentially use to automate okay so once you actually have it installed you definitely want to run through neon CTL o now I already actually authenticated this I did all of this earlier so if you haven't done that go ahead and do that now and make sure that you have access to your local project now in my case if I actually go into neon CCL projects and list hit enter what I will see is absolutely nothing I actually deleted my project and it's really simple so to create one let's go ahead and create one real quick with a name here and I'm going to call this serverless and node js API now this name flag there are it's pretty straightforward as far as how to create something I go ahead and create it and there we go I get all of the data that I might want including our connection URI or connection string that we can use to actually connect to this we're not going to use that yet instead what we're going to do is just delete this project I'm going to go aad and copy this uh Restless rice here and we'll go ahead and do neon CTL projects delete and use that ID and that will actually delete that project itself so then when I go to list everything out again I get nothing right and so that's really that simple to create a project and delete a project being that it is that simple we need to be aware that yes you could absolutely accidentally D delete a project that you didn't want to delete but in our case we're going to stick with just this for now and so I have my project created and I already have this connection URI now what I can do is I can actually run neon CTL and connection string hit enter and this will give me that same connection string right and so I can use this within the psql the actual local client that I have I can use that connection string and it's psql not pqsl but we can use that connection string just like that and it will allow me to jump into this like something like select now and I can run that and it'll give me the exact time that it is and we can quit out of here in a number of different ways you can write out quit you can write out exit it's actually pretty flexible as to how postgres works these days so that's the connection string so let's go back up there it is and so I can use it in many different ways now the other idea is if we needed another Branch right so if we were going to create a branch let's take a look at that so neon CTL and then branches G and actually let's go ahead and do branches list first we can see all of these things so to me the format works much like kubernetes if you're familiar with that but the idea is you get the neon CTL or just neon the actual resource that you're looking for and then the action you want to do right so it's going to be get list delete right or create and then in this case so we've got branches and if we do get let's go and grab this ID here and paste it in we get that particular Branch if we go neon CTL branches create well let's go and create a new one I'll go ahead and just write out Dev unknown command Dev now just like with our project we use D name and hit Dev and then we get a name so the name and the ID are not the same right so we've got the ID here which what do you know that ID is also corresponding to the actual connection string itself but once again if I go to list out all of the branches I can see the various branches in here if I do neon and CTL branches git let's say Dev once again now I can actually call Dev right so when I go to create it I need to pass in the name argument if I don't create it I don't have to do that the other thing is when we create one let's go ahead and just not have a name argument at all and what do you know it actually creates one for us so it'll actually come up with our its own name and we can list all of those things out just like that so then if we want the connection string we can use neon CTL connection string and then pass in the branch that we're looking for so we can use the autogenerated one that the name and the ID look to correspond or we can use our own let's go ahead and use Dev in this case that Branch should be different than the main branch right and enough it is right so the user that's being created is not different and we could absolutely create a different users to actually run this connection which you can just verify inside of here as well there's these different roles in here that you can create for any given user that takes some extra that we'd have to do in the post side as far as their permissions and whatnot whereas this user has full permission to the database itself um but the idea here is now we can create branches really quickly we can create connection strings really quickly and then of course we can actually use those connection strings so in the case of Linux and Mac you can export let's say for instance our database URL we can set this equal to that neon command so neon CTL connection string and then you could do our branch of Dev something like that you hit enter that will run that command now I can use this connection string with the psql like this and let's make sure that we have this in double quotes actually not single quotes and there we go so now I'm actually able to connect and hide the database URL as that is one of those key things that you would want to do and so when it comes to postes itself the different clients actually downloading installing psql is something that I I recommend you do if you want to learn more about postgres itself so using that SQL editor like we saw before we had all of this stuff I can absolutely do that inside of postes SQL right so I'd go line by line to just make it simple we've got this example here we created that table and then I can go ahead and bring in some of those values again once again it's doing that and then I can select all and see all of those values right here so that's really how you can do the second part of what we were doing within the console itself of the SQL editor you would actually have to use a postgres client of some kind now we can do a lot of these same sort of things within the actual JavaScript itself we could call SQL queries inside of the JavaScript itself as we'll see here very shortly but the idea overall is that we have the ability to automate so much of this being able to create branches on a whim is really great because then when we are actually committing our code we could create those branches get the connection strings and deploy it wherever we need to with that stage and all that so it just really lends really well for Automation and staging purposes for many many different stages which is why I wanted to show you this now again this is foundational for what we'll do later but overall I recommend playing around with this and being comfortable with just deleting a assets whenever you feel like it so in the case of our project neon projects right so we can do neon projects list rather and we can go ahead and delete this project and then start to get right so neon project delete and you get that ID now in this case I only have the ability to create one project let's go ahead and just try create hit enter notice it creates one for me I try to create another one I got my limit exceeded right so I can really only create one project but within that project you can create multiple databases um as you could you know experiment with on your own in there as well so if you wanted a multiple databases what you would with multiple brand brch is what you would do is you would have multiple branches each branch can have its own database itself so the project had can have multiple branches each branch can have multiple databases or they can just have all the same sort of things uh so that's pretty cool as well so yeah if you have any questions on the neon command line tool definitely consult this reference here there's a lot that it's just a matter of playing around with it uh to get the most out of it and then of course downloading and installing some sort of post client so you can really get your hand diry with a SQL if that's what you're wanting to do and if you're not wanting to do that if you're wanting to get using JavaScript that is what we need to do now it's time to actually integrate our neon uh connection string that database URL and also the neon seress package into our nodejs application so we can do a lot of these things with now heavy JavaScript so let's take a look at how to do that now we're going to go ahead and integrate our node.js application with our neon postgres database the key part of all of this is the fact that we are using serverless so the way we approach it has to be from a serverless perspective and that's how we have to configure things which means that we're going to be using the very specific package of neon database serverless there are a lot of postes packages out there to integrate with node.js specifically no postes could be one that you would end up using but the important part for us as we've said before is that we are using seress and we need to use the neon version of that seress so the first thing that I want to do is I want grab that database URL wherever that might be so in my case you can go into the neon console grab the database URL that's right here or we can use the neon CTL connection string as well right so you could always create a new project if you don't already have one I'm going to go ahead and grab this connection string right here and bring it into my EnV Dev file not justv but rather just dodev so with this in mind I'll go back into index.js and now I need to start configuring fing out my database client so I'm going to go and create a function called DB client and what we really want to do is we want to return for the moment just the database URL itself from the process so I'll go a and return that and so now what we want is our DB client is basically equal to that so const um let's go ahead and call this our DB is equal to that database client that's roughly speaking where we're going with this so to actually make this work we need to start integrating the neon database serverless so this package itself and so at this point I already have it installed and if you don't just go ahead and run that npm install if you once again we already have that database URL we just set that up next we need to use it so this is how we're going to use it don't worry about the actual SQL command that's right here we're not going to look at that one instead we're going to go ahead and use these two items right here so let's paste this into index.js and before we go any further I'm I'm going to go ahead and use this down in the DB client instead so that's really what I want to return is that SQL command from neon and that's the point of having that so this DB client then I can run different commands directly from it itself so we could do something like con results equals to DB with these back ticks it's really cool that you can use these back ticks and you can do select and now just like we've seen before this should give us some sort of result that we can then pass in to our API request okay so one thing that's important to note is this right here let's go ahead and actually try and run this if you recall we have mpm runev inside of our package.json to go with our serverless offline specifically with the stage of Dev which is why I put it into EnV dodev so let's go a and run that again and so what we'll get when we actually go to the endpoint of Local Host 300 000 is we cannot use an import statement outside of a module so by default we do not have a standard module we have commonjs so it's just a slight difference which means that we just need to change this from importing to being a constant and using require and if you wanted to convert it to a model module you could but that takes extra configuration that we're just not going to cover at this time instead we're going to go ahead and use the commonjs which is what is traditionally been used by expressjs in a lot of these node.js applications but now that we've got that we should be able to use this just fine so I'll go ahead and refresh in here and now I get something it's not necessarily exactly what I wanted but it is something now why is it not actually giving me what I want to see this is because we need this to be all asynchronous so the database client itself we want to change this to being async which which means that down here we need to change our call back Handler for the route itself that also needs to be in a sync right so if you think of how any given route Works inside of expressjs it is also a function so we can say like our index route and by default it comes in with the request the response next and this function itself could then handle all of the things from that function it could go like this and like that right so we can absolutely separate these things out like this that's just not a common pattern to use with the nodejs application itself you usually just put the call back right in there and so putting a sync in front of it is the same thing as putting async in front of a normal function itself now if you didn't know that that's what's going on because now that it's asynchronous we can now await our database client and then within our database client to actually use it we can await it as well and to really follow follow a little bit closer to the convention you could also just call this SQL as well and so that kind of fits a little bit more with what you'll see throughout the documentation on the SQL client itself directly from the neon database server list which is what we do see in the docs itself that's this right here so as you can see it says await and that's how we're actually able to do that is by making everything a synchronous and so now we should actually see some sort of result that goes into the actual database itself so back into our Local Host we rerun this and we should see something that is coming from our database now the first time we run it it might take a moment because our actual database on the neon console has to be active for it to work and so that first little bit of latency does make a difference now I'm also using a database that's in Us East it's quite literally on the other side of the country than me so how fast it's gone is really really great so it should in the speed should improve quite a bit when I actually deploy this application into AWS Lambda and so we've got here as results that actually is an array here with some data and that that's just what happens from this particular command so we can actually unpack the results like this if I refresh that that should give me a dictionary value I might need to refresh the server itself um or what we could do in here let's go ahead and run that again refresh in here now now it's a dictionary value now I can actually just do results. now and we might have to refresh the server once again so let's go ahead and try that out and I refresh and there are my results this is now a timestamp that's coming directly from the database like we saw when we were in the SQL editor itself and we ran you know select now just like that it's the exact same command going to the exact same database and return R turning roughly the same value of course it's changing just for however long I need to actually look at this um so yeah now we are fully integrated with our neon database inside of our node.js application so that means we can start doing a lot of more interesting things before we do that I want to point out this right here is raw SQL so this command what you can do with a weight SQL now is you could write out your own raw SQL so if you wanted to experiment with how you know SQL itself you could create an endpoint that would in theory accept data to run various SQL commands in other words you can create an endpoint that handles a form very similar to what we see inside of the console itself that's not something I want to do I want to make sure that the commands I am running are exactly to the structure that I want them to be I don't want to have pure SQL commands running through in my application at least that's not how I want to design this one right so now that we've got this all integrated there is one more thing that I want to add in to the configuration for neon and this just has to do with the connection itself when we go into our deployed serverless environment I want to just change the configuration to use an experimental feature called Fetch connections cache and we're going to set that being equal to true this is for HTTP connections and nonp pooling right that's really what's going on here feel free to look at the documentation themselves but we're not going to have pooling in a serverless environment because that server is going to be scaled down so it's it's a little bit more difficult to pull connections when they aren't persistent it's going to cause a lot of havoc and issues so that's why we're going to be using HTTP connections and that's also why I put that configuration just to make sure that that's in there well before we actually go into production itself pretty great now we're going to do our first deployment of our node.js application of course using the serverless framework to make sure that it runs into AWS Lambda now to make this work I'm going to go ahead and use this command seress but instead of saying serverless offline I want to do something different so we'll give it a new script name of deploy and then we'll go ahead and use serverless deploy and then the stage we want to deploy and of course that stage is going to work very similar to the offline where it has maybe in its own environment variable so let's actually change the stage to Simply prod and so now to run this I'm going to go ahead and run npm run deploy and of course it's going to run this command in here one of the big benefits of writing the script in here like this is that you don't have to remember what different flags you're going to be adding for different deployment options in this case the stage of prod will always be the stage whenever I run npm run deploy and so what we've got here is we've got loading environment variables from EnV so it's going to look into the EnV here and then it would look into dodev if that was the stage so if we wanted to add in ourr stage we could do that just like that and so now let's go ahead and add in just another key in here so hello world equal to this something like that we go ahead and run the deplo deploy again and we can see the different environment variables that are coming through in here and so of course it fails to deploy it for a number of reasons one of them the security token is invalid so at this point I have credentials on my local machine but they're not the valid ones and so I also have this Us East one in here so two kind of major issues that we want to address we want to verify that this is the correct region and in package.json I can actually bring in that flag of region and in this case I'll leave it in as us East one now the question of course is going to be how do we know which region we want to use well it has everything to do with our database we want to make sure that the region is as close as it as it can possibly be and so to do that we can use neon CTL and we can go ahead and do projects list and we can see the project that we're using and the the region ID which is Us East 2 we can ignore AWS Dash because Us East 2 is the AWS region itself so we can update this region to being that region itself and then we can run deploy again and this time it's going to attempt to do it in the correct region which is exactly what we were hoping for so the security credentials we need to address but before we do that I want to bring your attention to something important and interesting so yes it is deploying using this.r right so it's going to use all of these environment variables and it's going to build something and go and do what exactly well what it's happening is it's building this dot serverless folder inside of our local repo so it builds this folder out and then creates this zip file here so what we could do is we could actually unzip this ZIP file and we could take a look at what happens so inside of my project here I can actually see what do you know it's actually everything related to my repo my main code in itself is zipped up and brought into here it's actually Mi missing the EnV files so it doesn't actually have those things in there which is pretty nice and fairly important right and it also doesn't have the doget ignore so it is missing some things but it does have all of the node modules in there so it's kind of like a point in time instance of the application itself but it's also showing me something in this state file and that is all of my environment variables now on one hand this is great it actually uses my.pro and it's just going to go ahead and do that and I can just deploy this application and in theory this will just run which is nice but the thing is this state file is well it has our database URL directly in the state file so this is a security con concern for sure because we will show you where this is going to end up in just a moment so to make it end up where it needs to we need to actually update our credentials so if you've used the AWS CLI before you'll know that most of the time you'll log in there and it'll create something inside of a. AWS credentials in your application and you'll see something like this now these credentials in my case are no longer valid in general but they're also not valid to actually do the deployment they're not valid to actually use serverless serverless itself needs its own policies and its own credentials to be able to deploy everything well maybe it doesn't need its own credentials but ideally with each project you attach different policies and then create credentials for them now if you're not familiar with this process I'm definitely going to show you but before I show you I want to just show you this reference that we have here for this I am policy so this is a policy we're going to be using for the serverless framework it's pretty extensive one of the things that's important is when you you actually use this policy you want to change the AWS ID to the correct one for your account which is something we will do in just a moment but the idea is once you actually have those the permissions in place then in your EnV for example you'll have something called AWS uh access key ID and then AWS secret access key cool so we need to set these up let's go ahead and create these now to do this I'm going to go into IIM and I'm going to create a user group this group I'm going to just call it serverless framework okay just naming it calist framework and then we'll go ahead and create this group Next I'm going to go and create a user and I'll go ahead and create this user as serverless framework user realistically I would actually probably want to give it the name of my project so serverless nodejs API just like that we do not need to give them access to the Management console but what we do need to give them access to is the permissions that will'll add to this group so I'm going to go ahead and add into the serverless framework I'm going to add this user into that group Next what I want to do is we can review and create this user so let's go ahead and create them so all I did there was create a new user and add it to the group that I just created at this point they have no permissions and they also don't have any security credentials so going back into our user we go into security credentials and we'll go ahead and create a brand new access key go ahead and create this key we'll hit other and then I'll go ahead and say next and this is going to be our uh we don't actually need a description tag it unless you really want to here are those access keys I'm going to go ahead and copy this one uh the first one is the access key ID the next one is the secret and we'll paste that in there the names of these have to be exactly this the actual keys are going to be different of course but the actual AWS access key ID it has to be in this format it has to be written just like this now you could use it inside of credentials as well but the reason I'm using it into. EnV is so it doesn't conflict with my system at large and so I want to once again run my mpm run deploy and hit enter so this time what it's going to do is it's going to attempt to use these environment variables credentials but one of the problems is it's actually loading these into my environment as well this is definitely going to be causing an issue very very soon but now our new error is a little bit different it says that it's not authorized to perform this data so the user I just created is right here so that is the user we want to give permission to be able to perform all of the things that serverless wants to do and the reason this is working is because it's in the EnV itself this right here so now we need to give those permissions we actually need to create that policy so again in the actual GitHub repo itself you can go through and see this exact same policy I'm going to go ahead and copy it the repo is right here um and it will be linked in the description as well and so in my case I'll go ahead and create this policy reference and it's policy. Json I'm going to paste it in here I'm actually going to add this to get ignore because I do not need to commit this so I'll add it to get ignore and what we want to do is we want to change this AWS ID here so I'll do a command F or control F and we want to do a finding replace and I'll do a colon in front and back of it so we want to change that all across the board and the way we're going to do that is by finding the actual AWS ID for our account inside of I am so if I go ahead and close this out for the user go into the IM am dashboard we can see the account ID right here I'll go ahead and copy that and then we'll go ahead and replace it inside of this policy and we'll just do replace all just to make sure that our account ID ideas in here now you could in theory use just a star here but I like narrowing this down to specifically the account in the case of this particular policy so that if I were to reuse it again in the future I actually review it before I use it um and that's the idea okay so I will say that the permissions themselves if you were wanting to remove permissions you totally could but that might cause problems when it comes to the serverless framework itself that you'd have to look into even more but now that we've got this policy let's go ahead and create it by going back into the I am into policies here create policy and I'm going to go ahead and click on the Json and we'll go ahead and paste this in and so now I've got that policy in there with I think everything that we'll need I did test this a number of times so let's go ahead and create this policy now and I'll just call this the serverless framework uh permissions policy and then notice that it has very limited access I'll goad and create this policy and we want to attach this to either our user or our group I'll go ahead and attach it to the group click on the group itself click on permissions click on ADD permissions attach policies and do a quick search for the policy we made so the serverless framework policy I'll go ahead and attach that now the important thing about using permissions and policies within aw is you can attach too much permission right so the admin access gives quite literally all access to all AWS Services which could get you into trouble you could accidentally provision way too many things and cause a lot of Havoc within your account and charges as well and of course that also means that you maybe accidentally exposed your access keys and all that so the policy we're using is really limited to what the serverless framework needs to do it doesn't have unlimited policies just a few which is why we created our own and it's why it's working in this way so at this point we now have all the policies in place to run what we need to run within our serverless framework but before we do I will say the policy itself if for some reason the policy is not working we have inside of the gist itself for the policy reference we do have a gist available for you uh that will have the latest uptodate information related to this policy and the servess framework so be sure to check that out that's going to be linked in a lot of different places uh but the idea being that the we use needs to actually work for everything we're trying to do here so with that out of the way let's go ahead and actually run our deployment here right so we go ahead and do mpm run deploy hit enter and so this will attempt to deploy this now with all those policies in place and so since it's going to deploy or attempt to I'm also going to go ahead and create another script in here and we're going to call this remove so deploying and removing are basically two sides of the same coin it's really just that simple to deploy it and then turn around and remove it I'm going to let this finish it might take a minute or a little bit more depending on our speed and everything that's going on in the AWS configuration uh but I'll go ahead and let this finish and then we'll come back and actually test out that deployment it's not quite done yet but it did say just a moment ago that it was uploading to S3 so since that just finished I want to actually look at S3 to see what that means exactly so if I do a quick search in here for S3 inside AWS we go click on S3 and then in here the buckets themselves I want to search for serverless and what we see here is our serverless application right there so this is going to be autogenerated for us that's what the serverless framework does it will autogenerate these buckets for us yes we could designate our own buckets but that configuration is a number of steps that we're just not going to cover so the idea here is we can click on this bucket we can click on serverless we can look at our project the stage that we're deploying the actual commit of the stage and then all of that same sort of data the Json State and all this so if I click on this state once again if I go ahead and open this I will see that there's my database URL and I also have my AWS access keys in here that's not great either so all of these things we we definitely want to move towards getting rid of and so we actually might have an error in our deployment because of our AWS access keys and sure enough by the time I said that we do so we definitely do not want to have these access keys inside of our deployment now I wanted to show you this error because it's related to ourv so we're kind of at this weird spot it's like well I need to use these keys but I also don't want to deploy them so how do I actually handle that well luckily for us serverless diagel has the ability to do this so what we can do is we can actually customize what environment variables will be submitted to our backend so if we do custom and then and then exclude we can actually put a list of enir environment variables that we don't want so like the AWS access key here and the secret key here we can do all of that and it's really just that simple every once in a while we might have to use something like this AWS session token and you know we also will get to a point where we want to remove the database URL we're not quite there yet but we will have that for now so with this in mind I have some changes that I made I'm going to go ahead and run this deploy again uh notice that these are still coming up so it looks like maybe I did something wrong and this should say exclude so I canel that out and we'll write it again now those environment variables are gone the ones that we just simply don't want anymore great so this is exactly what we want so once again it's uploading and we can see once it's done that progress so we go into prod here it's going to show us a new folder based off of that and it's also based off of the time itself so this one looks to be a little bit later so I'd imagine this is the more recent recent one and sure enough it it's already uploaded I can verify the state once again open this up and once again the things that I'm okay with exposing for now are exposed um and the other ones are not and so once this actually finishes we will actually test out this Lambda function um and we can also review the function itself inside of Lambda which is something I'll leave for you to just explore on your own uh but for now I'll I'll wait for this to finish and there it is it's fully deployed so if I go to this URL here let's open it up in the web browser just to make it a little bit easier to see it might take a moment to spin up but once it does it's going to be really really fast as we see here so there it is and of course we can also curl this out and get the same results from it it has the timestamp that's coming directly from the database itself which is what this is right and it also is using the environment variable for that database URL which of course is not as secure as we want so what we need to do is move this towards being a lot more secure than it currently is and that's definitely something we will do um in the next part but overall what we see here is we removed ourv variables we can deploy really simply using basically the same things we've been doing with the offline then our policy this is the part that's going to be the trickiest to get configured and set up and working correctly if at any point you need to change things you need to test what policy problems are happen happening you can always go into I am and go into the policy directly and let's go ahead and search for it Ser a list click on this since we have Json in here now if I go to edit this I can navigate to any block section so let's say for instance our S3 isn't working correctly like this put but bucket tagging let's say for instance that wasn't on there and it showed you there was some issue with that what you could do is go into that one single block notice that over here it says edit statement now I can actually look for various statements so like put um tag or let's go ahead and just type out tag and we can scroll down here and we can see put job tagging put bucket tagging put object tagging we can just add these things in really simply like that into this one statement and then it would actually adjust as as it needs to so this is a good way that you could really experiment with what's absolutely necessary when you go through and start building things out so for example let's just try this out I'm going to go ahead and get rid of this describe right here so I want to just refresh real quick um and I'm going to go ahead and edit out just one single permission so we can see what that error might look like right so we're going to come in here it looks like I'm still editing I'm going to go ahead and get rid of this describe star I'll go a and cut that out for the cloud formation stack itself we'll hit next and we'll go ahead and hit save changes I now removed a pretty major permission so going back in into my local project here I'll now run mpm run remove which will of course attempt to remove this and right off the bat I get not authorized to perform this describe stack resource so I can't even do anything here as far as deleting it so that alone will stop it in its tracks for any given user and so I can actually go ahead and come back and change that once again by going back into cloud formation and bringing that back in and that one single permission allows me to do describe which we could also verify by going into the statement itself we can go to cloud formation and describe as soon as you put describe there it will describe all of these things so describe star does all of these actions right here which is why they're highlighted and selected because of this if I remove that and come back and type out describe I can see that none of them are selected so it's actually kind of nice that I can I can go about doing that whenever I need to okay so we definitely want to bring that back in I don't need to do it manually I'll just write in that one single change save everything and now we'll go ahead and attempt to remove this and it might take a moment for it to be fully propagated as far as the description but right a bit right away it it happens really really quick basically and so getting in the habit of deploying and removing is perfectly okay because our code itself is being tracked by git so we don't have to really worry about our code our actual database itself is not being managed by serus at all the only thing that's being managed by serus in relation to the database at this time is the database URL but very soon that is going to be abstracted away to something a lot more secure but overall the idea here is being able to deploy and remove really really quickly at a moment's notice and having the correct permissions to be able to do that for your AWS user and so you could also practice right now just take a pause practice right now and actually remove access to this particular user and see if you can recreate access because it's fairly straightforward um but if you don't know how to do it then this is a good time to practice that before we move into securing that database URL a little bit more so let's take a look at that now if you're ready now we're going to go ahead and configure our database URL to be more secure by using the AWS systems manager so if you do a quick search for systems manager inside of AWS we're going to go ahead and configure this to work with our application and more specifically through the parameter store of the AWS systems manager now this is not the only place that you can store Secrets or variables that you want to encrypt but it is one of the best ways to do it so we're going to go ahead and create this parameter here and we're going to start off with Slash I'm going to say project name slash stage and then SLV name something like that that's kind of the format how I think about creating these parameters themselves so what I'm going to do is this is going to be called the serverless and nodejs API the stage itself we'll just use prod and then I'll go ahead and do database URL that's it so this path right here is something we will use in the future so let's go ahead and grab it into index.js I'm going to go ahead and do const and this is going to be my um database URL SSM pram and we'll pass that in okay so that's just the name that we'll have next up we need the database URL itself so I'll go ahead and use the Neon command line tool for the connection string and we'll go ahead and place that in here I'll copy that and we will then place that inside of our parameter itself so we've got our name we can use this standard tier we're going to use a secure string so it encrypts this data and then we're going to go ahead and use our current account we don't need to change any of these things you can use all of the defaults the next thing is just going to be the value that we want to encrypt and then I'm going to go ahead and create this parameter so it's encrypted at rest so if I go go and look at it it's going to be stored in here crypted until I decrypt it so this is decrypting it right there I'm just toggling that value and then of course if I go to edit it I can have it decrypted as well okay so it is stored encrypted and so what that will allow us to do is well a couple things number one we could abstract this to an environment variable now I'm going to leave it in just as a standard variable but since you now know how to create environment variables that would be a good place to potentially put it in your environment is actually using that database URL SSM peram and it's called SSM that's just the default name for it for the AWS systems manager that's the name of it as we'll see when we actually start using the package itself inside of our application so the main thing here is then I want to actually decrypt this data I want to be able to look it up and grab it so before I can look it up and grab it I want to grab the database URL out of the environment variables and so inside of myv I can leave it there but inside of the Ser list yaml I'll go ahead and just add it into exclude just so it never goes in and so at this point I wouldn't have access to my database client at all and that's what we want to change so to change it we're going to be using the AWS um client for JavaScript which is the AWS SDK and so we want to install this I'll go ahead and copy the install call here and then we'll just go ahead and install it onto our project it's really important that we do have it in our project as a dependency because it is an actual dependency for our runtime so the way it works is inside of index.js we can bring this in and I'll do const AWS equals to require and this is going to be the AWS SDK no surprise there then we initialize a SSM connection so there's going to be SSM equals to AWS SSM and then the region itself so region we will pass in here as well and once again I'll go ahead and add another con in here as AWS region and we'll go ahead and use Us East to once again you can have this as an environment variable that you might want to change when you need to the region and the parameter work in tandem together okay so now this is going to get all of the parameters or at least allow for me to get all of the available parameters inside of that particular region right so the parameter store in that region it'll show all of them that are in here this is a going to allow me to do that so the next thing is our actual database URL so now I'm going to go ahead and say const DB URL equals to well what exactly well to do this I'm going to go ahead and bring in the data itself the data from the pram store so let's just call it pram store data and this is going to be await SSM the initialized class right here and we also want this to say new because we are creating a new instance of that class so await SSM doget parameter and we want to pass in some arguments here the first argument is going to be the name this is the name right here the second argument will be with decryption and we want to say true we stored it as a secure string if it just says string we would not need to use with Des decryption it would just work then if I do promise I can turn this into an asynchronous call which is exactly what I want after I've got that I can actually use this neon Connection in here with the parameter store data Dot and this is going to be do parameter. value just like that and so this should actually now allow for us to make this work um and so obviously we need to make sure that this is actually working at least locally before we even push it into production because there is something else we'll have to change with our production environment so let's go ahead and do mpm runev and once again we see that debug is in here now that database URL is not in here and I'm not even using it anyway so there's nowhere in my application that has that database URL for the client itself so what I want to do then is open up the local host and let's just take a look let's make sure that I can still connect and still get that data and sure enough I am it's still giving me the exact data that we were expecting from the database itself in other words this parameter store data is actually working so we can console log this out and see that parameter store data as well uh just like this and let's go ahead and just refresh our application and reload the page and we refresh in there there it comes through and there is that data coming directly from the parameter store great so this same package could be used to set the parameter as well that that might be something we do later uh but for now we have the ability to use a production ready parameter for any given stage so naturally we could also come in here and do our constant and say stage and this is going to be equal to process. env. Stage or just simply prod right and so then we would come in here and just change this to being something along those lines and it should actually be right here and that will give us at least the ability to modify the stage once again this could be just its own environment variable itself so now that we've got this what I need to do is I need to prepare this whole thing for production to be able to just use this so one of the things about AWS Services is they can communicate with other services without a secret key or an access key they can just do it together and so in our local environment we have those keys in EnV now the only reason I can actually communicate to SSM with these environment variables has to do with what we did in the last part which was in the Imam so if we go into the I am and into our user groups into our particular policy that we've got on here we can see something important to understand why this user can actually access that data it has to do with this right here SSM and star now generally speaking you probably don't want to give every resource every access to this parameter store I did this as a quick and easy way to have access locally yes it would be something I would want you to change as well but I knew I was going to point it out so here it is I'm pointing it out and this is where we'd want to change it right here now so that any different project just doesn't have access to all of your parameters on your entire application but the thing is when we deploy this to Lambda it doesn't have permission it won't necessarily have default permission in here just like this so we need to actually add in the permission that it needs we need to give it the permission now one of the ways to do that is by inside of ser. yaml what we could do is inside of this provider here we can add in additional permission and the way we do that is by creating Imam and then we're going to create a new role this role will give it a specific name that starts with serverless I'll show you why in a moment as well and I'll give this serverless SSM roll and then the statements we need for this is effect and we want to allow resource and we're going to go ahead and say all and then I'll go ahead and write out all of the things all the actions I want to allow and so I'll just copy and paste them and we'll talk about them right now and where you can diagnose these things SSM get parameter what do you know we just did get parameter get parameters well that would be a multiple parameters by path and so on right so these aren't all of the permissions that we necessarily need need but it is all also opening up the resource everywhere so this role will play the same exact role as this effect right here and so if we wanted to get more specific with that we can go into edit here into our policy or any policy this is how we can start learning about those various permissions like we've talked about before we've got SSM here which is this systems manager if I get rid of this all let's get rid of that all call right there and just search for systems manager oops get rid of that comma and then look for systems manager as far as the service is concerned we can start seeing all of the various commands that we can use in here and this of course is going to correspond to the SDK itself how the SDK can be used so each one of these policies is important so get Pam we see that there what if we do create and we've got a bunch of different create in here right create resource right well we also might want put Pam right there that would be adding in one right uh and then maybe delete and so all of these different parameters that you might want to have for a resource is well this is where you'd find them this is the easiest way to find them in my opinion now of course we could just do all of them here but our Lambda roll certainly does not need all of that data the actual serverless role is mostly just for our user so going back to these serverless permissions I'm going to go ahead and abort all the saves and changes these permissions right here have nothing to do with our Lambda project itself right that's what this role does the only way it has something to do with that Lambda project is so that my local environment or really this particular user right here has access to do what it needs to do so these Keys these access keys and secret keys are not on the AWS Lambda by default which means that this code would not be able to work unless of course I were to add that actual roll on here and the reason we have this serverless Dash roll on here is so that we can can actually manage this so inside of our Json data inside of our policy here if we scroll down a bit and look for our role let's do a quick command and we look down a little bit for I am and we will be able to see that we've got some roles here so create roll put roll and so on notice the name of it right so we've got serverless Dash is how it starts which is exactly why I call this serverless Dash and then the SSM Ro right or I could actually say to be even more safe I could say my SSM rooll or project SSM Ro if if it does conflict with anything else that the serverless framework gives us but the idea is if I went in here and called it my SSM rooll like that I would have to update this policy for this particular user to adjust for that like it wouldn't be able to create it this is only able to create if the actual name of it starts with serverless great so now that we've got this let's go ahead and try and deploy this here so we'll go ahead and do mpm run deploy and what we should see is two environment variables working that's great I don't have any errors so far about the role itself uh but the main thing that we want to verify is the fact that the application once it's all said done is going to go based off of this database parameter that will end up coming through so I'll let this finish and then we'll check that out and so now I get this error of I am tag roll action I I don't have the ability to tag this role so it's not performed the tag roll so this actually gives us the opportunity to see how we can adjust our policy to make that work so back into our seress framework permissions policy we're going to go ahead and edit it I'm going to navigate down all the way to theam stuff and so what we've got here is get roll create roll and so on so in here I'll select it we'll do the included service and we we'll go ahead and look for the put and I think it was what put or no tag roll so we'll go ahead and do tag roll and there is the tag roll and untag roll so I now add both of those just so I have the ability to do that tagging and so we'll once again save this so we hit next here and save changes and then of course now I'll go ahead and run that deploy once again and of course that's how it goes this is how I was able to build out that policy in general and these things modify every once in a while there's little things that the serverless framework adds on that it needs access to do like taging a roll I don't think is so critical that it needs to have it happen but there's also things behind the scenes that I don't know that's going on that might have a good reason to be able to tag that role or have that particular permission um so yeah I'll let this finish and then we'll come back and here we go we got this success that it was able to deploy so I'll go ahead and grab the URL itself we'll just do a quick curl call it might take a moment for it to boot up completely all across the board but once it does it actually gives us the results that we were expecting so now what I want to do is simulate the problem with something on Lambda itself like if there was some issue on Lambda let me just go ahead and use you I'm just getting rid of the L there altogether right and the only reason for that is just to see what's going to happen with these results here and I also want to bring in something else and that is I'm going to grab this right here here I'm going to just go ahead and say now and then I want to go ahead and declare a Delta and this is going to be equal to dates. now minus now. now. time and then we're going to divide this by a th000 I want to see how long it takes for the application which is what this will do to actually reach the database and come back and that's what's going to happen with this new Delta here just so I can see what exactly is happened happening within the database so we can once we actually get everything working we will see a Delta so it'll be something that changed at least a little bit and so uh this is going to be our now. now or maybe our um you know DB now result let's just make it a little bit more of aose so it's not too confusing here and there we go okay so now that we've got this let's go ahead and do another deploy so I don't need to push it or anything like that I just run npm run deploy and it will start that process so what's going to happen is it's going to fail right so it's not going to find the parameter and it's going to be a major system error so what we're going to need to do is look for AWS Lambda we'll actually go into Lambda itself we don't need to type out AWS actually uh so we'll go into Lambda itself we'll click on here and we will see very soon where we will be able to navigate the problem with this and it comes into monitor this tab right here we can go to view cloudwatch logs and this will allow us to see some of the errors that might come through for this particular project from these log streams and we can see other logs as well like the things actually working but the idea is this log stream is how you might be able to diagnose errors with your particular project so in my case that that error is going to be this parameter is invalid and so the application itself won't attempt to use that parameter until it attempts to use the database client at least at this point right so realistically I think when I wanted to boot up the application I might then want to initialize the database client right away instead of always in the endpoint itself but those are optimizations that I don't really need to worry about because they're not really that necessary at this point but the idea being that we've got an error coming through and I just want to see if it's done and here we go we've got it done I'll go ahead and do a curl here um and we should get an error back invalid or internal server error so let's go back into our production and we've got two new streams here I'll go to the newest one click on that and we're going to see that we've got a runtime error right here and then we'll go parameter not found right so that's the actual error itself and this obviously means that a parameter name was not found I don't see anything else in here for catching the error or handling the error as in right here we might want to handle that error or down here we might want to handle that error in the actual application itself U but I wanted to simulate what would happen if there were an error with the parameters themselves so we could actually update and make sure that it's working correctly on our next deploy right and so this would be something you'd also want to test locally and make sure that all of that stuff's working uh but it is important to be able to also see the logs when you need to inside of a Lambda function and just a quick recap as to how to get there you go to the Lambda function you go to Monitor and you go to view cloudwatch logs and then You' be able to see all of the various logs in there including the metrics that are happening for the application itself and how quickly all of that's working and so once this actually finishes I do want to see that updated Delta I want to see what that value is because it's pretty impressive so while that's still building I'm going to go ahead and create a new uh we'll do mpm run Dev and then I'll go ahead and go back up I want to compare and contrast the production version versus the local one so if I do curl HTP Local Host 3000 I should get that Delta and it gives me a pretty fast Delta right this is local and then if I go into production do the same thing and it will take a moment to boot so the first one might be a little bit longer but then it's going to be just as fast or a lot faster uh it really depends on where you are located in the world and how quickly the Delta ends up working as well uh but overall that's it we are able to now check is how fast we can actually get the database call back to us a return call back to us have the database spun up and then also have our application now live and you know out there so we can we can use it in many different ways uh which is great so from here on out we just need to build on top of this because realistically we have the foundation we would need to build fullfledged applications we've got our database we've got a way to deploy it and we've got a way to secure it with the secur data itself um it's fantastic now one of the things that we're not going to cover are custom domain names so adding a custom domain name to this system it's not negatively supported by serus because there's a lot of different ways to put custom domain names on AWS Lambda there are some support out there for custom domain names but that's just not something we're going to cover at this time that would be the only thing I would want to improve about this project for fully production consumer facing projects if it's not consumer facing projects which is kind of what we're assuming this could go anywhere now we could actually use it anywhere just by using that endpoint itself and then making updates as we see fit now we're going to go ahead and decouple our application so it's a little bit more concise on how it works and it's just cleaned up then we're also going to update our AWS SDK we want to upgrade it to a newer version now this older version is still valid and it still works and it'll probably work for years to come but overall we want to start adopting the newer stuff because the package itself is probably smaller as well and maybe just a little bit easier to use than what we've got here with this promise and all that that's kind of a mess so let's go ahead and use the new package which is awssdk client SSM so we'll go ahead and install this into our package since we're installing that I'll go ahead and do mpm uninstall the AWS SDK I want to get rid of that one right away just to make sure I don't have that any longer so with this in mind I want to just change how I'm going to grab that parameter itself so to do this I'm going to go ahead and create a new module with lib secrets. Js and I'm gonna quite literally copy everything from this DB client stuff up and so this is where we'll change everything so I'm G to get rid of all of the expressjs things that we just simply don't need in here and this will be just be get database URL now and it's going to be roughly the same values here I do not need the neon stuff just yet we will decouple that one as well um but overall if I were to keep the old version this is how I would decouple it right just like that and then I would update where I need to use this database URL with the Imports as we'll see in a little bit as well okay so with these things in mind I want to go ahead and use that new SDK so what we can do is we can look in the documentation and scroll down a bit you can see the installation process we've already done that you can also see the import process is basically this right here so I'm going to go ahead and copy this and we'll go ahead and paste this up here now and so I'm going to get rid of this AWS here and to create the client we'll go ahead and come down here and say const client equals to well let's see how they do it in the documentation it says new and SSM client hey what do you know is very similar to what we saw before in fact we can pass the region in like we saw before as well the exact same way so that modification was already really simple so we can get rid of that and it's not a whole lot different going throughout so next one is going to be the actual pram data so I'll leave that in but I'll get rid of this git SSM parameter way but rather just turn this into an object itself then we need to actually create this as a command so we say con command and this is going to be equal to well what we've see in the documentation is this list associations command which is not what we want we want something different and there are a lot of different commands that you can run in here if you scroll down you will see all of them the one we need is of course the git param command so G parameters is in here so let's go ahead and grab that and I'll just search by it unfortunately it doesn't give me the actual command that I want by searching in here if you go into the API reference you will see it but what I know is I can just go ahead and change this to just get parameter command it's really just that simple so the command itself will then be new of that command with these arguments in here and then we want to grab the result from it with a wait and this is going to be client. send and that command and then the final data is just simply that results parameter. value and that's it just a SM a small modification to how this ends up working and then we'll go ahead and do module. exorts and the git database URL is equal to that right there great so what we could do now of course is change our DB client to work off these secrets now I could do it in the index.js or I can follow along this pattern of decoupling and decouple the DB client to its own location so that's what I'll do I'll do DB clients I'll use the S in here for a reason as you'll see once we get to the drizzle stuff now I'll go ahead and copy all this once again just because it's already working mostly okay and I'll get rid of the things I don't need which is serverless HTTP and express I also won't need this AWS part here instead what I'll do is WR under the neon import I'll go ahead and do con Secrets equals to require and then my new libr library for secrets and that's the one I'll use so I'll go ahead and get rid of all of the SSM stuff in here now and so really this pram data store is no longer going to be that instead we'll get rid of all these comments even I'll just do con DB URL equals to secrets. database URL and of course this is an asynchronous function so we go ahead and do await those secrets and now I can actually just use that database URL just like that so it's cleaned up a lot in both places if you ask me the database URL SSM pram I might put in here or maybe I have it in as an argument itself if I needed to change it and support other things uh but overall it's definitely cleaned up a bit so last thing of course is just to bring in our database client into the index itself so to do that we'll do module exports and the database client is going to be that database client let's go ahead and stick with that same naming convention we just did with the git database URL to get DB client and just change it ever so slightly and now in index.js here all we need to do is bring that one in as well this time I'll just go ahead and import the function itself and so we'll do const getdb client and this is going to be require and then do/ DB and clients just like that and of course the index file itself is not inside of Any Given folder which is why we can just do that so now that we've got that here is the old DB client which I'll go ahead and get rid of and now SQL itself can be just a wait get DB client right there and I can get rid of all of the old AWS client stuff as well as Neon right inside of this index.js because now it's imported in places that it makes a lot more sense so of course the last thing to test here is that this is actually working so let's go ahead and do mpm run Dev here and so there is my Dev server running and let's just do a quick curl call and we'll go ahead and do curl and after a moment or two because everything needs to spin up it go it does that exact call great so there's one last thing that I want to fix and that is this Delta here this Delta I don't think is actually accurate so what we want here is we want to grab now which is going to be date now we want to actually call this before we call our SQL command this will now be in the past where this should be in the present hopefully we're going to go ahead and change the ordering of this just slightly and I'll go ahead and get rid of the divided by a th000 here we'll save that and I want to just refresh real quick and run this and just see how quickly I can grab this data okay so let's take a look at the Delta now it should be a different number so this is still in milliseconds so we can put that back of SL a000 just to make it a little bit more accurate as to seconds uh but overall we just wanted to see something a little bit closer to how long it actually takes so I'm making this request from the other side of the world and it actually is still going pretty fast which is pretty nice um but with this in mind we're going to go ahead and do mpm run deploy I did say the other side of the world I meant the other side of the country um that's all so I'm going to go ahead and deploy it let that run there's one more thing that I might want to have in my scripts in here which is just going to be simply info so every once a while when you run deploy it's not necessarily going to always show the endpoint that you are using so using just info will give us that endpoint so we can see that by running it in here with mpm run info this will give us that data from the current one that's being deployed which is going to be this endpoint right and so the endpoint will or could change if you took it down and then brought it back up that endpoint will might most likely be different uh but I'm going to let this finish and then we'll just verify everything is working with our stuff decoupled and all that looks like it finished it was pretty fast this time so we'll go ahead and curl it out and I'll run that and there we go so the Delta is really really small right that's the point of what we see there and so the actual commands themselves the time stamp it's a little bit tricky to get an accurate representation of the time stamp that's happening but over overall we do see that the actual Delta is not really that different it's like almost instantaneous because of how close it is whereas when I was when it was on my machine it was a little bit longer and that would make sense because of how far away it is uh but it is very fast cool so now that we've got this decoupled and we're using the new AWS client it's time to actually start building out our schemas so we can actually get to the process of actually putting stuff in to our database now we're going to go ahead and implement the drizzle omm now OMS are great because they allow us to basically use our runtime language to manage our database in this case our runtime language is Javascript and it's going to be managing our postgress database and drizzle omm is going to handle that interaction for us so we can just write a bunch of JavaScript instead of writing a bunch of SQL and what it's going to do is it's going to allow us to insert data like you would with SQL it's going to allow us to retrieve data delete data anything you would need to query it but it also treats the data with the the same data type in other words if in the database it's an integer javascript's also going to treat it as an integer this is also true with time stamps and we've already seen that so using just the neon database client we actually saw this select now doesn't come back as a string it comes back as a date object itself which is how we're just able to do this in other words I don't have to parse it I don't have to like parse the date from here and build out some complicated parsers just to use the value from the database itself and that's the key here is we want to make it so it's super easy to use values directly from the database without making any major changes so let's go ahead and create our first schema so inside of DB here we'll go ahead and do schema. JS or let's call it schemas actually so schema. JS is where we'll Define our tables so this is what what we're going to do is we're going to do const lead table and for now I'm going to go ahead and just put a dictionary here and really think out what I want in a lead table table well I'm going to go ahead and say I want an email uh perhaps I want to have a timestamp right like a created timestamp and maybe I want a description maybe a source right maybe the first name maybe last name right and so these are all of the columns that I might want to store in my database now to keep things simple I'm just going to go ahead and just do email and timestamp for now but instead of calling it timestamp I'll just call it created at so the these are going to be the fields we will want to use and so we need to actually design these in a way that makes sense within the drizzle way of doing things so let's go ahead and comment it out for a moment and let's take a look at the drizzle documentation and how this is done so inside of drizzles you know om. drizzle. team in the docs if you scroll down here we've got these column types here for managing the schema this is where I like to start because well I want this to manage my database so we need to start with designing the tables we're going to design and of course you could reverse engineer this too if there were already existing tables but that's outside the scope of what I want to accomplish instead we want to just create the ones that we want to create so in here we're going to go ahead and look for a column type called email and we're going to be severely disappointed there is no column type email now this is for good reason that's because email is really just a string of data right it's just like abc abc.com that's just data that's just a string of data so validating that it's email is going to be something we'll do later for now we just need to treat it as if it was data and that is going to be text right so it's really just text itself so we're going to go ahead and follow this line of thinking right now to build out our lead table so I'll go ahead and copy and paste it in and we can't do it in this fashion instead we have to change it to const and then equals to require and that's how we actually do our Imports with this es5 or the Comm common.js module um and so the next thing is going to be our table so the the const table the actual variable name itself or the constant name in this case we actually want to keep it in as lead table the actual database table itself I'm going to call it leads so this is going to be my JavaScript name for the table this is going to be the actual database name for the table and what do you know this is also the same so the first thing is going to be our email we want to leave it in as email this is going to be the text data type or the text column and inside of there we're going to go ahead and leave the database as email as well so they are corresponded they're the same you don't always have to do that but that's one way to do it next we're going to go ahead and do our created at now within JavaScript I typically use this camel case and I think that's the best practice within JavaScript but in the database I might want to change it a little bit differently and use use snake case which is common inside of databases but you could use camel cases if you want as well so what I got here is a leads table but hopefully you have a little bit of a red flag going off or a little bell going off is like should I treat something like created as or created at as text well no there's another data type and we've already seen it we've already used it which is related to a timestamp itself so going back into our column types we can scroll down and we can see there's one called timestamp there's one called time there's one called date so I can actually use a timestamp itself and that's what I want so I'll go ahead and come in into my schemas and use a timestamp in here and so I'll go ahead and use that instead so we can also set a default here as to default now so default now works on timestamps it doesn't work everywhere else so if you wanted to set a default let's say for instance for a description let's do that now we'll just add in that new column I can add in a default here with just simply default and say this is my comment or something like that right we can really build that out on how we want to do it based off of the data T tables that we have here and so another advantage of using specific integer types has to do with doing calculations or running functions inside of the database itself so if I were to jump back into the neon console here I want to show you this in action so first off this select here I can run this and this will give me a bunch of numbers right it's going to generate series it's going to generate numbers from well in theory 0 to 100 but it actually goes into 11 101 rows so it does from 0 to 100 into however many rows that takes which is what that generate series does and now we can also count how many rows there are by selecting and running that and this is a database function to count up all those rows we can also use a database function to sum up all of those rows there's many different functions that you can do and you can also see how this would affect maybe different rows in there as well so if you did a 100 and you multiply each row by 1.2 that would give us that sum whatever that ends up being right and of course you can also do much bigger rows as well so this is where databases start to really outshine doing something directly in JavaScript you would actually do the calculation something this big you would do it on the actual database side and then you would bring it into JavaScript and so that's where these data types have get another reason to use them as the correct data type right so in the case of the time stamp this is great because then I could use the JavaScript stuff to calculate how long it's been basically when I want to display it but also if I wanted to do like the number um you know of entries I could do something like that and then that way I could then grab maybe an in a integer type so we come up here we've got an integer I could bring in an integer in here as to what that would end up being right and we could have this default being zero now the integer itself we would also want to import it okay so there's a lot of different ways on how we can think about writing out these actual tables themselves I'm actually not going to do that number of entries this way because there's a way to count it in the database as well with the email being the same uh so that's outside the scope of this one but these are generally the fields that I want to have but there is one more field I want to keep as well and that's going to be an ID field so every time I do an entry I want to have a new ID field in here and the way we do that is by using something called serial and we can set this as our primary key just like this and this will be our primary key for the database which helps with indexing and speed and all sorts of stuff but overall the primary key here and we'll go ahead and say not null in fact wherever I don't want this to be empty in the database I will go ahead and say not null the description itself it could be empty that's okay and so now I've got my actual schema that I want to use for my entire project itself I've got something a table with ID email description and so on so it's time to actually bring this in to our table or our database itself but the thing is I don't have a way to bring it in just yet so let's go ah and do module. exorts and Lead table and so what we've got here now is the starting of creating an actual table in the database but we don't actually have the SQL behind it so we need to create the SQL behind it and then we need to run or migrate that SQL and actually execute that SQL and so that's what we'll do in the next few now we're going to convert this JavaScript schema into an actual SQL migration file so to do this I want to start off with creating a folder called migrations now I'm putting it inside of the SRC because I really just want this all bundled in one place sometimes it's recommended that you just put it in your main project itself so with it in here with this folder existing and my schemas existing I'm going to go ahead and create a new file called drizzle. config.js so this configuration itself is going to be fairly straightforward we declare a constant called config and we set this equal to well our schema the location as to where it is and then where we want to Output this schema as in that migrations folder and then we're going to go ahead and Export this as a default so export default and the config itself so the reason I can do this is because the way I'm going to actually use this file is going to be right inside of package.json I'm going to go ahead and come below here and do something called generate and this is where we'll use use drizzle kit itself so drizzle kit will allow me to do drizzle kit generate colon PG as in postres and then config equals to drizzle. config.js or that file name and that file path so in this case the file path for drizzle. config.js is right next to package.json if you had it into the SRC folder you would do something like that okay so with this in mind I now have a way to call this file but I don't have the configuration correct so what I'm going to do is I'm going to copy this relative path here bring it in and what do you know then I'm going to go ahead and copy the relative path to migrations also bring it in and there we go so we can use slash if we need to uh but the idea being that it's a relative path to wherever drizzle config is and once again if it was in SRC we would get rid of SRC no surprise there okay so with this let's go ahead and run this mpm run generate and hit enter and all this is going to do is create our SQL file this is quite literally the SQL that will run to match this schema right here and if we were to change anything let's say for instance we got rid of this description here I want to generate that migration file and once again it shows me exactly what I need to run in order for me to match my schema to my database right so it's going to do each one of these things and that's it that's really just that simple and if I were to add a whole another table let's go ahead and add in a whole another table I'll call this lead table 2 and leads to whatever right let's go ahead and Export it just make sure you export it as well and then I run the schemas again now it's going to go ahead and show me hey what do you know it's creating a new table in here so since I haven't actually touched the database yet I can actually feel comfortable about deleting this folder if I try to run it again it's going to probably give me an error or it's going to actually do the entire thing itself right so the actual migrations folder was created this time uh but I've had it where it didn't create but what we see is it actually will now just combine those two commands into one and so this is where you can really learn a lot about SQL itself because well we're using JavaScript to generate that SQL so in my case I want to get rid of all that and I really just want that one single uh migration file itself and that's where we're going to leave it is just like that now there's one other thing to note is this meta folder in here is giving a snapshot of each change that's happening so it's really keeping track of the changes between these SQL files themselves so when we go ahead and run it that's how it knows not to do anything it didn't actually change anything because we didn't change anything in our migrations so that's actually pretty nice that it has a history of these changes the other thing about this that's really great is I can check this into G so I can keep those changes throughout and I can see all of the schem of changes that might happen so if at any time I need to roll back those changes I could and also I could run these migrations on well maybe a branched version of the database and make sure everything's working correctly before I actually run those migrations on the main version of the database which is yet another thing to think about when it comes to developing with the various schemas and of course drizzle om so at this point now that we can create or generate our migrations we actually need to perform those migrations and to do that I'm going to make my own CLI tool to actually make that happen just because it automates it just a little bit to work with our serous environment that we have and more specifically with our neon database and all of our neon client related things itself because as it stands right here this actually won't do the migrations for us we need to do a slight variation on this so let's go aead and and take a look at how to do that now now there's a few ways to perform these migrations one of them is you could just copy and paste it and just run it wherever you run SQL like for example in index.js we could run it like this we could go into the console run it there but those things aren't scalable and they're prone to a lot of Errors instead we want to automate them so to automate them I'm going to create a new SRC file called CLI SL migrator JS so this right here is is going to use TSX to be called so TSX SRC CLI migrator JS so we want to call it this way and the way it's called is by doing if require domain equals to the module itself we're going to go ahead and console log run this exclamation mark so obviously we need to fill out all of these things but before I do I want to make sure that TSX is even working so inside of my my package.json I want to go ahead and add in my Dev dependencies two things so mpm install D saav dev. EnV and then also TSX so EnV is going to play an important role as well as we'll see in just a moment but in my migrator here let's go ahead and just run this I'll go ahead and use this command itself into package.json and we're just going to call this migrate there we go so MP M run migrate hit enter and run this great so that's exactly what we wanted to see the next thing I want to check is the environment variables access key and secret access key we just need one of them really so in my migrator here I'll go ahead and do console log and process. env. AWS access key write it again ooh undefined now this is why we use the EnV so if I just run require and. EnV then config run that and run it again now we can see that environment variable and it's coming directly from EnV so just like with our serverless package we have EnV being able to be used our now CLI package can also load in those environment variables like that now why is this important well hopefully you realize it's important because well we need to grab our secret we need to get that database your L and so back into my migrator here let's go ahead and import that database URL by doing cons secrets and this is going to be equal to require and dot dot slash and this going to be lib secrets and then now I'll go ahead and do my secret itself so how am I going to go ahead and call this secret because it's asynchronous well I'm going to go ahead and do async function and we're going to call this perform migration and then I'll go ahead and do the const DB URL equals to A wait secrets. git database URL and then I'll just console log that DB URL okay so then in this main module here I can just do the promise handling and we'll say Val equals to and something like this and then catch error and let's go ahead and console log the error okay and so we can exit the process with process. Exit 0 this is successful we can also do process. one as in unsuccessful okay so um there we go now I want to just see this database URL let's go ahead and run it again and our database URL shows up so this at the very minimum is how I can actually use those environment variables now we might need to change them we might need to use different ones but it's just important to realize that if I did actually use my environment variables let's go ahead and change it just slightly and do export the access key to something like ABC now it's in the environment I'm going to go ahead and comment out to this EnV here and I'll go ahead and run this again and now it just shows me ABC and I get an error right so that is why we actually use this required. EMV but it is actually not required as long as the environment variables are actually in the environment it will use them uh but if if are not in the environment which in our case they will not be we will go ahead and add in that that way can we can actually get the correct data itself which of course caused this error still so let's go ahead and just refresh that terminal itself and create a new one and then npm run migrate like that and there we go so now we've got that data Okay cool so now we need to build out and use that database URL and to do this we need to use a neon pool or a postgres pool uh but the neon seris pool now the idea for this is to have multiple levels of a transaction that might need to happen which is exactly why we're doing the serous pool and so the repo itself for this package actually has some good documentation on how to do exactly what we need which is doing our pool with a websocket itself so this is what we're going to be doing I'm going to go ahead and copy all of this and we're going to bring it on into our code itself for this command line so the first thing is I need to uh you know take out the actual import statements and change them so they are es5 and we'll do require and like that and then same with this and here we go great so let's go ahead and tab this in a little bit so first and foremost the connection string well what do you know it's to this database URL it's no longer process EMV it's just to that database URL okay so we would have a console if there's an error with the pool itself that's something that well basically like we would want to just reconnect or try something new so the next parts of this are how we actually run the query itself and so the things that are important that we're really going to change is this right here right and so that's where we need to actually grab drizzle and change how we use drizzle itself so inside of here is where we're going to run our migration so at the top we're going to go ahead and first off do const and drizzle and this is going to be require and it's drizzle o SL neon serverless and then I'll go ahead and also we're going to go ahead and implement this down here so we'll do const and DB equals to a weight drizzle of this client and our schema so we need to also Define where our schema is going to be so let's go ahead and grab the entire module itself so right above secrets I'll go ahead and do schema and this is going to be our DB and schemas okay there's our schemas there next what we want to do is bring in migrate so I'll go ahead and do const migrate and this is equal to require and this is drizzle o rm/ poost scjs SL migrator okay so this is a manual way to call migrate and then the next part is going to just be await and calling migrate on the database itself and then using the migrations folder which in our case is in SRC migrations right right here so grabbing that relative path we'll put that in there just like that so that's it that's how we will actually run those migrations at the very end here we'll await pool. end just to close out that connection after all of that stuff is done so this is definitely in a lot of different places for the documentation uh but the key thing here is that we got our database and if of course if there's not a database so we'll go ahead and say if not DB URL then we'll just go ahead and return something along those lines uh but now we can actually go ahead and run this migration again and so let's go ahead and run it and at this time what it should do we should get rid of these console logs but overall what it should do is actually update our database and so we'll see that with I'll just console log in here like migrations done and we'll say running or run migrations get rid of this and then migrations error great okay so with this in mind let's go ahead ahead and run it again no big deal running migrations migrations done great so to verify let's go back into our neon console into our tables here and what we should see is our leads table inside of that leads table if we wanted to look at some of the data about it we see the different columns and their data types as we see here great fantastic so let's go ahead and take a look at our actual schemas themselves uh notice the ID is down here as well em created that let's go ahead and bring in the description and just do an endtoend example where we go ahead and run our generate and then we run our migrate okay and done refresh in here leads look at the description is right there great so now we have a way to change data in our schema change the design of our schema based off of different columns that we we might want to have then we have a way to actually migrate that schema now the client the pool itself can do a lot of different transactions at once which is why we use the pool itself and it does it using websockets so you're not going to do this in a serverless environment that's why we needed to create a command line interface for it you would not run these migrations once you deploy it to service you run it beforehand and so that's even more reason to actually get the production database URL as well or whatever one you're wanting to deploy to which could add in some additional arguments that you might want to have for that maybe you always have the the production database URL as a part of your secrets so in here maybe you would want to grab this and go get prod database URL and change this stage in here just to be simply prod now I'm not going to do that I'm going to leave it as is uh because that's really the only string that we are using as the production but you could also change the stage itself in the environment if you wanted to change it to something different getting that database URL assuming that it is available uh and if it's not it'll just go ahead and say you know it'll return it and not try to migrate anything so yeah uh the pool is great you can do a lot of Big Time queries with this pool if you were not deploying to serverless this is probably the method you'd want to use for your database in general for some of those bigger transactions that might take a little bit longer so cool now that we've got this we just need to start inserting data into our database using crud methods and then we're going to need to validate that data to make sure it's valid data uh but that's really the main things we're going to do as far as the actual uh you know schema and database related queries are concerned with our schema created our migrations done we are now ready to start actually creating and retrieving data from our database using drizzle and neon so to do this we're going to actually Implement a curl function so we use Curl HTTP col SL localhost 3000 and then leads that will be how we get that data which in this case it's not found currently then we'll go ahead and also use the same exact idea with grabbing post data so to do Post data with curl we dox say post we're going to go ahead and pass in a header this particular header is going to be content type of application Json which is very common for data that's using an API of some kind and then we pass in data in this case I'm going to go ahead and put single quotes on the outside of it and then double quotes on the inside so we'll say something like email and test test.com and then we'll use our endpoint right there so those are the two endpoints we need to configure so the first step of this is to allow our actual data from the expressjs application to work so the first one is going to go into index.js we're going to go ahead and grab this git path here and we're going to go ahead and just say leads so leads this will allow us to get that data so if we were to run that curl command after we restart our server here we can actually see that data come through or in this case it's just saying hello world now the post data is not any different than this we're going to go ahead and grab this and just do post so instead of dogit it's just poost and that's it so with that in mind we can go ahead and curl it again actually we will go ahead and restart the server one time and then I'll go ahead and press up a few times to that curl post and once again it says hello from path and so both methods are now working HTTP post and HTP git typically speaking you use the post method to you know create data or update it sometimes you use a different method than update but we'll just leave it in as create data so that's what we want to do here and the way we actually can create data is first and foremost we're going to change the Callback Handler to asynchronous and then we're going to go ahead and say that we want to grab the actual data that's coming through from this request itself so that means we're going to go ahead and say the email data is going to be equal to await requestbody we'll just start with request. body or R eq. body so what we want to see here is just echoing back maybe the body data in here and we'll just go ahead and use it's something like that so once again we'll go ahead and restart our server here and then I'll go ahead and do another curl command run this and there's our body data coming back but it looks kind of weird it says buffer so the reason for this is because natively this data is not going to just automatically be turned into Json data so to change it into Json data and basically to accept this application Json content type header we just change change our Express app just a little bit by doing app.use and express. Json just like that so with this in place again restarting our server what we'll be able to do is we'll be able to run this call again and now it'll actually Echo back that data to us which means we can now start to actually bring this into our database which of course is going to require us to do that and so the actual body we can go ahead and now say that's the data and then we can go ahead and say the email is inside of that data itself and now I can just Echo back that email great so here is where we need to insert data to the database so how are we're going to go about doing this now first and foremost in the DB folder here I'm going to go ahead and create a file called crud DJs now what we're going to be focusing is the C and the r of crud that is creating data and retrieving data that's it we're not doing anything more than that at this point but once you have this found in place you'll be able to use it wherever you need to so the main thing here is we want to go ahead and create another asynchronous function so async function and this function itself we're going to just go ahead and call it add lead or maybe new lead new lead might be better and then in here we just want to bring in the email itself and then I'll go ahead and console log the email okay so that's that function then we'll go ahead and create another function and we'll call this list leads and once again we don't necessarily need an argument here and I won't console log anything at this point and then we'll go ahead and do our module exports and do new lead equals to new lead and then module do exports and list leads equals to list leads now no surprise here to actually use this data back into our expressjs app we can go ahead and grab it so what I'm going to do here is I'm going to say const and we'll just call it crud and this is equal to require and this is going to be DB and crud okay so the reason for that is then in my leads here I'll be able to grab my data so let's go ahead and just say that our results are equal to a wait cr. list leads and then our results down here for the data itself which will be a single result this is going to be uh new lead and then we would pass in this data right here now my git command I need to change this to be an asynchronous callback as well and so that's the goal that we're working towards here basically just have a quick and easy way to add a new lead in and all that so of course this shouldn't be anything new nothing about this so far is really that challenging to do at this point hopefully if it is challenging then yeah this might be a little bit too advanced for you but overall what we want to do now is we want to use this schema to do those two things getting new leads in our database and listing things out now of course the first thing I actually need to do is add data in there before I can list any of that data out so how do we do this well if we jump into our client we have a database client but we don't have anything related to drizzle drizzle has its own database clients itself so if we actually go into access your data in query what we see here is this drizzle Command right here where it takes in a client and then it also can take in a schema so what we need is just the drizzle portion of this and basically we want to have a drizzle client itself so now what I'm going to do is change a new client function inside of my clients for my database and this is now going to be git drizzle DB client just like that roughly speaking and then the SQL itself is going to go from the other normal git DB client cuz we definitely need to use that one and all I need to do is I'm going to go ahead and grab in drizzle and it's going to be equal to require and drizzle omm and what we want within drizzle omm is going to be related to the database itself in our case the database is neon so we'll do neon HTTP this is coming directly from their docs and so we'll go ahead and wrap this SQL client into drizzle itself and so now we'll go ahead and Export that get drizzle DB client in here just like that great so that's the first step back into crud here now we're going to go ahead and do con client equals to require and this is going to be ourclients or we'll go ahead and say clients in here and now I'll do con DB equals to await clients. G drizzle DB client great so this is the drizzle DB portion and so what we see in the documentation is we now are at this part so we're close to doing something like this and the documentation in drizzle documentation for the querying is vest there's a lot that you could do in here so we're really just scratching the surface of what you can do but the idea is something not too dissimilar to this once we actually get into listing out our leads but before we get there I actually want to insert new leads and the way I'm going to do that is by doing const and schemas equals to require and/ schemas and so now with that schema what I'll do is I want to go ahead and grab in a result and this is going to be equals to await dp. insert where we're going to insert it in is related to the schema and the lead table all right so we're inserting it into that specific schema this again is from the documentation so if you go into the documentation under under accessing your data you can see this insert here and we see that it says DB insert this users corresponds to what we've been calling lead table right I just made it this way so it's a little bit easier for me to understand what's going on and to unpack at any time but then we see thist values here and that's the key of actually inserting the data is values and then what data we want to insert so in our case our schema only has really one thing that we're wanting to insert in here and that's the email but if we had other fields in here this is where we would do it we would actually bring in those other fields so what I want to do here is then this is going to be an actual object we'll pass in here that object we will unpack and then go ahead and bring it in to the values as well so email being email in other words once we have this data Maybe we're going to go ahead and say new lead object and then you would pass that in something like that uh but for now I'll just leave it very verbose so it's going to be the actual email itself so what's happening here is it's going to insert that data but then it's just going to be done so if we actually wanted to return data we can do returning just like that so that will actually return back the result that's come through in here and we can narrow this down even more if we wanted to to let's say for instance the time stamp or uh let's go ahead and do that we'll just go ahead and bring back timestamp and this is going to be the data based off of the schema again and the data that's being inserted here so timestamp and then dot uh let's go ahead and grab what the schema value is in our case it's created at so there we go so this allows me to return back that timestamp as some sort of result so we'll go ahead and return back that result and see what ends up happening there okay so now it already is in place in that actual location so I'm just going to go ahead and console well let's go ahead and Echo back the results I think I have something close to that I'm going to go ahead and just do results and results so just changing it slightly we're not echoing back the data but rather echoing back what goes into the database so with this in mind we want to actually call that that one curl command that we were doing just a moment ago with the post and then I'll go ahead and run that and of course it might take a moment for it to fully boot up and I actually might have to restart my server alt together uh so let's go ahead and try that once again and we'll go ahead and email put that email in there we get an empty reply from server and so if we come back in here I get this n column value okay so part of the problem that I'm currently facing has to do with validation right so is this valid data the other problem is how I'm actually passing the data into my new lead I just talked about it I actually turned it into an object and then I unpacked that object right here so we actually want to just pass in the Raw data right here for now and then we'll go ahead and undo that so now the request body goes directly into the new lead but we still need that validation of some kind so while this is working with this we'll go ahead and just run it and see if we get anything back from the server in this case I do get something back I get that time stamp back or at least what I've been calling a time stamp and notice that I get an actual array back and a list of items back so what's happening here is inside of this result it's inserting it into the database and then returning back a list of items so basically we will say then if the result. length is equal to let's say one then we're going to go ahead and return back the result zero right and then otherwise we'll go ahead and return back the whole list of results themselves or we could say null or just an empty list or something like that but the idea being that if this is a list then we'll just return back that first the index value in that list and so returning back a specific time step we can also return back so so uh new email or the actual inserted email and then just do something like this right and that will actually change back what we return as we can see we might have to restart the server but if I go ahead and try it again uh yeah let's go ahead and restart the server here and we'll go ahead and run this one more time and when we get back should be that email or new email basically as the object that's coming back to the you know expressjs application so great so we now have a way to insert data now in this case I actually don't want just one single thing I want all of the data the entire schema to come back to me and that's what's going to go into the front end but we could narrow that down if we were interested in doing that so now if I do it again let's go ahead and restart that server again and we'll go ahead and run this and what we get of course is all of the data including the ID in here so at this point it now seems that I have seven items in here based off of how many times I submitted it to the backend and notice that the ID is auto incrementing the created timestamp is changing as well the description has that default text that we had in here uh right there so the last thing that we would need to really do with this crud is to well build out this list leads so it's actually very similar to what we've got here except it's just using different database commands so when you think about listing things out you're not inserting in any data but you're going to be using the same schema so instead of inserting we're going to go ahead and use db. select and then instead of insert it will be just from and that's roughly all we need to do so I can go ahead and get rid of this values here this time we're just going to leave it as results and I'll get rid of this all together and so I'll go ahead and put that in there just like that okay very simple very straightforward and then we can also do something called limit we can call limit on here on the number that we want to send back and so now with this in mind we've got got our leads here there's our results I'll go ahead and give out results here as the results I probably don't need that message in there any longer on either one so let's go ahead and save that and this time I'm going to go ahead and just curl the one single leads so we'll just run curl which will be a get request by default I'm going to go ahead and restart that server and then I'll go ahead and run that curl command and what I should see is all of those leads coming back in the item of results right and then again if I were to submit some more I can curl it again and once again I'm getting more results so back into crud what we could do is a lot of different things to just make this a little bit more organized for example so I'm going to just add in one little quick element here and we're going to do uh const and this is going to be descending and we're going to say equals to require and this going to be drizzle or in okay so I'm going to just reorder this and then do just order by and this is a camel case here and what we want is the sending value of the schema data so we'll go ahead and grab this schema data here and then I'll grab the field I want it to be ordered by so in this case we have created at so descending value of created at and let's go ahead and restart that server again and then I'll go ahead and refresh this request and so what we should see is a reordering of it so ID of nine is coming first instead of three and so if we wanted to see this a little bit further we can also do like maybe get lead so git lead would be an individual item and maybe we're just going to grab it by the ID and so once again we're going to select that same table but now we can use a condition in here called where and within where we can actually import something from drizzle called EQ as an equal so we would say where EQ and this is going to be our leads table. ID is equal to the argument of ID and then we'll go ahead and Export this as well so get lead get lead get lead and these results should actually be a single result now similar to like what we saw with our new lead and once again I'll go ahead and grab this single result if it exists otherwise well go ahead and say null right so if it's not a single result then null now this should be a single result because of the ID itself and so back into our index what we could do is just try this out with Git lead and then some result that we are pretty sure is going to happen which is the ID of nine and so let's go ahead and restart our server and then we'll rerun our call our curl call and then this should give us that one single result back and sure enough it does right so this wear Clause is also very powerful because it can filter things down in this case it filtered it down to just one thing where it's equal to this and this if you're familiar with SQL this is a very SQL like command that's happening here and of course if we only wanted let's say for instance the ID in here we could do table. ID and put this in as ID and like that so this is selecting the ID from that value and then we can curl it again let's go ahead and restart that server again and run that one more time and so that would actually give us just that data and that would be true whether or not it's an individual item or a listing of items and so drizzle gives us this really powerful way to perform SQL without writing a bunch of SQL commands but they're SQL like so it's getting closer to what you would need to do to write the SQL commands yourself and so while you're learning drizzle you're also learning a lot about SQL without writing a bunch of of it and it's really just that straightforward so at this point I will challenge you to go into the documentation and just find how you can play around with this data more because there are so many different queries that you could write and then of course you would want to update data and delete data as well but with all three of these we have a foundation to how you might end up doing all of those things uh so I do encourage you to read more about the database itself but at this point it's actually pretty powerful where we're at we can definitely improve it but as far as an API is concerned to store data and more specifically email data this is it we can absolutely do it but we are leaving one critical piece out and that is of course this data validation so that's something we need to do now let's take a look at Zod now we're going to do some very basic validation using Zod now you can absolutely make this more robust by using drizzle Zod which actually corresponds really closely to our schemas that we've already built and then even more robust is now we're going to go ahead and use Z as a way to validate the data we have coming through now eventually you can make this now we're going to go ahead and validate our data that's coming through from our request so I'm actually going to change this data to post data in instead and then we'll go ahead and grab the data itself from somewhere else which will be a wait and we'll call it validators do let's say validate uh lead and then we'll pass in the post data there so naturally we need to bring these things in and actually make them happen so to do that we're going to go into our DB and we're going to create something called validators JS and then I'm going to go ahead and generate some sort of validation function so we'll go ahead and do async and we wanted to call call it the function of validate lead so I'll go ahead and do that function validate lead and it's going to take in post data and then it's going to return some stuff and we'll do our module exports and validate lead equals to validate lead so the things we want to return is for sure the data itself the actual validated data right that's a key part of this now I also want to see if there's an error so I'll go and say has error and then maybe a message now the reason for that is then I can change the respon response based off of those two values so in other words if the has error is true then we're going to go ahead and return one type of response and then if the otherwise we'll go ahead and say else if the has error is equal to undefined then we'll go ahead and do another response now if it's undefined I'll go ahead and just give a message of server error and has error the actual status code will be 500 and then if it does have an error in general we'll just go ahead and say 400 and I'll give it a message based off of the message that's going to come back or we'll go ahead and say something like invalid request please try again okay and then down here with the new lead itself assuming that that actually worked and we didn't have to do a tri Block in there uh we'll go ahead and change the result to being 2011 because that's typically what happens when you create data the status code will be 2011 so with this in place we need to build out this validate lead function so the first things is are how we're going to respond back and that's going to be has error true and then message just being an empty string so the way we're going to validate this is by using a package called Zod now Zod itself is very popular and it also has a lot of other features to it as well as Integrations to drizzle and then maybe even more robust validation messages with this Zod validation error but what we're doing is something very straightforward we're just validating the email we just want to make sure that that's working so when we do insert data into our database it's valid data and it's not invalid data so to do this I'm going to go ahead and import or actually install Zod so we'll do mpm install Zod and then inside of my validators here we'll do const and z and this is going to be equal to require and simply Zod so what I would normally do is I would probably put this schema the actual validator and the schema together i' put them right next to each other because they're based off of each other but in this case I'm just going to go ahead and put it in this validate lead function and so that means I'm going to go ahead and say my lead object validation something like that or just simply lead equals to Z doob and then we can start to design or format what we want to validate in here and so what it's going to what's going to happen is we can put any sort of data in here we we can put a name we can put an email we can put a description and then in here we can put those things that correspond so like name email description we can fill these out more but the idea is what you put in here might correspond directly to what's inside of the lead itself so in our case it's just going to be the email itself so instead of name and description we're just going to use email and the way we validate that is by having an actual dictionary an actual Z object in here and that means z. string so we definitely want it to be a string in here so it's not going to be a number that's for sure and then we can use email as well and so using email is another method that's available to us because of Zod so if we look in Zod and do a quick email search in here we can actually see some validation that's going on and there are a lot more places where we can actually call email and you can actually see that in the Baseline validations so we can also validate the size of it we can make it bigger or smaller right we can validate the length of it which is similar to that uh we can do what it starts with what it ends with if we wanted a certain domain name um there's a lot of different things on how we could go about doing the validation itself which is beyond the scope of what we're trying to do here I really just want some simple validation and so this is really designing the way we're going to validate it then when we actually want to validate it we will use lead. part so let's go ahead and say valid let's do valid data equals to lead. parse and now what we're parsing is we're actually going to pass in an object of some kind so this needs to be an object which in our case it is it's the request post body object itself and of course if this is incorrect it's going to run a validation error itself so I'm going to go ahead and run catch and error so we want to put it into this catch statement like that and basically what we would do then is say let has error right so we'll leave it as undefined and then I'll go ahead and say has error equals to false and then has error equals to True right so if it doesn't have an error then we will you know set it accordingly so the error is only going to come when we try to parse out this data so this valid data really is actually going to be let valid data and I'm going to set this equal to just an empty dictionary and basically that's what's happening here this parse call is going to parse out all of the data that I actually set inside of here so if the post data had a name for example it's only going to validate the things that I declare and it will only return the things that I declare so that now that I've got this valid data here I can actually pass it in and then we'll go ahead and just do let message and then I'll give it a message of equaling to an empty string here and then the message being invalid data invalid email please try again okay so now we've got our validations and our various validation errors so let's go ahead and try to run this now so mpm runev and then I'll go ahead and do a curl call in here just like we've seen I'll go ahead and run this I get an empty reply from the server so let's go ahead and see what's going on my validators is not defined so let's go back into this page here and we'll go ahead and valid or actually bring in our validators the entire module let's make sure we exported it and it looks like we did so let's go ahead and try that again we'll rerun the server and then I'll go ahead and trigger a call and I get invalid request please try again so that invalid request please try again is coming from this right here so the message doesn't seem to be coming back from this validate lead so let's go ahead and see why let me bring this back here so we got our message and message ah there there it is right here I need to go ahead and pass in that message so we'll save that and let's go ahead and refresh that server again and then we'll go ahead and come back in here run it again and now I get invalid email please try again so let's go ahead and do an actual valid email by just putting this at the at sign hit enter and now what we should see is of course the database is going to have to spin up and then we see this so the nice thing about this validation is it's actually not going to hit our database until it's actually valid data which is another reason to do this and of course this is sort of the basic level of doing this kind of validation the more robust and more advanced level would be using drizzle omm because you can build out schemas based off of other schemas and what I would actually do from here is to modify my schema of validation that I've been doing and do something along these lines where insert user schema and you could import that in this validate lead if you'd like or you could just just use the schema directly in the view as well but the reason I like doing this validate lead here is I can actually have a little bit more robust methods but of course there's nothing stopping you from actually putting in the post data itself uh so yeah that's how we can actually validate this data and we do it using Zod feel free to make this more powerful and also research a little bit more about Zod itself on how you can just do that much more validation to me the validation is as good as you can actually use it as as you can actually implement it right so I could definitely spend a lot of time trying to figure out how to give a much better message than invalid email as in the validator itself actually will give you better validation errors than just invalid email right so I can actually see what those messages would be directly from the validator itself so let me just show you real quick if I restart this and we do an invalid data again I run that again it says invalid email if I go back into the node I can see just exactly what different validation errors are going so yeah we can absolutely make the messages themselves much more robust than what we've got here but to me it's like we better just validate the data and then provide some sort of message that makes sense as well so another thing that you could consider doing in addition to validating the data is you could also check to see if it's already exists in your database right if it's already there and the way you would do that is very similar to like what we were talking about with crud where it's like listing out the leads but more specifically looking where the email matches the requested email and if that exists then we run another sort of validation error which is something that's outside the scope of this but we're working that direction the key of all of this with whenever it comes to inserting data from the Internet is you almost always want to clean that data and validate it before you want to send it into the database the database itself will help with the data types whereas the validator itself will help us with these kinds of messages and prevent as many database server errors that they could possibly be because of poorly formed data uh so yeah validators are great and fairly important so yeah hopefully you start using them now uh it's pretty simple and of course it can be a lot more robust thanks to zods and its powerful tooling so let's keep going now we're going to go ahead and automate the process of staging our application so like the dev stage for example when I make some changes with the code I want to be able to deploy it to GitHub and then GitHub automate the process of deploying the application on AWS Lambda but also branching our database at that point in time so we can work off of that and the way we're going to do this is by of course updating our secrets this is the key part we actually need to update our secrets for that stage so if you think about this in seress diagel we have our production environment going through and this database URL well we actually don't use that anymore so we're going to be gone altogether with that one so what we need instead is really the stage itself and so this is going to be based off of an environment variable that we will then later set but the default stage is going to be prod we're going to use the production stage as default now the reason for this of course is going into our secrets here where our database URL is actually stored that's where that stage is being set right so then in my get database URL I can actually modify what stage is going through so in other words in my Dev stage I can add in stage of Dev and get rid of this database URL right here and get rid of it basically anywhere so we don't accidentally expose it when we don't need to so of course our AWS access keys have the ability to get that database URL that's stored in this parameter of course one of the things that you're thinking about hopefully is the fact that we set this parameter manually so we actually went into AWS to set it manually that isn't automated it's quite literally the opposite so what we need to do is we need to automate the process of not just the stage itself but also setting in that database URL now luckily for us actually using the neon CTL it's really easy to Branch the database so if we look at the branches that are available uh we can see that with neon CTL branches and list we can see all the branches that are available and right now it's just the main branch so if we want to create a new one we could do it like this create name and then like Dev right so that gives us back this connection string here and of course if I wanted to use that connection string or just the connection string I can use neon CTL and connection string and then the branch itself of Dev and so that's going to be the exact same connection string as we see and so this one we want to treat as ephemeral we want to be able to delete it at any time and then Branch it later at any time both things we want to be true but that presents A new challenge with our production application and when I say production I mean one that's running on AWS Lambda regardless of the current stage itself so we need to be able to automate the process of updating our parameter store and luckily for us it's actually not a whole lot different than getting data so we'll go ahead and do that now and let's go ahead and put data we'll do put and parameter command and that's the command we're going to use now it's almost identical to this git database URL so I'll go ahead and paste that over and copy it and so the idea then is the param path needs to change based off an argument so we'll go ahead and do the put database URL and there's going to be a stage that we'll pass in here and I'll just call this cons Pam stage which is going to be equal to the stage that's coming through and otherwise it's going to just be simply Dev what I want to check right away is if the Pam stage is equal to the prod then I'll just go ahead and return I don't want to change the pro the production stage ever right I don't want to change this automatically that's something I'm happy to go in manually and change if I need to and so this pram stage is now going to be what the actual path is for this item or really the database URL name which of course comes down here so we go based off of that for am stage and now we'll be able to modify it in just a moment so this put URL database URL we actually need to have the DB URL value as well and so that's going to be part of the arguments we'll end up using everything else is pretty much the same of course if we wanted to change the region based off of our environment variables we could totally do that I'm going to leave it in as just the default one that we've been using uh so because I'm not really going to be changing anything else but the idea here is the other parameters that we want is the value so we use value here and we'll pass in that database value now once again if that value does not exist then we'll go ahead and return this element here and basically it's incorrect it's not going to work um so we still want to use this decryption but the way we do that is instead of with decryption we just declare the type itself and this is going to be secure string and of course we need to put that in the string itself so the idea of secure string is of course coming directly back into the console itself when we did create a new parameter here we have these three type options here I'm just using secure string and all of the other default options in there yes there are ways to modify this as well so feel free to look at the AWS SDK documentation for that next up the final thing is overwrite and I'm going to go ahead and say true so basically whenever I want to put a new stage database URL I want to overwrite the value that's currently in there just to make sure that that it is my latest value of course there could be issues and errors with this process which is why we don't do it with our production database we only do it with everything that's not production everything that's not the main stage that we're using so of course the final commands are basically the same but instead of get parameter command we'll use put parameter command in there and then I'll just return the result we'll save it just like that so before I go any further I can I will tell you now we can also do something called delete parameter command where we can actually delete and basically undo everything we just did and it's a little bit closer to this git URL where you just basically use the name itself but the reason I'm actually not going to be deleting the command itself is it makes it a little bit more complicated but also since we are going to be overwriting the database on a regular basis it doesn't really matter if that's the correct connection string or not so I'll go ahead and just leave it in as just put next up we'll go ahead and do module and and export. put database URL we'll go ahead and Export that out so we can start the process of automating it now the way we're going to automate it this is by using a command line tool we're going to make our own command line tool so we're going to copy this migrator one and I'll go ahead and just call this put secret. JS now this is actually more like put database secret so maybe you want to change the name of it uh but I'm not going to change the name now first off the migration command we don't need this function anymore we already have a function we just created that function on what we're going to end up doing and so I'm going to go ahead and get rid of all of the things related to my gr now the main reason I actually copied that is to remember I definitely do need the EnV configuration because remember in order for me to use the AWS SDK itself I need to have the environment variables for the AWS secret access key and the ID both of those things I need in there which is exactly why we've got this command line tool going and why I like copying these other command line tools I use the same sort of stuff um and so now the question of course is like what are we going to put in here so before I do that what I want to do is I want to declare args and there's going to be process. argv and then we'll go ahead and do slice of two now the reason I have slice of two is because the way this works is going to be the same as the migrator but of course instead of it being the migrator it's going to be put secrets and then we'll go ahead and do something like stage and then the DB URL so this is going to allow me to get everything after like basically these two items here and so all I want to do then is just check if the rs. length is equal to two then we're going to go ahead and do process. exit one that's it so we just want to exit it and say something like along the lines of console log and usage is that simple enough okay so now let's go ahead and try and run this one I'll copy the relative path here or basically run TSX so I'm gon to go aead and clear this out we'll do npx and then TSX run all of that I'll start out with hitting enter and it'll say run migrations so this is a problem this should be not equal to two let's try that again we run it again and now it's giving me the usage itself so we'll go ahead and try Dev and then sum DB your L hit enter and now it says run migrations which of course should be you know update secret right something like that there we go so how are we going to update the secret well we're going to grab the arguments themselves and the way we do that is by doing con stage and DB URL and it's going to be based off of those args so this is of course is unpacking it based off of the position it's in which means that if we accidentally do the incorrect position we've got some issues here here so this might be where you want to check if the stage is equal to like Dev or something along those lines and then go ahead and run everything else I'm not going to do that I'm just going to assume that we can have any stage we want in any database URL we want so we'll go ahead and set the actual Secrets now from our new Secrets function so to do this I'll go ahead and say const Secrets or Secrets equals to require and lib and then Secrets okay and we'll do secrets. put database URL what do you know we can put in our stage and our DB URL here so the idea here is it is an asynchronous function so you want to do a wait but the nice thing is with asynchronous functions is we can actually treat them as promises so we can just do then and we'll go ahead and say value and we'll do console log secret set and with that value whatever that might be so we go ahead and do value and then put these into back ticks and then process and exit being zero because it's successful and then we'll go ahead and do catch and in this case it's going to be error and once again we'll go ahead and run something similar to this but now secret not set and the error and we'll go ahead and exit at one okay cool so now that we've got this let's go ahead and set a secret I'll just go ahead and run this value here hit enter and we've got update secret secret set object. object so what is the object that's actually being logged in here so we'll go ahead and do console log value and then we'll go ahead and do Secret set and let's just call this updating uh you know database URL let's try that again again and we run that and here's the value that comes back now this is the default result from the actual put database URL that's what this is this is right here so we can actually see that the status code is 200 which is successful right of course we could also then get the result we could quite literally use something very similar to what we did with this git database we could quite literally go and grab the result itself to get the actual value but it's not necessary really we just need to know that the status code is 200 and I'll let you unpack that and handle those errors later um but overall we just want to verify that that secret is actually in our parameter store now so in this the correct region that I set it in I'm going to go in here and what do you know there it is and so I'll go ahead and scroll down here and debug the value or decrypt the value and we've got our sum DB URL here so it's definitely working it's actually putting that database in there and every time I run it it creates a brand new version of it so I can refresh in here and we you can see that version and that thing is actually changing so if I change it again and refresh in here we can see what that is so actually the nice thing about this is there is a way to get different versions of it as well so if for some reason in the future you want to be able to change this to different versions you could totally do that uh which is pretty cool so now we actually need to put the correct database in there right so again using the same idea we're going to now do our NE on CTL and this is going to be our Branch or rather connection string and then we want to get the branch of Dev okay so this is going to be that connection string the way I'm going to automate this is by putting it into a export command now if you're on Windows this right here is not going to work for you until you go into GitHub actions there's a way to do it this is just not it anyways so we're going to export this as our uh we'll go ahead and do this our database what's call this our stage DB URL and this is going to be grabbing it just like that okay so now I can use MPX right here Dev and then use dollar sign of that and oops not dollar sign equal dollar sign let's get rid of that dollar sign equal dollar sign and now we've got the updated database version six let's go and refresh in here and so now we should actually see that database URL and it's there so of course we just need to verify that that part is actually working and we can do that thanks to of course this process. env. stage and our EnV dev has that stage as well so let's go ahead and run our local mpm run Dev our local application version our offline version notice that's that stage is coming in there so one of the things we might also want to check maybe is on our homepage is putting that stage out there so let's go ahead and grab that argument like this and we'll go ahead and put this on our homepage just to make make sure that we actually are using the correct stage in that environment so let's go ahead and restart that run it again so I'll go ahead and open up local host and so what we've got is that stage of Dev so what we can see then is we can actually start adding in some data now of course inside of the neon console we can verify this as well so going into the tables we see that we have our main branch and our Dev Branch so let's go ahead and add a few leads in here and of course the way I'm going to do this is by using that curl command so let's go ahead and I'm going to copy and paste what we used in that last part uh but there we go I'm going to run this a few times and then I'll jump back into my neon console here and we should see all of those new leads and we do so if I go back and change the branch to main something like that and also see those new leads they are not there so it is now working on that branch which is great so of course the last few things that we would really need to do is to push this into production but one of the things that I also want to make sure that I'm doing when I actually run this is inside of package.json perhaps this is where I'm going to want to run that command just to remember it um but the thing is since I'm going to be automating it what I'm really going to be doing is using this inside of GitHub actions itself so this is going to be a GitHub actions CLI command that will run when we get there uh but overall this is fairly straightforward on how we can set the staged URL and set the branch so of course the last thing that we could really test here now is to delete that branch and do it all over again so we go ahead and do that with neon CTL branches and list right so let's just make sure it's in there I'll go ahead and do branches and delete then the name of Dev all right and maybe just Dev try that again there we go and so we've got our deleted Branch now so let's go ahead and create a new one neon CTL branches and create the name of Dev okay so I'm going to go ahead and scroll up a few times to get that database string again to our environment variables there and then I'm going to go ahead and use it once again like that so I'm putting that secret on there updated it's number seven now we can verify that it is a different branch let's go ahead and take a look in the parameter store and we'll go ahead and refresh in here and so I'm going to do a quick search we got EP soft Glade and we got EP soft Glade great so let's go ahead and refresh our node again and let's go ahead and add one more so if I remember correctly off the top of my head this is going to have 13 or 14 items so this should be item 13 and we see that with ID of 13 we do it one more time we got the ID of 14 and so now that same exact branch that we originally tested on is gone and if we look at the new Dev Branch we we can see that it's down to 14 and of course the time and all of that has changed great this is awesome so the next part of this of course is now turning this into a GitHub action that it does all of that for us so we don't actually have to touch it anymore uh so let's go ahead and take a look at how we're going to do that now now we're going to go ahead and use GitHub actions to automate our deployment stages first we'll automate the production deployment and see how that works then we're going to go a little bit more advanced and use a different git Branch to deploy a different stage the dev stage itself once it's deployed then we will go ahead and submit a poll request to the production Branch to have that run then at that point so I actually already started with some of the workflows themselves so if you were interested in going from what we have now just go into the workflows on GitHub and we're going to go through these things right now so the idea behind GitHub actions is it's a serverless way to just run some code the code we run is really up to what we Define in these GitHub actions so in our case what's happening in this production one is it's going to go ahead and grab a copy of our code it's going to set up no. JS and specifically version TW 20 and then it's going to go ahead and run our mpm install and then run our mpm run deploy which of course is this Command right here and so that's it that's the production stage itself now one of the dependencies this production stage has is the actual environment variables from AWS so let's go ahead and start this off right now let's go ahead and put that into our GitHub actions repo so inside of our settings here there's a couple things that we want to do the very first thing is under GitHub actions we're going to go into General we want to go ahead and scroll down and allow for workflow permissions to read and write permissions as well as allow GitHub actions to create and improve poll requests those things are important for what we'll do in the staging process the next thing is we want to scroll to the secrets and variables in the GitHub actions um section and then we'll go ahead ahead and scroll down to repository Secrets this is where we're going to put in our environment variables okay so what we want to do is these two in variables here so the first one is going to be the access key ID I actually want to create a brand new access key ID specifically for GitHub actions so inside of I am I'm going to navigate to my user here into security credentials go into access Keys create a brand new one we'll go ahead and say other next up we'll go a and give it a actual description this time so GitHub actions secret basically or GitHub actions workflow basically and we'll go ahead and hit create we're going to grab that access key there and we're going to bring it into our GitHub action secret we'll go ahead and create that one next we're going to go ahead and create the secret access key itself and we'll scroll down and paste this in here and then I'll go ahead and copy this secret access key and paste that just like that okay great so now our GitHub action should have everything necessary to deploy our production application so I'll go ahead and run this workflow now the nice thing about these workflows is they are fast and efficient and they have really good logs to figure out what's going on and what might be wrong with them so one of the things that would have been wrong is if I didn't actually fill out these secrets now something that's cool about how the environment variables work in the serverless application is that it will use the AWS access key and secret key instead of using EnV so that's actually a really nice feature of the seress framework so the next thing I actually want to do is I want to put the stage here and I'm going to go ahead and say it's prod now the reason for that is because in my Dev stage I also want that to be Dev which I already have in there but I just want to make sure that I do have that in the production one that also means that since I'm declaring the stage in more places I want to make sure that when I'm grabbing my database secret that stage has an environment variable one right so it's certainly possible that you're just like oh I'm just going to use the production one but hopefully you didn't do that based off of a a number of things that we did with the last part of actually setting and automating the secrets themselves so with that stage in mind I also want to update serverless.yml and I want to change the I Ro here to also match that stage because originally it was just for the production stage we only had one stage really creating a role now we're going to have multiple stages creating roles so we just want to update that as well so we're going to save everything and then in my GitHub I'll go and update these and we'll go ahead and say prepare for stages and we'll hit commit and we can sync those changes at this time okay so back in GitHub notice that my deployment actually did happen deploy our production app and it happened in 43 seconds we can click on this deploy section here and in here we actually see our endpoint come through so I can open that up and sure enough this should actually work for me and then if I go into leads I think I just get one lead back based off of some of the configuration we set up which of course we could verify in our code itself for leads the actual default page we actually have G lead in here instead of get leads okay so now that with this change it's a minor change that maybe I want to be in Dev first in to the dev stage first so what we want to do is in our prod stage yaml I want to change this a little bit to being based off of poll requests so in other words my Dev stage workflow is going to have to generate that poll request so that's going to take a number of steps and it gets a little bit more complicated than what we just did so if you want things really simple and you just want to run just straight to production which I don't advise but you definitely could you could automate this with a push request and basically change this to the branch that you're going to be using for your main branch or your primary branch on GitHub which would just be Main and then if you had this active it would automatically run that workflow that we just did workflow dispatch okay so I'm going to go ahead and just leave only workflow dispatch for now and work on the dev stage and really figure out how what we're going to be doing here now we already have a few of the environment variables in place we do not have the neon API key which we'll need in a moment we have our stage here we also already have a builtin GitHub token this is provided to us by GitHub actions it actually we don't need to set it we don't need to find it anywhere which is really nice for later when we use the uh GitHub CLI which actually comes in by default which we'll see and moment as well so again we'll we actually check out the code itself we grab all versions of the code mostly for the poll request a uh action that we'll do very soon once again we set up node.js we do the installations the default project installations then I want to make sure that I have the neon command line tool and TSX this of course is so that we can automate creating the branches like we've seen before as well as actually getting the connection string and then outputting that into our AWS sec secrets that parameter store so all of this stuff is mostly things that we've already seen before something that's a little bit new now is this API key call so basically anywhere we need to run a neon CTL command inside of GitHub actions workflow we just pass in an API key in there and that's how we're able to do all of these things so how I designed this workflow was to delete the previous Dev Branch so that I can take a point in time version of the branch or of the actual database itself to actually create a new version of the dev Branch at that time when I run this workflow so that then my staged Dev deployment actually has a new Branch from the database itself it actually is working off a new instance of the database having more of that data that's been stored from the production Branch now of course that's optional you don't have to do that I just want to show you how it's done next of course is getting the actual connection string itself from that branch and then putting that into our secrets and then finally we're going to go ahead and grab info about the branch itself so this is the one I want to run so in order for this to work correctly we need to bring in the neon API key so let's goe and do that now I'm going to go into my production app here we'll go into settings I'm going to go down into those Secrets again and we're going to go and create a new repository secret for the neon API key now where we find this is in the neon console we go into our user account go into the profile go into developer settings generate a new key here and I'll call this GitHub actions and then we'll go ahead and grab this and bring it into GitHub actions now do be aware that this key right here has access to our neon project so it could quite literally delete all of our databases which might be something you want to have happened but it also might not want you also might not want that but just keep that in mind when you do actually use that API key itself okay so with this in mind now we're going to go ahead and run that GitHub action so I want to bring that Dev stage in here so we'll go ahead and do get status and I'm just going to go ahead and say updated all for Dev we're going to commit it and we'll go ahead and sync those changes and then back into GI up actions I'll go ahead and deploy this Dev stage now so the key thing that I've been doing so far is actually just working on the single branch of main here so what we actually want to do is work on a different branch we want to work on a Dev Branch specifically with Git the dev Branch will will correlate to the dev application the dev stage of the application as well as the dev branch in neon so it's going to be Dev all across the board just to keep things consistent and so I know exactly what's going on now the way we do this locally is by doing get checkout dasb as in Branch Dev and this switches us over to the dev Branch so all of the changes we do in here will only go into this de Branch until we merge it into the other Branch itself so that's into the main branch that is so for example if I change this right here I just change that one single push on this workflow we'll go and say updated Dev Branch go ahead and Commit This and I'll go ahead and P publish this branch and push it into GitHub so now back in GitHub if I refresh in here um what I should see is something related to my Dev Branch so I have this Branch here and I have the ability to now bring in and merge these two branches together right so I can actually merge the dev Branch with the main branch itself so it's one commit ahead as we see here by just changing it we can go ahead and create a manual poll request and go ahead and hit create poll request here and then now I've got a poll request right so in my case it says 11 just because I did a bunch of testing yours hasn't or yours probably will say one maybe and then we can just go ahead and merge this and go ahead and confirm that merge and then we can also delete that Branch if we wanted to and so all of that did was now in our main branch we still only have one branch in here and then in my workflow we can double check in this Dev stage here that that Branch was actually pushed it actually did something in here so it actually did make a change into that workflow so let's go ahead and actually do this once again and still on the dev branch and still doing it manually before we automate it so the way we're going to do this is by going into index.js this time I'm going to go ahead and get rid of this homepage P message right I just want the Delta and that's it so I'll go ahead and save that and we'll do get status get add get commit updated homepage and then get push okay so once again it pushes it and then we can go right here and click to create a new poll request and do that right here now before I actually create that poll request I want to modify my production one just a little bit so back into the production workflow here I've got this poll request and we can do this types of closed so when the poll request is closed it triggers a GitHub event for that poll request and we can verify if it's merged here so if it is merged then we could run all of the production deployment stuff so we'll save that and now we'll go ahead and do get status get add Dall get commit prepare for auto deploy via merged PR go ahead and push okay so now I refreshed it everything should be up to par and I'm going to go ahead and now create this poll request I could have created the poll request before but now I have it no descriptions in here yet but this poll request now when I actually merge it what should happen is it should actually start a deployment process but since I just pushed some code into the dev stage itself I might want to actually wait for this to finish before I run anything else with serverless or maybe not I don't know let's go ahead and give it a shot I'm going to go ahead and merge this poll request and delete the branch so we goe merge confirm merge and delete branch and it's deleted going back into our GI up actions now we've got this deploy production app it's starting to deploy it based off of that merged poll request cool so if we wanted to automate this it's just one more step and it's a sort of major step inside of the Dev stage but not really that major and all it is is taking this Branch information here and then using the GitHub actions PR create command which is the GitHub command line tool to actually create a poll request for US based off of this actual branch and the branch that we're going to so what we want to do then is actually run the command here so I'm going to go ahead and say title and we'll give the title of automated PR from Dev right or really from Dev stage okay so that's the title we'll give it and then I'll go ahead and pass in a body and coming soon for now and then I'll go ahead and pass in the actual uh base branch which is going to be our default branch and then the pr branch is going to be the next one and that's going to be head branch and there we go and then we can also do our GitHub repo in here and just say repo and the repository okay so there we go we've got our branches that we're going to do here and let's go ahead and see how this ends up working once again I'm still in that Dev Branch here so I'll go ahead and do get status get add get commit and we'll do updated Dev workflow for auto PRS get push okay and that's going to take a moment for that to finish so going back into get up actions we see that the production app did was successful because it's really not that different than what we've been doing the poll request itself I should have none in here if I have any Dev poll requests in here this GitHub action will fail you can't have more than one for any given Branch now there are more advanced ways to have multiple branches and stuff like that um I'm not doing those Advanced ways I'm just going to be doing this really hopefully simple and straightforward one okay and so we've got this brand info here which I need to change the name of it but it does do the pr looks like everything was successful itself and then we can see the branch is the pr branch is the dev and then the main branch is right there for the default Branch great so now we can go into our poll requests here and there it is our automated PR from the dev stage the actual body itself needs to change we'll change that in a moment but overall now I can actually merge this data and again I can delete the branch if I want to it's really up to you on if you keep the branch or not um because locally I can still work on that branch and then push it again so the last part of this is really going to be do our uh poll request so it's going to be our Dev stage poll request and what we want in the body data itself instead of coming soon we want to actually get the info from our deployment right so the deployment info itself uh which is going to be in here so we need to update that as well and so let's see our Dev stage looks like our Dev stage has been running our deployment in here as well no our Dev stage is actually missing the deployment and the info so that was a key part that is missing but let's go back into package.json and make the deployment happen for our Dev stage as well but overall the GitHub action stuff uh is working correctly it's just there was one piece that was missing which is deploy Dev stage and then of course this is going to change it to just simply Dev and all that next up is we're going to go ahead and get info Dev stage and info now the reason for this is going back into the dev stage itself we'll go ahead and just grab one more name here and we'll go ahead and do deploy Dev stage and then we'll go ahead and run the you know mpm run deploy Dev stage and then that's going to take as long as it takes and then down here we're going to go ahead and get export the dev stage info equals to the dollar sign of mpm repo and now info Dev stage and then that is what's going to be inside of this body right here we're going to go ahead and pass that in just like that now we've got a new body stage and let's go ahead and try this out with get status get add all get commit and updated put the message in here updated for full Dev deployment stage get push and of course we'll let that go through I need to make sure that my poll request is empty looks like it is of course if I were to cancel any of the poll requests in here or deny them the production stage won't run it will only run if they are merged and that's it great so there's obviously a lot of options on how you could actually go about running those things as well now I'll just go ahead and let this finish so we can just kind of polish off this GitHub actions workflow and there we go the workflow looks to be successful so let's go into our poll requests here now we can see the automated poll request from the dev stage and what do you know there is the endpoint URL so so much of this poll request was reliant on this so that somebody could then go in and review some of those changes right so we would actually a be able to open up that page into like a new tab and review what's going on I made something change to the leads and I get this internal server error so clearly that's something I do not want to actually have go into production so then let's go back into our local environment here and see what's going on with that problem and then we'll go ahead and just change and see how we might fix it so of course the leads itself is having a problem get leads is simply not a function so let's go back into our actual uh DB crud and it's list leads there it is okay so we found the error itself go back into our index page fix this error in our Dev stage still get status get add get commit updated to list leads and then get push and then again you know that's going to go ahead and run through and finish and so this is why we actually have those PRS in place is to just do some checks to make sure everything is good in this case we do not want this we're just going to close this poll request uh and we will delete that Branch as well and that's it so we actually do not want that one of course we could put comments and stuff like that uh but once you close it what will not happen is the deployment actually running right so this won't build it it won't go through and the reason of course is it was skipped because of the way we set it up and that nothing merged nothing changed and so that is also showing that this is working correctly as well so that's actually pretty exciting to me I think is we can now see the dev stage in its full capacity we can do all sorts of tests there um in the sense that I could even share this with developers and stuff like that to they go through the different tests that they might need for this particular API and so yeah granted this is beyond the scope of really building out a expressjs application because the GitHub action itself could be used in many different places but the thing that's really nice is our Dev stage itself has a lot of features built into it that are really only possible by using neon's database service and as well as using AWS Lambda and of course is because when I'm not using the database or the aw with Lambda it's not charging me it's practically zero cost to be able to have this deployment stage or this Dev stage that I can then use when I need it to use and then that's when the cost will start to incur but only for a few minutes or so of course I get this little error here and I think it's related to actually deleting out that Branch uh too far in advance so I'm going to go ahead and just make another little change I'll just add a line here and do get status get add Dall get commit and minor fix and then I'll go ahead and push this just to verify that the actual GitHub action is working before we call this part all said and done and there we are with that minor fix that should be in there so I'm going to go ahead and now merge this poll request and again we can test this out as we see fit and I'll go ahead and run this and confirm that merge and I'm going to leave the branch now you could absolutely delete it if you needed to um but I think one of the problems that we saw with that previous minor fix or at least the up updated it to list leads the problem was that well when I went to merge it um I think it deleted it during the GitHub action workflow running which caused that issue um but now we've got a brand new one where it's going to go ahead and go into production and all that now the other thing about this is the actual production workflow does not have to actually go into using servus at all we can now pick wherever we want is far as when this thing gets merged and all that and of course the actual source code itself is going to change based off of that being merged and there's our list leads in there like that and also that just little extra space in there to make sure that that is all working as well uh so it's fairly straightforward on how we go about going from here uh the most important part of all of this was making sure that we could automate the dev stage itself so now at any time when I need to make a change to my code I work in that Dev branch and then you know commit it and do all that let the dev stage actually do its thing so then my main branch can actually work based off of that when whenever I need to so I basically won't need to work off of the main branch any longer unless there's some serious reason that I really need to but of course if I were working through this Dev branch and running through this Dev stage I should be pretty good from here on out yeah GitHub actions is pretty great for a lot of reasons let's take a look at another place to deploy this application than just AWS Lambda but still working in everything that we've been working with now as you may know expressjs was not really designed for the serverless world but the reason we can actually run it in servus is number one it's a minimal framework there's not a lot of overhead to run Express itself number two is that we have servess environments out there that we can run it on like AWS Lambda it has no JS and it can actually run Express for us but almost as important if not more important is actually our database so as soon as expressjs needs a database something like neon in servess post CR is how we're actually able to run and execute all of these things effectively without overloading our database and being ready to scale up if we need to now of course a better option for Ser list is probably nexts nextjs was designed for frontend and backend operations those backend operations are designed to be running on serverless they're like small little functions that can just run on serverless and it's really really fast but the thing is you actually have to learn react to really get the most out of xjs and maybe you're not ready to do that yet but what we will do is we're going to go ahead and integrate every everything we just did with our project and bring it right into reactor where it feels very native to our nextjs projects if you start building them out and this is thanks to versell of course versell manages nextjs but also they made it really simple to have some rewriting of rules for our API routes as we'll see in just a moment now generally speaking expresss is not going to be deployed directly to versell but I want to show you a way to do that as well and that's something we'll do in a moment but first let's go ahead and see how we can actually integrate nextjs with our entire ecosystem that we've already created in our serverless expressjs application so I actually created a repo specifically for this one and the only thing that you need to change for this repo is in versel Json this rewrites here that's all we need to change to make all of this work and more specifically the actual destination like we've got here now you can simply clone this project or you could just go to your local computer run n PX create next app at latest run through all the default options and then you just want to bring in this versell Json here and so what we want to do is we want to have these rewrites in here because of how nextjs handles API calls so inside of nextjs you can actually add in an a place to run backend API calls which is why we're actually doing this rewrite so basically we can treat our backend that we already have in in existence as one of the routes that you use for the back end of your nextjs application so that's what we want to implement now to do this I want to jump back into my serverless node.js API application so in here I'll go ahead and grab that with my serverless and info and then we'll go ahead and grab the you know stage being pra and then the region being the region we've been using which is Us East 2 hit enter and this will give me the actual endpoint I want to use so go ahead and cop Cy this endpoint and we'll paste it in just like that okay so I'm going to leave it as is and then what I want to do also is I want to make sure that versel is installed with mpm install DG versell at latest the reason for that is so we can actually use versel to well run these rewrites traditionally nextjs does not need to run those rewrites it won't actually do it by default so using versell will allow for that to happen so we'll go ahead and just call cell to start out it's going to ask us to set up a project I'm going to select all of the defaults in here I don't need to set up an existing project I'm going to use the you know directory's name I'm going to keep it in the local directory itself I'm not going to modify any settings and now it's going to go ahead and push this into versell itself and maybe even make a production version of this so we can verify this by going into our versell account which of course I'm in a free account here there is my application right and so we of course want to have a production build um and we'll do that in just a moment but before I do that I'm going to go ahead and create a new item here and we'll just call it versell and Dev so this is the versell development environment that's running our nextjs application with a port value which in this case is giving us Local Host 3,000 now my other application that we've been working on is not running at all right so that's another part of this it's not running through local it's going to quite literally go to our production endpoint here so I'll go ahe and open up Local Host now and I'll go into SL API leads what I get is not found but this is coming directly from expressjs so quite literally is calling expressjs our actual endpoint directly from a versell managed API right so this this actual inpoint is from versell so just having this alone can allow me to create a custom domain so right in versell I could just add in a customer domain right now and that will allow me to have this/ API leads so in other words if I wanted my example.com SL API leads I could quite literally just deploy it as is now of course I need to update my actual API itself to manage things a little bit differently which we'll see in a moment um but the idea is using versell with this rewrite is just a a quick and easy way to sort of do a proxy service itself uh which gives us another Advantage as well okay so going back into API and SL leads I actually want to not have it not found but actually go to those leads themselves so as it stands right now the API is at slash leads that's that's how we designed it we didn't create a path for API so we can actually change that destination just like that we can run it again and then I'll refresh in here and now it actually should do an API request to my actual endpoint on AWS Lambda with that neon database and getting back that neon data and of course I can also post dat as well but overall What's Happening Here is it's actually now treating this as just another path an API path in nextjs so if you're familiar with nextjs you'll know that typically speaking the API itself is going to be that backend part of things so I'm not going to go into the nextjs stuff of it just this right here that part is great so we can also test this so inside of page.js I'm going to go ahead and do a quick little test for this page.js is just the homepage of our nextjs application and it's all in react so if you're not familiar with react this might be a little confusing so just bear with me if it is I'm going to go ahead and get rid of all of the stuff in between main save that and I'll refresh in here notice everything's gone if I put H1 of hello world save that refresh in here and now it says hello world great so all I really want to do here is have a button of some kind that's going to emulate you know press me that's going to emulate actually calling our API there's that button right right there great okay so this is all Tailwind CSS classes in here so I can do BG green like 500 or 400 uh to actually have a background again that's another thing that's outside the scope of this but nextjs has a lot of features that come into it by default which can make it a little overwhelming to start out with but the idea here is we want to actually press this and actually do an API call that's really what I want to see and to make this happen inside of next we're going to go ahead and get rid of this import here and I'm going to go ahead and pass in the string of use CL client at the top to basically treat this page as a frontend page then I'm going to go ahead and bring in something from react itself we go ahead and import use State and this is going to be from react itself and then I'll go ahead and do const and new or let's go ahead and just say data and then set data equals to use State and there going to be just an empty string for the moment and then I want to actually have a click event in here so I'll go and do const handle click which is an event and we'll go ahead and console log that event and so we can add in a click Handler on the button itself so we'll do onclick and that's equal to that function itself now this is straight up react if you don't know this stuff it's okay I just wanted to do one simple thing with the fetch call on this onclick here and then the data itself we're going to go ahead and just say if the data exists we'll go ahead and do json. stringify of that data we really just want to see that it is working as we see fit so the way we're going to do that is by doing Fetch and then we want to fetch to/ API leads what do you know it's going to the rewrite itself I do not have to put that entire URL in there just the destination can be left inside of R sell. Json so what that does is it allows us to treat this like you normally would in a versell application um so or an xjs application in versel I don't really have to think about how to actually handle this I don't need to think about what URL I'm using or anything like that but realistically it's not a whole lot different than calling any normal API itself but in this case it's actually going to augment it in a way that's really cool as we'll see so now that we've got this let's go ahead and actually get the data back from this which I'll just say response equals to a weight fetch which means I need to turn this handle click into an asynchronous function and then I'll go ahead and do set data being a weight response uh response. Json like that this of course could have errors itself which we're not going to handle right now but overall I got a now click event that I can do in here let's just make sure everything's running I'll refresh in my create app and I'll hit press me and what should happen maybe at first it doesn't necessarily come through uh because it needs to spin up and all that uh but after I press it it will actually come through in there which we could console log the event each time it's happening and I'll go and press it now and you can see that it's clicking and it actually is doing that call for us and it's getting that data from our API in a way that's very nextjs based in other words the developer the nextjs developer if you were working with somebody else they could just work with their normal development process altogether and of course we could post data in here as well so for example to post that data we could just change this method being post and then the header uh is going to be our you know our content type and this going to be application Json and then our body data is going to be uh json.stringify a dictionary here and that's going to be email abc1 123@gmail.com and again it will still have a response so this is how I can post data and of course if you know react you will be able to change this as UC see fit as well so let's go ahead and try that out out I'll post out some data here and there is the result back from it right so this is coming directly from my API itself uh and there we go so it's now almost fully integrated here so that's pretty cool so now at this point what we could do is we could actually add this into our repo which I will so I go get add and we'll do get uh you know first prod deploy and we'll go ahead and push this into production and so what's going to happen here is it's going to go into vers sell uh it's going to go to the repo itself and it's going to do all of the build that it needs to to make sure that this is working properly uh which might take a moment or so uh but overall the idea being that once this is ready the production application version will be able to use API leads right in there uh and we already can but of course we could then customize our domain and do all that stuff but once it actually gets fully ready it will work the way we needed to and so the way I would actually change the r API itself is I would change this to being API leads and run off of that so that process would be then get status get add get commit and then updated versel inpoint for you know nextjs API so I'm basically going backwards now and so in my actual serverless node API itself I would then need to add in these paths here which I already did off the video and then I would actually have to go back into that application itself uh into serverless no jsapi look at those poll requests and then we would want to actually commit these things right so this is of course assuming that the dev stage was correct I'm not actually going to go through that process but the idea being that we now release the production version a new version of the API itself and really just kind of separating these two things out so realistically what our node.js application is doing is just consuming a preexisting API it's not actually deploying our expressjs application on versell uh but it is getting close to that it's getting close to what we might want to do with our application here and so every time you run a release it should actually start building out a production version of it uh assuming that we are in the right you know branch and all that and it actually does go into our repo look looks like it has and so that would take just a moment for the deployments to start filling out uh right on here so it looks like my deployment is not actually filling out so let's go into our settings here and into G and oh yes we need to connect our git repository so I'll go in GitHub here and then let's go ahead and connect the serverless uh one that we have which is the API next so I'll go ahead and connect that one now and I might have to run another production push but there's our main here um and so now we have our project connected to get so it actually should do that deployment when I do make those changes so let's just do a quick change here and just call it a day and this one I'll go ahead and say Hello World V2 and we'll go ahead and do a commit again and push it again so that we can trigger a deployment automatically and so that should actually happen now that I actually did that push and there it goes so it's now going to build that next version and by now hopefully our API leads is working correctly again it's still working correctly based off of that new rewrite that we just did right here uh and let's go ahead and just refresh that server just to make sure that that did happen and so I can refresh in here and all of that happened because of everything we set up with our seress framework and all of that serverless automation within there our production build takes a minute to actually deploy a new version of the API whereas the nextjs application might take a little bit longer than that uh depends on how big our nextjs application ends up being but overall once we actually deploy this we'll have a new version right on versell as we see here and now we can visit this and here's my production version on verell and there it is it's actually committing and making new data in my API backend and you know API leads is definitely showing that backend as well so great this is actually showing us how easy and how powerful can be to start leveraging that API into our projects into our xjs projects the other huge benefit of this is the developer who knows nextjs might not know how you built out this back end and there it is I want to point out also I never actually put any AWS keys in here I never put anything related to Neon in here I could quite literally just share this back end this API as I see fit and build down to my front end so like if you're working with another developer that's not necessarily on your team but you want them to build out a feature this is how this is one of the ways you could do it you could just send them whether it's your production API or your development API you could send them this link and say hey this is how you're going to do it and now you can actually build out what I need and then we can go from there of course you can still use wild cards there's a lot of different things you can do with these rewrites so just check the ver sale documentation for that um but this is the methodology I would take to leverage everything we've done to then start actually learning and building out an xjs application and then maybe modifying and migrating over to nextjs because a lot of the code itself can migrate not exactly because of how it's written we've got all this require here you have to change some of the import statements and all that but overall it's like now going towards what you would want to do within a nextjs application which is really exciting the final piece that I want to do is actually deploy our serverless node.js API itself this pure API itself directly to versell as well and just see what that looks like and how well it's not actually a whole lot different than what we just did but it's not going to give us the functionality of the reactjs framework we'd have to worry about that ourselves so let's take a look at how to do that now now we're going to take a look at how to deploy our expressjs application directly into versell now this can be combined with what we did with nextjs but I wouldn't recommend it generally speaking you probably don't want to deploy expressjs to versell even if it's technically possible nextjs is a much better alternative specifically to leverage a lot of the things that versell does really well and part of the reason I showed you the previous one first is because that's a better method to start leveraging your expressjs application and building off of that so the first thing that I want to do inside of here is I want to go ahead and add a new project this project is going to take a few things in here so let's go ahead and grab this serverless nodejs API the actual repo itself that we've been using and I'll go ahead and just deploy this as is now the problem with this of course is it's not going to work it's not going to work at all because of what versel attempts to do so we need to make it work and we need to change a few things on our project so the first thing that I want to change is I'm going to go ahead and add in versel Json and let's go ahead and change that to actually Json and I'm going to bring in that rewrites again so this time the destination is a little bit different and it's going to be a catchall and it's going to go into the destination of just simply SL API so in order for this to work we need to create a folder called simply API and in this folder we're going to go ahead and do index.js and this we need to import using the actual es6 modules from dot dot dot source and in our case it's Index right there so what we're importing here is the expressjs application which is defined right there and we also export it or we need to export it with module. exports and app equaling to app just make sure that it is exported so then it can be imported and then we'll go ahead and do export default of app okay so far so good so we've did that same rewrite we just did we also are adding in the index itself the actual app itself inside of the API and that rewrite is going to just return everything from that app to the API itself okay next up of course is going to be that we need a public directory so forel is really good at leveraging the public directory to serve the project so when it comes to nextjs it actually generates a public directory for you that will then manage all of the frontend side of things for sales optimized for front end and serverless in the back end and we're trying to emulate that here a little bit with just Express so the first thing I'm going to do is put get keep in here this is if I didn't want to actually put a front end of any kind but I'll also go ahead and add in index.html and then I'll just put in poorly formatted HTML here just to have a Hello World call of some kind okay so I didn't actually change this in terms of the expressjs application itself it's still using this SL API paths here like we saw before so we might need to change that as well uh but the reason I changed those in the first place was really for that integration with the xjs application based off of a rewrite okay so with this m let's go ahead and change it we'll do get status get add Dall get commit and we're going to prepare for versel and then we go ahead and do get push so what vercel is going to do is it's going to wait until this is on the main branch right now we're working on that Dev Branch so what versel will also do is well it doesn't really care about this production stage at all it's going to do its own thing it's going to basically do a similar item of this to deploy it which means we need one more step and that is our package.json versel is actually looking for something called build or versel we'll do versel build it's also looking for a script called build itself in this case we're just going to go ahead and do Echo the word of Simply hello and that's it in other words our build command is not going to do anything and the reason for this is that we maybe we add in another build script in here of some kind but ver cell build is going to attempt to build something and if this command does not exist then it's going to fail so during the versell build command it does something like our mpm run deploy but instead it's going to be versell build if that's available so we'll go ahead and leave that as is and then I need to redeploy this again but to do that I need to go back into GitHub uh and specifically into this repo itself so we go ahead and look for that repo and here it is right here and so what I want to do is just accept the pr that I just did um and commit pull and confirm okay and so there we go notice that versel actually looked at that as well so I'll go ahead and do another one with get status and get add and then get commit and then we'll go ahead and do updated script for versell and we'll go ahead and push that okay so it's going to take it a moment to build out so back in for sale we actually need to update this a little bit more too right so back into our project we've got our project right here well the problem with our project as it stands is the fact that well it doesn't have our environment variables that we need so our environment variables are going to be related to AWS right so the reason we need that of course is for that runtime so let's think about our production stage itself we've got en.pr which might have the stage variable in here for prod it also might need well not might it definitely will need various things related to our secrets so in our secret Library here we've got the AWS client that's coming through now when we deploy it to AWS those variables are going to be in there as in the AWS access key ID and the secret access key and of course it also has a role in here that we created specifically for the different stages and all that but we aren't pushing that into production anymore so we need to make sure verell has AWS access Keys itself so of course what we could do is we could go into AWS and create some new ones I'm just going to go ahead and use these local ones that I've got here just so I can emulate or show this example working correctly and there we go and I'll go ahead and save this the environment itself I don't necessarily need a bunch of different environments but I'll go ah and save those variables for all of those environments and there we go so one of the things I don't need in this version of my expressjs application is a neon API of any kind or my database URL because of course we've already done this but the idea being that my secrets are now stored on AWS so if you wanted to get rid of that functionality You' have to readjust how you're doing this and you'd probably be read addding your database URL as an environment variable but again this isn't a deployment that I recommend but it is one that you could do the question of course is whether or not you should so now with this deployed I see that it does say hello world so it looks like I'm on the right track that hello world of course is coming from the public index.html here so let's go ahead and visit this and at least the front end is going to work here so we've got the front end working which is kind of cool um I I want to visit the actual main the deployment as well so let's go back into the project and grab the actual deployment uh URL which is this one right here and so there we go and now I'll go ahead and jump into the API SL leads and if I did everything correctly it will work but of course the problem that it is facing right now is merely the fact that I didn't have those environment variables when I deployed this version so let's go ahead and just create them now I'm going to go ahead and update my index page and we'll go ahead and do p and EnV working try this again and actually one thing I might need to do also is just verify any poll requests that I have in here so this will also trigger another deployment so I'll go ahead and run that what this will actually do is solve that environment variable thing so I probably don't even need to submit this yet so I'm not going to do that yet let's go ah and see if this recent deployment solves the environment variable thing that we had with versel so get going back in here we'll just go ahead and take a look at the deployments that are coming through and of course it's building one notice it doesn't take very long that's not surprising because it's only installing a few things here it's not using the cache in the same way that AWS Lambda is it's literally installing all of the things in our package.json all over again and there we go we've got a new production version so I'll go ahead and open up my app again I can actually just go back into this URL here and if I refresh now everything's working and there is my expressjs application running on versel so what we could do is yes we could keep this I don't recommend it but we could keep it and then build out your front end using something different so expresss has a lot of different frontend support as well in other words the index.html could then bring in something like pug or some other JavaScript that you might want to have in there that then calls the API itself uh but that starts to get way too complicated then it's like oh you might as well use for yourself you're going to start building out the front end um itself forell and xjs uh to build it out and make it even more powerful but I really wanted to show you how those two different approaches worked so that if you wanted to use something like versell to deploy your applications you totally could now all all of this is possible because of how easy it is to be flexible with our deployment options neon is well suited to do all of that for us right so the fact that our database we didn't have to touch our database at all for those last few deployments was so nice and that's because of how we configured everything to work and in this like automated fashion so when it comes to deploying to versel the other part of this that you could consider is you don't always have to have your database URL exposed inside of the application itself so even if you were using nextjs you could still use the parameter store that we did and then in your workflows you could still do the dev stage where you would basically stop uh as far as the dev stage is concerned you would stop you would create a branch maybe the deployment Branch you could then also create that uh pole stage actually everything in here with maybe the exception of deploying a development stage on AWS could still work in a nextjs application and nextjs itself can also do a bunch of things with that and of course if we wanted to deploy our development version on versell well we could go back into versell into our project here the first thing that I need to do for this project is jump into the settings go into my environment variables here and we want to add one specifically for a preview environment I'll go ahead and grab the branch itself which will be our Dev environment I want to go ahead and add the stage of Dev right and then we'll go ahead and save that so now we've got a preview environment on that stage right so it's going to give us that Dev there so the next thing that I would want to do is jump into our deployments here and we're going to go ahe and create a new deployment so it's going to be commit or Branch reference this is going to be Dev so let's do a quick search for the dev Branch there it is right here it was happened just a little bit ago I'm going to create a deployment for that and this should work right so it's going to be basically the same thing just using the dev Branch itself so in other words the branch itself should also have the same data that our production one has because well we've been deleting and recreating those branches uh but overall what what I really want to see here is that the dev Branch actually works and it is something that's different and again it's just another way to just Branch out our database do some tests and preview environments and all that so here it is there's our preview environment I think it's working so let's go ahead and give this a shot by looking at the Domain itself um and so there's our world let's go into our API just make sure that that's working so API leads and what we should have in here is something maybe slightly different than the other one this has 18 values in here um and so once again I could just go back into this one let's go ahead and just create a preview or uh you know a stag version I'll go ahead and update this and go ahead and do staged version and we'll go ahead and push it okay so of course one of the challenges with pushing this is the fact that I have my Dev stage here is going to quite literally create and delete a branch which could cause Havoc or issues with staging this version uh so it's not necessarily something I recommend doing but we could we could just try it out just to see that you can absolutely deploy different versions of your application with different stages and different branch databases at that time as well um which in our case the stage version uh it just came out 20 seconds ago so we can go ahead and take a look I'm not I'm not confident that the API is going to work here but we can give it a try at least we've got some you know other text in there so let's go ahead and grab in the SL API and leads and I I would imagine that it's either going to work or it's not in this case it actually did work uh which is great so we have the ability to now also have those different stages as well on versell and each stage could have it a different domain as well as the main production one can also have its own custom domain which is maybe one of the main reasons you end up deploying to versell as well is because it's quite a bit easier to put a custom domain on versell than it is on AWS and the servess environment so yeah that's pretty cool I think for sale is a great tool and now I would recommend that you start modifying converting all this stuff to nextjs which is really the the next part of so much of this but even if you didn't we now have a really solid foundation for an API service that we can start building out a lot more endpoints if we needed to hey thanks so much for watching hopefully you got a lot out of this now I want to leave you with two challenges one is how do you actually Implement authentication whether it's users or other applications how do you actually secure our API more now that will be a really good challenge to undertake at this point if you were able to get this far the other one is well how do you actually convert this expressjs application into a nextjs application those are the two challenges I want to leave you with they are fairly straightforward especially because there's a lot of thir third party support out there but I think those are really good challenges that you can Undertake and guess what they can feed into each other as well so thanks again for watching my name is Justin Mitchell I really enjoyed sharing this with you because I think servus is one of the most fantastic things that you can learn about and Implement because it really just helps us focus on just our code we don't have to worry about the infrastructure that infrastructure is taken care of by the different services that we leveraged right so AWS Lambda will scale up if we get a lot of users coming in so we our on database so would versel if we went there it's really really great to just focus on the code itself and not really worry about the infrastructure that is one of the keys of serus and that's why I wanted to create this course for you so thanks again for watching and I look forward to seeing you again in the near future take care
