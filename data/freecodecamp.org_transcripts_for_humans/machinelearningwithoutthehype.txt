With timestamps:

00:02 - okay everybody good to go everybody sit
00:06 - awake good let's see if we say manage to
00:11 - stay that way
00:12 - so I'm Philip I want to talk about some
00:15 - machine learning some hyping some making
00:18 - fun of stuff so let's see what we can
00:20 - take this so I work for elastic the
00:23 - company behind elastic Serge dogfish
00:25 - Cubana like previously called the elk
00:26 - stick now we call it the elastic stick
00:29 - I'm part of our infrastructure team I
00:31 - always say in the middle that is a UNIX
00:33 - pipe a kind of piped it out into
00:35 - developer advocacy so I try to speak a
00:37 - lot about the good stuff that we do and
00:38 - why am i talking about machine learning
00:40 - well every company needs to have some
00:44 - machine learning in their stack you can
00:45 - see we have kind of hitting it on the
00:47 - right-hand side at the bottom there kind
00:49 - of like we have something about cloud we
00:50 - have something about machine learning
00:51 - like all the usual stuff that you need
00:53 - to have we'll get back to that later on
00:56 - but at first I want to talk a bit about
00:58 - machine learning in general and what it
01:01 - is and what it is not and what it could
01:03 - do for you so some months ago there was
01:07 - this like machine learning is going
01:09 - viral and everybody was going oh we need
01:11 - machine learning we need to do more
01:12 - about that and one thing about going
01:15 - viral was also that who remembers that
01:17 - iOS park when suddenly the eyes were
01:19 - exchanged for a different character that
01:22 - was actually machine learning as well
01:24 - because that was kind of some Apple
01:26 - added some machine learning for the
01:27 - keyboard that it would replace specific
01:30 - characters or words and that was kind of
01:32 - as soon as you received that bad eye
01:34 - which was replaced by some other
01:35 - character that was already machine
01:37 - learning at work because then your
01:38 - system would learn that other way to to
01:41 - write the replace the term and only once
01:43 - you receive such a change diet then you
01:46 - would be or would do the same when you
01:48 - device would do the same thing because
01:49 - your device had learned that so that was
01:51 - kind of the power of machine learning
01:52 - which was kind of a bark and like kind
01:55 - of a bit of a virus since it was
01:56 - spreading like that but yeah machine
01:59 - learning is kind of going viral in that
02:00 - regard yeah and then there are always
02:04 - the people who are saying like oh we're
02:05 - doing some very fancy technology and
02:07 - even though we probably would have just
02:08 - needed a few if statements which would
02:11 - have been much easier to write in debug
02:12 - which probably looks something like this
02:15 - like
02:15 - have this big digger and then it's just
02:19 - using that little shovel there and to
02:22 - get going so that is some people we are
02:24 - wielding kind of the powers of machine
02:26 - learning just to do something very small
02:29 - and confined and read like easy to get
02:31 - going with sometimes you might need it
02:34 - sometimes you might not need it I guess
02:38 - some of you are probably familiar with
02:39 - that that is the Gartner hype cycle that
02:41 - is probably the only thing that is true
02:43 - that Gartner is always bringing out and
02:44 - because everything else is kind of like
02:46 - just making stuff up or at least it was
02:48 - always my impression that whatever the
02:50 - Gartner is predicting for the next five
02:52 - years is probably not going to be the
02:54 - way it will be but their hype cycle is
02:56 - kind of like very very true like you
02:58 - have some technology trigger and then
03:01 - you have like this peak where everybody
03:03 - is saying like oh this will be so
03:04 - awesome and this will do all the things
03:05 - I can still remember when everybody said
03:08 - that about no sequel and then people
03:10 - were kind of like oh maybe a relational
03:13 - databases were not all that bad and I
03:15 - have the feeling that kind of machine
03:16 - learning and AI and whatever you want to
03:18 - call it this kind of falling into the
03:20 - same perception every now and then and
03:22 - so you have some peak and then you get
03:24 - kind of like a bit of a disillusionment
03:26 - and at some point you may be reached at
03:28 - 2 or 3 activity will machine learning or
03:31 - AI solve all your problems um
03:33 - unfortunately it probably will not but
03:37 - that's probably no surprise so anybody
03:38 - who has been in the field for some years
03:40 - knows ok all the promises like
03:41 - everything will be solved by this
03:43 - technology unfortunately it's not that
03:46 - easy or fortunately it's not that easy
03:47 - otherwise nobody would need us anymore
03:50 - so I want to talk about like a bit
03:53 - machine learning in general and like
03:55 - putting it in with AI and whatever then
03:58 - I will pick a domain of what we can do
04:00 - since our use case is always about
04:03 - machine learning for ops data I will use
04:06 - some op stater for that and then I'll
04:09 - just play around with some data set for
04:11 - a quick finish let's see where we can go
04:14 - so machine learning there's always this
04:16 - misconception of this discussion like
04:19 - what is artificial intelligence what is
04:20 - machine learning what is deep learning
04:22 - how do they belong together isn't all
04:24 - the same thing because some people
04:25 - always say oh it's basically the same
04:27 - thing it's just like depending on when
04:29 - people
04:29 - started with something they had to come
04:31 - up with a new fancy term and that fancy
04:34 - term could be either AI or if you came a
04:36 - bit later it would be machine learning
04:38 - or if you're very up-to-date and it
04:39 - would be deep learning I kind of find
04:43 - this very nice this graphics so you can
04:45 - see it all kind of started in the 50s
04:47 - already back in the days I don't assume
04:51 - that most of you were around an IT back
04:53 - then but maybe somewhere or maybe a
04:56 - little later so in the beginning there
04:58 - was machine learning and there were high
04:59 - hopes and after some initial
05:01 - breakthroughs people assumed well
05:03 - general artificial intelligence will be
05:06 - sooner things so general artificial
05:08 - intelligence will be like machines will
05:11 - be as good as humans at almost
05:13 - everything and they will just learn one
05:15 - field by the other and and initially it
05:18 - was just it started as a very narrow
05:19 - field like you would do something where
05:21 - machines would be good in one specific
05:23 - field and then the idea was well we will
05:25 - just generalize that and this will be
05:27 - applicable to any other thing afterwards
05:29 - and it will just keep generalizing that
05:31 - until you're kind of on human level
05:33 - unfortunately that didn't play out that
05:35 - well and then in the 80s we had like
05:39 - with machine learning we had another
05:40 - wave of trying debt and then like the
05:42 - 2010s we had another wave of that with
05:46 - deep learning yeah
05:49 - so the general purpose of generally AI
05:51 - is always let's get something that is
05:54 - human level this is something we have
05:56 - not reached yet and we don't even have a
05:59 - proper timeline like in the 50s 60s and
06:01 - 70s it was assumed like oh we will get
06:03 - there but it didn't turn out that way
06:06 - because in the 70s there was the
06:07 - so-called AI winter and this was also
06:10 - one of the reasons why you had to find a
06:12 - new fancy name because a lot of people
06:14 - got burnt by that they assumed Oh
06:15 - artificial intelligence will solve
06:17 - everything and a lot of money was pumped
06:19 - into that and people wrote papers and
06:21 - plans and everything and at some point
06:23 - it didn't really plan will work out the
06:26 - way people planned so there was kind of
06:28 - like the first wave of ai ai
06:30 - then came the air I winter and
06:32 - afterwards people came back with better
06:34 - algorithms more computing power it's
06:37 - kind of like coming back in waves and
06:39 - you're getting closer to your target but
06:41 - with generally AI that's something we
06:43 - have not
06:43 - really achieved yet and we don't really
06:45 - know when that will happen what we are
06:47 - very good at right now is something like
06:49 - narrow AI where you have one specific
06:52 - task and the goal is to have something
06:55 - where you're as good or nearly as good
06:57 - as a human for example to identify an
07:00 - image so I was two weeks ago I was in
07:03 - San Francisco and I uploaded this
07:05 - picture to Facebook and then Facebook
07:07 - does the classification automatically
07:09 - for you so for example if you look in
07:11 - the meta tags what Facebook does is it
07:13 - adds the alt tag and it says like image
07:15 - may contain and then it's just guessing
07:17 - what could be in that image and it's
07:19 - actually pretty good
07:20 - because yeah they have a lot of data
07:23 - they have trained on a lot of data and
07:25 - they're pretty good at just detecting
07:27 - what might be in your image even though
07:30 - I haven't told anything like it didn't
07:31 - tag and say okay this is the Golden Gate
07:33 - Bridge
07:34 - I've just uploaded the image without
07:35 - other any other meta information and the
07:39 - narrow eye in that case like the image
07:40 - classification that just kind of found
07:43 - or detected what might be in that image
07:46 - and it's actually pretty good another
07:49 - thing you might have seen which was
07:50 - pretty recent is that cylinder is
07:52 - cutting some marketing jobs where
07:54 - they're automating the marketing now
07:56 - because previously they had a lot of
07:58 - handcrafted content and now they kind of
08:01 - said well we can automate a lot of that
08:03 - with machine learning we can hyper
08:05 - localize the content where we know okay
08:07 - you're interested in these products and
08:09 - you're living in that specific city and
08:10 - previously somebody would probably hand
08:13 - curate the set of data that or the
08:15 - products that might be interesting for
08:16 - you or would write some content that is
08:18 - tailored for someone in a specific
08:20 - region but they came to the conclusion
08:22 - with machine learning this is much
08:24 - easier to do or will scale much better
08:26 - and they don't need all those humans
08:28 - anymore so they now start to kick out
08:31 - some of their marketing people's some
08:33 - will move to a different department but
08:35 - a lot of them will be just kicked out
08:38 - which sucks and people are very afraid
08:40 - that all machine learning will take over
08:42 - all my jobs probably some but probably
08:45 - also not all of them because you might
08:48 - have seen this example as well where
08:50 - some people used Google Translate to
08:52 - translate sorry for the German if you
08:56 - don't speak German
08:57 - some economic topic and this sentence
08:59 - here is actually the very first sentence
09:02 - of Wikipedia if you look up folks with
09:05 - shirts really which is ya know some
09:08 - German sentence and this is what people
09:12 - got when they use Google Translate
09:13 - actually I tried it yesterday evening by
09:15 - now Google got kind of clever I don't
09:18 - know if somebody went in there to fix
09:20 - that because the people tweeted about
09:21 - that and it made the rounds but by now
09:24 - Google Translate got a bit smarter again
09:27 - and the same thing is still not great
09:29 - what you're getting now but it's still
09:30 - got a bit better and it's not just
09:32 - economics of economics and yeah the same
09:34 - word over and over again but yeah having
09:38 - the fear that professional translators
09:40 - will go out of business like next week
09:42 - it's probably a bit premature we're not
09:44 - there yet
09:45 - like it's kind of general idea yes you
09:47 - can do nice stuff and if you don't
09:49 - understand some language you can use
09:51 - Google Translate to get the idea
09:53 - but it's probably not going to replace
09:56 - proper writing so for example our
09:58 - support team when we do a support in
10:00 - some localized language we always say
10:02 - for example people in Japan don't want
10:03 - to speak English and for them it's super
10:05 - important to get support in Japanese and
10:07 - we always say yes you will get support
10:09 - in Japanese during Japanese business
10:11 - hours because then our people are online
10:12 - but if you want to get help after hours
10:15 - either you can write you text in English
10:17 - in English and somebody will help you or
10:19 - we will use Google Translate and we're
10:22 - actually handling support cases with
10:24 - Google Translate and whatever comes out
10:27 - of Google Translate we will try to help
10:28 - you with and if it's no good
10:29 - unfortunately we will need to wait until
10:31 - somebody wakes up who speaks Japanese
10:34 - yeah and then there's always this chat
10:37 - pod thing I know that Vienna has this
10:39 - kind of I always said the chat pod
10:41 - bubble because a lot of people are
10:42 - working in in chat pods I hope not too
10:45 - many are here and come with the
10:47 - pitchforks afterwards but I have no or I
10:51 - had this thing once or twice where
10:54 - somebody came to me and I was or I was
10:56 - talking to somebody and they were saying
10:57 - I'm doing AI and I Wow that sounds super
11:00 - advanced what are you doing and then
11:02 - they people say well we're using this
11:04 - Facebook API and we're sending some data
11:06 - they are in it senses something back and
11:08 - so we're doing AI and I'm always like
11:11 - you doing api's it's just like the pee
11:14 - is probably silent in your AI yeah and I
11:21 - like to say yeah
11:22 - AI API is kind of it's very close some
11:25 - people don't make the distinction we
11:27 - can't have a discussion about that but
11:29 - at least for me when you say I'm doing
11:31 - AI is not just calling some Facebook or
11:33 - whatever API and get something back and
11:35 - use that this is one of my favorite
11:39 - examples I think this is the Pancho one
11:42 - of the chat BOTS which you can ask like
11:44 - what is the word I like and then you can
11:46 - see yeah it's working well or not that
11:49 - well and I get the idea that is trying
11:52 - to be cute but like cute is not the main
11:55 - thing I want if I want some information
11:56 - like just because it's trying to be to
11:59 - speak French or saying it dozed off for
12:01 - a few seconds and that doesn't make
12:03 - something very smart or intelligent it
12:06 - might be cute at first but if I want to
12:08 - get some information this is not going
12:10 - to make me very happy with something
12:12 - else I recently had I'm using a one
12:15 - Telecom and I recently called their
12:17 - hotline and normally you know you dial
12:20 - in and then you need to press five for
12:21 - that service and press three for
12:23 - whatever and they didn't have that
12:25 - anymore they had the whole thing voice
12:27 - automated so you would need to speak to
12:30 - the system and it didn't work at all
12:33 - like my internet was not working and I
12:35 - was trying to in various ways to
12:38 - describe like what my problem was and
12:40 - even when I said internet it would say
12:41 - oh you want to know something about your
12:43 - bill and I like no that's not where I
12:45 - went and then you do like five circles
12:48 - or so and in the end you could get back
12:49 - to that dial like five to get support
12:51 - for your internet or whatever so yeah
12:54 - the future might be very bright about
12:55 - all these automated systems with voice
12:57 - control and chat pots and whatever um
12:59 - there was sometimes there is super
13:01 - annoying or maybe I'm just too old and
13:03 - grumpy by now anyway um and this is
13:07 - other thing maybe you've once seen a
13:09 - stateless jet pod which makes for great
13:11 - conversations because it just knows the
13:13 - current sentence and it doesn't have any
13:15 - context context about what which is
13:21 - another kind of common problem this is
13:22 - solved you can totally do that
13:23 - but like simple chat pod
13:25 - will not be able to do that because yeah
13:28 - life is hard so what is machine learning
13:31 - after all kind of together so you
13:34 - basically have an algorithm than cat
13:36 - that can work with some data you can
13:39 - kind of learn from that and then you
13:41 - make some determination or prediction
13:43 - out of that data so it's basically a
13:46 - trained machine
13:47 - you haven't hand coded the rules so for
13:49 - example one common example could be like
13:51 - classification of spam emails initially
13:55 - people would write rules like if there's
13:57 - something like I'm from Nigeria and I
14:00 - will send you 100 million if you detect
14:03 - something like that in an email it
14:04 - there's a very good chance that this
14:06 - might be spam but of course people are
14:08 - clever and they will reword stuff and
14:10 - then it's kind of like it's something
14:13 - point five billion it's not Nigeria but
14:16 - some other country but so people will
14:17 - found various ways around but with
14:19 - machine learning you can actually not
14:21 - kind of write these rules explicitly but
14:23 - can just learn like what does the spam
14:26 - email look like and probably can find
14:27 - out these are the different kinds of
14:28 - spam emails like these are the ones who
14:30 - want to sell you viagra did this one
14:32 - wants to get your money and this one is
14:34 - just some plain annoying thing yeah and
14:39 - this is the proper definition what
14:42 - machine learning is but you will not get
14:44 - any venture capital with if you just
14:46 - write this because it sounds very dry
14:48 - you learn from some experience with some
14:51 - some class of tasks and then you perform
14:53 - some measure and then the performance at
14:56 - that task
14:56 - improves with the experience the more
14:58 - experience you have the better the
15:01 - machine learning should get this is kind
15:03 - of like what happens this is kind of the
15:04 - algorithmic view but again this is not
15:07 - getting you any venture capital or you
15:09 - cannot build a start-up just based on
15:11 - this one sentence what machine learning
15:14 - is generally doing is something like
15:16 - it's one indie of these four categories
15:18 - commonly so you could have some
15:21 - classification so you could have an
15:23 - either like binary classification guess
15:26 - no or you could have a classification
15:30 - which has kind of like more outputs for
15:32 - example you could have a picture of
15:33 - somebody and then you could try to do a
15:35 - sentiment analysis like is this person
15:37 - happy
15:39 - said angry whatever and then your
15:41 - algorithm tries to classify what kind of
15:43 - facial expression does this person have
15:46 - so that could be a classification then
15:48 - you have regressions where you basically
15:50 - try to know or calculate what a specific
15:54 - value will be in the future which could
15:56 - be like a temperature or a stock price
15:58 - where you do the prediction of all the
16:00 - stock prices will go up or go down you
16:03 - can do ranking ranking is pretty much
16:05 - like you search for something and then
16:08 - you don't just have the hits but the
16:10 - system learns over time like what is it
16:12 - good hit and you kind of like feedback
16:14 - information of what is a good hit into
16:16 - that search system so that your ranking
16:18 - will improve over time and then
16:20 - clustering is just putting stuff that
16:22 - belongs together or isn't kind of some
16:25 - groups that make sense together you just
16:27 - try to classify similar things and then
16:30 - say okay these are all the things that
16:32 - are in that classification or clustering
16:37 - yeah this is what machine learning might
16:40 - look like that one machine is teaching
16:43 - the other little machines of what it
16:45 - knows or at least that's how we humans
16:48 - do it pretty much would be nice like if
16:52 - it was like that so the most common
16:56 - thing is when people say oh we were
16:57 - leveraging machine learning is that they
16:59 - really doing linear regression so
17:01 - basically you have some some information
17:04 - these would be like the little dots and
17:06 - then you want to find the linear
17:08 - regression which is basically this red
17:10 - line and you try to optimize that line
17:12 - and try to keep the distance of all
17:15 - these dots as close as possible to the
17:17 - red line and the more data you have the
17:20 - better that line can be and the more you
17:22 - kind of get to the value of what makes
17:24 - most sense for your data set so if you
17:26 - have like very sparse data and you only
17:28 - have three or five points you will have
17:31 - a very coarse linear regression if you
17:33 - get very or have lots of data the linear
17:36 - regression can kind of optimize much
17:38 - better for all the data points you have
17:39 - and if people are just saying oh we're
17:42 - doing a machine learning oftentimes it
17:45 - just means yes we have this linear
17:47 - regression which is not all that fancy
17:50 - but it can still do meaningful stuff
17:53 - for you yeah whenever people say that
17:56 - assume if there is no other information
17:59 - it's probably a linear regression not
18:01 - something more fancy and then you can
18:04 - still make the distinction between
18:05 - supervised and unsupervised machine
18:07 - learning supervised machine learning is
18:09 - and you have labeled data so you have
18:10 - some input data and then you have some
18:12 - output and you know this is for example
18:15 - with spam emails if enough people have
18:17 - said like this is spam this is not spam
18:19 - you have a huge data set which Google
18:21 - probably has and then oh this is a spam
18:23 - email and this is not a spam email and
18:25 - they can turn on that data because they
18:27 - have some supervised data await and no
18:30 - these are the inputs these are the
18:31 - outputs and then you can take a lot of
18:33 - that data to for your training data and
18:37 - then you can keep some of the remaining
18:39 - data for your test data you could do the
18:42 - same for unsupervised machine learning
18:44 - where you don't have any annotated data
18:46 - you just try to find some hidden
18:49 - relationship in your data so you don't
18:52 - know what is actually this good this is
18:54 - bad and it's dissonant normally you just
18:56 - have data and you try to find some
18:58 - relationship out of that the finding
19:00 - relationship in data can be kind of
19:02 - tricky there's this very nice xkcd comic
19:06 - which I've broken up for better
19:07 - readability here where somebody says
19:10 - like Oh jelly beans cause acne and then
19:12 - the scientists have to investigate even
19:15 - though they just want to play Minecraft
19:16 - so you can see that was kind of like one
19:19 - year ago whenever minecraft was very
19:21 - popular so that first to say like know
19:24 - what like with the 95 percent confidence
19:27 - we can say like no acne is not caused by
19:30 - jelly beans and then somebody says well
19:32 - but it I have heard it it's just a
19:34 - specific color and then they do the test
19:36 - for all the different colors and it's a
19:40 - bit hard to see but all of them like no
19:42 - we found no link and then there is the
19:45 - one where it says whoa and there it says
19:47 - oh there is like a link the problem is
19:50 - if you have like a 95% confidence
19:53 - interval and you make and you run the
19:56 - trial on 20 different data sets just by
19:59 - the mathematics one of them will then be
20:02 - kind of true and then you can write the
20:05 - big news article
20:06 - the green jellybeans obviously daling a
20:09 - workhorse or we're causing acne with a
20:11 - 95 percent confidence and only 5% chance
20:15 - is in there but that's kind of like if
20:19 - you run the same thing 20 times there is
20:21 - a chance that well with 95% confidence
20:26 - one of them will fire if you run it just
20:28 - 20 times and that's pretty much what is
20:30 - happening here then you can do
20:33 - reinforcement learning where you
20:34 - basically have a feedback loop so
20:36 - whatever your outputs are you're kind of
20:38 - continuously optimizing that on the
20:41 - feedback you're getting so for example
20:44 - if people are saying this is helpful
20:45 - this is not helpful and you can then
20:46 - keep optimizing whatever your machine
20:49 - learning algorithm is doing and then
20:52 - there is deep learning which is you have
20:55 - the neural network which is also very
20:56 - fancy nowadays because now finally we
20:59 - have enough computing power or mainly
21:01 - good graphics cards to calculate those
21:04 - to basically calculate a probability
21:07 - vector and you have lots of training and
21:10 - then parallel sted that is if you get
21:13 - enough graphics card to actually do that
21:15 - and not and they haven't been sold out
21:17 - to mine bitcoins before which is kind of
21:21 - sad that people are using all that
21:23 - computing power for mining some
21:24 - cryptocurrency but yeah that's a totally
21:27 - different topic so what basically
21:29 - happens here this is a very simple
21:31 - example and we were having you have
21:34 - slept X hours the night before a test
21:36 - and you have studied Y hours before a
21:39 - test and then you try to find like what
21:41 - does this mean for your test score like
21:43 - the green thing on the right hand side
21:45 - that is basically the outcome of the
21:46 - test score and then you have with these
21:48 - two inputs like they're weighted like
21:50 - how important is sleep how important is
21:52 - how much time you have spent studying
21:54 - and then you can have one or more layers
21:59 - the more layers you have the deeper it
22:02 - gets or as soon as it's multiple once
22:05 - then you will have deep learning and
22:06 - they're kind of a hidden input so you
22:08 - you can see they say it's as mysterious
22:10 - you don't really know how the system
22:12 - behaves in there you just have some
22:15 - weighted inputs and then at the end some
22:17 - value plops out and it says okay if you
22:20 - yep X hours and you studied Y hours your
22:23 - test score will probably be I don't know
22:25 - you will pass you will not pass so you
22:26 - will probably get that great so that is
22:29 - what you can do with neural networks
22:32 - which is also very common for example
22:34 - for image classification or the standard
22:36 - example that a lot of people have is
22:38 - like you have this detection of
22:39 - handwriting so people write the numbers
22:42 - from 0 to 10 to 9 and then you try to
22:45 - detect each image or each number and try
22:50 - to defer like ok which number could that
22:52 - be and then you can extract some
22:54 - characteristics of each number and that
22:56 - is what makes up your neural network and
22:58 - then we will say ok this was the number
23:00 - 5 for example whatever is what is really
23:05 - most important there is to have some big
23:07 - data set because you will always need
23:09 - something to Train there so companies
23:11 - like Google Facebook Amazon they have a
23:14 - huge advantage because they just have a
23:16 - shitload of data to work with so for
23:18 - example when people are now surprised oh
23:21 - how can Facebook be so good at detect
23:23 - stuff um well you have helped with their
23:25 - training data for years and years so
23:27 - this is if you have been on Facebook for
23:31 - a long time or a long time ago probably
23:33 - this was what it looked like 10 years
23:35 - ago so it didn't do any machine learning
23:38 - back then but what you could do is you
23:40 - could upload a photo and you could tag
23:42 - people and you could just say ok this is
23:44 - that person and that gives Facebook
23:47 - actually a huge amount of data because
23:48 - it knows now ok this there is the face
23:51 - of a person and that's probably somebody
23:53 - who is male and from the profile of that
23:56 - person they know like the age the gender
23:58 - the ethnicity whatever so it knows a lot
24:05 - of information and can use all of that
24:07 - information later on to then train its
24:09 - machine learning and that's an advantage
24:11 - that you have given kind of to Facebook
24:13 - and all these other big vendors who can
24:15 - build cool stuff and it's very hard to
24:17 - replicate that for you because you don't
24:19 - have this huge amount of data to work
24:20 - with so yes you can make data great
24:26 - again but only if you have huge data
24:29 - that's the one thing you really need
24:33 - yeah and that's what big companies are
24:35 - doing and then people are like Oh a IML
24:41 - will it just use them interchangeably
24:42 - and it really depends like for example
24:44 - when you're raising money then you want
24:46 - to call it AI when you're hiring then
24:47 - normally people say machine learning
24:49 - that's also some distinction I have seen
24:52 - or some people have framed differently
24:54 - they say when you're fundraising then
24:55 - it's AI when you hire in its machine
24:57 - learning when you're implementing it
24:59 - then it's just a linear regression and
25:01 - as soon as you debugging it it's back to
25:03 - the good old printf so like all the
25:05 - fanciness you have kind of at the start
25:07 - is going kind of like down down down
25:10 - until you back it's basics so while it
25:12 - sounds very fancy and it can do some
25:14 - cool stuff some basics are not going
25:16 - away or there are these cattle contests
25:21 - where you have some problem and you try
25:23 - to solve the problem and at the end at
25:25 - the end you might win I don't know a
25:26 - million dollars depending on the carrot
25:28 - contest so what people are doing is
25:30 - we're just starting a competition we are
25:32 - using some machine learning on it and at
25:34 - the end we will profit but normally what
25:36 - you won't really get the profit you will
25:38 - mainly get some disappointment because
25:40 - what is still totally relevant is that
25:43 - you will need to know the domain and
25:45 - need to know what it is about to
25:46 - optimize that it's not that you can just
25:48 - throw some generic machine learning on
25:50 - it and then it will magically find out
25:52 - what it's just the right thing to do and
25:54 - will make you rich unfortunately it's
25:56 - not that easy okay so enough making fun
26:02 - of machine learning let's do something
26:03 - more productive so since I have an
26:08 - office background mm-hmm and our product
26:11 - does work pretty well with that I've
26:13 - picked something so you have some trend
26:16 - over time and we try to determine some
26:18 - pattern in our data so for example this
26:20 - could be people visiting your website
26:22 - this could be traffic this could be
26:25 - error messages you have or return HTTP
26:28 - return codes you have all of these you
26:30 - can't try to find some pattern in data
26:32 - like that so the first one is you have
26:35 - some trend which is just like it's
26:37 - growing linearly or it's kind of like
26:39 - falling linearly or you have exponential
26:42 - growth or whatever anything that is not
26:45 - stationary so if you just have
26:47 - some stable value you're not going to
26:49 - find out any trend out of that because
26:50 - well no change then oftentimes you have
26:54 - some cycles in the data cyclists would
26:56 - be the days of the week so Monday to
26:58 - Friday you have kind of the same traffic
27:00 - patterns for example I said they Sunday
27:02 - will have totally different traffic
27:04 - patterns at least on the average website
27:06 - and you have some seasonality so for
27:09 - example you know if you have some
27:10 - ecommerce shop like Christmas will be
27:13 - much higher I think like Amazon is
27:15 - making or amazon.com for shipping stuff
27:18 - and it's I don't know making fifty
27:21 - percent or so of its revenue around
27:22 - Christmas or at least some crazy number
27:24 - so they just have a seasonal trend and
27:27 - they know ok November and December will
27:29 - be different than all the other months
27:30 - because everybody is doing their online
27:32 - shopping there well the one thing that
27:35 - is not great if you have machine
27:37 - learning is you if you have like some
27:38 - irregular pattern where there is no real
27:41 - pattern
27:41 - it's just irregular because then you
27:43 - will not be able to properly find
27:44 - anything unfortunately we cannot help
27:47 - with that
27:48 - and then you often try to find anomalies
27:50 - in that data so you have for example a
27:53 - point anomaly a point anomaly would be
27:55 - assumed your your bank knows how much
27:59 - money you get out of the ATM machine
28:01 - every time so whenever you go to the ATM
28:03 - machine you get I don't know let's say
28:05 - 50 euro and you normally do that and at
28:08 - some point you get a thousand-year out
28:10 - of the ATM machine let's assume that's
28:12 - possible then that would be a point
28:14 - anomaly because normally it's always
28:16 - like you get money you get money you get
28:18 - money always the same amount but that
28:19 - one time is a totally different amount
28:21 - so that would be a point normally and
28:24 - you could have a contextual anomaly
28:27 - where kind of the context doesn't fit in
28:30 - with the rest the contextual anomaly
28:32 - could be like and you have more requests
28:34 - and you have more CPU and memory usage
28:36 - in your system but the number of
28:39 - requests is actually lower than before
28:41 - all the traffic is lower than before so
28:44 - you have some kind of metrics are
28:46 - pointing in one direction but another
28:48 - one is kind of like in a different one
28:49 - then you have this contextual anomaly
28:51 - we're kind of in the context of all of
28:53 - them together it's kind of going the
28:55 - wrong way and then you have collective
28:58 - anomalies where you have
29:01 - assume that everything is going in one
29:04 - direction but something is kind of like
29:07 - an outlier out of how the collective
29:09 - kind of direction is going then you can
29:13 - have breakouts these are kind of like
29:15 - changes in your data which are not
29:18 - really anomalies they're just changes
29:20 - like you sometimes have ramp up for
29:21 - example you make an ad campaign and more
29:24 - and more people come to your website
29:25 - then you probably see some shift we're
29:29 - just more and more people are coming
29:31 - this is probably not an anomaly but the
29:33 - result of your ad campaign or you can
29:35 - have a mean shift which basically you're
29:37 - deploying a new version of your software
29:39 - and suddenly you only need half of the
29:41 - CPU anymore because you fix some bugs or
29:43 - optimized something there would be a
29:45 - mean shift where you have like just like
29:48 - you deploy the software it's just
29:49 - dropping to some other value and that's
29:51 - kind of like the new baseline which will
29:53 - always follow whereas the ramp up is
29:55 - just something that develops over time
29:59 - yeah if you want to detect anomalies
30:02 - with machine learning you basically have
30:04 - two options you have a supervised or
30:06 - unsupervised machine learning the good
30:10 - thing is in supervised machine learning
30:11 - is you know okay these are the inputs
30:13 - and these are than the outputs the bad
30:15 - thing about supervised machine learning
30:17 - is you will need to have that annotated
30:19 - data so somebody will have to kind of
30:21 - have gone through all of the data and
30:23 - say like okay with this input like this
30:25 - is an anomaly now and this is not an
30:27 - anomaly and oftentimes you don't have
30:29 - that data so very commonly you don't
30:32 - have supervise but instead unsupervised
30:34 - machine learning where you say okay this
30:36 - is the input this is the output and we
30:38 - assume this isn't normally because this
30:40 - is kind of different of what we had
30:41 - before um yeah examples out of the ITER
30:46 - you get suddenly a spike of five
30:48 - hundreds because somebody made a bad
30:50 - deployment and you know okay something
30:52 - is wrong
30:53 - more 500 something changed in our system
30:56 - we need to fix it again and you could
30:58 - have something like security events for
31:02 - example you have some unusual DNS
31:03 - activity where you could do DNS
31:05 - exfiltration for example if an attacker
31:08 - wants to get out information out of your
31:09 - network in a very sneaky way they could
31:11 - just call subdomains under there
31:14 - control with that specific information
31:16 - they want to get out in as a subdomain
31:18 - and most systems don't detect that but
31:21 - if you kind of like look at the DNS
31:23 - traffic you can spot that even though it
31:25 - can be pretty well hidden or you could
31:28 - have some business analytics where you
31:30 - know okay Jenny we have these log
31:31 - entries or log events suddenly we have
31:34 - something totally different
31:35 - and then your machine learning will
31:37 - probably or hopefully picked it up and
31:38 - say okay here we have something that
31:40 - doesn't follow the regular patterns or
31:42 - events that we had before so something
31:45 - must be off here you could totally do
31:48 - visual inspections and we always say
31:51 - they are not enough interns in the world
31:53 - to monitor all the dashboards that you
31:54 - could build with sorry for the interns
31:59 - but the Sproles are not what you want to
32:01 - spend your summer on especially if you
32:03 - have like some complex or fast-moving
32:05 - data yes you can stare at the graphs the
32:08 - entire time and after one or two weeks
32:10 - you probably know okay these are the
32:11 - patterns that I'm expecting but hey you
32:13 - can easily miss something and B that's
32:15 - probably not where you want to spend
32:17 - your time at what this time is kind of
32:19 - best used for so for example if you have
32:22 - this data here this would be unique IP
32:25 - addresses which have connected to my
32:27 - system worse the anomaly we don't really
32:30 - see one we'll get back to the data that
32:33 - is these are actually locks from our own
32:36 - website where we have counted or it's
32:40 - basically it's an engine X proxy and
32:42 - we're just using the engine X X s log to
32:45 - see what is going on on our site and
32:47 - then to find anomalies of what is going
32:49 - on and what is not really going well on
32:52 - our site so you could totally define
32:57 - some static rules then the problem is to
32:59 - define rules somebody needs to know the
33:01 - system well enough to define the rules
33:03 - you could have false positives and
33:06 - negatives depending on how kind of like
33:08 - narrow the band of alerts would be and
33:11 - as soon as your system starts changing
33:13 - over time you will need to adjust it so
33:15 - if you have more visitors to your site
33:16 - you probably will need to change your
33:18 - alerts on a kind of a frequent basis or
33:20 - adjusting to whatever your system looks
33:22 - like today so if you want to set a
33:25 - threshold here like what is the number
33:27 - of unique
33:28 - Pietrus is connecting to your system
33:30 - which values would you pick like yes you
33:32 - could say maybe here at the bottom to
33:35 - below 2,000 might be a problem you could
33:38 - say that and then you could say oh maybe
33:40 - more than 14,000 would also be a good
33:43 - threshold but there's a huge gap in
33:45 - between and you probably don't really
33:46 - see like is there anything going on in
33:49 - my system that is okay or not okay
33:52 - where's the anomaly like yes you can't
33:54 - set the thresholds but there will be
33:56 - super coarse-grained yeah so we need
34:00 - some kind of machine learning let's see
34:04 - what we can do there and yeah normally
34:08 - machine learning is pretty CPU intensive
34:11 - or if it can run on a GPU you can also
34:14 - run it on a GPU since all my demos are
34:16 - on my laptop it's like running slack or
34:18 - zoom or whatever which will also eat up
34:21 - all my CPU so you can totally build it
34:24 - yourself on some very common frameworks
34:26 - for that are tensorflow is especially
34:28 - well known from Google Kara's psych it
34:31 - and loads of others are where you can
34:33 - just build your own machine learning
34:35 - algorithms or you have some data and
34:37 - basically you want to extract some value
34:40 - out of that and find your own am√©lie's
34:41 - so these are just very widely used tools
34:44 - you can use how do you build that
34:47 - pipeline kind of like the machine
34:49 - learning component that is the fun part
34:50 - and people who are data scientists will
34:53 - always say like yeah we're doing data
34:54 - science the entire day but what they
34:56 - basically do is they probably spent five
34:59 - or ten percent of their time on data
35:00 - science and the rest is like 70% is
35:03 - probably of the rest of the time is like
35:05 - getting the data and cleaning up the
35:07 - data and 30% is complaining about not
35:10 - having having clean data yeah because
35:15 - it's kind of a very common thing you
35:16 - need to find the right data then you
35:18 - need to prepare it in the right way so
35:19 - you can load it and then you need to
35:21 - clean it up you need a proper data
35:23 - storage system where you can keep all of
35:25 - the data that you want to keep around
35:26 - and then you probably want to have some
35:28 - optimization algorithm to make stuff
35:30 - scale and make it faster yeah and who
35:34 - have takes the expectation that your
35:35 - data will be clean and will work the
35:37 - right way it's always kind of cute so
35:40 - yeah finally data scientists ask them
35:42 - and they wait we'll probably make an
35:44 - very unhappy face about data cleaning
35:46 - but it happens and then people always
35:49 - ask like which one is the fastest
35:50 - machine learning algorithm and for all
35:54 - these performance things this is my
35:56 - favorite comic well you have yeah and
35:58 - the similar conditions you're testing to
36:01 - systems we're testing the squid and the
36:03 - house cat and we want to know which one
36:04 - of them is more intelligent or faster or
36:06 - more performant or whatever and you just
36:10 - need to find the right scenario because
36:12 - then your system will always win and
36:14 - this is really the bond number one thing
36:16 - whenever some company benchmarks their
36:18 - tool against some competitors they will
36:20 - probably find one use case that is very
36:22 - good for them and very bad for their
36:24 - competitors that's why I would not trust
36:26 - any vendor based benchmarks I would not
36:29 - normally call them bench marketing and
36:32 - not proper benchmarks because well yeah
36:35 - people do something like this so if you
36:37 - ask me like which is the best machine
36:38 - learning algorithm um this is the answer
36:42 - so what you generally want to do is when
36:45 - you want to find some anomalies you want
36:47 - to find out like what is normal like
36:49 - what is the baseline of stuff that
36:51 - happens here for example you could see
36:53 - we have the black line is some I don't
36:56 - know I think it's that the data transfer
36:57 - the visitors on your website or
36:59 - something like that and you can see
37:00 - there is this pattern where probably the
37:03 - first two a week days then the next two
37:05 - are probably Saturday Sunday and then
37:07 - you have five more days which are
37:09 - probably a weekday again and then you
37:12 - try to get that baseline of kind of what
37:14 - is normal in your system so you normally
37:16 - calculate within some confidence
37:18 - interval you calculate what is the upper
37:21 - into lower bound so the upper bound is
37:23 - here at the lower bound is blue and
37:24 - you're just trying to find the system
37:26 - that is getting closer and closer to the
37:28 - actual kind of X curve or the actual
37:31 - value that you have to get kind of like
37:33 - this dynamic baseline of how close are
37:36 - you getting to what you're expecting and
37:39 - yeah most of the data is normally
37:41 - distributed
37:42 - sometimes you're unlucky and it's kind
37:44 - of like paranormally distributed and
37:45 - that will totally throw off any machine
37:48 - learning so if you don't have like a
37:49 - good regular distribution of your data
37:51 - and no machine learning for that will
37:53 - save you yeah so this would be another
37:56 - example so you can see here we had the
37:59 - first three directions we didn't really
38:01 - know what the system was up to so and
38:03 - here the upper and lower bound is just
38:04 - like this light blue background and that
38:08 - the darker blue lines kind of like this
38:10 - is the actual gate that is coming in but
38:12 - then after about three durations this
38:14 - system here has learned like what is the
38:16 - baseline and what is kind of expected of
38:18 - the values and then you can see the band
38:21 - that the light blue band around the
38:23 - actual data is getting narrower and
38:25 - narrower so it's learning what is normal
38:27 - in that system and at some point you
38:29 - have these colored dots the more it's in
38:31 - the red the more of an anomaly it is so
38:33 - you can for example see pretty much at
38:37 - the right hand side there is this where
38:38 - it's falling down there is kind of like
38:40 - this is an anomaly which would probably
38:42 - be pretty hard to spot otherwise in in
38:45 - that system here and the one thing that
38:50 - you need to keep in mind is that
38:51 - whatever your baseline is your baseline
38:54 - will probably evolve unless you have a
38:56 - system that is super stable which is not
38:57 - that common is that depending on how
39:00 - much more visitors you have or whatever
39:03 - you change in your system the baseline
39:06 - will change and you should have a system
39:07 - that keeps learning the new stuff and
39:09 - ages out the other information so that
39:12 - is not stuck on any other information so
39:14 - here yeah I think it's even the same
39:16 - data you can see three iterations and
39:18 - then it kind of knows what what is
39:20 - happening and then you can see we have
39:23 - since I shouldn't move out of the way
39:25 - I'm not trying to head over all the way
39:30 - so here you can see here we have this
39:33 - anomaly and you can see here the system
39:36 - has also learned the anomaly so it's
39:38 - kind of like expecting there might be
39:40 - something going on here so it doesn't
39:42 - really since it's not a supervised
39:44 - machine learning it's just learning what
39:46 - kind of is happening in the system and
39:47 - it's then continuing that trend and kind
39:50 - of feeding that normally into the model
39:53 - for the future and will keep up to date
39:55 - with whatever happen in your system yeah
39:59 - that's another example where you can see
40:02 - stuff is happening and at some point
40:05 - there you have an anomaly and then it
40:07 - keeps learning like it's aging
40:10 - normally again because here you have the
40:11 - anomaly here it's expecting Myanmar
40:13 - normally since it doesn't happen here
40:15 - the anomaly is getting smaller smaller
40:17 - and smaller so the anomalies are kind of
40:19 - being aged out over time is getting
40:21 - again as well yeah you can have a single
40:25 - time series for example unusual traffic
40:27 - where you can see okay this is a perfect
40:29 - pattern I'm expecting this is what might
40:31 - happen you could also break that up into
40:33 - multiple time series so you could have
40:35 - not one metric but multiple metrics you
40:37 - would be interested in and then each of
40:39 - those would be modeled as an independent
40:42 - baseline so you have a machine learning
40:45 - model for each one of them so for
40:48 - example instead of having two global
40:49 - traffic you could say I want to have
40:50 - traffic by country and then you can see
40:53 - you can break it down into different
40:55 - country countries and then you can see
40:57 - sometimes you just don't have traffic
40:58 - for example here in France probably they
41:00 - had either a strike or a national
41:02 - holiday you never know with the French
41:04 - but but something happened and nobody
41:07 - was online anymore there and so it can
41:10 - make sense to break it up into different
41:12 - intervals to see how that they tourism
41:15 - morning okay so as I said I'm just using
41:19 - for my example I'm using from our own
41:21 - website some some visitors so it's just
41:24 - I think one month so or no it's two
41:27 - weeks of data or something like that
41:28 - it's pretty much the energy engine
41:30 - x-axis lock that you're expecting so you
41:32 - can see you have some bytes that you
41:34 - have transferred with you the geoip look
41:36 - up so we know okay this visitor here
41:38 - probably came from India this is the
41:41 - approximate longitude and latitude where
41:42 - they have come from and then you get
41:45 - like the other information that you want
41:46 - so you see then was the remote IP
41:48 - address they did a get and got a 200
41:51 - back though we couldn't extract any
41:54 - proper browser information from that
41:57 - okay and since I always try to do at
42:02 - least something like this what this move
42:04 - might look like so for example here you
42:07 - can see I have just taken January to
42:09 - okay one and a half month march website
42:12 - visitors which were like around 15
42:15 - million website hits and this is the
42:18 - general distribution so you can already
42:20 - from this data you get a rough idea what
42:21 - is going on in the system so you can see
42:23 - here
42:23 - this was probably weekend these were
42:25 - weekdays weekend again so you get kind
42:28 - of the general pattern then you could
42:31 - see this is just some some default
42:35 - dashboard that we have you can see where
42:37 - are people coming from like where from
42:39 - where is your website being accessed so
42:41 - you can see for example Germany and
42:44 - France are doing pretty well if we zoom
42:46 - in a bit more here we could probably see
42:48 - Austria as well but since I'm not online
42:51 - my map data would need to load online so
42:54 - here somewhere would be Austria and then
42:57 - you can see ok these are the return
42:58 - codes of my data like this is pretty
43:00 - much all the data you can extract from
43:02 - an engine x-axis log so this would be
43:04 - just the return codes you have you could
43:05 - extract like these where the top URLs
43:07 - you hit you see since you have to buy it
43:10 - sent you see how much traffic your
43:12 - website was sending and then you could
43:15 - also do like a breakdown of okay
43:16 - this was the these were the operating
43:18 - systems that were accessing our website
43:20 - and these were I don't know the browsers
43:22 - and their specific versions so that is
43:24 - what you get in the engine x-axis lock
43:25 - there is no machine learning necessary
43:27 - for that that's just block parsing so no
43:29 - no major magic here then you could write
43:33 - your own visualization so for example
43:35 - this was the number of unique remote IP
43:38 - addresses I've shown you before and this
43:41 - was the the bytes that we were
43:43 - transferring from our website which they
43:45 - kind of correlate slightly but not that
43:48 - wrongly for especially here kind of
43:50 - there is no strong correlation to that
43:51 - one and does anybody see the anomaly in
43:54 - the top chart here or should I let you
43:59 - stare at that dashboard for a week and
44:01 - maybe then you can see it I think if I'm
44:04 - not mistaken the anomaly is this one
44:05 - here so this like this downward thing
44:09 - though it's very hard to see we can
44:13 - actually leverage the machine learning
44:14 - stuff now our to see what is going on
44:16 - here
44:16 - so I've quickly built two visualizations
44:20 - for that the first one is about having a
44:23 - single metric that is the same
44:24 - information that I've just shown you
44:26 - that is the unique IP address that we
44:29 - had in our system and you can see our
44:33 - system needs approximately three
44:34 - directions to learn so you can see here
44:37 - these are the first three days this was
44:40 - Wednesday Thursday Friday and then the
44:43 - algorithm thought well I know what to
44:44 - expect the problem was then we have
44:46 - Saturday Sunday and the traffic pattern
44:48 - is totally different so you're kind of
44:49 - throwing off the machine learning
44:50 - algorithm here a bit because that is
44:53 - here you're having your five days of the
44:54 - week days again and then you have your
44:56 - weekend again and you can see the
44:58 - pattern continues that you need about
45:00 - three durations to learn a pattern
45:02 - because we can one weekend to week and
45:04 - three about here it got tweaked days
45:08 - pretty well already and here it also got
45:10 - the week end then and from then onwards
45:12 - it knows okay this is kind of the
45:15 - weekday pattern that I'm expecting this
45:17 - is the weekend is the weekday pattern so
45:19 - system kind of learned where how stuff
45:21 - works and this is here where we have an
45:23 - anomaly so let's quickly see normally
45:34 - and you can see here something was
45:38 - happening like generally we would expect
45:40 - traffic in this area here but somehow it
45:43 - was falling down and each of these
45:45 - points is actually an anomaly and it
45:47 - actually has an anomaly score between
45:49 - zero and hundred this is how we modeled
45:51 - that but you could totally build
45:52 - something similar the closer it gets to
45:55 - 100 the worse the anomaly is and then
45:57 - you could even have like the list which
45:58 - says like Oh normally I'm expecting that
46:01 - value here 1,400 something I got only 86
46:06 - so this is 17 times lower than our than
46:10 - I was expecting and this was on the 27th
46:16 - of February last year anybody remembers
46:18 - what happened then no okay let's jump
46:25 - back to my slides I think we've seen
46:27 - most of those already and that was when
46:31 - there was the first ever bigger as3
46:33 - outage from aw yes and half of the
46:37 - internet basically relied on that so
46:40 - yeah three went down and that affected a
46:43 - lot of stuff including Amazon stuff as
46:45 - well because there were also themselves
46:47 - relying on s3 to be available like
46:50 - everybody else
46:51 - so unfortunately we had the dependents
46:54 - in our website which basically killed
46:55 - our website and our downloads and lot of
46:58 - other stuff we then thought about should
47:01 - we make some multi cloud strategy or
47:03 - whatever but in the end nobody
47:05 - complained because half of the internet
47:06 - was down anyway and if the internet or
47:08 - if your own site is down and everybody
47:10 - everything is on fire nobody complains
47:11 - that your downloads are not working so
47:13 - we kind of gave up on making stuff
47:15 - unnecessarily complex because there was
47:17 - just no point of that but we you could
47:20 - very nicely see that normally there yeah
47:26 - we are nearly out of time okay
47:29 - then I then I will stick to slides then
47:33 - you can build the same thing with
47:34 - multiple models here I have broken my
47:37 - data down into the response codes that
47:39 - my website is sending out and then you
47:43 - can find further anomalies so for
47:45 - example here I have an over the normal
47:47 - is code that is like the top one is kind
47:49 - of like this is anomalies over all the
47:51 - red things are where the anomalies are
47:52 - and then you can break the entire thing
47:55 - down into the response code so you can
47:57 - see for the 404s we have a band of
48:00 - trouble over there this was for example
48:04 - our on our block and we suddenly had a
48:07 - spike of 404
48:08 - this is when we published a bad link on
48:10 - our own block and suddenly the 404
48:12 - started to spike and this was when we
48:16 - had in our CMS since we're using a CMS
48:18 - and CMS are mostly crap when we kind of
48:20 - broke some links internally
48:22 - unintentionally with the CMS change you
48:25 - could totally see that here that we
48:26 - suddenly had this pipe of anomalies
48:28 - where yeah suddenly stuff is not going
48:30 - well you can have combined multiple
48:35 - models and see what they're doing so
48:36 - here I'm combining this was the remote
48:39 - IP addresses the example which I've
48:41 - shown before where you can see this was
48:43 - when our website went down because of
48:45 - Amazon s3 so this is here then was the
48:48 - a3 outage because nobody could access
48:51 - our data anymore and these are the 404
48:53 - and these two totally look correlated
48:55 - right so something is happening here and
48:58 - then suddenly the 404 spiked
48:59 - unfortunately these two events were
49:01 - totally unrelated because this was at
49:03 - the AWS outage and
49:05 - this was our own change on the CMS and
49:07 - that will screw off or throw off any
49:09 - machine running because it tries to
49:11 - learn and correlate some data but if
49:13 - there is no real correlation it will
49:15 - just be kind of like try to infer one
49:17 - which is not there so yeah correlation
49:19 - doesn't mean it's actually a causation
49:21 - everybody knows that but always keep
49:23 - that in mind
49:25 - and yeah it's kcd and extract the best
49:28 - once you take a statistics class does it
49:31 - help to make the distinction maybe yeah
49:35 - and then they're always like what is the
49:38 - cause and what is the reason or like
49:41 - what is the cause and what is the effect
49:42 - um for example for cancer waste it's
49:45 - very interesting study that in the US
49:46 - and the cancer rate has spiked and cell
49:49 - phone usage spiked afterwards so here
49:52 - the theory could be that cancer is
49:54 - causing cell phones which is also not
49:58 - the right thing but if you just look at
49:59 - the data and misinterpreted that is what
50:02 - you might get um yeah
50:05 - any correlated features would mess up
50:07 - your system there's unfortunately no way
50:09 - around that yeah finally thing here what
50:14 - you can also do is future predictions so
50:16 - the yellow stuff here these are future
50:18 - predictions and you can see the further
50:19 - it goes into the future the less certain
50:22 - they are because the more coarse-grained
50:24 - the entire system gets but this is very
50:26 - helpful if you like I'm expecting this
50:28 - number of visitors or this is the disk
50:29 - space I'm expecting in the future or the
50:31 - resource usage and this can predict for
50:33 - the future like this is what you will
50:35 - probably have a new system given that
50:37 - there are no other anomalies yeah then
50:40 - you could start doing like categorized
50:42 - users but let's keep that for today so
50:45 - to wrap up we've seen machine learning
50:47 - domain where I picked like mainly ops
50:50 - data and then some nginx data set where
50:53 - I've played around with if you want to
50:56 - know more about machine learning one a
50:57 - very nice paper I've seen is this one
51:00 - here best practices for machine learning
51:03 - engineering which has some very nice
51:05 - rules so for example the rule number one
51:07 - is don't be afraid to launch a product
51:09 - without machine learning because you
51:11 - might not need it and then yeah if you
51:13 - can interpret the model properly and
51:16 - debugging will be much easier if it's
51:17 - not just a black box
51:19 - spits out random data and plant launch
51:22 - and iterate on stuff but it has 43 rules
51:25 - all about machine learning which
51:26 - probably makes sense if you build
51:28 - anything in that space and yeah and
51:30 - don't forget maybe at some point machine
51:32 - learning is not the hot new thing
51:33 - anymore
51:35 - but something else comes around so yeah
51:37 - Silicon Valley is probably heading
51:39 - somewhere else already again anyway
51:42 - that's it I think we're pretty much out
51:45 - of time if you have questions come to me
51:47 - find me afterwards talk to me if you
51:49 - want to have stickers I have loads of
51:51 - stickers here so if anybody wants
51:52 - stickers yeah very nice take them with
51:56 - that I think we're done
51:57 - thanks very much wait I'm always taking
52:02 - a picture because my colleagues don't
52:03 - know where I am and this is my way to
52:06 - prove that I've been working today smile
52:10 - everybody
52:12 - yeah you can always wave thank you enjoy
52:16 - the break

Cleaned transcript:

okay everybody good to go everybody sit awake good let's see if we say manage to stay that way so I'm Philip I want to talk about some machine learning some hyping some making fun of stuff so let's see what we can take this so I work for elastic the company behind elastic Serge dogfish Cubana like previously called the elk stick now we call it the elastic stick I'm part of our infrastructure team I always say in the middle that is a UNIX pipe a kind of piped it out into developer advocacy so I try to speak a lot about the good stuff that we do and why am i talking about machine learning well every company needs to have some machine learning in their stack you can see we have kind of hitting it on the righthand side at the bottom there kind of like we have something about cloud we have something about machine learning like all the usual stuff that you need to have we'll get back to that later on but at first I want to talk a bit about machine learning in general and what it is and what it is not and what it could do for you so some months ago there was this like machine learning is going viral and everybody was going oh we need machine learning we need to do more about that and one thing about going viral was also that who remembers that iOS park when suddenly the eyes were exchanged for a different character that was actually machine learning as well because that was kind of some Apple added some machine learning for the keyboard that it would replace specific characters or words and that was kind of as soon as you received that bad eye which was replaced by some other character that was already machine learning at work because then your system would learn that other way to to write the replace the term and only once you receive such a change diet then you would be or would do the same when you device would do the same thing because your device had learned that so that was kind of the power of machine learning which was kind of a bark and like kind of a bit of a virus since it was spreading like that but yeah machine learning is kind of going viral in that regard yeah and then there are always the people who are saying like oh we're doing some very fancy technology and even though we probably would have just needed a few if statements which would have been much easier to write in debug which probably looks something like this like have this big digger and then it's just using that little shovel there and to get going so that is some people we are wielding kind of the powers of machine learning just to do something very small and confined and read like easy to get going with sometimes you might need it sometimes you might not need it I guess some of you are probably familiar with that that is the Gartner hype cycle that is probably the only thing that is true that Gartner is always bringing out and because everything else is kind of like just making stuff up or at least it was always my impression that whatever the Gartner is predicting for the next five years is probably not going to be the way it will be but their hype cycle is kind of like very very true like you have some technology trigger and then you have like this peak where everybody is saying like oh this will be so awesome and this will do all the things I can still remember when everybody said that about no sequel and then people were kind of like oh maybe a relational databases were not all that bad and I have the feeling that kind of machine learning and AI and whatever you want to call it this kind of falling into the same perception every now and then and so you have some peak and then you get kind of like a bit of a disillusionment and at some point you may be reached at 2 or 3 activity will machine learning or AI solve all your problems um unfortunately it probably will not but that's probably no surprise so anybody who has been in the field for some years knows ok all the promises like everything will be solved by this technology unfortunately it's not that easy or fortunately it's not that easy otherwise nobody would need us anymore so I want to talk about like a bit machine learning in general and like putting it in with AI and whatever then I will pick a domain of what we can do since our use case is always about machine learning for ops data I will use some op stater for that and then I'll just play around with some data set for a quick finish let's see where we can go so machine learning there's always this misconception of this discussion like what is artificial intelligence what is machine learning what is deep learning how do they belong together isn't all the same thing because some people always say oh it's basically the same thing it's just like depending on when people started with something they had to come up with a new fancy term and that fancy term could be either AI or if you came a bit later it would be machine learning or if you're very uptodate and it would be deep learning I kind of find this very nice this graphics so you can see it all kind of started in the 50s already back in the days I don't assume that most of you were around an IT back then but maybe somewhere or maybe a little later so in the beginning there was machine learning and there were high hopes and after some initial breakthroughs people assumed well general artificial intelligence will be sooner things so general artificial intelligence will be like machines will be as good as humans at almost everything and they will just learn one field by the other and and initially it was just it started as a very narrow field like you would do something where machines would be good in one specific field and then the idea was well we will just generalize that and this will be applicable to any other thing afterwards and it will just keep generalizing that until you're kind of on human level unfortunately that didn't play out that well and then in the 80s we had like with machine learning we had another wave of trying debt and then like the 2010s we had another wave of that with deep learning yeah so the general purpose of generally AI is always let's get something that is human level this is something we have not reached yet and we don't even have a proper timeline like in the 50s 60s and 70s it was assumed like oh we will get there but it didn't turn out that way because in the 70s there was the socalled AI winter and this was also one of the reasons why you had to find a new fancy name because a lot of people got burnt by that they assumed Oh artificial intelligence will solve everything and a lot of money was pumped into that and people wrote papers and plans and everything and at some point it didn't really plan will work out the way people planned so there was kind of like the first wave of ai ai then came the air I winter and afterwards people came back with better algorithms more computing power it's kind of like coming back in waves and you're getting closer to your target but with generally AI that's something we have not really achieved yet and we don't really know when that will happen what we are very good at right now is something like narrow AI where you have one specific task and the goal is to have something where you're as good or nearly as good as a human for example to identify an image so I was two weeks ago I was in San Francisco and I uploaded this picture to Facebook and then Facebook does the classification automatically for you so for example if you look in the meta tags what Facebook does is it adds the alt tag and it says like image may contain and then it's just guessing what could be in that image and it's actually pretty good because yeah they have a lot of data they have trained on a lot of data and they're pretty good at just detecting what might be in your image even though I haven't told anything like it didn't tag and say okay this is the Golden Gate Bridge I've just uploaded the image without other any other meta information and the narrow eye in that case like the image classification that just kind of found or detected what might be in that image and it's actually pretty good another thing you might have seen which was pretty recent is that cylinder is cutting some marketing jobs where they're automating the marketing now because previously they had a lot of handcrafted content and now they kind of said well we can automate a lot of that with machine learning we can hyper localize the content where we know okay you're interested in these products and you're living in that specific city and previously somebody would probably hand curate the set of data that or the products that might be interesting for you or would write some content that is tailored for someone in a specific region but they came to the conclusion with machine learning this is much easier to do or will scale much better and they don't need all those humans anymore so they now start to kick out some of their marketing people's some will move to a different department but a lot of them will be just kicked out which sucks and people are very afraid that all machine learning will take over all my jobs probably some but probably also not all of them because you might have seen this example as well where some people used Google Translate to translate sorry for the German if you don't speak German some economic topic and this sentence here is actually the very first sentence of Wikipedia if you look up folks with shirts really which is ya know some German sentence and this is what people got when they use Google Translate actually I tried it yesterday evening by now Google got kind of clever I don't know if somebody went in there to fix that because the people tweeted about that and it made the rounds but by now Google Translate got a bit smarter again and the same thing is still not great what you're getting now but it's still got a bit better and it's not just economics of economics and yeah the same word over and over again but yeah having the fear that professional translators will go out of business like next week it's probably a bit premature we're not there yet like it's kind of general idea yes you can do nice stuff and if you don't understand some language you can use Google Translate to get the idea but it's probably not going to replace proper writing so for example our support team when we do a support in some localized language we always say for example people in Japan don't want to speak English and for them it's super important to get support in Japanese and we always say yes you will get support in Japanese during Japanese business hours because then our people are online but if you want to get help after hours either you can write you text in English in English and somebody will help you or we will use Google Translate and we're actually handling support cases with Google Translate and whatever comes out of Google Translate we will try to help you with and if it's no good unfortunately we will need to wait until somebody wakes up who speaks Japanese yeah and then there's always this chat pod thing I know that Vienna has this kind of I always said the chat pod bubble because a lot of people are working in in chat pods I hope not too many are here and come with the pitchforks afterwards but I have no or I had this thing once or twice where somebody came to me and I was or I was talking to somebody and they were saying I'm doing AI and I Wow that sounds super advanced what are you doing and then they people say well we're using this Facebook API and we're sending some data they are in it senses something back and so we're doing AI and I'm always like you doing api's it's just like the pee is probably silent in your AI yeah and I like to say yeah AI API is kind of it's very close some people don't make the distinction we can't have a discussion about that but at least for me when you say I'm doing AI is not just calling some Facebook or whatever API and get something back and use that this is one of my favorite examples I think this is the Pancho one of the chat BOTS which you can ask like what is the word I like and then you can see yeah it's working well or not that well and I get the idea that is trying to be cute but like cute is not the main thing I want if I want some information like just because it's trying to be to speak French or saying it dozed off for a few seconds and that doesn't make something very smart or intelligent it might be cute at first but if I want to get some information this is not going to make me very happy with something else I recently had I'm using a one Telecom and I recently called their hotline and normally you know you dial in and then you need to press five for that service and press three for whatever and they didn't have that anymore they had the whole thing voice automated so you would need to speak to the system and it didn't work at all like my internet was not working and I was trying to in various ways to describe like what my problem was and even when I said internet it would say oh you want to know something about your bill and I like no that's not where I went and then you do like five circles or so and in the end you could get back to that dial like five to get support for your internet or whatever so yeah the future might be very bright about all these automated systems with voice control and chat pots and whatever um there was sometimes there is super annoying or maybe I'm just too old and grumpy by now anyway um and this is other thing maybe you've once seen a stateless jet pod which makes for great conversations because it just knows the current sentence and it doesn't have any context context about what which is another kind of common problem this is solved you can totally do that but like simple chat pod will not be able to do that because yeah life is hard so what is machine learning after all kind of together so you basically have an algorithm than cat that can work with some data you can kind of learn from that and then you make some determination or prediction out of that data so it's basically a trained machine you haven't hand coded the rules so for example one common example could be like classification of spam emails initially people would write rules like if there's something like I'm from Nigeria and I will send you 100 million if you detect something like that in an email it there's a very good chance that this might be spam but of course people are clever and they will reword stuff and then it's kind of like it's something point five billion it's not Nigeria but some other country but so people will found various ways around but with machine learning you can actually not kind of write these rules explicitly but can just learn like what does the spam email look like and probably can find out these are the different kinds of spam emails like these are the ones who want to sell you viagra did this one wants to get your money and this one is just some plain annoying thing yeah and this is the proper definition what machine learning is but you will not get any venture capital with if you just write this because it sounds very dry you learn from some experience with some some class of tasks and then you perform some measure and then the performance at that task improves with the experience the more experience you have the better the machine learning should get this is kind of like what happens this is kind of the algorithmic view but again this is not getting you any venture capital or you cannot build a startup just based on this one sentence what machine learning is generally doing is something like it's one indie of these four categories commonly so you could have some classification so you could have an either like binary classification guess no or you could have a classification which has kind of like more outputs for example you could have a picture of somebody and then you could try to do a sentiment analysis like is this person happy said angry whatever and then your algorithm tries to classify what kind of facial expression does this person have so that could be a classification then you have regressions where you basically try to know or calculate what a specific value will be in the future which could be like a temperature or a stock price where you do the prediction of all the stock prices will go up or go down you can do ranking ranking is pretty much like you search for something and then you don't just have the hits but the system learns over time like what is it good hit and you kind of like feedback information of what is a good hit into that search system so that your ranking will improve over time and then clustering is just putting stuff that belongs together or isn't kind of some groups that make sense together you just try to classify similar things and then say okay these are all the things that are in that classification or clustering yeah this is what machine learning might look like that one machine is teaching the other little machines of what it knows or at least that's how we humans do it pretty much would be nice like if it was like that so the most common thing is when people say oh we were leveraging machine learning is that they really doing linear regression so basically you have some some information these would be like the little dots and then you want to find the linear regression which is basically this red line and you try to optimize that line and try to keep the distance of all these dots as close as possible to the red line and the more data you have the better that line can be and the more you kind of get to the value of what makes most sense for your data set so if you have like very sparse data and you only have three or five points you will have a very coarse linear regression if you get very or have lots of data the linear regression can kind of optimize much better for all the data points you have and if people are just saying oh we're doing a machine learning oftentimes it just means yes we have this linear regression which is not all that fancy but it can still do meaningful stuff for you yeah whenever people say that assume if there is no other information it's probably a linear regression not something more fancy and then you can still make the distinction between supervised and unsupervised machine learning supervised machine learning is and you have labeled data so you have some input data and then you have some output and you know this is for example with spam emails if enough people have said like this is spam this is not spam you have a huge data set which Google probably has and then oh this is a spam email and this is not a spam email and they can turn on that data because they have some supervised data await and no these are the inputs these are the outputs and then you can take a lot of that data to for your training data and then you can keep some of the remaining data for your test data you could do the same for unsupervised machine learning where you don't have any annotated data you just try to find some hidden relationship in your data so you don't know what is actually this good this is bad and it's dissonant normally you just have data and you try to find some relationship out of that the finding relationship in data can be kind of tricky there's this very nice xkcd comic which I've broken up for better readability here where somebody says like Oh jelly beans cause acne and then the scientists have to investigate even though they just want to play Minecraft so you can see that was kind of like one year ago whenever minecraft was very popular so that first to say like know what like with the 95 percent confidence we can say like no acne is not caused by jelly beans and then somebody says well but it I have heard it it's just a specific color and then they do the test for all the different colors and it's a bit hard to see but all of them like no we found no link and then there is the one where it says whoa and there it says oh there is like a link the problem is if you have like a 95% confidence interval and you make and you run the trial on 20 different data sets just by the mathematics one of them will then be kind of true and then you can write the big news article the green jellybeans obviously daling a workhorse or we're causing acne with a 95 percent confidence and only 5% chance is in there but that's kind of like if you run the same thing 20 times there is a chance that well with 95% confidence one of them will fire if you run it just 20 times and that's pretty much what is happening here then you can do reinforcement learning where you basically have a feedback loop so whatever your outputs are you're kind of continuously optimizing that on the feedback you're getting so for example if people are saying this is helpful this is not helpful and you can then keep optimizing whatever your machine learning algorithm is doing and then there is deep learning which is you have the neural network which is also very fancy nowadays because now finally we have enough computing power or mainly good graphics cards to calculate those to basically calculate a probability vector and you have lots of training and then parallel sted that is if you get enough graphics card to actually do that and not and they haven't been sold out to mine bitcoins before which is kind of sad that people are using all that computing power for mining some cryptocurrency but yeah that's a totally different topic so what basically happens here this is a very simple example and we were having you have slept X hours the night before a test and you have studied Y hours before a test and then you try to find like what does this mean for your test score like the green thing on the right hand side that is basically the outcome of the test score and then you have with these two inputs like they're weighted like how important is sleep how important is how much time you have spent studying and then you can have one or more layers the more layers you have the deeper it gets or as soon as it's multiple once then you will have deep learning and they're kind of a hidden input so you you can see they say it's as mysterious you don't really know how the system behaves in there you just have some weighted inputs and then at the end some value plops out and it says okay if you yep X hours and you studied Y hours your test score will probably be I don't know you will pass you will not pass so you will probably get that great so that is what you can do with neural networks which is also very common for example for image classification or the standard example that a lot of people have is like you have this detection of handwriting so people write the numbers from 0 to 10 to 9 and then you try to detect each image or each number and try to defer like ok which number could that be and then you can extract some characteristics of each number and that is what makes up your neural network and then we will say ok this was the number 5 for example whatever is what is really most important there is to have some big data set because you will always need something to Train there so companies like Google Facebook Amazon they have a huge advantage because they just have a shitload of data to work with so for example when people are now surprised oh how can Facebook be so good at detect stuff um well you have helped with their training data for years and years so this is if you have been on Facebook for a long time or a long time ago probably this was what it looked like 10 years ago so it didn't do any machine learning back then but what you could do is you could upload a photo and you could tag people and you could just say ok this is that person and that gives Facebook actually a huge amount of data because it knows now ok this there is the face of a person and that's probably somebody who is male and from the profile of that person they know like the age the gender the ethnicity whatever so it knows a lot of information and can use all of that information later on to then train its machine learning and that's an advantage that you have given kind of to Facebook and all these other big vendors who can build cool stuff and it's very hard to replicate that for you because you don't have this huge amount of data to work with so yes you can make data great again but only if you have huge data that's the one thing you really need yeah and that's what big companies are doing and then people are like Oh a IML will it just use them interchangeably and it really depends like for example when you're raising money then you want to call it AI when you're hiring then normally people say machine learning that's also some distinction I have seen or some people have framed differently they say when you're fundraising then it's AI when you hire in its machine learning when you're implementing it then it's just a linear regression and as soon as you debugging it it's back to the good old printf so like all the fanciness you have kind of at the start is going kind of like down down down until you back it's basics so while it sounds very fancy and it can do some cool stuff some basics are not going away or there are these cattle contests where you have some problem and you try to solve the problem and at the end at the end you might win I don't know a million dollars depending on the carrot contest so what people are doing is we're just starting a competition we are using some machine learning on it and at the end we will profit but normally what you won't really get the profit you will mainly get some disappointment because what is still totally relevant is that you will need to know the domain and need to know what it is about to optimize that it's not that you can just throw some generic machine learning on it and then it will magically find out what it's just the right thing to do and will make you rich unfortunately it's not that easy okay so enough making fun of machine learning let's do something more productive so since I have an office background mmhmm and our product does work pretty well with that I've picked something so you have some trend over time and we try to determine some pattern in our data so for example this could be people visiting your website this could be traffic this could be error messages you have or return HTTP return codes you have all of these you can't try to find some pattern in data like that so the first one is you have some trend which is just like it's growing linearly or it's kind of like falling linearly or you have exponential growth or whatever anything that is not stationary so if you just have some stable value you're not going to find out any trend out of that because well no change then oftentimes you have some cycles in the data cyclists would be the days of the week so Monday to Friday you have kind of the same traffic patterns for example I said they Sunday will have totally different traffic patterns at least on the average website and you have some seasonality so for example you know if you have some ecommerce shop like Christmas will be much higher I think like Amazon is making or amazon.com for shipping stuff and it's I don't know making fifty percent or so of its revenue around Christmas or at least some crazy number so they just have a seasonal trend and they know ok November and December will be different than all the other months because everybody is doing their online shopping there well the one thing that is not great if you have machine learning is you if you have like some irregular pattern where there is no real pattern it's just irregular because then you will not be able to properly find anything unfortunately we cannot help with that and then you often try to find anomalies in that data so you have for example a point anomaly a point anomaly would be assumed your your bank knows how much money you get out of the ATM machine every time so whenever you go to the ATM machine you get I don't know let's say 50 euro and you normally do that and at some point you get a thousandyear out of the ATM machine let's assume that's possible then that would be a point anomaly because normally it's always like you get money you get money you get money always the same amount but that one time is a totally different amount so that would be a point normally and you could have a contextual anomaly where kind of the context doesn't fit in with the rest the contextual anomaly could be like and you have more requests and you have more CPU and memory usage in your system but the number of requests is actually lower than before all the traffic is lower than before so you have some kind of metrics are pointing in one direction but another one is kind of like in a different one then you have this contextual anomaly we're kind of in the context of all of them together it's kind of going the wrong way and then you have collective anomalies where you have assume that everything is going in one direction but something is kind of like an outlier out of how the collective kind of direction is going then you can have breakouts these are kind of like changes in your data which are not really anomalies they're just changes like you sometimes have ramp up for example you make an ad campaign and more and more people come to your website then you probably see some shift we're just more and more people are coming this is probably not an anomaly but the result of your ad campaign or you can have a mean shift which basically you're deploying a new version of your software and suddenly you only need half of the CPU anymore because you fix some bugs or optimized something there would be a mean shift where you have like just like you deploy the software it's just dropping to some other value and that's kind of like the new baseline which will always follow whereas the ramp up is just something that develops over time yeah if you want to detect anomalies with machine learning you basically have two options you have a supervised or unsupervised machine learning the good thing is in supervised machine learning is you know okay these are the inputs and these are than the outputs the bad thing about supervised machine learning is you will need to have that annotated data so somebody will have to kind of have gone through all of the data and say like okay with this input like this is an anomaly now and this is not an anomaly and oftentimes you don't have that data so very commonly you don't have supervise but instead unsupervised machine learning where you say okay this is the input this is the output and we assume this isn't normally because this is kind of different of what we had before um yeah examples out of the ITER you get suddenly a spike of five hundreds because somebody made a bad deployment and you know okay something is wrong more 500 something changed in our system we need to fix it again and you could have something like security events for example you have some unusual DNS activity where you could do DNS exfiltration for example if an attacker wants to get out information out of your network in a very sneaky way they could just call subdomains under there control with that specific information they want to get out in as a subdomain and most systems don't detect that but if you kind of like look at the DNS traffic you can spot that even though it can be pretty well hidden or you could have some business analytics where you know okay Jenny we have these log entries or log events suddenly we have something totally different and then your machine learning will probably or hopefully picked it up and say okay here we have something that doesn't follow the regular patterns or events that we had before so something must be off here you could totally do visual inspections and we always say they are not enough interns in the world to monitor all the dashboards that you could build with sorry for the interns but the Sproles are not what you want to spend your summer on especially if you have like some complex or fastmoving data yes you can stare at the graphs the entire time and after one or two weeks you probably know okay these are the patterns that I'm expecting but hey you can easily miss something and B that's probably not where you want to spend your time at what this time is kind of best used for so for example if you have this data here this would be unique IP addresses which have connected to my system worse the anomaly we don't really see one we'll get back to the data that is these are actually locks from our own website where we have counted or it's basically it's an engine X proxy and we're just using the engine X X s log to see what is going on on our site and then to find anomalies of what is going on and what is not really going well on our site so you could totally define some static rules then the problem is to define rules somebody needs to know the system well enough to define the rules you could have false positives and negatives depending on how kind of like narrow the band of alerts would be and as soon as your system starts changing over time you will need to adjust it so if you have more visitors to your site you probably will need to change your alerts on a kind of a frequent basis or adjusting to whatever your system looks like today so if you want to set a threshold here like what is the number of unique Pietrus is connecting to your system which values would you pick like yes you could say maybe here at the bottom to below 2,000 might be a problem you could say that and then you could say oh maybe more than 14,000 would also be a good threshold but there's a huge gap in between and you probably don't really see like is there anything going on in my system that is okay or not okay where's the anomaly like yes you can't set the thresholds but there will be super coarsegrained yeah so we need some kind of machine learning let's see what we can do there and yeah normally machine learning is pretty CPU intensive or if it can run on a GPU you can also run it on a GPU since all my demos are on my laptop it's like running slack or zoom or whatever which will also eat up all my CPU so you can totally build it yourself on some very common frameworks for that are tensorflow is especially well known from Google Kara's psych it and loads of others are where you can just build your own machine learning algorithms or you have some data and basically you want to extract some value out of that and find your own am√©lie's so these are just very widely used tools you can use how do you build that pipeline kind of like the machine learning component that is the fun part and people who are data scientists will always say like yeah we're doing data science the entire day but what they basically do is they probably spent five or ten percent of their time on data science and the rest is like 70% is probably of the rest of the time is like getting the data and cleaning up the data and 30% is complaining about not having having clean data yeah because it's kind of a very common thing you need to find the right data then you need to prepare it in the right way so you can load it and then you need to clean it up you need a proper data storage system where you can keep all of the data that you want to keep around and then you probably want to have some optimization algorithm to make stuff scale and make it faster yeah and who have takes the expectation that your data will be clean and will work the right way it's always kind of cute so yeah finally data scientists ask them and they wait we'll probably make an very unhappy face about data cleaning but it happens and then people always ask like which one is the fastest machine learning algorithm and for all these performance things this is my favorite comic well you have yeah and the similar conditions you're testing to systems we're testing the squid and the house cat and we want to know which one of them is more intelligent or faster or more performant or whatever and you just need to find the right scenario because then your system will always win and this is really the bond number one thing whenever some company benchmarks their tool against some competitors they will probably find one use case that is very good for them and very bad for their competitors that's why I would not trust any vendor based benchmarks I would not normally call them bench marketing and not proper benchmarks because well yeah people do something like this so if you ask me like which is the best machine learning algorithm um this is the answer so what you generally want to do is when you want to find some anomalies you want to find out like what is normal like what is the baseline of stuff that happens here for example you could see we have the black line is some I don't know I think it's that the data transfer the visitors on your website or something like that and you can see there is this pattern where probably the first two a week days then the next two are probably Saturday Sunday and then you have five more days which are probably a weekday again and then you try to get that baseline of kind of what is normal in your system so you normally calculate within some confidence interval you calculate what is the upper into lower bound so the upper bound is here at the lower bound is blue and you're just trying to find the system that is getting closer and closer to the actual kind of X curve or the actual value that you have to get kind of like this dynamic baseline of how close are you getting to what you're expecting and yeah most of the data is normally distributed sometimes you're unlucky and it's kind of like paranormally distributed and that will totally throw off any machine learning so if you don't have like a good regular distribution of your data and no machine learning for that will save you yeah so this would be another example so you can see here we had the first three directions we didn't really know what the system was up to so and here the upper and lower bound is just like this light blue background and that the darker blue lines kind of like this is the actual gate that is coming in but then after about three durations this system here has learned like what is the baseline and what is kind of expected of the values and then you can see the band that the light blue band around the actual data is getting narrower and narrower so it's learning what is normal in that system and at some point you have these colored dots the more it's in the red the more of an anomaly it is so you can for example see pretty much at the right hand side there is this where it's falling down there is kind of like this is an anomaly which would probably be pretty hard to spot otherwise in in that system here and the one thing that you need to keep in mind is that whatever your baseline is your baseline will probably evolve unless you have a system that is super stable which is not that common is that depending on how much more visitors you have or whatever you change in your system the baseline will change and you should have a system that keeps learning the new stuff and ages out the other information so that is not stuck on any other information so here yeah I think it's even the same data you can see three iterations and then it kind of knows what what is happening and then you can see we have since I shouldn't move out of the way I'm not trying to head over all the way so here you can see here we have this anomaly and you can see here the system has also learned the anomaly so it's kind of like expecting there might be something going on here so it doesn't really since it's not a supervised machine learning it's just learning what kind of is happening in the system and it's then continuing that trend and kind of feeding that normally into the model for the future and will keep up to date with whatever happen in your system yeah that's another example where you can see stuff is happening and at some point there you have an anomaly and then it keeps learning like it's aging normally again because here you have the anomaly here it's expecting Myanmar normally since it doesn't happen here the anomaly is getting smaller smaller and smaller so the anomalies are kind of being aged out over time is getting again as well yeah you can have a single time series for example unusual traffic where you can see okay this is a perfect pattern I'm expecting this is what might happen you could also break that up into multiple time series so you could have not one metric but multiple metrics you would be interested in and then each of those would be modeled as an independent baseline so you have a machine learning model for each one of them so for example instead of having two global traffic you could say I want to have traffic by country and then you can see you can break it down into different country countries and then you can see sometimes you just don't have traffic for example here in France probably they had either a strike or a national holiday you never know with the French but but something happened and nobody was online anymore there and so it can make sense to break it up into different intervals to see how that they tourism morning okay so as I said I'm just using for my example I'm using from our own website some some visitors so it's just I think one month so or no it's two weeks of data or something like that it's pretty much the energy engine xaxis lock that you're expecting so you can see you have some bytes that you have transferred with you the geoip look up so we know okay this visitor here probably came from India this is the approximate longitude and latitude where they have come from and then you get like the other information that you want so you see then was the remote IP address they did a get and got a 200 back though we couldn't extract any proper browser information from that okay and since I always try to do at least something like this what this move might look like so for example here you can see I have just taken January to okay one and a half month march website visitors which were like around 15 million website hits and this is the general distribution so you can already from this data you get a rough idea what is going on in the system so you can see here this was probably weekend these were weekdays weekend again so you get kind of the general pattern then you could see this is just some some default dashboard that we have you can see where are people coming from like where from where is your website being accessed so you can see for example Germany and France are doing pretty well if we zoom in a bit more here we could probably see Austria as well but since I'm not online my map data would need to load online so here somewhere would be Austria and then you can see ok these are the return codes of my data like this is pretty much all the data you can extract from an engine xaxis log so this would be just the return codes you have you could extract like these where the top URLs you hit you see since you have to buy it sent you see how much traffic your website was sending and then you could also do like a breakdown of okay this was the these were the operating systems that were accessing our website and these were I don't know the browsers and their specific versions so that is what you get in the engine xaxis lock there is no machine learning necessary for that that's just block parsing so no no major magic here then you could write your own visualization so for example this was the number of unique remote IP addresses I've shown you before and this was the the bytes that we were transferring from our website which they kind of correlate slightly but not that wrongly for especially here kind of there is no strong correlation to that one and does anybody see the anomaly in the top chart here or should I let you stare at that dashboard for a week and maybe then you can see it I think if I'm not mistaken the anomaly is this one here so this like this downward thing though it's very hard to see we can actually leverage the machine learning stuff now our to see what is going on here so I've quickly built two visualizations for that the first one is about having a single metric that is the same information that I've just shown you that is the unique IP address that we had in our system and you can see our system needs approximately three directions to learn so you can see here these are the first three days this was Wednesday Thursday Friday and then the algorithm thought well I know what to expect the problem was then we have Saturday Sunday and the traffic pattern is totally different so you're kind of throwing off the machine learning algorithm here a bit because that is here you're having your five days of the week days again and then you have your weekend again and you can see the pattern continues that you need about three durations to learn a pattern because we can one weekend to week and three about here it got tweaked days pretty well already and here it also got the week end then and from then onwards it knows okay this is kind of the weekday pattern that I'm expecting this is the weekend is the weekday pattern so system kind of learned where how stuff works and this is here where we have an anomaly so let's quickly see normally and you can see here something was happening like generally we would expect traffic in this area here but somehow it was falling down and each of these points is actually an anomaly and it actually has an anomaly score between zero and hundred this is how we modeled that but you could totally build something similar the closer it gets to 100 the worse the anomaly is and then you could even have like the list which says like Oh normally I'm expecting that value here 1,400 something I got only 86 so this is 17 times lower than our than I was expecting and this was on the 27th of February last year anybody remembers what happened then no okay let's jump back to my slides I think we've seen most of those already and that was when there was the first ever bigger as3 outage from aw yes and half of the internet basically relied on that so yeah three went down and that affected a lot of stuff including Amazon stuff as well because there were also themselves relying on s3 to be available like everybody else so unfortunately we had the dependents in our website which basically killed our website and our downloads and lot of other stuff we then thought about should we make some multi cloud strategy or whatever but in the end nobody complained because half of the internet was down anyway and if the internet or if your own site is down and everybody everything is on fire nobody complains that your downloads are not working so we kind of gave up on making stuff unnecessarily complex because there was just no point of that but we you could very nicely see that normally there yeah we are nearly out of time okay then I then I will stick to slides then you can build the same thing with multiple models here I have broken my data down into the response codes that my website is sending out and then you can find further anomalies so for example here I have an over the normal is code that is like the top one is kind of like this is anomalies over all the red things are where the anomalies are and then you can break the entire thing down into the response code so you can see for the 404s we have a band of trouble over there this was for example our on our block and we suddenly had a spike of 404 this is when we published a bad link on our own block and suddenly the 404 started to spike and this was when we had in our CMS since we're using a CMS and CMS are mostly crap when we kind of broke some links internally unintentionally with the CMS change you could totally see that here that we suddenly had this pipe of anomalies where yeah suddenly stuff is not going well you can have combined multiple models and see what they're doing so here I'm combining this was the remote IP addresses the example which I've shown before where you can see this was when our website went down because of Amazon s3 so this is here then was the a3 outage because nobody could access our data anymore and these are the 404 and these two totally look correlated right so something is happening here and then suddenly the 404 spiked unfortunately these two events were totally unrelated because this was at the AWS outage and this was our own change on the CMS and that will screw off or throw off any machine running because it tries to learn and correlate some data but if there is no real correlation it will just be kind of like try to infer one which is not there so yeah correlation doesn't mean it's actually a causation everybody knows that but always keep that in mind and yeah it's kcd and extract the best once you take a statistics class does it help to make the distinction maybe yeah and then they're always like what is the cause and what is the reason or like what is the cause and what is the effect um for example for cancer waste it's very interesting study that in the US and the cancer rate has spiked and cell phone usage spiked afterwards so here the theory could be that cancer is causing cell phones which is also not the right thing but if you just look at the data and misinterpreted that is what you might get um yeah any correlated features would mess up your system there's unfortunately no way around that yeah finally thing here what you can also do is future predictions so the yellow stuff here these are future predictions and you can see the further it goes into the future the less certain they are because the more coarsegrained the entire system gets but this is very helpful if you like I'm expecting this number of visitors or this is the disk space I'm expecting in the future or the resource usage and this can predict for the future like this is what you will probably have a new system given that there are no other anomalies yeah then you could start doing like categorized users but let's keep that for today so to wrap up we've seen machine learning domain where I picked like mainly ops data and then some nginx data set where I've played around with if you want to know more about machine learning one a very nice paper I've seen is this one here best practices for machine learning engineering which has some very nice rules so for example the rule number one is don't be afraid to launch a product without machine learning because you might not need it and then yeah if you can interpret the model properly and debugging will be much easier if it's not just a black box spits out random data and plant launch and iterate on stuff but it has 43 rules all about machine learning which probably makes sense if you build anything in that space and yeah and don't forget maybe at some point machine learning is not the hot new thing anymore but something else comes around so yeah Silicon Valley is probably heading somewhere else already again anyway that's it I think we're pretty much out of time if you have questions come to me find me afterwards talk to me if you want to have stickers I have loads of stickers here so if anybody wants stickers yeah very nice take them with that I think we're done thanks very much wait I'm always taking a picture because my colleagues don't know where I am and this is my way to prove that I've been working today smile everybody yeah you can always wave thank you enjoy the break
