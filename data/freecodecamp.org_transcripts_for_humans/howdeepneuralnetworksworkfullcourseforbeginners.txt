With timestamps:

00:02 - neural networks are good for learning
00:03 - lots of different types of patterns
00:07 - to give an example of how this would
00:08 - work i imagine you had a four pixel
00:10 - camera
00:11 - so not not four megapixels but just four
00:13 - pixels and it was only black and white
00:17 - and you wanted to go around and take
00:18 - pictures of things and
00:20 - determine automatically then whether
00:23 - these pictures were of a solid all-white
00:27 - or all-dark image
00:29 - a vertical line
00:30 - or a diagonal line
00:32 - or a horizontal line
00:35 - this is tricky because you can't do this
00:37 - with simple rules about the brightness
00:40 - of the pixels
00:41 - both of these are horizontal lines but
00:44 - if you tried to make a rule about which
00:46 - pixel was bright and which was dark you
00:49 - wouldn't be able to do it
00:53 - so to do this with the neural network
00:55 - you start by taking all of your inputs
00:57 - in this case are four pixels and you
00:59 - break them out into input neurons
01:02 - and you assign a number to each of these
01:04 - depending on the brightness or darkness
01:07 - of the pixel
01:08 - plus one is all the way white minus one
01:11 - is all the way black and then gray is
01:13 - zero right in the middle
01:15 - [Music]
01:17 - so these values once you have them
01:19 - broken out and listed like this on the
01:21 - input neurons it's also called the input
01:23 - vector or array it's just a list of
01:26 - numbers
01:27 - that represents your inputs right now
01:33 - it's a useful notion to think about the
01:36 - receptive field of a neuron all this
01:40 - means is what set of inputs
01:44 - makes the value of this neuron as high
01:46 - as it can possibly be
01:48 - for input neurons this is pretty easy
01:51 - each one is associated with just one
01:53 - pixel
01:54 - and when
01:55 - that pixel is all the way white
01:58 - the value of that input neuron is as
02:00 - high as it can go
02:03 - the black and white checkered areas show
02:04 - pixels that an input neuron doesn't care
02:07 - about
02:08 - if they're all the way white or all the
02:10 - way black it still doesn't affect the
02:11 - value of that input neuron at all
02:17 - now to build a neural network
02:19 - we create
02:21 - a neuron
02:22 - the first thing this does is it adds up
02:25 - all of the values
02:26 - of the input neurons
02:29 - so in this case if we add up all of
02:31 - those values we get a 0.5
02:35 - now to complicate things just a little
02:37 - bit
02:38 - each of the connections
02:40 - are
02:40 - weighted meaning they're multiplied by a
02:43 - number that number can be one
02:46 - or minus one
02:47 - or anything in between so for instance
02:50 - if something has a weight of minus one
02:53 - it's multiplied and you get the negative
02:55 - of it and that's added in if something
02:57 - has a weight of zero then it's
02:59 - effectively ignored
03:02 - so here's what those weighted
03:03 - connections might look like
03:05 - and you'll notice that after the values
03:07 - of the input neurons are weighted
03:10 - and added the values can the final value
03:13 - is completely different
03:18 - graphically it's convenient to represent
03:20 - these weights as
03:21 - white links being positive weights black
03:25 - links being negative weights
03:27 - and the thickness of the line is roughly
03:29 - proportional to the magnitude of the
03:31 - weight
03:36 - then after you add the weighted input
03:38 - neurons
03:40 - they get squashed
03:42 - and i'll show you what that means
03:44 - you have a sigmoid squashing function
03:47 - sigmoid just means s-shaped
03:50 - and what this does is you put a value in
03:54 - let's say 0.5
03:56 - and you run a vertical line up to your
03:59 - sigmoid and then a horizontal horizontal
04:01 - line over from where it crosses
04:03 - and then where that hits the y axis
04:06 - that's the output of your function so in
04:08 - this case slightly less than 0.5 it's
04:10 - pretty close
04:13 - as your input number gets larger
04:16 - your output number also gets larger but
04:18 - more slowly and eventually
04:21 - no matter how big the number you put in
04:23 - the answer is always
04:26 - less than one
04:27 - similarly when you go negative the
04:29 - answer is always greater than negative
04:31 - one
04:32 - so this ensures that that neuron's value
04:36 - never gets outside of the range of plus
04:38 - one to minus one
04:40 - which is helpful for keeping the
04:43 - computations in the neural network
04:46 - bounded and stable
04:50 - so after you sum the weighted values of
04:53 - the neurons and squash the result you
04:55 - get the output in this case 0.746
04:59 - that is a neuron
05:02 - so we can call this we can collapse all
05:03 - that down and this is a neuron that does
05:06 - a weighted sum and squash the result
05:09 - and now instead of just one of those
05:12 - assume you have a whole bunch there are
05:14 - four shown here but
05:17 - there could be 400 or 4 million
05:20 - now to keep our picture clear
05:22 - we'll assume for now that the weights
05:24 - are either plus one
05:26 - white lines
05:27 - minus one black lines or zero in which
05:30 - case they're missing entirely
05:33 - but
05:33 - in actuality all of these neurons that
05:36 - we created are each
05:38 - attached to all of the input neurons
05:42 - and they all have some weight between
05:44 - minus one and plus one
05:49 - when we create this first layer of our
05:51 - neural network
05:53 - the receptive fields get more complex
05:55 - for instance here each of those end up
05:58 - combining two of our input neurons
06:01 - and so the value the receptive field
06:05 - the pixel values that make
06:07 - that first layer neuron
06:10 - as large as it can possibly be
06:12 - look now like pairs of pixels
06:16 - either
06:17 - all white or a mixture of white and
06:19 - black
06:20 - depending on
06:21 - the weights
06:24 - so for instance
06:26 - this neuron here
06:28 - is attached to this input pixel
06:32 - which is upper left and this input pixel
06:34 - which is lower left and both of those
06:37 - weights are positive
06:39 - so it combines the two of those and
06:41 - that's its receptive field the receptive
06:43 - field of this one plus the receptive
06:45 - field of this one
06:47 - however if we look at this neuron
06:49 - it combines
06:52 - our
06:53 - this pixel
06:54 - upper right
06:56 - and this pixel lower right
06:58 - it has a weight of minus one for the
07:00 - lower right pixel so that means it's
07:03 - most active
07:05 - when this pixel is black
07:07 - so here is its receptive field
07:12 - now
07:13 - the because we were careful of how we
07:16 - created that first layer
07:18 - its values look a lot like input values
07:22 - and we can turn right around and create
07:24 - another layer on top of it the exact
07:26 - same way with the output of one layer
07:29 - being the input to the next layer
07:32 - and we can repeat this
07:34 - three times or seven times or 700 times
07:37 - for additional layers
07:41 - each time the receptive fields get even
07:43 - more complex
07:45 - so you can see here using the same logic
07:47 - now they cover
07:49 - all of the pixels
07:51 - and more uh more special arrangement of
07:54 - which are black and which are white
07:59 - we can create another layer
08:03 - again all of these neurons in one layer
08:05 - are connected to all of the neurons in
08:07 - the previous layer but we're assuming
08:09 - here that most of those weights are zero
08:11 - and not shown
08:13 - it's not generally the case
08:16 - so just to mix things up we'll create a
08:18 - new layer but if you notice our
08:20 - squashing function isn't there anymore
08:22 - we have something new called a rectified
08:25 - linear unit
08:26 - this is another
08:28 - popular neuron type
08:31 - so you do your weighted sum of all your
08:33 - inputs and instead of squashing
08:36 - you do rectified linear units you
08:40 - rectify it so if it is negative
08:44 - you make the value zero if it's positive
08:46 - you keep the value
08:49 - this is obviously very easy to compute
08:52 - and it turns out to have very nice
08:55 - stability properties for neural networks
08:57 - as well in practice
09:01 - so after we do this because some of our
09:04 - weights are positive and some are
09:05 - negative connecting to those rectified
09:07 - linear units we get
09:10 - receptive fields and they're opposites
09:13 - if you look at the patterns there
09:16 - and then finally when we've created as
09:18 - many layers with as many neurons as we
09:20 - want we create an output layer
09:23 - here we have four outputs that we're
09:25 - interested in
09:26 - is the image solid
09:29 - vertical diagonal or horizontal
09:34 - so to walk through an example here of
09:37 - how this would work let's say we start
09:38 - with this input image
09:41 - shown on the left
09:43 - dark pixels on top white on the bottom
09:46 - as we propagate that to our input layer
09:50 - this is what those values would look
09:52 - like
09:53 - the top pixels the bottom pixels
09:56 - as we move that to our first layer
10:00 - we can see the combination of a dark
10:03 - pixel
10:04 - and a light pixel some together
10:07 - get us zero
10:08 - gray
10:11 - whereas down here we have the
10:13 - combination of a dark pixel
10:15 - plus a light pixel with a negative
10:17 - weight
10:18 - so that gets us a value of negative one
10:21 - there
10:22 - which makes sense because if we look at
10:23 - the receptive field here
10:26 - upper left pixel white
10:28 - lower left pixel black
10:30 - is the exact opposite of the input that
10:33 - we're getting
10:34 - and so we would expect its value to be
10:36 - as low as possible
10:38 - minus one
10:42 - as we move to the next layer we see the
10:44 - same types of things combining zeros to
10:47 - get zeros
10:49 - um combining a negative
10:52 - and a negative with a negative weight
10:54 - which makes a positive
10:56 - to get a zero
10:57 - and here we have combining two negatives
10:59 - to get a negative
11:01 - so again you'll notice the receptive
11:03 - field of this is exactly the inverse of
11:05 - our input so it makes sense that its
11:08 - weight would be negative
11:10 - or its value would be negative
11:14 - and we move to the next layer
11:16 - all of these of course these zeros
11:18 - propagate forward
11:21 - here
11:22 - this is a negative has a negative value
11:25 - and it gets has a positive weight so it
11:28 - just moves straight forward because we
11:29 - have a rectified linear unit
11:32 - negative values become zero
11:34 - so now it is zero again two but this one
11:37 - gets rectified and becomes positive
11:39 - negative times a negative is positive
11:42 - and so when we finally get to the output
11:44 - we can see they're all zero except for
11:46 - this horizontal which is positive and
11:48 - that's the answer our neural network
11:51 - said this is
11:53 - an image of a horizontal line
11:57 - now
11:58 - neural networks usually aren't that good
12:00 - not that clean
12:02 - so there's a notion of
12:03 - with an input what is truth
12:06 - in this case the truth is this has a
12:08 - zero for all of these values but a one
12:12 - for
12:12 - horizontal it's not solid it's not
12:14 - vertical it's not diagonal
12:16 - yes it is horizontal
12:19 - an arbitrary neural network will give
12:22 - answers that are not exactly truth it
12:24 - might be off by a little or a lot
12:27 - and then the error
12:29 - is the magnitude of the difference
12:32 - between the truth and the answer given
12:35 - and you can add all these up to get the
12:36 - total error for the neural network
12:40 - so the idea
12:42 - the whole idea with learning and
12:44 - training
12:45 - is to adjust the weights to make the
12:48 - error as low as possible
12:51 - so the way this is done is
12:54 - we put an image in we calculate the
12:56 - error at the end
12:58 - then we look for how to adjust those
13:00 - weights higher or lower to either make
13:03 - that error go up
13:04 - or down and we of course adjust the
13:06 - weights in the way that make the error
13:08 - go down
13:11 - now the problem with doing this is each
13:14 - time we go back and calculate the error
13:17 - we have to multiply all of those weights
13:20 - by all of the neuron values at each
13:23 - layer
13:24 - and we have to do that again and again
13:25 - once for each weight
13:28 - this takes forever
13:29 - in computing terms
13:32 - on computing scale and so it's not a
13:34 - practical way to train a big neural
13:36 - network
13:39 - you can imagine instead of just rolling
13:41 - down to the bottom of a simple valley we
13:43 - have a very high dimensional valley and
13:45 - we have to find our way down and because
13:48 - there are so many dimensions one for
13:50 - each of these weights that the
13:52 - computation just becomes prohibitively
13:54 - expensive
13:57 - luckily there was an insight
13:59 - that
14:00 - lets us do this in a very reasonable
14:02 - time
14:03 - and that's that
14:05 - if we're careful about how we design our
14:06 - neural network we can calculate the
14:09 - slope directly the gradient we can
14:12 - figure out the direction that we need to
14:14 - adjust the weight
14:15 - without going all the way back through
14:17 - our neural network and recalculating
14:22 - so just to review the slope that we're
14:25 - talking about is when we make a change
14:27 - in weight the error will change a little
14:30 - bit
14:31 - and that
14:32 - relation of the change in weight to the
14:34 - change in error
14:36 - is the slope
14:38 - mathematically there are several ways to
14:40 - write this
14:41 - we'll favor the one on the bottom it's
14:43 - technically most correct
14:45 - we'll call it d-e-d-w for shorthand
14:48 - every time you see it just think
14:52 - the change in error
14:53 - when i change a weight
14:56 - or the change in the thing on the top
14:57 - when i change the thing on the bottom
15:00 - um this is uh does get into a little bit
15:03 - of calculus we do take derivatives
15:06 - that's how we calculate slope if it's
15:08 - new to you
15:10 - i strongly recommend a good semester of
15:12 - calculus just because the concepts are
15:14 - so
15:15 - universal
15:17 - and uh a lot of them have very nice
15:19 - physical interpretations which i find
15:20 - very appealing
15:22 - but don't worry otherwise just gloss
15:25 - over this and pay attention to the rest
15:27 - and you'll get a general sense for how
15:28 - this works
15:31 - so in this case
15:32 - if we change the weight by plus one the
15:34 - error changes by minus two which gives
15:36 - us a slope of minus two
15:39 - that tells us the direction that we
15:42 - should adjust our weight and how much we
15:44 - should adjust it to bring the error down
15:49 - now to do this you have to know what
15:50 - your error function is
15:52 - so assume we had an error function that
15:54 - was the square of the weight
15:57 - and you can see that our weight
15:59 - is right at -1
16:02 - so the first thing we do is we take the
16:03 - derivative
16:05 - change in error divided by change in
16:07 - weight d e d w
16:09 - the derivative of weight squared is two
16:11 - times the weight
16:13 - and so we plug in our weight of minus
16:14 - one and we get a slope d e d w of minus
16:18 - two
16:22 - now the other trick that lets us do this
16:24 - with deep neural networks is chaining
16:27 - and to show you how this works imagine a
16:29 - very simple trivial neural network with
16:31 - just one hidden layer one input layer
16:34 - one output layer and one weight
16:37 - connecting each of them
16:39 - so it's obvious to see that the value y
16:42 - is just the value x times the weight
16:44 - connecting them w1
16:48 - so if we change w1 a little bit we just
16:50 - take the derivative of y with respect to
16:53 - w1
16:54 - and we get x the slope is x if i change
16:56 - w1 by a little bit then y will change by
17:00 - x times the size of that adjustment
17:04 - similarly for the next step we can see
17:06 - that e
17:08 - is just the value y
17:09 - times the weight w2
17:11 - and so when we calculate d e d y it's
17:14 - just w
17:16 - 2. because this network is so simple we
17:19 - can calculate from one end to the other
17:22 - x times w1 times w2
17:25 - is the error e
17:27 - and so if we want to calculate how much
17:30 - will the error change if i change w1 we
17:32 - just take the derivative of that with
17:34 - respect to w1
17:36 - and get x times w2
17:38 - so this illustrates you can see here now
17:41 - that what we just calculated is actually
17:44 - the product
17:45 - of
17:46 - our first derivative that we took the
17:49 - the dydw1
17:51 - times
17:52 - the derivative for the next step d e d y
17:55 - multiplied together
17:59 - this is chaining
18:01 - you can calculate the slope of each tiny
18:04 - step
18:05 - and then multiply all of those together
18:08 - to get the slope of the full chain the
18:11 - derivative of the full chain
18:13 - so in a deeper neural network what this
18:15 - would look like is if i want to know how
18:18 - much the error will change
18:20 - if i adjust a weight that's deep in the
18:22 - network i just calculate the derivative
18:26 - of each tiny little step all the way
18:29 - back to the weight that i'm trying to
18:30 - calculate
18:32 - and then multiply them all together
18:35 - this computationally is many many times
18:38 - cheaper
18:39 - than what we had to do before of
18:41 - recalculating the error for the whole
18:42 - neural network for every weight
18:47 - now in the neural network that we've
18:49 - created there are several
18:51 - types of back propagation we have to do
18:54 - there are several operations we have to
18:55 - do for each one of those
18:58 - we have to be able to calculate the
18:59 - slope
19:00 - so for the first one is just a weighted
19:02 - connection between two neurons a and b
19:07 - so let's assume we know the change in
19:10 - error with respect to b
19:11 - we want to know the change in error with
19:14 - respect to a
19:15 - to get there we need to know db da
19:20 - so to get that we just write the
19:22 - relationship between b and a
19:24 - take the derivative of b with respect to
19:26 - a we get the weight w
19:28 - and now we know how to make that step we
19:31 - know how to do that
19:32 - little nugget of back propagation
19:36 - another element that we've seen
19:38 - is sums
19:39 - all of our neurons sum up a lot of
19:41 - inputs
19:43 - to take this back propagation step
19:46 - we do the same thing we write our
19:48 - expression
19:50 - and then we take the derivative
19:52 - of our endpoint z with respect to our
19:56 - step that we're uh propagating to a
19:59 - and dz da in this case is just one
20:02 - which makes sense if we have a sum of a
20:05 - whole bunch of elements
20:07 - we increase one of those elements by one
20:09 - we expect the sum to increase by one
20:12 - that's
20:13 - the definition of a slope of one
20:16 - one to one relation there
20:20 - another element that we have that we
20:22 - need to be able to back propagate is the
20:24 - sigmoid function
20:26 - so this one's a little bit more
20:28 - interesting mathematically
20:30 - we'll just write it
20:31 - shorthand like this the sigma function
20:34 - um it is entirely feasible to uh go
20:37 - through and take the derivative of this
20:39 - analytically
20:40 - and calculate it
20:42 - it just so happens that this function
20:45 - has a nice property
20:47 - that to get its derivative you just
20:49 - multiply it by
20:51 - 1 minus itself
20:53 - so this is very straightforward to
20:56 - calculate
21:00 - another element that we've used is the
21:01 - rectified linear unit
21:03 - again to figure out how to back
21:05 - propagate this we just write out the
21:07 - relation
21:08 - b is equal to a if a is positive
21:11 - otherwise it's zero
21:13 - and
21:14 - piecewise for each of those we take the
21:16 - derivative so db da is either one if a
21:20 - is positive or zero
21:24 - and so with all of these
21:26 - little back propagation steps
21:29 - and the ability to chain them together
21:32 - we can calculate the effect of adjusting
21:35 - any given weight
21:37 - on the error
21:38 - for any given input
21:41 - and so to train
21:43 - then
21:44 - we start with a fully connected network
21:47 - we don't know what any of these weights
21:49 - should be
21:51 - and so we assign them all random values
21:53 - we create a completely
21:55 - arbitrary random neural network
21:58 - we put in an input that we know the
22:01 - answer to we know whether it's solid
22:03 - vertical diagonal or horizontal so we
22:06 - know what truth should be
22:07 - and so we can calculate the error
22:11 - then
22:12 - we run it through calculate the error
22:14 - and using back propagation go through
22:17 - and adjust all of those weights a tiny
22:20 - bit in the right direction
22:23 - and then we do that again with another
22:24 - input and again with another input for
22:28 - if we can get away with it
22:29 - many thousands or even millions of times
22:33 - and eventually all of those weights will
22:36 - gravitate they'll roll down that many
22:39 - dimensional valley to a nice low spot
22:42 - in the bottom
22:44 - where it performs really well and does
22:46 - pretty close to truth on most of the
22:49 - images
22:52 - if we're really lucky it'll look like
22:54 - what we started with with intuitively um
22:58 - understandable
23:00 - receptive fields for those neurons and a
23:03 - relatively sparse
23:05 - representation meaning that most of the
23:07 - weights are
23:08 - small or close to zero and it doesn't
23:11 - always turn out that way but
23:13 - what we are guaranteed is that it'll
23:16 - find a pretty good representation of you
23:19 - know the best that it can do adjusting
23:21 - those weights
23:22 - to get as close as possible to the right
23:24 - answer for all of the inputs
23:30 - so what we've covered is just a very
23:32 - basic introduction to the principles
23:33 - behind neural networks i haven't told
23:36 - you quite enough to be able to go out
23:38 - and build one of your own
23:39 - but if you're feeling motivated to do so
23:41 - i highly encourage it
23:44 - here are a few resources that you'll
23:46 - find useful
23:47 - you'll want to go and learn about bias
23:49 - neurons
23:50 - dropout is a useful training tool
23:53 - there are several resources available
23:56 - from andre carpathi who is an expert in
24:00 - neural networks and great at teaching
24:02 - about it also there's a fantastic
24:05 - article called the black magic of deep
24:07 - learning that just has a bunch of
24:09 - practical from the trenches tips
24:12 - on how to get them working well
24:16 - neural networks are famously difficult
24:17 - to interpret
24:19 - it's hard to know what they're actually
24:20 - learning when we train them
24:22 - so let's take a closer look and see
24:23 - whether we can get a good picture of
24:25 - what's going on inside
24:27 - just like every other supervised machine
24:30 - learning model
24:31 - neural networks learn relationships
24:33 - between input variables and output
24:35 - variables
24:37 - in fact we can even see how it's related
24:39 - to the most iconic model of all
24:41 - linear regression
24:44 - simple linear regression assumes a
24:46 - straight-line relationship between an
24:47 - input variable x
24:50 - and an output variable y
24:52 - x is multiplied by a constant
24:55 - m
24:55 - which also happens to be the slope of
24:57 - the line
24:59 - and it's added to another constant b
25:01 - which happens to be where the line
25:03 - crosses the y axis
25:06 - we can represent this in a picture
25:08 - our input value x
25:10 - is multiplied by m our constant
25:15 - b is multiplied by one
25:18 - and then they get added together to get
25:19 - y
25:21 - this is a graphical representation of y
25:24 - equals m x
25:26 - plus b
25:30 - on the far left the circular symbols
25:32 - just indicate that the value is passed
25:34 - through
25:35 - the rectangles l labeled m and b
25:39 - indicate that whatever goes in on the
25:40 - left comes out multiplied by m or b on
25:44 - the right
25:46 - and the box with the capital sigma
25:48 - indicates that whatever goes in on the
25:50 - left gets added together and spit out on
25:53 - the right
25:55 - we can change the names of all the
25:57 - symbols for a different representation
25:59 - this is still a straight line
26:01 - relationship we've just changed the
26:02 - names of all the variables
26:05 - the reason we're doing this is to
26:07 - translate our linear regression into the
26:10 - notation we'll use in neural networks
26:13 - this will help us keep track of things
26:15 - as we move forward
26:18 - at this point we have turned a straight
26:20 - line equation into a network
26:23 - a network is anything that has nodes
26:25 - connected by edges
26:28 - in this case
26:29 - x sub 0 and x sub 1
26:33 - are our input nodes
26:35 - v sub 0 is an output node
26:39 - and our weights connecting them are
26:41 - edges
26:43 - this is not the traditional sense of a
26:44 - graph meaning a plot or a grid like in a
26:48 - graphing calculator or graph paper
26:51 - it's just the formal word for a network
26:54 - for nodes connected by edges
26:57 - another piece of terminology you might
26:58 - hear is a directed acyclic graph
27:02 - abbreviated as d-a-g or dag a directed
27:06 - graph is one where the edges just go in
27:08 - one direction
27:09 - in our case
27:11 - input goes to output but output never
27:14 - goes back to input
27:16 - our edges are directed
27:19 - acyclic
27:20 - means that you can't ever draw a loop
27:23 - once you have visited a node there's no
27:25 - way to jump from edges to nodes to edges
27:28 - to nodes to get back to where you
27:29 - started
27:30 - everything flows in one direction
27:32 - through the graph
27:35 - we can get a sense of the type of models
27:36 - that this network is capable of learning
27:39 - by choosing random values for the
27:40 - weights
27:42 - w sub 0 0 and w sub 1
27:47 - and then seeing what relationship pops
27:49 - out between
27:51 - x sub 1
27:52 - and v sub 0.
27:54 - remember that we set x of 0 equal to 1
27:57 - and are holding it there always
27:59 - this is a special node called a bias
28:02 - node
28:04 - it should come as no surprise that the
28:06 - relationships that come out of this
28:08 - linear model are all straight lines
28:11 - after all we've taken our equation for
28:13 - the line and rearranged it but we
28:16 - haven't changed it in any substantial
28:18 - way
28:21 - there's no reason we have to limit
28:22 - ourselves to just one input variable
28:25 - we can add an additional one
28:27 - now here we have an x of 0
28:30 - an x sub 1 and an x sub 2.
28:34 - we draw an edge between x sub 2 and our
28:37 - summation
28:38 - with the weight
28:40 - w
28:41 - sub 2 0
28:43 - x sub 2
28:45 - times w sub 2 0
28:48 - is again
28:49 - u sub 2 0
28:52 - and all of our u's get added together to
28:55 - make a v sub 0.
28:58 - and we could add more inputs as many as
29:00 - we want
29:02 - this is still a linear equation but
29:04 - instead of being two dimensional we can
29:06 - make it three dimensional or higher
29:09 - writing this out mathematically could
29:11 - get very tedious so we'll use a shortcut
29:14 - we'll substitute the subscript i
29:17 - for the index of the input it's the
29:19 - number of the input we're talking about
29:23 - this allows us to write u sub i zero
29:27 - where our
29:28 - u sub i
29:30 - equals x sub i
29:32 - times w sub i zero
29:36 - and again our output v sub zero is just
29:39 - the summation over all values of i
29:43 - of u sub i zero
29:47 - for this three dimensional case we can
29:48 - again look at the models that emerge
29:50 - when we randomly choose our w sub i
29:54 - zeros our weights
29:57 - as we would expect
29:59 - we still get the three dimensional
30:01 - equivalent of a line a plane in this
30:03 - case
30:05 - and if we were to extend this to more
30:06 - inputs we would get the m dimensional
30:08 - equivalent of a line which is called an
30:10 - m-dimensional hyperplane
30:13 - so far so good
30:15 - now we can start to get fancier
30:17 - our input
30:19 - x sub 1 looks a lot like our output v
30:22 - sub zero
30:24 - in fact there's nothing to prevent us
30:26 - from taking our output
30:28 - and then using it as an input to another
30:31 - network just like this one
30:34 - now we have two separate identical
30:37 - layers
30:38 - we can add a subscript roman numeral i
30:42 - and a subscript roman numeral i i or two
30:46 - to our equations depending on which
30:49 - layer we're referring to
30:51 - and we just have to remember that our x
30:53 - sub 1
30:54 - in layer 2
30:56 - is the same as our v sub 0 in layer 1.
31:01 - because these equations are identical
31:03 - and each our layer each of our layers
31:05 - work just the same we can reduce this to
31:07 - one set of equations adding a subscript
31:10 - capital l to represent which layer we're
31:12 - talking about
31:15 - as we continue here we'll be assuming
31:17 - that all the layers are identical and to
31:19 - keep the equations cleaner we'll leave
31:21 - out the capital l
31:23 - but just keep in mind that if we were
31:25 - going to be completely correct and
31:26 - verbose we would add the l
31:29 - subscript onto the end of everything to
31:32 - specify the layer it belongs to
31:36 - now that we have two layers
31:38 - there's no reason that we can't connect
31:40 - them in more than one place
31:43 - instead of our first layer generating
31:45 - just one output we can make several
31:47 - outputs
31:49 - in our diagram we'll add a second output
31:52 - v sub 1
31:53 - and we'll connect this
31:55 - to a third input into our second layer
31:58 - x sub 2.
32:01 - keep in mind that the x sub 0 input to
32:04 - every layer will always be equal to 1.
32:08 - that bias node shows up again in every
32:10 - layer
32:12 - now there are two nodes
32:14 - shared by both layers
32:16 - we can modify our equations accordingly
32:19 - to specify which of the shared nodes we
32:22 - are talking about
32:24 - they behave exactly the same so we can
32:26 - be efficient and reuse our equation
32:29 - but we can specify subscript j
32:33 - to indicate which output we're talking
32:35 - about
32:37 - so now if i'm connecting the ith input
32:40 - to the jth output
32:43 - then
32:44 - i and j will determine which weight
32:48 - is applied
32:49 - and which u's get added together to
32:53 - create the output v sub j
32:57 - and we can do this as many times as we
32:58 - want
33:00 - we can add as many of these shared nodes
33:02 - as we care to
33:03 - the model as a whole only knows about
33:06 - the input x sub 1 into the first layer
33:10 - and the output
33:11 - v sub 0 of the last layer
33:15 - from the point of view of someone
33:16 - sitting outside the model the shared
33:18 - nodes between layer 1 and layer 2 are
33:20 - hidden
33:21 - they are inside the black box
33:24 - because of this they're called hidden
33:26 - nodes
33:28 - we can take this two layer linear
33:30 - network
33:31 - create a hundred hidden nodes
33:34 - set all of the weights randomly
33:36 - and see what model it produces
33:40 - even after adding all of this structure
33:43 - the resulting models are still straight
33:45 - lines
33:46 - in fact it doesn't matter how many
33:48 - layers you have or how many hidden nodes
33:49 - each layer has
33:51 - any combination of these linear elements
33:54 - with weights and sums will always
33:57 - produce a straight line result
33:59 - this is actually one of the traits of
34:01 - linear computation that makes it so easy
34:03 - to work with
34:04 - but unfortunately for us it also makes
34:07 - really boring models
34:09 - sometimes a straight line is good enough
34:11 - but that's not why we go to neural
34:13 - networks we're going to want something a
34:15 - little more sophisticated
34:18 - in order to get more flexible models
34:20 - we're going to need to add some
34:22 - non-linearity
34:24 - we'll modify our linear equation here
34:27 - after we calculate our output v sub 0
34:31 - we subject it to another function
34:34 - f
34:35 - which is not linear
34:37 - and we'll call the result
34:38 - y sub zero
34:41 - one really common non-linear function to
34:44 - add here is the logistic function it's
34:48 - shaped like an s so sometimes it's
34:50 - called a sigmoid function too
34:53 - although that can be confusing because
34:55 - technically any function shaped like an
34:57 - s is a sigmoid
35:00 - we can get a sense of what logistic
35:02 - functions look like by choosing random
35:04 - weights for this one input
35:06 - one output one layer network
35:09 - and meeting the family
35:13 - one notable characteristic of logistic
35:15 - functions is that they live between zero
35:17 - and one
35:18 - for this reason they're also called
35:19 - squashing functions
35:22 - you can imagine taking a straight line
35:24 - and then squashing the edges and bending
35:27 - and hammering it down so that the whole
35:29 - thing fits between zero and one no
35:30 - matter how far out you go
35:34 - working with logistic functions brings
35:36 - us to another connection with machine
35:37 - learning models logistic regression
35:41 - this is a bit confusing
35:43 - because regression refers to finding a
35:45 - relationship between an input and an
35:48 - output
35:49 - usually in the form of a line or a curve
35:51 - or a surface of some type
35:55 - logistic regression is actually used as
35:57 - a classifier most of the time
36:00 - it finds a relationship between a
36:01 - continuous input variable and a
36:04 - categorical output variable
36:07 - it treats observations of one category
36:09 - as zeros
36:11 - treats observations of the other
36:12 - category as ones
36:14 - and then finds the logistic function
36:16 - that best fits all those observations
36:20 - then to interpret the model we add a
36:22 - threshold often around 0.5 and wherever
36:25 - the curve crosses the threshold there's
36:28 - a demarcation line
36:30 - everything to the left of that line is
36:32 - predicted to fall into one category and
36:34 - everything to the right of that line is
36:36 - predicted to fall into the other
36:39 - this is how a regression algorithm gets
36:41 - modified to become a classification
36:43 - algorithm
36:45 - as with linear functions there's no
36:48 - reason not to add more inputs
36:51 - we know that logistic regression can
36:52 - work with many input variables and we
36:54 - can represent that in our graph as well
36:58 - here we just add one
37:00 - in order to keep the plot three
37:01 - dimensional but we could add as many as
37:04 - we want
37:05 - to see what type of functions this
37:07 - network can create we can choose a bunch
37:09 - of random values for the weights
37:12 - as you might have expected the functions
37:14 - we create are still s-shaped
37:16 - but now they're three-dimensional
37:19 - they look like a tablecloth laid across
37:21 - two tables of unequal height
37:25 - more importantly if you look at the
37:27 - contour lines projected down onto the
37:30 - floor of the plot
37:32 - you can see that they are all perfectly
37:34 - straight the result of this is that any
37:38 - threshold we choose for doing
37:40 - classification
37:41 - will split our input space up into two
37:44 - halves
37:45 - with the divider being a straight line
37:48 - this is why logistic regression is is
37:51 - described as a linear classifier
37:55 - whatever the number of inputs you have
37:57 - whatever dimensional space you're
37:58 - working in
37:59 - logistic regression will always split it
38:01 - into two halves using a line or a plane
38:04 - or a hyperplane of the appropriate
38:06 - dimensions
38:09 - another popular non-linear function is
38:11 - the hyperbolic tangent
38:14 - it's closely related to the logistic
38:16 - function and can be written in a very
38:18 - symmetric way
38:20 - we can see when we choose some random
38:22 - weights and look at examples
38:24 - that hyperbolic tangent curves look just
38:27 - like logistic curves
38:29 - except that they vary between -1 and
38:31 - plus 1.
38:34 - just like we tried to do before with
38:35 - linear functions we can use the output
38:37 - of one layer as the input to another
38:40 - layer
38:41 - we can stack them in this way and can
38:43 - even add hidden nodes the same way we
38:45 - did before
38:47 - here we just show two hidden nodes in
38:49 - order to keep the diagram simple but you
38:51 - can imagine as many as you want there
38:56 - when we choose random weights for this
38:58 - network and look at the output we find
39:00 - that things get interesting
39:03 - we've left the realm of the linear
39:06 - because the hyperbolic tangent function
39:08 - is non-linear
39:10 - when we add them together we get
39:12 - something that doesn't necessarily look
39:14 - like a hyperbolic tangent
39:17 - we get curves
39:18 - wiggles
39:19 - peaks and valleys and a much wider
39:22 - variety of behavior than we ever saw
39:24 - with single layer networks
39:28 - we can take the next step and add
39:30 - another layer to our network
39:32 - now
39:33 - we have a set of hidden nodes between
39:36 - layer 1 and layer 2 and another set of
39:38 - hidden nodes between layer 2 and layer
39:41 - 3.
39:43 - again we choose random values for all
39:46 - the weights and look at the types of
39:48 - curves it can produce
39:50 - again we see wiggles and peaks
39:54 - valleys and a wide selection of shapes
39:58 - if it's hard to tell the difference
39:59 - between these curves and the curves
40:01 - generated by a two-layer network
40:03 - that's because they're mathematically
40:05 - identical
40:07 - we won't try to prove it here but
40:08 - there's a cool result
40:10 - that shows that any curve you can create
40:13 - learning a menu using a many layered
40:14 - network you can also create using a two
40:17 - layer network as long as you have enough
40:20 - hidden nodes
40:23 - the advantage of having a many layered
40:24 - network is that it can help you create
40:26 - more complex curves
40:28 - using fewer total nodes
40:30 - for instance in our two layer network we
40:32 - used a hundred hidden nodes
40:35 - in our three layer network we used 11
40:37 - hidden nodes in the first layer and nine
40:40 - hidden nodes in the second layer
40:42 - that's only a fifth of the total number
40:44 - we used in our two layer network but the
40:46 - curves it produces show similar richness
40:51 - we can use these fancy wiggly lines to
40:53 - make a classifier as we did with
40:55 - logistic regression
40:58 - here we use the zero line as the cutoff
41:01 - everywhere that our curve crosses the
41:03 - zero line there's a divider
41:06 - in every region that the curve sits
41:08 - above the zero line we'll call this
41:10 - category a
41:12 - and similarly everywhere the curve is
41:13 - below the zero line we have category b
41:18 - what distinguishes these nonlinear
41:20 - classifiers from linear ones is that
41:22 - they don't just split the space into two
41:25 - halves
41:27 - in this example regions of a and b are
41:29 - interleaved
41:31 - building a classifier around a
41:33 - multi-layer non-linear network gives it
41:35 - a lot more flexibility
41:37 - it can learn more complex relations
41:40 - this particular combination of
41:42 - multi-layer network
41:43 - with hyperbolic tangent non-linear
41:45 - function has its own name a multi-layer
41:49 - perceptron
41:51 - as you can guess when you have only one
41:53 - layer it's just called a perceptron and
41:55 - in that case you don't even need to add
41:57 - the nonlinear function to make it work
41:59 - the function will still cross the x-axis
42:02 - at all the same places
42:05 - here is the full network diagram of a
42:08 - multi-layer perceptron
42:11 - this representation is helpful because
42:13 - it makes every single operation explicit
42:16 - however it's also visually cluttered
42:18 - it's difficult to work with
42:20 - because of this it's most often
42:22 - simplified to look like circles
42:25 - connected by lines
42:27 - this implies all the operations we saw
42:30 - on the previous diagram
42:32 - connecting lines each have a weight
42:33 - associated with them
42:35 - hidden nodes and output nodes perform
42:37 - summation and nonlinear squashing
42:39 - but in this diagram all of that is
42:41 - implied
42:44 - in fact our bias nodes
42:46 - the nodes that always have a value of
42:47 - one in each layer
42:49 - are omitted for clarity
42:52 - so our original network reduces to this
42:56 - the bias nodes are still present and
42:57 - their operation hasn't changed at all
42:59 - but we leave them out to make a cleaner
43:01 - picture
43:04 - we only show two hidden nodes from each
43:06 - layer here but in practice we used quite
43:08 - a few more
43:10 - again to make the diagram as clean as
43:12 - possible we often don't show all the
43:14 - hidden nodes we just show a few and the
43:17 - rest are implied
43:20 - here's a generic diagram then
43:22 - for a three layer
43:24 - single input single output network
43:28 - notice that if we specify the number of
43:30 - inputs the number of outputs and the
43:32 - number of layers
43:33 - and the number of hidden nodes in each
43:35 - layer
43:36 - then we can fully define a neural
43:38 - network
43:41 - we can also take a look at a two input
43:44 - single output neural network
43:47 - because it has two inputs
43:49 - when we plot its outputs it will be a
43:51 - three dimensional curve
43:54 - we can once again choose random weights
43:56 - and generate curves to see what types of
43:58 - functions this neural network might be
43:59 - able to represent
44:02 - this is where it gets really fun
44:04 - with multiple inputs multiple layers
44:07 - and non-linear activation functions
44:10 - neural networks can make really crazy
44:11 - shapes
44:13 - it's almost correct to say that they
44:15 - could make any shape you want
44:18 - it's worth taking a moment though to
44:19 - notice what its limitations are
44:22 - first notice that all of the functions
44:25 - fall between plus and minus one
44:27 - the dark red and the dark green regions
44:31 - kiss the floor and the ceiling of this
44:33 - range but they never cross it this
44:36 - neural network would not be able to fit
44:38 - a function that extended outside of this
44:39 - range
44:42 - also notice that these functions all
44:44 - tend to be smooth
44:46 - they have hills and dips and valleys and
44:48 - wiggles and even points and wells
44:51 - but it all happens relatively smoothly
44:54 - if we hope to fit a function with a lot
44:56 - of jagged jumps and drops this neural
44:58 - network might not be able to do a very
45:00 - good job of it
45:03 - however aside from these two limitations
45:06 - the variety of functions that this
45:07 - neural network can produce is a little
45:10 - mind-boggling
45:12 - we modified a single output neural
45:15 - network to be a classifier when we
45:17 - looked at the multi-layer perceptron
45:19 - now there's another way to do this
45:22 - we can use a two output neural network
45:24 - instead
45:26 - outputs of a three layer
45:29 - one input
45:30 - to output neural network like this
45:34 - we can see that there are many cases
45:36 - where the two curves cross and in some
45:38 - instances they cross in several places
45:42 - we can use this to make a classifier
45:45 - wherever the one output is greater than
45:48 - another
45:49 - it can signify that one category
45:51 - dominates another
45:54 - graphically
45:55 - wherever the two output functions cross
45:57 - we can draw a vertical line
46:00 - this chops up the input space into
46:02 - regions
46:03 - in each region one output is greater
46:05 - than the other
46:07 - for instance wherever the blue line is
46:09 - greater we can assign that to be
46:11 - category a
46:13 - then wherever the peach colored line is
46:15 - greater those regions are category b
46:19 - just like the multi-layer perceptron
46:21 - this lets us chop the space up in more
46:23 - complex ways than a linear classifier
46:25 - could
46:27 - regions of category a and category b can
46:29 - be shuffled together arbitrarily
46:35 - when you only have two outputs the
46:37 - advantages of doing it this way over a
46:39 - multi-layer perceptron with just one
46:41 - output are not at all clear
46:43 - however
46:45 - if you move to three or more outputs the
46:47 - story changes
46:49 - now we have three separate outputs and
46:52 - three separate output
46:54 - functions we can use our same criterion
46:57 - of letting the function with the maximum
47:00 - value determine the category
47:03 - we start by chopping up the input space
47:06 - according to which function has the
47:08 - highest value
47:10 - each function represents one of our
47:12 - categories
47:13 - we're going to assign our first function
47:15 - to be category a and label every region
47:19 - where it's on top
47:21 - as category a
47:23 - then we can do the same with our second
47:24 - function
47:26 - and our third
47:28 - using this trick we are no longer
47:30 - limited to two categories
47:33 - we can create as many output nodes as we
47:35 - want
47:36 - and learn and chop up the input space
47:38 - into that many categories
47:41 - it's worth pointing out that the winning
47:43 - category may not be the best by very
47:46 - much
47:47 - in some cases you can see they can be
47:49 - very close
47:51 - one category will be declared the winner
47:53 - but the next runner up may be almost as
47:55 - good a fit
47:58 - there's no reason that we can't extend
48:00 - this approach to two or more inputs
48:02 - unfortunately it does get harder to
48:04 - visualize
48:06 - you have to imagine several of these
48:07 - lumpy landscape plots on top of each
48:10 - other
48:10 - and in some regions one will be greater
48:12 - than the others
48:14 - in that region that category associated
48:17 - with that output will be dominant
48:20 - to get a qualitative sense for what
48:22 - these regions might look like you can
48:25 - look at the projected contours on the
48:27 - floor of these plots
48:30 - in the case of a multi-layer perceptron
48:32 - these plots are all sliced at the y
48:35 - equals zero level
48:38 - that means if you look at the floor of
48:39 - the plot
48:40 - everything in any shade of green will be
48:43 - one category and everything in any shade
48:45 - of red will be the other category
48:49 - the first thing that jumps out about
48:50 - these category boundaries is how diverse
48:52 - they are
48:54 - some of them are nearly straight lines
48:56 - albeit with a small wiggle
48:59 - some of them have wilder bends and
49:01 - curves and some of them chop the input
49:03 - space up into several disconnected
49:05 - regions of green and red
49:07 - sometimes there's a small island of
49:09 - green or island of red in the middle of
49:12 - a sea of the other color
49:15 - the variety of boundaries is what makes
49:17 - this such a powerful classification tool
49:21 - the one limitation we can see looking at
49:23 - it this way is that the boundaries are
49:25 - all smoothly curved
49:28 - sometimes those curves are quite sharp
49:30 - but usually they're gentle and rounded
49:33 - this shows the natural preference that
49:36 - neural networks with hyperbolic tangent
49:38 - activation functions
49:40 - have for smooth functions and smooth
49:43 - boundaries
49:44 - the goal of this exploration was to get
49:46 - an intuitive sense for what types of
49:48 - functions and category boundaries neural
49:50 - networks can learn when used for
49:52 - regression or classification
49:55 - we've seen both their power and their
49:57 - distinct preference for smoothness
50:00 - we've only looked at two nonlinear
50:02 - activation functions
50:04 - logistic and hyperbolic tangent both of
50:06 - which are very closely related
50:09 - there are lots of others and some of
50:10 - them do a bit better at capturing sharp
50:12 - non-linearities
50:14 - rectified linear units or relu's for
50:16 - instance
50:17 - produce surfaces and boundaries that are
50:19 - quite a bit sharper
50:21 - but my hope was to seed your intuition
50:23 - with some examples of what's actually
50:25 - going on under the hood
50:27 - when you train your neural network
50:29 - here are the most important things to
50:31 - walk away with
50:33 - neural networks learn functions and can
50:35 - be used for regression
50:37 - some activation functions limit the
50:39 - output range but as long as that matches
50:41 - the expected range of your outputs it's
50:43 - not a problem
50:46 - second neural networks are most often
50:48 - used for classification
50:50 - they've proven pretty good at it
50:53 - third
50:54 - neural networks tend to create smooth
50:56 - functions when used for regression
50:59 - and smooth category boundaries when used
51:01 - for classification
51:04 - fourth
51:06 - for fully connected vanilla neural
51:08 - networks
51:09 - a two-layer network can learn any
51:11 - function that a deep network can learn
51:15 - however a deep network might be able to
51:17 - learn it with fewer nodes
51:21 - fifth making sure that inputs are
51:23 - normalized that is they have a mean near
51:26 - zero and a standard deviation of less
51:28 - than one
51:30 - this helps neural networks to be more
51:31 - sensitive to their relationships
51:35 - i hope this helps you as you jump into
51:36 - your next project
51:38 - happy building
51:40 - welcome to how convolutional neural
51:42 - networks work
51:46 - convolutional neural networks or
51:48 - convnets or cnns
51:51 - can do some pretty cool things
51:53 - if you feed them a bunch of pictures of
51:55 - faces for instance they'll learn some
51:57 - basic things like edges and dots bright
52:00 - spots dark spots
52:02 - and then because they're a multi-layer
52:04 - neural network
52:06 - that's what gets learned in the first
52:07 - layer the second layer are things that
52:09 - are recognizable as eyes noses mouths
52:13 - and the third layer are things that look
52:14 - like faces
52:16 - similarly if you feed it a bunch of
52:18 - images of cars down to the lowest layer
52:21 - you'll get things again that look like
52:22 - edges
52:23 - and then higher up look at things that
52:25 - look like tires and wheel wells and
52:27 - hoods
52:28 - and at a level above that things that
52:30 - are clearly identifiable as cars
52:35 - cnn's can even learn to play video games
52:38 - by forming patterns of the pixels as
52:41 - they appear on the screen
52:43 - and learning what is the best action to
52:46 - take when it sees a certain pattern
52:48 - a cnn can learn to play video games in
52:51 - some cases far better than a human ever
52:53 - could
52:56 - not only that if you take a couple of
52:59 - cnns and have them set to watching
53:02 - youtube videos
53:03 - one can learn objects
53:06 - by again picking out patterns and the
53:08 - other one can learn types of grasps
53:11 - this then coupled with some other
53:13 - execution software
53:15 - can let a robot learn to cook
53:18 - just by watching youtube
53:22 - so there's no doubt cnns are powerful
53:25 - usually when we talk about them we do so
53:28 - in the same way we might talk about
53:29 - magic
53:30 - but they're not magic what they do is
53:33 - based on some pretty basic ideas
53:35 - applied in a clever way
53:38 - so to illustrate these we'll talk about
53:40 - a very simple toy convolutional neural
53:43 - network
53:44 - what this one does
53:45 - is takes in an image a two-dimensional
53:48 - array of pixels you can think of it as a
53:51 - checkerboard and each square on the
53:53 - checkerboard is either light or dark
53:56 - and then by looking at that the cnn
53:58 - decides whether it's a picture of an x
54:01 - or of an o
54:03 - so for instance
54:05 - on top there we see an image with
54:08 - an x drawn in white pixels on a black
54:11 - background and we would like to identify
54:13 - this as an x
54:15 - and the o
54:17 - we'd like to identify as an o
54:20 - so how a cnn does this
54:23 - is uh has several steps in it
54:26 - what makes it tricky
54:29 - is that the x is not exactly the same
54:31 - every time
54:32 - the x or the o can be shifted it can be
54:35 - bigger or smaller can be rotated a
54:37 - little bit
54:38 - thicker or thinner
54:39 - and in every case we would still like to
54:41 - identify whether it's an x or an o
54:45 - now the reason that this is challenging
54:47 - is because for us
54:49 - deciding whether these two things are
54:51 - similar is straightforward we don't even
54:53 - have to think about it
54:54 - for a computer it's very hard
54:58 - what a computer sees
54:59 - is this checkerboard this
55:01 - two-dimensional array
55:03 - as a bunch of numbers
55:05 - ones and minus ones a one is a bright
55:07 - pixel a minus one is a black pixel
55:11 - and what it can do is go through pixel
55:13 - by pixel and compare whether they match
55:15 - or not
55:16 - so to computer to a computer
55:19 - it looks like there are a lot of pixels
55:21 - that match but some that don't quite a
55:23 - few that don't actually
55:24 - and so it might look at this and say ah
55:26 - i'm really not sure whether these are
55:28 - the same
55:31 - and so it would because the computer is
55:32 - so literal i would say
55:34 - uncertain i can't say that they're equal
55:39 - now one of the tricks that convolutional
55:41 - neural networks use is to match parts of
55:44 - the image
55:45 - rather than the whole thing
55:47 - so if you break it down into its smaller
55:49 - parts or features
55:51 - then it becomes much more clear
55:54 - whether these two things are similar
55:59 - so examples of these little features are
56:02 - little
56:03 - mini images in this case just three
56:05 - pixels by three pixels
56:07 - the one on the left is a diagonal line
56:10 - slanting downward from left to right the
56:13 - one on the right is also a diagonal line
56:15 - slanting in the other direction
56:17 - and the one in the middle is a little x
56:20 - these are little pieces of the bigger
56:21 - image
56:23 - and you can see as we go through if you
56:26 - choose the right feature and put it in
56:27 - the right place it matches the image
56:30 - exactly
56:32 - so okay we have the bits and pieces
56:35 - now to take a step deeper
56:38 - there the math behind matching these
56:41 - is called filtering
56:43 - and the way this is done is a feature
56:46 - is lined up with the little patch of the
56:48 - image
56:49 - and then one by one the pixels are
56:52 - compared
56:54 - they're multiplied by each other
56:57 - and then add it up and divide it by the
56:59 - total number of pixels
57:02 - so to step through this to see why it
57:03 - makes sense to do this you can see
57:05 - starting in the upper left-hand pixel in
57:08 - both the feature and the image patch
57:10 - multiplying one by a one gives you a one
57:14 - and we can keep track of that by putting
57:16 - that in the position of the pixel that
57:18 - we're comparing
57:21 - we step to the next one minus one times
57:23 - minus one is also a one
57:26 - and we continue to step through
57:29 - pixel by pixel
57:31 - multiplying them all by each other and
57:33 - because they're always the same the
57:35 - answer is always one
57:37 - when we're done we take all these ones
57:39 - and add them up and divide by nine
57:42 - and the answer is one
57:44 - so now we want to keep track of where
57:46 - that feature was in the image and we put
57:49 - a one there say when we put the feature
57:51 - here we get a match of one
57:54 - that is filtering
57:58 - now we can take uh that same feature
58:02 - and move it to another position and
58:04 - perform the filtering again and we start
58:07 - with the same pattern the first pixel
58:08 - matches the second pixel matches the
58:11 - third pixel does not match
58:13 - minus one times one equals minus one so
58:16 - we record that
58:17 - in our results
58:19 - and we go through and do that through
58:20 - the rest of the image patch
58:22 - and when we're done we notice we have
58:23 - two minus ones this time
58:25 - so we add up all the pixels to add up to
58:28 - five divide by nine
58:30 - and we get a point five five so this is
58:32 - very different than our one
58:34 - and we can record the 0.55 in that
58:37 - position where we were where it occurred
58:41 - so by moving our filter around to
58:43 - different places in the image
58:45 - we actually find different values for
58:47 - how well that filter matches or how well
58:51 - that feature is represented at that
58:53 - position
58:54 - so this becomes a map
58:56 - of where the feature occurs
59:00 - by moving it around to every possible
59:03 - position
59:04 - we do convolution
59:06 - that's just the repeated application of
59:09 - this feature this filter over and over
59:11 - again
59:13 - and what we get is a nice
59:15 - map across the whole image of where this
59:18 - feature occurs
59:20 - and if we look at it it makes sense this
59:22 - feature is a diagonal line slanting
59:24 - downward left to right
59:26 - which matches
59:28 - the downward left to right diagonal of
59:30 - the x so if we look at our filtered
59:33 - image we see that all of the high
59:35 - numbers ones and .77s are all right
59:39 - along that diagonal that suggests that
59:42 - that feature
59:43 - matches along that diagonal much better
59:45 - than it does elsewhere in the image
59:51 - to use a shorthand notation here we'll
59:53 - do a little x with a circle in it to
59:55 - represent convolution
59:57 - the act of trying every possible match
60:00 - and we repeat that with other features
60:03 - we can repeat that with our x filter in
60:06 - the middle and with our upward slanting
60:08 - diagonal line on the bottom
60:10 - and in each case the map that we get of
60:13 - where that feature occurs
60:15 - is consistent with what we would expect
60:17 - based on what we know about the x and
60:19 - about where our features match
60:25 - this
60:26 - act of convolving an image
60:28 - with a bunch of filters a bunch of
60:30 - features
60:31 - and creating a stack of filtered images
60:35 - is we'll call a convolution layer
60:39 - a layer because it's an operation that
60:41 - we can stack with others as we'll show
60:44 - in a minute
60:49 - in convolution one image becomes a stack
60:52 - of filtered images we get as many
60:54 - filtered images out as we have filters
60:58 - so convolution layer is one trick that
61:01 - we have
61:03 - the next big trick that we have is
61:05 - called pooling
61:07 - this is how we shrink the image stack
61:10 - and this is pretty straightforward
61:12 - we start with a window size usually two
61:14 - by two pixels or three by three pixels
61:17 - and a stride usually two pixels just in
61:20 - practice these work best
61:22 - and then we take that window and walk it
61:25 - in strides across each of the filtered
61:28 - images
61:29 - from each window we take the maximum
61:31 - value
61:33 - so to illustrate this we start with our
61:35 - first filtered image
61:37 - we have our 2 pixel by 2 pixel window
61:39 - within that pixel the maximum value is
61:41 - 1.
61:42 - so we track that and then move to
61:45 - our stride of 2 pixels we move 2 pixels
61:48 - to the right and repeat
61:50 - out of that window the maximum value is
61:52 - 0.33
61:53 - etc 0.55
61:56 - and when we get to the end we have to be
61:57 - creative we have
61:59 - don't have all the pixels representative
62:01 - so we take the max of what's there
62:04 - and we continue doing this across the
62:05 - whole image and when we're done
62:08 - what we end up with is a similar pattern
62:11 - but smaller
62:13 - we can still see our high values are all
62:16 - on the diagonal
62:18 - but instead of seven by seven pixels in
62:22 - our filtered image we have a four by
62:23 - four pixel image so it's half as big as
62:26 - it was about
62:29 - this makes a lot of sense to do
62:31 - if you can imagine if instead of
62:32 - starting with a nine by nine pixel image
62:35 - we had started with a nine thousand by
62:37 - nine thousand pixel image shrinking it
62:40 - is convenient for uh working with it
62:43 - makes it smaller
62:45 - the other thing it does
62:46 - is pooling doesn't care where in that
62:50 - window that maximum value occurs
62:53 - so that makes it a little less sensitive
62:56 - to position
62:57 - and the way this plays out is that if
62:59 - you're looking for a
63:01 - particular feature in an image it can be
63:03 - a little to the left a little to the
63:05 - right maybe a little rotated and it'll
63:07 - still get picked up
63:12 - so we do max pooling with all of our
63:15 - stack of filtered images
63:17 - and get in every case
63:19 - smaller set of filtered images
63:25 - now that's our second trick third trick
63:28 - normalization
63:31 - this is just a step to keep the math
63:33 - from blowing up
63:34 - and keep it from going to zero
63:38 - all you do here
63:40 - is everywhere in your image
63:42 - that there is a negative value
63:45 - change it to zero
63:47 - so for instance if we're looking back at
63:49 - our filtered image we have these what
63:51 - are called rectified linear units that's
63:54 - the little computational unit that does
63:57 - this but all it does is steps through
64:00 - everywhere there's a negative value
64:02 - change it to zero
64:04 - another negative value change it to zero
64:07 - by the time you're done you have a very
64:09 - similar looking image
64:11 - except there's no negative values
64:13 - they're just zeros
64:16 - and we do this with all of our images
64:18 - and this becomes another
64:20 - type of layer
64:21 - so in a rectified linear unit layer a
64:24 - stack of images becomes a stack of
64:26 - images with no negative values
64:31 - now what's really fun the magic starts
64:32 - to happen here
64:34 - when we take these layers convolution
64:36 - layers rectified linear unit layers and
64:39 - pooling layers and we stack them up so
64:42 - that the output of one becomes the input
64:44 - of the next
64:46 - you'll notice that what goes into each
64:48 - of these and what comes out of these
64:50 - looks like an array of pixels or
64:53 - an array of an array of pixels
64:56 - and
64:58 - because of that we can stack them nicely
65:00 - we can use the output of one for the
65:02 - input of the next and by stacking them
65:05 - we get these operations building on top
65:08 - of each other
65:10 - what's more we can repeat the stacks we
65:13 - can do deep stacking you can imagine
65:15 - making a sandwich that is not just
65:18 - one patty and one slice of cheese and
65:20 - one lettuce and one tomato but a whole
65:21 - bunch of
65:22 - layers double tripper triple quadruple
65:24 - deckers as many times as you want
65:27 - each time
65:29 - the image gets
65:30 - more filtered as it goes through
65:32 - convolution layers
65:34 - and it gets smaller as it goes through
65:36 - pooling layers
65:40 - now the final layer
65:43 - in our toolbox is called a fully
65:45 - connected layer
65:47 - here every value gets a vote
65:51 - on what the answer is going to be
65:53 - so we take our now much filtered and
65:56 - much reduced in size stack of images we
65:58 - break them out we just rearrange and put
66:00 - them into a single list because it's
66:01 - easier to visualize that way
66:03 - and then each of those connects to one
66:05 - of our answers that we're going to vote
66:07 - for
66:09 - when we feed this in x
66:12 - there will be certain values here that
66:14 - tend to be high they tend to predict
66:16 - very strongly this is going to be an x
66:18 - they get a lot of vote
66:20 - for the x outcome
66:24 - similarly
66:25 - when we feed in a picture of an o
66:28 - to our convolutional neural network
66:31 - there are certain values here at the end
66:33 - that tend to be very high and tend to
66:35 - predict strongly when we're going to
66:36 - have an o at the end
66:38 - so they get a lot of weight a strong
66:40 - vote
66:41 - for the o category
66:44 - now when we get a new input and we don't
66:46 - know what it is and we want to decide
66:49 - the way this works
66:51 - is the input goes through all of our
66:53 - convolutional
66:55 - our
66:56 - rectified linear unit our pooling layers
66:58 - and comes out to the end here we get a
67:00 - series of votes
67:02 - and then based on the weights
67:06 - that each value gets to vote with
67:08 - we get a nice average vote at the end
67:10 - in this case this this particular set of
67:12 - inputs votes for an x with a strength of
67:14 - 0.92
67:17 - and an o with a strength of 0.51
67:20 - so here definitely x is the winner
67:23 - and so
67:24 - the neural network would categorize this
67:27 - input as an x
67:32 - so in a fully connected layer
67:35 - a list of feature values
67:37 - becomes a list of votes
67:40 - now
67:41 - again what's cool here is that a list of
67:43 - votes
67:44 - looks a whole lot like a list of feature
67:46 - values so you can use the output of one
67:49 - for the input of the next
67:51 - and so you can have intermediate
67:53 - categories that aren't your final votes
67:56 - or sometimes these are called hidden
67:57 - units in a neural network and you can
68:00 - stack as many of these together as you
68:01 - want also
68:03 - but in the end they all end up voting
68:05 - for an x or an o and whoever gets the
68:07 - most votes wins
68:11 - so if we put this all together
68:14 - then
68:15 - a two-dimensional array of pixels in
68:17 - results in a set of votes for a category
68:20 - out at the far end
68:24 - so there are some things that we have
68:26 - glossed over here
68:28 - you might be asking yourself where all
68:30 - of the magic numbers come from
68:33 - things that i pulled out of thin air
68:35 - include the features in the
68:38 - convolutional layers those convenient
68:42 - three pixel by three pixel diagonal
68:44 - lines of the x
68:46 - also
68:47 - the voting weights
68:49 - in the fully connected layers i really
68:50 - waved my hands about how those are
68:52 - obtained
68:54 - in all these cases the answer is the
68:56 - same there is a trick called back
68:58 - propagation
69:00 - all of these are learned you don't have
69:02 - to know them you don't have to guess
69:03 - them
69:05 - the deep neural network does this on its
69:08 - own
69:10 - so the underlying principle behind back
69:13 - propagation is that the error in the
69:16 - final answer is used to determine how
69:19 - much the network adjusts
69:22 - and changes
69:24 - so in this case
69:25 - if we knew we were putting in an x
69:28 - and we got a 0.92 vote for an x
69:32 - and that would be an error of 0.08
69:36 - and we got a 0.51 vote for o
69:40 - we know that that would be an error of
69:42 - 0.49
69:43 - actually an error of 0.51 because it
69:45 - should be 0. then if we add all that up
69:48 - we get an error of what should be 0.59
69:53 - so
69:54 - what happens with this error signal
69:57 - is it helps drive a process called
69:59 - gradient descent
70:02 - if there is another bit of something
70:04 - that
70:05 - is pretty special sauce to deep neural
70:07 - networks it is the ability to do
70:09 - gradient descent
70:11 - so for each of these magic numbers each
70:14 - of the feature pixels each voting weight
70:16 - they're adjusted up and down by a very
70:19 - small amount
70:20 - to see how the error changes
70:24 - the amount that they're adjusted
70:26 - is determined by how big the error is
70:29 - large error they're adjusted a lot
70:31 - smaller just a tiny bit no error they're
70:34 - not adjusted at all you have the right
70:36 - answer stop messing with it
70:39 - as they're adjusted you can think of
70:41 - that as
70:42 - sliding a ball slightly to the left and
70:45 - slightly to the right on a hill you want
70:47 - to find the direction where it goes
70:49 - downhill you want to go down that slope
70:52 - down that gradient to find the very
70:54 - bottom because the bottom is where you
70:56 - have the very least error that's your
70:58 - happy place
70:59 - so after sliding it to the left and to
71:01 - the right you find the downhill
71:02 - direction and you leave it there
71:07 - doing that many times over lots of lots
71:10 - of iterations lots of steps helps all of
71:13 - these values across all the features
71:15 - and all of the weights
71:17 - settle in to what's called a minimum
71:20 - and it
71:21 - uh and it at that point the network is
71:23 - performing as well as it possibly can if
71:25 - it adjusts any of those a little bit its
71:27 - behavior its error will go up
71:32 - now there are some things called hyper
71:34 - parameters and these are knobs that the
71:37 - designer gets to turn decisions the
71:39 - designer gets to make these are not
71:42 - learned automatically
71:45 - in convolution
71:47 - figuring out how many features should be
71:49 - used
71:50 - how big those features should be how
71:52 - many pixels on the side
71:55 - in the pooling layers choosing the
71:57 - window size and the window stride
72:01 - and in fully connected layers choosing
72:03 - the number of hidden neurons
72:05 - intermediate neurons
72:06 - all of these things are decisions that
72:09 - the designer gets to make
72:11 - right now there are some
72:14 - common practices that tend to work
72:16 - better than others but there is no
72:18 - principled way there's no hard and fast
72:20 - rules for the right way to do this
72:23 - and in fact a lot of the advances in
72:26 - convolutional neural networks are in
72:28 - getting combinations of these they work
72:30 - really well
72:33 - now in addition to this
72:35 - there are other decisions the designer
72:36 - gets to make like how many of each type
72:39 - of layer
72:40 - and in what order
72:42 - and for those that really like to go off
72:44 - the rails can we design new types of
72:46 - layers entirely and slip them in there
72:49 - and get new fun behaviors
72:51 - these are all things that people are
72:54 - playing with to try to eke out more
72:57 - performance and address stickier
72:59 - problems with cnns
73:06 - now what's really cool about these we've
73:07 - been talking about images
73:10 - but you can use any two-dimensional or
73:12 - even for that matter three or four
73:13 - dimensional data
73:15 - but what's important is that in your
73:18 - data
73:19 - things closer together are more closely
73:21 - related than things far away
73:26 - what i mean by that is if you look at an
73:28 - image
73:29 - two rows of pixels or two columns of
73:32 - pixels are right next to each other
73:34 - they're more closely related than rows
73:36 - or columns that are far away
73:41 - now what you can do
73:43 - is you can take something like sound
73:46 - and you can chop it up into little time
73:49 - steps
73:50 - and for each piece of time
73:52 - the
73:53 - time step right before it and right
73:54 - after is more closely related than time
73:58 - steps that are far away and the order
74:01 - matters
74:03 - you can also chop it up into different
74:04 - frequency bands
74:06 - base mid-range treble you can slice it a
74:10 - whole lot more finally than that
74:12 - and again those frequency bands
74:15 - are the ones closer together are more
74:17 - closely related
74:19 - and
74:20 - you can't rearrange them the order
74:21 - matters
74:24 - once you do this with sound it looks
74:26 - like a picture it looks like an image
74:28 - and you can use convert convolutional
74:30 - neural networks with them
74:33 - you can do something similar with text
74:36 - where
74:37 - the position in the sentence becomes the
74:40 - column
74:41 - and the row
74:42 - is words in a dictionary
74:45 - in this case
74:47 - it's hard to argue whether order matters
74:50 - that order matters it's hard to argue
74:52 - that words in the dictionary are that
74:54 - some are more closely related than
74:56 - others in all cases and so the trick
74:58 - here is to take a window that spans the
75:02 - entire column top to bottom
75:05 - and then slide it left to right that way
75:07 - it captures all of the words but it only
75:10 - captures a few positions in the sentence
75:12 - at a time
75:17 - now the other side of this limitation of
75:19 - convolutional neural networks is that
75:22 - they're really designed to capture
75:25 - local spatial patterns spatial in the
75:28 - sense of things that are next together
75:30 - next to each other matter quite a bit
75:33 - so if the data can't be made to look
75:35 - like an image
75:36 - then they're not as useful
75:40 - so an example of this is say some
75:42 - customer data
75:44 - if i have each row it's a separate
75:46 - customer
75:47 - each column is a separate piece of
75:49 - information about that customer
75:51 - such as their name their address what
75:54 - they bought what websites they visited
75:58 - then this doesn't so much look like a
76:00 - picture i can take and rearrange those
76:02 - columns and rearrange those rows
76:05 - and this
76:07 - still means the same thing it's still
76:09 - equally easy to interpret
76:12 - if i were to take an image
76:14 - and rearrange the columns and rearrange
76:16 - the rows it would result in a scramble
76:18 - of pixels and it would be difficult or
76:20 - impossible to say what the image was of
76:22 - there i would lose a lot of information
76:26 - so as a rule of thumb
76:29 - if your data is just as useful after
76:31 - swapping out any of the columns for each
76:33 - other then you can't use
76:36 - convolutional neural networks
76:42 - so the take home is that convolutional
76:45 - neural networks are great at finding
76:46 - patterns and using them to classify
76:49 - images
76:50 - if you can make your problem look like
76:53 - finding cats on the internet
76:56 - then they're a huge asset
76:59 - applications of machine learning have
77:02 - gotten a lot of traction in the last few
77:03 - years
77:04 - there's a couple of big categories that
77:06 - have had winds one
77:08 - is
77:09 - identifying pictures
77:11 - the equivalent of finding cats on the
77:13 - internet and any problem that can be
77:15 - made to look like that
77:16 - and the other is sequence to sequence
77:19 - translation this can be speech to text
77:22 - or one language to another
77:24 - most of the former are done with
77:26 - convolutional neural networks most of
77:28 - the latter are done with recurrent
77:31 - neural networks uh particularly long
77:33 - short-term memory to give an example of
77:36 - how long short-term memory works we will
77:38 - consider the question of what's for
77:40 - dinner
77:41 - let's say for a minute that you are a
77:42 - very lucky apartment dweller and you
77:44 - have a flatmate who loves to cook dinner
77:47 - every night he cooks one of three things
77:50 - sushi
77:51 - waffles
77:53 - or pizza
77:55 - and you would like to be able to predict
77:57 - what you're going to have on a given
77:58 - night so you can plan the rest of your
78:00 - days eating accordingly
78:02 - in order to predict what you're going to
78:04 - have for dinner
78:05 - you set up a neural network
78:07 - the inputs to this neural network are
78:10 - a bunch of items like the day of the
78:12 - week the month of the year whether or
78:15 - not your flatmate was in a late meeting
78:17 - variables that might reasonably affect
78:20 - what you're going to have for dinner
78:24 - now if you're new to neural networks i
78:26 - highly recommend you take a minute and
78:29 - stop to watch the how neural networks
78:31 - work tutorial there's a link down in the
78:34 - comments section
78:35 - if you'd rather not do that right now
78:38 - and you're still not familiar with
78:39 - neural networks you can think of them as
78:41 - a voting process
78:43 - and so in the neural network that you
78:45 - set up there's a complicated voting
78:47 - process and all of the inputs like day
78:49 - of the week and month of the year go
78:51 - into it
78:52 - and then you train it
78:54 - on your history of what you've had for
78:56 - dinner
78:57 - and you learn
78:58 - how to predict what's going to be for
79:00 - dinner tonight
79:03 - the trouble is that your network doesn't
79:05 - work very well
79:06 - despite carefully choosing your inputs
79:09 - and training it thoroughly
79:11 - you still can't get much better than
79:12 - chance predictions on dinner
79:15 - as is often the case with complicated
79:17 - machine learning problems it's useful to
79:19 - take a step back
79:21 - and just look at the data
79:23 - and when you do that you notice a
79:24 - pattern
79:25 - your flat mate makes
79:27 - pizza
79:28 - then sushi then waffles then pizza again
79:32 - in a cycle
79:34 - it doesn't depend on the day of the week
79:35 - or anything else it's in a regular cycle
79:39 - so knowing this we can make a new neural
79:41 - network
79:43 - in our new one the only inputs that
79:45 - matter are what we had for dinner
79:47 - yesterday
79:48 - so if we know if we had pizza for dinner
79:50 - yesterday it'll be sushi tonight sushi
79:52 - yesterday waffles tonight and waffles
79:54 - yesterday pizza tonight
79:56 - it becomes a very simple voting process
79:59 - and and it's right all the time because
80:02 - your flatmate is incredibly consistent
80:06 - now if you happen to be gone on a given
80:09 - night let's say yesterday you were out
80:12 - you don't know what was for dinner
80:13 - yesterday
80:15 - you can still predict what's going to be
80:17 - for dinner tonight by thinking back two
80:19 - days ago
80:21 - think what was for dinner then
80:24 - so what would be predicted
80:26 - for you last night
80:28 - and then you can use that prediction in
80:30 - turn to make a prediction for tonight
80:33 - so we make use of not only
80:36 - our actual information from yesterday
80:39 - but also
80:40 - what our prediction was yesterday
80:43 - so at this point it's helpful to take a
80:44 - little detour and talk about vectors
80:46 - a vector is just a fancy word for a list
80:49 - of numbers if i wanted to describe the
80:51 - weather to you for a given day i could
80:53 - say
80:54 - the high is 76 degrees fahrenheit the
80:56 - low is 43 the wind's 13 miles an hour
80:59 - there's going to be a quarter inch of
81:00 - rain and the relative humidity is 83
81:03 - percent
81:05 - that's all a vector is the reason that
81:07 - it's useful is vectors list of numbers
81:10 - are computers native language
81:12 - if you want to get something into a
81:13 - format that it's natural for a computer
81:15 - to compute to do operations on to do
81:18 - statistical machine learning lists of
81:20 - numbers are the way to go
81:21 - everything gets reduced to a list of
81:24 - numbers before it goes through an
81:25 - algorithm we can also have a vector for
81:28 - statements like it's tuesday
81:32 - in order to encode this kind of
81:33 - information what we do is we make a list
81:35 - of all the possible values it could have
81:38 - in this case all the days of the week
81:40 - and we assign a number to each
81:42 - and then we go through and set them all
81:44 - equal to zero except for the one that is
81:47 - true right now
81:48 - uh this format is called one hot
81:51 - encoding and it's very common to see a
81:53 - long vector of zeros with just one
81:55 - element being one
81:57 - it seems inefficient but for a computer
81:59 - this is a lot easier way to ingest that
82:02 - information
82:05 - so we can make a one hot vector for our
82:08 - prediction for dinner tonight we set
82:10 - everything equal to zero except for the
82:13 - dinner item that we predict so in this
82:15 - case we'll be predicting sushi
82:19 - now we can group together
82:21 - our
82:22 - uh we can group together our inputs and
82:24 - outputs into vectors separate lists of
82:26 - numbers
82:27 - and it becomes a useful shorthand for
82:29 - describing this neural network
82:32 - so we can have our dinner yesterday
82:34 - vector
82:35 - our predictions for yesterday vector and
82:37 - our prediction for today vector
82:41 - and the neural network is just
82:42 - connections between every element in
82:45 - each of those input vectors to every
82:47 - element in the output vector
82:50 - and to complete our picture we can show
82:53 - how the prediction for today will get
82:55 - recycled the dotted line there means
82:58 - hold on to it for a day and then reuse
83:00 - it tomorrow
83:01 - and it becomes our yesterday's
83:03 - predictions tomorrow now we can see how
83:06 - if we were lacking some information
83:08 - let's say we were out of town for two
83:09 - weeks we can still make a good guess
83:11 - about what's going to be for dinner
83:13 - tonight
83:14 - we just
83:15 - ignore the new information part and we
83:17 - can unwrap or unwind this vector in time
83:22 - until we do have some information to
83:24 - base it on and then just play it forward
83:28 - and when it's unwrapped it looks like
83:30 - this
83:31 - and we can go back as far as we need to
83:33 - and see what was for dinner and then
83:34 - just trace it forward and play out our
83:37 - menu over the last two weeks until we
83:39 - find out what's for dinner tonight
83:42 - so this is a nice simple example that
83:44 - showed recurrent neural networks now to
83:47 - show how they don't meet all of our
83:49 - needs
83:50 - we're going to write a children's book
83:51 - it'll have sentences of the format
83:54 - doug saw jane period
83:56 - jane saw spot period spot saw doug
84:00 - period and so on
84:04 - so our dictionary is small just the
84:06 - words doug
84:08 - jane
84:09 - spot
84:10 - saw and a period
84:12 - and the task of the neural network is to
84:14 - put these together in the right order to
84:16 - make a good children's book
84:18 - so to do this we replace our
84:21 - food vectors with our dictionary vectors
84:24 - here again it's just a list of numbers
84:27 - representing each of the words so for
84:30 - instance
84:31 - if doug was the most recent word that i
84:33 - saw
84:34 - my new information vector would be all
84:37 - zeros except for a one in the dug
84:40 - position
84:42 - and we similarly can represent our
84:43 - predictions and our predictions from
84:45 - yesterday
84:48 - now
84:49 - after training
84:51 - this neural network and teaching it what
84:53 - to do we would expect to see certain
84:55 - patterns
84:56 - for instance anytime a name comes up
84:59 - jane dug or spot
85:01 - we would expect that to vote heavily for
85:04 - the word saw or for a period
85:07 - because those are the two words in our
85:09 - dictionary that can follow a name
85:13 - similarly if we had predicted a name on
85:16 - the previous time step we would expect
85:18 - those to vote also for the word saw or
85:21 - for a period
85:24 - and then by a similar method anytime we
85:27 - come across the word saw or a period
85:30 - we know that a name has to come after
85:32 - that so it will learn to vote very
85:35 - strongly for a name
85:37 - jane doug or spot
85:42 - so in this form in this formulation we
85:45 - have a recurrent neural network
85:48 - for simplicity i'll take the vectors and
85:50 - the weights and collapse them down to
85:52 - that little symbol with the dots and the
85:54 - arrows the dots and the lines connecting
85:56 - them
85:57 - and there's one more symbol we haven't
85:59 - talked about yet
86:00 - this is a squashing function and it just
86:03 - helps the network to behave
86:05 - how it works is you take all of your
86:08 - votes
86:10 - coming out and you subject them to this
86:12 - squashing function for instance if
86:14 - something received a total vote of 0.5
86:17 - you draw a vertical line up where it
86:18 - crosses the function you draw a
86:20 - horizontal line over to the y axis and
86:23 - there is your squashed version out
86:26 - for small numbers the squashed version
86:28 - is pretty close to the original version
86:30 - but as your number gets larger
86:32 - the number that comes out is closer and
86:35 - closer to one
86:37 - and similarly if you put in a big
86:38 - negative number then what you'll get out
86:40 - will be very close to minus one
86:43 - no matter what you put in
86:45 - what comes out is between minus one and
86:47 - one
86:49 - so this is really helpful when you have
86:51 - a loop like this where the same values
86:54 - get processed again and again day after
86:56 - day
86:58 - it is possible you can imagine if in the
87:00 - course of that processing say something
87:02 - got voted for twice it got multiplied by
87:04 - two
87:06 - in that case it would get twice as big
87:08 - every time and very soon blow up to be
87:12 - astronomical
87:14 - by ensuring that it's always less than
87:17 - one but more than minus one
87:19 - you can multiply it
87:21 - as many times as you want you can go
87:22 - through that loop and it won't explode
87:25 - in a feedback loop this is an example of
87:28 - negative feedback or attenuating
87:29 - feedback
87:31 - so you may have noticed our neural
87:32 - network in its current state is subject
87:34 - to some mistakes
87:37 - we could get a sentence for instance of
87:38 - the form doug
87:40 - saw
87:41 - doug
87:42 - period
87:43 - because
87:44 - doug strongly votes for the word saw
87:47 - which in turn strongly votes for a name
87:49 - any name which could be doug
87:52 - similarly we could get something like
87:54 - doug saw jane
87:56 - saw spot saw
87:59 - because each of our predictions only
88:02 - looks back one time step it has very
88:05 - short-term memory
88:07 - then it doesn't use the information from
88:09 - further back
88:10 - and it's subject to these types of
88:12 - mistakes
88:14 - in order to overcome this we take our
88:16 - recurrent neural network and we expand
88:18 - it and we add some more pieces to it
88:22 - the critical part that we add to the
88:24 - middle here
88:25 - is memory
88:27 - we want to be able to remember what
88:29 - happened many times steps ago
88:33 - so in order to explain how this works
88:35 - i'll have to describe a few new symbols
88:37 - we've introduced here
88:39 - one is another squashing function this
88:41 - one with a flat bottom
88:44 - one is an x in a circle and one is a
88:46 - cross in a circle
88:50 - so the cross in a circle
88:52 - is element by element addition
88:55 - the way it works is you start with two
88:57 - vectors of equal size
88:59 - and you go down each one
89:01 - you add the first element of one vector
89:04 - to the first element of another vector
89:06 - and then the total goes into the first
89:08 - element of the output vector
89:11 - so 3
89:12 - plus 6 equals 9. then you go to the next
89:15 - element 4 plus 7 equals 11.
89:18 - and so your output vector is the same
89:20 - size of each of your input vectors
89:23 - just a list of numbers same length but
89:25 - it's the sum element by element of the
89:28 - two
89:31 - and very closely related to this you've
89:33 - probably guessed the
89:35 - x in the circle is element by element
89:37 - multiplication
89:39 - it's just like addition except instead
89:41 - of adding you multiply for instance 3
89:44 - times 6 gives you a first element of 18.
89:47 - 4 times 7 gets you 28.
89:50 - again the output vector is the same size
89:52 - of each of the input vectors
89:57 - now element-wise multiplication lets you
89:59 - do something pretty cool
90:02 - um
90:02 - you imagine that you have
90:05 - a signal and it's like a bunch of pipes
90:08 - and they have a certain amount of water
90:10 - trying to flow down them
90:12 - in this case we'll just assign the
90:14 - number to that of 0.8 it's like a signal
90:17 - now
90:18 - on each of those pipes we have a faucet
90:21 - and we can open it all the way close it
90:23 - all the way or keep it somewhere in the
90:25 - middle to either let that signal come
90:27 - through or block it
90:29 - so in this case an open gate an open
90:32 - faucet would be a one and a closed
90:34 - faucet would be a zero
90:36 - and the way this works with element wise
90:39 - multiplication we get 0.8 times one
90:42 - equals 0.8
90:44 - that signal passed right through into
90:46 - the output vector but the last element
90:49 - 0.8 times 0
90:52 - equals 0.
90:53 - that signal the original signal was
90:55 - effectively blocked
90:57 - and then with the gating value of 0.5
91:00 - the signal was passed through but it's
91:02 - smaller it's attenuated
91:05 - so
91:06 - gating lets us control what passes
91:10 - through and what gets blocked
91:12 - which is really useful
91:15 - now in order to do gating
91:17 - it's nice to have a value that you know
91:19 - is always between zero and one
91:21 - so we introduce another squashing
91:24 - function
91:25 - this will represent with a circle with a
91:26 - flat bottom
91:28 - and this is it's called the logistic
91:30 - function it's very similar to the other
91:32 - squashing function the hyperbolic
91:33 - tangent
91:34 - except that it just goes between zero
91:36 - and one instead of minus one and one
91:41 - now when we introduce all of these
91:43 - together what we get
91:45 - we still have the combination of our
91:48 - previous predictions and our new
91:49 - information
91:51 - those vectors get passed
91:53 - and we make predictions based on them
91:56 - those predictions get passed through but
91:59 - the other thing that happens is a copy
92:02 - of those predictions
92:04 - is held onto
92:05 - for the next time step the next pass
92:08 - through the network
92:10 - and some of them here's a gate right
92:12 - here
92:13 - some of them are forgotten some of them
92:15 - are remembered
92:17 - the ones that are remembered are added
92:19 - back into the prediction
92:21 - so now we have not just prediction but
92:23 - predictions plus the memories that we've
92:26 - accumulated
92:27 - and that we haven't chosen to forget yet
92:31 - now there's an entirely separate neural
92:33 - network here that learns
92:36 - when to forget what
92:39 - based on what we're seeing right now
92:40 - what do we want to remember what do we
92:42 - want to forget
92:45 - so you can see this is powerful this
92:47 - will let us hold on to things for as
92:49 - long as we want
92:52 - now you've probably noticed though
92:55 - when we are
92:56 - combining our predictions with our
92:58 - memories we may not necessarily want to
93:01 - release all of those memories out as new
93:03 - predictions each time
93:06 - so we want a little filter to keep our
93:08 - memories inside and let our predictions
93:10 - get out
93:12 - and that's we add another gate for that
93:14 - to do selection
93:16 - it has its own neural network so its own
93:19 - voting process so that our new
93:21 - information and our previous predictions
93:23 - can be used
93:24 - to vote on what all the gates should be
93:27 - what should be kept internal and what
93:29 - should be released as a prediction
93:32 - we've also introduced another squashing
93:34 - function here since we do an addition
93:36 - here it's possible that things could
93:38 - become greater than one or smaller than
93:40 - minus one so we just squash it to be
93:42 - careful to make sure it never gets out
93:44 - of control
93:46 - and now
93:47 - when we bring in new predictions we
93:50 - make a lot of possibilities
93:52 - and then we collect those with memory
93:55 - over time and of all of those possible
93:57 - predictions
93:59 - at each time step
94:00 - we select just a few to release as the
94:03 - prediction for that moment
94:06 - each of these things
94:08 - when to forget
94:09 - and when to let things out of our memory
94:12 - are learned by their own neural networks
94:16 - and the only other piece we need to add
94:19 - to complete our picture here is yet
94:21 - another set of gates
94:23 - this lets us actually ignore
94:26 - uh possible predictions possibilities as
94:29 - they come in
94:30 - this is an attention mechanism
94:33 - it lets things that aren't immediately
94:35 - relevant
94:36 - be set aside so they don't cloud the
94:39 - predictions in memory going forward
94:42 - it has its own neural network and its
94:45 - own
94:46 - logistic squashing function and its own
94:48 - gating activity right here
94:51 - now long short term memory has a lot of
94:53 - pieces a lot of bits that work together
94:56 - and it's a little much to wrap your head
94:58 - around it all at once so what we'll do
94:59 - is take a very simple example and step
95:02 - through it just to illustrate how a
95:03 - couple of these pieces work
95:06 - it's admittedly an overly simplistic
95:08 - example
95:09 - and feel free to poke holes at it later
95:11 - when you get to that point then you know
95:13 - you're ready to move on to the next
95:14 - level of material
95:16 - so
95:16 - we are now in the process of writing our
95:19 - children's book
95:21 - and for the purposes of demonstration
95:24 - we'll assume that this lstm has been
95:27 - trained
95:28 - on our children's books examples that we
95:31 - want to mimic
95:33 - and all of the appropriate votes and
95:36 - weights in those neural networks have
95:37 - been learned
95:39 - now we'll show it in
95:40 - action so so far our story so far is
95:45 - jane saw spot period
95:47 - doug
95:48 - so doug is the most recent word that's
95:51 - occurred in our story
95:53 - and also not surprisingly for this time
95:55 - step
95:57 - the names doug jane and spot were all
95:59 - predicted as viable options
96:03 - this makes sense we just wrapped up a
96:05 - sentence with a period the new sentence
96:07 - can start with any name so these are all
96:08 - great predictions so we have our
96:11 - new information
96:13 - which is the word doug
96:15 - we have our recent prediction which is
96:17 - doug jane and spot
96:19 - and we pass these two vectors together
96:22 - to all four of our neural networks which
96:25 - are learning to make predictions
96:28 - to do it ignoring to do forgetting and
96:31 - to do selection
96:34 - so
96:36 - the first one of these
96:38 - makes some predictions
96:40 - given that the word doug just occurred
96:43 - this is learned that the word saw
96:46 - is a great
96:47 - guess to make for a next word
96:50 - but it's also learned that having seen
96:53 - the word doug
96:55 - that it should not see the word doug
96:57 - again very soon seeing the word doug at
97:00 - the beginning of a sentence so it makes
97:02 - a positive prediction for saul and a
97:04 - negative prediction for doug it says i
97:06 - do not expect to see doug in the near
97:08 - future
97:09 - so that's why doug is in black
97:12 - so this example is so simple we don't
97:14 - need to focus on attention or ignoring
97:16 - so we'll skip over it for now
97:18 - and this prediction of saw
97:21 - not doug is passed forward
97:24 - and again for the purposes of simplicity
97:27 - let's say there's no memory at the
97:28 - moment so
97:30 - saw and doug get passed forward
97:33 - and then the selection mechanism here
97:36 - has learned
97:37 - that when the most recent word was a
97:40 - name
97:41 - then what comes next is either going to
97:43 - be
97:44 - the word saw or a period so it blocks
97:47 - any other names from coming out so the
97:49 - fact that there's a vote for not doug
97:51 - gets blocked here and the word saw
97:55 - gets sent out
97:56 - as the prediction for the next time step
98:00 - so we take a step forward in time now
98:04 - the word saw is our most recent word and
98:06 - our most recent prediction they get
98:08 - passed forward to all of these neural
98:10 - networks and we get a new set of
98:12 - predictions
98:13 - because the word saw just occurred
98:16 - we now predict that the words doug jane
98:19 - or spot might come next
98:23 - we'll pass over ignoring and attention
98:25 - in this example and we'll take those
98:27 - predictions forward
98:29 - now the other thing that happened
98:31 - is our previous set of
98:34 - possibilities
98:35 - the word saw and not doug that we were
98:38 - maintaining internally
98:40 - get passed to a forgetting gate
98:43 - now the forgetting gate
98:45 - says hey my last word
98:47 - that came uh that occurred was the word
98:50 - saw
98:51 - based on my past experience then i for
98:53 - can forget about you know i know that it
98:55 - occurred i can forget that it happened
98:57 - but i want to keep
98:59 - any predictions having to do with names
99:02 - so it forgets saw
99:04 - holds on to the vote for not doug
99:07 - and now at this
99:10 - element by element addition we have a
99:11 - positive vote for doug a negative o for
99:14 - doug and so they cancel each other out
99:16 - so now we just have votes for jane and
99:19 - spot
99:21 - those get passed forward
99:23 - our selection gate
99:25 - it knows that the word saw just occurred
99:28 - and based on experience a name will
99:30 - happen next and so it passes through
99:32 - these predictions for names and for the
99:35 - next time step then we get predictions
99:38 - of only jane and spot
99:41 - not doug
99:42 - this avoids the doug saw doug period
99:46 - type of error and the other errors that
99:48 - we saw
99:51 - what this shows
99:52 - is that long short-term memory can look
99:55 - back
99:56 - two three many time steps
99:59 - and use that information to make good
100:02 - predictions about what's going to happen
100:04 - next
100:05 - now to be fair to vanilla recurrent
100:07 - neural networks they can actually look
100:09 - back several time steps as well but not
100:12 - very many
100:14 - lstm can look back many time steps and
100:17 - has shown that successfully
100:22 - this is really useful in
100:24 - some surprisingly practical applications
100:28 - if i have text in one language and i
100:30 - want to translate it to text to another
100:32 - language lstms work very well
100:36 - even
100:37 - though translation is not a word to word
100:41 - process
100:42 - it's a phrase to phrase or even in some
100:44 - cases a sentence to sentence process
100:46 - lstms are able to represent those
100:50 - grammar structures that are specific to
100:52 - each language and
100:55 - what it looks like is that they
100:58 - find the higher level idea and translate
101:00 - it from one mode of expression to
101:02 - another just using the bits and pieces
101:05 - that we just walked through
101:08 - another thing that they do well is
101:10 - translating speech to text
101:13 - speech is just
101:14 - some signals that vary in time
101:17 - it takes them and uses that then to
101:19 - predict
101:20 - what text what word is being spoken and
101:23 - it can use
101:25 - the history the recent history of words
101:28 - to make a better guess for what's going
101:29 - to come next
101:32 - lstms are a great fit
101:34 - for
101:35 - any information that's embedded in time
101:38 - audio
101:40 - video
101:41 - my favorite application of all of course
101:42 - is robotics
101:44 - robotics is nothing more than
101:47 - uh an agent taking in information from a
101:50 - set of sensors
101:51 - and then based on that information
101:54 - making a decision
101:56 - and carrying out an action
101:58 - it's inherently sequential and actions
102:01 - taken now can influence what is sensed
102:04 - and what should be done many times steps
102:07 - down the line
102:11 - if you're curious what lstms look like
102:14 - in math
102:15 - this is it this is lifted straight from
102:17 - the wikipedia page
102:19 - i won't step through it but it's
102:21 - encouraging that something that looks so
102:23 - complex
102:24 - expressed mathematically
102:26 - actually makes a fairly straightforward
102:28 - picture and story
102:30 - and if you'd like to dig into it more i
102:33 - encourage you to go to the wikipedia
102:35 - page
102:35 - also there are a collection of really
102:38 - good
102:38 - tutorials and discussions
102:41 - other ways of explaining lstms that you
102:44 - may find helpful as well
102:46 - i'd also strongly encourage you to visit
102:49 - andre carpathi's blog post showing
102:51 - examples of what lstms can do in text
102:55 - you can be forgiven if when reading on
102:58 - the internet you can substitute magic
103:00 - for deep learning and it fits perfectly
103:03 - in all of the articles
103:05 - it's hard to know what it can't do we
103:07 - don't get to talk about that very much
103:09 - so the goal of this talk is just to talk
103:11 - about a really simple nuts and bolts
103:13 - level
103:14 - the summary case you want to take a nap
103:17 - deep learning is not magic
103:20 - but it's really good at finding patterns
103:23 - so
103:24 - if this
103:25 - is our brain
103:28 - this is deep learning
103:30 - an owl can fly
103:32 - uh a fighter jet can fly
103:35 - but there are a whole lot of things that
103:37 - an owl can do
103:39 - and arguably it's a much more complex
103:41 - thing
103:42 - although what the fighter jet does it
103:44 - does really really well
103:47 - so deep learning is the fighter jet
103:49 - highly
103:50 - specialized highly
103:52 - engineered
103:56 - we're today going to talk about the
103:57 - basics
103:59 - the wright brothers airplane
104:01 - if you understand the principles that it
104:03 - works on then you can see easy to branch
104:06 - out into the finer engineering details
104:09 - but there's a whole lot of things that
104:10 - go on into fighter jet that we're not
104:11 - going to talk about in detail
104:14 - but this is nice we can talk about this
104:16 - at a comfortable level
104:19 - this is a neuron
104:20 - like all neurons it's got a big body in
104:23 - the middle a long tail and some arms
104:25 - that branch off
104:27 - here's an artist's conception of a
104:29 - neural network or a bunch of neurons
104:31 - again
104:32 - big body long tails
104:35 - arms
104:37 - this is an actual picture of neurons in
104:40 - some brain tissue
104:42 - here the bodies look like dots or blobs
104:45 - you can see long tails some of which
104:46 - branch and the arms are pretty much
104:48 - invisible
104:51 - and again a picture of some brain tissue
104:54 - here the neurons are small dots and you
104:56 - can barely see any of the tails at all
104:59 - this is just to give you a sense of how
105:01 - tightly these things are packed together
105:03 - and how many of them there are
105:07 - big numbers with lots of zeros and the
105:09 - crazy part is that a lot of them are
105:11 - connected to many more of their
105:14 - neighbors this
105:16 - is one of our very first pictures of a
105:19 - neuron
105:20 - santiago ramona kahal
105:22 - found a stain that he could introduce
105:24 - into a cell turn the whole thing dark
105:27 - under his
105:29 - 19th century
105:31 - light microscope was able to see this
105:34 - and then draw it with a pen and paper
105:36 - this is old school
105:38 - what you see here though
105:39 - bodies
105:40 - long tails
105:42 - lots of arms
105:44 - um these have we're going to turn it
105:46 - upside down because that's how they're
105:47 - typically represented in neural networks
105:49 - and these these pieces actually have
105:51 - names the bodies are called the soma
105:54 - the long tails are called axons
105:57 - and the arms are called dendrites
106:00 - we're going to draw a cartoon version of
106:02 - them
106:03 - this is what they look like in uh
106:05 - in powerpoint
106:08 - now the way that neurons work
106:10 - is that a dendrite you can think of it
106:12 - as feelers or whiskers and they look for
106:14 - electrical activity they pick it up and
106:16 - send it to the body
106:19 - the soma takes this and adds it together
106:21 - and accumulates it and then depending on
106:23 - how fast it's accumulating it it will
106:25 - activate the axon and send that signal
106:28 - along
106:29 - down the tail
106:32 - the more dendrite activity there is
106:35 - the more
106:36 - axonal activity there is
106:38 - and if you get all of the dendrites
106:40 - really firing then that axon is just as
106:42 - active as it can possibly be
106:44 - in a very simplistic way
106:47 - a neuron adds things up
106:52 - now a synapse is where the axon from one
106:56 - neuron touches the dendrite of another
106:58 - that's an artist's conception of it
107:01 - you can see in ramona tahoe's drawings
107:03 - these little
107:04 - nubs or buttons
107:06 - they're actually called boutons on
107:09 - the dendrites
107:11 - and these are sites
107:13 - where the axon of another neuron touches
107:16 - that
107:18 - so you can imagine there's a little
107:19 - connection there
107:22 - will represent that connection by a
107:23 - circle and the diameter of that circle
107:25 - is the strength of that connection big
107:27 - circle strong connection
107:29 - and it can connect strongly or weakly or
107:31 - somewhere in between
107:34 - we can put a number on this connection
107:35 - between zero and one
107:38 - so a medium connection
107:40 - we'll uh call it a 0.6
107:42 - when the axon of the input neuron
107:45 - the upstream neuron
107:47 - is active
107:48 - then it
107:49 - activates the dendrite of the output
107:52 - neuron
107:53 - it passes that on with a modest strength
107:56 - if that connection is strong
107:58 - then it passes the signal on very
108:00 - strongly that connection is a one then
108:03 - when the axon is active the next
108:04 - dendrite is very active
108:07 - likewise if that connection is very weak
108:09 - say a 0.2
108:11 - then when the axon is active the
108:13 - dendrite of the output neuron is only
108:15 - weakly activated
108:18 - no connection at all is a zero
108:22 - now this starts to get interesting
108:25 - because many different input neurons can
108:28 - connect to the dendrites of a single
108:30 - output neuron
108:31 - and each connection has its own strength
108:36 - we can redraw this by taking out all of
108:38 - the parallel dendrites and just drawing
108:39 - each axon and the single dendrite that
108:41 - it connects to and the connection
108:43 - strength represent like this with the
108:45 - dots
108:47 - we can substitute in numbers for that
108:50 - the weights
108:51 - uh although most often
108:53 - uh oh we can also separate substitute in
108:56 - line thicknesses to show how strongly
108:58 - these things are connected most of the
109:00 - time neural networks are drawn like this
109:03 - and this is what we have we went from
109:05 - the
109:06 - super complex slice of brain tissue
109:10 - with many subtleties in its operation
109:12 - and interconnection
109:13 - to
109:14 - a nice circle stick diagram
109:17 - where each one of those sticks
109:18 - represents a weight
109:22 - in its current form
109:23 - it can still do some pretty cool things
109:26 - the input neurons input neurons can
109:29 - connect to many output neurons
109:32 - so actually what you get here is many
109:34 - inputs many outputs
109:36 - and the connection between each is
109:38 - distinct and has its own weight
109:42 - this is uh it's good for making pretty
109:44 - pictures it's also great for
109:46 - representing combinations of things
109:49 - and the way this is done
109:51 - let's say you have five inputs
109:53 - labeled a b c e
109:57 - in this case this output neuron has a
109:59 - strong connection to a
110:02 - c and e
110:03 - very weak connections to b and d
110:06 - that means when the a c and e input
110:09 - neurons are active
110:11 - all together that really strongly
110:13 - activates the output neuron
110:16 - b and d don't matter because they only
110:18 - connect weakly
110:20 - so a way to think about this output
110:22 - neuron is in terms of the inputs that's
110:24 - strongly activated so we call this the
110:26 - ace neuron
110:29 - and here we have
110:31 - an atomic example of
110:33 - what happens here this output neuron
110:35 - represents a combination of the input
110:38 - neurons
110:40 - this is neural networks in a nutshell
110:44 - you can do this with any kind of input
110:47 - so you have a really low tech 4 pixel
110:49 - camera
110:50 - each of those four inputs is uh one of
110:54 - the pixels upper left lower left
110:56 - upper right or lower right
110:59 - in this particular neural network with
111:02 - strong connections to the upper left and
111:04 - upper right pixel
111:05 - we have a neuron and output neuron that
111:07 - represents this bar in the top half of
111:10 - the image
111:12 - so we can combine letters we can combine
111:15 - pixels to make
111:17 - small images
111:19 - if you're doing text processing the
111:21 - input neurons can represent individual
111:23 - words
111:25 - so in this case
111:26 - we're pulling words out of text
111:29 - this output neuron is strongly connected
111:31 - to the anterior neurons for eye and ball
111:34 - so we can call it the eyeball neuron
111:37 - similarly we can have a sunglasses
111:39 - neuron
111:40 - and infant neurons can connect to many
111:42 - outputs we could have an eyeglasses
111:44 - neuron just as easily
111:50 - so going a little deeper into this this
111:53 - is a somewhat trivial example to show
111:56 - how these things work in practice
112:00 - so there's a guy at the shawarma place
112:03 - and makes shawarma like nobody else and
112:05 - so you want to make sure and go when
112:07 - he's working there
112:09 - and
112:10 - taking a step back we actually have some
112:11 - domain knowledge here we know he's got
112:13 - two schedules
112:15 - working in the morning
112:17 - off in the evening and off in the
112:19 - morning working in the evening
112:22 - now if we were to instrument this with
112:23 - sensors
112:25 - we would have the working in the morning
112:28 - off in the morning
112:29 - working in the evening
112:31 - off in the evening
112:33 - and it might be useful to represent
112:36 - his working patterns in terms of a
112:38 - couple of output neurons that combine
112:40 - those
112:43 - so this is the network that we would
112:44 - expect to end up with
112:46 - working in the morning off in the
112:48 - evening is one pattern
112:50 - off in the morning working in the
112:51 - evening is the other path and you can
112:54 - see based on their connection strengths
112:56 - how they combine those inputs
113:00 - here would be the weights associated
113:02 - with those
113:03 - now the question is how do we learn this
113:06 - if we have to go in and fill it all in
113:08 - by hand we haven't learned anything it's
113:10 - just a fancier way of programming
113:13 - and a lot of
113:14 - hard work especially if you're dealing
113:16 - with many millions of input neurons
113:19 - so we want to learn this automatically
113:23 - so to start with might be a little
113:24 - counter-intuitive we create our neural
113:26 - network
113:28 - we have our input neurons
113:29 - all we choose is the number of output
113:32 - neurons in this case we'll choose two
113:34 - because we happen to know we're learning
113:35 - two patterns
113:37 - and then we randomly assign weights
113:40 - randomly number generate numbers for
113:42 - each of these it's a neural network this
113:44 - completely you roll the dice you throw
113:45 - the sticks and whatever falls out that's
113:48 - what you start with
113:52 - and then we start to gather data we go
113:54 - stand on the other side of the street
113:56 - and we observe that the shawarma guy on
113:58 - this particular day worked in the
114:00 - morning
114:01 - and then went home did not work in the
114:03 - evening
114:04 - that means
114:06 - this input
114:07 - is active we'll say it's at a level one
114:11 - often the morning is at a level zero
114:12 - because we didn't observe it
114:14 - working in the morning is at zero
114:17 - and
114:18 - off in the evening is out of one because
114:20 - we observed that too
114:23 - so the next step is for each of the
114:26 - output neurons we calculate the activity
114:30 - so in this case
114:32 - an appropriately simple way to do this
114:34 - is just take the average of the inputs
114:37 - so here
114:38 - this weight is 0.3 and this weight is
114:40 - 0.1
114:41 - so the average of those is 0.2
114:44 - these neurons don't contribute anything
114:46 - because those inputs aren't active
114:49 - similarly we can take the weight between
114:52 - this input and that output 0.8
114:55 - and 0.4
114:56 - the average of those is 0.6
114:59 - the upper neuron on the right has a
115:01 - higher activity that's the one we care
115:03 - about we ignore all the others there's a
115:05 - million others we ignore the rest and
115:07 - focus on this one for this time step
115:12 - first thing we figure out is how wrong
115:13 - is it
115:14 - well if it was perfect if our neural
115:16 - network was perfect that would have an
115:18 - activity of one it would be uh perfectly
115:20 - aligned with our inputs but it only has
115:23 - an activity of 0.6 so the error is 0.4
115:26 - the bigger that error is that's a signal
115:29 - for how much we need to adjust our
115:31 - weights when that error gets very small
115:34 - it means that the weights really
115:35 - represent what's going on and we don't
115:37 - need to make any more changes
115:42 - now the trick here
115:44 - gradient descent if there is an element
115:47 - of magic
115:48 - in deep learning it's gradient descent
115:52 - what you do
115:53 - is you go through and adjust each of
115:55 - these weights
115:57 - all through
115:59 - you adjust it a little bit up and a
116:00 - little bit down and see which way
116:02 - decreases the error
116:05 - the idea the concept in gradient descent
116:09 - is
116:10 - weight is a quantity that you can shift
116:12 - a little bit side to side
116:15 - as you do this error will change you can
116:18 - think of it as taking this ball
116:20 - if you shift it a little bit to the left
116:22 - it has to climb the hill if you shift it
116:24 - a little to the right it has to fall
116:26 - down the hill and you like the direction
116:29 - you choose the direction in which it
116:30 - gets lower you want to bring that error
116:32 - down as low as it can get
116:35 - and you take small incremental steps to
116:37 - keep everything numerically stable
116:41 - so we go through and we do these for all
116:43 - of the neurons uh sorry for all of the
116:45 - weights that attach input neurons to our
116:48 - output
116:49 - and we find that yes we want to increase
116:51 - this one
116:53 - because these aren't active we actually
116:55 - have a bias toward low weights so it
116:58 - doesn't hurt to decreasing so we'll go
117:00 - ahead and decrease that weight and
117:02 - decrease that weight and increase that
117:04 - one
117:05 - when we do that sure enough our new
117:07 - activity is seven and so our error went
117:10 - from a point four to a point three it's
117:12 - a little bit better at representing what
117:15 - we saw
117:17 - so that was one data point
117:19 - we go back and we do the same thing the
117:21 - next day
117:22 - it just so happens that this day
117:24 - he's off in the morning working in the
117:26 - evening
117:27 - we adjust the weights and we do that day
117:30 - after day after day
117:32 - and eventually the weights will stop
117:34 - changing
117:35 - or they'll slow down changing quite a
117:37 - bit they'll get stable
117:39 - and we get
117:40 - the system of weights that we originally
117:43 - saw because we had knowledge of the
117:44 - problem
117:46 - so this is the
117:48 - wright brothers airplane version
117:50 - of how
117:51 - training by back propagation using
117:54 - gradient descent works back propagation
117:56 - is a very specific way to do this that
117:58 - is computationally cheap and slick and
118:01 - fast and you get your your jet engine
118:03 - instead of the flapping of wings so
118:07 - this is the underlying mechanism by
118:10 - which it works
118:13 - so what we just looked at was a single
118:15 - layer we have inputs we have outputs
118:18 - every output is a combination of things
118:21 - that were on the previous layer
118:23 - now there's no reason
118:25 - then that we can't turn around and take
118:27 - that output layer and make it inputs for
118:30 - the next layer
118:31 - and do that again
118:32 - and again
118:34 - if a network has more than three layers
118:36 - or so we call it deep
118:39 - uh some have more than 12.
118:42 - in some recent research in microsoft
118:44 - their deep neural networks would lay
118:45 - more than a thousand dollars there's no
118:48 - theoretical reason to limit the number
118:51 - of layers that you have
118:52 - it just depends on the specifics of your
118:54 - problem
118:56 - now
118:57 - what does deep get you why is deep
119:00 - special
119:02 - if your input neurons are say letters in
119:05 - the alphabet
119:07 - your first layer outputs
119:09 - sorry this is a deep neural network with
119:11 - all of the connections emitted for
119:13 - clarity
119:14 - so these are your inputs this is your
119:16 - first layer of outputs
119:18 - they're combinations
119:19 - of those letters
119:22 - each level you go up you get
119:24 - combinations of what happened the level
119:26 - before so by the time you get to your
119:28 - second level of outputs you're getting
119:30 - perhaps words
119:31 - in the english language if that's what
119:33 - you're training on
119:35 - the layer above that you get
119:36 - combinations of words
119:38 - short phrases
119:39 - and so forth and you can do this as deep
119:42 - as you like
119:45 - so there's a variety of things you can
119:47 - learn with deep neural networks a very
119:50 - popular one is images
119:53 - if you take as your inputs pixels
119:56 - and show instead of looking at uh
119:59 - shawarma guy schedule you're looking at
120:02 - individual pictures as your training
120:03 - data set
120:05 - what you start to learn after a while is
120:07 - these little representations of short
120:09 - lines and dots and patches
120:13 - these are the primitives of an image
120:15 - if you train on image of faces
120:18 - then your first
120:19 - layer output sorry your second layer
120:21 - outputs start to look like eyes and
120:24 - noses and mouths and chins and your
120:27 - third layer output start to look clearly
120:29 - recognizable as faces
120:33 - similarly if you train on automobiles
120:35 - your second layer outputs start to look
120:37 - like wheels and doors and windows and
120:40 - your third layer output
120:42 - look like automobiles
120:45 - so that's pretty cool we didn't have to
120:46 - go in and twiddle any of those weights
120:48 - this just learned that from seeing a
120:50 - bunch of pictures
120:52 - you can do it on color images too here's
120:54 - an uh output of an eight some of the
120:57 - output neurons of an eight layer neural
120:59 - network and as you get deeper you can
121:01 - see things that are clearly recognizable
121:03 - and quite complex you get spiders and
121:06 - rocking chairs and sailing and chips and
121:08 - teddy bears
121:11 - you can also plug information in about
121:14 - music artists
121:16 - so here's some research where
121:19 - output neurons were learned based on
121:21 - information about artists and then their
121:23 - uh
121:24 - representation was plotted based on how
121:26 - similar they were in that in the those
121:29 - output neurons
121:30 - and so we see things like
121:33 - kelly clarkson
121:34 - and beyonce are similar over here which
121:38 - is also not too far from taylor swift
121:40 - and avril lavigne
121:42 - whereas if we go up here we get weezer
121:45 - flat keys modest mouse presidents of the
121:47 - united states of america all in the same
121:49 - neighborhood
121:51 - this is a network that didn't know
121:53 - anything still doesn't know anything
121:55 - about music
121:56 - but
121:57 - because of the data that it gets on the
121:59 - input neurons it's able to group these
122:01 - things appropriately it finds patterns
122:04 - and then finds things that most closely
122:06 - fits those patterns
122:09 - turns out you can take atari 2600 games
122:12 - take the pixel representations feed
122:14 - those in as input neurons learn some fun
122:17 - features and then pair it with something
122:18 - else called reinforcement learning that
122:21 - learns appropriate actions
122:23 - and when you do this for a certain class
122:24 - of games it can learn to play them far
122:27 - better than any human player ever has or
122:29 - is ever likely to
122:33 - and it turns out that you can take a
122:35 - robot and let it watch youtube videos
122:38 - about how to cook
122:40 - and it loses uses a pair of deep neural
122:42 - networks one to interpret the video one
122:45 - to learn to understand its own movements
122:48 - and then uses those pairs that with some
122:50 - other execution software to cook
122:53 - based on the video representations that
122:55 - it's seen
122:57 - so
122:59 - while it's not magic it's pretty cool
123:02 - you can do some stuff
123:06 - so as you're going through reading
123:08 - literature reading popular articles
123:10 - about this you can kind of play bingo
123:11 - there are some buzzwords some popular
123:13 - algorithms you can think of a lot of
123:15 - these as the you know model numbers for
123:17 - the various fighter jets that are out
123:19 - there
123:20 - but when you see any of these terms if
123:22 - you like you can mentally do the
123:24 - substitution of
123:26 - deep learning and apply what you know
123:29 - about the wright brothers airplane and
123:30 - most of it will still be accurate
123:34 - so the bottom line it's good at learning
123:36 - patterns it doesn't do anything but it's
123:37 - pretty good at learning patterns
123:40 - i'm excited to get to talk about two of
123:42 - my favorite topics at once
123:44 - machine intelligence and robots
123:47 - they go together pretty well but they're
123:49 - not the same thing you can definitely
123:51 - have one without the other
123:53 - first some disclaimers i'm not going to
123:56 - give you the answer to human level
123:57 - intelligence
123:59 - i would if i had it but i don't
124:02 - next these are my own personal opinions
124:04 - they're definitely not those of any
124:06 - current or former employer
124:08 - and they don't reflect those of many
124:09 - experts in the field
124:11 - take them with a huge grain of salt
124:15 - if they are useful you're welcome to
124:17 - them
124:18 - and if they're not please discard them
124:21 - also
124:22 - this story that i'm going to tell you
124:25 - is not rigorous
124:27 - it doesn't have any equations it's
124:29 - conceptual
124:30 - and i just intended to start a
124:33 - discussion and foster
124:36 - ideas throughout this presentation we'll
124:39 - be talking about intelligence
124:41 - the working and definition that i
124:43 - propose is that it's a combination of
124:45 - how much one can do and how well one can
124:47 - do it
124:49 - notionally you could be extremely good
124:52 - at only one thing and not be that
124:54 - intelligent
124:56 - also you could do many things but do
124:58 - them all very poorly and also not be
125:00 - that intelligent
125:02 - intelligence is the combination of being
125:04 - able to do many things and to do them
125:06 - well
125:08 - this is a functional definition of
125:10 - intelligence
125:12 - there are many other potential
125:14 - definitions some of them can be measured
125:16 - experimentally and some can't
125:19 - particular definition has the advantage
125:22 - that if we wanted to reduce it down to a
125:24 - measurable set of tasks we could
125:28 - because it is at least in theory
125:30 - observable this allows us to have a
125:32 - scientific discussion about machine
125:34 - level intelligence
125:36 - it lets us form hypotheses that we could
125:38 - potentially falsify
125:41 - and it lets us compare the relative
125:42 - intelligence of two separate agents
125:45 - in short it's practical and useful
125:49 - undoubtedly some will find it
125:50 - philosophically unsatisfying
125:53 - that's a separate conversation but one
125:55 - that i would be happy to have in another
125:56 - forum
125:59 - using fake math we can say that
126:01 - intelligence equals performance times
126:03 - generality
126:05 - it's only fake because we haven't
126:07 - defined performance or generality yet
126:10 - but assuming we do you can imagine
126:12 - plotting them
126:13 - and putting them on a set of axes like
126:15 - this
126:17 - although
126:18 - we haven't defined it quantitatively for
126:20 - human performance
126:22 - what i would like to propose
126:24 - is that
126:26 - human performance is the level at which
126:28 - a human expert can do something
126:32 - this can be measured in lots of
126:34 - different ways
126:35 - it can be
126:37 - error rates
126:38 - or
126:39 - the time required to execute a task
126:42 - the time required to learn a task
126:45 - the number of demonstrations required
126:47 - before one learns a task
126:49 - the amount of energy
126:51 - expended when one performs a task
126:55 - the subjective judgment of performance
126:56 - from a panel of human judges
126:59 - the speed that someone does something
127:02 - there are many aspects of performance
127:04 - and i'm not going to try and specify or
127:06 - quantify them all here i only list them
127:08 - to illustrate that we are considering
127:10 - performance in a broad sense
127:22 - we're considering performance in a broad
127:24 - sense rather than in a narrow machine
127:27 - learning accuracy leaderboard sense
127:32 - if we consider human level performance
127:34 - to be something of a baseline we can
127:36 - place it on our x-axis and then chop up
127:39 - the rest of the axes in equal increments
127:41 - we'll make this a logarithmic scale to
127:44 - enable us to compare a very wide range
127:45 - of performances
127:47 - equal steps along this axis represent
127:50 - equal multiplicative factors of change
127:57 - human level generality is the set of all
127:59 - the tasks that humans can do
128:01 - and have undertaken
128:03 - these include things like
128:05 - writing stories cooking pies
128:08 - building cities
128:10 - gathering and transmitting information
128:12 - all around the world
128:14 - and even exploring the origin of the
128:16 - universe
128:18 - it's a very broad set of activities
128:22 - we can represent human level generality
128:25 - on the y-axis
128:28 - roughly this is the set of all tasks
128:30 - that a human or group of humans can do
128:34 - we'll make the y-axis logarithmic as
128:36 - well
128:37 - so an equal interval is a factor of 10
128:39 - in performance either
128:41 - multiplied or divide depending on
128:43 - whether you're moving up or down
128:47 - so human intelligence can be notionally
128:49 - represented by the area
128:51 - that's entailed
128:53 - by this point
128:57 - just want to point out there's no reason
128:59 - to believe that machines might not
129:01 - exceed human performance in some areas
129:04 - humans have a number of limitations that
129:07 - are built into the way in which we've
129:09 - achieved our intelligence through
129:10 - evolution
129:12 - things that may have been very useful at
129:14 - one point or may be useful broadly but
129:16 - now may not be useful in pushing the
129:18 - limits of intelligence
129:20 - things like
129:22 - limited attention
129:23 - instinctive drives
129:26 - how every part of us fatigues
129:29 - and a host of cognitive biases all of
129:32 - which put some distance between us
129:34 - and perfectly rational or perfectly
129:36 - efficient or optimal behavior
129:43 - machines by comparison have a more
129:45 - nurturing environment in which to take
129:47 - root
129:48 - they don't have to evolve we're trying
129:50 - everything we can to encourage them
129:54 - now you can imagine
129:56 - on the performance generality axis
129:59 - another agent
130:00 - that
130:01 - can do a much larger set of tasks than
130:04 - humans although do them all
130:07 - more poorly than a human could so it
130:10 - might look like this
130:11 - the area under that triangle the overall
130:14 - intelligence would still be comparable
130:16 - to that of a human
130:17 - so we would call that human level
130:19 - intelligence also
130:21 - you can also imagine an agent who can do
130:25 - a subset of the tasks that humans do but
130:27 - do them so very much better that the
130:30 - area under that rectangle is also about
130:33 - the same as the humans
130:36 - so again human level intelligence
130:40 - now if we take the set of all of these
130:42 - agents that have about the same area
130:45 - under their intelligence rectangle
130:48 - then we get this curve representing
130:50 - human level intelligence
130:52 - any agent who falls along that curve
130:55 - would be comparable to a human any agent
130:58 - down and to the left of it subhuman
131:00 - intelligence and any agent up and to the
131:03 - right
131:04 - superhuman intelligence
131:08 - now let's look at a few agents that you
131:09 - might be familiar with and see where
131:11 - they fall in this scheme
131:13 - chest pain
131:14 - chest plane computers have been around
131:16 - for going on 30 years now um which is a
131:19 - world champion chess playing computers
131:21 - have been around for almost 30 years
131:23 - ibm's deep blue beat gary kasparov in
131:26 - 1989
131:28 - this was a task that people assumed hugh
131:31 - computers would have a very tough time
131:33 - with
131:34 - it involved planning
131:36 - strategy thinking mental models of your
131:39 - opponent
131:41 - it seemed to take in the very peak of
131:44 - human cognition
131:46 - and it seemed unlikely that such a thing
131:48 - could be done
131:50 - by a fancy calculator but
131:53 - it did
131:54 - and now a chess program running on your
131:56 - phone can do about the same as deep blue
131:58 - did
132:00 - the current state of the art is a
132:01 - program called stockfish which has an
132:04 - elo rating which is like a chess skill
132:07 - score
132:08 - of 34 47.
132:10 - compare this to the top human
132:13 - uh top-rated human player of all time
132:14 - magnus carlsen
132:16 - who achieved a 2882
132:20 - the program and the human are not even
132:22 - comparable they're not even close
132:26 - it's worth noting that stockfish is an
132:27 - open source project with code freely
132:30 - available and has a number of
132:31 - contributors scattered around the globe
132:36 - now in terms of generality
132:39 - stockfish
132:40 - understands the rules of chess and in
132:42 - fact it understands them very well
132:45 - it has a bunch of hints and tips and
132:48 - tricks built in by humans that are
132:50 - specific to chess
132:53 - it uses a point system to evaluate
132:56 - pieces
132:57 - based on where they are and what the
132:58 - stage of the game is
133:01 - it uses a full table of end games once
133:04 - there are just a few pieces left on the
133:05 - board
133:06 - the number of possibilities for how the
133:08 - game will play out are few enough that
133:10 - they can be completely enumerated
133:12 - so there's no search there's no solving
133:14 - there's just essentially looking up in a
133:16 - giant table and figuring out what to do
133:19 - next
133:20 - there are hand-tuned strategies for each
133:22 - phase of the game
133:24 - and then what the computer does is
133:26 - uses tree search to evaluate future
133:29 - moves
133:30 - each choice of move is a branch on this
133:33 - tree and it can look and say for each
133:36 - move what's the likely outcome
133:39 - for each of those outcomes what are the
133:41 - possible moves my opponent can make for
133:43 - each of those outcomes then what are my
133:45 - possible responses
133:47 - and by fully going down this branching
133:50 - tree and looking at all the
133:51 - possibilities
133:53 - the program can then figure out
133:56 - what its best choices now one of the
133:58 - things that makes stockfish so clever
134:01 - and good at its games it's very good at
134:03 - pruning this tree and ignoring moves
134:05 - that are unlikely to lead to any good
134:07 - end and not exploring them very far
134:11 - but all told
134:13 - this
134:14 - program is pretty useless at anything
134:17 - that is not chess
134:19 - it is the opposite of general
134:22 - so on our plot
134:24 - we would put chess well above human
134:26 - level performance
134:28 - but also very low on the generality axis
134:32 - now compare that to uh
134:35 - go
134:36 - also a board game
134:38 - if you're not familiar with it they're
134:40 - 19 by 19 grid and opponents take turns
134:43 - placing white and black stones
134:45 - the rules are actually in some ways
134:48 - simpler than chess at each turn you pick
134:51 - a junction on which to place a stone
134:55 - now the strategy however uh some argue
134:58 - is even
134:59 - more complex and more importantly more
135:01 - subtle
135:04 - it's what is undoubtedly true is there
135:06 - are more possible board configurations
135:09 - where chess has an eight by eight board
135:12 - go has a 19 by 19 board
135:14 - where each piece in chess has a small
135:16 - prescribed number of motions
135:19 - any stone and go can be placed on any
135:22 - open junction
135:24 - so when doing a tree search
135:27 - the number of moves
135:30 - explodes
135:31 - much more quickly than in chess
135:34 - now despite this two years ago already
135:38 - alphago
135:39 - a program
135:41 - built by researchers at deepmind
135:44 - beat
135:45 - lee sidol a professional nine dan player
135:49 - professional nine dan would put him in
135:52 - the all-stars of the go world
135:57 - a later version alphago master was
136:00 - clocked with an elo rating of 48.58
136:03 - compare this to the highest rated human
136:05 - player park jung-won who's rated at
136:08 - 66-69
136:10 - so
136:11 - not only has go beaten the best players
136:13 - of the world but now already by a very
136:16 - healthy margin
136:19 - now as we mentioned
136:20 - the program understands the rules of go
136:22 - or knows the rules of go
136:24 - and use the tree search to evaluate
136:26 - moves
136:28 - because there are so many possible board
136:30 - configurations however it can't memorize
136:32 - them all so it uses convolutional neural
136:35 - networks to learn common configurations
136:38 - and most importantly it can
136:41 - see patterns that are not exactly
136:43 - repeated but are similar to things that
136:45 - have seen before
136:46 - this is an important innovation
136:48 - and we'll come back to convolutional
136:50 - neural networks in a few minutes
136:54 - it also uses reinforcement learning on a
136:58 - library of human games to learn which
137:00 - moves are good
137:02 - reinforcement learning
137:04 - in
137:05 - very straightforward terms is
137:07 - looking at a configuration
137:10 - an action
137:12 - and the outcome
137:13 - and learning the pattern
137:15 - for a given configuration if i take
137:17 - action a good things happen if i take
137:19 - action b bad things tend to happen
137:22 - after i learn those patterns the next
137:24 - time i see configuration a
137:26 - i can take the action that leads to good
137:28 - things
137:30 - um so using reinforcement learning on a
137:33 - library of human games then kind of
137:35 - bootstrapped alphago and let it learn
137:38 - from the history of human play and human
137:41 - insights to get started
137:45 - but like stockfish it's useless at
137:47 - anything that's not go
137:48 - so while it has a few tricks that allow
137:51 - it to be more general it is still very
137:53 - narrow in its
137:54 - applications so on our plot we might put
137:57 - it here again far exceeding human level
138:00 - performance
138:01 - but still very
138:03 - low on the generality axis
138:08 - now let's take a jump to a different
138:10 - category entirely
138:12 - um
138:13 - image classification
138:15 - there is a wonderful data set called
138:17 - imagenet
138:18 - which has many thousands of images
138:20 - classified by hand by humans
138:25 - into a thousand pre-defined categories
138:29 - these categories include household items
138:32 - cars and doors and chairs includes
138:34 - animals giraffes rhinoceroses
138:37 - it also includes for instance
138:40 - many different species of dog
138:42 - so
138:43 - it's not a trivial thing to take an
138:45 - image and accurately categorize it and
138:48 - in fact a typical human score on this is
138:51 - about five percent error about one out
138:54 - of every 20 images a human will classify
138:56 - incorrectly
138:58 - so it's a tough
139:00 - task
139:02 - in 2011 uh there was began a large scale
139:05 - visual recognition challenge in which
139:08 - teams got to put together their
139:09 - algorithms for classifying these images
139:13 - and in 2011 the very best one got 26
139:16 - percent error so about one in every four
139:19 - image was wrongly classified still three
139:21 - out of every four was correct which was
139:23 - pretty amazing performance
139:27 - each year this error rate decreased by
139:30 - close to half
139:32 - which is an amazing rate of progress
139:36 - finally in 2015 the error rate got lower
139:39 - than human so we had a computer program
139:41 - classifying images better than a human
139:44 - does
139:46 - now in 2017 one of the more recent
139:49 - competitions more than half of the teams
139:51 - got fewer than five percent wrong
139:54 - so
139:55 - now machines are routinely beating
139:57 - humans at this task
139:59 - uh
139:59 - pretty impressive
140:02 - in terms of generality this task is
140:04 - definitely
140:06 - harder more general more challenging
140:09 - than a board game
140:11 - there are more variations more
140:13 - possibilities
140:15 - to get at it it uses convolutional
140:17 - neural networks
140:18 - which
140:19 - are a deep neural network architecture
140:23 - specifically designed for finding
140:25 - patterns
140:26 - in two-dimensional arrays of data
140:30 - like
140:31 - a pixels
140:33 - or
140:34 - squares on a chessboard
140:36 - or on a go board they're very good at
140:38 - finding patterns for that could be
140:40 - visually represented
140:44 - this is
140:45 - good
140:47 - but it has been shown to break easily
140:49 - outside of the set of images that you
140:51 - train it on
140:53 - to give an example of this if you look
140:55 - at the images on the right in the left
140:58 - hand column we see soap dispensers
141:01 - praying mantis a puppy
141:04 - these are all images that were correctly
141:06 - categorized by a convolutional neural
141:08 - network
141:10 - with the addition of a little bit of
141:12 - distortion shown in the middle column
141:14 - which you're seeing that correctly is
141:16 - just a little bit of noise but the gray
141:17 - has no change at all
141:19 - you get the images on the right
141:22 - visually to us
141:23 - they look identical or
141:26 - very similar you might be able to see a
141:28 - little bit of warping and distortion
141:29 - there
141:31 - but for whatever reason uh convolutional
141:33 - neural networks confidently predicted
141:36 - all of these to be ostriches
141:39 - so this is not to say that they are not
141:42 - powerful and good but they see something
141:45 - different than we are seeing they are
141:46 - not learning to see in the same way that
141:48 - we see
141:51 - since then the fragile nature of
141:52 - convolutional neural networks has been
141:55 - demonstrated through other ways
141:58 - in some images changing a single pixel
142:01 - the right pixel to the right value can
142:03 - change how that image is classified
142:06 - others have found that you don't even
142:08 - have to go into the digital domain you
142:10 - can take carefully designed stickers and
142:13 - affix them to something
142:15 - and have that object be confidently
142:17 - classified as a banana whatever it
142:20 - is and in my favorite demonstration
142:25 - a physical plastic turtle
142:28 - was rotated and from every direction the
142:30 - convolutional neural network confidently
142:33 - predicted that it was a turtle
142:35 - then after just repainting a different
142:37 - pattern
142:38 - not symbolic or representative of
142:40 - anything
142:42 - but carefully chosen that same
142:44 - convolutional neural network
142:46 - categorized it as a handgun
142:50 - these examples show
142:52 - that
142:53 - uh at least as currently done our image
142:56 - classification generality is not quite
142:59 - where we would like it to be
143:01 - so
143:03 - definitely higher than human performance
143:05 - but
143:06 - classifying imagenet is a much narrower
143:10 - task than it might appear on the surface
143:12 - so we'll put it pretty low on the
143:14 - generality axis
143:19 - now here's a really fun example of video
143:21 - game performance
143:23 - so again
143:24 - the folks at deepmind put together
143:27 - uh deep q learning or deep reinforcement
143:31 - learning architecture to play video
143:33 - games we'll talk about more about what
143:35 - that is in a second but what they did is
143:37 - they took 49 classic
143:40 - atari games
143:41 - and
143:42 - let the algorithm just look at the
143:44 - pixels and make random moves
143:48 - and the algorithm didn't know if this
143:50 - was supposed to be a move after a move
143:51 - right or a jump or a shoot it just took
143:54 - moves
143:54 - and then used reinforcement learning to
143:58 - learn from the outcome and then learn
144:00 - this pattern of oh when i see this and i
144:03 - do this
144:04 - either something good happens or
144:06 - something bad happens or something
144:07 - neutral happens
144:10 - after doing that for long enough
144:13 - it learned the patterns
144:15 - that let it choose the right thing to do
144:19 - and in 29 of these 49 games it did at or
144:25 - above
144:26 - human expert level play
144:29 - and that was super impressive so this is
144:31 - not just looking at a picture and saying
144:33 - this is a cat this is looking at a
144:35 - picture in the moment and saying for
144:36 - this particular instance the right thing
144:38 - to do is jump
144:41 - and then by jumping
144:43 - that changes the image and then having
144:45 - to respond to the new configuration and
144:47 - doing that again and again and again and
144:49 - doing this better than a human
144:54 - now
144:54 - the other part of this is there were 20
144:57 - games then at which it did more poorly
145:00 - than a human
145:02 - so after using convolutional neural
145:04 - networks to learn the pixel patterns for
145:07 - which this is perfectly suited because
145:09 - the pixels are big and coarse there's no
145:11 - noise they don't change
145:13 - the patterns are clear so what the
145:15 - algorithm is seeing is very close to
145:17 - what we as humans see
145:19 - and after uni using reinforcement
145:21 - learning to learn what actions to take
145:23 - in each situation
145:25 - 20 of these games it wasn't able to
145:26 - match human performance on
145:29 - and the pattern among those games is
145:31 - they tended to require longer term
145:33 - planning
145:35 - the one of them was ms pac-man and if
145:38 - you've ever played that you know you're
145:40 - trying to eat all the dots in a maze
145:42 - while avoiding ghosts who are chasing
145:44 - you
145:45 - it involves planning routes out several
145:48 - turns ahead anticipating where ghosts
145:51 - are going to be
145:52 - a lot of things that you can't get from
145:54 - a single snapshot
145:56 - and without thinking ahead several steps
146:01 - in its current state this algorithm
146:03 - didn't do that
146:04 - and in fact the game that did the
146:06 - poorest on a game called montezuma's
146:07 - revenge required much more extensive
146:10 - planning going to one location and
146:11 - grabbing an object so you could go to
146:13 - another location and open a door
146:15 - and the computer just was not able to
146:18 - make those connections
146:22 - so we'll add video games to our plot
146:24 - here
146:25 - again more general than image
146:26 - classification there's more going on the
146:28 - task is broader
146:30 - and performance is about human level
146:35 - now you may notice a pattern here these
146:38 - fit roughly into a line or a curve and
146:40 - we'll see this pattern continue
146:44 - looking at machine translation taking
146:47 - text and changing it from one language
146:49 - to another
146:50 - if you ever gone to an online translator
146:53 - and typed in a phrase or copied a phrase
146:56 - from a language you weren't familiar
146:57 - with to one that you were
147:00 - you will probably notice that the
147:02 - translation is surprisingly good at
147:05 - getting some of the sense
147:07 - which is
147:08 - even five years ago was complete science
147:10 - fiction to be able to do this in a
147:12 - reliable way
147:14 - you'll probably also
147:16 - notice that the result is nothing that a
147:18 - native language speaker would ever be
147:20 - likely to say
147:23 - so it's
147:24 - it's okay it's definitely in the right
147:26 - direction
147:27 - but it's far from
147:28 - perfect now what's really impressive to
147:32 - me about this is that the state of the
147:34 - art and language translation
147:36 - takes
147:37 - over a hundred languages and instead of
147:40 - having specific models to translate from
147:42 - each language to each language
147:45 - all of these languages are translated to
147:48 - a sum
147:49 - uber intermediate representation
147:51 - which is then able to be translated back
147:54 - into any one of these hundred plus
147:55 - languages so all to all language
147:58 - translator
147:59 - so the sheer scope of that is really
148:02 - impressive
148:06 - now in order to do this
148:07 - it uses long short term memory
148:10 - lstm which is a neural network
148:14 - architecture and it actually uses
148:16 - several
148:17 - deep neural networks
148:19 - in concert
148:20 - one to
148:22 - carefully ignore parts of the input
148:24 - one to choose what to remember one to
148:27 - choose what to forget when to choose
148:28 - what to pass on
148:30 - there's quite a bit of computation
148:32 - involved
148:33 - and this architecture uses several
148:35 - levels of those even
148:38 - so the the amount of effort and
148:42 - computing power
148:44 - thrown at this in general um
148:46 - is
148:48 - if we use one of our metrics as
148:50 - efficiency it's a little bit could be
148:53 - considered a little bit of a hit
148:56 - in addition to the inaccuracies
148:59 - it is worth noting that this uses an
149:01 - attention mechanism so
149:04 - i called out attention earlier as a
149:06 - possible limitation of human performance
149:09 - but it also proves to be a really useful
149:12 - tool
149:13 - when dealing with a massive amount of
149:15 - information
149:17 - too much
149:18 - to
149:19 - look at in depth
149:21 - and so by pre-filtering and focusing
149:23 - down on what's most likely to be helpful
149:26 - then in an algorithm can be much more
149:28 - efficient in how it handles it
149:31 - so
149:33 - for machine translations
149:35 - amazing
149:36 - performance still short of human
149:38 - and for the
149:40 - wildly
149:42 - ambitious scope
149:44 - it gets a
149:46 - little step up on the generality axis a
149:48 - little bit of a hit on their performance
149:50 - axis
149:51 - translation is still a very small part
149:53 - of all the things that humans do
149:55 - but
149:56 - i would definitely say this is more
149:57 - general than playing video games
150:01 - now looking at recommenders
150:04 - so if you think back to your last
150:06 - experience with an on time online real
150:09 - online retailer
150:12 - probably
150:13 - the recommendations that you got were
150:16 - maybe one in 10 was really really
150:18 - relevant
150:19 - some of the others were close but near
150:21 - misses and some of the others were
150:23 - obviously way out in left field
150:26 - so
150:27 - this is
150:28 - still pretty good like this is a tough
150:30 - task if you can imagine like uh back
150:33 - when there were video stores going to a
150:35 - video store with your friend and trying
150:36 - to guess
150:37 - what your friend even a friend you knew
150:39 - very well
150:40 - would want to watch on a given night you
150:42 - know you would be hard-pressed to do
150:44 - better than like one and three or one
150:46 - and four
150:47 - so you know one in ten is not terrible
150:50 - or just ballpark
150:53 - it's common to assume among these
150:56 - algorithms that order doesn't matter
150:59 - so it just looks at everything you've
151:01 - ever bought
151:02 - today yesterday last year
151:05 - and it doesn't think about how these
151:06 - things are related or how many you might
151:08 - have or how many you might need or how
151:10 - something that you bought previously
151:12 - might be related to what you might need
151:14 - tomorrow
151:15 - it just looks at what people have bought
151:19 - in the past
151:20 - what they've bought together
151:22 - it also doesn't
151:23 - adapt to the fact that your selections
151:25 - might change with time so
151:27 - even if you bought one jar of mayonnaise
151:29 - a year ago and then another one six
151:31 - months ago and another one a few months
151:32 - ago it might not
151:34 - track the fact that your preference has
151:36 - changed
151:37 - one of my favorite examples of these
151:39 - came up from
151:40 - jack raynor twitter user who said dear
151:44 - amazon i bought a toilet seat because i
151:46 - needed one
151:48 - necessity not desire
151:50 - i do not collect them i am not a toilet
151:53 - seat addict
151:54 - no matter how temptingly you email me
151:57 - i'm not going to think
151:58 - oh go on then just one more toilet seat
152:01 - i'll treat myself
152:06 - so recommenders
152:08 - they do
152:09 - okay
152:10 - relative to humans and i would argue
152:12 - that the knowledge of the world required
152:15 - to do really well
152:17 - is pretty deep so we'll boost it up on
152:20 - the generality scale but it takes a hit
152:22 - on performance
152:26 - now
152:27 - we get to robots finally
152:30 - something physical bumping around in the
152:32 - world self-driving cars
152:37 - their performance is impressive
152:40 - taken overall per mile driven
152:43 - self-driving cars accident rates are
152:46 - lower than humans
152:49 - and this is pretty amazing when you
152:51 - consider
152:52 - all of the things that a car has to deal
152:54 - with
152:55 - construction
152:57 - pedestrians bicycle riders
153:01 - changing weather conditions changing
153:02 - road conditions
153:04 - they're not perfect but they're
153:06 - surprisingly good
153:12 - um now in terms of generality
153:15 - there are a few things
153:17 - that make self-driving cars less general
153:20 - than they might at first appear and in
153:23 - fact the biggest tricks to making them
153:25 - successful is to reducing the difficulty
153:29 - of the task and so reducing the
153:31 - necessary generality of the solution
153:35 - so one of the things that happens is
153:36 - especially during training humans still
153:39 - have to take over in some challenging
153:40 - situations
153:42 - when the human gets uneasy or the car
153:44 - signals it doesn't know what to do then
153:45 - it falls back to the human
153:49 - and while driving is still a pretty
153:51 - complicated task
153:53 - it's still very simple compared to say
153:56 - walking on rough terrain
153:58 - while eating a bagel
154:00 - and walking a dog who's pulling on the
154:02 - leash
154:04 - there's a lot more to consider there and
154:06 - it's a lot tougher than
154:07 - a car who is statically stable on four
154:11 - wheels on a road that's flat and mostly
154:14 - straight and mostly marked
154:17 - and
154:18 - where the rules are well prescribed
154:22 - to further simplify the task and narrow
154:25 - the scope of what needs to be learned
154:27 - self-driving car's driving style tends
154:29 - to be cautious
154:31 - they definitely do not tend to speed
154:33 - they tend to not follow closely or turn
154:36 - aggressively
154:38 - or do anything else that many human
154:40 - drivers do
154:42 - this is absolutely good practice
154:46 - and should be lauded and as a model for
154:48 - all of us to follow
154:50 - but what that means is that the
154:52 - raw driving skill
154:54 - required by a self-driving car
154:57 - is in general
154:58 - less than that required by a human
155:03 - and it should also be noted that
155:04 - solutions are custom engineered for
155:06 - driving
155:07 - the selection of sensors the algorithms
155:10 - used to process them the way everything
155:12 - is put together is
155:14 - not updated on the fly
155:17 - it's gathered
155:18 - evaluated by humans
155:20 - and
155:22 - then very
155:24 - carefully and deliberately
155:26 - the heuristics the rules behind how
155:28 - that's interpreted and processed are
155:31 - then updated and tested and released
155:32 - again
155:34 - this makes sense for deploying anything
155:37 - that is has such a high consequence as a
155:40 - car
155:42 - but
155:43 - from a machine learning side it means
155:45 - that the solution is actually not as
155:46 - general as it seems it's
155:48 - very specific to a given
155:51 - car with a given set of sensors and
155:55 - sometimes even to a given environment
155:58 - some of some self-driving cars at least
156:00 - early failures
156:02 - had to do with being deployed in
156:04 - climates that they weren't familiar with
156:05 - for instance
156:07 - so until
156:08 - their set of training data encompasses
156:11 - all of the conditions in which they will
156:13 - be
156:14 - deployed
156:15 - they will be even narrower than human
156:18 - drivers
156:21 - so
156:22 - all these things considered on the task
156:25 - of driving in general
156:28 - i chose to rate self-driving cars at
156:30 - lower performance than human
156:34 - still its physical interaction and its
156:37 - interaction with other people in other
156:39 - cars so it's
156:41 - quite a lot going on and definitely more
156:43 - complex than machine translation or any
156:45 - even recommendations
156:50 - now
156:51 - humanoid robots
156:52 - the apex of
156:54 - cool applications
156:56 - um if you have not yet done it
156:59 - get on the internet and search for
157:00 - robots doing backflips and check it out
157:04 - when you see something like this it is
157:07 - easy to make the jump
157:09 - to believe that robotics has been solved
157:12 - like when a robot can do physical feats
157:16 - of acrobatics that i can't do
157:18 - then i mean it's done i'm ready to call
157:21 - it and um
157:24 - yeah it's just uh
157:26 - it puts a smile on my face that i can't
157:28 - wipe off
157:31 - now in terms of generality
157:34 - um do another search for robots falling
157:37 - down
157:38 - and
157:39 - you'll be treated to a montage of
157:42 - really
157:43 - humorous shorts of robots trying to do
157:46 - very simple things
157:48 - like open a door or lift an empty box
157:51 - or even stay standing up
157:54 - and they really
157:56 - struggle with this
157:58 - um because
158:01 - the systems are so complex because the
158:04 - hardware and the sensors have so much
158:06 - going on
158:07 - and because most of these are deployed
158:09 - as as research projects
158:12 - most of these activities are
158:14 - fairly hard-coded and pretty fragile
158:17 - they make a lot of assumptions about
158:20 - the nature of the hardware what's going
158:22 - on the nature of the environment and if
158:23 - any of these assumptions are violated
158:25 - the robot's performance
158:27 - fails
158:31 - so as a result uh plotting them here
158:35 - the generality in the sense that the
158:37 - types of things that they have
158:39 - experienced taken together as a whole
158:42 - are
158:43 - now getting to be a non
158:46 - negligible fraction of things that
158:47 - humans can do you know maybe it's 0.1
158:50 - maybe it's 0.01 somewhere in between
158:52 - but
158:53 - an amazing set of things that can be
158:55 - quite hard
158:57 - but performance is still sometimes
159:00 - laughably low compared to human level
159:07 - um we can
159:08 - you know compare humanoid robots as
159:10 - agents
159:12 - to humans and see that it's much less
159:15 - more interestingly here
159:17 - our trend now is quite clear
159:21 - there's a fat line here that runs
159:24 - roughly parallel
159:26 - offset from the human level intelligence
159:28 - line
159:30 - as solutions
159:32 - tend to get higher performance they also
159:34 - tend to get less general and vice versa
159:37 - but it's rare that we get big steps
159:40 - toward the human intelligence line
159:44 - now
159:45 - this is this is what i think is cool
159:46 - this is the whole point of this talk
159:48 - there is one example that i would like
159:50 - to show of this before i
159:52 - jump to the conclusion
159:54 - which is again from deep mind a program
159:57 - called alpha zero
160:00 - so alpha zero is like alphago
160:03 - except
160:05 - everything it knows about go
160:07 - has been taken out
160:09 - it doesn't know the rules of any game
160:11 - now it just sees visual patterns
160:14 - tries actions and learns to see what is
160:17 - successful and what's not
160:22 - the way it was used is you can think of
160:25 - a brand new alpha zero instance as being
160:28 - an infant in terms of the gameplay and
160:31 - two alpha zero infants were created and
160:34 - they started to play each other one was
160:36 - allowed to learn the other one was not
160:38 - so the one that learned gradually got
160:40 - just a little bit better stumbling into
160:42 - some good moves by accident until it
160:45 - became an okay beginner player of the
160:48 - game
160:49 - then
160:50 - it cloned itself
160:52 - one of the two learned the other did not
160:54 - and they played and played until the one
160:56 - became an intermediate level player of
160:58 - the game
160:59 - and it repeated this process of cloning
161:03 - and playing itself with one learning and
161:05 - the other not
161:07 - and used
161:08 - its
161:09 - intermediate steps
161:11 - as scaffolding to get better and better
161:15 - and it turns out
161:16 - that when
161:18 - using this approach
161:19 - with go
161:21 - within four hours it was as good as the
161:24 - best human player
161:26 - and within eight hours
161:28 - it had beat
161:29 - the previous best computer its
161:32 - uncle
161:33 - alphago
161:35 - um
161:36 - because it did not build any rules of
161:39 - the game it was also able to learn chess
161:42 - and beat the current best chess playing
161:46 - program stockfish and another board game
161:49 - called shogi
161:50 - and be the current best shogi playing
161:54 - program as well
161:55 - all of which beats humans by a wide
161:57 - margin
162:00 - so this is cool because it
162:05 - does both better performance and it's
162:07 - more general it's not specific to any
162:09 - board game and presumably if there were
162:11 - other board games that had
162:13 - two-dimensional grids and a set of rules
162:17 - that was not wildly wildly different it
162:19 - could learn to play those as well so
162:22 - generality and performance so what we
162:24 - have now is a point that is both
162:28 - farther to the right
162:29 - higher performance
162:31 - and farther up higher generality than
162:34 - the original that it
162:36 - was from so this is a real increase in
162:39 - area under that rectangle an increase in
162:42 - intelligence this is the direction that
162:44 - we want to go
162:47 - so it's worth taking a step and
162:50 - taking a moment and thinking about what
162:52 - is it that allowed us to step in this
162:54 - direction
162:55 - well alpha zero made many fewer
162:58 - assumptions about what was going on
163:01 - and it also was able to practice as many
163:03 - times as it needed to through self-play
163:07 - in general
163:09 - assumptions are what prevent generality
163:13 - they enable performance so if i build in
163:16 - knowledge of the rules of chess i'm able
163:17 - to take advantage of those much more
163:19 - quickly
163:20 - but it also prevents me from doing
163:22 - anything that's not chess
163:24 - so if i turn that around making fewer
163:27 - assumptions
163:28 - i mean it takes me longer to do
163:30 - something but it means i might be able
163:32 - to learn to do more
163:34 - things
163:36 - so some common assumptions
163:40 - sensor information is noise free we have
163:42 - ideal sensors
163:44 - that makes sense
163:45 - if we're playing chess
163:47 - when we sense that a piece is on a given
163:48 - square we expect that it will be
163:51 - but if we're dealing with say a
163:52 - self-driving car
163:54 - maybe there's a smudge of mud on the
163:56 - camera maybe the calibration of the
163:58 - lidar is off a little bit
164:00 - we can't assume ideal sensors when we're
164:03 - interacting with the physical world
164:04 - there are too many things we can't
164:06 - control for
164:09 - another common assumption is determinism
164:12 - that's when i take an action i know that
164:14 - it will have the same outcome every time
164:17 - again makes great sense when i have a
164:19 - board game make sense if i'm classifying
164:22 - images if i say an image is an image of
164:24 - a cat i know that it will be labeled as
164:26 - a cat image right or wrong
164:29 - however if i'm a humanoid robot
164:32 - and i
164:33 - make an action to reach for a doorknob
164:37 - the motor might not perform the way i
164:39 - expect
164:41 - my
164:41 - feet might slip on the ground i might
164:44 - have unanticipated challenges to my
164:46 - balance the action may not turn out
164:48 - exactly as i expect
164:50 - and i need to be able to adapt to this
164:54 - another really common assumption
164:57 - unimodality
164:58 - all of the sensors are the same type
165:01 - so this is
165:02 - an assumption in convolutional neural
165:04 - networks for instance
165:06 - it's great at bringing in a
165:08 - two-dimensional array
165:10 - of information that is all the same type
165:13 - it's all pixels where it's all board
165:15 - squares
165:17 - a general solution
165:19 - needs not to make this assumption
165:22 - another assumption
165:24 - stationarity
165:25 - this is a very common one it's that the
165:28 - world doesn't change the things that i
165:31 - learned
165:32 - yesterday
165:33 - are still true today the things that i
165:35 - learned five minutes ago are still true
165:37 - right now
165:39 - now we have to make some change sorry we
165:42 - have to make some assumptions about
165:43 - continuity
165:45 - otherwise what i learned yesterday
165:47 - doesn't do me any good at all
165:49 - but we also need to allow for the fact
165:52 - that the world has changed a little bit
165:54 - maybe the lubrication in my ankle joint
165:57 - is a little low so it's going to respond
165:59 - differently than it did yesterday
166:02 - maybe there are clouds covering the sun
166:04 - so the lighting conditions i learned to
166:06 - operate in yesterday have changed as
166:08 - well and i'll need to to be able to
166:09 - adapt to that
166:14 - another common assumption is
166:15 - independence which is the world is not
166:18 - changed by what i do to it
166:21 - physical interaction violates this
166:22 - entirely if i am a robot operating in a
166:26 - household and i
166:28 - bump into a chair and i scoot it six
166:30 - inches sideways then whatever map i've
166:33 - made of that house
166:35 - will need to be changed a little bit i
166:37 - have changed it myself if i pick up a
166:40 - mug and move it from this table to that
166:41 - table i have changed the position of
166:43 - that mug
166:44 - the things that i do
166:46 - change the world and i need to keep
166:48 - track of that and any algorithm i use
166:50 - needs to be able to account for that
166:54 - another common assumption ergodicity
166:58 - everything i need to know about how i
167:00 - operate i can sense right at this moment
167:04 - this is a common assumption also known
167:07 - as a markov assumption
167:09 - but it's also
167:10 - commonly broken in physical interaction
167:14 - for instance if i can sense position
167:16 - that's great but that doesn't tell me
167:17 - anything about velocity and sometimes i
167:21 - need to know velocity to know how to
167:23 - respond to something
167:28 - another assumption that is very common
167:30 - is that the effects of my actions become
167:33 - apparent very soon
167:35 - this is something that does not hold
167:36 - true for instance in chess
167:39 - where the opening move will affect
167:42 - whether or not i win
167:44 - many many time steps away
167:47 - there are different tricks for handling
167:49 - this in chess for instance assigning
167:51 - point values to intermediate
167:53 - positions of pieces on the board
167:55 - but in physical interaction it's much
167:57 - more difficult to do this
167:59 - to know that
168:01 - given a set of actions that i take right
168:03 - now
168:04 - which is most likely to result in
168:06 - something that's desirable five minutes
168:07 - from now or a day from now
168:14 - all of these assumptions are very common
168:16 - in the algorithms currently being used
168:19 - that we call
168:20 - ai
168:23 - these algorithms
168:25 - are not sufficient for achieving human
168:27 - level intelligence
168:29 - these assumptions will prevent them from
168:32 - doing that
168:37 - so one thing that all of these
168:39 - assumptions have in common
168:42 - is that they do not hold when you're
168:44 - working with humanoid robotics or in
168:47 - fact any
168:49 - robot that's physically interacting with
168:51 - the world
168:54 - so
168:54 - my proposal
168:56 - is that focusing on
168:59 - interact physical interaction
169:01 - is a great way
169:03 - to force us to confront these
169:06 - assumptions to find out which ones we
169:08 - can bend to find out which we can avoid
169:10 - all together and to drive us
169:12 - to create algorithms that are less
169:15 - fragile and able to accommodate a much
169:19 - more general set of tasks
169:21 - that will then take us
169:23 - one step closer to human level
169:25 - intelligence
169:27 - when you hear about artificial
169:29 - intelligence about a half of the time
169:31 - what people are talking about is
169:33 - convolutional neural networks
169:35 - understanding how they work is really
169:37 - helpful in getting a peek behind the
169:40 - curtain at the magic of artificial
169:41 - intelligence so we're going to walk
169:43 - through it in some detail
169:47 - convolutional neural networks
169:49 - takes images
169:51 - and from them
169:52 - they learn the patterns the building
169:54 - blocks that make them up
169:56 - so for instance in the first level of
169:58 - this network you might learn things like
170:00 - line segments that are at different
170:02 - angles
170:03 - and then at subsequent layers those get
170:06 - built into things like faces or element
170:09 - of cars
170:10 - depending on the images that you train
170:13 - the network on
170:15 - you can pair this with reinforcement
170:17 - learning algorithms to get algorithms
170:19 - that play video games learn to play go
170:23 - or even control robots
170:27 - so to dig into how these work we'll
170:30 - start with a very simple example much
170:32 - simpler than all of these
170:34 - a convolutional neural network
170:36 - that can look at a very small image
170:38 - and determine whether it's a picture of
170:40 - an x or an o
170:42 - just two categories
170:46 - so for example this image on the left is
170:48 - an eight by eight pixel image of an x
170:51 - we want our network to classify it as an
170:54 - x
170:55 - similarly with the image of the o we
170:57 - want the network to classify it as an o
171:01 - now this is not entirely straightforward
171:03 - because we also wanted to handle cases
171:05 - where these inputs are of different
171:07 - sizes or they're rotated or they're
171:09 - heavier or they're lighter and every
171:10 - time we'd like it to give us the correct
171:13 - answer
171:14 - a human has no problem looking at these
171:16 - in deciding what to do but for a
171:18 - computer this is much harder when trying
171:21 - to decide if these two things are equal
171:24 - what it does is it goes through
171:26 - pixel by pixel black pixels might be a
171:28 - minus one white pixels might be a plus
171:30 - one and it'll compare them pixel by
171:33 - pixel find the ones that match and here
171:36 - the red pixels are the ones that don't
171:38 - match
171:38 - so a computer looking at this would say
171:41 - uh no these are not the same they have
171:43 - some matches but they have a lot that
171:44 - don't match
171:46 - so the way convolutional neural networks
171:48 - do this one of the tricks they use is
171:50 - that they match pieces of the image
171:53 - so you can look at these pieces and
171:55 - shift them around a little bit but as
171:58 - long as the
171:59 - tiny bits still match then the overall
172:02 - image is still considered a pretty good
172:04 - match
172:06 - so these tiny bits might look like this
172:11 - we'll call them features you can see the
172:13 - one on the left looks like a diagonal
172:16 - arm of the x that's leaning left the one
172:19 - in the middle looks like the center of
172:21 - the x where it crosses
172:22 - and the one on the right looks like a
172:23 - diagonal arm
172:25 - leaning right
172:28 - and you can see how these different
172:29 - pieces these different features of the
172:32 - image
172:33 - match different patches
172:36 - within the overall image
172:40 - so the math behind finding this match
172:43 - applying features is called filtering
172:46 - it's pretty straightforward but it's
172:48 - worth walking through
172:50 - the way that it's done is you line the
172:51 - feature up on the image patch you're
172:53 - concerned with
172:55 - multiply pixel by pixel
172:57 - add up the values and then divide by the
173:00 - total number of pixels this is one way
173:01 - to do it
173:04 - here for instance
173:05 - we start with this feature of the arm of
173:09 - the x leaning left
173:11 - we align it with
173:13 - this arm on the image
173:15 - and we start with the upper left pixel
173:18 - and we multiply both values one by one
173:20 - equals one
173:22 - now because we started with the upper
173:24 - left pixel we can keep track of our
173:26 - answers here so this pixel when
173:28 - multiplied equals one
173:32 - the upper center pixel is minus one
173:36 - in both of the feature and the image so
173:40 - minus 1 times -1 is also equal to 1 so
173:43 - that
173:44 - when you multiply them and you get a 1
173:47 - that indicates a perfect and a strong
173:49 - match
173:51 - and we can continue doing this
173:53 - throughout
173:54 - the entire feature and the entire image
173:57 - patch
173:58 - and because they are a perfect match
174:00 - every single one of these
174:02 - matches will come back as a one
174:05 - and so to find the overall match we just
174:07 - add up all these nine ones divide by the
174:10 - total number which is nine and we get a
174:12 - a match of one
174:15 - now we can create another array to keep
174:17 - track
174:18 - of how well the feature
174:21 - when placed in this position
174:24 - matched our image
174:26 - so this average value is one we'll put a
174:28 - one right there to keep track of that
174:32 - you can see what it looks like if we
174:33 - were to move this feature then and align
174:36 - it to a different patch so let's say we
174:38 - move it down to the center of the x
174:40 - and we go pixel by pixel and find what
174:42 - matches and after a few pixels we
174:45 - actually find one that doesn't match we
174:47 - end up with a minus 1 times a plus 1
174:49 - giving us a minus 1 back
174:52 - this indicates a non-match of these
174:55 - pixels
174:58 - and as we go through the rest of the
175:00 - feature we can see that there are a
175:01 - couple of pixels that don't match
175:04 - so here when we add these up and divide
175:06 - by nine we get a number that's less than
175:08 - one point five five
175:10 - so it indicates a partial match but not
175:12 - a perfect one
175:16 - it turns out you can go through and do
175:18 - this for every possible location in the
175:21 - image you can chop it up into every
175:23 - possible image patch
175:25 - compare the feature to each one
175:27 - and here's what you would get in this
175:29 - particular case
175:31 - this is what convolution is it's taking
175:34 - a feature and applying it across every
175:37 - possible patch across a whole image
175:40 - and you can see here
175:42 - why it's called filtering
175:44 - what we have is a map
175:46 - of where this feature
175:49 - matches the image
175:51 - you can see a strong bunch of plus ones
175:54 - on the diagonal line from the lower
175:56 - right to the upper left
175:58 - and then
176:00 - lesser values everywhere else
176:03 - so it's a filtered version of the
176:04 - original image that shows where the
176:07 - feature matches
176:10 - you can do this
176:11 - we can represent this with this notation
176:14 - uh we just invented this little
176:15 - convolution
176:16 - operator for shorthand
176:19 - and we can do this with our other
176:20 - features as well
176:22 - we can see where our center x matches
176:26 - not surprisingly it matches strongest in
176:28 - the center of the image
176:30 - we can see where our leaning right arm
176:34 - matches and not surprisingly it matches
176:36 - along this diagonal from the lower left
176:38 - to the upper right
176:41 - we have three filtered versions of the
176:43 - original image
176:46 - so this is what
176:48 - a convolution layer
176:50 - in a convolutional neural network does
176:54 - it has a set of features in it
176:57 - and it can be three or thirty or three
176:59 - hundred or three thousand
177:01 - but it has a set of features
177:04 - and it takes the original image
177:06 - and returns
177:08 - a set of filtered images one for each of
177:12 - the features
177:14 - so this is how we'll represent it
177:19 - that's the number one ingredient in
177:22 - convolutional neural networks that is
177:24 - the magic
177:25 - special sauce the special trick
177:27 - that gets from
177:30 - a non-exact match
177:32 - and let's the algorithm is able to pull
177:34 - out okay well it's not a perfect match
177:36 - but it's still a pretty good match
177:38 - because it does this convolution and
177:40 - moves the feature across the image and
177:42 - finds everywhere that it might match
177:45 - another piece of this is called pooling
177:49 - so we took our original image and now we
177:51 - have a stack of images
177:53 - what this step does is it shrinks it
177:55 - down a little bit
177:57 - we start by picking a window size
177:59 - usually two or three pixels
178:02 - picking a stride usually two pixels has
178:04 - been shown to work well
178:06 - and then walking this window
178:08 - across the filtered images
178:11 - and then from each window taking the
178:13 - maximum value that you see so this is
178:15 - called max pooling
178:18 - so to see how this works we start with
178:20 - one of our filtered images
178:22 - we have our window which is two pixels
178:23 - by two pixels
178:25 - and within that the maximum value is one
178:29 - so we create another little array to
178:31 - keep track of all of our results and we
178:33 - put a 1 in it
178:35 - and then we step it over by our stride
178:38 - which is 2 pixels
178:40 - look at the window choose the maximum
178:42 - value in this case it's 0.33
178:45 - record it and go again
178:48 - and we keep doing this recording the
178:49 - maximum value each time all the way
178:52 - through the image
178:53 - and when we're done
178:55 - we have which if you squint looks like a
178:58 - shrunken version of the original
179:01 - we still have this strong set of plus
179:04 - ones on the diagonal from upper left to
179:06 - lower right
179:08 - and then everywhere else it's less than
179:09 - that
179:10 - so it maintains kind of the original
179:12 - signal but shrinks it down kind of picks
179:15 - off the high points
179:16 - and this gives us a smaller image but
179:21 - still similar to the original
179:24 - and we can just represent it with this
179:26 - little shrinking arrow we can do this
179:28 - with each of our filtered images and
179:30 - again you see that very roughly the
179:32 - pattern of the original is maintained
179:36 - so in a pooling layer
179:38 - a stack of images becomes a stack of
179:40 - smaller images
179:44 - now the last ingredient we need is
179:46 - normalization
179:47 - so this keeps the math from breaking by
179:50 - taking and tweaking these values just a
179:52 - little bit it takes everything that's
179:53 - negative and changes it to zero
179:56 - this keeps things from
179:58 - becoming unmanageably large as you
180:01 - progress through subsequent layers
180:05 - this function is called a rectified
180:07 - linear unit
180:08 - it's a fancy name for something that
180:10 - just takes anything that is negative and
180:12 - makes it zero so 0.77 not negative it
180:15 - doesn't touch it but a minus .11 it's
180:19 - negative just bumps it up to zero
180:21 - and by the time you've gone through all
180:23 - of your images and done this all of your
180:24 - pixels and done this this is what you
180:26 - have so everything that was negative is
180:28 - now zero
180:29 - so just a nice little bit of
180:31 - normalization some conditioning to keep
180:34 - things numerically well behaved
180:38 - a stack of images becomes a stack of
180:40 - images with no negative values
180:44 - now
180:45 - you can notice that the output of one
180:48 - layer looks like the input to the next
180:52 - there are always arrays of numbers
180:56 - an image and an array of number are the
180:58 - same thing they're interchangeable so
180:59 - you can take the output of the
181:00 - convolution layer feed it through the
181:02 - rectified linear unit layer feed that
181:04 - through the pooling layer
181:06 - and when you're done
181:08 - you have something that has had all of
181:10 - these operations done to it
181:13 - and in fact you can do this again and
181:15 - again
181:16 - and
181:18 - this
181:19 - recipe you can imagine making like a
181:20 - scooby-doo sandwich of all of these
181:22 - different layers again and again and in
181:24 - different orders
181:25 - um and uh some of the most successful
181:28 - convolutional neural networks are
181:31 - kind of like
181:32 - accidentally
181:34 - discovered
181:36 - groups of these that just happen to work
181:37 - really well so they get used again and
181:39 - again
181:40 - so over time each convolution layer
181:44 - filters through a number of features
181:46 - each rectified linear unit layer changes
181:49 - everything to be non-negative and each
181:52 - pooling layer shrinks it
181:54 - so by the time you're done you get a
181:56 - very tall stack of filtered images with
181:58 - no negative values that has been
182:01 - shrunken down in size
182:05 - now by the time we've gone through
182:07 - several iterations of this
182:09 - we take and run it through a fully
182:12 - connected layer this is more of a
182:14 - standard neural network where every
182:18 - input
182:19 - gets connected to
182:21 - everything
182:22 - in the next layer
182:24 - with a weight
182:26 - every single value you can think of it
182:27 - as a voting process
182:29 - so every single pixel value that's left
182:31 - in these filtered shrunken images gets a
182:33 - vote on what the answer should be
182:37 - and this vote depends on how strongly it
182:40 - is tends to predict an x or an o when
182:43 - this pixel
182:45 - is high
182:47 - is this output usually an x or is it
182:50 - usually an o
182:51 - so you can see for this particular input
182:53 - the input was an x here's what the
182:56 - imaginary convolved and filtered values
182:59 - are
183:00 - and
183:02 - over time we would learn that these
183:03 - things that are high
183:05 - when it sees an x get a strong vote for
183:08 - the x
183:10 - category similarly
183:13 - if you have an input that's an o
183:16 - these
183:17 - final pixel values that tend to be
183:19 - really high when the right answer is row
183:22 - is o gives a strong vote for the o
183:25 - category
183:27 - the thicknesses of these lines represent
183:29 - the weights
183:30 - the strength of the vote between these
183:32 - pixels and these answers
183:37 - so now if we get a new input that we've
183:38 - never seen before these might be the
183:40 - final pixel values we can use these
183:43 - votes and do a weighted voting process
183:45 - for both of these
183:47 - add them up
183:48 - and in this case you know it's a 0.92
183:51 - total for x
183:52 - and a 0.51 total for o
183:55 - 0.92 is obviously more than 0.51 we
183:57 - declare x the winner so this input will
183:59 - have been categorized as an x
184:04 - so this is a fully connected layer so it
184:07 - just takes a list of feature values in
184:09 - this case our filtered shrunken pixels
184:13 - and it becomes a list of votes for each
184:15 - of our output categories in this case an
184:18 - x or an o
184:21 - now these can also be stacked you can
184:23 - have like little they call them hidden
184:24 - layers but like secret hidden categories
184:27 - in between here so one votes on the
184:29 - first layer votes on the first set of
184:31 - hidden categories and then those vote on
184:32 - the next layer and so forth until you
184:34 - get to your final ones we'll dig into
184:36 - this more in just a little sec
184:37 - but
184:38 - these
184:40 - all stack onto the end
184:42 - now to go into uh the next level of
184:45 - detail on these neural networks
184:48 - set aside our x and o detector for a
184:50 - while there we had eight by eight
184:53 - pixel images so 64 pixels in all
184:56 - consider now a two by two pixel image so
184:59 - just a four pixel camera
185:02 - and what we would like to do
185:04 - is categorize the images that it takes
185:07 - as either being a solid image all light
185:10 - or all dark
185:12 - a vertical image
185:14 - a diagonal image
185:16 - or a horizontal image
185:19 - now the trick here is that simple rules
185:21 - can't do it so both of these are
185:24 - horizontal images but
185:26 - the pixel values are completely opposite
185:28 - in both of them so you can't say well if
185:30 - the upper left pixel is white
185:33 - and the upper right pixel is white then
185:34 - it must be horizontal because that's
185:36 - violated by the other one
185:39 - now of course you could do more
185:40 - complicated rules to do this the point
185:42 - is that when you go to larger images you
185:44 - can't make simple rules that capture all
185:47 - the cases that you want
185:49 - so
185:50 - how do we go about it
185:52 - we take these four input pixels and
185:55 - break them out we call them input
185:56 - neurons but they just take these pixels
185:58 - and turn them into a list of numbers
186:01 - the numbers correspond to the brightness
186:03 - minus one is black plus one is white
186:06 - zero is middle gray and everything else
186:08 - is in between
186:11 - so this takes this little image and
186:13 - turns it into a list of numbers
186:15 - that's our input vector
186:18 - now each of these you can think of it as
186:20 - having a receptive field
186:22 - this is
186:23 - an image
186:25 - which makes the value of this input as
186:28 - high as possible
186:30 - so if you look at our very top input
186:32 - neuron
186:34 - the image that makes that number as high
186:35 - as possible is an upper left pixel
186:38 - that's white
186:40 - that makes the value of that one and it
186:42 - doesn't care what the other pixels are
186:43 - that's why they're checkered
186:45 - so you can see that each of these has
186:46 - its own corresponding receptive field
186:49 - the image that makes the value as high
186:51 - as it can go
186:54 - now we're going to build a neuron
186:57 - so when uh people talk about artificial
186:59 - neural networks and a neuron we are
187:01 - going to build it bit by bit
187:03 - the first thing you do to build a neuron
187:04 - is you take all of these inputs you add
187:06 - them up
187:07 - so in this case this is what we'd get so
187:09 - the neuron value at this point is 0.5
187:13 - now the next thing we do is we add a
187:15 - weight we mentioned the weighted voting
187:17 - process before so what that looks like
187:20 - is each of these inputs gets assigned a
187:22 - weight between plus and minus one
187:26 - and
187:27 - it gets the value gets multiplied by
187:29 - that weight before it gets added
187:32 - so now we have a weighted sum
187:34 - of these input neurons
187:37 - and we will represent this visually by
187:39 - showing positive weights in white
187:41 - negative weights in black and the
187:42 - thickness of the line being
187:44 - approximately
187:45 - proportional to the weight and when the
187:47 - weight is zero we'll leave it out to
187:49 - minimize visual
187:50 - clutter so now we have a weighted sum of
187:53 - the inputs
187:55 - the next thing that we need to do is
187:57 - squash the result
187:59 - so because we're going to do this a lot
188:01 - of times it's nice if we always
188:03 - guarantee the answer is between plus and
188:05 - minus 1 after each step that keeps it
188:08 - from growing numerically large
188:13 - a very convenient function is this
188:15 - s-shaped so sigmoid squashing function
188:19 - this particular one is called the
188:20 - hyperbolic tangent there is confusingly
188:23 - something else that's called the sigmoid
188:25 - it's a little bit different but the same
188:27 - general shape but the characteristic of
188:30 - this
188:30 - is that you can
188:32 - put in your input
188:34 - you know draw a vertical line see where
188:36 - it crosses the curve track that over to
188:37 - the
188:38 - using a horizontal line to the y axis
188:40 - and you can see what the smashed version
188:42 - the squashed version of your number is
188:45 - so in this case 0.5 comes out to be just
188:47 - under 0.5
188:50 - 0.65 comes out to be about 0.6 and as
188:53 - you go up this curve you can see that no
188:56 - matter how large your number gets
188:59 - what you get out will never be greater
189:01 - than one
189:02 - and similarly it'll never be less than
189:04 - minus one so it takes this
189:07 - infinitely long number line and squashes
189:10 - it so that it all falls between plus and
189:11 - minus one
189:15 - so we apply this function to the output
189:17 - of our weighted sum
189:19 - and then we get our final answer
189:21 - so this
189:22 - weighted sum and squash
189:25 - is almost always what people are talking
189:27 - about when they talk about an artificial
189:29 - neuron
189:31 - now we don't have to do this just once
189:33 - we can do it as many times as we want
189:35 - with different weights
189:37 - and this collection of weighted
189:40 - sum and squash neurons
189:42 - is you can think of it as a layer
189:45 - loosely inspired by the biological
189:49 - layers of neurons in the human cortex
189:52 - so
189:53 - each of these has a different set of
189:55 - weights here
189:56 - to keep our picture really simple we'll
189:58 - assume these weights are either plus one
190:00 - white lines
190:01 - minus one black lines or zero missing
190:05 - entirely
190:07 - so in this case now
190:09 - we have our
190:10 - layer of neurons
190:12 - we can see that the receptive fields
190:14 - have gotten more complex
190:16 - if you look at the neuron in the first
190:19 - layer on the top you can see how it
190:21 - combines the inputs from the upper left
190:24 - pixel and the lower left pixel
190:27 - both of the weights are positive those
190:29 - lines are white
190:30 - and so what comes out its receptive
190:33 - field is if both of those pixels on the
190:36 - left are white
190:38 - then it has the highest value it can
190:39 - possibly have
190:42 - if we look at that layer of neurons and
190:43 - look at the one on the bottom we can see
190:45 - that it takes its inputs
190:47 - from both of the pixels on the left oh
190:49 - sorry on the right
190:51 - but it has a negative weight
190:53 - collecting connecting it to the lower
190:55 - right neuron
190:56 - so its receptive field its what
190:59 - maximally activates it
191:01 - is a white pixel in the upper right and
191:03 - a black pixel in the lower right
191:08 - now we can repeat this because the
191:11 - outputs of those that first layer of
191:12 - neurons looks a whole lot like our input
191:15 - layer still a list of numbers between -1
191:18 - and 1 and so we can add additional
191:20 - layers and we can do this as many times
191:22 - as we want
191:23 - each time
191:25 - each neuron in one layer is connected to
191:27 - each neuron in the other layer by some
191:29 - weight
191:31 - so in this case you can see how the
191:33 - receptive fields might get still more
191:35 - complex
191:37 - and now we're starting to see patterns
191:39 - that look like
191:41 - the things that we're interested in
191:43 - solids verticals diagonals horizontals
191:46 - by combining these elements
191:49 - now there's one more thing that we can
191:51 - do remember our rectified linear unit
191:54 - we can have different neurons here
191:56 - instead of a weighted sum and squash we
191:58 - can just have something that takes the
192:00 - input and spits out
192:03 - 0 if it's negative
192:05 - and
192:06 - the original value if it's positive
192:09 - and so for instance if we have
192:13 - if we have an input whose receptive
192:17 - field is the one on the very top and the
192:19 - second layer all solid white and we
192:22 - connect it with a positive weight
192:25 - to the rectified linear unit neuron on
192:28 - top
192:29 - then of course what would maximize that
192:31 - is all solid white input
192:34 - but if we look at the
192:36 - neuron just below that
192:38 - that's connected to it with a negative
192:39 - weight
192:41 - then
192:42 - that that flips everything around and
192:44 - what maximally activates that
192:47 - is an input that's all solid black
192:51 - now we're really starting to get the set
192:53 - of patterns
192:55 - that we can imagine using to decide
192:57 - what our images
193:00 - so we connect these again to a final
193:03 - output layer this output layer is the
193:06 - list of all the possible answers that we
193:08 - expect to get out of our classifier
193:12 - originally it was x's and o's now it's
193:15 - four categories solid vertical diagonal
193:17 - and horizontal
193:20 - and
193:21 - each of these
193:22 - inputs
193:24 - into them have a vote
193:27 - but you can see that very few of them
193:28 - are connected this network assumes that
193:30 - most of those votes are zero
193:33 - so to see how this plays out
193:35 - let's say we start with an input that
193:37 - looks like the one on the left
193:40 - with uh this is obviously a horizontal
193:43 - image with a dark bar on top and a white
193:46 - bar on the bottom
193:47 - we propagate that to the input layer
193:51 - and then we propagate that to the first
193:53 - hidden layer
193:55 - and you can see for instance the neuron
193:57 - on the very top
193:58 - it combines
194:00 - two input neurons that one is light and
194:03 - one is dark
194:05 - so you can imagine it's summing a plus
194:07 - one
194:08 - and a minus one
194:09 - and getting a sum of zero so that's why
194:12 - it's gray
194:13 - its value is zero
194:16 - now if you look at the neuron in the
194:17 - very bottom
194:18 - in that first hidden layer
194:20 - you can see that it sums also an input
194:23 - that is
194:25 - negative and one that's positive
194:28 - but
194:29 - it's connected to one by a negative
194:31 - weight and the other by a positive
194:32 - weight so it actually what it sees its
194:35 - weighted sum is minus one and minus 1.
194:38 - so what it is getting you can see is the
194:41 - opposite
194:42 - of its receptive field so that means
194:44 - it's maximally activated but negatively
194:47 - so that's why it's black
194:50 - we move to the next layer and you can
194:53 - trace these things through so anything
194:55 - zero plus zero is going to get you zero
194:58 - if you look at the neuron on the very
195:01 - bottom of this second hidden layer
195:03 - you can see that yes it's adding up a
195:06 - negative and a negative both connected
195:08 - by positive weight so it's also going to
195:10 - be negative
195:11 - which makes sense because you can see
195:13 - that its receptive field is the exact
195:15 - opposite of what the input is right now
195:16 - so it's maximally activated
195:19 - just negative
195:22 - and then when we track this to our next
195:24 - layer
195:25 - you can see that
195:27 - following that
195:28 - bottom pair of neurons
195:30 - because it's a negative value
195:32 - it goes through the rectified linear
195:34 - unit and becomes zero so that's gray but
195:37 - if you look at the very bottom neuron
195:39 - there
195:40 - it has it's connected with a negative
195:42 - weight so it becomes positive
195:44 - so that rectified linear unit really
195:47 - likes it
195:48 - so it gives it a maximum value so
195:50 - everything is zero except for that
195:52 - neuron on the bottom
195:54 - and then finally
195:56 - what that means is that the only output
196:00 - that is non-zero is this horizontal one
196:03 - so this network
196:05 - would classify
196:06 - the input image
196:08 - as being horizontal because of this
196:13 - now there's some magic here where did we
196:16 - get those weights where did we get the
196:17 - filters in between um
196:21 - this is where we start to get down
196:23 - to the
196:25 - when we talk about learning adaptation
196:27 - you know the learning and machine
196:28 - learning it is all about optimization
196:32 - these are
196:33 - learned through a bunch of examples
196:36 - over time
196:38 - so we're going to set that aside for
196:41 - just a minute we'll come back to how
196:42 - those get learned we need to talk about
196:44 - optimization first
196:46 - so consider drinking tea
196:49 - there is a temperature range where it is
196:51 - a delightful experience it's warm and
196:54 - delicious and comfortable
196:56 - if your tea is too much hotter than that
196:58 - it's very painful and not good not fun
197:00 - at all and if your tea is cooler than
197:02 - that it's lukewarm and it's really meh
197:05 - it's really not worth your time
197:07 - so this area at the top is the peak this
197:10 - is the best this is what we're trying to
197:13 - find
197:14 - in optimization we're just trying to
197:15 - find the best experience the best
197:18 - performance
197:20 - now if we want to find that
197:22 - mathematically first thing we do is we
197:23 - flip it upside down
197:25 - just because this is how optimization
197:28 - problems are formulated
197:30 - but it's the same type of thing instead
197:31 - of maximizing tea drinking pleasure we
197:33 - want to minimize minimize tea drinking
197:36 - suffering we want to find the bottom of
197:38 - that valley the lowest possible
197:40 - suffering
197:43 - there's a few different ways we could do
197:44 - this
197:46 - the first is to look at
197:48 - every point on this curve and just pick
197:50 - the lowest one
197:52 - now the trick with that is we don't
197:53 - actually know what this curve is
197:55 - beforehand so in order to pick the
197:57 - lowest one we have to do exhaustive
198:00 - search which in this case would be make
198:02 - a cup of tea have someone drink it ask
198:05 - them how they like it make another one
198:08 - ask them how they like that one do it
198:10 - again and again for every possible
198:12 - temperature
198:13 - and then
198:14 - pick the one with the lowest suffering
198:16 - the most enjoyment
198:19 - this is effective very effective also it
198:22 - can be very time consuming for a lot of
198:24 - problems and so
198:27 - we search for a shortcut
198:30 - now
198:31 - because
198:32 - this is a valley we can use our physical
198:35 - intuition and say hey well what if we
198:36 - just had a marble and we let it roll to
198:39 - the bottom of this valley we wouldn't
198:40 - have to explore every single piece of it
198:43 - so this is what's behind gradient
198:45 - descent
198:48 - the way it works is we start not knowing
198:50 - anything about this function we make a
198:52 - cup of tea someone tells us how they
198:54 - like it
198:55 - and then we change the temperature a
198:57 - little bit we make another cup of tea
199:00 - just a little bit cooler
199:02 - we ask someone how they like that and we
199:03 - find out they actually like it just a
199:05 - little bit less
199:07 - that tells us what direction we need to
199:09 - go we need to make our next cup of tea
199:12 - warmer
199:13 - and
199:14 - the change the difference between how
199:16 - much they like those two tells us the
199:18 - slope tells us the steepness gives us a
199:21 - sense of how much warmer we can expect
199:23 - to make that next cup of tea
199:25 - so we make another one
199:27 - and we repeat the process
199:29 - and then we
199:30 - again scoot a little ways off to the
199:33 - side make another cup of tea and figure
199:35 - out again which direction we need to go
199:37 - are we do we need to go warmer to make a
199:40 - better cup or cooler to make a better
199:42 - cup
199:43 - and we repeat this until we get to the
199:45 - bottom
199:46 - of the curve
199:47 - you'll know you're at the bottom
199:49 - when you change the temperature just a
199:52 - little bit and the tea drinker says yeah
199:54 - it's exactly the same i like that just
199:56 - as much as the last one
199:58 - that means that you're there kind of at
199:59 - the flat bottom of the valley
200:04 - so um
200:06 - gradient descent is the first level
200:10 - trick
200:11 - for brewing fewer cups of tea there's
200:14 - another thing you can do which is to use
200:16 - curvature this is kind of an advanced
200:17 - method is you can
200:20 - make
200:21 - your original cup of tea and then make
200:23 - one a little bit warmer and one a little
200:25 - bit cooler and you can look to see how
200:27 - that curve of your function goes
200:31 - and if it's very steep and getting
200:33 - steeper then you know you can take a
200:35 - giant step because you're probably not
200:37 - anywhere close to the bottom
200:40 - and then you can do it again and if that
200:42 - curvature is starting to bottom out then
200:44 - you can take a smaller step because you
200:47 - the signal that you're getting closer to
200:48 - the bottom and it helps you to do this
200:51 - in fewer steps
200:53 - as long as your curve is relatively well
200:56 - behaved
200:57 - which is not always the case
201:01 - so uh ways that this can break
201:05 - imagine we're doing this on a hot day
201:07 - and actually it turns out that if we
201:09 - were to cool our tea way down we'd get a
201:12 - really nice iced tea
201:14 - which turns out to be even more popular
201:16 - with our tea drinkers
201:18 - but gradient descent would never find
201:20 - this gradient descent always rolls down
201:22 - to the bottom of the nearest valley it
201:24 - doesn't hop around to see if there are
201:26 - any valleys hiding anywhere else
201:30 - another problem is
201:32 - let's say there's a wiggle on our curve
201:35 - there is something happening in the
201:36 - environment we have noisy buses driving
201:39 - by and it affects how people enjoy their
201:42 - tea
201:43 - we might not be able to find this very
201:45 - lowest dip because we might get stuck in
201:48 - a dip further up the curve
201:52 - similarly if we ask our tea drinkers to
201:54 - rate their tea drinking experience on a
201:57 - scale from one to ten
201:58 - we get these discrete jumps in our
202:01 - function
202:02 - and if you imagine a marble rolling
202:04 - downhill
202:05 - it downstairs it doesn't always work
202:08 - well and it can get stuck on a step
202:11 - without making it all the way to the
202:12 - bottom
202:14 - now all of these things happen
202:16 - in real machine learning problems
202:19 - another one imagine you have really
202:21 - picky tea drinkers and if the tea is
202:23 - anything but perfect they hate it hate
202:26 - it hate it
202:28 - and so you have these plateaus on either
202:29 - side and there's no signal to tell you
202:32 - that if you
202:33 - move in a little bit you'll find that
202:35 - deep valley
202:39 - so
202:40 - for cases like this
202:42 - there of course we can always fall back
202:44 - to exhaustive exploration
202:46 - it will find the best answer in every
202:49 - single one of those cases
202:51 - but a lot of times we just don't have
202:53 - the time like if i have to brew and
202:56 - measure the pleasure drinking pleasure
202:58 - of 10 million cups of tea to get a good
203:00 - answer to this is not going to happen in
203:02 - my lifetime
203:04 - so
203:05 - luckily there are some things in the
203:06 - middle that are
203:08 - more sample efficient than exhaustive
203:10 - exploration
203:12 - but a little bit more robust than
203:14 - gradient descent things like genetic
203:16 - algorithms simulated annealing things
203:18 - that
203:19 - their defining characteristic is they
203:21 - have a little bit of random jumping
203:23 - around they're a little bit of
203:24 - unpredictability and so they make it
203:26 - harder to slip things by them
203:29 - they all have their strength and
203:30 - weaknesses
203:32 - there tend to be good for different
203:34 - types of problems or different types of
203:36 - pathologies in your loss function
203:40 - but
203:41 - all of them
203:42 - help avoid getting stuck in the local
203:46 - minima the little small valleys that
203:48 - gradient descent will get stuck in
203:51 - they get away with this by making fewer
203:53 - assumptions
203:54 - and they can take a little longer to
203:56 - compute than gradient descent but not
203:58 - nearly so long as exhaustive exploration
204:02 - you can think of gradient descent as
204:04 - being like a formula one race car and if
204:06 - you have a really nice well-behaved
204:07 - track it is fast
204:10 - but if you put a speed bump in the truck
204:11 - you're done
204:13 - um
204:14 - genetic algorithms simulated annealing
204:17 - evolutionary algorithms those are like
204:19 - you know a four-wheel drive pickup truck
204:21 - you can take a fairly rough road with
204:23 - those and get where you're going
204:26 - you won't get there in record time
204:28 - perhaps but you'll get there
204:31 - and then exhaustive exploration is like
204:33 - traveling on foot
204:34 - there is nothing that will stop you from
204:36 - getting anywhere you can travel little
204:38 - or literally anywhere but it just might
204:41 - take you a really really long time
204:46 - so to illustrate how this works
204:49 - imagine we have a model that we would
204:51 - like to optimize
204:53 - we have a research question how many m
204:56 - m's are in a bag of m ms
204:59 - so answering this is easy you buy a bag
205:01 - of m ms you eat it
205:03 - 53 you can count those m ms
205:06 - so
205:07 - great we know how many were in the first
205:09 - bag
205:11 - now when i did this i made a mistake and
205:13 - i bought another bag
205:14 - and i tried that one and i got a
205:16 - different answer so now
205:19 - i can answer 53
205:21 - or i can answer 57
205:24 - either way i'm only right half the time
205:28 - because
205:29 - i can't capture both bags with one
205:31 - answer and i could answer somewhere in
205:33 - the middle but
205:35 - that's never right i have never opened a
205:37 - bag that had 55 m ms in it so it's
205:40 - unclear that that's the right answer
205:41 - either
205:43 - and
205:44 - the situation does not improve with the
205:47 - more bags of m ms that i ate it just
205:49 - gets a little bit
205:51 - out of control and so
205:53 - i change my goal
205:55 - from
205:56 - answering the answer answering the
205:58 - question right to answering the question
206:00 - in a way that is less
206:02 - wrong so in order to do that i have to
206:05 - get really specific about what i mean by
206:08 - how wrong i am
206:10 - and to do that i have
206:13 - this distance function this deviation
206:16 - which is the difference between my
206:19 - actual guess
206:20 - and
206:22 - the actual number of m m's in a bag
206:24 - so we call for for bag number i this
206:27 - deviation is d sub i
206:30 - it's just the difference between the
206:32 - guess and the actual number
206:35 - and then i have to take this deviation
206:36 - and turn it into a cost
206:39 - so one common way to do this is to
206:41 - square it it's nice as the further away
206:44 - things get kind of the more costly it is
206:47 - perceived as and it goes up faster so if
206:50 - there's a bag that's off by twice as
206:53 - much as another the cost is four times
206:56 - so i really it penalizes the things that
206:58 - are way off things that are close it
207:00 - doesn't penalize so much
207:04 - and if we don't care if we don't want to
207:06 - overly
207:07 - penalize the things that are way out
207:09 - there we could use the absolute value of
207:11 - the deviation so if it's off by twice as
207:14 - much it'll just be twice the cost
207:17 - but really we could use anything we
207:19 - could use the square root of the
207:20 - absolute value we could use 10 to the
207:22 - power of the absolute value of this
207:24 - deviation anything that goes up the
207:28 - further away you get from zero
207:32 - we'll stick with squared
207:34 - deviation this is super common it has
207:36 - some nice properties and
207:39 - makes for a good example
207:42 - so for the total cost of any guess that
207:45 - we make if i guess
207:46 - n estimated
207:49 - bags m m's in a bag
207:51 - then the loss function this fancy curly
207:54 - q l of that guess
207:57 - is just adding up
208:00 - the square of the deviation associated
208:03 - with each bag of m ms d1 through dm
208:06 - squared
208:08 - so each deviation is actually the number
208:10 - of m ms in that bag
208:12 - minus the guess
208:14 - so i square that
208:17 - and we can write that with fancy
208:18 - summation notation like this so this is
208:21 - my loss function this is the total cost
208:23 - this is how wrong i am when i make a
208:26 - guess
208:27 - nst
208:30 - so
208:31 - because we have computers you can write
208:33 - a little bit of code and you can do
208:34 - exhaustive exploration and i can say if
208:37 - i guess anything between 40 and 70
208:40 - how wrong would i be with this data and
208:43 - you can plot it and
208:45 - visually we can look at this and we can
208:47 - say hey look there's the lowest value
208:50 - and we can say what is the value
208:54 - of the guess
208:55 - that gives me the lowest loss that's
208:57 - what that argument that
208:59 - um notation means right there
209:01 - and this best guess is just about 55 and
209:04 - a half m ms
209:06 - problem solved
209:08 - so
209:08 - this is an example of numerical
209:11 - optimization where we calculate this
209:14 - loss function and then we can do
209:16 - essentially because it's simulated we
209:17 - can do exhaustive exploration and just
209:20 - pick off the lowest value
209:23 - now for this particular example there's
209:25 - a fun other way to find it
209:28 - we know at the bottom of this curve the
209:31 - slope is 0. it's the only place in the
209:33 - whole curve where that's true where it's
209:36 - flat
209:38 - we can use a little bit of calculus to
209:39 - find that
209:40 - feel free to tune out if calculus is not
209:42 - your thing
209:44 - but it's not too bad so we find the
209:46 - slope
209:48 - of the loss function with respect to our
209:50 - guesses and we set it equal to 0 and we
209:52 - solve it to find
209:54 - what for what guess is that true
209:58 - so we take our loss function this sum of
210:00 - the square of the differences
210:02 - of the count and the guess and we take
210:05 - the derivative of that with respect to
210:08 - our guess
210:10 - and the
210:12 - derivative of a sum is the same as the
210:15 - sum of the derivatives
210:17 - we take the derivative of that just
210:18 - bring down the exponent so two times
210:20 - that
210:21 - summed because all that's equal to zero
210:23 - we can divide it by two and it'll still
210:25 - be true
210:26 - so now the sum
210:28 - of our deviations is 0.
210:32 - so
210:33 - to further simplify this
210:36 - it's the sum of all of the counts of the
210:38 - actual bags times the sum of our guess
210:43 - once for each bag if we have m bags then
210:47 - it's m times our guess
210:50 - and then we can move that to the other
210:52 - side of the equal sign and divide both
210:54 - sides by the number of bags m and what
210:56 - we get is that our best guess
211:00 - is the sum total
211:02 - of the number of m ms we found in all
211:03 - the bags divided by the number of bags
211:06 - or the average count per bag
211:09 - so this is a really slick result and
211:12 - it's things like this that make people
211:14 - so excited about optimization with a
211:16 - little bit of math and calculus you can
211:17 - get this nice theoretical result
211:20 - um now it's worth noting that this is
211:22 - only true if you use a
211:25 - deviation squared as your cost function
211:28 - so that's one reason people like it so
211:30 - much is because it tends to give some
211:31 - nice results like this
211:33 - but there is this analytical shortcut to
211:37 - find what the best guess is we're going
211:40 - to come back to this in a few minutes
211:45 - now
211:46 - how does optimization change
211:49 - how do we use it
211:51 - in our neural network to find these
211:53 - weights and these features
211:57 - so what we want to do
212:00 - we know what our error function is it's
212:02 - how wrong our guesses are
212:04 - so in this case we have a labeled data
212:08 - set which means that a human has already
212:10 - looked at this input on the left and
212:12 - they said hey that's a horizontal image
212:15 - the truth values are what we know
212:17 - should be the right answer
212:19 - zero votes for everything except
212:21 - horizontal that should have a vote of
212:24 - one
212:26 - so let's say
212:27 - initially we've got a neural network
212:30 - that
212:31 - all the weights are random
212:33 - and it gives us
212:34 - nonsense results it says well yeah
212:37 - everything is
212:39 - has some number associated with it but
212:40 - it's nothing like the right answer
212:43 - well we can find the error for each
212:45 - category and add it up and find a total
212:48 - error and this is how wrong our neural
212:51 - network would be for this one example
212:54 - here's our loss here's our error
212:59 - now
213:00 - the idea with gradient descent is we're
213:02 - not just adjusting one thing we're not
213:05 - just adjusting our guess of the number
213:06 - of m ms
213:08 - we're adjusting many things we want to
213:10 - go through and adjust every single
213:13 - weight
213:14 - in every single layer to bring this
213:17 - error down a little bit
213:20 - now
213:21 - that is a little bit challenging to do
213:23 - because in order to do that one thing
213:26 - you can do
213:27 - is
213:28 - find a
213:30 - analytical solution like we did before
213:34 - to go through and
213:37 - move our guess a little bit up and a
213:39 - little bit down and find the slope is
213:40 - really expensive when you consider that
213:42 - this is not a one-dimensional problem
213:44 - anymore it might have hundreds or
213:47 - millions of different weights that we
213:49 - need to adjust
213:50 - so calculating that gradient that slope
213:53 - requires
213:54 - hundreds or millions of more passes
213:57 - through the neural network to find out
213:59 - which direction is downhill
214:03 - enter
214:04 - back propagation
214:06 - so remember we found the nice
214:09 - analytical
214:10 - solution
214:12 - to what we had going on
214:14 - in the case of the
214:17 - m m estimate
214:19 - so we would love to be able to do
214:20 - something like that again if we had an
214:21 - analytical solution we could jump right
214:23 - to the right answer
214:25 - so
214:27 - slope in this case it's
214:29 - change in weight
214:31 - or sorry it's change in error for a
214:33 - given change in weight
214:35 - that's the slope here so there's lots of
214:37 - ways to write that
214:38 - but delta error delta weight d air d
214:41 - wait we'll use this partial error
214:42 - partial weight just because it's most
214:44 - correct but all these things mean the
214:45 - same thing
214:47 - if i change the weight
214:49 - by one
214:50 - how much will the error change what is
214:52 - the slope
214:54 - so in this case it would be -2 and we
214:57 - would know that we need to increase the
215:00 - weight in order to get closer to the
215:03 - bottom this tells us not only the
215:05 - direction we need to move but gives us a
215:07 - sense of about how far we should go
215:10 - doesn't tell us exactly where the bottom
215:12 - is but it tells us which way it needs to
215:13 - adjust
215:15 - now if we do know the error function
215:18 - example we can make
215:20 - a
215:20 - an analytic solution and we can find
215:23 - that we can calculate that slope exactly
215:26 - so in this case
215:27 - the change in error
215:29 - for a given change in weight is just the
215:32 - derivative of our error function here
215:34 - which is in this case is the weight
215:36 - squared
215:36 - so the derivative is two times the
215:38 - weight
215:40 - the weight is minus one and so the
215:42 - answer is oh a slope of minus two that
215:44 - tells us what we need to know about
215:46 - which way to adjust
215:49 - now with neural networks of course
215:51 - they're a lot more complex than that
215:54 - but
215:55 - we can actually
215:57 - analytically
215:58 - compute the slope
216:01 - of the function where we are we don't
216:02 - know where the minimum is but we can
216:04 - find the slope without having to
216:06 - recalculate the value
216:08 - of everything each time and this is how
216:11 - it works
216:12 - imagine the world's most trivial neural
216:15 - network that has one input one output
216:18 - one hidden layer with one neuron in it
216:22 - so it's got an input connected by a
216:24 - weight w1
216:26 - to an intermediate value connected by a
216:28 - weight w2
216:29 - to an output value
216:31 - so the intermediate value is just x
216:33 - times that weight
216:35 - so
216:36 - the derivative
216:38 - of y with respect to the weight is x
216:41 - what that means is if i change
216:43 - w1
216:45 - and i move it by one
216:47 - then the value of y
216:49 - will change by the value
216:52 - x whatever x is
216:55 - we have the slope of this piece
216:58 - of the function
217:01 - similarly it's
217:03 - straightforward we can just read off
217:05 - that whatever the value of y is
217:07 - multiply it by the weight w2 we get e
217:11 - so if we want to find the slope of the
217:13 - error function
217:15 - for a given change in y
217:17 - the answer is w2 if i changed y by one
217:21 - unit
217:22 - then the error changes by the amount w
217:25 - two
217:27 - now chaining
217:29 - means that we can take these two things
217:33 - and just
217:35 - multiply them together
217:37 - so
217:38 - by inspection we can see that
217:41 - in this little neural network
217:42 - if we take x
217:44 - multiply it by w1 multiply that by w2 we
217:48 - get the error e
217:50 - now what we'd like to know is if i
217:52 - change
217:54 - that w1 by a certain amount how much
217:56 - does the error change
217:58 - well in this case we just take that
218:00 - whole expression
218:02 - and take the derivative derivative with
218:04 - respect to w1
218:06 - and
218:08 - fairly trivial bit of calculus it comes
218:10 - out to be x times w2 and
218:13 - what we can see then is we can
218:15 - substitute in these steps
218:18 - this
218:19 - change in y with respect to w1 is the
218:23 - same as x
218:25 - w 2 is the same as the change in error
218:27 - with respect to y
218:29 - and what this breaks down is
218:33 - if we want to step down the chain we
218:35 - want to know how much a change in w1
218:38 - affects the error
218:40 - what is d e d w 1
218:43 - we can actually break it down into steps
218:45 - and say okay well if i change w 1 how
218:47 - much does y change
218:49 - and then if i change y how much does the
218:51 - error change
218:52 - this is chaining and this is what lets
218:55 - us
218:56 - if we know
218:58 - which way we want to change the error
219:00 - it lets us calculate how much we can
219:02 - change this weight to help that happen
219:07 - and there's nothing to prevent us from
219:10 - doing this again and again if i have a
219:12 - weight that's deep into my neural
219:13 - network
219:15 - and i want to know how much my error is
219:17 - going to change if i tweak it
219:20 - up or down
219:21 - i want to know the slope of my loss
219:23 - function with respect to that weight
219:26 - then i can just break it down and say
219:27 - okay well if i change the weight how
219:29 - much does a change if i change a how
219:31 - much does b change if i change b how
219:32 - much does c change and chain it all the
219:35 - way down
219:37 - now it's called back propagation
219:40 - because in order to calculate it we
219:43 - actually need the value at the end we
219:46 - have to start with the error value
219:48 - in order to calculate each of these all
219:51 - the way back down into the depths of the
219:53 - network but still we can do that
219:56 - now the way
219:57 - the reason you have to go backwards
219:59 - is that
220:00 - let's say
220:02 - we want to know what this should be
220:05 - if i change the error if i change a how
220:08 - much does the error change
220:10 - it's like well let's assume that i
220:11 - already know how much the error is going
220:13 - to change if i change b
220:14 - what is this back propagation step what
220:17 - is the additional link i need to add to
220:19 - this chain
220:20 - it's like well it's how much does b
220:22 - change if i change a
220:25 - if they're connected by a weight
220:28 - then how do i incorporate that weight
220:33 - we know that
220:35 - two neurons connected in this way are
220:38 - represented by this
220:39 - b is the weight times the value of a
220:42 - and so we can just take a little
220:43 - derivative here and get
220:45 - the change in b with respect to a
220:48 - is w
220:49 - so this
220:50 - step
220:52 - in the back propagation change
220:54 - can be represented by whatever that
220:56 - weight is
220:57 - cool
220:58 - now we know that we have sums in our
221:00 - neural network that's another thing we
221:01 - have to handle
221:02 - if i know how much my error changes with
221:05 - a change in z
221:07 - then how much would it change
221:09 - with a change in one of the inputs to
221:11 - this to z where that input goes into a
221:14 - sum
221:16 - well i have can write the expression for
221:18 - z
221:19 - adding up all the inputs
221:21 - if i want to know how much z changes
221:23 - with respect to a change in a i just
221:25 - take the derivative
221:26 - and it is turns out to be one
221:28 - so this is a trivial back propagation
221:31 - step
221:33 - now the most interesting one of all
221:37 - if
221:38 - i know how much the error changes with
221:39 - respect to a change in b
221:42 - and then i want to know how much it
221:43 - changes with the input and to that
221:46 - sigmoid function
221:48 - then i can just say okay well a sigmoid
221:50 - function mathematically looks like this
221:54 - and i can take the derivative of b with
221:56 - respect to a
221:59 - and um
222:01 - one of the beautiful things about the
222:02 - sigmoid function is that the derivative
222:04 - actually looks like this
222:05 - um it's just the value of the function
222:08 - times one minus the value of the
222:10 - function which is one of the reasons
222:11 - that sigmoids perhaps are so popular in
222:13 - deep neural networks
222:16 - so this step is also straightforward to
222:18 - calculate
222:20 - in none of these steps have we had to
222:22 - recalculate all of the values in
222:26 - the
222:27 - neural network
222:28 - we've been able to rely on things that
222:30 - have already been calculated what the
222:31 - values are
222:33 - at each of these neurons
222:35 - that's what makes
222:37 - back propagation so mathematically
222:39 - efficient and is what allows us
222:42 - to
222:44 - efficiently train neural networks
222:47 - that is why each
222:49 - element in a neural network no matter
222:51 - how exotic it is
222:53 - needs to remain differentiable so that
222:56 - we can go through this exercise of
222:59 - finding what the link in the chain is
223:02 - when we're doing the chain rule on our
223:04 - derivatives so that we can compute the
223:06 - back propagation we can back propagate
223:08 - it
223:10 - and again rectified linear units
223:12 - if we know how much the output
223:14 - affects a change in error we want to
223:17 - know how that extends to the input we
223:19 - can write the function of a rectified
223:20 - linear unit we can take the derivative
223:23 - of it and then use that
223:25 - in our chain rule
223:29 - so imagine now
223:31 - we have this labeled example
223:34 - we calculate the answer that this random
223:38 - neural network that's not special at all
223:40 - it'll give an answer that's completely
223:42 - wrong
223:43 - and then we back propagate the error and
223:46 - adjust every one of those weights a
223:49 - little bit in the right direction
223:51 - and we do that again and again after you
223:54 - do that a few thousand times this
223:56 - stochastic gradient descent goes from
223:59 - this fully connected totally random
224:02 - neural network to something that is a
224:04 - lot more efficient and that is able to
224:07 - give answers that are much closer to the
224:10 - right answer
224:14 - so coming back up
224:16 - to our convolutional neural networks
224:19 - these are the fully connected layers
224:22 - that's how they're trained
224:24 - they can also be stacked
224:27 - this back propagation
224:29 - up applies not only to these fully
224:31 - connected layers but also to the
224:33 - convolutional layers and the pooling
224:35 - layers we won't go through and calculate
224:37 - the chain rule for them but you can do
224:39 - that as well
224:40 - and
224:42 - going through this this whole
224:44 - stack
224:45 - of different layers gets trained on a
224:48 - bunch of examples in this cases of
224:50 - labeled x's and o's give it a bunch of
224:53 - inputs that we know the right answer to
224:55 - and we let it adjust all of those
224:58 - connections
224:59 - not only that it also adjusts all of the
225:04 - pixels in the features for each
225:07 - convolutional layer so it learns not
225:09 - only the weights but also the features
225:13 - and then over time those representations
225:16 - become
225:17 - something that lets it predict very well
225:19 - what is an x and what is an o
225:23 - on top of that there are other things
225:26 - that we can use optimization for so
225:29 - there's a bunch of decisions here that
225:30 - we haven't addressed yet how do we know
225:33 - how many features to put in each
225:34 - convolutional layer
225:36 - how do we know how big those should be
225:37 - how many pixels on a side how do we
225:40 - choose the
225:41 - size and stride of our pooling windows
225:46 - in our fully connected layers how many
225:48 - layers do we have and how many hidden
225:49 - neurons do we put in each
225:52 - each of these decisions
225:54 - are called hyper parameters they are
225:56 - also values that we get to choose but
225:58 - they're the next level up they kind of
226:00 - control how everything happens below
226:03 - and in order to
226:04 - see how well they perform we have to
226:06 - train the whole thing on all the images
226:08 - start to finish
226:09 - so but the same principles apply we can
226:12 - adjust these
226:14 - and choose them
226:15 - to get the best
226:17 - result possible
226:19 - in a lot of cases it's worth pointing
226:21 - out that there's just not enough
226:23 - computation available in the world to
226:25 - try out all the possible examples
226:28 - and so what we have right now
226:31 - are some recipes some things that
226:34 - researchers have stumbled onto that seem
226:36 - to work well
226:37 - and they get reused
226:39 - but there are a lot of places a lot of
226:42 - combinations of these hyper parameters
226:44 - that actually haven't been tried yet
226:46 - and so there is always the possibility
226:49 - that there are some combinations that
226:50 - work even much better than what we've
226:52 - seen so far
226:57 - now
226:58 - uh
226:59 - we don't have to use convolutional
227:00 - neural networks for just images
227:03 - any
227:04 - two-dimensional or three-dimensional
227:06 - data
227:07 - works well
227:09 - the thing that matters
227:10 - is that in this data
227:12 - things that are closer together
227:14 - are more closely related than things far
227:16 - away
227:17 - it matters if two things are in adjacent
227:19 - rows or in adjacent columns
227:22 - so in images this is plainly the case
227:25 - the location of a pixel in an array of
227:28 - pixels is part of the information if you
227:30 - were to randomly jumble the rows and
227:32 - columns that would lose the information
227:35 - that's there
227:36 - that's what makes this well suited to
227:37 - convolutional neural networks
227:42 - anything you can make look like an image
227:45 - may also be suited to convolutional
227:47 - neural networks
227:48 - for instance if you're working with
227:50 - audio
227:52 - you have a really nice
227:53 - x-axis your columns can be subsequent
227:57 - time steps
227:58 - you don't want to jumble those because
228:00 - the time the order in which things occur
228:02 - in time matters
228:04 - and you can make your rows
228:07 - the intensity in different frequency
228:09 - bands going from low frequency to high
228:12 - frequency
228:13 - again the order matters there
228:16 - and so being able to take sound then and
228:20 - make it look like an image you can apply
228:22 - this processing to it and find patterns
228:24 - in the sound that you wouldn't be able
228:26 - to find conveniently any other way
228:30 - you can also do this with text with a
228:32 - little bit of work you can make each of
228:34 - your rows a different word in the
228:36 - dictionary and then you can make your
228:38 - columns the position and sentence or
228:40 - position
228:41 - location that occurs in time
228:46 - now there are some limitations here
228:50 - convolutional neural networks only
228:52 - capture local spatial patterns so if
228:56 - your data can't be made to look like an
228:58 - image or if it doesn't make sense to
229:00 - then they're less useful
229:02 - so for example imagine you have customer
229:04 - data
229:05 - that has columns representing things
229:07 - like names and ages addresses emails
229:10 - purchases transactions browsing
229:12 - histories and these customers are listed
229:15 - if you were to rearrange the rows
229:18 - or rearrange the columns
229:20 - the information itself wouldn't really
229:23 - be compromised it would all still be
229:25 - there it would also be queryable
229:26 - searchable and interpretable
229:29 - convolutional neural networks don't help
229:30 - you here they look for spatial patterns
229:33 - so if the spatial organization of your
229:35 - data is not important it will not be
229:38 - able to find
229:39 - what matters
229:42 - so rule of thumb if your data is just as
229:45 - useful
229:46 - after swapping your columns with each
229:48 - other
229:49 - then you can't use convolutional neural
229:51 - networks you shouldn't use convolutional
229:52 - neural networks that's a big takeaway
229:54 - from this
229:57 - so they're really good at finding
229:59 - patterns and using them to classify
230:01 - images that is what is they are the best
230:03 - at
230:05 - now the takeaway from this is not that
230:07 - you should go and code up your own
230:09 - convolutional neural networks from
230:10 - scratch
230:12 - um you can it's a great exercise it's a
230:14 - lot of fun but when you go to actually
230:16 - use it there are a lot of mature tools
230:18 - out there that are
230:21 - helpful and just waiting to be applied
230:22 - to this
230:24 - the takeaway from this is that you will
230:26 - be asked to make a lot of subtle
230:28 - decisions
230:29 - about how to prepare your data
230:31 - and feed it in how to interpret the
230:33 - results and how to choose these hyper
230:35 - parameters for this
230:37 - it helps to know
230:39 - what's going to be done with your data
230:41 - and what it all means so you can get the
230:43 - most out of these tools
230:47 - all right good luck

Cleaned transcript:

neural networks are good for learning lots of different types of patterns to give an example of how this would work i imagine you had a four pixel camera so not not four megapixels but just four pixels and it was only black and white and you wanted to go around and take pictures of things and determine automatically then whether these pictures were of a solid allwhite or alldark image a vertical line or a diagonal line or a horizontal line this is tricky because you can't do this with simple rules about the brightness of the pixels both of these are horizontal lines but if you tried to make a rule about which pixel was bright and which was dark you wouldn't be able to do it so to do this with the neural network you start by taking all of your inputs in this case are four pixels and you break them out into input neurons and you assign a number to each of these depending on the brightness or darkness of the pixel plus one is all the way white minus one is all the way black and then gray is zero right in the middle so these values once you have them broken out and listed like this on the input neurons it's also called the input vector or array it's just a list of numbers that represents your inputs right now it's a useful notion to think about the receptive field of a neuron all this means is what set of inputs makes the value of this neuron as high as it can possibly be for input neurons this is pretty easy each one is associated with just one pixel and when that pixel is all the way white the value of that input neuron is as high as it can go the black and white checkered areas show pixels that an input neuron doesn't care about if they're all the way white or all the way black it still doesn't affect the value of that input neuron at all now to build a neural network we create a neuron the first thing this does is it adds up all of the values of the input neurons so in this case if we add up all of those values we get a 0.5 now to complicate things just a little bit each of the connections are weighted meaning they're multiplied by a number that number can be one or minus one or anything in between so for instance if something has a weight of minus one it's multiplied and you get the negative of it and that's added in if something has a weight of zero then it's effectively ignored so here's what those weighted connections might look like and you'll notice that after the values of the input neurons are weighted and added the values can the final value is completely different graphically it's convenient to represent these weights as white links being positive weights black links being negative weights and the thickness of the line is roughly proportional to the magnitude of the weight then after you add the weighted input neurons they get squashed and i'll show you what that means you have a sigmoid squashing function sigmoid just means sshaped and what this does is you put a value in let's say 0.5 and you run a vertical line up to your sigmoid and then a horizontal horizontal line over from where it crosses and then where that hits the y axis that's the output of your function so in this case slightly less than 0.5 it's pretty close as your input number gets larger your output number also gets larger but more slowly and eventually no matter how big the number you put in the answer is always less than one similarly when you go negative the answer is always greater than negative one so this ensures that that neuron's value never gets outside of the range of plus one to minus one which is helpful for keeping the computations in the neural network bounded and stable so after you sum the weighted values of the neurons and squash the result you get the output in this case 0.746 that is a neuron so we can call this we can collapse all that down and this is a neuron that does a weighted sum and squash the result and now instead of just one of those assume you have a whole bunch there are four shown here but there could be 400 or 4 million now to keep our picture clear we'll assume for now that the weights are either plus one white lines minus one black lines or zero in which case they're missing entirely but in actuality all of these neurons that we created are each attached to all of the input neurons and they all have some weight between minus one and plus one when we create this first layer of our neural network the receptive fields get more complex for instance here each of those end up combining two of our input neurons and so the value the receptive field the pixel values that make that first layer neuron as large as it can possibly be look now like pairs of pixels either all white or a mixture of white and black depending on the weights so for instance this neuron here is attached to this input pixel which is upper left and this input pixel which is lower left and both of those weights are positive so it combines the two of those and that's its receptive field the receptive field of this one plus the receptive field of this one however if we look at this neuron it combines our this pixel upper right and this pixel lower right it has a weight of minus one for the lower right pixel so that means it's most active when this pixel is black so here is its receptive field now the because we were careful of how we created that first layer its values look a lot like input values and we can turn right around and create another layer on top of it the exact same way with the output of one layer being the input to the next layer and we can repeat this three times or seven times or 700 times for additional layers each time the receptive fields get even more complex so you can see here using the same logic now they cover all of the pixels and more uh more special arrangement of which are black and which are white we can create another layer again all of these neurons in one layer are connected to all of the neurons in the previous layer but we're assuming here that most of those weights are zero and not shown it's not generally the case so just to mix things up we'll create a new layer but if you notice our squashing function isn't there anymore we have something new called a rectified linear unit this is another popular neuron type so you do your weighted sum of all your inputs and instead of squashing you do rectified linear units you rectify it so if it is negative you make the value zero if it's positive you keep the value this is obviously very easy to compute and it turns out to have very nice stability properties for neural networks as well in practice so after we do this because some of our weights are positive and some are negative connecting to those rectified linear units we get receptive fields and they're opposites if you look at the patterns there and then finally when we've created as many layers with as many neurons as we want we create an output layer here we have four outputs that we're interested in is the image solid vertical diagonal or horizontal so to walk through an example here of how this would work let's say we start with this input image shown on the left dark pixels on top white on the bottom as we propagate that to our input layer this is what those values would look like the top pixels the bottom pixels as we move that to our first layer we can see the combination of a dark pixel and a light pixel some together get us zero gray whereas down here we have the combination of a dark pixel plus a light pixel with a negative weight so that gets us a value of negative one there which makes sense because if we look at the receptive field here upper left pixel white lower left pixel black is the exact opposite of the input that we're getting and so we would expect its value to be as low as possible minus one as we move to the next layer we see the same types of things combining zeros to get zeros um combining a negative and a negative with a negative weight which makes a positive to get a zero and here we have combining two negatives to get a negative so again you'll notice the receptive field of this is exactly the inverse of our input so it makes sense that its weight would be negative or its value would be negative and we move to the next layer all of these of course these zeros propagate forward here this is a negative has a negative value and it gets has a positive weight so it just moves straight forward because we have a rectified linear unit negative values become zero so now it is zero again two but this one gets rectified and becomes positive negative times a negative is positive and so when we finally get to the output we can see they're all zero except for this horizontal which is positive and that's the answer our neural network said this is an image of a horizontal line now neural networks usually aren't that good not that clean so there's a notion of with an input what is truth in this case the truth is this has a zero for all of these values but a one for horizontal it's not solid it's not vertical it's not diagonal yes it is horizontal an arbitrary neural network will give answers that are not exactly truth it might be off by a little or a lot and then the error is the magnitude of the difference between the truth and the answer given and you can add all these up to get the total error for the neural network so the idea the whole idea with learning and training is to adjust the weights to make the error as low as possible so the way this is done is we put an image in we calculate the error at the end then we look for how to adjust those weights higher or lower to either make that error go up or down and we of course adjust the weights in the way that make the error go down now the problem with doing this is each time we go back and calculate the error we have to multiply all of those weights by all of the neuron values at each layer and we have to do that again and again once for each weight this takes forever in computing terms on computing scale and so it's not a practical way to train a big neural network you can imagine instead of just rolling down to the bottom of a simple valley we have a very high dimensional valley and we have to find our way down and because there are so many dimensions one for each of these weights that the computation just becomes prohibitively expensive luckily there was an insight that lets us do this in a very reasonable time and that's that if we're careful about how we design our neural network we can calculate the slope directly the gradient we can figure out the direction that we need to adjust the weight without going all the way back through our neural network and recalculating so just to review the slope that we're talking about is when we make a change in weight the error will change a little bit and that relation of the change in weight to the change in error is the slope mathematically there are several ways to write this we'll favor the one on the bottom it's technically most correct we'll call it dedw for shorthand every time you see it just think the change in error when i change a weight or the change in the thing on the top when i change the thing on the bottom um this is uh does get into a little bit of calculus we do take derivatives that's how we calculate slope if it's new to you i strongly recommend a good semester of calculus just because the concepts are so universal and uh a lot of them have very nice physical interpretations which i find very appealing but don't worry otherwise just gloss over this and pay attention to the rest and you'll get a general sense for how this works so in this case if we change the weight by plus one the error changes by minus two which gives us a slope of minus two that tells us the direction that we should adjust our weight and how much we should adjust it to bring the error down now to do this you have to know what your error function is so assume we had an error function that was the square of the weight and you can see that our weight is right at 1 so the first thing we do is we take the derivative change in error divided by change in weight d e d w the derivative of weight squared is two times the weight and so we plug in our weight of minus one and we get a slope d e d w of minus two now the other trick that lets us do this with deep neural networks is chaining and to show you how this works imagine a very simple trivial neural network with just one hidden layer one input layer one output layer and one weight connecting each of them so it's obvious to see that the value y is just the value x times the weight connecting them w1 so if we change w1 a little bit we just take the derivative of y with respect to w1 and we get x the slope is x if i change w1 by a little bit then y will change by x times the size of that adjustment similarly for the next step we can see that e is just the value y times the weight w2 and so when we calculate d e d y it's just w 2. because this network is so simple we can calculate from one end to the other x times w1 times w2 is the error e and so if we want to calculate how much will the error change if i change w1 we just take the derivative of that with respect to w1 and get x times w2 so this illustrates you can see here now that what we just calculated is actually the product of our first derivative that we took the the dydw1 times the derivative for the next step d e d y multiplied together this is chaining you can calculate the slope of each tiny step and then multiply all of those together to get the slope of the full chain the derivative of the full chain so in a deeper neural network what this would look like is if i want to know how much the error will change if i adjust a weight that's deep in the network i just calculate the derivative of each tiny little step all the way back to the weight that i'm trying to calculate and then multiply them all together this computationally is many many times cheaper than what we had to do before of recalculating the error for the whole neural network for every weight now in the neural network that we've created there are several types of back propagation we have to do there are several operations we have to do for each one of those we have to be able to calculate the slope so for the first one is just a weighted connection between two neurons a and b so let's assume we know the change in error with respect to b we want to know the change in error with respect to a to get there we need to know db da so to get that we just write the relationship between b and a take the derivative of b with respect to a we get the weight w and now we know how to make that step we know how to do that little nugget of back propagation another element that we've seen is sums all of our neurons sum up a lot of inputs to take this back propagation step we do the same thing we write our expression and then we take the derivative of our endpoint z with respect to our step that we're uh propagating to a and dz da in this case is just one which makes sense if we have a sum of a whole bunch of elements we increase one of those elements by one we expect the sum to increase by one that's the definition of a slope of one one to one relation there another element that we have that we need to be able to back propagate is the sigmoid function so this one's a little bit more interesting mathematically we'll just write it shorthand like this the sigma function um it is entirely feasible to uh go through and take the derivative of this analytically and calculate it it just so happens that this function has a nice property that to get its derivative you just multiply it by 1 minus itself so this is very straightforward to calculate another element that we've used is the rectified linear unit again to figure out how to back propagate this we just write out the relation b is equal to a if a is positive otherwise it's zero and piecewise for each of those we take the derivative so db da is either one if a is positive or zero and so with all of these little back propagation steps and the ability to chain them together we can calculate the effect of adjusting any given weight on the error for any given input and so to train then we start with a fully connected network we don't know what any of these weights should be and so we assign them all random values we create a completely arbitrary random neural network we put in an input that we know the answer to we know whether it's solid vertical diagonal or horizontal so we know what truth should be and so we can calculate the error then we run it through calculate the error and using back propagation go through and adjust all of those weights a tiny bit in the right direction and then we do that again with another input and again with another input for if we can get away with it many thousands or even millions of times and eventually all of those weights will gravitate they'll roll down that many dimensional valley to a nice low spot in the bottom where it performs really well and does pretty close to truth on most of the images if we're really lucky it'll look like what we started with with intuitively um understandable receptive fields for those neurons and a relatively sparse representation meaning that most of the weights are small or close to zero and it doesn't always turn out that way but what we are guaranteed is that it'll find a pretty good representation of you know the best that it can do adjusting those weights to get as close as possible to the right answer for all of the inputs so what we've covered is just a very basic introduction to the principles behind neural networks i haven't told you quite enough to be able to go out and build one of your own but if you're feeling motivated to do so i highly encourage it here are a few resources that you'll find useful you'll want to go and learn about bias neurons dropout is a useful training tool there are several resources available from andre carpathi who is an expert in neural networks and great at teaching about it also there's a fantastic article called the black magic of deep learning that just has a bunch of practical from the trenches tips on how to get them working well neural networks are famously difficult to interpret it's hard to know what they're actually learning when we train them so let's take a closer look and see whether we can get a good picture of what's going on inside just like every other supervised machine learning model neural networks learn relationships between input variables and output variables in fact we can even see how it's related to the most iconic model of all linear regression simple linear regression assumes a straightline relationship between an input variable x and an output variable y x is multiplied by a constant m which also happens to be the slope of the line and it's added to another constant b which happens to be where the line crosses the y axis we can represent this in a picture our input value x is multiplied by m our constant b is multiplied by one and then they get added together to get y this is a graphical representation of y equals m x plus b on the far left the circular symbols just indicate that the value is passed through the rectangles l labeled m and b indicate that whatever goes in on the left comes out multiplied by m or b on the right and the box with the capital sigma indicates that whatever goes in on the left gets added together and spit out on the right we can change the names of all the symbols for a different representation this is still a straight line relationship we've just changed the names of all the variables the reason we're doing this is to translate our linear regression into the notation we'll use in neural networks this will help us keep track of things as we move forward at this point we have turned a straight line equation into a network a network is anything that has nodes connected by edges in this case x sub 0 and x sub 1 are our input nodes v sub 0 is an output node and our weights connecting them are edges this is not the traditional sense of a graph meaning a plot or a grid like in a graphing calculator or graph paper it's just the formal word for a network for nodes connected by edges another piece of terminology you might hear is a directed acyclic graph abbreviated as dag or dag a directed graph is one where the edges just go in one direction in our case input goes to output but output never goes back to input our edges are directed acyclic means that you can't ever draw a loop once you have visited a node there's no way to jump from edges to nodes to edges to nodes to get back to where you started everything flows in one direction through the graph we can get a sense of the type of models that this network is capable of learning by choosing random values for the weights w sub 0 0 and w sub 1 and then seeing what relationship pops out between x sub 1 and v sub 0. remember that we set x of 0 equal to 1 and are holding it there always this is a special node called a bias node it should come as no surprise that the relationships that come out of this linear model are all straight lines after all we've taken our equation for the line and rearranged it but we haven't changed it in any substantial way there's no reason we have to limit ourselves to just one input variable we can add an additional one now here we have an x of 0 an x sub 1 and an x sub 2. we draw an edge between x sub 2 and our summation with the weight w sub 2 0 x sub 2 times w sub 2 0 is again u sub 2 0 and all of our u's get added together to make a v sub 0. and we could add more inputs as many as we want this is still a linear equation but instead of being two dimensional we can make it three dimensional or higher writing this out mathematically could get very tedious so we'll use a shortcut we'll substitute the subscript i for the index of the input it's the number of the input we're talking about this allows us to write u sub i zero where our u sub i equals x sub i times w sub i zero and again our output v sub zero is just the summation over all values of i of u sub i zero for this three dimensional case we can again look at the models that emerge when we randomly choose our w sub i zeros our weights as we would expect we still get the three dimensional equivalent of a line a plane in this case and if we were to extend this to more inputs we would get the m dimensional equivalent of a line which is called an mdimensional hyperplane so far so good now we can start to get fancier our input x sub 1 looks a lot like our output v sub zero in fact there's nothing to prevent us from taking our output and then using it as an input to another network just like this one now we have two separate identical layers we can add a subscript roman numeral i and a subscript roman numeral i i or two to our equations depending on which layer we're referring to and we just have to remember that our x sub 1 in layer 2 is the same as our v sub 0 in layer 1. because these equations are identical and each our layer each of our layers work just the same we can reduce this to one set of equations adding a subscript capital l to represent which layer we're talking about as we continue here we'll be assuming that all the layers are identical and to keep the equations cleaner we'll leave out the capital l but just keep in mind that if we were going to be completely correct and verbose we would add the l subscript onto the end of everything to specify the layer it belongs to now that we have two layers there's no reason that we can't connect them in more than one place instead of our first layer generating just one output we can make several outputs in our diagram we'll add a second output v sub 1 and we'll connect this to a third input into our second layer x sub 2. keep in mind that the x sub 0 input to every layer will always be equal to 1. that bias node shows up again in every layer now there are two nodes shared by both layers we can modify our equations accordingly to specify which of the shared nodes we are talking about they behave exactly the same so we can be efficient and reuse our equation but we can specify subscript j to indicate which output we're talking about so now if i'm connecting the ith input to the jth output then i and j will determine which weight is applied and which u's get added together to create the output v sub j and we can do this as many times as we want we can add as many of these shared nodes as we care to the model as a whole only knows about the input x sub 1 into the first layer and the output v sub 0 of the last layer from the point of view of someone sitting outside the model the shared nodes between layer 1 and layer 2 are hidden they are inside the black box because of this they're called hidden nodes we can take this two layer linear network create a hundred hidden nodes set all of the weights randomly and see what model it produces even after adding all of this structure the resulting models are still straight lines in fact it doesn't matter how many layers you have or how many hidden nodes each layer has any combination of these linear elements with weights and sums will always produce a straight line result this is actually one of the traits of linear computation that makes it so easy to work with but unfortunately for us it also makes really boring models sometimes a straight line is good enough but that's not why we go to neural networks we're going to want something a little more sophisticated in order to get more flexible models we're going to need to add some nonlinearity we'll modify our linear equation here after we calculate our output v sub 0 we subject it to another function f which is not linear and we'll call the result y sub zero one really common nonlinear function to add here is the logistic function it's shaped like an s so sometimes it's called a sigmoid function too although that can be confusing because technically any function shaped like an s is a sigmoid we can get a sense of what logistic functions look like by choosing random weights for this one input one output one layer network and meeting the family one notable characteristic of logistic functions is that they live between zero and one for this reason they're also called squashing functions you can imagine taking a straight line and then squashing the edges and bending and hammering it down so that the whole thing fits between zero and one no matter how far out you go working with logistic functions brings us to another connection with machine learning models logistic regression this is a bit confusing because regression refers to finding a relationship between an input and an output usually in the form of a line or a curve or a surface of some type logistic regression is actually used as a classifier most of the time it finds a relationship between a continuous input variable and a categorical output variable it treats observations of one category as zeros treats observations of the other category as ones and then finds the logistic function that best fits all those observations then to interpret the model we add a threshold often around 0.5 and wherever the curve crosses the threshold there's a demarcation line everything to the left of that line is predicted to fall into one category and everything to the right of that line is predicted to fall into the other this is how a regression algorithm gets modified to become a classification algorithm as with linear functions there's no reason not to add more inputs we know that logistic regression can work with many input variables and we can represent that in our graph as well here we just add one in order to keep the plot three dimensional but we could add as many as we want to see what type of functions this network can create we can choose a bunch of random values for the weights as you might have expected the functions we create are still sshaped but now they're threedimensional they look like a tablecloth laid across two tables of unequal height more importantly if you look at the contour lines projected down onto the floor of the plot you can see that they are all perfectly straight the result of this is that any threshold we choose for doing classification will split our input space up into two halves with the divider being a straight line this is why logistic regression is is described as a linear classifier whatever the number of inputs you have whatever dimensional space you're working in logistic regression will always split it into two halves using a line or a plane or a hyperplane of the appropriate dimensions another popular nonlinear function is the hyperbolic tangent it's closely related to the logistic function and can be written in a very symmetric way we can see when we choose some random weights and look at examples that hyperbolic tangent curves look just like logistic curves except that they vary between 1 and plus 1. just like we tried to do before with linear functions we can use the output of one layer as the input to another layer we can stack them in this way and can even add hidden nodes the same way we did before here we just show two hidden nodes in order to keep the diagram simple but you can imagine as many as you want there when we choose random weights for this network and look at the output we find that things get interesting we've left the realm of the linear because the hyperbolic tangent function is nonlinear when we add them together we get something that doesn't necessarily look like a hyperbolic tangent we get curves wiggles peaks and valleys and a much wider variety of behavior than we ever saw with single layer networks we can take the next step and add another layer to our network now we have a set of hidden nodes between layer 1 and layer 2 and another set of hidden nodes between layer 2 and layer 3. again we choose random values for all the weights and look at the types of curves it can produce again we see wiggles and peaks valleys and a wide selection of shapes if it's hard to tell the difference between these curves and the curves generated by a twolayer network that's because they're mathematically identical we won't try to prove it here but there's a cool result that shows that any curve you can create learning a menu using a many layered network you can also create using a two layer network as long as you have enough hidden nodes the advantage of having a many layered network is that it can help you create more complex curves using fewer total nodes for instance in our two layer network we used a hundred hidden nodes in our three layer network we used 11 hidden nodes in the first layer and nine hidden nodes in the second layer that's only a fifth of the total number we used in our two layer network but the curves it produces show similar richness we can use these fancy wiggly lines to make a classifier as we did with logistic regression here we use the zero line as the cutoff everywhere that our curve crosses the zero line there's a divider in every region that the curve sits above the zero line we'll call this category a and similarly everywhere the curve is below the zero line we have category b what distinguishes these nonlinear classifiers from linear ones is that they don't just split the space into two halves in this example regions of a and b are interleaved building a classifier around a multilayer nonlinear network gives it a lot more flexibility it can learn more complex relations this particular combination of multilayer network with hyperbolic tangent nonlinear function has its own name a multilayer perceptron as you can guess when you have only one layer it's just called a perceptron and in that case you don't even need to add the nonlinear function to make it work the function will still cross the xaxis at all the same places here is the full network diagram of a multilayer perceptron this representation is helpful because it makes every single operation explicit however it's also visually cluttered it's difficult to work with because of this it's most often simplified to look like circles connected by lines this implies all the operations we saw on the previous diagram connecting lines each have a weight associated with them hidden nodes and output nodes perform summation and nonlinear squashing but in this diagram all of that is implied in fact our bias nodes the nodes that always have a value of one in each layer are omitted for clarity so our original network reduces to this the bias nodes are still present and their operation hasn't changed at all but we leave them out to make a cleaner picture we only show two hidden nodes from each layer here but in practice we used quite a few more again to make the diagram as clean as possible we often don't show all the hidden nodes we just show a few and the rest are implied here's a generic diagram then for a three layer single input single output network notice that if we specify the number of inputs the number of outputs and the number of layers and the number of hidden nodes in each layer then we can fully define a neural network we can also take a look at a two input single output neural network because it has two inputs when we plot its outputs it will be a three dimensional curve we can once again choose random weights and generate curves to see what types of functions this neural network might be able to represent this is where it gets really fun with multiple inputs multiple layers and nonlinear activation functions neural networks can make really crazy shapes it's almost correct to say that they could make any shape you want it's worth taking a moment though to notice what its limitations are first notice that all of the functions fall between plus and minus one the dark red and the dark green regions kiss the floor and the ceiling of this range but they never cross it this neural network would not be able to fit a function that extended outside of this range also notice that these functions all tend to be smooth they have hills and dips and valleys and wiggles and even points and wells but it all happens relatively smoothly if we hope to fit a function with a lot of jagged jumps and drops this neural network might not be able to do a very good job of it however aside from these two limitations the variety of functions that this neural network can produce is a little mindboggling we modified a single output neural network to be a classifier when we looked at the multilayer perceptron now there's another way to do this we can use a two output neural network instead outputs of a three layer one input to output neural network like this we can see that there are many cases where the two curves cross and in some instances they cross in several places we can use this to make a classifier wherever the one output is greater than another it can signify that one category dominates another graphically wherever the two output functions cross we can draw a vertical line this chops up the input space into regions in each region one output is greater than the other for instance wherever the blue line is greater we can assign that to be category a then wherever the peach colored line is greater those regions are category b just like the multilayer perceptron this lets us chop the space up in more complex ways than a linear classifier could regions of category a and category b can be shuffled together arbitrarily when you only have two outputs the advantages of doing it this way over a multilayer perceptron with just one output are not at all clear however if you move to three or more outputs the story changes now we have three separate outputs and three separate output functions we can use our same criterion of letting the function with the maximum value determine the category we start by chopping up the input space according to which function has the highest value each function represents one of our categories we're going to assign our first function to be category a and label every region where it's on top as category a then we can do the same with our second function and our third using this trick we are no longer limited to two categories we can create as many output nodes as we want and learn and chop up the input space into that many categories it's worth pointing out that the winning category may not be the best by very much in some cases you can see they can be very close one category will be declared the winner but the next runner up may be almost as good a fit there's no reason that we can't extend this approach to two or more inputs unfortunately it does get harder to visualize you have to imagine several of these lumpy landscape plots on top of each other and in some regions one will be greater than the others in that region that category associated with that output will be dominant to get a qualitative sense for what these regions might look like you can look at the projected contours on the floor of these plots in the case of a multilayer perceptron these plots are all sliced at the y equals zero level that means if you look at the floor of the plot everything in any shade of green will be one category and everything in any shade of red will be the other category the first thing that jumps out about these category boundaries is how diverse they are some of them are nearly straight lines albeit with a small wiggle some of them have wilder bends and curves and some of them chop the input space up into several disconnected regions of green and red sometimes there's a small island of green or island of red in the middle of a sea of the other color the variety of boundaries is what makes this such a powerful classification tool the one limitation we can see looking at it this way is that the boundaries are all smoothly curved sometimes those curves are quite sharp but usually they're gentle and rounded this shows the natural preference that neural networks with hyperbolic tangent activation functions have for smooth functions and smooth boundaries the goal of this exploration was to get an intuitive sense for what types of functions and category boundaries neural networks can learn when used for regression or classification we've seen both their power and their distinct preference for smoothness we've only looked at two nonlinear activation functions logistic and hyperbolic tangent both of which are very closely related there are lots of others and some of them do a bit better at capturing sharp nonlinearities rectified linear units or relu's for instance produce surfaces and boundaries that are quite a bit sharper but my hope was to seed your intuition with some examples of what's actually going on under the hood when you train your neural network here are the most important things to walk away with neural networks learn functions and can be used for regression some activation functions limit the output range but as long as that matches the expected range of your outputs it's not a problem second neural networks are most often used for classification they've proven pretty good at it third neural networks tend to create smooth functions when used for regression and smooth category boundaries when used for classification fourth for fully connected vanilla neural networks a twolayer network can learn any function that a deep network can learn however a deep network might be able to learn it with fewer nodes fifth making sure that inputs are normalized that is they have a mean near zero and a standard deviation of less than one this helps neural networks to be more sensitive to their relationships i hope this helps you as you jump into your next project happy building welcome to how convolutional neural networks work convolutional neural networks or convnets or cnns can do some pretty cool things if you feed them a bunch of pictures of faces for instance they'll learn some basic things like edges and dots bright spots dark spots and then because they're a multilayer neural network that's what gets learned in the first layer the second layer are things that are recognizable as eyes noses mouths and the third layer are things that look like faces similarly if you feed it a bunch of images of cars down to the lowest layer you'll get things again that look like edges and then higher up look at things that look like tires and wheel wells and hoods and at a level above that things that are clearly identifiable as cars cnn's can even learn to play video games by forming patterns of the pixels as they appear on the screen and learning what is the best action to take when it sees a certain pattern a cnn can learn to play video games in some cases far better than a human ever could not only that if you take a couple of cnns and have them set to watching youtube videos one can learn objects by again picking out patterns and the other one can learn types of grasps this then coupled with some other execution software can let a robot learn to cook just by watching youtube so there's no doubt cnns are powerful usually when we talk about them we do so in the same way we might talk about magic but they're not magic what they do is based on some pretty basic ideas applied in a clever way so to illustrate these we'll talk about a very simple toy convolutional neural network what this one does is takes in an image a twodimensional array of pixels you can think of it as a checkerboard and each square on the checkerboard is either light or dark and then by looking at that the cnn decides whether it's a picture of an x or of an o so for instance on top there we see an image with an x drawn in white pixels on a black background and we would like to identify this as an x and the o we'd like to identify as an o so how a cnn does this is uh has several steps in it what makes it tricky is that the x is not exactly the same every time the x or the o can be shifted it can be bigger or smaller can be rotated a little bit thicker or thinner and in every case we would still like to identify whether it's an x or an o now the reason that this is challenging is because for us deciding whether these two things are similar is straightforward we don't even have to think about it for a computer it's very hard what a computer sees is this checkerboard this twodimensional array as a bunch of numbers ones and minus ones a one is a bright pixel a minus one is a black pixel and what it can do is go through pixel by pixel and compare whether they match or not so to computer to a computer it looks like there are a lot of pixels that match but some that don't quite a few that don't actually and so it might look at this and say ah i'm really not sure whether these are the same and so it would because the computer is so literal i would say uncertain i can't say that they're equal now one of the tricks that convolutional neural networks use is to match parts of the image rather than the whole thing so if you break it down into its smaller parts or features then it becomes much more clear whether these two things are similar so examples of these little features are little mini images in this case just three pixels by three pixels the one on the left is a diagonal line slanting downward from left to right the one on the right is also a diagonal line slanting in the other direction and the one in the middle is a little x these are little pieces of the bigger image and you can see as we go through if you choose the right feature and put it in the right place it matches the image exactly so okay we have the bits and pieces now to take a step deeper there the math behind matching these is called filtering and the way this is done is a feature is lined up with the little patch of the image and then one by one the pixels are compared they're multiplied by each other and then add it up and divide it by the total number of pixels so to step through this to see why it makes sense to do this you can see starting in the upper lefthand pixel in both the feature and the image patch multiplying one by a one gives you a one and we can keep track of that by putting that in the position of the pixel that we're comparing we step to the next one minus one times minus one is also a one and we continue to step through pixel by pixel multiplying them all by each other and because they're always the same the answer is always one when we're done we take all these ones and add them up and divide by nine and the answer is one so now we want to keep track of where that feature was in the image and we put a one there say when we put the feature here we get a match of one that is filtering now we can take uh that same feature and move it to another position and perform the filtering again and we start with the same pattern the first pixel matches the second pixel matches the third pixel does not match minus one times one equals minus one so we record that in our results and we go through and do that through the rest of the image patch and when we're done we notice we have two minus ones this time so we add up all the pixels to add up to five divide by nine and we get a point five five so this is very different than our one and we can record the 0.55 in that position where we were where it occurred so by moving our filter around to different places in the image we actually find different values for how well that filter matches or how well that feature is represented at that position so this becomes a map of where the feature occurs by moving it around to every possible position we do convolution that's just the repeated application of this feature this filter over and over again and what we get is a nice map across the whole image of where this feature occurs and if we look at it it makes sense this feature is a diagonal line slanting downward left to right which matches the downward left to right diagonal of the x so if we look at our filtered image we see that all of the high numbers ones and .77s are all right along that diagonal that suggests that that feature matches along that diagonal much better than it does elsewhere in the image to use a shorthand notation here we'll do a little x with a circle in it to represent convolution the act of trying every possible match and we repeat that with other features we can repeat that with our x filter in the middle and with our upward slanting diagonal line on the bottom and in each case the map that we get of where that feature occurs is consistent with what we would expect based on what we know about the x and about where our features match this act of convolving an image with a bunch of filters a bunch of features and creating a stack of filtered images is we'll call a convolution layer a layer because it's an operation that we can stack with others as we'll show in a minute in convolution one image becomes a stack of filtered images we get as many filtered images out as we have filters so convolution layer is one trick that we have the next big trick that we have is called pooling this is how we shrink the image stack and this is pretty straightforward we start with a window size usually two by two pixels or three by three pixels and a stride usually two pixels just in practice these work best and then we take that window and walk it in strides across each of the filtered images from each window we take the maximum value so to illustrate this we start with our first filtered image we have our 2 pixel by 2 pixel window within that pixel the maximum value is 1. so we track that and then move to our stride of 2 pixels we move 2 pixels to the right and repeat out of that window the maximum value is 0.33 etc 0.55 and when we get to the end we have to be creative we have don't have all the pixels representative so we take the max of what's there and we continue doing this across the whole image and when we're done what we end up with is a similar pattern but smaller we can still see our high values are all on the diagonal but instead of seven by seven pixels in our filtered image we have a four by four pixel image so it's half as big as it was about this makes a lot of sense to do if you can imagine if instead of starting with a nine by nine pixel image we had started with a nine thousand by nine thousand pixel image shrinking it is convenient for uh working with it makes it smaller the other thing it does is pooling doesn't care where in that window that maximum value occurs so that makes it a little less sensitive to position and the way this plays out is that if you're looking for a particular feature in an image it can be a little to the left a little to the right maybe a little rotated and it'll still get picked up so we do max pooling with all of our stack of filtered images and get in every case smaller set of filtered images now that's our second trick third trick normalization this is just a step to keep the math from blowing up and keep it from going to zero all you do here is everywhere in your image that there is a negative value change it to zero so for instance if we're looking back at our filtered image we have these what are called rectified linear units that's the little computational unit that does this but all it does is steps through everywhere there's a negative value change it to zero another negative value change it to zero by the time you're done you have a very similar looking image except there's no negative values they're just zeros and we do this with all of our images and this becomes another type of layer so in a rectified linear unit layer a stack of images becomes a stack of images with no negative values now what's really fun the magic starts to happen here when we take these layers convolution layers rectified linear unit layers and pooling layers and we stack them up so that the output of one becomes the input of the next you'll notice that what goes into each of these and what comes out of these looks like an array of pixels or an array of an array of pixels and because of that we can stack them nicely we can use the output of one for the input of the next and by stacking them we get these operations building on top of each other what's more we can repeat the stacks we can do deep stacking you can imagine making a sandwich that is not just one patty and one slice of cheese and one lettuce and one tomato but a whole bunch of layers double tripper triple quadruple deckers as many times as you want each time the image gets more filtered as it goes through convolution layers and it gets smaller as it goes through pooling layers now the final layer in our toolbox is called a fully connected layer here every value gets a vote on what the answer is going to be so we take our now much filtered and much reduced in size stack of images we break them out we just rearrange and put them into a single list because it's easier to visualize that way and then each of those connects to one of our answers that we're going to vote for when we feed this in x there will be certain values here that tend to be high they tend to predict very strongly this is going to be an x they get a lot of vote for the x outcome similarly when we feed in a picture of an o to our convolutional neural network there are certain values here at the end that tend to be very high and tend to predict strongly when we're going to have an o at the end so they get a lot of weight a strong vote for the o category now when we get a new input and we don't know what it is and we want to decide the way this works is the input goes through all of our convolutional our rectified linear unit our pooling layers and comes out to the end here we get a series of votes and then based on the weights that each value gets to vote with we get a nice average vote at the end in this case this this particular set of inputs votes for an x with a strength of 0.92 and an o with a strength of 0.51 so here definitely x is the winner and so the neural network would categorize this input as an x so in a fully connected layer a list of feature values becomes a list of votes now again what's cool here is that a list of votes looks a whole lot like a list of feature values so you can use the output of one for the input of the next and so you can have intermediate categories that aren't your final votes or sometimes these are called hidden units in a neural network and you can stack as many of these together as you want also but in the end they all end up voting for an x or an o and whoever gets the most votes wins so if we put this all together then a twodimensional array of pixels in results in a set of votes for a category out at the far end so there are some things that we have glossed over here you might be asking yourself where all of the magic numbers come from things that i pulled out of thin air include the features in the convolutional layers those convenient three pixel by three pixel diagonal lines of the x also the voting weights in the fully connected layers i really waved my hands about how those are obtained in all these cases the answer is the same there is a trick called back propagation all of these are learned you don't have to know them you don't have to guess them the deep neural network does this on its own so the underlying principle behind back propagation is that the error in the final answer is used to determine how much the network adjusts and changes so in this case if we knew we were putting in an x and we got a 0.92 vote for an x and that would be an error of 0.08 and we got a 0.51 vote for o we know that that would be an error of 0.49 actually an error of 0.51 because it should be 0. then if we add all that up we get an error of what should be 0.59 so what happens with this error signal is it helps drive a process called gradient descent if there is another bit of something that is pretty special sauce to deep neural networks it is the ability to do gradient descent so for each of these magic numbers each of the feature pixels each voting weight they're adjusted up and down by a very small amount to see how the error changes the amount that they're adjusted is determined by how big the error is large error they're adjusted a lot smaller just a tiny bit no error they're not adjusted at all you have the right answer stop messing with it as they're adjusted you can think of that as sliding a ball slightly to the left and slightly to the right on a hill you want to find the direction where it goes downhill you want to go down that slope down that gradient to find the very bottom because the bottom is where you have the very least error that's your happy place so after sliding it to the left and to the right you find the downhill direction and you leave it there doing that many times over lots of lots of iterations lots of steps helps all of these values across all the features and all of the weights settle in to what's called a minimum and it uh and it at that point the network is performing as well as it possibly can if it adjusts any of those a little bit its behavior its error will go up now there are some things called hyper parameters and these are knobs that the designer gets to turn decisions the designer gets to make these are not learned automatically in convolution figuring out how many features should be used how big those features should be how many pixels on the side in the pooling layers choosing the window size and the window stride and in fully connected layers choosing the number of hidden neurons intermediate neurons all of these things are decisions that the designer gets to make right now there are some common practices that tend to work better than others but there is no principled way there's no hard and fast rules for the right way to do this and in fact a lot of the advances in convolutional neural networks are in getting combinations of these they work really well now in addition to this there are other decisions the designer gets to make like how many of each type of layer and in what order and for those that really like to go off the rails can we design new types of layers entirely and slip them in there and get new fun behaviors these are all things that people are playing with to try to eke out more performance and address stickier problems with cnns now what's really cool about these we've been talking about images but you can use any twodimensional or even for that matter three or four dimensional data but what's important is that in your data things closer together are more closely related than things far away what i mean by that is if you look at an image two rows of pixels or two columns of pixels are right next to each other they're more closely related than rows or columns that are far away now what you can do is you can take something like sound and you can chop it up into little time steps and for each piece of time the time step right before it and right after is more closely related than time steps that are far away and the order matters you can also chop it up into different frequency bands base midrange treble you can slice it a whole lot more finally than that and again those frequency bands are the ones closer together are more closely related and you can't rearrange them the order matters once you do this with sound it looks like a picture it looks like an image and you can use convert convolutional neural networks with them you can do something similar with text where the position in the sentence becomes the column and the row is words in a dictionary in this case it's hard to argue whether order matters that order matters it's hard to argue that words in the dictionary are that some are more closely related than others in all cases and so the trick here is to take a window that spans the entire column top to bottom and then slide it left to right that way it captures all of the words but it only captures a few positions in the sentence at a time now the other side of this limitation of convolutional neural networks is that they're really designed to capture local spatial patterns spatial in the sense of things that are next together next to each other matter quite a bit so if the data can't be made to look like an image then they're not as useful so an example of this is say some customer data if i have each row it's a separate customer each column is a separate piece of information about that customer such as their name their address what they bought what websites they visited then this doesn't so much look like a picture i can take and rearrange those columns and rearrange those rows and this still means the same thing it's still equally easy to interpret if i were to take an image and rearrange the columns and rearrange the rows it would result in a scramble of pixels and it would be difficult or impossible to say what the image was of there i would lose a lot of information so as a rule of thumb if your data is just as useful after swapping out any of the columns for each other then you can't use convolutional neural networks so the take home is that convolutional neural networks are great at finding patterns and using them to classify images if you can make your problem look like finding cats on the internet then they're a huge asset applications of machine learning have gotten a lot of traction in the last few years there's a couple of big categories that have had winds one is identifying pictures the equivalent of finding cats on the internet and any problem that can be made to look like that and the other is sequence to sequence translation this can be speech to text or one language to another most of the former are done with convolutional neural networks most of the latter are done with recurrent neural networks uh particularly long shortterm memory to give an example of how long shortterm memory works we will consider the question of what's for dinner let's say for a minute that you are a very lucky apartment dweller and you have a flatmate who loves to cook dinner every night he cooks one of three things sushi waffles or pizza and you would like to be able to predict what you're going to have on a given night so you can plan the rest of your days eating accordingly in order to predict what you're going to have for dinner you set up a neural network the inputs to this neural network are a bunch of items like the day of the week the month of the year whether or not your flatmate was in a late meeting variables that might reasonably affect what you're going to have for dinner now if you're new to neural networks i highly recommend you take a minute and stop to watch the how neural networks work tutorial there's a link down in the comments section if you'd rather not do that right now and you're still not familiar with neural networks you can think of them as a voting process and so in the neural network that you set up there's a complicated voting process and all of the inputs like day of the week and month of the year go into it and then you train it on your history of what you've had for dinner and you learn how to predict what's going to be for dinner tonight the trouble is that your network doesn't work very well despite carefully choosing your inputs and training it thoroughly you still can't get much better than chance predictions on dinner as is often the case with complicated machine learning problems it's useful to take a step back and just look at the data and when you do that you notice a pattern your flat mate makes pizza then sushi then waffles then pizza again in a cycle it doesn't depend on the day of the week or anything else it's in a regular cycle so knowing this we can make a new neural network in our new one the only inputs that matter are what we had for dinner yesterday so if we know if we had pizza for dinner yesterday it'll be sushi tonight sushi yesterday waffles tonight and waffles yesterday pizza tonight it becomes a very simple voting process and and it's right all the time because your flatmate is incredibly consistent now if you happen to be gone on a given night let's say yesterday you were out you don't know what was for dinner yesterday you can still predict what's going to be for dinner tonight by thinking back two days ago think what was for dinner then so what would be predicted for you last night and then you can use that prediction in turn to make a prediction for tonight so we make use of not only our actual information from yesterday but also what our prediction was yesterday so at this point it's helpful to take a little detour and talk about vectors a vector is just a fancy word for a list of numbers if i wanted to describe the weather to you for a given day i could say the high is 76 degrees fahrenheit the low is 43 the wind's 13 miles an hour there's going to be a quarter inch of rain and the relative humidity is 83 percent that's all a vector is the reason that it's useful is vectors list of numbers are computers native language if you want to get something into a format that it's natural for a computer to compute to do operations on to do statistical machine learning lists of numbers are the way to go everything gets reduced to a list of numbers before it goes through an algorithm we can also have a vector for statements like it's tuesday in order to encode this kind of information what we do is we make a list of all the possible values it could have in this case all the days of the week and we assign a number to each and then we go through and set them all equal to zero except for the one that is true right now uh this format is called one hot encoding and it's very common to see a long vector of zeros with just one element being one it seems inefficient but for a computer this is a lot easier way to ingest that information so we can make a one hot vector for our prediction for dinner tonight we set everything equal to zero except for the dinner item that we predict so in this case we'll be predicting sushi now we can group together our uh we can group together our inputs and outputs into vectors separate lists of numbers and it becomes a useful shorthand for describing this neural network so we can have our dinner yesterday vector our predictions for yesterday vector and our prediction for today vector and the neural network is just connections between every element in each of those input vectors to every element in the output vector and to complete our picture we can show how the prediction for today will get recycled the dotted line there means hold on to it for a day and then reuse it tomorrow and it becomes our yesterday's predictions tomorrow now we can see how if we were lacking some information let's say we were out of town for two weeks we can still make a good guess about what's going to be for dinner tonight we just ignore the new information part and we can unwrap or unwind this vector in time until we do have some information to base it on and then just play it forward and when it's unwrapped it looks like this and we can go back as far as we need to and see what was for dinner and then just trace it forward and play out our menu over the last two weeks until we find out what's for dinner tonight so this is a nice simple example that showed recurrent neural networks now to show how they don't meet all of our needs we're going to write a children's book it'll have sentences of the format doug saw jane period jane saw spot period spot saw doug period and so on so our dictionary is small just the words doug jane spot saw and a period and the task of the neural network is to put these together in the right order to make a good children's book so to do this we replace our food vectors with our dictionary vectors here again it's just a list of numbers representing each of the words so for instance if doug was the most recent word that i saw my new information vector would be all zeros except for a one in the dug position and we similarly can represent our predictions and our predictions from yesterday now after training this neural network and teaching it what to do we would expect to see certain patterns for instance anytime a name comes up jane dug or spot we would expect that to vote heavily for the word saw or for a period because those are the two words in our dictionary that can follow a name similarly if we had predicted a name on the previous time step we would expect those to vote also for the word saw or for a period and then by a similar method anytime we come across the word saw or a period we know that a name has to come after that so it will learn to vote very strongly for a name jane doug or spot so in this form in this formulation we have a recurrent neural network for simplicity i'll take the vectors and the weights and collapse them down to that little symbol with the dots and the arrows the dots and the lines connecting them and there's one more symbol we haven't talked about yet this is a squashing function and it just helps the network to behave how it works is you take all of your votes coming out and you subject them to this squashing function for instance if something received a total vote of 0.5 you draw a vertical line up where it crosses the function you draw a horizontal line over to the y axis and there is your squashed version out for small numbers the squashed version is pretty close to the original version but as your number gets larger the number that comes out is closer and closer to one and similarly if you put in a big negative number then what you'll get out will be very close to minus one no matter what you put in what comes out is between minus one and one so this is really helpful when you have a loop like this where the same values get processed again and again day after day it is possible you can imagine if in the course of that processing say something got voted for twice it got multiplied by two in that case it would get twice as big every time and very soon blow up to be astronomical by ensuring that it's always less than one but more than minus one you can multiply it as many times as you want you can go through that loop and it won't explode in a feedback loop this is an example of negative feedback or attenuating feedback so you may have noticed our neural network in its current state is subject to some mistakes we could get a sentence for instance of the form doug saw doug period because doug strongly votes for the word saw which in turn strongly votes for a name any name which could be doug similarly we could get something like doug saw jane saw spot saw because each of our predictions only looks back one time step it has very shortterm memory then it doesn't use the information from further back and it's subject to these types of mistakes in order to overcome this we take our recurrent neural network and we expand it and we add some more pieces to it the critical part that we add to the middle here is memory we want to be able to remember what happened many times steps ago so in order to explain how this works i'll have to describe a few new symbols we've introduced here one is another squashing function this one with a flat bottom one is an x in a circle and one is a cross in a circle so the cross in a circle is element by element addition the way it works is you start with two vectors of equal size and you go down each one you add the first element of one vector to the first element of another vector and then the total goes into the first element of the output vector so 3 plus 6 equals 9. then you go to the next element 4 plus 7 equals 11. and so your output vector is the same size of each of your input vectors just a list of numbers same length but it's the sum element by element of the two and very closely related to this you've probably guessed the x in the circle is element by element multiplication it's just like addition except instead of adding you multiply for instance 3 times 6 gives you a first element of 18. 4 times 7 gets you 28. again the output vector is the same size of each of the input vectors now elementwise multiplication lets you do something pretty cool um you imagine that you have a signal and it's like a bunch of pipes and they have a certain amount of water trying to flow down them in this case we'll just assign the number to that of 0.8 it's like a signal now on each of those pipes we have a faucet and we can open it all the way close it all the way or keep it somewhere in the middle to either let that signal come through or block it so in this case an open gate an open faucet would be a one and a closed faucet would be a zero and the way this works with element wise multiplication we get 0.8 times one equals 0.8 that signal passed right through into the output vector but the last element 0.8 times 0 equals 0. that signal the original signal was effectively blocked and then with the gating value of 0.5 the signal was passed through but it's smaller it's attenuated so gating lets us control what passes through and what gets blocked which is really useful now in order to do gating it's nice to have a value that you know is always between zero and one so we introduce another squashing function this will represent with a circle with a flat bottom and this is it's called the logistic function it's very similar to the other squashing function the hyperbolic tangent except that it just goes between zero and one instead of minus one and one now when we introduce all of these together what we get we still have the combination of our previous predictions and our new information those vectors get passed and we make predictions based on them those predictions get passed through but the other thing that happens is a copy of those predictions is held onto for the next time step the next pass through the network and some of them here's a gate right here some of them are forgotten some of them are remembered the ones that are remembered are added back into the prediction so now we have not just prediction but predictions plus the memories that we've accumulated and that we haven't chosen to forget yet now there's an entirely separate neural network here that learns when to forget what based on what we're seeing right now what do we want to remember what do we want to forget so you can see this is powerful this will let us hold on to things for as long as we want now you've probably noticed though when we are combining our predictions with our memories we may not necessarily want to release all of those memories out as new predictions each time so we want a little filter to keep our memories inside and let our predictions get out and that's we add another gate for that to do selection it has its own neural network so its own voting process so that our new information and our previous predictions can be used to vote on what all the gates should be what should be kept internal and what should be released as a prediction we've also introduced another squashing function here since we do an addition here it's possible that things could become greater than one or smaller than minus one so we just squash it to be careful to make sure it never gets out of control and now when we bring in new predictions we make a lot of possibilities and then we collect those with memory over time and of all of those possible predictions at each time step we select just a few to release as the prediction for that moment each of these things when to forget and when to let things out of our memory are learned by their own neural networks and the only other piece we need to add to complete our picture here is yet another set of gates this lets us actually ignore uh possible predictions possibilities as they come in this is an attention mechanism it lets things that aren't immediately relevant be set aside so they don't cloud the predictions in memory going forward it has its own neural network and its own logistic squashing function and its own gating activity right here now long short term memory has a lot of pieces a lot of bits that work together and it's a little much to wrap your head around it all at once so what we'll do is take a very simple example and step through it just to illustrate how a couple of these pieces work it's admittedly an overly simplistic example and feel free to poke holes at it later when you get to that point then you know you're ready to move on to the next level of material so we are now in the process of writing our children's book and for the purposes of demonstration we'll assume that this lstm has been trained on our children's books examples that we want to mimic and all of the appropriate votes and weights in those neural networks have been learned now we'll show it in action so so far our story so far is jane saw spot period doug so doug is the most recent word that's occurred in our story and also not surprisingly for this time step the names doug jane and spot were all predicted as viable options this makes sense we just wrapped up a sentence with a period the new sentence can start with any name so these are all great predictions so we have our new information which is the word doug we have our recent prediction which is doug jane and spot and we pass these two vectors together to all four of our neural networks which are learning to make predictions to do it ignoring to do forgetting and to do selection so the first one of these makes some predictions given that the word doug just occurred this is learned that the word saw is a great guess to make for a next word but it's also learned that having seen the word doug that it should not see the word doug again very soon seeing the word doug at the beginning of a sentence so it makes a positive prediction for saul and a negative prediction for doug it says i do not expect to see doug in the near future so that's why doug is in black so this example is so simple we don't need to focus on attention or ignoring so we'll skip over it for now and this prediction of saw not doug is passed forward and again for the purposes of simplicity let's say there's no memory at the moment so saw and doug get passed forward and then the selection mechanism here has learned that when the most recent word was a name then what comes next is either going to be the word saw or a period so it blocks any other names from coming out so the fact that there's a vote for not doug gets blocked here and the word saw gets sent out as the prediction for the next time step so we take a step forward in time now the word saw is our most recent word and our most recent prediction they get passed forward to all of these neural networks and we get a new set of predictions because the word saw just occurred we now predict that the words doug jane or spot might come next we'll pass over ignoring and attention in this example and we'll take those predictions forward now the other thing that happened is our previous set of possibilities the word saw and not doug that we were maintaining internally get passed to a forgetting gate now the forgetting gate says hey my last word that came uh that occurred was the word saw based on my past experience then i for can forget about you know i know that it occurred i can forget that it happened but i want to keep any predictions having to do with names so it forgets saw holds on to the vote for not doug and now at this element by element addition we have a positive vote for doug a negative o for doug and so they cancel each other out so now we just have votes for jane and spot those get passed forward our selection gate it knows that the word saw just occurred and based on experience a name will happen next and so it passes through these predictions for names and for the next time step then we get predictions of only jane and spot not doug this avoids the doug saw doug period type of error and the other errors that we saw what this shows is that long shortterm memory can look back two three many time steps and use that information to make good predictions about what's going to happen next now to be fair to vanilla recurrent neural networks they can actually look back several time steps as well but not very many lstm can look back many time steps and has shown that successfully this is really useful in some surprisingly practical applications if i have text in one language and i want to translate it to text to another language lstms work very well even though translation is not a word to word process it's a phrase to phrase or even in some cases a sentence to sentence process lstms are able to represent those grammar structures that are specific to each language and what it looks like is that they find the higher level idea and translate it from one mode of expression to another just using the bits and pieces that we just walked through another thing that they do well is translating speech to text speech is just some signals that vary in time it takes them and uses that then to predict what text what word is being spoken and it can use the history the recent history of words to make a better guess for what's going to come next lstms are a great fit for any information that's embedded in time audio video my favorite application of all of course is robotics robotics is nothing more than uh an agent taking in information from a set of sensors and then based on that information making a decision and carrying out an action it's inherently sequential and actions taken now can influence what is sensed and what should be done many times steps down the line if you're curious what lstms look like in math this is it this is lifted straight from the wikipedia page i won't step through it but it's encouraging that something that looks so complex expressed mathematically actually makes a fairly straightforward picture and story and if you'd like to dig into it more i encourage you to go to the wikipedia page also there are a collection of really good tutorials and discussions other ways of explaining lstms that you may find helpful as well i'd also strongly encourage you to visit andre carpathi's blog post showing examples of what lstms can do in text you can be forgiven if when reading on the internet you can substitute magic for deep learning and it fits perfectly in all of the articles it's hard to know what it can't do we don't get to talk about that very much so the goal of this talk is just to talk about a really simple nuts and bolts level the summary case you want to take a nap deep learning is not magic but it's really good at finding patterns so if this is our brain this is deep learning an owl can fly uh a fighter jet can fly but there are a whole lot of things that an owl can do and arguably it's a much more complex thing although what the fighter jet does it does really really well so deep learning is the fighter jet highly specialized highly engineered we're today going to talk about the basics the wright brothers airplane if you understand the principles that it works on then you can see easy to branch out into the finer engineering details but there's a whole lot of things that go on into fighter jet that we're not going to talk about in detail but this is nice we can talk about this at a comfortable level this is a neuron like all neurons it's got a big body in the middle a long tail and some arms that branch off here's an artist's conception of a neural network or a bunch of neurons again big body long tails arms this is an actual picture of neurons in some brain tissue here the bodies look like dots or blobs you can see long tails some of which branch and the arms are pretty much invisible and again a picture of some brain tissue here the neurons are small dots and you can barely see any of the tails at all this is just to give you a sense of how tightly these things are packed together and how many of them there are big numbers with lots of zeros and the crazy part is that a lot of them are connected to many more of their neighbors this is one of our very first pictures of a neuron santiago ramona kahal found a stain that he could introduce into a cell turn the whole thing dark under his 19th century light microscope was able to see this and then draw it with a pen and paper this is old school what you see here though bodies long tails lots of arms um these have we're going to turn it upside down because that's how they're typically represented in neural networks and these these pieces actually have names the bodies are called the soma the long tails are called axons and the arms are called dendrites we're going to draw a cartoon version of them this is what they look like in uh in powerpoint now the way that neurons work is that a dendrite you can think of it as feelers or whiskers and they look for electrical activity they pick it up and send it to the body the soma takes this and adds it together and accumulates it and then depending on how fast it's accumulating it it will activate the axon and send that signal along down the tail the more dendrite activity there is the more axonal activity there is and if you get all of the dendrites really firing then that axon is just as active as it can possibly be in a very simplistic way a neuron adds things up now a synapse is where the axon from one neuron touches the dendrite of another that's an artist's conception of it you can see in ramona tahoe's drawings these little nubs or buttons they're actually called boutons on the dendrites and these are sites where the axon of another neuron touches that so you can imagine there's a little connection there will represent that connection by a circle and the diameter of that circle is the strength of that connection big circle strong connection and it can connect strongly or weakly or somewhere in between we can put a number on this connection between zero and one so a medium connection we'll uh call it a 0.6 when the axon of the input neuron the upstream neuron is active then it activates the dendrite of the output neuron it passes that on with a modest strength if that connection is strong then it passes the signal on very strongly that connection is a one then when the axon is active the next dendrite is very active likewise if that connection is very weak say a 0.2 then when the axon is active the dendrite of the output neuron is only weakly activated no connection at all is a zero now this starts to get interesting because many different input neurons can connect to the dendrites of a single output neuron and each connection has its own strength we can redraw this by taking out all of the parallel dendrites and just drawing each axon and the single dendrite that it connects to and the connection strength represent like this with the dots we can substitute in numbers for that the weights uh although most often uh oh we can also separate substitute in line thicknesses to show how strongly these things are connected most of the time neural networks are drawn like this and this is what we have we went from the super complex slice of brain tissue with many subtleties in its operation and interconnection to a nice circle stick diagram where each one of those sticks represents a weight in its current form it can still do some pretty cool things the input neurons input neurons can connect to many output neurons so actually what you get here is many inputs many outputs and the connection between each is distinct and has its own weight this is uh it's good for making pretty pictures it's also great for representing combinations of things and the way this is done let's say you have five inputs labeled a b c e in this case this output neuron has a strong connection to a c and e very weak connections to b and d that means when the a c and e input neurons are active all together that really strongly activates the output neuron b and d don't matter because they only connect weakly so a way to think about this output neuron is in terms of the inputs that's strongly activated so we call this the ace neuron and here we have an atomic example of what happens here this output neuron represents a combination of the input neurons this is neural networks in a nutshell you can do this with any kind of input so you have a really low tech 4 pixel camera each of those four inputs is uh one of the pixels upper left lower left upper right or lower right in this particular neural network with strong connections to the upper left and upper right pixel we have a neuron and output neuron that represents this bar in the top half of the image so we can combine letters we can combine pixels to make small images if you're doing text processing the input neurons can represent individual words so in this case we're pulling words out of text this output neuron is strongly connected to the anterior neurons for eye and ball so we can call it the eyeball neuron similarly we can have a sunglasses neuron and infant neurons can connect to many outputs we could have an eyeglasses neuron just as easily so going a little deeper into this this is a somewhat trivial example to show how these things work in practice so there's a guy at the shawarma place and makes shawarma like nobody else and so you want to make sure and go when he's working there and taking a step back we actually have some domain knowledge here we know he's got two schedules working in the morning off in the evening and off in the morning working in the evening now if we were to instrument this with sensors we would have the working in the morning off in the morning working in the evening off in the evening and it might be useful to represent his working patterns in terms of a couple of output neurons that combine those so this is the network that we would expect to end up with working in the morning off in the evening is one pattern off in the morning working in the evening is the other path and you can see based on their connection strengths how they combine those inputs here would be the weights associated with those now the question is how do we learn this if we have to go in and fill it all in by hand we haven't learned anything it's just a fancier way of programming and a lot of hard work especially if you're dealing with many millions of input neurons so we want to learn this automatically so to start with might be a little counterintuitive we create our neural network we have our input neurons all we choose is the number of output neurons in this case we'll choose two because we happen to know we're learning two patterns and then we randomly assign weights randomly number generate numbers for each of these it's a neural network this completely you roll the dice you throw the sticks and whatever falls out that's what you start with and then we start to gather data we go stand on the other side of the street and we observe that the shawarma guy on this particular day worked in the morning and then went home did not work in the evening that means this input is active we'll say it's at a level one often the morning is at a level zero because we didn't observe it working in the morning is at zero and off in the evening is out of one because we observed that too so the next step is for each of the output neurons we calculate the activity so in this case an appropriately simple way to do this is just take the average of the inputs so here this weight is 0.3 and this weight is 0.1 so the average of those is 0.2 these neurons don't contribute anything because those inputs aren't active similarly we can take the weight between this input and that output 0.8 and 0.4 the average of those is 0.6 the upper neuron on the right has a higher activity that's the one we care about we ignore all the others there's a million others we ignore the rest and focus on this one for this time step first thing we figure out is how wrong is it well if it was perfect if our neural network was perfect that would have an activity of one it would be uh perfectly aligned with our inputs but it only has an activity of 0.6 so the error is 0.4 the bigger that error is that's a signal for how much we need to adjust our weights when that error gets very small it means that the weights really represent what's going on and we don't need to make any more changes now the trick here gradient descent if there is an element of magic in deep learning it's gradient descent what you do is you go through and adjust each of these weights all through you adjust it a little bit up and a little bit down and see which way decreases the error the idea the concept in gradient descent is weight is a quantity that you can shift a little bit side to side as you do this error will change you can think of it as taking this ball if you shift it a little bit to the left it has to climb the hill if you shift it a little to the right it has to fall down the hill and you like the direction you choose the direction in which it gets lower you want to bring that error down as low as it can get and you take small incremental steps to keep everything numerically stable so we go through and we do these for all of the neurons uh sorry for all of the weights that attach input neurons to our output and we find that yes we want to increase this one because these aren't active we actually have a bias toward low weights so it doesn't hurt to decreasing so we'll go ahead and decrease that weight and decrease that weight and increase that one when we do that sure enough our new activity is seven and so our error went from a point four to a point three it's a little bit better at representing what we saw so that was one data point we go back and we do the same thing the next day it just so happens that this day he's off in the morning working in the evening we adjust the weights and we do that day after day after day and eventually the weights will stop changing or they'll slow down changing quite a bit they'll get stable and we get the system of weights that we originally saw because we had knowledge of the problem so this is the wright brothers airplane version of how training by back propagation using gradient descent works back propagation is a very specific way to do this that is computationally cheap and slick and fast and you get your your jet engine instead of the flapping of wings so this is the underlying mechanism by which it works so what we just looked at was a single layer we have inputs we have outputs every output is a combination of things that were on the previous layer now there's no reason then that we can't turn around and take that output layer and make it inputs for the next layer and do that again and again if a network has more than three layers or so we call it deep uh some have more than 12. in some recent research in microsoft their deep neural networks would lay more than a thousand dollars there's no theoretical reason to limit the number of layers that you have it just depends on the specifics of your problem now what does deep get you why is deep special if your input neurons are say letters in the alphabet your first layer outputs sorry this is a deep neural network with all of the connections emitted for clarity so these are your inputs this is your first layer of outputs they're combinations of those letters each level you go up you get combinations of what happened the level before so by the time you get to your second level of outputs you're getting perhaps words in the english language if that's what you're training on the layer above that you get combinations of words short phrases and so forth and you can do this as deep as you like so there's a variety of things you can learn with deep neural networks a very popular one is images if you take as your inputs pixels and show instead of looking at uh shawarma guy schedule you're looking at individual pictures as your training data set what you start to learn after a while is these little representations of short lines and dots and patches these are the primitives of an image if you train on image of faces then your first layer output sorry your second layer outputs start to look like eyes and noses and mouths and chins and your third layer output start to look clearly recognizable as faces similarly if you train on automobiles your second layer outputs start to look like wheels and doors and windows and your third layer output look like automobiles so that's pretty cool we didn't have to go in and twiddle any of those weights this just learned that from seeing a bunch of pictures you can do it on color images too here's an uh output of an eight some of the output neurons of an eight layer neural network and as you get deeper you can see things that are clearly recognizable and quite complex you get spiders and rocking chairs and sailing and chips and teddy bears you can also plug information in about music artists so here's some research where output neurons were learned based on information about artists and then their uh representation was plotted based on how similar they were in that in the those output neurons and so we see things like kelly clarkson and beyonce are similar over here which is also not too far from taylor swift and avril lavigne whereas if we go up here we get weezer flat keys modest mouse presidents of the united states of america all in the same neighborhood this is a network that didn't know anything still doesn't know anything about music but because of the data that it gets on the input neurons it's able to group these things appropriately it finds patterns and then finds things that most closely fits those patterns turns out you can take atari 2600 games take the pixel representations feed those in as input neurons learn some fun features and then pair it with something else called reinforcement learning that learns appropriate actions and when you do this for a certain class of games it can learn to play them far better than any human player ever has or is ever likely to and it turns out that you can take a robot and let it watch youtube videos about how to cook and it loses uses a pair of deep neural networks one to interpret the video one to learn to understand its own movements and then uses those pairs that with some other execution software to cook based on the video representations that it's seen so while it's not magic it's pretty cool you can do some stuff so as you're going through reading literature reading popular articles about this you can kind of play bingo there are some buzzwords some popular algorithms you can think of a lot of these as the you know model numbers for the various fighter jets that are out there but when you see any of these terms if you like you can mentally do the substitution of deep learning and apply what you know about the wright brothers airplane and most of it will still be accurate so the bottom line it's good at learning patterns it doesn't do anything but it's pretty good at learning patterns i'm excited to get to talk about two of my favorite topics at once machine intelligence and robots they go together pretty well but they're not the same thing you can definitely have one without the other first some disclaimers i'm not going to give you the answer to human level intelligence i would if i had it but i don't next these are my own personal opinions they're definitely not those of any current or former employer and they don't reflect those of many experts in the field take them with a huge grain of salt if they are useful you're welcome to them and if they're not please discard them also this story that i'm going to tell you is not rigorous it doesn't have any equations it's conceptual and i just intended to start a discussion and foster ideas throughout this presentation we'll be talking about intelligence the working and definition that i propose is that it's a combination of how much one can do and how well one can do it notionally you could be extremely good at only one thing and not be that intelligent also you could do many things but do them all very poorly and also not be that intelligent intelligence is the combination of being able to do many things and to do them well this is a functional definition of intelligence there are many other potential definitions some of them can be measured experimentally and some can't particular definition has the advantage that if we wanted to reduce it down to a measurable set of tasks we could because it is at least in theory observable this allows us to have a scientific discussion about machine level intelligence it lets us form hypotheses that we could potentially falsify and it lets us compare the relative intelligence of two separate agents in short it's practical and useful undoubtedly some will find it philosophically unsatisfying that's a separate conversation but one that i would be happy to have in another forum using fake math we can say that intelligence equals performance times generality it's only fake because we haven't defined performance or generality yet but assuming we do you can imagine plotting them and putting them on a set of axes like this although we haven't defined it quantitatively for human performance what i would like to propose is that human performance is the level at which a human expert can do something this can be measured in lots of different ways it can be error rates or the time required to execute a task the time required to learn a task the number of demonstrations required before one learns a task the amount of energy expended when one performs a task the subjective judgment of performance from a panel of human judges the speed that someone does something there are many aspects of performance and i'm not going to try and specify or quantify them all here i only list them to illustrate that we are considering performance in a broad sense we're considering performance in a broad sense rather than in a narrow machine learning accuracy leaderboard sense if we consider human level performance to be something of a baseline we can place it on our xaxis and then chop up the rest of the axes in equal increments we'll make this a logarithmic scale to enable us to compare a very wide range of performances equal steps along this axis represent equal multiplicative factors of change human level generality is the set of all the tasks that humans can do and have undertaken these include things like writing stories cooking pies building cities gathering and transmitting information all around the world and even exploring the origin of the universe it's a very broad set of activities we can represent human level generality on the yaxis roughly this is the set of all tasks that a human or group of humans can do we'll make the yaxis logarithmic as well so an equal interval is a factor of 10 in performance either multiplied or divide depending on whether you're moving up or down so human intelligence can be notionally represented by the area that's entailed by this point just want to point out there's no reason to believe that machines might not exceed human performance in some areas humans have a number of limitations that are built into the way in which we've achieved our intelligence through evolution things that may have been very useful at one point or may be useful broadly but now may not be useful in pushing the limits of intelligence things like limited attention instinctive drives how every part of us fatigues and a host of cognitive biases all of which put some distance between us and perfectly rational or perfectly efficient or optimal behavior machines by comparison have a more nurturing environment in which to take root they don't have to evolve we're trying everything we can to encourage them now you can imagine on the performance generality axis another agent that can do a much larger set of tasks than humans although do them all more poorly than a human could so it might look like this the area under that triangle the overall intelligence would still be comparable to that of a human so we would call that human level intelligence also you can also imagine an agent who can do a subset of the tasks that humans do but do them so very much better that the area under that rectangle is also about the same as the humans so again human level intelligence now if we take the set of all of these agents that have about the same area under their intelligence rectangle then we get this curve representing human level intelligence any agent who falls along that curve would be comparable to a human any agent down and to the left of it subhuman intelligence and any agent up and to the right superhuman intelligence now let's look at a few agents that you might be familiar with and see where they fall in this scheme chest pain chest plane computers have been around for going on 30 years now um which is a world champion chess playing computers have been around for almost 30 years ibm's deep blue beat gary kasparov in 1989 this was a task that people assumed hugh computers would have a very tough time with it involved planning strategy thinking mental models of your opponent it seemed to take in the very peak of human cognition and it seemed unlikely that such a thing could be done by a fancy calculator but it did and now a chess program running on your phone can do about the same as deep blue did the current state of the art is a program called stockfish which has an elo rating which is like a chess skill score of 34 47. compare this to the top human uh toprated human player of all time magnus carlsen who achieved a 2882 the program and the human are not even comparable they're not even close it's worth noting that stockfish is an open source project with code freely available and has a number of contributors scattered around the globe now in terms of generality stockfish understands the rules of chess and in fact it understands them very well it has a bunch of hints and tips and tricks built in by humans that are specific to chess it uses a point system to evaluate pieces based on where they are and what the stage of the game is it uses a full table of end games once there are just a few pieces left on the board the number of possibilities for how the game will play out are few enough that they can be completely enumerated so there's no search there's no solving there's just essentially looking up in a giant table and figuring out what to do next there are handtuned strategies for each phase of the game and then what the computer does is uses tree search to evaluate future moves each choice of move is a branch on this tree and it can look and say for each move what's the likely outcome for each of those outcomes what are the possible moves my opponent can make for each of those outcomes then what are my possible responses and by fully going down this branching tree and looking at all the possibilities the program can then figure out what its best choices now one of the things that makes stockfish so clever and good at its games it's very good at pruning this tree and ignoring moves that are unlikely to lead to any good end and not exploring them very far but all told this program is pretty useless at anything that is not chess it is the opposite of general so on our plot we would put chess well above human level performance but also very low on the generality axis now compare that to uh go also a board game if you're not familiar with it they're 19 by 19 grid and opponents take turns placing white and black stones the rules are actually in some ways simpler than chess at each turn you pick a junction on which to place a stone now the strategy however uh some argue is even more complex and more importantly more subtle it's what is undoubtedly true is there are more possible board configurations where chess has an eight by eight board go has a 19 by 19 board where each piece in chess has a small prescribed number of motions any stone and go can be placed on any open junction so when doing a tree search the number of moves explodes much more quickly than in chess now despite this two years ago already alphago a program built by researchers at deepmind beat lee sidol a professional nine dan player professional nine dan would put him in the allstars of the go world a later version alphago master was clocked with an elo rating of 48.58 compare this to the highest rated human player park jungwon who's rated at 6669 so not only has go beaten the best players of the world but now already by a very healthy margin now as we mentioned the program understands the rules of go or knows the rules of go and use the tree search to evaluate moves because there are so many possible board configurations however it can't memorize them all so it uses convolutional neural networks to learn common configurations and most importantly it can see patterns that are not exactly repeated but are similar to things that have seen before this is an important innovation and we'll come back to convolutional neural networks in a few minutes it also uses reinforcement learning on a library of human games to learn which moves are good reinforcement learning in very straightforward terms is looking at a configuration an action and the outcome and learning the pattern for a given configuration if i take action a good things happen if i take action b bad things tend to happen after i learn those patterns the next time i see configuration a i can take the action that leads to good things um so using reinforcement learning on a library of human games then kind of bootstrapped alphago and let it learn from the history of human play and human insights to get started but like stockfish it's useless at anything that's not go so while it has a few tricks that allow it to be more general it is still very narrow in its applications so on our plot we might put it here again far exceeding human level performance but still very low on the generality axis now let's take a jump to a different category entirely um image classification there is a wonderful data set called imagenet which has many thousands of images classified by hand by humans into a thousand predefined categories these categories include household items cars and doors and chairs includes animals giraffes rhinoceroses it also includes for instance many different species of dog so it's not a trivial thing to take an image and accurately categorize it and in fact a typical human score on this is about five percent error about one out of every 20 images a human will classify incorrectly so it's a tough task in 2011 uh there was began a large scale visual recognition challenge in which teams got to put together their algorithms for classifying these images and in 2011 the very best one got 26 percent error so about one in every four image was wrongly classified still three out of every four was correct which was pretty amazing performance each year this error rate decreased by close to half which is an amazing rate of progress finally in 2015 the error rate got lower than human so we had a computer program classifying images better than a human does now in 2017 one of the more recent competitions more than half of the teams got fewer than five percent wrong so now machines are routinely beating humans at this task uh pretty impressive in terms of generality this task is definitely harder more general more challenging than a board game there are more variations more possibilities to get at it it uses convolutional neural networks which are a deep neural network architecture specifically designed for finding patterns in twodimensional arrays of data like a pixels or squares on a chessboard or on a go board they're very good at finding patterns for that could be visually represented this is good but it has been shown to break easily outside of the set of images that you train it on to give an example of this if you look at the images on the right in the left hand column we see soap dispensers praying mantis a puppy these are all images that were correctly categorized by a convolutional neural network with the addition of a little bit of distortion shown in the middle column which you're seeing that correctly is just a little bit of noise but the gray has no change at all you get the images on the right visually to us they look identical or very similar you might be able to see a little bit of warping and distortion there but for whatever reason uh convolutional neural networks confidently predicted all of these to be ostriches so this is not to say that they are not powerful and good but they see something different than we are seeing they are not learning to see in the same way that we see since then the fragile nature of convolutional neural networks has been demonstrated through other ways in some images changing a single pixel the right pixel to the right value can change how that image is classified others have found that you don't even have to go into the digital domain you can take carefully designed stickers and affix them to something and have that object be confidently classified as a banana whatever it is and in my favorite demonstration a physical plastic turtle was rotated and from every direction the convolutional neural network confidently predicted that it was a turtle then after just repainting a different pattern not symbolic or representative of anything but carefully chosen that same convolutional neural network categorized it as a handgun these examples show that uh at least as currently done our image classification generality is not quite where we would like it to be so definitely higher than human performance but classifying imagenet is a much narrower task than it might appear on the surface so we'll put it pretty low on the generality axis now here's a really fun example of video game performance so again the folks at deepmind put together uh deep q learning or deep reinforcement learning architecture to play video games we'll talk about more about what that is in a second but what they did is they took 49 classic atari games and let the algorithm just look at the pixels and make random moves and the algorithm didn't know if this was supposed to be a move after a move right or a jump or a shoot it just took moves and then used reinforcement learning to learn from the outcome and then learn this pattern of oh when i see this and i do this either something good happens or something bad happens or something neutral happens after doing that for long enough it learned the patterns that let it choose the right thing to do and in 29 of these 49 games it did at or above human expert level play and that was super impressive so this is not just looking at a picture and saying this is a cat this is looking at a picture in the moment and saying for this particular instance the right thing to do is jump and then by jumping that changes the image and then having to respond to the new configuration and doing that again and again and again and doing this better than a human now the other part of this is there were 20 games then at which it did more poorly than a human so after using convolutional neural networks to learn the pixel patterns for which this is perfectly suited because the pixels are big and coarse there's no noise they don't change the patterns are clear so what the algorithm is seeing is very close to what we as humans see and after uni using reinforcement learning to learn what actions to take in each situation 20 of these games it wasn't able to match human performance on and the pattern among those games is they tended to require longer term planning the one of them was ms pacman and if you've ever played that you know you're trying to eat all the dots in a maze while avoiding ghosts who are chasing you it involves planning routes out several turns ahead anticipating where ghosts are going to be a lot of things that you can't get from a single snapshot and without thinking ahead several steps in its current state this algorithm didn't do that and in fact the game that did the poorest on a game called montezuma's revenge required much more extensive planning going to one location and grabbing an object so you could go to another location and open a door and the computer just was not able to make those connections so we'll add video games to our plot here again more general than image classification there's more going on the task is broader and performance is about human level now you may notice a pattern here these fit roughly into a line or a curve and we'll see this pattern continue looking at machine translation taking text and changing it from one language to another if you ever gone to an online translator and typed in a phrase or copied a phrase from a language you weren't familiar with to one that you were you will probably notice that the translation is surprisingly good at getting some of the sense which is even five years ago was complete science fiction to be able to do this in a reliable way you'll probably also notice that the result is nothing that a native language speaker would ever be likely to say so it's it's okay it's definitely in the right direction but it's far from perfect now what's really impressive to me about this is that the state of the art and language translation takes over a hundred languages and instead of having specific models to translate from each language to each language all of these languages are translated to a sum uber intermediate representation which is then able to be translated back into any one of these hundred plus languages so all to all language translator so the sheer scope of that is really impressive now in order to do this it uses long short term memory lstm which is a neural network architecture and it actually uses several deep neural networks in concert one to carefully ignore parts of the input one to choose what to remember one to choose what to forget when to choose what to pass on there's quite a bit of computation involved and this architecture uses several levels of those even so the the amount of effort and computing power thrown at this in general um is if we use one of our metrics as efficiency it's a little bit could be considered a little bit of a hit in addition to the inaccuracies it is worth noting that this uses an attention mechanism so i called out attention earlier as a possible limitation of human performance but it also proves to be a really useful tool when dealing with a massive amount of information too much to look at in depth and so by prefiltering and focusing down on what's most likely to be helpful then in an algorithm can be much more efficient in how it handles it so for machine translations amazing performance still short of human and for the wildly ambitious scope it gets a little step up on the generality axis a little bit of a hit on their performance axis translation is still a very small part of all the things that humans do but i would definitely say this is more general than playing video games now looking at recommenders so if you think back to your last experience with an on time online real online retailer probably the recommendations that you got were maybe one in 10 was really really relevant some of the others were close but near misses and some of the others were obviously way out in left field so this is still pretty good like this is a tough task if you can imagine like uh back when there were video stores going to a video store with your friend and trying to guess what your friend even a friend you knew very well would want to watch on a given night you know you would be hardpressed to do better than like one and three or one and four so you know one in ten is not terrible or just ballpark it's common to assume among these algorithms that order doesn't matter so it just looks at everything you've ever bought today yesterday last year and it doesn't think about how these things are related or how many you might have or how many you might need or how something that you bought previously might be related to what you might need tomorrow it just looks at what people have bought in the past what they've bought together it also doesn't adapt to the fact that your selections might change with time so even if you bought one jar of mayonnaise a year ago and then another one six months ago and another one a few months ago it might not track the fact that your preference has changed one of my favorite examples of these came up from jack raynor twitter user who said dear amazon i bought a toilet seat because i needed one necessity not desire i do not collect them i am not a toilet seat addict no matter how temptingly you email me i'm not going to think oh go on then just one more toilet seat i'll treat myself so recommenders they do okay relative to humans and i would argue that the knowledge of the world required to do really well is pretty deep so we'll boost it up on the generality scale but it takes a hit on performance now we get to robots finally something physical bumping around in the world selfdriving cars their performance is impressive taken overall per mile driven selfdriving cars accident rates are lower than humans and this is pretty amazing when you consider all of the things that a car has to deal with construction pedestrians bicycle riders changing weather conditions changing road conditions they're not perfect but they're surprisingly good um now in terms of generality there are a few things that make selfdriving cars less general than they might at first appear and in fact the biggest tricks to making them successful is to reducing the difficulty of the task and so reducing the necessary generality of the solution so one of the things that happens is especially during training humans still have to take over in some challenging situations when the human gets uneasy or the car signals it doesn't know what to do then it falls back to the human and while driving is still a pretty complicated task it's still very simple compared to say walking on rough terrain while eating a bagel and walking a dog who's pulling on the leash there's a lot more to consider there and it's a lot tougher than a car who is statically stable on four wheels on a road that's flat and mostly straight and mostly marked and where the rules are well prescribed to further simplify the task and narrow the scope of what needs to be learned selfdriving car's driving style tends to be cautious they definitely do not tend to speed they tend to not follow closely or turn aggressively or do anything else that many human drivers do this is absolutely good practice and should be lauded and as a model for all of us to follow but what that means is that the raw driving skill required by a selfdriving car is in general less than that required by a human and it should also be noted that solutions are custom engineered for driving the selection of sensors the algorithms used to process them the way everything is put together is not updated on the fly it's gathered evaluated by humans and then very carefully and deliberately the heuristics the rules behind how that's interpreted and processed are then updated and tested and released again this makes sense for deploying anything that is has such a high consequence as a car but from a machine learning side it means that the solution is actually not as general as it seems it's very specific to a given car with a given set of sensors and sometimes even to a given environment some of some selfdriving cars at least early failures had to do with being deployed in climates that they weren't familiar with for instance so until their set of training data encompasses all of the conditions in which they will be deployed they will be even narrower than human drivers so all these things considered on the task of driving in general i chose to rate selfdriving cars at lower performance than human still its physical interaction and its interaction with other people in other cars so it's quite a lot going on and definitely more complex than machine translation or any even recommendations now humanoid robots the apex of cool applications um if you have not yet done it get on the internet and search for robots doing backflips and check it out when you see something like this it is easy to make the jump to believe that robotics has been solved like when a robot can do physical feats of acrobatics that i can't do then i mean it's done i'm ready to call it and um yeah it's just uh it puts a smile on my face that i can't wipe off now in terms of generality um do another search for robots falling down and you'll be treated to a montage of really humorous shorts of robots trying to do very simple things like open a door or lift an empty box or even stay standing up and they really struggle with this um because the systems are so complex because the hardware and the sensors have so much going on and because most of these are deployed as as research projects most of these activities are fairly hardcoded and pretty fragile they make a lot of assumptions about the nature of the hardware what's going on the nature of the environment and if any of these assumptions are violated the robot's performance fails so as a result uh plotting them here the generality in the sense that the types of things that they have experienced taken together as a whole are now getting to be a non negligible fraction of things that humans can do you know maybe it's 0.1 maybe it's 0.01 somewhere in between but an amazing set of things that can be quite hard but performance is still sometimes laughably low compared to human level um we can you know compare humanoid robots as agents to humans and see that it's much less more interestingly here our trend now is quite clear there's a fat line here that runs roughly parallel offset from the human level intelligence line as solutions tend to get higher performance they also tend to get less general and vice versa but it's rare that we get big steps toward the human intelligence line now this is this is what i think is cool this is the whole point of this talk there is one example that i would like to show of this before i jump to the conclusion which is again from deep mind a program called alpha zero so alpha zero is like alphago except everything it knows about go has been taken out it doesn't know the rules of any game now it just sees visual patterns tries actions and learns to see what is successful and what's not the way it was used is you can think of a brand new alpha zero instance as being an infant in terms of the gameplay and two alpha zero infants were created and they started to play each other one was allowed to learn the other one was not so the one that learned gradually got just a little bit better stumbling into some good moves by accident until it became an okay beginner player of the game then it cloned itself one of the two learned the other did not and they played and played until the one became an intermediate level player of the game and it repeated this process of cloning and playing itself with one learning and the other not and used its intermediate steps as scaffolding to get better and better and it turns out that when using this approach with go within four hours it was as good as the best human player and within eight hours it had beat the previous best computer its uncle alphago um because it did not build any rules of the game it was also able to learn chess and beat the current best chess playing program stockfish and another board game called shogi and be the current best shogi playing program as well all of which beats humans by a wide margin so this is cool because it does both better performance and it's more general it's not specific to any board game and presumably if there were other board games that had twodimensional grids and a set of rules that was not wildly wildly different it could learn to play those as well so generality and performance so what we have now is a point that is both farther to the right higher performance and farther up higher generality than the original that it was from so this is a real increase in area under that rectangle an increase in intelligence this is the direction that we want to go so it's worth taking a step and taking a moment and thinking about what is it that allowed us to step in this direction well alpha zero made many fewer assumptions about what was going on and it also was able to practice as many times as it needed to through selfplay in general assumptions are what prevent generality they enable performance so if i build in knowledge of the rules of chess i'm able to take advantage of those much more quickly but it also prevents me from doing anything that's not chess so if i turn that around making fewer assumptions i mean it takes me longer to do something but it means i might be able to learn to do more things so some common assumptions sensor information is noise free we have ideal sensors that makes sense if we're playing chess when we sense that a piece is on a given square we expect that it will be but if we're dealing with say a selfdriving car maybe there's a smudge of mud on the camera maybe the calibration of the lidar is off a little bit we can't assume ideal sensors when we're interacting with the physical world there are too many things we can't control for another common assumption is determinism that's when i take an action i know that it will have the same outcome every time again makes great sense when i have a board game make sense if i'm classifying images if i say an image is an image of a cat i know that it will be labeled as a cat image right or wrong however if i'm a humanoid robot and i make an action to reach for a doorknob the motor might not perform the way i expect my feet might slip on the ground i might have unanticipated challenges to my balance the action may not turn out exactly as i expect and i need to be able to adapt to this another really common assumption unimodality all of the sensors are the same type so this is an assumption in convolutional neural networks for instance it's great at bringing in a twodimensional array of information that is all the same type it's all pixels where it's all board squares a general solution needs not to make this assumption another assumption stationarity this is a very common one it's that the world doesn't change the things that i learned yesterday are still true today the things that i learned five minutes ago are still true right now now we have to make some change sorry we have to make some assumptions about continuity otherwise what i learned yesterday doesn't do me any good at all but we also need to allow for the fact that the world has changed a little bit maybe the lubrication in my ankle joint is a little low so it's going to respond differently than it did yesterday maybe there are clouds covering the sun so the lighting conditions i learned to operate in yesterday have changed as well and i'll need to to be able to adapt to that another common assumption is independence which is the world is not changed by what i do to it physical interaction violates this entirely if i am a robot operating in a household and i bump into a chair and i scoot it six inches sideways then whatever map i've made of that house will need to be changed a little bit i have changed it myself if i pick up a mug and move it from this table to that table i have changed the position of that mug the things that i do change the world and i need to keep track of that and any algorithm i use needs to be able to account for that another common assumption ergodicity everything i need to know about how i operate i can sense right at this moment this is a common assumption also known as a markov assumption but it's also commonly broken in physical interaction for instance if i can sense position that's great but that doesn't tell me anything about velocity and sometimes i need to know velocity to know how to respond to something another assumption that is very common is that the effects of my actions become apparent very soon this is something that does not hold true for instance in chess where the opening move will affect whether or not i win many many time steps away there are different tricks for handling this in chess for instance assigning point values to intermediate positions of pieces on the board but in physical interaction it's much more difficult to do this to know that given a set of actions that i take right now which is most likely to result in something that's desirable five minutes from now or a day from now all of these assumptions are very common in the algorithms currently being used that we call ai these algorithms are not sufficient for achieving human level intelligence these assumptions will prevent them from doing that so one thing that all of these assumptions have in common is that they do not hold when you're working with humanoid robotics or in fact any robot that's physically interacting with the world so my proposal is that focusing on interact physical interaction is a great way to force us to confront these assumptions to find out which ones we can bend to find out which we can avoid all together and to drive us to create algorithms that are less fragile and able to accommodate a much more general set of tasks that will then take us one step closer to human level intelligence when you hear about artificial intelligence about a half of the time what people are talking about is convolutional neural networks understanding how they work is really helpful in getting a peek behind the curtain at the magic of artificial intelligence so we're going to walk through it in some detail convolutional neural networks takes images and from them they learn the patterns the building blocks that make them up so for instance in the first level of this network you might learn things like line segments that are at different angles and then at subsequent layers those get built into things like faces or element of cars depending on the images that you train the network on you can pair this with reinforcement learning algorithms to get algorithms that play video games learn to play go or even control robots so to dig into how these work we'll start with a very simple example much simpler than all of these a convolutional neural network that can look at a very small image and determine whether it's a picture of an x or an o just two categories so for example this image on the left is an eight by eight pixel image of an x we want our network to classify it as an x similarly with the image of the o we want the network to classify it as an o now this is not entirely straightforward because we also wanted to handle cases where these inputs are of different sizes or they're rotated or they're heavier or they're lighter and every time we'd like it to give us the correct answer a human has no problem looking at these in deciding what to do but for a computer this is much harder when trying to decide if these two things are equal what it does is it goes through pixel by pixel black pixels might be a minus one white pixels might be a plus one and it'll compare them pixel by pixel find the ones that match and here the red pixels are the ones that don't match so a computer looking at this would say uh no these are not the same they have some matches but they have a lot that don't match so the way convolutional neural networks do this one of the tricks they use is that they match pieces of the image so you can look at these pieces and shift them around a little bit but as long as the tiny bits still match then the overall image is still considered a pretty good match so these tiny bits might look like this we'll call them features you can see the one on the left looks like a diagonal arm of the x that's leaning left the one in the middle looks like the center of the x where it crosses and the one on the right looks like a diagonal arm leaning right and you can see how these different pieces these different features of the image match different patches within the overall image so the math behind finding this match applying features is called filtering it's pretty straightforward but it's worth walking through the way that it's done is you line the feature up on the image patch you're concerned with multiply pixel by pixel add up the values and then divide by the total number of pixels this is one way to do it here for instance we start with this feature of the arm of the x leaning left we align it with this arm on the image and we start with the upper left pixel and we multiply both values one by one equals one now because we started with the upper left pixel we can keep track of our answers here so this pixel when multiplied equals one the upper center pixel is minus one in both of the feature and the image so minus 1 times 1 is also equal to 1 so that when you multiply them and you get a 1 that indicates a perfect and a strong match and we can continue doing this throughout the entire feature and the entire image patch and because they are a perfect match every single one of these matches will come back as a one and so to find the overall match we just add up all these nine ones divide by the total number which is nine and we get a a match of one now we can create another array to keep track of how well the feature when placed in this position matched our image so this average value is one we'll put a one right there to keep track of that you can see what it looks like if we were to move this feature then and align it to a different patch so let's say we move it down to the center of the x and we go pixel by pixel and find what matches and after a few pixels we actually find one that doesn't match we end up with a minus 1 times a plus 1 giving us a minus 1 back this indicates a nonmatch of these pixels and as we go through the rest of the feature we can see that there are a couple of pixels that don't match so here when we add these up and divide by nine we get a number that's less than one point five five so it indicates a partial match but not a perfect one it turns out you can go through and do this for every possible location in the image you can chop it up into every possible image patch compare the feature to each one and here's what you would get in this particular case this is what convolution is it's taking a feature and applying it across every possible patch across a whole image and you can see here why it's called filtering what we have is a map of where this feature matches the image you can see a strong bunch of plus ones on the diagonal line from the lower right to the upper left and then lesser values everywhere else so it's a filtered version of the original image that shows where the feature matches you can do this we can represent this with this notation uh we just invented this little convolution operator for shorthand and we can do this with our other features as well we can see where our center x matches not surprisingly it matches strongest in the center of the image we can see where our leaning right arm matches and not surprisingly it matches along this diagonal from the lower left to the upper right we have three filtered versions of the original image so this is what a convolution layer in a convolutional neural network does it has a set of features in it and it can be three or thirty or three hundred or three thousand but it has a set of features and it takes the original image and returns a set of filtered images one for each of the features so this is how we'll represent it that's the number one ingredient in convolutional neural networks that is the magic special sauce the special trick that gets from a nonexact match and let's the algorithm is able to pull out okay well it's not a perfect match but it's still a pretty good match because it does this convolution and moves the feature across the image and finds everywhere that it might match another piece of this is called pooling so we took our original image and now we have a stack of images what this step does is it shrinks it down a little bit we start by picking a window size usually two or three pixels picking a stride usually two pixels has been shown to work well and then walking this window across the filtered images and then from each window taking the maximum value that you see so this is called max pooling so to see how this works we start with one of our filtered images we have our window which is two pixels by two pixels and within that the maximum value is one so we create another little array to keep track of all of our results and we put a 1 in it and then we step it over by our stride which is 2 pixels look at the window choose the maximum value in this case it's 0.33 record it and go again and we keep doing this recording the maximum value each time all the way through the image and when we're done we have which if you squint looks like a shrunken version of the original we still have this strong set of plus ones on the diagonal from upper left to lower right and then everywhere else it's less than that so it maintains kind of the original signal but shrinks it down kind of picks off the high points and this gives us a smaller image but still similar to the original and we can just represent it with this little shrinking arrow we can do this with each of our filtered images and again you see that very roughly the pattern of the original is maintained so in a pooling layer a stack of images becomes a stack of smaller images now the last ingredient we need is normalization so this keeps the math from breaking by taking and tweaking these values just a little bit it takes everything that's negative and changes it to zero this keeps things from becoming unmanageably large as you progress through subsequent layers this function is called a rectified linear unit it's a fancy name for something that just takes anything that is negative and makes it zero so 0.77 not negative it doesn't touch it but a minus .11 it's negative just bumps it up to zero and by the time you've gone through all of your images and done this all of your pixels and done this this is what you have so everything that was negative is now zero so just a nice little bit of normalization some conditioning to keep things numerically well behaved a stack of images becomes a stack of images with no negative values now you can notice that the output of one layer looks like the input to the next there are always arrays of numbers an image and an array of number are the same thing they're interchangeable so you can take the output of the convolution layer feed it through the rectified linear unit layer feed that through the pooling layer and when you're done you have something that has had all of these operations done to it and in fact you can do this again and again and this recipe you can imagine making like a scoobydoo sandwich of all of these different layers again and again and in different orders um and uh some of the most successful convolutional neural networks are kind of like accidentally discovered groups of these that just happen to work really well so they get used again and again so over time each convolution layer filters through a number of features each rectified linear unit layer changes everything to be nonnegative and each pooling layer shrinks it so by the time you're done you get a very tall stack of filtered images with no negative values that has been shrunken down in size now by the time we've gone through several iterations of this we take and run it through a fully connected layer this is more of a standard neural network where every input gets connected to everything in the next layer with a weight every single value you can think of it as a voting process so every single pixel value that's left in these filtered shrunken images gets a vote on what the answer should be and this vote depends on how strongly it is tends to predict an x or an o when this pixel is high is this output usually an x or is it usually an o so you can see for this particular input the input was an x here's what the imaginary convolved and filtered values are and over time we would learn that these things that are high when it sees an x get a strong vote for the x category similarly if you have an input that's an o these final pixel values that tend to be really high when the right answer is row is o gives a strong vote for the o category the thicknesses of these lines represent the weights the strength of the vote between these pixels and these answers so now if we get a new input that we've never seen before these might be the final pixel values we can use these votes and do a weighted voting process for both of these add them up and in this case you know it's a 0.92 total for x and a 0.51 total for o 0.92 is obviously more than 0.51 we declare x the winner so this input will have been categorized as an x so this is a fully connected layer so it just takes a list of feature values in this case our filtered shrunken pixels and it becomes a list of votes for each of our output categories in this case an x or an o now these can also be stacked you can have like little they call them hidden layers but like secret hidden categories in between here so one votes on the first layer votes on the first set of hidden categories and then those vote on the next layer and so forth until you get to your final ones we'll dig into this more in just a little sec but these all stack onto the end now to go into uh the next level of detail on these neural networks set aside our x and o detector for a while there we had eight by eight pixel images so 64 pixels in all consider now a two by two pixel image so just a four pixel camera and what we would like to do is categorize the images that it takes as either being a solid image all light or all dark a vertical image a diagonal image or a horizontal image now the trick here is that simple rules can't do it so both of these are horizontal images but the pixel values are completely opposite in both of them so you can't say well if the upper left pixel is white and the upper right pixel is white then it must be horizontal because that's violated by the other one now of course you could do more complicated rules to do this the point is that when you go to larger images you can't make simple rules that capture all the cases that you want so how do we go about it we take these four input pixels and break them out we call them input neurons but they just take these pixels and turn them into a list of numbers the numbers correspond to the brightness minus one is black plus one is white zero is middle gray and everything else is in between so this takes this little image and turns it into a list of numbers that's our input vector now each of these you can think of it as having a receptive field this is an image which makes the value of this input as high as possible so if you look at our very top input neuron the image that makes that number as high as possible is an upper left pixel that's white that makes the value of that one and it doesn't care what the other pixels are that's why they're checkered so you can see that each of these has its own corresponding receptive field the image that makes the value as high as it can go now we're going to build a neuron so when uh people talk about artificial neural networks and a neuron we are going to build it bit by bit the first thing you do to build a neuron is you take all of these inputs you add them up so in this case this is what we'd get so the neuron value at this point is 0.5 now the next thing we do is we add a weight we mentioned the weighted voting process before so what that looks like is each of these inputs gets assigned a weight between plus and minus one and it gets the value gets multiplied by that weight before it gets added so now we have a weighted sum of these input neurons and we will represent this visually by showing positive weights in white negative weights in black and the thickness of the line being approximately proportional to the weight and when the weight is zero we'll leave it out to minimize visual clutter so now we have a weighted sum of the inputs the next thing that we need to do is squash the result so because we're going to do this a lot of times it's nice if we always guarantee the answer is between plus and minus 1 after each step that keeps it from growing numerically large a very convenient function is this sshaped so sigmoid squashing function this particular one is called the hyperbolic tangent there is confusingly something else that's called the sigmoid it's a little bit different but the same general shape but the characteristic of this is that you can put in your input you know draw a vertical line see where it crosses the curve track that over to the using a horizontal line to the y axis and you can see what the smashed version the squashed version of your number is so in this case 0.5 comes out to be just under 0.5 0.65 comes out to be about 0.6 and as you go up this curve you can see that no matter how large your number gets what you get out will never be greater than one and similarly it'll never be less than minus one so it takes this infinitely long number line and squashes it so that it all falls between plus and minus one so we apply this function to the output of our weighted sum and then we get our final answer so this weighted sum and squash is almost always what people are talking about when they talk about an artificial neuron now we don't have to do this just once we can do it as many times as we want with different weights and this collection of weighted sum and squash neurons is you can think of it as a layer loosely inspired by the biological layers of neurons in the human cortex so each of these has a different set of weights here to keep our picture really simple we'll assume these weights are either plus one white lines minus one black lines or zero missing entirely so in this case now we have our layer of neurons we can see that the receptive fields have gotten more complex if you look at the neuron in the first layer on the top you can see how it combines the inputs from the upper left pixel and the lower left pixel both of the weights are positive those lines are white and so what comes out its receptive field is if both of those pixels on the left are white then it has the highest value it can possibly have if we look at that layer of neurons and look at the one on the bottom we can see that it takes its inputs from both of the pixels on the left oh sorry on the right but it has a negative weight collecting connecting it to the lower right neuron so its receptive field its what maximally activates it is a white pixel in the upper right and a black pixel in the lower right now we can repeat this because the outputs of those that first layer of neurons looks a whole lot like our input layer still a list of numbers between 1 and 1 and so we can add additional layers and we can do this as many times as we want each time each neuron in one layer is connected to each neuron in the other layer by some weight so in this case you can see how the receptive fields might get still more complex and now we're starting to see patterns that look like the things that we're interested in solids verticals diagonals horizontals by combining these elements now there's one more thing that we can do remember our rectified linear unit we can have different neurons here instead of a weighted sum and squash we can just have something that takes the input and spits out 0 if it's negative and the original value if it's positive and so for instance if we have if we have an input whose receptive field is the one on the very top and the second layer all solid white and we connect it with a positive weight to the rectified linear unit neuron on top then of course what would maximize that is all solid white input but if we look at the neuron just below that that's connected to it with a negative weight then that that flips everything around and what maximally activates that is an input that's all solid black now we're really starting to get the set of patterns that we can imagine using to decide what our images so we connect these again to a final output layer this output layer is the list of all the possible answers that we expect to get out of our classifier originally it was x's and o's now it's four categories solid vertical diagonal and horizontal and each of these inputs into them have a vote but you can see that very few of them are connected this network assumes that most of those votes are zero so to see how this plays out let's say we start with an input that looks like the one on the left with uh this is obviously a horizontal image with a dark bar on top and a white bar on the bottom we propagate that to the input layer and then we propagate that to the first hidden layer and you can see for instance the neuron on the very top it combines two input neurons that one is light and one is dark so you can imagine it's summing a plus one and a minus one and getting a sum of zero so that's why it's gray its value is zero now if you look at the neuron in the very bottom in that first hidden layer you can see that it sums also an input that is negative and one that's positive but it's connected to one by a negative weight and the other by a positive weight so it actually what it sees its weighted sum is minus one and minus 1. so what it is getting you can see is the opposite of its receptive field so that means it's maximally activated but negatively so that's why it's black we move to the next layer and you can trace these things through so anything zero plus zero is going to get you zero if you look at the neuron on the very bottom of this second hidden layer you can see that yes it's adding up a negative and a negative both connected by positive weight so it's also going to be negative which makes sense because you can see that its receptive field is the exact opposite of what the input is right now so it's maximally activated just negative and then when we track this to our next layer you can see that following that bottom pair of neurons because it's a negative value it goes through the rectified linear unit and becomes zero so that's gray but if you look at the very bottom neuron there it has it's connected with a negative weight so it becomes positive so that rectified linear unit really likes it so it gives it a maximum value so everything is zero except for that neuron on the bottom and then finally what that means is that the only output that is nonzero is this horizontal one so this network would classify the input image as being horizontal because of this now there's some magic here where did we get those weights where did we get the filters in between um this is where we start to get down to the when we talk about learning adaptation you know the learning and machine learning it is all about optimization these are learned through a bunch of examples over time so we're going to set that aside for just a minute we'll come back to how those get learned we need to talk about optimization first so consider drinking tea there is a temperature range where it is a delightful experience it's warm and delicious and comfortable if your tea is too much hotter than that it's very painful and not good not fun at all and if your tea is cooler than that it's lukewarm and it's really meh it's really not worth your time so this area at the top is the peak this is the best this is what we're trying to find in optimization we're just trying to find the best experience the best performance now if we want to find that mathematically first thing we do is we flip it upside down just because this is how optimization problems are formulated but it's the same type of thing instead of maximizing tea drinking pleasure we want to minimize minimize tea drinking suffering we want to find the bottom of that valley the lowest possible suffering there's a few different ways we could do this the first is to look at every point on this curve and just pick the lowest one now the trick with that is we don't actually know what this curve is beforehand so in order to pick the lowest one we have to do exhaustive search which in this case would be make a cup of tea have someone drink it ask them how they like it make another one ask them how they like that one do it again and again for every possible temperature and then pick the one with the lowest suffering the most enjoyment this is effective very effective also it can be very time consuming for a lot of problems and so we search for a shortcut now because this is a valley we can use our physical intuition and say hey well what if we just had a marble and we let it roll to the bottom of this valley we wouldn't have to explore every single piece of it so this is what's behind gradient descent the way it works is we start not knowing anything about this function we make a cup of tea someone tells us how they like it and then we change the temperature a little bit we make another cup of tea just a little bit cooler we ask someone how they like that and we find out they actually like it just a little bit less that tells us what direction we need to go we need to make our next cup of tea warmer and the change the difference between how much they like those two tells us the slope tells us the steepness gives us a sense of how much warmer we can expect to make that next cup of tea so we make another one and we repeat the process and then we again scoot a little ways off to the side make another cup of tea and figure out again which direction we need to go are we do we need to go warmer to make a better cup or cooler to make a better cup and we repeat this until we get to the bottom of the curve you'll know you're at the bottom when you change the temperature just a little bit and the tea drinker says yeah it's exactly the same i like that just as much as the last one that means that you're there kind of at the flat bottom of the valley so um gradient descent is the first level trick for brewing fewer cups of tea there's another thing you can do which is to use curvature this is kind of an advanced method is you can make your original cup of tea and then make one a little bit warmer and one a little bit cooler and you can look to see how that curve of your function goes and if it's very steep and getting steeper then you know you can take a giant step because you're probably not anywhere close to the bottom and then you can do it again and if that curvature is starting to bottom out then you can take a smaller step because you the signal that you're getting closer to the bottom and it helps you to do this in fewer steps as long as your curve is relatively well behaved which is not always the case so uh ways that this can break imagine we're doing this on a hot day and actually it turns out that if we were to cool our tea way down we'd get a really nice iced tea which turns out to be even more popular with our tea drinkers but gradient descent would never find this gradient descent always rolls down to the bottom of the nearest valley it doesn't hop around to see if there are any valleys hiding anywhere else another problem is let's say there's a wiggle on our curve there is something happening in the environment we have noisy buses driving by and it affects how people enjoy their tea we might not be able to find this very lowest dip because we might get stuck in a dip further up the curve similarly if we ask our tea drinkers to rate their tea drinking experience on a scale from one to ten we get these discrete jumps in our function and if you imagine a marble rolling downhill it downstairs it doesn't always work well and it can get stuck on a step without making it all the way to the bottom now all of these things happen in real machine learning problems another one imagine you have really picky tea drinkers and if the tea is anything but perfect they hate it hate it hate it and so you have these plateaus on either side and there's no signal to tell you that if you move in a little bit you'll find that deep valley so for cases like this there of course we can always fall back to exhaustive exploration it will find the best answer in every single one of those cases but a lot of times we just don't have the time like if i have to brew and measure the pleasure drinking pleasure of 10 million cups of tea to get a good answer to this is not going to happen in my lifetime so luckily there are some things in the middle that are more sample efficient than exhaustive exploration but a little bit more robust than gradient descent things like genetic algorithms simulated annealing things that their defining characteristic is they have a little bit of random jumping around they're a little bit of unpredictability and so they make it harder to slip things by them they all have their strength and weaknesses there tend to be good for different types of problems or different types of pathologies in your loss function but all of them help avoid getting stuck in the local minima the little small valleys that gradient descent will get stuck in they get away with this by making fewer assumptions and they can take a little longer to compute than gradient descent but not nearly so long as exhaustive exploration you can think of gradient descent as being like a formula one race car and if you have a really nice wellbehaved track it is fast but if you put a speed bump in the truck you're done um genetic algorithms simulated annealing evolutionary algorithms those are like you know a fourwheel drive pickup truck you can take a fairly rough road with those and get where you're going you won't get there in record time perhaps but you'll get there and then exhaustive exploration is like traveling on foot there is nothing that will stop you from getting anywhere you can travel little or literally anywhere but it just might take you a really really long time so to illustrate how this works imagine we have a model that we would like to optimize we have a research question how many m m's are in a bag of m ms so answering this is easy you buy a bag of m ms you eat it 53 you can count those m ms so great we know how many were in the first bag now when i did this i made a mistake and i bought another bag and i tried that one and i got a different answer so now i can answer 53 or i can answer 57 either way i'm only right half the time because i can't capture both bags with one answer and i could answer somewhere in the middle but that's never right i have never opened a bag that had 55 m ms in it so it's unclear that that's the right answer either and the situation does not improve with the more bags of m ms that i ate it just gets a little bit out of control and so i change my goal from answering the answer answering the question right to answering the question in a way that is less wrong so in order to do that i have to get really specific about what i mean by how wrong i am and to do that i have this distance function this deviation which is the difference between my actual guess and the actual number of m m's in a bag so we call for for bag number i this deviation is d sub i it's just the difference between the guess and the actual number and then i have to take this deviation and turn it into a cost so one common way to do this is to square it it's nice as the further away things get kind of the more costly it is perceived as and it goes up faster so if there's a bag that's off by twice as much as another the cost is four times so i really it penalizes the things that are way off things that are close it doesn't penalize so much and if we don't care if we don't want to overly penalize the things that are way out there we could use the absolute value of the deviation so if it's off by twice as much it'll just be twice the cost but really we could use anything we could use the square root of the absolute value we could use 10 to the power of the absolute value of this deviation anything that goes up the further away you get from zero we'll stick with squared deviation this is super common it has some nice properties and makes for a good example so for the total cost of any guess that we make if i guess n estimated bags m m's in a bag then the loss function this fancy curly q l of that guess is just adding up the square of the deviation associated with each bag of m ms d1 through dm squared so each deviation is actually the number of m ms in that bag minus the guess so i square that and we can write that with fancy summation notation like this so this is my loss function this is the total cost this is how wrong i am when i make a guess nst so because we have computers you can write a little bit of code and you can do exhaustive exploration and i can say if i guess anything between 40 and 70 how wrong would i be with this data and you can plot it and visually we can look at this and we can say hey look there's the lowest value and we can say what is the value of the guess that gives me the lowest loss that's what that argument that um notation means right there and this best guess is just about 55 and a half m ms problem solved so this is an example of numerical optimization where we calculate this loss function and then we can do essentially because it's simulated we can do exhaustive exploration and just pick off the lowest value now for this particular example there's a fun other way to find it we know at the bottom of this curve the slope is 0. it's the only place in the whole curve where that's true where it's flat we can use a little bit of calculus to find that feel free to tune out if calculus is not your thing but it's not too bad so we find the slope of the loss function with respect to our guesses and we set it equal to 0 and we solve it to find what for what guess is that true so we take our loss function this sum of the square of the differences of the count and the guess and we take the derivative of that with respect to our guess and the derivative of a sum is the same as the sum of the derivatives we take the derivative of that just bring down the exponent so two times that summed because all that's equal to zero we can divide it by two and it'll still be true so now the sum of our deviations is 0. so to further simplify this it's the sum of all of the counts of the actual bags times the sum of our guess once for each bag if we have m bags then it's m times our guess and then we can move that to the other side of the equal sign and divide both sides by the number of bags m and what we get is that our best guess is the sum total of the number of m ms we found in all the bags divided by the number of bags or the average count per bag so this is a really slick result and it's things like this that make people so excited about optimization with a little bit of math and calculus you can get this nice theoretical result um now it's worth noting that this is only true if you use a deviation squared as your cost function so that's one reason people like it so much is because it tends to give some nice results like this but there is this analytical shortcut to find what the best guess is we're going to come back to this in a few minutes now how does optimization change how do we use it in our neural network to find these weights and these features so what we want to do we know what our error function is it's how wrong our guesses are so in this case we have a labeled data set which means that a human has already looked at this input on the left and they said hey that's a horizontal image the truth values are what we know should be the right answer zero votes for everything except horizontal that should have a vote of one so let's say initially we've got a neural network that all the weights are random and it gives us nonsense results it says well yeah everything is has some number associated with it but it's nothing like the right answer well we can find the error for each category and add it up and find a total error and this is how wrong our neural network would be for this one example here's our loss here's our error now the idea with gradient descent is we're not just adjusting one thing we're not just adjusting our guess of the number of m ms we're adjusting many things we want to go through and adjust every single weight in every single layer to bring this error down a little bit now that is a little bit challenging to do because in order to do that one thing you can do is find a analytical solution like we did before to go through and move our guess a little bit up and a little bit down and find the slope is really expensive when you consider that this is not a onedimensional problem anymore it might have hundreds or millions of different weights that we need to adjust so calculating that gradient that slope requires hundreds or millions of more passes through the neural network to find out which direction is downhill enter back propagation so remember we found the nice analytical solution to what we had going on in the case of the m m estimate so we would love to be able to do something like that again if we had an analytical solution we could jump right to the right answer so slope in this case it's change in weight or sorry it's change in error for a given change in weight that's the slope here so there's lots of ways to write that but delta error delta weight d air d wait we'll use this partial error partial weight just because it's most correct but all these things mean the same thing if i change the weight by one how much will the error change what is the slope so in this case it would be 2 and we would know that we need to increase the weight in order to get closer to the bottom this tells us not only the direction we need to move but gives us a sense of about how far we should go doesn't tell us exactly where the bottom is but it tells us which way it needs to adjust now if we do know the error function example we can make a an analytic solution and we can find that we can calculate that slope exactly so in this case the change in error for a given change in weight is just the derivative of our error function here which is in this case is the weight squared so the derivative is two times the weight the weight is minus one and so the answer is oh a slope of minus two that tells us what we need to know about which way to adjust now with neural networks of course they're a lot more complex than that but we can actually analytically compute the slope of the function where we are we don't know where the minimum is but we can find the slope without having to recalculate the value of everything each time and this is how it works imagine the world's most trivial neural network that has one input one output one hidden layer with one neuron in it so it's got an input connected by a weight w1 to an intermediate value connected by a weight w2 to an output value so the intermediate value is just x times that weight so the derivative of y with respect to the weight is x what that means is if i change w1 and i move it by one then the value of y will change by the value x whatever x is we have the slope of this piece of the function similarly it's straightforward we can just read off that whatever the value of y is multiply it by the weight w2 we get e so if we want to find the slope of the error function for a given change in y the answer is w2 if i changed y by one unit then the error changes by the amount w two now chaining means that we can take these two things and just multiply them together so by inspection we can see that in this little neural network if we take x multiply it by w1 multiply that by w2 we get the error e now what we'd like to know is if i change that w1 by a certain amount how much does the error change well in this case we just take that whole expression and take the derivative derivative with respect to w1 and fairly trivial bit of calculus it comes out to be x times w2 and what we can see then is we can substitute in these steps this change in y with respect to w1 is the same as x w 2 is the same as the change in error with respect to y and what this breaks down is if we want to step down the chain we want to know how much a change in w1 affects the error what is d e d w 1 we can actually break it down into steps and say okay well if i change w 1 how much does y change and then if i change y how much does the error change this is chaining and this is what lets us if we know which way we want to change the error it lets us calculate how much we can change this weight to help that happen and there's nothing to prevent us from doing this again and again if i have a weight that's deep into my neural network and i want to know how much my error is going to change if i tweak it up or down i want to know the slope of my loss function with respect to that weight then i can just break it down and say okay well if i change the weight how much does a change if i change a how much does b change if i change b how much does c change and chain it all the way down now it's called back propagation because in order to calculate it we actually need the value at the end we have to start with the error value in order to calculate each of these all the way back down into the depths of the network but still we can do that now the way the reason you have to go backwards is that let's say we want to know what this should be if i change the error if i change a how much does the error change it's like well let's assume that i already know how much the error is going to change if i change b what is this back propagation step what is the additional link i need to add to this chain it's like well it's how much does b change if i change a if they're connected by a weight then how do i incorporate that weight we know that two neurons connected in this way are represented by this b is the weight times the value of a and so we can just take a little derivative here and get the change in b with respect to a is w so this step in the back propagation change can be represented by whatever that weight is cool now we know that we have sums in our neural network that's another thing we have to handle if i know how much my error changes with a change in z then how much would it change with a change in one of the inputs to this to z where that input goes into a sum well i have can write the expression for z adding up all the inputs if i want to know how much z changes with respect to a change in a i just take the derivative and it is turns out to be one so this is a trivial back propagation step now the most interesting one of all if i know how much the error changes with respect to a change in b and then i want to know how much it changes with the input and to that sigmoid function then i can just say okay well a sigmoid function mathematically looks like this and i can take the derivative of b with respect to a and um one of the beautiful things about the sigmoid function is that the derivative actually looks like this um it's just the value of the function times one minus the value of the function which is one of the reasons that sigmoids perhaps are so popular in deep neural networks so this step is also straightforward to calculate in none of these steps have we had to recalculate all of the values in the neural network we've been able to rely on things that have already been calculated what the values are at each of these neurons that's what makes back propagation so mathematically efficient and is what allows us to efficiently train neural networks that is why each element in a neural network no matter how exotic it is needs to remain differentiable so that we can go through this exercise of finding what the link in the chain is when we're doing the chain rule on our derivatives so that we can compute the back propagation we can back propagate it and again rectified linear units if we know how much the output affects a change in error we want to know how that extends to the input we can write the function of a rectified linear unit we can take the derivative of it and then use that in our chain rule so imagine now we have this labeled example we calculate the answer that this random neural network that's not special at all it'll give an answer that's completely wrong and then we back propagate the error and adjust every one of those weights a little bit in the right direction and we do that again and again after you do that a few thousand times this stochastic gradient descent goes from this fully connected totally random neural network to something that is a lot more efficient and that is able to give answers that are much closer to the right answer so coming back up to our convolutional neural networks these are the fully connected layers that's how they're trained they can also be stacked this back propagation up applies not only to these fully connected layers but also to the convolutional layers and the pooling layers we won't go through and calculate the chain rule for them but you can do that as well and going through this this whole stack of different layers gets trained on a bunch of examples in this cases of labeled x's and o's give it a bunch of inputs that we know the right answer to and we let it adjust all of those connections not only that it also adjusts all of the pixels in the features for each convolutional layer so it learns not only the weights but also the features and then over time those representations become something that lets it predict very well what is an x and what is an o on top of that there are other things that we can use optimization for so there's a bunch of decisions here that we haven't addressed yet how do we know how many features to put in each convolutional layer how do we know how big those should be how many pixels on a side how do we choose the size and stride of our pooling windows in our fully connected layers how many layers do we have and how many hidden neurons do we put in each each of these decisions are called hyper parameters they are also values that we get to choose but they're the next level up they kind of control how everything happens below and in order to see how well they perform we have to train the whole thing on all the images start to finish so but the same principles apply we can adjust these and choose them to get the best result possible in a lot of cases it's worth pointing out that there's just not enough computation available in the world to try out all the possible examples and so what we have right now are some recipes some things that researchers have stumbled onto that seem to work well and they get reused but there are a lot of places a lot of combinations of these hyper parameters that actually haven't been tried yet and so there is always the possibility that there are some combinations that work even much better than what we've seen so far now uh we don't have to use convolutional neural networks for just images any twodimensional or threedimensional data works well the thing that matters is that in this data things that are closer together are more closely related than things far away it matters if two things are in adjacent rows or in adjacent columns so in images this is plainly the case the location of a pixel in an array of pixels is part of the information if you were to randomly jumble the rows and columns that would lose the information that's there that's what makes this well suited to convolutional neural networks anything you can make look like an image may also be suited to convolutional neural networks for instance if you're working with audio you have a really nice xaxis your columns can be subsequent time steps you don't want to jumble those because the time the order in which things occur in time matters and you can make your rows the intensity in different frequency bands going from low frequency to high frequency again the order matters there and so being able to take sound then and make it look like an image you can apply this processing to it and find patterns in the sound that you wouldn't be able to find conveniently any other way you can also do this with text with a little bit of work you can make each of your rows a different word in the dictionary and then you can make your columns the position and sentence or position location that occurs in time now there are some limitations here convolutional neural networks only capture local spatial patterns so if your data can't be made to look like an image or if it doesn't make sense to then they're less useful so for example imagine you have customer data that has columns representing things like names and ages addresses emails purchases transactions browsing histories and these customers are listed if you were to rearrange the rows or rearrange the columns the information itself wouldn't really be compromised it would all still be there it would also be queryable searchable and interpretable convolutional neural networks don't help you here they look for spatial patterns so if the spatial organization of your data is not important it will not be able to find what matters so rule of thumb if your data is just as useful after swapping your columns with each other then you can't use convolutional neural networks you shouldn't use convolutional neural networks that's a big takeaway from this so they're really good at finding patterns and using them to classify images that is what is they are the best at now the takeaway from this is not that you should go and code up your own convolutional neural networks from scratch um you can it's a great exercise it's a lot of fun but when you go to actually use it there are a lot of mature tools out there that are helpful and just waiting to be applied to this the takeaway from this is that you will be asked to make a lot of subtle decisions about how to prepare your data and feed it in how to interpret the results and how to choose these hyper parameters for this it helps to know what's going to be done with your data and what it all means so you can get the most out of these tools all right good luck
