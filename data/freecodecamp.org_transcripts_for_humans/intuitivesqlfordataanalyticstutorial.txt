With timestamps:

00:00 - learn SQL for analytics Vlad is a data
00:03 - engineer and in this course he covers
00:05 - both the theory and the practice so you
00:07 - can confidently solve hard SQL
00:09 - challenges on your own no previous
00:12 - experience required and you'll do
00:14 - everything in your browser using big
00:17 - query hi everyone my name is Vlad and
00:20 - I'm a date
00:21 - engineer welcome to intuitive SQL for
00:25 - analytics this here is the main web page
00:28 - for the course you will find it in the
00:31 - video description and this will get
00:33 - updated over time with links and
00:36 - resources so be sure to bookmark it now
00:39 - the goal of this course is to quickly
00:42 - enable you to use SQL to analyze and
00:46 - manipulate data this is arguably the
00:49 - most important use case for SQL and the
00:53 - Practical objective is that by the end
00:55 - of this course you should be able to
00:58 - confidently solve hard SQL problems of
01:01 - the kind that are suggested during data
01:05 - interviews the course assumes no
01:08 - previous knowledge of SQL or programming
01:11 - although it will be helpful if you've
01:13 - work with spreadsheets such as Microsoft
01:16 - Excel or Google Sheets because there's a
01:19 - lot of analogies between manipulating
01:22 - data in spreadsheets and doing it in SQL
01:25 - and I also like to use spreadsheets to
01:28 - explain SQL Concepts
01:30 - now there are two parts to this course
01:34 - theory and
01:35 - practice the theory part is a series of
01:40 - short and sweet explainers about the
01:43 - fundamental concepts in SQL and for this
01:46 - part we will use Google
01:49 - bigquery bigquery which you can see here
01:52 - is a Google service that allows you to
01:55 - upload your own data and run SQL on top
01:59 - of it so in the course I will teach you
02:02 - how to do that and how to do it for free
02:05 - you won't have to to spend anything and
02:08 - then we will load our data and we will
02:10 - run SQL code and besides this there will
02:14 - be drawings and we will also be working
02:18 - with spreadsheets and anything it takes
02:21 - to make the SQL Concepts as simple and
02:24 - understandable as
02:26 - possible the practice part involves
02:29 - doing SQL exercises and for this purpose
02:32 - I recommend this website postest SQL
02:36 - exercises this is a free and open-source
02:39 - website where you will find plenty of
02:42 - exercises and you will be able to run
02:45 - SQL code to solve these exercises check
02:48 - your answer and then see a suggested way
02:51 - to do it so I will encourage you to go
02:54 - here and attempt to solve these
02:56 - exercises on your own however I have
03:00 - also solved 42 of these exercises the
03:05 - most important ones and I have filmed
03:08 - explainers where I solve the exercise
03:12 - break it apart and then connect it to
03:14 - the concepts of the course so after
03:17 - you've attempted the exercise you will
03:19 - be able to see me solving it and connect
03:23 - it to the rest of the course so how
03:26 - should you take this course there are
03:29 - actually many ways to do it and you're
03:31 - free to choose the one that works best
03:34 - if you are a total beginner I recommend
03:37 - doing the following you should watch the
03:40 - theory lectures and try to understand
03:43 - everything and then once you are ready
03:47 - you should attempt to do the exercises
03:50 - on your own on the exercise uh website
03:53 - that I've shown you here and if you get
03:56 - stuck or after you're done you can Watch
04:00 - How I solved the exercise but like I
04:03 - said this is just a suggestion and uh
04:06 - you can combine theory and practice as
04:08 - you wish and for example a more
04:11 - aggressive way of doing this course
04:14 - would be to jump straight into the
04:16 - exercises and try to do them and every
04:20 - time that you are stuck you can actually
04:23 - go to my video and see how I solved the
04:26 - exercise and then if you struggle to
04:29 - understand the solution that means that
04:31 - maybe there's a theoretical Gap and then
04:33 - you can go to the theory and see how the
04:35 - fundamental concepts work so feel free
04:38 - to experiment and find the way that
04:40 - works best for you now let us take a
04:44 - quick look at the syllabus for the
04:46 - course so one uh getting started this is
04:50 - a super short explainer on what SQL
04:53 - actually is and then I teach you how to
04:56 - set up bigquery the Google service where
04:59 - we will load our data and run SQL for
05:02 - the theory
05:04 - part the second uh chapter writing your
05:07 - first query so here I explained to you
05:10 - how big query works and how you can use
05:13 - it um and how you are able to take your
05:16 - own data and load it in big query so you
05:19 - can run SQL on top of it and at the end
05:22 - of it we finally run our first SQL query
05:25 - chapter 3 is about exploring some ESS
05:29 - IAL SQL Concepts so this is a short
05:33 - explainer of how data is organized in
05:37 - SQL how the SQL statement Works meaning
05:40 - how we write code in SQL and here is
05:44 - actually the most important concept of
05:47 - the whole course the order of SQL
05:50 - operations this is something that is not
05:53 - usually taught properly and a lot of
05:55 - beginners Miss and this causes a lot of
05:58 - trouble when you're you're trying to
06:00 - work with SQL so once you learn this
06:02 - from the start you will be empowered to
06:04 - progress much faster in your SQL
06:09 - knowledge and then finally we get into
06:11 - the meat of the course this is where we
06:14 - learn all the different components in
06:17 - SQL how they work and how to combine
06:19 - them together so this happens in a few
06:22 - phases in the first phase we look at the
06:25 - basic components of SQL so these are uh
06:29 - there's a few of them uh there's select
06:31 - and from uh there's learning how to
06:34 - transform columns the wear filter the
06:37 - distinct Union order by limit and then
06:40 - finally we see how to do simple
06:43 - aggregations at the end of this part you
06:46 - will be empowered to do the first batch
06:50 - of
06:51 - exercises um don't worry about the fact
06:55 - that there's no links yet I will I will
06:57 - add them but this is basically involves
07:00 - going to this post SQL exercises website
07:04 - and going here and doing this uh first
07:07 - batch of exercises and like I said
07:10 - before after you've done the exercises
07:12 - you can watch the video of me also
07:15 - solving them and breaking them down next
07:18 - we take a look at complex queries and
07:21 - this involves learning about subqueries
07:24 - and Common Table
07:26 - expressions and then we look at joining
07:29 - tables so here is where we understand
07:31 - how SQL tables are connected uh with
07:34 - each other and how we can use different
07:36 - types of joints to bring them together
07:40 - and then you are ready for the second
07:42 - batch of exercises which are those that
07:45 - involve joints and subqueries and here
07:48 - there are eight exercises the next step
07:51 - is learning about aggregations in SQL so
07:54 - this involves the group bu the having
07:57 - and window functions and then finally
08:00 - you are ready for the final batch of
08:03 - exercises which actually bring together
08:06 - all the concepts that we've learned in
08:08 - this course and these are 22 exercises
08:12 - and like before for each exercise you
08:14 - have a video for me solving it and
08:16 - breaking it apart and then finally we
08:19 - have the conclusion in the conclusion we
08:22 - see how we can put all of this knowledge
08:25 - together and then we take a look at how
08:27 - to use this knowledge to actually go out
08:30 - there and solve SQL challenges such as
08:33 - the ones that are done in data
08:35 - interviews and then here you'll find uh
08:38 - all the resources that are connected to
08:40 - the course so you have the files with
08:43 - our data you have the link to the
08:45 - spreadsheet that we will use the
08:48 - exercises and all the drawings that we
08:50 - will do this will definitely evolve over
08:53 - over time as the course evolves so
08:56 - bookmark this page and keep an eye on it
08:59 - that was that was all you needed to know
09:00 - to get started so I will see you in the
09:04 - course if you are working with SQL or
09:07 - you are planning to work with SQL you're
09:09 - certainly a great company in the 2023
09:12 - developer survey by stack Overflow there
09:15 - is a ranking of the most popular
09:17 - Technologies out there if we look at
09:19 - professional developers where we have
09:21 - almost 70,000 responses we can see that
09:25 - SQL is ranked as the third most popular
09:28 - technology SQL is certainly one of the
09:31 - most in demand skills out there not just
09:33 - for developers but for anyone who works
09:36 - with data in any capacity and in this
09:39 - course I'm going to help you learn SQL
09:41 - the way I wish I would have learned it
09:43 - when I started out on my
09:46 - journey since this is a practical course
09:50 - we won't go too deep into the
09:52 - theory all you need to know for our
09:54 - purposes is that SQL is a language for
09:57 - working with data
09:59 - like most languages SQL has several
10:03 - dialects you may have heard of post SQL
10:06 - or my sqil for
10:08 - example you don't need to worry about
10:10 - these dialects because they're all very
10:13 - similar so if you learn SQL in any one
10:16 - of the dialects you'll do well on all
10:19 - the
10:20 - others in this course we will be working
10:23 - with B query and thus we will write SQL
10:26 - in the Google SQL dialect
10:31 - here is the documentation for Google big
10:33 - query the service that we will use to
10:35 - write SQL code in this
10:38 - course you can see that big query uses
10:41 - Google SQL a dialect of SQL which is an
10:47 - compliant an compliant means that Google
10:50 - SQL respects the generally recognized
10:54 - standard for creating SQL dialects and
10:56 - so it is highly compatible with with all
11:00 - other common SQL
11:02 - dialects as you can read here Google SQL
11:06 - supports many types of statements and
11:08 - statements are the building blocks that
11:11 - we use in order to get work done with
11:15 - SQL and there are several types of
11:17 - statements listed here for example query
11:20 - statements allow us to retrieve and
11:23 - analyze and transform
11:26 - data data definition language statements
11:29 - allow us to create and modify database
11:32 - objects such as tables and
11:35 - Views whereas data manipulation language
11:38 - statements allows us to update and
11:41 - insert and delete data from our
11:44 - tables now in this course we focus
11:47 - exclusively on query statements
11:50 - statements that allow us to retrieve and
11:53 - process
11:55 - data and the reason for this is that if
11:58 - you're going to start working with big
12:00 - query you will most likely start working
12:03 - with this family of
12:05 - statements furthermore query statements
12:08 - are in a sense the foundation for all
12:11 - other families of statements so if you
12:14 - understand uh query statements you'll
12:16 - have no trouble learning the others on
12:19 - your
12:20 - own why did I pick big query for this
12:23 - course I believe that the best way to
12:26 - learn is to load your own data and
12:30 - follow questions that interest you and
12:33 - play around with your own projects and P
12:36 - query is a great tool to do just
12:39 - that first of all it is free at least
12:43 - for the purposes of learning and for the
12:46 - purposes of this course it has a great
12:49 - interface that will give you U really
12:52 - good insights into your
12:54 - data and most importantly it is really
12:58 - easy to get
12:59 - started you don't have to install
13:03 - anything on your computer you don't have
13:05 - to deal with complex software you just
13:07 - sign up for Google cloud and you're
13:09 - ready to
13:11 - go and finally as you will see
13:15 - next big query gives you many ways to
13:19 - load your own data easily and quickly
13:22 - and get started writing SQL right
13:27 - away I will now show you how you can
13:30 - sign up for Google cloud and get started
13:32 - with bigquery so it all starts with this
13:35 - link which I will share in the resources
13:39 - and this is the homepage of Google cloud
13:43 - and if you don't have an account with
13:45 - Google Cloud you can go here and select
13:47 - sign
13:50 - in and here you need to sign in with
13:53 - your Google account which you probably
13:56 - have but if you don't you can go here
13:59 - and select create
14:02 - account so I have now signed in with my
14:05 - Google account which you can see here in
14:07 - the upper right corner and now I get a
14:10 - button that says start free so I'm going
14:13 - to click
14:15 - that and now I get taken to this page
14:19 - and on the right you see that the first
14:21 - time you sign up for Google Cloud you
14:24 - get $300 of free credits so that you can
14:27 - try the services and that's pretty
14:31 - neat and here I have to enter some extra
14:35 - information about myself so I will keep
14:37 - it as is and agree to the terms of
14:40 - service and
14:42 - continue finally I need to do the
14:45 - payment information verification so
14:48 - unfortunately this is something I need
14:50 - to do even though I'm not going to be
14:53 - charged for the services and this is for
14:56 - Google to be able to verify my my
15:00 - identity so I will pick individual as
15:03 - account type and insert my
15:08 - address and finally I need to add a
15:12 - payment method and again uh I need to do
15:15 - this even though I'm not going to
15:18 - pay I will actually not do it here
15:21 - because I don't intend to sign up but
15:23 - after you are done you can click Start
15:26 - my free trial and then you should be
15:29 - good to
15:30 - go now your interface may look a bit
15:33 - different but essentially after you've
15:36 - signed up for Google Cloud you will need
15:38 - to create a project and the project is a
15:43 - tool that organizes all your work in
15:46 - Google cloud and essentially every work
15:49 - that you do in Google cloud has to
15:52 - happen inside a specific
15:55 - project now as you can see here there is
15:59 - a limited quota of projects but that's
16:02 - not an issue because we will only need
16:05 - one project to work in this
16:08 - course and of course creating a new
16:11 - project is totally
16:13 - free so I will go ahead and give it a
16:18 - name and I don't need any organization
16:22 - and I will simply click on
16:26 - create once that's done I can go back
16:29 - back to the homepage for Google cloud
16:31 - and here as you can see I can select a
16:35 - project and here I find the project that
16:38 - I have created before and once I select
16:41 - it the rest of the page won't change but
16:44 - you will see the name of the project in
16:46 - the upper bar
16:48 - here now although I've created this
16:52 - project as an example for you for the
16:54 - rest of the course you will see me
16:56 - working within this other project which
16:59 - was the one that I had
17:02 - originally now I will show you how you
17:05 - can avoid paying for Google cloud
17:07 - services if you don't want to so from
17:10 - the homepage you have the search bar
17:12 - over here and you can go here and write
17:17 - billing and click payment overview to go
17:21 - to the billing
17:24 - service now here on the left you will
17:27 - see your billing account account which
17:29 - could be called like this or have
17:31 - another name and clicking here I can go
17:34 - to manage billing
17:39 - accounts now here I can go to my
17:41 - projects Tab and I see a list of all of
17:45 - my projects in Google
17:48 - cloud and a project might or might not
17:52 - be connected to a billing account if a
17:55 - project is not connected to a billing
17:57 - account then then Google won't be able
18:01 - to charge you for this
18:03 - project although keep in mind that if
18:06 - you link your project with a billing
18:08 - account and then you incur some expenses
18:12 - if you then remove the billing account
18:14 - you will still owe Google Cloud for
18:16 - those uh expenses so what I can do here
18:20 - is go to my
18:22 - projects and on actions I can select
18:26 - disabled building in case I have a
18:28 - billing account
18:30 - connected now while this is probably the
18:33 - shest way to avoid incurring any charges
18:37 - you will see that you will be severely
18:39 - limited in what you can do in your
18:41 - project if that project is not linked to
18:44 - any billing
18:46 - account however you should still be able
18:49 - to do most of what you need to do in B
18:52 - query at least for this course and we
18:56 - can get more insight into how that works
18:58 - by by going to the big query pricing
19:02 - table so this page gives us an overview
19:05 - of how pricing works for big query I
19:08 - will not analyze this in depth but what
19:10 - you need to know is that when you work
19:13 - with bigquery you can fundamentally be
19:15 - charged for two things one is compute
19:19 - pricing and this basically means all the
19:22 - data that bigquery scans in order to
19:26 - return the results that you need when
19:30 - you write your
19:32 - query and then you have storage pricing
19:35 - which is the what you pay in order to
19:38 - store your data inside
19:42 - bigquery now if I click on compute
19:44 - pricing I will go to the pricing table
19:47 - and here you can select the region that
19:51 - uh most reflects where you are
19:53 - located and I have selected Europe here
19:57 - and as you can see you are charged
20:01 - $625 at the time of this video for
20:04 - scanning a terabyte of
20:07 - data however the first terabyte per
20:11 - month is free so every month you can
20:14 - write queries that scan one terabyte of
20:17 - data and not pay for them and as you
20:19 - will see more in detail this is more
20:21 - than enough for what we will be doing in
20:24 - this course and also for for what you'll
20:27 - be doing on your own
20:29 - in order to experiment with
20:32 - SQL and if I go back to the top of the
20:35 - page and then click on storage
20:38 - pricing you can see here that again you
20:41 - can select your region and see um
20:45 - several pricing uh units but here you
20:48 - can see that the first 10 gab of storage
20:52 - per month is free so you can put up to
20:55 - 10 gigabytes of data in B query and you
20:59 - won't need a billing account you won't
21:01 - pay for storage and this is more than
21:04 - enough for our needs in order to learn
21:07 - SQL in short bigquery gives us a pretty
21:11 - generous free allowance for us to load
21:14 - data and play with it and we should be
21:18 - fine however I do urge you to come back
21:21 - to this page and read it again because
21:24 - things may have changed since I recorded
21:28 - this video
21:29 - video to summarize go to the billing
21:33 - service check out your billing account
21:35 - and you have the option to decouple your
21:37 - project from the billing account to
21:39 - avoid incurring any charges and you
21:41 - should still be able to use B query but
21:44 - as a disclaimer I cannot guarantee that
21:47 - things will work just the same uh at the
21:50 - time that you are watching this
21:53 - video so be sure to check the
21:56 - documentation or maybe discuss with
21:58 - Google Cloud support to um avoid
22:02 - incurring any unexpected expenses please
22:06 - do your research and be careful in your
22:09 - usage of these
22:12 - services for this course I have created
22:15 - an imaginary data set with the help of
22:18 - chat GPT the data set is about a group
22:22 - of fantasy characters as well as their
22:24 - items and
22:26 - inventories I then proceed proed to load
22:29 - this data into bigquery which is our SQL
22:33 - system I also loaded it into Google
22:36 - Sheets which is a spreadsheet system
22:39 - similar to Microsoft
22:41 - Excel this will allow me to manipulate
22:44 - the data visually and help you develop a
22:47 - strong intuition about SQL
22:52 - operations I'm going to link a separate
22:54 - video which explains how you can also
22:57 - use chat PT to generate imaginary data
23:01 - according to your needs and then load
23:03 - this data in Google Sheets or
23:05 - bigquery I will also link the files for
23:09 - this data in the description which you
23:12 - can use to reproduce this data on your
23:16 - side next I will show you how we can
23:19 - load the data for this course into
23:22 - bigquery so I'm on the homepage of
23:25 - Google cloud and I have a search bar up
23:27 - here and I can write big query and
23:31 - select it from here and this will take
23:33 - me to the big query
23:36 - page now there is a panel on the left
23:41 - side that appears here if I hover or it
23:43 - could be fixed and this is showing you
23:47 - several tools that you can use within
23:49 - bigquery and you can see that we are in
23:52 - the SQL workspace and this is actually
23:54 - the only tool that we will need for this
23:56 - course so if you if you're seeing this
23:59 - panel on the left I recommend going to
24:02 - this arrow in the upper left corner and
24:04 - clicking it so you can disable it and
24:06 - make more room for
24:08 - yourself now I want to draw your
24:10 - attention to the Explorer tab which
24:14 - shows us where our data is and how it is
24:17 - organized so I'm going to expand it
24:23 - here now data in bigquery and in SQL in
24:28 - general exists in the form of tables and
24:33 - a table looks just like
24:36 - this as you can see here the customer's
24:39 - table it is a collection of rows and
24:43 - columns and it is quite similar to a
24:46 - spreadsheet so this will be familiar to
24:49 - you if you've ever worked with Microsoft
24:51 - Excel or Google Sheets or any
24:54 - spreadsheet
24:55 - program so your data is actually living
24:58 - in a table and you could have as many
25:02 - tables as you need in B query there
25:04 - could be quite a lot of them so in order
25:07 - to organize our tables we use data sets
25:10 - for example in this case my data is a
25:14 - data set which contains the table
25:16 - customers and employee data and a data
25:19 - set is is just that it's a collection of
25:23 - tables and it's similar to how a folder
25:27 - Works in a file setem system it is like
25:29 - a for folder for
25:33 - tables finally in bigquery each data set
25:37 - belongs to a project so you can see here
25:42 - that we have two data sets SQL course
25:44 - and my data and they both belong to this
25:47 - project idelic physics and so on and
25:50 - this is actually the ID of my project
25:53 - this is the ID of the project that I'm
25:55 - working in right now
25:59 - the reason the Explorer tab shows the
26:02 - project as well is that in big query I'm
26:04 - not limited to working with data that
26:07 - leaves in my project I could also from
26:10 - within my project query data that leaves
26:13 - in another project for example the
26:16 - bigquery public data is a project that
26:19 - is not mine but it's actually a public
26:22 - project by bigquery and if I expand this
26:25 - you will see that it contains a
26:27 - collection of of several data sets which
26:31 - are in themselves um collections of
26:34 - tables and I would be able to query
26:37 - these uh tables as
26:39 - well but you don't need to worry about
26:41 - that now because in this course we will
26:44 - only focus on our own data that lives in
26:47 - our own project so this in short is how
26:50 - data is organized in big
26:52 - query now for the purpose of this course
26:55 - I recommend creating a new data set so
26:58 - so that our tables can be neatly
27:00 - organized and to do that I can click the
27:03 - three dots next to the project uh ID
27:06 - over here and select create data
27:09 - set and here I need to pick a name for
27:13 - the data set so I will call this fantasy
27:16 - and I suggest you use the same name
27:19 - because if you do then the code that I
27:21 - share with you will work
27:24 - immediately then as for the location you
27:27 - can select the multi region and choose
27:30 - the region that is closest to you and
27:33 - finally click on create data
27:38 - set so now the data set fantasy has been
27:42 - created and if I try to expand it here I
27:45 - will see that it is empty because I
27:46 - haven't loaded any data
27:49 - yet the next step is to load our tables
27:53 - so I assume that you have downloaded the
27:57 - zip file with the tables and extracted
28:00 - it on your local computer and then we
28:03 - can select the action point here next to
28:06 - the fantasy data set and select create
28:11 - table now as a source I will select
28:15 - upload and here I will click on
28:18 - browse and access the files that I have
28:24 - downloaded and I will select the first
28:27 - table here here which is the characters
28:32 - table the file format is CSV so Google
28:36 - has already understood that and
28:39 - scrolling down here I need to choose a
28:42 - name for my table so I will call it just
28:45 - like the file uh which is characters and
28:49 - very important under schema I need to
28:52 - select autodetect and we will see what
28:54 - this means in a bit but basically this
28:57 - is all we need so now I will select
28:59 - create
29:04 - table and now you will see that the
29:07 - characters table has appeared under the
29:09 - fantasy data set and if I click on the
29:12 - table and then go on preview I will
29:14 - should be able to see my
29:17 - data I will now do the same for the
29:19 - other two tables so again create
29:23 - table source is upload
29:28 - file is
29:31 - inventory repeat the
29:35 - name and select autod
29:43 - detect and I have done the same with the
29:46 - third table so at the end of this
29:48 - exercise the fantasy data set should
29:51 - have three tables and you can select
29:54 - them and go on preview to make sure that
29:57 - the data looks as
30:01 - expected now our data is fully loaded
30:04 - and we are ready to start querying it
30:07 - within big
30:08 - query now let's take a look at how the
30:11 - bigquery interface works so on the left
30:14 - here you can see the Explorer which
30:17 - shows all the data that I have access to
30:21 - and so to get a table in big query first
30:24 - of all you open the name of the project
30:27 - and then you look at the data sets that
30:30 - are available within this project you
30:32 - open a data set and finally you see a
30:35 - table such as characters and if I click
30:38 - now on characters I will open the table
30:43 - view now in the table view I will find a
30:46 - lot of important information about my
30:49 - table in these tabs over here so let's
30:52 - look at the first tab
30:54 - schema the schema tab shows me the
30:58 - structure of my table which as we shall
31:01 - see is very important and the schema is
31:04 - defined essentially by two things the
31:08 - name of each column in my table and the
31:12 - data type of each column so here we see
31:16 - that the characters table contains a few
31:18 - columns such as ID name Guild class and
31:22 - so
31:23 - on and these columns have different data
31:26 - types for example ID is an integer which
31:30 - means that it contains natural numbers
31:33 - whereas name is string which means that
31:36 - it contains
31:38 - text and as we shall see the schema is
31:41 - very important because it defines what
31:44 - you can do with the
31:46 - table and next we have the details tab
31:50 - which contains a few things first of all
31:53 - is the table ID and this ID represents
31:57 - the full address of the
31:59 - table and this address is made up of
32:02 - three components first of all you have
32:05 - the ID of the project which is as you
32:09 - can see the project in which I'm working
32:12 - and it's the same that you see here on
32:14 - the left in the Explorer
32:16 - tab the next component is the data set
32:20 - that contains the table and again you
32:22 - see it in the Explorer Tab and finally
32:24 - you have the name of the table this
32:28 - address is important because it's what
32:30 - we use to reference the table and it's
32:32 - what we use to get data from this
32:36 - table and then we see a few more things
32:38 - about the table such as when it was
32:40 - created when it was last
32:44 - modified and here we can see the storage
32:48 - information so we can see here that this
32:51 - table has 15 rows and on the dis it
32:55 - occupies approximately one
32:59 - kilobyte if you work extensively with P
33:01 - query this information will be important
33:04 - for two reasons number one it defines
33:07 - how much you are paying every month to
33:09 - store this table and number two it
33:13 - defines how much you would pay for a
33:15 - query that scans all the data in this
33:18 - table and as we have seen in the lecture
33:21 - on bigquery pricing these are the two
33:23 - determinants of bigquery costs however
33:25 - for the purpose of this course you don't
33:27 - need to worry about this because the
33:29 - tables we are working with are so small
33:32 - that they won't put a dent in your free
33:35 - month monthly allowance for using big
33:38 - query next we have the preview tab which
33:42 - is really cool to get a sense of the
33:44 - data and this basically shows you a
33:47 - graphical representation of your table
33:49 - and as you will notice it looks very
33:51 - similar to a
33:53 - spreadsheet so you can see our columns
33:56 - the same ones that we saw in the schema
33:58 - tab ID name Guild and so on and as you
34:02 - remember we saw that ID is an integer
34:04 - column so you can only contain numbers
34:07 - name is a text
34:08 - column and then you see that this table
34:11 - has 15 rows and because it's such a
34:14 - small table all of it fits into this
34:16 - graphical representation but in the real
34:19 - world you may have tables with millions
34:21 - of rows and in this case the preview
34:24 - will show you only a small portion of
34:27 - that table table but still enough to get
34:29 - a good sense of the
34:31 - data now there are a few more tabs in
34:34 - the table view we have lineage data
34:37 - profile data quality but I'm not going
34:40 - to look at them now because they are
34:43 - like Advanced features in bigquery and
34:46 - you won't need them in this
34:48 - course instead I will run a very basic
34:52 - query on this
34:53 - table and this is not for the purpose of
34:57 - understanding query that will come soon
34:59 - it is for the purpose of showing you
35:02 - what the interface looks like after you
35:04 - run a
35:06 - query so I have a very basic query here
35:09 - that will run on my table and you can
35:11 - see that the interface is telling me how
35:14 - much data this query will process and
35:17 - this is important because this is the
35:20 - main determinant of cost in bigquery
35:23 - every query scans a certain amount of
35:25 - data and you have to pay for that
35:29 - but as we saw in the lecture of bigquery
35:32 - pricing this table is so small that you
35:35 - could run a million or more of these
35:38 - queries and not exhaust your monthly
35:40 - allowance so if you see 1 kilobyte you
35:43 - don't have to worry about
35:46 - that so now I will click run and my
35:50 - query will execute and here I get the
35:54 - query results view this is the view that
35:57 - that appears after you have successfully
36:00 - run a
36:02 - query so we have a few tabs here and the
36:05 - first step that you see is results and
36:08 - this shows you graphically the table
36:11 - that was returned by your
36:13 - query so as we shall see every query in
36:17 - SQL runs on a table and returns a table
36:22 - and just like the preview tab showed you
36:24 - a graphical view of your table the
36:26 - results tab shows you a graphical view
36:29 - of the table that your query has
36:32 - returned and this is really the only tab
36:35 - in the query results view that you will
36:37 - need on this course the other ones show
36:41 - different features or more advanced
36:43 - features that we won't look at but feel
36:45 - free to explore them on your own if you
36:47 - are
36:48 - curious but what's also important in
36:51 - this view is this button over here save
36:54 - results which you can use to EXP report
36:58 - the result of your query towards several
37:01 - different
37:03 - destinations such as Google drive or
37:06 - local files on your computer in
37:08 - different formats or another big query
37:12 - table a spreadsheet in Google Sheets or
37:15 - even copying them to your clipboard so
37:18 - that you can paste them somewhere
37:21 - else but we shall discuss this more in
37:23 - detail in the lecture on getting data in
37:27 - and out of big
37:29 - query finally if you click on this
37:32 - little keyboard icon up here you can see
37:35 - a list of shortcuts that you can use in
37:39 - the big query interface and if you end
37:42 - up running a lot of queries and you want
37:44 - to be fast this is a nice way to improve
37:48 - your experience with big query so be
37:51 - sure to check these
37:53 - out we are finally ready to write our
37:56 - first query and in the process we will
37:59 - keep exploring the Fantastic bigquery
38:02 - interface so one way to get started
38:05 - would be to click this plus symbol over
38:07 - here so that we can open a new
38:10 - tab now to write the query the first
38:13 - thing I will do is to tell big query
38:16 - where the data that I want leaves and to
38:19 - do that I will use the from Clause so I
38:23 - will simply write from and my data lives
38:26 - in the
38:28 - fantasy data set and in the characters
38:35 - table next I will tell SQL what data I
38:39 - actually want from this table and the
38:41 - simplest thing to ask for is to get all
38:45 - the data and I can do this by writing
38:48 - select
38:52 - star now my query is ready and I can
38:55 - either click run up here or I can press
38:59 - command enter on my Mac
39:03 - keyboard and the query will run and here
39:07 - I get a new tab which shows me the
39:11 - results now the results here are
39:14 - displayed as a table just as uh we saw
39:19 - in the preview tab of the table and I
39:22 - can get an idea of uh my results and
39:25 - this is actually the whole table because
39:27 - this is what I asked for in the
39:31 - query there are also other ways to see
39:34 - the results which are provided by
39:36 - bigquery such as Json which shows the
39:39 - same data but in a different format but
39:41 - we're not going to be looking into that
39:44 - for this
39:45 - course one cool option that the
39:48 - interface provides is if I click on this
39:52 - Arrow right here in my tab I can select
39:55 - split tab to right
39:58 - and now I have a bit of less room in my
40:01 - interface but I am seeing the table on
40:04 - the left and the query on the right so
40:08 - that I can look at the structure of the
40:10 - table while writing my query for example
40:12 - if I click on schema here I could see
40:15 - which columns I'm able to um reference
40:18 - in my query and that can be pretty
40:20 - handy I could also click this toggle to
40:24 - close the Explorer tab temporarily if I
40:26 - don't need to look look at those tables
40:28 - so I can make a bit more room or I can
40:30 - reactivate it when
40:35 - needed I will now close this tab over
40:40 - here go back to the characters table and
40:43 - show you another way that I can write a
40:47 - query which is to use this query command
40:50 - over here so if I click here I can
40:53 - select whether I want my query in a new
40:55 - tab or in a split tab let let me say in
40:57 - new tab and now bigquery has helpfully
41:02 - uh written a temp template for a query
41:05 - that I can easily modify in order to get
41:08 - my
41:09 - data and to break down this template as
41:12 - you can see we have the select Clause
41:14 - that we used before we have the from
41:16 - clause and then we have a new one called
41:21 - limit now the from Clause is doing the
41:24 - same job as before it is telling query
41:28 - where we want to get our data but you
41:30 - will notice that the address looks a bit
41:33 - different from the one that I had
41:36 - used specifically I used the address
41:40 - fantasy.
41:42 - characters so what's happening here is
41:45 - that fantasy. characters is a useful
41:49 - shorthand for the actual address of the
41:52 - table and what we see here that big
41:54 - query provided is the actual full
41:57 - address of the table or in other words
42:00 - it is the table
42:01 - ID and as you remember the table ID
42:05 - indicates the project ID the data set
42:08 - name and the table name and importantly
42:11 - this ID is usually enclosed by back
42:15 - ticks which are a quite specific
42:18 - character long story short if you want
42:21 - to be 100% sure you can use the full
42:24 - address of the table and bigquery will
42:26 - provide it for you but if you are
42:29 - working within the same project where
42:31 - the data lives so you don't need to
42:33 - reference the project you can also use
42:36 - this shorthand here to make writing the
42:38 - address easier and in this course I will
42:42 - use these two ways to reference a table
42:47 - interchangeably I will now keep the
42:49 - address that bigquery provided now the
42:52 - limit statement as we will see is simply
42:54 - limiting the number of rows that will be
42:56 - returned by this query no more than
42:58 - 1,000 rows will be returned and next to
43:01 - the select we have to say what data we
43:04 - want to get from this table and like
43:06 - before I can write star and now my query
43:10 - will be
43:12 - complete before we run our query I want
43:15 - to draw your attention to this message
43:17 - over here this query will process 1
43:21 - kilobyte when run so this is very
43:24 - important because here big query is
43:27 - telling you how much data will be
43:29 - scanned in order to give you the results
43:32 - of this query in this case we are
43:36 - returning um all the data in the table
43:39 - therefore all of the table will be
43:41 - scanned and actually limit does not have
43:45 - any influence on that it doesn't reduce
43:48 - how much data is
43:49 - scanned so this query will scan 1
43:53 - kilobyte of data and the amount of data
43:56 - that scanned by the query is the primary
43:59 - determinant of bigquery
44:04 - costs now as you remember we are able to
44:09 - scan up to one terabyte of data each
44:12 - month within the sandbox program and if
44:15 - we wanted to scan more data then we
44:17 - would have to
44:19 - pay so the question is how many of these
44:22 - queries could we run before running out
44:25 - of our free allowance
44:30 - well to answer that we could check how
44:33 - many kilobytes are in a terabyte and if
44:36 - you Google this the conversion says it's
44:39 - one to um multipli by 10 to the power of
44:44 - 9 which ends up being 1
44:47 - billion therefore we can run 1 billion
44:51 - of these queries each month before
44:55 - running out of our allowance
44:58 - now you understand why I've told you
45:01 - that as long as you work with small
45:03 - tables you won't really run out of your
45:06 - allowance and you don't really have to
45:08 - worry about
45:10 - costs however here's an example of a
45:13 - query that will scan a large amount of
45:15 - data and what I've done here is I've
45:18 - taken one of the public tables provided
45:20 - by big query which I've seen to be quite
45:22 - large and I have told big query to get
45:25 - me all the data for this table and as
45:29 - you can see here big query says that 120
45:33 - gabt of data will be processed once this
45:36 - query
45:38 - runs now you would need about eight of
45:42 - these queries to get over your free
45:45 - allowance and if you had connected
45:49 - to B query you could also be charged
45:51 - money for any extra work that you do so
45:55 - be very careful about this and if you
45:57 - work with large tables always check this
46:00 - message over here before running the
46:03 - query and remember you won't actually be
46:06 - charged until you actually hit run on
46:10 - the
46:12 - query and there you have it we learned
46:14 - how the big query interface works and
46:17 - wrote our first SQL
46:19 - query it is important that we understand
46:22 - how data is organized in SQL so we've
46:26 - already seen a a preview of the
46:28 - characters table and we've said that
46:31 - this is quite similar to how you would
46:33 - see data in a spreadsheet namely you
46:36 - have a table which is a collection of
46:38 - rows and columns and then in this case
46:41 - on every row you have a character and
46:44 - for every character you have a number of
46:47 - information points such as their ID
46:50 - their name their class level and so
46:53 - on the first fundamental difference with
46:56 - the spreadsheet is that if I want to
46:59 - have some data in a spreadsheet I can
47:01 - just open a new one and uh insert some
47:06 - data in here
47:07 - right so ID level name and so on then I
47:12 - could say that I have a character id one
47:15 - who is level 10 and his name is Gandalf
47:20 - and this looks like the data I have in
47:23 - SQL and I can add some more data as well
47:26 - well a new character id 2 level five and
47:31 - the name is
47:34 - frao now I will save this spreadsheet
47:36 - and then some days later someone else
47:39 - comes in let's say a colleague and they
47:42 - want to add some new data and they say
47:45 - oh ID uh is
47:49 - unknown level
47:52 - is um 20.3
47:57 - and the name here and then I also want
48:01 - to uh show their class so I will just
48:04 - add another column here and call this
48:09 - Mage now spreadsheets are of course
48:12 - extremely flexible because you can
48:14 - always um add another column and write
48:17 - in more cells and you can basically
48:20 - write wherever you want but this
48:22 - flexibility comes at a price because the
48:26 - more additions we make to this uh to the
48:29 - data model that is represented here the
48:32 - more complex it will get with time and
48:35 - the more likely it will be that we make
48:37 - confusions or mistakes which is what
48:40 - actually happens in real life when
48:42 - people work with
48:44 - spreadsheets SQL takes a different
48:47 - approach in SQL before we insert any
48:52 - actual data we have to agree on the data
48:56 - model that we are going to use and the
48:59 - data model is essentially defined by two
49:02 - elements the name of our columns and the
49:07 - data type that each column will
49:10 - contain for example we can agree that we
49:13 - will use three columns in our table
49:18 - ID level and name and then we can agree
49:23 - that ID will be an integer meaning that
49:26 - it will contain contain whole numbers
49:28 - level will be a integer as well and name
49:33 - will be a string meaning that it
49:35 - contains
49:36 - text now that we've agreed on this
49:39 - structure we can start inserting data on
49:41 - the table and we have a guarantee that
49:44 - the structure will not change with
49:47 - time and so any queries that we write on
49:51 - top of this table any sort of analysis
49:54 - that we create for this table will also
49:56 - be durable in time because it will have
49:59 - the guarantee that the data model of the
50:02 - table will not
50:04 - change and then if someone else comes in
50:07 - and wants to insert this row they will
50:10 - actually not be allowed to first of all
50:13 - because they are trying to insert text
50:15 - into an integer column and so they're
50:18 - violating the data type of the column
50:20 - and they are not allowed to do that in
50:23 - level they are also violating the data
50:25 - type of the column because this column
50:27 - only accepts whole numbers and they're
50:29 - trying to put a floating Point number in
50:32 - there and then finally there are also
50:34 - violating the column definition because
50:36 - they're they're trying to add a column
50:39 - class that was not actually included in
50:42 - our data model and that we didn't agree
50:45 - on so the most important difference
50:47 - between spreadsheets and SQL is that for
50:50 - each SQL table you have a schema and as
50:54 - we've seen before the schema defines
50:56 - exactly which columns our table has and
50:59 - what is the data type of each
51:02 - column so in this case for the
51:05 - characters table we have several columns
51:08 - uh and here we can see their names and
51:10 - then each column has a specific data
51:12 - types and all the most important data
51:15 - types are actually represented
51:17 - here specifically by integer we mean a
51:21 - whole number and by float we mean a
51:24 - floating Point number
51:27 - string is a piece of text Boolean is a
51:31 - value that is either true or false and
51:35 - time stamp is a value that represents a
51:37 - specific point in
51:39 - time all of this information so the
51:42 - number of columns the name of each
51:44 - column and the type of each column they
51:46 - constitute the schema of the table and
51:49 - like we've said the schema is as assumed
51:53 - as a given when working in SQL and it is
51:55 - assumed that will not change over
51:58 - time now in special circumstances there
52:01 - are ways to alter the schema of a table
52:04 - but it is generally assumed as a given
52:07 - when writing queries and we shall do the
52:10 - same in this
52:11 - course and why is it important to keep
52:15 - track of the data type why is it
52:16 - important to distinguish between integer
52:19 - string
52:20 - Boolean the simple answer is that the
52:23 - data type defines the type of operations
52:26 - that you you can do to a column for
52:28 - example if you have an integer or a
52:30 - float you can multiply the value by two
52:33 - or divide it and so on if you have a
52:36 - string you can turn that string to
52:39 - uppercase or lowercase if you have a
52:42 - time stamp you can subtract 30 days from
52:45 - that specific moment in time and so
52:48 - on so by looking at the data type you
52:51 - can find out what type of work you can
52:53 - do with a column
52:56 - the second fundamental difference from
52:59 - spreadsheets is that spreadsheets are
53:02 - usually
53:03 - disconnected but SQL has a way to define
53:09 - connections between
53:12 - tables so what we see here is a
53:15 - representation of our three tables and
53:18 - for each table you can see the schema
53:20 - meaning the list of columns and their
53:23 - types but the extra information
53:26 - that we have here is the connection
53:29 - between the
53:30 - tables so you can see that the inventory
53:33 - table is connected to the items table
53:36 - and also to the character table moreover
53:40 - the characters table is connected with
53:43 - itself now we're not going to explore
53:46 - this in depth now because I don't want
53:48 - to add too much Theory we will see this
53:51 - in detail in the chapter on
53:55 - joints but it is a fundamental
53:57 - difference from spreadsheets that SQL
54:00 - tables can be clearly connected with
54:03 - each
54:04 - other and that's basically all you need
54:07 - to understand how data is organized in
54:09 - SQL for now you create a table and when
54:14 - creating that table you define the
54:16 - schema the schema is the list of columns
54:21 - and their names and their data types you
54:24 - then insert data into this table
54:27 - and finally you have a way to define how
54:29 - the tables are connected with each
54:32 - other I will now show you how SQL code
54:37 - is structured and give you the most
54:40 - important concept that you need to
54:43 - understand in order to succeed at
54:47 - SQL now this is a SQL statement it is
54:53 - like a complete sentence in the SQL
54:58 - language the statement defines where we
55:02 - want to get our data from and how we
55:06 - want to receive these data including any
55:09 - processing that we want to apply to it
55:13 - and once we have a statement we can
55:16 - select
55:17 - run and it will give us our
55:21 - data now the statement is made up of
55:25 - building block blocks which we call
55:28 - Clauses and in this statement we have a
55:31 - clause for every line so the Clauses
55:34 - that we see here are select from where
55:38 - Group by having order and
55:42 - limit and clauses are really the
55:45 - building blocks that we assemble in
55:47 - order to build
55:51 - statements what this course is about is
55:54 - understanding what each Clause is and
55:57 - how it works and then understanding how
56:00 - we can put together these Clauses in
56:03 - order to write effective
56:05 - statements now the first thing that you
56:07 - need to understand is that there is an
56:10 - order to write in these Clauses you have
56:13 - to write them in the correct order and
56:16 - there is no flexibility there if you
56:19 - write them in the wrong order you will
56:22 - simply get an
56:24 - error for example if I I were to take
56:26 - the work clause and put it below the
56:30 - group
56:31 - Clause you can see that I'm already
56:33 - getting an error here which is a syntax
56:38 - error but you don't have to worry about
56:41 - memorizing this now because you will
56:44 - pick up this order as we learn each
56:47 - clause in
56:50 - turn now the essential thing that you
56:53 - need to understand and that slows down
56:57 - so many SQL Learners is that while we
57:00 - are forced by SQL to write Clauses in
57:03 - this specific order this is not actually
57:06 - the order in which the Clauses are
57:11 - executed if you've interacted with
57:14 - another programming language such as
57:16 - python or or JavaScript you're used to
57:19 - the fact that each line of your program
57:22 - is executed in turn from top to bottom
57:26 - generally speaking and that is pretty
57:29 - transparent to understand but this is
57:32 - not what is happening here in
57:35 - SQL to give you a sense of the order in
57:39 - which these Clauses are run on a logical
57:42 - level what SQL does is that first it
57:46 - reads the
57:48 - from then it does the wear then the
57:52 - group by then the
57:54 - having then it does the select
57:58 - part after the select part is do it does
58:01 - the order by and finally the
58:05 - limit all of this just to show that the
58:09 - order in which operations are executed
58:12 - is not the same as the order in which
58:14 - they're
58:16 - written in fact we can distinguish three
58:21 - orders that pertain to SQL Clauses and
58:25 - this distinction is so important to help
58:29 - you master
58:31 - SQL the first level is what we call the
58:35 - lexical order and this is simply what
58:39 - I've just shown you it's the order in
58:41 - which you have to write these Clauses so
58:44 - that SQL can actually execute the
58:47 - statement and not throw you an
58:50 - error then there's the logical order and
58:54 - this is the order in which the clause
58:56 - are actually run logically in the
58:59 - background and understanding this
59:02 - logical order is crucial for
59:04 - accelerating your learning of
59:08 - SQL and finally for the sake of
59:11 - completeness I had to include the
59:14 - effective order here because what
59:17 - happens in practice is that your
59:19 - statement is executed by a SQL
59:23 - engine and that engine will usually try
59:27 - to take shortcuts and optimize things
59:30 - and save on processing power and memory
59:34 - and so the actual order might be a bit
59:37 - different because the Clauses might be
59:39 - moved around um in the process of
59:43 - optimization but like I said I've only
59:46 - included it for the sake of completeness
59:48 - and we're not going to worry about that
59:51 - level in this
59:54 - course with we are going to focus on
59:58 - mastering the lexical order and The
60:01 - Logical order of SQL
60:05 - Clauses and to help you master The
60:08 - Logical order of SQL Clauses or SQL
60:13 - operations I have created this
60:18 - schema and this is the fundamental tool
60:22 - that you will use in this course
60:27 - this schema as you learn it
60:30 - progressively will allow you to build a
60:33 - powerful mental model of SQL that will
60:35 - allow you to tackle even the most
60:39 - complex and tricky SQL
60:42 - problems now what this schema shows you
60:46 - is all of the Clauses that you will work
60:50 - with when writing SQL statements so
60:53 - these are the building blocks that you
60:56 - will use in order to assemble your
61:01 - logic and then the sequence in which
61:04 - they're shown is corresponding to The
61:08 - Logical order in which they are actually
61:12 - executed and there are three simple
61:16 - rules for you to understand this
61:20 - schema the first rule is that operations
61:25 - are EX executed sequentially from left
61:29 - to
61:31 - right the second rule is that each
61:35 - operation can only use data that was
61:39 - produced by operations that came before
61:42 - it and the third rule is that each
61:46 - operation cannot know anything about
61:50 - data that is produced by operations that
61:54 - follow it
61:56 - what this means in practice is that if
61:59 - you take any of these components for
62:01 - example the having
62:04 - component you already know that having
62:07 - will have access to data that was
62:10 - produced by the operations that are to
62:13 - to its left so aggregations Group by
62:17 - where and
62:18 - from however having will have absolutely
62:22 - no idea of information that is produced
62:26 - by the operations that follow for
62:29 - example window or select or Union and so
62:33 - on of course you don't have to worry
62:36 - about understanding this and memorizing
62:39 - it now because we will tackle this
62:41 - gradually throughout the
62:44 - course and throughout the course we will
62:46 - go back to the schema again and again in
62:50 - order to make sense of the work we're
62:52 - doing and understand the typical errors
62:55 - and Pitfall that happen when working
62:58 - with
62:59 - SQL now you may be wondering why there
63:02 - are these two cases where you actually
63:05 - see two components stacked on top of
63:08 - each other that being from and join and
63:12 - then select an
63:14 - alas these are actually components that
63:16 - are tightly coupled together and they
63:20 - occur at the same place in The Logical
63:23 - ordering which is why I have stacked
63:25 - them like
63:28 - this in this section we tackle the basic
63:32 - components that you need to master in
63:35 - order to write simple but powerful SQL
63:39 - queries and we are back here with our
63:43 - schema of The Logical order of SQL
63:46 - operations which is also our map for
63:49 - everything that we learn in this
63:52 - course but as you can see there is now
63:56 - some empty space in the schema because
63:59 - to help us manage the complexity I have
64:03 - removed all of the components that we
64:06 - will not be tackling in this
64:10 - section let us now learn about from and
64:15 - select which are really the two
64:19 - essential components that you need in
64:22 - order to write the simplest SQL
64:27 - queries going back now to our data let's
64:30 - say that we wanted to retrieve all of
64:32 - the data from the characters table in
64:36 - the fantasy data
64:39 - set now when you have to write a SQL
64:42 - query the first question you need to ask
64:46 - yourself is where is the data that I
64:51 - need because the first thing that you
64:54 - have to do is to retrieve the
64:59 - data which you can then process and
65:03 - display as needed so in this case it's
65:08 - pretty simple we know that the data we
65:11 - want leaves in the characters
65:14 - table once you figured out where your
65:16 - data leaves you can write the from
65:19 - Clause so I always suggest starting
65:22 - queries with the from clause
65:26 - and to get the table that we need we can
65:29 - write the name of the data set followed
65:32 - by a DOT followed by the name of the
65:36 - table and you can see that bigquery has
65:39 - recognized the table
65:41 - here so I have written the from clause
65:45 - and I have specified the address of the
65:48 - table which is where the data
65:51 - leaves and now I can write the select
65:54 - clause
65:56 - and in the select Clause I can specify
65:59 - which Columns of the table I want to
66:03 - see so if I click on the characters
66:08 - table here it will open in a new tab in
66:12 - my panel and as you remember the it
66:15 - shows me here the schema of the table
66:18 - and the schema includes the list of all
66:21 - the
66:22 - columns now I can simply decide that I
66:25 - want to see the name and the
66:27 - guilt and so in the select statement
66:30 - here I will write name and
66:35 - guilt and when I run this I get the
66:38 - table with the two columns that I
66:41 - need and one neat thing about this I
66:44 - could write the columns in any order it
66:48 - doesn't have to be the original order of
66:49 - the schema and the result will show that
66:53 - order and if I I wanted to get all of
66:57 - The Columns of the table I could write
67:00 - them here one by one or I could write
67:03 - star with which is a shorthand for
67:06 - saying please give me all of the
67:14 - columns so this is the corresponding
67:17 - data to our table in Google Sheets and
67:21 - if you want to visualize select in your
67:24 - mind you can imagine it as vertically
67:27 - selecting the parts of the table that
67:30 - you need for example if I were to write
67:33 - select Guild and level this would be
67:37 - equivalent to taking these two columns
67:39 - over
67:40 - here and selecting
67:44 - them let us now think of The Logical
67:47 - order of these operations so first comes
67:51 - the from and then comes the select
67:55 - and this makes logical sense right
67:58 - because the first thing you need to do
68:00 - is to Source the data and later you can
68:04 - select the parts of the data that you
68:07 - need in fact if we look at our schema
68:10 - over here from is the very first
68:14 - component in The Logical order of
68:17 - operations because the first thing that
68:19 - we need to do is to get our
68:23 - data we have seen that the select Clause
68:27 - allows us to get any columns from our
68:30 - table in any order but the select Clause
68:34 - has many other powers so let's see what
68:38 - else we can do with
68:40 - it one useful thing to know about SQL is
68:44 - that you can add comments in the code
68:47 - and comments are parts of text which are
68:50 - not uh executed as code they're just
68:52 - there for you to um keep track track of
68:55 - things or or explain what you are doing
68:58 - so I'm going to write a few comments
69:00 - now and the way we do comments is by
69:03 - doing Dash Dash and now I'm going to
69:06 - show you
69:08 - aliasing aliasing is simply renaming a
69:12 - column so I could take the level column
69:16 - and say as
69:19 - character level provided a new name and
69:22 - after I run this we can see that the
69:24 - name of the colum has
69:27 - changed now one thing that's important
69:30 - to understand as we now start
69:32 - transforming the data with our queries
69:35 - is that any sort of change that we apply
69:39 - such as in this case we change the name
69:41 - of the column it only affects our
69:45 - results it does not affect the original
69:48 - table that we are
69:51 - querying so no matter what we do here
69:54 - moving forward Ward the actual table
69:57 - fantasy characters will not change all
70:01 - that will change are the results that we
70:03 - get after running our
70:06 - query and of course there are ways to go
70:10 - back to Fantasy characters and
70:11 - permanently change it but that is
70:14 - outside the scope for
70:16 - us and going back to our schema you will
70:20 - see that Alias has its own
70:23 - component and it happens happens at the
70:26 - same time as the select
70:28 - component and this is important because
70:31 - as we will see in a bit that it's a
70:34 - common temptation to use these aliases
70:37 - these labels that we give to columns in
70:40 - the phases that precede this stage which
70:42 - typically fails because as our rules say
70:46 - um every component does not have access
70:50 - to data that is computed after it so
70:53 - something that we will come back to
70:56 - now another power of Select that we want
70:59 - to show is
71:01 - constants and constants is the ability
71:05 - of creating new columns which have a
71:08 - constant
71:09 - value for example let's say that I
71:12 - wanted to implement a versioning system
71:15 - for my characters and I would say that
71:18 - right now all the characters I have are
71:21 - version one but then in the future every
71:24 - time I change a character I will
71:26 - increase that version and so that will
71:28 - allow me to keep track of
71:30 - changes I can do that by simply writing
71:34 - one over here in the column
71:39 - definition and when I run this you will
71:42 - see that SQL has created a new column
71:45 - and it has put one for every Row in that
71:49 - column this is why we call it a constant
71:53 - column so if I scroll down down all of
71:55 - it will be one and this column has a
71:58 - weird name because we haven't provided a
72:01 - name for it yet but we already know how
72:04 - to do this we can use the alas sync
72:07 - command to say to call it
72:12 - version and here we
72:14 - go so in short when you write a column
72:18 - name in the select statement SQL looks
72:22 - for that column in the table and gives
72:24 - you that column but when instead you
72:26 - write a value SQL creates a new column
72:29 - and puts that value in every
72:32 - Row the next thing that SQL allows me to
72:36 - do is
72:39 - calculations so let me call the
72:42 - experience column here as
72:45 - well and get my
72:49 - data now one thing I could do is to take
72:52 - experience and divide it by
72:58 - 100 so what we see here is a new column
73:02 - which is the result of this
73:07 - calculation now 100 is a constant value
73:10 - right so you can imagine in the
73:12 - background SQL has created a new column
73:15 - and put 100 in every row and then it has
73:18 - done the calculation between experience
73:20 - and that new column and we get this
73:23 - result and and in short we can do any
73:28 - sort of calculation we want combining
73:31 - current columns and constants as
73:35 - well for example although this doesn't
73:38 - make any sense I could take
73:41 - experience add 100 to it divided by
73:47 - character
73:49 - level and then multiply it by
73:53 - two and and we see that we got an error
73:58 - can you understand why we got this error
74:01 - pause the video and think for a
74:04 - second I am referring to my column as
74:07 - character level but what is character
74:10 - level really it is a label that I have
74:13 - assigned over
74:16 - here now if we go back to our schema we
74:19 - can see that select and Alias happen at
74:23 - the same time so so this is the phase in
74:27 - which we assign our label and this is
74:30 - also the phase in which we try to call
74:32 - our
74:33 - label now if you look at our rules this
74:36 - is not supposed to work because an
74:38 - operation can only use data produced by
74:42 - operations before
74:44 - it and Alias does not happen before
74:47 - select it happens at the same
74:51 - time in other
74:53 - words this part part over here when we
74:56 - say character level is attempting to use
74:59 - information that was produced right here
75:01 - when we assigned the label but because
75:03 - these parts happen at the same time it's
75:06 - not aware of the
75:07 - label all this to say that the logical
75:11 - order of operations matters and that
75:13 - what we want here is to actually call it
75:16 - level because that is the name of the
75:18 - column in the
75:20 - table and now when I run
75:23 - this I get a resulting
75:27 - number and so going back to our original
75:30 - point we are able to combine columns and
75:34 - constants with any sort of arithmetic
75:38 - operations another very powerful thing
75:41 - that SQL can do is to apply
75:45 - functions and a function is a
75:49 - prepackaged piece of logic that you can
75:53 - apply to our data and it works like this
75:57 - there is a function called sqrt which
76:00 - stands for square root which takes a
76:03 - number and computes the square
76:05 - root so you call the function by name
76:08 - and then you open round brackets and in
76:10 - round brackets you provide the argument
76:14 - and the argument can be a constant such
76:16 - as 16 or it can be a column such as
76:23 - level and when I run this you can see
76:26 - that in this case the square root of 16
76:29 - is calculated as four and this creates a
76:31 - constant column and then here for each
76:34 - value of level we have also computed the
76:37 - square
76:38 - root there are many functions in SQL and
76:43 - they vary according to the data type
76:45 - which you provide as you remember we
76:47 - said that knowing the data types of
76:49 - columns such as distinguishing between
76:52 - numbers and text is important because it
76:54 - it allows us to know which operations we
76:57 - can apply and so there are functions
76:59 - that work only on certain data types for
77:02 - example here we see square root which
77:05 - only works on numbers but we also have
77:08 - text functions or string functions which
77:10 - only work on text one of them is upper
77:14 - so if I take upper and provide Guild as
77:17 - an argument what do you expect will
77:21 - happen we have created a new column
77:24 - where the G is shown in all
77:27 - uppercase so how can I remember which
77:30 - functions there are and how to use them
77:33 - the short answer is I don't uh there are
77:37 - many many functions in SQL and here in
77:41 - the documentation you can see a very
77:44 - long list of all the functions that you
77:46 - can use in big query and as we said the
77:51 - functions vary according to the data
77:53 - that they can work on so if you look
77:54 - look here on the left we have array
77:58 - functions um date
78:01 - functions mathematical functions
78:04 - numbering functions time functions and
78:06 - so on and so
78:08 - on it is impossible to remember all of
78:10 - these functions so all you need to know
78:13 - is how to look them up when you need
78:16 - them for example if I know I need to
78:20 - work with numbers I could scroll down
78:22 - here and go to mathematic iCal functions
78:26 - and here I have a long list of all the
78:28 - mathematical functions and I can see
78:31 - them all on the right and I should be
78:33 - able to find the square root function
78:36 - that I have showed
78:38 - you and here the description tells me
78:41 - what the function does and it also
78:43 - provides some
78:46 - examples to summarize these are some of
78:49 - the most powerful things you can do with
78:52 - a select statement not only you can
78:55 - retrieve every column you need in any
78:58 - order you can rename columns according
79:01 - to your needs you can Define constant
79:04 - columns with a value that you choose you
79:08 - can combine columns and constant columns
79:11 - in all sorts of
79:13 - calculations and you can apply functions
79:16 - to do more complex
79:19 - work I definitely invite you to go ahead
79:22 - and put your own data in big query as a
79:24 - I've shown you and then start playing
79:26 - around with select and see how you can
79:29 - transform your data with
79:31 - it one thing worth knowing is that I can
79:35 - also write queries that only include
79:37 - select without the front part that is
79:41 - queries that do not reference a
79:43 - table let's see how that works now after
79:48 - I write select I clearly cannot
79:49 - reference any columns because there is
79:52 - no table but I can still reference
79:54 - constant
79:56 - for example I could say
79:58 - hello one and
80:01 - false and if I run
80:04 - this I get this result so remember in
80:08 - SQL we always query tables and we always
80:10 - get back tables in this case we didn't
80:14 - reference any previous table we've just
80:16 - created constants so what we have here
80:18 - are three columns with constant values
80:22 - and there is only one row in the
80:24 - resulting
80:26 - table this is useful mainly to test
80:29 - stuff so let's say I wanted to make sure
80:32 - that the square root function does what
80:34 - I expect it to do so I could call it
80:36 - right here and uh look at the
80:41 - result let's use this capability to look
80:45 - into the order of arithmetic operations
80:48 - in SQL so if I write an expression like
80:51 - this would you be able to compute the
80:53 - final result
80:56 - in order to do that you should be able
80:58 - to figure out the order in which all
81:01 - these operations are done and you might
81:03 - remember this from arithmetic in school
81:05 - because SQL applies the same order that
81:09 - is taught in
81:11 - school and we could Define the order as
81:14 - follows first you would execute any
81:17 - specific functions that take a number as
81:20 - Target and uh then you have multi
81:24 - multiplication and
81:27 - division then you have addition and
81:31 - subtraction and
81:33 - finally
81:35 - brackets go first so you first execute
81:38 - things that are within
81:40 - brackets so pause the video and apply
81:43 - these rules and see if you can figure
81:45 - out what this result will give
81:48 - us now let's do this operation and do it
81:51 - in stages like we were doing in school
81:54 - so first of all we want to worry about
81:57 - what's in the brackets right so I will
81:59 - now consider this bracket over
82:02 - here and in this bracket we have the
82:05 - multiplication and addition
82:08 - multiplication goes first so first I
82:10 - will execute this which will give me
82:12 - four and then I will have 3 + 4 + 1
82:16 - which should give me
82:19 - 8 next I will copy the rest of the
82:23 - operation and here here I reach another
82:27 - bracket to execute what is in these
82:30 - brackets I need to First execute the
82:33 - function so this is the power function
82:37 - so it takes two and exponentiate it to
82:40 - the power of two which gives four and
82:44 - then 4 minus 2 will give me
82:48 - two and this is what we get now we can
82:52 - solve this line and first of all we need
82:55 - to execute multiplication and division
82:58 - in the order in which they occur so the
83:00 - first operation that occurs here is 4 /
83:03 - 2 which is
83:05 - 2 and I will just copy this for clarity
83:09 - 8 - 2 * 2 /
83:14 - 2 the next operation that occurs now is
83:17 - 2 * 2 which is 4 so that would be 8 - 4
83:22 - / 2
83:25 - and the next operation is 4 / 2 which is
83:29 - two so I will have 8 - 2 and all of
83:32 - these will give me a
83:34 - six now all of these are comments and we
83:37 - only have one line of code here and to
83:39 - see whether I was right I just need to
83:41 - execute this
83:44 - code and indeed I get
83:47 - six so that's how you can use the select
83:50 - Clause only to test your assumptions and
83:54 - uh your operations and a short refresher
83:57 - on the order of arithmetic operations
83:59 - which will be important for solving
84:01 - certain sequal
84:03 - problems let us now see how the where
84:06 - statement works now looking at the
84:08 - characters table I see that there is a
84:11 - field which is called is alive and this
84:14 - field is of type
84:16 - Boolean that means that the value can be
84:19 - either true or false so if I go to the
84:22 - preview
84:23 - here and scroll to the right I can see
84:26 - that for some characters this is true
84:29 - and for others it is
84:31 - false now let's say I only wanted to get
84:34 - those characters which are actually
84:37 - alive and so to write my query I would
84:42 - first write the address of the table
84:45 - which is fantasy
84:48 - characters next I could use the where
84:51 - Clause to get those rows where is a five
84:54 - is
84:56 - true and finally I could do a simple
84:59 - select star to get all the
85:04 - columns and here I see that I only get
85:07 - the rows where is alive is equal to
85:12 - true so where is effectively a tool for
85:16 - filtering table rows it filters them
85:19 - because it only keeps rows where a
85:22 - certain condition is true and discards
85:25 - all of the other
85:27 - rows so if you want to visualize how the
85:30 - wear Filter Works you can see it as a
85:33 - horizontal selection of certain slices
85:36 - of the table like in this case where I
85:39 - have colored all of the rows in which is
85:42 - alive is
85:44 - true now the we statement is not limited
85:48 - to Boolean Fields it's not limited to
85:51 - columns that can only be true or false
85:54 - we can run the we filter on any column
85:58 - by making a logical statement about it
86:02 - for example I could ask to keep all the
86:06 - rows where Health number is bigger than
86:11 - 50 this is a logical statement Health
86:14 - bigger than 50 because it is either true
86:18 - or fals for every row and of course the
86:20 - wh filter will only keep those rows
86:23 - where this statement evaluates to
86:28 - true and if I run this I can see that in
86:32 - all of my results health will be bigger
86:35 - than
86:37 - 50 and I can also
86:40 - combine smaller logical statements with
86:43 - each other to make more complex logical
86:46 - statements for example I could say that
86:48 - I want all the rows where health is
86:51 - bigger than 50 and is a live is equal to
86:56 - true now all of this becomes one big
87:00 - logical statement and again this will be
87:03 - true or false for every row and we will
87:05 - only keep the rows where it is
87:08 - true and if I run this you will see that
87:12 - in the resulting
87:14 - table the health value is always above
87:17 - 50 and is alive is always
87:21 - true in the next lecture we will see in
87:24 - detail how these logical statements work
87:27 - and how we can combine them
87:31 - effectively but now let us focus on the
87:35 - order of operations and how the wear
87:38 - statement fits in there when it comes to
87:41 - the lexical order the order in which we
87:44 - write things it is pretty clear from
87:46 - this example first you have select then
87:49 - from and after from you have the WHERE
87:52 - statement and you have to respect this
87:57 - order when it comes to The Logical order
88:00 - you can see that the where Clause
88:03 - follows right after the from Clause so
88:07 - it is second actually in The Logical
88:11 - order if you think about it this makes a
88:14 - lot of sense because the first thing
88:16 - that I need to do is to get the data
88:19 - from where it Lees and then the first
88:22 - thing I want to do after that is is that
88:25 - I'm going to drop all the rows that I
88:27 - don't need so that my table becomes
88:30 - actually smaller and easier to deal with
88:33 - there is no reason why I should carry
88:36 - over rows that I don't actually need
88:38 - data that I don't actually want and
88:41 - waste memory and processing power on it
88:43 - so I want to drop those unneeded rows as
88:47 - soon as
88:49 - possible and now that you know that
88:52 - where happens at this stage in The
88:54 - Logical order you can avoid many of the
88:58 - pitfalls that happen when you're just
89:00 - learning SQL let's see an
89:04 - example now take a look at this query
89:07 - I'm going to the fantasy characters
89:08 - table and then I'm getting the name and
89:11 - the level and then I'm defining a new
89:13 - column this is simply level divided by
89:16 - 10 and I'm calling this level
89:19 - scaled now let's say that I wanted to
89:22 - only keep the rows that have at at least
89:25 - three as level scaled so I would go here
89:30 - and write aware filter where level
89:34 - scaled bigger than
89:38 - three and if I run this I get an error
89:41 - unrecognized name can you figure out why
89:44 - we get this
89:46 - error level scaled is an alas that we
89:50 - assign in the select stage but the we
89:55 - Clause occurs before the select stage so
89:58 - the we Clause has no way to know about
90:03 - this
90:04 - alias in other
90:06 - words the we Clause is at this point and
90:11 - our rules say that an operation can only
90:15 - use data produced by operations before
90:18 - it so the we Clause has no way of
90:21 - knowing about the label which is a sign
90:24 - at this
90:26 - stage so how can we solve this problem
90:29 - right here the solution is to not use
90:33 - the Alias and to instead repeat the
90:37 - logic of the
90:42 - transformation and this actually works
90:45 - because it turns out that when you write
90:48 - logical statements in the we filter you
90:53 - can not only reference The Columns of
90:56 - the tables but you can also reference
90:59 - operations on
91:01 - columns and this way of writing
91:04 - operations of on columns and
91:07 - combinations between columns works just
91:10 - as what we have shown in the select part
91:14 - so that was all you need to know to get
91:17 - started with the wear clause which is a
91:19 - powerful Clause that allows us to filter
91:22 - out the row that we don't need and keep
91:24 - the rows that we need based on logical
91:28 - conditions now let's delve a bit deeper
91:31 - into how exactly these logical
91:33 - statements work in
91:36 - SQL and here is a motivating example for
91:40 - you this is a selection from the
91:42 - characters table and we have a wear
91:45 - filter and this we filter is needlessly
91:50 - complicated and I did this intentionally
91:53 - because by the end of this lecture you
91:55 - should have no trouble at all
91:57 - interpreting this statement and figuring
91:59 - out for which rows it will be
92:02 - true and likewise you will have no
92:04 - problem writing complex statements
92:07 - yourself or deciphering them when you
92:10 - encounter them in the
92:13 - wild the way that these logical
92:16 - statements work is through something
92:19 - called Boolean algebra which is an
92:22 - essential theory for working with SQL
92:26 - but also for working with any other
92:29 - programming language and is indeed
92:31 - fundamental to the way that computers
92:34 - work and though the name may sound a bit
92:37 - scary it is really easy to understand
92:40 - the fundamentals of Boolean
92:43 - algebra now let's look back at so-called
92:47 - normal algebra meaning the common form
92:51 - that is taught in schools
92:55 - in this algebra you have a bunch of
92:57 - elements which in this case I'm only
93:00 - showing a few positive numbers such as 0
93:03 - 25 100 you also have operators that act
93:08 - on these elements for example the square
93:11 - root symbol the plus sign the minus sign
93:14 - the division sign or the multiplication
93:17 - sign and finally you have operations
93:21 - right so in operations you
93:24 - apply The Operators to your elements and
93:27 - then you get some new elements out of
93:31 - them so here are two different types of
93:34 - operation in one case we take this
93:37 - operator the square root and we apply it
93:40 - to a single element and out of this we
93:43 - get another
93:45 - element in the second kind of operation
93:48 - we use this operator the plus sign to
93:51 - actually combine two elements and again
93:55 - we get another element in
94:00 - return Boolean algebra is actually very
94:04 - similar except that it's simpler in a
94:08 - way because you can only have two
94:11 - elements either true or false those are
94:15 - all the elements that you are working
94:19 - with and of course this is why when
94:22 - there's a Boolean field in SQL it is a
94:26 - column that can only have these two
94:28 - values which are true and
94:31 - false now just like normal algebra
94:35 - Boolean algebra has several operators
94:38 - that we can use to transform the
94:40 - elements and for now we will only focus
94:44 - on the three most important ones which
94:46 - are not and and
94:50 - or and finally in Boolean algebra we
94:54 - also have operations and in operations
94:58 - we combine operators and elements and
95:02 - get back
95:05 - elements now we need to understand how
95:07 - these operators work so let us start
95:10 - with the not operator to figure out how
95:14 - a Boolean operator works we have to look
95:16 - at something that's called a truth table
95:20 - so let me look up the truth table for
95:22 - the not operator
95:24 - and in this Wikipedia article this is
95:27 - available here at logical
95:30 - negation now first of all we see that
95:33 - logical negation is an operation on one
95:37 - logical value what does this
95:40 - mean it means that the not operator
95:45 - works on a single element such as not
95:49 - true or not
95:52 - false and this this is similar to the
95:55 - square root operator in algebra that
95:57 - works on a single element a single
96:02 - number next we can see how exactly this
96:06 - works so given an element that we call P
96:10 - and of course P can only be true or
96:12 - false the negation of p is simply the
96:16 - opposite value so not true is false and
96:21 - not false is true and we can easily test
96:25 - this in our SQL code so if I say select
96:28 - not true what do you expect to
96:32 - get we get
96:34 - false and if I do select not false I
96:38 - will of course
96:41 - get
96:43 - true next let's see how the end operator
96:47 - works so we've seen that the not
96:50 - operator works on a single element
96:54 - on the other hand the end operator
96:56 - connects two elements such as writing
97:00 - true and false and in this sense the end
97:04 - operator is more similar to the plus
97:07 - sign here which is connecting two
97:10 - elements so what is the result of true
97:13 - and false to figure this out we have to
97:16 - go back to our truth tables and I can
97:20 - see here at The Logical conjunction
97:23 - function section which is another word
97:26 - for the end
97:28 - operator now the end operator combines
97:31 - two elements and each element can either
97:35 - be true or false so this creates four
97:39 - combinations that we see here in this
97:42 - table and what we see here is that only
97:47 - if both elements are true then the end
97:50 - operator will return true in any other
97:53 - case it will return
97:56 - false so going back here if I select
98:00 - true and false what do you expect to
98:06 - see I am going to get
98:09 - false and it's only in the case when I
98:12 - do true and true that the result here
98:16 - will be
98:18 - true and finally we can look at the or
98:22 - operator which is also known as a
98:24 - logical disjunction it's also combining
98:27 - two elements it also has four
98:30 - combinations but in this case if at
98:33 - least one of the two elements is true
98:36 - then you get true and only if both
98:39 - elements are false then you get
98:42 - false and so going back to our
98:47 - SQL true or true will of course be
98:52 - true but but even if one of them is
98:55 - false we will still get
98:57 - true and only if both are false we will
99:01 - get
99:02 - false so now you know how the three most
99:06 - important operators in Boolean algebra
99:09 - work now the next step is to be able to
99:13 - solve long and complex Expressions such
99:16 - as this one and you already know how
99:19 - operators work the only information
99:22 - you're missing is the order of
99:24 - operations and just like in arithmetic
99:27 - we have an agreed upon order of
99:29 - operations that helps us solve complex
99:31 - expressions and the Order of Operations
99:34 - is written here first you solve for not
99:38 - then you solve for and and finally for
99:41 - or and as with arithmetic you first
99:45 - solve for the brackets so let's see how
99:49 - that works in practice let us now
99:51 - simplify this expression
99:53 - so the first thing I want to do is to
99:55 - deal with the
99:57 - brackets so if I copy all of this part
100:00 - over
100:02 - here as a comment so it doesn't run as
100:05 - code you will see that this is the most
100:09 - nested bracket the innermost bracket in
100:12 - our expression and we have to solve for
100:14 - this so what is true or true this is
100:18 - true
100:19 - right and now I can copy the rest of my
100:23 - EXP
100:23 - expression up to
100:26 - here and here I can solve the innermost
100:30 - bracket as well so I can say true and
100:34 - what I have here is false and true so
100:38 - this is false right because when you
100:39 - have end both of them need to be true
100:42 - for you to return true otherwise it's
100:44 - false so I will write
100:48 - false moving on to the next line I need
100:51 - to solve what's in the bracket
100:54 - so I can copy the knot and now I have to
100:58 - solve what's in this bracket over here
101:01 - now there are several operators here but
101:04 - we have seen that not has the Precedence
101:07 - right so I will copy true and here I
101:12 - have not false which becomes
101:17 - true and then I can copy the last of the
101:20 - bracket I'm not going to do any more at
101:22 - this step to avoid
101:24 - confusion and then I have or and I can
101:27 - solve for this bracket over here and
101:30 - true and false is actually
101:34 - false moving on I can keep working on my
101:38 - bracket and so I have a lot of
101:41 - operations here but I need to give
101:43 - precedence to the ends so the first end
101:47 - that occurs is this one and that means I
101:49 - have to start with this expression over
101:52 - here true and and true results in
101:56 - true and then moving on I will copy the
101:59 - or over here and now I have another
102:04 - end which means that I have to isolate
102:07 - this expression false and true results
102:11 - in
102:14 - false and finally I can copy the final
102:18 - end because I'm not able to compute it
102:20 - yet because I needed to compute the left
102:22 - side
102:24 - and I can copy the remaining part as
102:28 - well moving on to the next
102:31 - line um I need to still do the end
102:35 - because the end takes precedence and so
102:37 - this is the expression that I have to
102:39 - compute so I will say true or and then
102:44 - this expression false and true computes
102:46 - to
102:48 - false and then copy the
102:51 - rest now let me make some rul over here
102:54 - and go to the next line and I can
102:57 - finally compute this bracket we have
103:00 - true or false which we know is
103:06 - true next I need to invert this value
103:09 - because I have not true which is false
103:13 - and then I have or false and finally
103:17 - this computes to
103:19 - false and now for the Moment of Truth F
103:23 - intended I can run my code and see if
103:28 - the result actually corresponds to what
103:30 - we
103:32 - got and the result is
103:35 - false so in short this is how you can
103:38 - solve complex expressions in Boolean
103:40 - algebra you just need to understand how
103:43 - these three operators work and you can
103:45 - use truth tables like like this one over
103:48 - here to help you with that and then you
103:51 - need to remember to respect the order of
103:54 - operations and then if you proceed step
103:57 - by step you will have no problem solving
104:00 - this but now let's go back to the query
104:03 - with which we started because what we
104:05 - have here is a complex logical
104:09 - statement that is plugged into the wear
104:11 - filter and it isolates only certain rows
104:15 - and we want to understand exactly how
104:17 - this statement works so let us apply
104:21 - what we've just learned about Boolean
104:23 - algebra to decipher this
104:25 - statement now what I've done here is to
104:29 - take the first row of our results which
104:31 - you see here and just copi the values in
104:34 - a comment and then I've taken our
104:36 - logical statement and copied it here as
104:39 - well so let us see what SQL does when it
104:43 - checks for this Row the first thing that
104:47 - we need to do is to take all of these
104:50 - statements in our wear filter
104:53 - and convert them to true or false and to
104:57 - do that we have to look at our
104:59 - data let us start with the first
105:02 - component which is level bigger than 20
105:05 - so for the row that we are considering
105:07 - level is 12 so this comes out as
105:12 - false next I will copy this end and here
105:16 - we have is alive equals true now for our
105:20 - row is alive equals false so this
105:23 - statement computes as
105:26 - false Mentor ID is not null with null
105:31 - representing absence of data in our case
105:35 - Mentor ID is one so it is indeed not
105:38 - null so here we have
105:43 - true and finally what we have in here is
105:47 - class in Mage Archer so we have not seen
105:51 - this before but it should be pretty
105:53 - intuitive this is a membership test this
105:57 - is looking at class which in this case
105:59 - is Hobbit and checking whether it can be
106:02 - found in this list and in our case this
106:05 - is now
106:07 - false so now that we've plugged in all
106:10 - the values for our row what we have here
106:13 - is a classic Boolean algebra expression
106:16 - and we are able to solve this based on
106:19 - what we've
106:21 - learned so let us go and solve this and
106:24 - first I need to deal with the brackets
106:26 - and what I have here I have an end and
106:29 - an or and the end TR takes precedence so
106:32 - false and false is false and I will copy
106:36 - the rest and here I have not false which
106:39 - is
106:41 - true next we have false or true which is
106:45 - true and true and in the end this
106:48 - computes to
106:50 - true now in this case we sort of knew
106:54 - that the result was meant to come out as
106:56 - true because we started from a row that
107:00 - survived the wear filter and that means
107:03 - that for this particular row this
107:06 - statement had to compute as
107:09 - true but it's still good to know exactly
107:13 - how SQL has computed
107:15 - this and understand exactly what's going
107:20 - on and this is how SQL deals with
107:23 - complex logical statements for each row
107:26 - it looks at the relevant values in the
107:28 - row so that it can convert the statement
107:31 - to a Boolean algebra expression and then
107:34 - it uses the Boolean algebra rules to
107:37 - compute a final result which is either
107:39 - true or false and then if this computes
107:42 - as true for the row then the row is kept
107:45 - and otherwise the row is
107:48 - discarded and this is great to know
107:51 - because this way of resoling solving
107:53 - logical statements applies not only to
107:55 - the word component but to all components
107:58 - in SQL which use logical statements and
108:03 - which we shall see in this
108:06 - course let us now look at the distinct
108:09 - clause which allows me to remove
108:12 - duplicate
108:14 - rows so let's say that I wanted to
108:17 - examine the class column in my data so I
108:21 - could simply select it and check out the
108:27 - results so what if I simply wanted to
108:30 - see all the unique types of class that I
108:33 - have in my data this is where distinct
108:36 - comes in handy if I write distinct
108:39 - here I will see that there are only four
108:43 - unique classes in my
108:46 - data now what if I was interested in the
108:50 - combinations between class and guilt in
108:53 - my
108:55 - data so let me remove the distinct from
108:58 - now and add guilt here and for us to
109:02 - better understand the results I'm going
109:04 - to add an
109:10 - ordering and here are the combinations
109:12 - of class and Guild in my data there is a
109:16 - character who is an Archer and belongs
109:18 - to Gondor and there are actually two
109:21 - characters who are archers and belong
109:22 - belong to mirkwood and there are many
109:25 - Hobbits from sholk and so
109:28 - on but again what if I was interested in
109:32 - the unique combinations of class and
109:35 - Guild in my data I could add the
109:38 - distinct keyword
109:42 - here and as you can see there are no
109:45 - more repetitions here Archer and
109:49 - merkwood occurs only once Hobbit and
109:52 - Shar f occurs only once because I'm only
109:55 - looking at unique
109:59 - combinations and of course I could go on
110:02 - and on and add more
110:05 - columns and expand the results to show
110:09 - the unique combinations between these
110:11 - columns so here Hobbit and sherol has
110:15 - expanded again because some Hobbits are
110:18 - alive and others unfortunately are not
110:24 - at the limit I could have a star
110:28 - here and what I would get back is
110:31 - actually my whole data all the 15 rows
110:34 - because what we're doing here is looking
110:38 - at rows that have the same value on all
110:41 - columns rows that are complete
110:43 - duplicates and there are no such rows in
110:45 - the data so when I do select star in
110:48 - this case distinct has no effect
110:53 - so in short how distinct works it looks
110:56 - at the columns that you've selected only
111:00 - those which you have selected and then
111:02 - it looks at all the rows and two rows
111:05 - are duplicate if they have the exact
111:08 - same values on every column that you
111:11 - have selected and then duplicate rows
111:14 - are removed and only unique values are
111:19 - preserved so just like the wear filter
111:22 - the distinct is a clause that removes
111:25 - certain rows but it is more strict and
111:29 - less flexible in a sense it only want
111:31 - does one job and that job is to remove
111:34 - duplicate rows based on your
111:37 - selection and if we look at our map of
111:40 - SQL operations we can place distinct it
111:43 - occurs right after select right and and
111:47 - this makes sense because we have seen
111:49 - that distinct Works only on the columns
111:51 - that you have
111:53 - selected and so it has to wait for
111:55 - select to choose the columns that we're
111:58 - interested in and then we can D
112:01 - duplicate based on
112:04 - those for the following lecture on
112:06 - unions I wanted to have a very clear
112:09 - example so I decided to go to the
112:11 - characters table and split it in two and
112:14 - create two new tables out of it and then
112:17 - I thought that I should show you how I'm
112:19 - doing this because it's a pretty neat
112:22 - thing to know and it will help you when
112:24 - you are working with SQL in
112:27 - bigquery so here's a short primer on yet
112:30 - another way to create a table in
112:33 - bigquery you can use your newly acquired
112:37 - power of writing cql queries to turn
112:40 - those queries into permanent
112:43 - tables so here's how you can do it first
112:46 - I've written a simple query here and you
112:49 - should have no trouble understanding it
112:51 - by now go to the fantasy characters
112:54 - table keep only rows where is alive is
112:57 - true and then get all the columns next
113:00 - we need to choose where the new table
113:02 - will live and how it will be called so
113:05 - I'm placing it also in the fantasy data
113:07 - set and I'm calling it characters
113:10 - alive and finally I have a simple
113:13 - command which is create
113:16 - table now what you see here is a single
113:20 - statement in SQL it's a single command
113:23 - that will create the
113:26 - table and you can have in fact multiple
113:30 - statements within the same
113:34 - code and you can run all the statements
113:37 - together when you hit run the trick is
113:40 - to separate all of them with this
113:44 - semicolon over
113:46 - here the semicolon tell SQL hey this
113:50 - command over here is over and and uh
113:53 - next I might add another
113:57 - one so here we have the second statement
114:00 - that we're going to run and this looks
114:04 - just like the one above except that our
114:07 - query has changed because we're getting
114:10 - rows where is alive is false and then we
114:13 - are calling these table characters
114:17 - dead so I have my two statements they're
114:20 - separated by semicolons and I can just
114:23 - hit
114:24 - run and I will see over here that
114:29 - bigquery is showing me the two
114:32 - statements on two different
114:35 - rows and you can see that they are both
114:38 - done now so if I open my Explorer over
114:42 - here I will see that I have two new
114:45 - tables characters alive and characters
114:49 - dead and if I go here for characters
114:53 - alive is alive will of course be true on
114:57 - every
114:58 - row now what do you think would happen
115:01 - if I ran this script again let's try
115:09 - it so I get an
115:12 - error the error says that the table
115:15 - already exists and this makes sense
115:18 - because I've told SQL to create a table
115:20 - but SQL says that table already exists I
115:24 - cannot create it
115:26 - again so there are ways that we can tell
115:30 - SQL what to do if the table already
115:33 - exists again so that we specify the
115:36 - behavior we want and we are not going to
115:40 - just get an
115:41 - error one way is to say create or
115:45 - replace table fantasy characters alive
115:49 - and what this will do is that if the
115:52 - table already exists uh big query will
115:55 - delete it and then create it again or in
115:58 - other words it will overwrite the
116:02 - data so let's write it down to and let's
116:05 - make sure that this query actually
116:12 - works so when I run this I will get no
116:16 - errors even if the table already existed
116:19 - because bigquery was able to remove the
116:23 - previous table and create a new
116:25 - one alternatively we may want to create
116:29 - the table only if it doesn't exist yet
116:32 - and leave it untouched otherwise so in
116:36 - that case we could say create table if
116:40 - not
116:42 - exists so what this will do is that if
116:45 - this table is already existing big query
116:48 - won't touch it and it won't throw an
116:50 - error but if it doesn't exist it will
116:52 - create
116:53 - it so let us write it down
116:59 - two and make sure that this query runs
117:02 - without
117:05 - errors and we see that also here we get
117:09 - no
117:09 - errors and that in short is how you can
117:13 - save the results of your queries in big
117:16 - query and make them into full-fledged
117:19 - tables that you can save and and create
117:22 - query at will and I think this is a
117:25 - really useful feature if you're
117:27 - analyzing data in big query because any
117:30 - results of your queries that you would
117:32 - like to keep you can just save them and
117:35 - then come back and find them
117:39 - later let's learn about unions now to
117:43 - show you how this works I have taken our
117:46 - characters table and I have split it
117:48 - into two parts and I believe the name is
117:51 - quite self descriptive there is a
117:54 - separate table now for characters who
117:56 - are alive and a separate table for
117:59 - characters who are dead and you can look
118:03 - at the previous lecture to see how I've
118:04 - done this how I've used a query to
118:07 - create two new
118:09 - tables but this is exactly the
118:11 - characters table with you know the same
118:13 - schema the same columns the same times
118:16 - is just split in two based on the E
118:19 - alive
118:20 - column now now let us imagine that we do
118:24 - not have the fantasy. characters table
118:28 - anymore we do not have the table with
118:30 - all the characters because it was
118:33 - deleted or we never had it in the first
118:35 - place and let's pretend that we only
118:37 - have these two tables now characters
118:40 - alive and characters
118:42 - dead and we want to
118:45 - reconstruct the characters table out of
118:47 - it we want to create a table with all
118:50 - the characters how can we do
118:53 - that now what I have here are two simple
118:56 - queries select star from fantasy
118:59 - characters alive and select star from
119:01 - fantasy characters dead so these are two
119:04 - separate queries but actually in big
119:06 - query there are ways to run multiple
119:09 - queries at the same time so I'm going to
119:12 - show you first how to do
119:14 - that now an easy way to do that is to
119:17 - write your queries and then add a
119:20 - semicolon at the end
119:22 - and so what you have here is basically a
119:24 - SQL script which contains multiple SQL
119:27 - statements in this case two and if you
119:29 - hit run all of these will be
119:31 - executed
119:34 - sequentially and when you look at the
119:36 - results so you're not just getting a
119:38 - table anymore because it's not just a
119:40 - single query that has been executed but
119:43 - you can see that there have been two
119:44 - commands uh that have been executed
119:47 - which are here and then for each of
119:49 - those two you can simply click View
119:52 - results and you will get to the familiar
119:54 - results tab for that and if I want to
119:57 - see the other one I will click on the
120:00 - back arrow here and click on the other
120:02 - view results and then I can see the
120:04 - other
120:06 - one another way to handle this is that I
120:10 - can select the query that I'm interested
120:14 - in and then click run and here I see the
120:18 - results so big query has only executed
120:22 - the part that I have
120:24 - selected or I can decide to run the
120:27 - other query in my script select it click
120:30 - run and then I will see the results for
120:32 - that
120:33 - query and this is a pretty handy
120:35 - functionality in big query it's also
120:37 - functionality that might give you some
120:39 - headaches if you don't know about it
120:41 - because if for some reason you selected
120:43 - a part of the code uh during your work
120:46 - and then you just want to run everything
120:48 - you might hit run and get an error here
120:51 - because B queer is only seeing the part
120:53 - that you selected and cannot make sense
120:55 - of it so it's good to know about
120:57 - this but our problem has not been solved
121:00 - yet because remember we want to
121:02 - reconstruct the characters table and
121:05 - what we have here are two queries and we
121:07 - can run them separately and we can look
121:09 - at the results separately but we still
121:11 - don't have a single table with all the
121:14 - results and this is where Union comes
121:17 - into play Union allows me to stack the
121:20 - results from these two tables so so if I
121:23 - take first I will take off the semic
121:25 - columns because this will become a
121:27 - single statement and then in between
121:29 - these two queries I will write Union
121:34 - distinct and when I run
121:36 - this you can verify for yourself we have
121:40 - 15 rows and we have indeed reconstructed
121:43 - the characters
121:45 - table so what exactly is going on here
121:49 - well it's actually pretty simple SQL is
121:52 - taking all of the rows from this first
121:55 - query and then all of the rows for the
121:58 - second query and then it's stacking them
122:01 - on top of each other so you can really
122:03 - imagine the act of horizontally stacking
122:07 - a table on top of the other to create a
122:10 - new table which contains all of the rows
122:13 - of these two queries combined and that
122:16 - in short is what union does now there
122:19 - are a few details that we need to know
122:21 - when working with Union and to figure
122:24 - them out let us look at a toy
122:27 - example so I've created two very simple
122:31 - tables toy one and toy two and you can
122:33 - see how they look in these comments over
122:35 - here they all have three columns which
122:37 - are called imaginatively call One Call
122:40 - two call three and then this is the uh
122:44 - Toy one table and then this is the toy 2
122:48 - table now just like before we can
122:51 - combine this table tabls by selecting
122:53 - all of them and then writing a union in
122:56 - between them now in B query you're not
122:59 - allow to write Union without the further
123:02 - qualifier a keyword and it has to be
123:05 - either all or
123:07 - distinct so you have to choose one of
123:10 - these two and what is the choice about
123:13 - well if you do Union all you will get
123:16 - all of the rows that are in the first
123:18 - table and those that are in the second
123:21 - table
123:22 - regardless of whether they are
123:25 - duplicate okay but with Union distinct
123:29 - you will get again all of the rows from
123:32 - the two tables but you will only
123:34 - consider unique rows you will not get
123:37 - any
123:38 - duplicates now we can see that these two
123:41 - table share a column which is actually
123:43 - identical one through yes over here and
123:47 - the same row over
123:49 - here now if I write Union all I expect
123:53 - the result to include this row
123:56 - twice so let us verify that and you can
124:00 - see that here you have one true yes and
124:02 - at the end you also have one true yes
124:04 - and in total you get four rows which are
124:06 - all the rows in the two
124:09 - tables however if I do Union distinct I
124:13 - expect to get three rows and I expect
124:16 - this row to appear only once and not to
124:19 - be duplicated
124:22 - again you need to make sure you're not
124:25 - selecting any little part of your script
124:27 - before you run it so the whole script
124:30 - will be run and as you can see we have
124:32 - three rows and there are no
124:34 - duplicates now it's interesting that big
124:37 - query actually forces you to choose
124:40 - between all or distinct because in many
124:43 - SQL systems for your information you are
124:47 - able to write Union without any
124:49 - qualifier and in that case it means
124:53 - Union distinct so in other SQL systems
124:58 - when you write Union it is understood
125:00 - that you want Union distinct and if you
125:03 - actually want to keep the duplicate rows
125:06 - you will explicitly write Union all but
125:08 - in big query you always have to
125:10 - explicitly say whether you want Union
125:12 - all or Union
125:14 - distinct now the reason this command is
125:17 - called Union and not like stack or or
125:20 - something else is is that this is a set
125:25 - terminology right this comes from the
125:28 - mathematical theory of sets which you
125:30 - might remember from school and the idea
125:34 - is that a table is simply a set of rows
125:39 - so this table over here is a set of two
125:42 - rows and this table over here is a set
125:44 - of two rows and once you have two sets
125:47 - you can do various set operations
125:49 - between them and the most common
125:51 - operation that we do in SQL is unioning
125:55 - and unioning means combining the values
125:59 - of two
126:00 - sets so you might remember from school
126:03 - the V diagram which is a typical way to
126:07 - visualize the relations between sets so
126:11 - in this simple vent diagram we have two
126:13 - circles A and B which represent two sets
126:17 - and in our case a represents the
126:21 - collection of rows in the first table
126:23 - and B represents the all the rows that
126:26 - are in the second
126:27 - table so what does it mean to Union
126:31 - these sets it means taking all of the
126:34 - elements that are in both sets so taking
126:39 - all of the rows that are in both tables
126:41 - and what is the difference here between
126:44 - union distinct and Union all where you
126:48 - can see that the rows of a are this part
126:51 - over here
126:52 - plus this part over here and the rows of
126:55 - B are this part over here plus this part
126:59 - over here and so when we combine them
127:02 - we're actually counting the intersection
127:04 - twice we are counting this part twice
127:08 - and so what do you do with this double
127:10 - counting do you keep it or do you
127:12 - discard it if you do Union all you will
127:15 - keep it so rows that are in common
127:18 - between A and B will duplicate you will
127:20 - see them twice twice but if you do Union
127:23 - distinct you will discard it and so um
127:26 - you won't have any duplicates in the
127:28 - results so that's one way to think about
127:30 - it in terms of
127:32 - sets but we also know that Union is not
127:36 - the only set operation right there are
127:39 - other set operations a very popular one
127:42 - is the intersect
127:44 - operation now the intersect looks like
127:47 - this right it it says take only the El
127:51 - elements that are in common between
127:53 - these two
127:55 - sets so can we do that in SQL can we say
127:59 - give me only the rows that are in common
128:02 - between the two
128:04 - tables and the answer is yes we can do
128:07 - this and if we go back here we can
128:11 - instead of Union write intersect and
128:14 - then
128:15 - distinct and what do you expect to see
128:18 - after I run this command
128:22 - take a minute to think about
128:26 - it so what I expect to see is to get
128:30 - only the rows that are shared between
128:33 - the two tables now there is one row
128:36 - which is shared between these two tables
128:38 - which is uh the one true yes row which
128:41 - we have seen and if I run this I will
128:44 - get exactly this
128:47 - row so intersect distinct gives me the
128:51 - rows that are shared between the two
128:53 - tables and I have to write intersect
128:55 - distinct I cannot write intersect all
128:58 - because actually doesn't mean anything
129:00 - so it's not going to
129:02 - work and here's another set operation
129:05 - which you might consider which is
129:07 - subtraction so what if I told you give
129:11 - me all of the elements in a except the
129:15 - elements that a shares with B so what
129:18 - would that look on the drawing it would
129:20 - look like this right so this is taking
129:23 - all of the elements that are in a except
129:26 - these ones over here because they are in
129:28 - a but they're also in B and I don't want
129:30 - the elements shared with
129:33 - b and yes I can also do that in squl I
129:37 - can come here and I could say give me
129:40 - everything from Toy one except distinct
129:44 - everything from Toy
129:46 - two and what this means is that I want
129:49 - to get all of my rows from Toy one
129:52 - except the rows that are shared with toy
129:56 - two so what do you expect to see when I
129:59 - run
130:01 - this let's hit run and I expect to see
130:05 - only this row over here because this
130:08 - other row is actually shared with
130:11 - b and this is what I
130:14 - get
130:16 - again you have to write accept distinct
130:19 - you cannot write accept all because it's
130:21 - actually actually doesn't mean
130:23 - anything and keep in mind that unlike
130:26 - the previous two operations which are
130:28 - union and distinct the accept operation
130:31 - is not symmetric right so if I swap the
130:35 - tables over here I actually expect to
130:38 - get a different result right I expect to
130:40 - see this row over here selected because
130:43 - I'm saying give me everything from this
130:45 - table uh Toy 2 except the rows that are
130:49 - shared with toy one so so let us run
130:52 - this and make
130:57 - sure and in fact I get the three through
131:00 - uh maybe row so careful that the accept
131:03 - operation is not symmetric the order in
131:06 - which you put the two tables
131:08 - matters so that was a short overview of
131:11 - Union intersect except and I will link
131:15 - this here which is the bigquery
131:17 - documentation on this and you can see
131:19 - that they're actually called set
131:22 - operators in fact in real life you
131:26 - almost always see Union very rarely you
131:29 - will see somebody using intersect or
131:32 - accept a lot of people also don't know
131:35 - about them but I think it's worth it
131:37 - that we briefly looked at all three and
131:40 - it's especially good for you to get used
131:42 - to thinking about tables as sets of
131:47 - rows and thinking about SQL operations
131:50 - in terms of set set operations and that
131:53 - will also come in handy when we study
131:56 - joints but let us quickly go back to our
131:59 - toy example and there are two essential
132:03 - prerequisites for you to be able to do a
132:06 - union or any type of sort
132:11 - operations number one the tables must
132:14 - have the same number of columns and
132:18 - number two the columns must have the
132:20 - same same data
132:22 - type so as you can see here we are
132:25 - combining toy 2 and toy 1 and both of
132:30 - them have three columns and the First
132:32 - Column is an integer the second is a
132:34 - Boolean and the third is a string in
132:37 - both tables and this is how we are able
132:40 - to combine
132:41 - them so what would happen if I went to
132:44 - the first table and I got only the first
132:47 - two columns and then I tried to combine
132:49 - it
132:51 - you guessed it I would get an error
132:53 - because I have a mismatched column
132:57 - count so if I want to select only the
133:00 - first two columns in a table I need to
133:03 - select only the first two columns in
133:05 - another
133:06 - table and then the union will
133:10 - work now what would happen if I messed
133:13 - up the order of the columns so let's say
133:16 - that here I will select uh column one
133:19 - and column 3
133:21 - and here I will select column one and
133:24 - column
133:26 - two let me run
133:29 - this and I will get an error because of
133:33 - incompatible types string and
133:36 - bull so what's happening here is
133:40 - that SQL is trying to get the values of
133:43 - call three over here and put it into
133:46 - call two over here and it's trying to
133:51 - get a string and put it into a Boolean
133:54 - column and that simply doesn't work
133:56 - because as you know SQL enforces streak
133:59 - Types on columns and so this will not
134:03 - work but of course I could select call
134:07 - three in here as
134:09 - well and now again we will have a string
134:13 - column going into a string column and of
134:16 - course this will
134:19 - work so so to summarize you can Union or
134:25 - intersect or accept any two tables as
134:29 - long as they have the same number of
134:32 - columns and the columns have the same
134:36 - data
134:37 - types let us now illustrate a union with
134:41 - a more concrete example so we have our
134:45 - items table here and our characters
134:48 - table here so the items table repres
134:51 - represents like magical items right
134:53 - while the characters table we're
134:55 - familiar with it represents actual
134:58 - characters so let's say that you are
135:00 - managing a video game and someone asks
135:03 - you for a single table that contains all
135:06 - of the entities in that video game right
135:09 - and the entities include both characters
135:12 - and items so you want to create a table
135:16 - which combines these two tables into
135:19 - one we know we can use Union to do that
135:22 - we know we can use Union to stack all
135:24 - the rows but we cannot directly Union
135:27 - these two tables be because they have a
135:30 - different schema right they have a
135:32 - different number of columns and then
135:33 - those columns have different data
135:36 - types but let's analyze what these two
135:39 - tables have in common and how we could
135:41 - maybe combine that so first of all they
135:46 - both have an ID and in both cases it's
135:49 - an integer so that's already pretty good
135:52 - they both have a name and in both cases
135:54 - the name is a string so we can combine
135:57 - that as as well the item type can be
136:01 - thought of being similar to the class
136:04 - and then each item has a level of power
136:07 - which is expressed as an
136:09 - integer and each character has a level
136:12 - of experience which is expressed as an
136:14 - integer and you can think that they are
136:16 - kind of similar and then finally we have
136:19 - a timestamp field representing a moment
136:22 - in time for both columns which are date
136:25 - added and last active so looking at this
136:29 - columns that the two have sort of in
136:31 - common we can find a way to combine them
136:35 - and here's how we can translate this
136:37 - into SQL right so I'm went to the
136:40 - fantasy items table and I selected The
136:44 - Columns that I wanted and then I went to
136:47 - the characters table and I selected the
136:50 - columns that I wanted to combine with
136:52 - those um in in the right order so we
136:55 - have ID with ID name with name class
136:58 - with item type level with power and last
137:01 - active with date added so I have my
137:03 - columns they're in the right order I
137:05 - wrote Union distinct and if I run this
137:09 - you will see that I have successfully
137:11 - combined the rows from these two tables
137:15 - by finding out which columns they have
137:17 - in
137:19 - common and then writing them in the
137:21 - right order and then adding Union
137:24 - distinct now all the columns that we've
137:26 - chosen for the combination have the same
137:28 - type but what would happen if I wanted
137:31 - to combine two columns that are not
137:33 - actually the same type so let's say what
137:36 - if we wanted to combine Rarity which is
137:39 - a string with experience which is an
137:42 - integer as you know I cannot do this
137:45 - directly but I can go around it by
137:48 - either taking Rarity and turning it into
137:51 - an integer or taking um experience and
137:56 - turning it into a string I just have to
137:59 - make sure that they both have the same
138:01 - data
138:02 - type now the easiest way is usually to
138:05 - take um any other data type and turn it
138:09 - into a string because we you just turn
138:11 - it into text so let's say that for the
138:13 - sake of this demonstration we will take
138:16 - integer experience which is an integer
138:18 - and turn it into a string which is text
138:21 - and then combine that with
138:23 - Rarity so I will go back to my
138:26 - code and I will make some room over here
138:31 - and here in items I will add
138:34 - Rarity and here in characters I will add
138:40 - experience and you can see that I
138:42 - already get an error here saying that
138:44 - the union distinct has incompatible
138:47 - types just like expected so what I want
138:50 - to do here is to take experience and
138:53 - turn it into string and I can do that
138:55 - with the cast function so I can do cast
138:59 - experience as
139:02 - string and what this will do is
139:05 - basically take these values and convert
139:06 - them to string and if I run this you can
139:09 - see that this has
139:11 - worked so we combined two tables into
139:15 - one and now the result is a single table
139:18 - it has a column called Rarity the reason
139:20 - it's called Rarity is that um it's it's
139:23 - taking the name from the first table in
139:26 - the in the operation but we could of
139:29 - course rename it to whatever we need and
139:31 - this is now a text column because we
139:33 - have combined a text column with also a
139:37 - text column thanks to the casting
139:39 - function so what we see here are a bunch
139:42 - of numbers which came originally from
139:44 - The Experience uh column from the
139:48 - character table but they're now
139:50 - converted to text and if I scroll down
139:52 - then I will also see the original values
139:54 - of Rarity from the items
139:58 - table finally let us examine Union in
140:02 - the context of The Logical order of SQL
140:06 - operations so you can see here that we
140:08 - have our logical map but it looks a bit
140:12 - different than
140:13 - usual and the reason it's different is
140:16 - that we are considering what happens
140:19 - when you un two tables and here the blue
140:24 - represents one table and the red
140:26 - represents the other
140:28 - table so I wanted to show you that all
140:31 - of the ordering that we have seen until
140:34 - now so first get the table then use the
140:37 - filter with where then select the
140:39 - columns you want and if you want use
140:41 - this thing to remove duplicates all of
140:44 - these happens in the same order
140:46 - separately for the two tables that you
140:49 - are unioning
140:50 - and this applies to all of the other
140:53 - operations like joining and grouping
140:56 - which we will see um later in the course
141:00 - so at first the two tables are working
141:03 - on two separate tracks and SQL is doing
141:06 - all this operations on them in this
141:09 - specific
141:10 - order and only at the end of all this
141:14 - only after all of these operations have
141:16 - run then we have the union and in the
141:20 - Union these two tables are combined into
141:24 - one and only after that only after the
141:29 - tables have been combined into one you
141:32 - apply the last two operations which are
141:36 - order by and
141:38 - limit and actually nothing forces you to
141:41 - combine only two tables you could
141:44 - actually have any number of tables that
141:46 - you are combining in Union but then the
141:49 - logic doesn't change at all all of these
141:51 - operations will happen separately for
141:55 - each of the tables and then only when
141:58 - all of these operations are done only
142:00 - when all of the tables are ready then
142:03 - they will be combined into
142:06 - one and if you think about it it makes a
142:09 - lot of sense because first of all you
142:12 - need the select to have run in order to
142:15 - know what is the schema of the tables
142:18 - that you are combining and then you also
142:20 - also need to know if distinct has run on
142:23 - each uh table because you need to know
142:25 - which rows you need to combine in the
142:30 - union and that is all you need to know
142:33 - to get started with Union this very
142:35 - powerful statement that allows us to
142:37 - combine rows from different
142:41 - tables let us now look at order by so
142:45 - I'm looking at the characters table here
142:48 - and as you can see we have an ID
142:51 - column that goes from one to 15 which
142:54 - assigns an ID to every character but you
142:56 - will see that the IDS don't appear in
142:58 - any particular order and in fact this is
143:02 - a general rule for SQL there is
143:05 - absolutely no order guarantee for your
143:09 - data your data is not stored in any
143:12 - specific order and your data is not
143:14 - going to be returned in any specific
143:17 - order and the reason for this is fun
143:20 - fundamentally one of efficiency because
143:24 - if we had to always make sure that our
143:27 - data was perfectly ordered that would
143:29 - add a lot of work it would add a lot of
143:31 - overhead to the engine that makes the
143:34 - queries work and uh there's really no
143:37 - reason to do
143:38 - this however we do often want to order
143:42 - our data when we are querying it we want
143:45 - to order the way that it is displayed
143:47 - and this is why the order by clause is
143:51 - here so let us see how it works I am
143:54 - selecting everything from fantasy
143:56 - characters and again I'm going to get
143:58 - the results in no particular order but
144:01 - let's say I wanted to see them in uh
144:03 - ordered by name so then I would do order
144:06 - by
144:08 - name and as you can see the rows are now
144:12 - ordered alphabetically according to the
144:16 - name I could also invert the order by
144:20 - writing desk which stands for descending
144:24 - and that means U descending alphabetical
144:27 - order which means from the last letter
144:29 - in the alphabet to the
144:32 - first I can of course also order by
144:35 - number columns such as
144:38 - level and we would see that the level is
144:42 - increasing
144:43 - here and of course that could also be
144:49 - descending to to go in the opposite
144:52 - direction and the corresponding keyword
144:55 - here is ask which stands for ascending
144:58 - and this is actually the default
144:59 - Behavior so even if you omit this you
145:02 - will get the same going from the
145:03 - smallest to the largest I can also order
145:07 - by multiple columns so I could say order
145:10 - by class and then
145:14 - level and what that looks like is that
145:18 - first of all the rows are ordered by
145:20 - class
145:20 - so as you can see this is done
145:22 - alphabetically so first Archer and then
145:25 - the last is Warrior and then within each
145:27 - class the values within the class are
145:30 - ordered according to the level going
145:32 - from the smallest level to the biggest
145:35 - level and I can invert the order of one
145:40 - of them for example class and in this
145:43 - case we will start with Warriors and
145:46 - then within the warrior class we will
145:49 - still will order the level in ascending
145:53 - order so I can for every column uh
145:56 - that's in the ordering I can decide
145:58 - whether that ordering is in ascending
146:00 - order or descending
146:02 - order now let us remove this and select
146:06 - the name and the class and once again I
146:10 - get my rows in no particular order and
146:12 - I'm seeing the name and the class so I
146:15 - wanted to show you that you can also
146:17 - order by columns which you have not
146:19 - selected Ed so I could order these
146:23 - elements by level even though I'm not
146:25 - looking at at
146:28 - level and it will work all the
146:32 - same and finally I can also order by
146:37 - operations so I could say take level
146:41 - divide it by
146:42 - experience and then multiply that by two
146:46 - for some reason and it would also work
146:49 - in the order ordering even though I am
146:51 - not seeing that calculation that
146:53 - calculation is being done in the
146:55 - background and used for the
146:58 - ordering so I could actually take this
147:01 - here and copy it create a new column
147:05 - call it calc for
147:06 - calculation and if I show you this you
147:09 - will see the results are not uh very
147:12 - meaningful but you will see that they
147:15 - are in ascending order so we have
147:18 - ordered by that
147:21 - and sometimes you will see this notation
147:23 - over here order by 21 for
147:26 - example and as you can see what we've
147:29 - done here is that we've ordered by class
147:32 - first of all because we starting with
147:34 - archers and going to Warriors and then
147:37 - within each class we are ordering by
147:40 - name uh also in ascending order so this
147:44 - is basically referring to the columns
147:46 - that are referenced in the select two
147:49 - means order by the second column which
147:51 - you have referenced which in this case
147:53 - is class and one means order by the
147:56 - First Column that you referenced so it's
147:58 - basically a shortcut that people
148:00 - sometimes use to avoid rewriting the
148:02 - names of columns that they have
148:04 - selected and finally when we go back to
148:07 - the order of operations we can see that
148:09 - order bu is happening really at the end
148:12 - of all of this process so as you will
148:15 - recall I have created this diagram
148:18 - that's a bit more complex to show show
148:20 - what happens when we Union different
148:22 - tables together what happens is that
148:25 - basically all these operations they run
148:27 - independently on each table and then
148:30 - finally the tables get uh unioned
148:34 - together and after all of this is done
148:36 - SQL knows the final list of rows that we
148:40 - will include in our results and that's
148:42 - the right moment to order those rows it
148:46 - would not be possible to do that before
148:48 - so it makes sense that order is located
148:52 - here let us now look at the limit Clause
148:55 - so what I have here is a simple
148:59 - query it goes to the characters table it
149:02 - filters for the rows where the character
149:04 - is alive and then it gets three columns
149:07 - out of this so let's run this
149:10 - query and you can see that this query
149:13 - returns 11 rows now let us say that I
149:16 - only wanted to see five of those rows
149:19 - and this is where limit comes into place
149:22 - limit will look at the final results and
149:25 - then pick five rows out of those results
149:28 - reducing the size of my output and here
149:31 - you can see that we get five
149:34 - rows now as we said in the lecture of
149:37 - ordering by default there is no
149:39 - guarantee of order in a SQL system so
149:42 - when you are getting all your data with
149:44 - a query and then you run limit five on
149:46 - top of it you have no way of kn knowing
149:50 - which of the rows will be selected to
149:53 - fit amongst those five you're basically
149:55 - saying that you're okay with getting any
149:58 - five of all of the rows from your
150:02 - result because of this people often will
150:06 - use limit in combination with order by
150:10 - for example I could say order by level
150:14 - and then limit
150:16 - five and what I would get here is
150:19 - essenti
150:20 - the first five most inexperienced
150:24 - characters in my data
150:27 - set and let us say that you have a
150:30 - problem of finding the least experienced
150:33 - character in your data the character
150:36 - with the lowest level so of course you
150:40 - could say order by level and then limit
150:43 - one and you would get the character with
150:46 - the lowest level
150:48 - right and this works
150:50 - however it is not ideal there is a
150:53 - problem with this solution so can you
150:55 - figure out what the problem with this
150:57 - solution
151:01 - is the problem will be obvious once I go
151:04 - back to limit 5 and I look here and I
151:08 - see that I actually have two characters
151:11 - which have the lowest level in my data
151:14 - set so in theory I should be able to
151:17 - return both of them because they both
151:19 - have the lowest level however when I
151:21 - write limit one it simply cuts the rows
151:25 - in my output and it is unaware of that
151:28 - uh further information that is here in
151:31 - this second
151:32 - row and in the further lectures we will
151:36 - see how we can solve this better and get
151:39 - results which are more
151:41 - precise and if we look at The Logical
151:44 - order of operations we can see that
151:47 - limit is the very last
151:51 - operation and so all of the logic of our
151:54 - query is
151:56 - executed all our data is computed and
152:00 - then based on that final result we
152:03 - sometimes decide to not output all of it
152:06 - but to Output a limited number of
152:09 - rows so a common mistake for someone who
152:12 - is starting with SQL is thinking that
152:15 - they can use limit in order to have a
152:18 - cheaper query
152:21 - for example you could say oh this is a
152:24 - really large table this table has two
152:27 - terabytes of data it would cost a lot to
152:30 - scan the whole table so I will say
152:34 - select star but then I will put limit 20
152:37 - because I only want to see the first 20
152:40 - rows and that will means that I will
152:42 - only scan 20 rows and my query will be
152:45 - very cheap
152:47 - right no that is actually wrong that
152:50 - doesn't save you anything and you can
152:53 - understand this if you look at the map
152:55 - because all of the logic is going to
152:58 - execute before you get to limit so
153:00 - you're going to scan the whole table
153:03 - when you say select star and you're
153:04 - going to apply all of the logic and the
153:07 - limit is actually just changing the way
153:09 - your result is displayed it's not
153:12 - actually changing the way the your
153:13 - result is
153:15 - computed if you did want to write your
153:18 - query so that it scans less rows one
153:20 - thing you should do is focus on the
153:22 - where statement actually because the
153:24 - where statement is the one that runs in
153:26 - the beginning right after getting the
153:28 - table and it is able to actually
153:30 - eliminate rows which usually saves you
153:34 - on computation and money and so
153:37 - on however I do need to say that there
153:40 - are systems where writing limit may
153:43 - actually turn into savings because
153:47 - different systems are optimized in
153:48 - different ways and um allow you to do
153:52 - different things with the commands but
153:54 - as a rule usually with SQL limit is just
153:58 - changing the way your result is
154:00 - displayed and doesn't actually change
154:03 - anything in the logic of
154:05 - execution let us now look at the case
154:08 - clause which allows us to apply
154:12 - conditional logic in
154:14 - SQL so you can see here a simple query I
154:18 - am getting the data from the characters
154:21 - table I am filtering it so that we only
154:24 - look at characters who are alive and
154:26 - then for each character we're getting
154:28 - the name and the level now when you have
154:32 - a column that contains numbers such as
154:35 - level one typical thing that you do in
154:38 - data analysis is
154:41 - bucketing and bucketing basically means
154:44 - that I look at all these multiple values
154:48 - that level can have and and I reduce
154:50 - them to a smaller number of values so
154:53 - that whoever looks at the data can make
154:56 - sense of it uh more
154:59 - easily now the simplest form of
155:01 - bucketing that you can have is the one
155:03 - that has only two buckets right so
155:07 - looking at level our two buckets for
155:09 - example could be uh in one bucket we put
155:12 - values that are equal or bigger than 20
155:16 - so characters who have a level that's at
155:18 - least 20 and in the other bucket we put
155:21 - all the characters that have a level
155:23 - that is less than 20 for
155:26 - example now how could I Define those two
155:30 - buckets so we know that we can Define
155:35 - new columns in the select statement and
155:37 - that we can use calculations and logical
155:40 - statements to define those
155:42 - columns so one thing that I could do
155:44 - would be to go here and then write level
155:49 - bigger than bigger or equal than
155:53 - 20 and then call this new column
155:58 - level at least 20 for
156:02 - example and when I run
156:06 - this I get my
156:08 - column now of course this is a logical
156:12 - statement and for each row this will be
156:15 - true or false and then you can see that
156:18 - our new column here gives us true or
156:21 - false on every
156:23 - column and this is a really basic form
156:26 - of bucketing because it allows us to
156:30 - take you know level has basically 11
156:34 - different values in our data and it can
156:37 - be complicated to look at this many
156:39 - values at once and now we've taken these
156:41 - 11 values and reduced them to two uh to
156:44 - two buckets so that we have um organized
156:48 - our data better and it's easier to
156:50 - read but there are two limitations with
156:53 - this approach one I might not want to
156:57 - call my buckets true or false I might
156:59 - want to give more informative names to
157:02 - my buckets such as experienced or
157:05 - inexperienced for
157:07 - example the other limitation is that
157:10 - with this approach I can effectively
157:13 - only divide my data in two buckets
157:16 - because once I write a logical statement
157:19 - it's either either true or false so my
157:21 - data gets divided in two but often it's
157:24 - the case that I want to use multiple
157:26 - buckets for my use case now bucketing is
157:30 - a typical use case for the case when
157:32 - statement so let's see it in action
157:35 - now so let me first write a comment not
157:39 - any actual code where I Define what I
157:42 - want to do and then I will do it with
157:45 - the
157:46 - code so I have written here the buckets
157:50 - that I want to use to classify the
157:52 - characters level so up to 15 they are
157:56 - considered low experience between 15 and
157:59 - 25 they are considered mid and anything
158:03 - above 25 we will classify as
158:07 - super now let us apply the case Clause
158:10 - to make this work so the case Clause Is
158:15 - Always bookended by these two parts case
158:19 - and end so it starts with case it ends
158:22 - with end and a typical error when you're
158:25 - getting started is to forget about the
158:27 - end part so my recommendation is to
158:30 - always start by writing both of these
158:32 - and then going in the middle to write
158:34 - the
158:35 - rest now in the middle we're going to
158:38 - Define all the conditions that we're
158:39 - interested in and each condition starts
158:42 - with the keyword
158:44 - when and is Then followed by a logical
158:47 - condition so our logical condition here
158:50 - is
158:52 - level smaller than
158:56 - 15 now we have to Define what to do when
158:59 - this condition is true and it follows
159:02 - with the keyword then and when this
159:05 - condition is true we want to return the
159:08 - value low which is a string a piece of
159:11 - text that says
159:14 - low next we proceed with the following
159:17 - condition so when
159:20 - level is bigger and equal to
159:24 - 15 and level is lower than
159:30 - 25 so if you have trouble understanding
159:34 - this logical statement I suggest you go
159:36 - back to the lecture about Boolean
159:38 - algebra but what we have here there are
159:41 - two micro statements right Level under
159:45 - 25 and level equal or bigger than 15
159:48 - they are conect connected by end which
159:50 - means that both of these statements have
159:52 - to be true in order for the whole
159:55 - statement to be true which is what we
159:58 - want in this case right and what do we
160:01 - want to return in this case we will
160:04 - return the value
160:07 - mid and the last condition that we want
160:09 - to apply when level is bigger or equal
160:14 - than 25 then we will return super
160:19 - now all of this that you see here this
160:22 - is the case Clause right or the case
160:26 - statement and all of this is basically
160:28 - defining a new column in my table and
160:33 - given that it's a new column I can use
160:35 - the alas sync to also give it a name and
160:40 - I can call this level
160:43 - bucket now let's run this and see what
160:46 - we get
160:51 - and as you can see we have our level
160:53 - bucket and the characters that are above
160:56 - 25 are super and then we have a few Ms
161:01 - and then everyone who's under 15 is
161:06 - low so we got the results we wanted and
161:09 - now let us see exactly how the case
161:12 - statement
161:13 - works so I'm going to take Gandalf over
161:16 - here and he has level 30 so I'm going to
161:20 - write over here level equals 30 because
161:24 - we're looking at the first low row and
161:26 - that is the value of level and then I'm
161:29 - going to take the conditions for the
161:31 - case statement that we are examining and
161:35 - add them here as a
161:38 - comment now because in our first row
161:41 - level equals 30 I'm going to take the
161:44 - value and substitute it here for level
161:51 - now what we have here is a sequence of
161:54 - logical statements and we have seen how
161:56 - to work with these logical statements in
161:59 - the lecture on Boolean
162:01 - algebra now our job is to go through
162:04 - each of these logical statements in turn
162:07 - and evaluate them and then as soon as we
162:10 - find one that's true we will
162:14 - stop so the first one is 30 smaller than
162:18 - 50
162:20 - now this is false so we
162:24 - continue the second one is a more
162:27 - complex statement we have 30 greater or
162:30 - equal to 15 which is actually
162:33 - true
162:35 - and 30 Oops I did not substitute it
162:39 - there but I will do it now and 30
162:42 - smaller than 25 which is false and we
162:46 - know from our Boolean algebra that true
162:48 - and false evaluates to false therefore
162:52 - the second statement is also false so we
162:56 - continue and now we have 30 greater or
163:01 - equal than 25 which is true so we
163:05 - finally found a line which evaluates as
163:08 - true and that means that we return the
163:11 - value super and as you can see for
163:13 - Gandalf we have indeed gotten the value
163:18 - super let us look very quickly at one
163:21 - more example we get Legolas which is
163:24 - level
163:25 - 22 and so I will once again copy this
163:29 - whole
163:31 - thing and comment
163:37 - it and I will substitute 22 for every
163:41 - value of level cuz that's the row we're
163:43 - looking
163:44 - at and then looking at the first row 22
163:48 - small than 15 is
163:50 - false so we proceed and then looking at
163:53 - the second row 22 bigger than 15 is true
163:59 - and 22 smaller than 25 is also true so
164:02 - we get true and true which evaluates to
164:05 - true and so we return mid and then
164:08 - looking at Legolas we get
164:11 - mid so this is how the case when
164:13 - statement Works in short for each row
164:16 - you insert the values that correspond to
164:18 - your Row in this case the value of level
164:22 - and then you evaluate each of these
164:24 - logical conditions in turn and as soon
164:27 - as one of them returns true then you
164:31 - return the value that corresponds to
164:33 - that condition and then you move on to
164:37 - the next
164:38 - row now I will clean this up a bit and
164:43 - now looking at this statement now and
164:46 - knowing what we know about the way way
164:49 - it works can we think of a way to
164:52 - optimize it to make it nicer to remove
164:56 - redundancies think about it for a
165:02 - minute now one thing we could do to
165:05 - improve it is to remove this little bit
165:07 - over here because if you think about it
165:10 - this part that I have highlighted is
165:13 - making sure that the character is not
165:17 - under 15
165:19 - so that it can be classified as meat but
165:21 - actually we already have the first
165:24 - condition that makes sure that if the
165:27 - character is under 15 then the statement
165:30 - will output low and then move on so if
165:33 - the character is under 15 we will never
165:36 - end up in the second statement but if we
165:39 - do end up in the second statement we
165:41 - already know that the character is not
165:43 - under 15 this is due to the fact that
165:46 - case when proceeds condition by
165:50 - condition and exits as soon as the
165:52 - condition is
165:53 - true so effectively I can remove this
165:56 - part over here and then at the second
166:00 - condition only make sure that the level
166:02 - is below 25 and you will see if you run
166:05 - this that our bucketing system works
166:08 - just the
166:09 - same and the other Improvement that I
166:12 - can add is to replace this last line
166:17 - with an else CL Clause so the else
166:21 - Clause takes care of all the cases that
166:24 - did not meet any of the conditions that
166:26 - we specified so the case statement will
166:31 - go condition by condition and look for a
166:34 - condition that's true but in the end if
166:36 - none of the conditions were true it will
166:39 - return what the else Clause says so it's
166:44 - like a fallback for the cases when none
166:47 - of our conditions turned out to be
166:51 - true and if you look at our logic you
166:54 - will see that if this has returned false
166:58 - and this has returned false all that's
167:01 - left is characters that have a level
167:05 - which is either 25 or bigger than 25 so
167:09 - it is sufficient to use an else and to
167:12 - call those super and if I run
167:17 - this you will see that our bucketing
167:20 - works just the same for example Gandalf
167:23 - is still marked as super because in the
167:26 - case of Gandalf this condition has
167:29 - returned false and this condition has
167:31 - returned false and so the else output
167:35 - has been written
167:37 - there now what do you think would happen
167:40 - if I completely removed the else what do
167:43 - you think would happen if I only had two
167:46 - conditions but it can be the the case
167:49 - that none of them is true what will SQL
167:51 - do in that case let us try it and see
167:55 - what
167:58 - happens so the typical response in SQL
168:02 - when it doesn't know what to do is to
168:04 - select the null value right and if you
168:07 - think about it it makes sense because we
168:10 - have specified what happens when level
168:12 - is below 15 and when level is is below
168:15 - 25 but none of these are true and we
168:19 - haven't specified what we want to do
168:21 - when none of these are true and because
168:23 - we have been silent on this issue SQL
168:26 - has no choice but to put a null value in
168:30 - there so this is practically equivalent
168:34 - to saying else null this is the default
168:37 - behavior for SQL when you don't specify
168:40 - an else
168:42 - Clause now like every other piece of SQL
168:46 - the case statement is quite flexible for
168:49 - instance you are not forced to always
168:52 - create a text column out of it you can
168:54 - also create an integer column so you
168:57 - could Define a simpler leveling system
169:00 - for your characters by using one and two
169:03 - else three for the higher level
169:06 - characters and uh this of course will
169:09 - also
169:12 - work as you can see here however one
169:17 - thing that you cannot do is to mix types
169:21 - right because what this does is that it
169:24 - results in one column in a new column
169:27 - and as you know in SQL you're not
169:29 - allowed to mix types between
169:32 - columns so always keep it consistent
169:36 - when it comes to
169:38 - typing and then when it comes to writing
169:42 - the when condition all the computational
169:45 - power of SQL is available so you can
169:48 - reference columns that you are not
169:51 - selecting you can run calculations as I
169:54 - am doing here and you can change logical
169:57 - statements right Boolean statements in
170:01 - complex ways you can really do anything
170:03 - you
170:04 - want although I generally suggest to
170:07 - keep it as simple as possible for your
170:10 - sake and the sake of the people who use
170:12 - your
170:13 - code and that is really all you need to
170:15 - know to get started with the case
170:17 - statement
170:19 - to summarize the case statement allows
170:22 - us to define a new columns whose values
170:25 - are changing conditional on the other
170:29 - values of my
170:31 - row this is also called conditional
170:33 - logic which means that we consider
170:36 - several conditions and then we do have
170:39 - different behaviors based on which
170:41 - condition is
170:42 - true and the way it works is that in the
170:45 - select statement when you are mentioning
170:48 - all your columns you create a new column
170:51 - which in our case is this one and you
170:54 - bookend it with a case and
170:57 - end and then between those you write
171:00 - your actual conditions so every
171:03 - condition starts with a when is followed
171:06 - by a logical statement which needs to
171:08 - evaluate to true or false and then has
171:11 - the keyword then and then a value and
171:15 - then the case when statement will go
171:18 - through each of these conditions in turn
171:21 - and as soon as one of them evaluates to
171:23 - true you will output the value that you
171:25 - have
171:26 - specified if none of the conditions
171:28 - evaluate to true then it will output the
171:31 - value that you specify in the else
171:33 - keyword and if the lse keyword is
171:36 - missing it will output
171:38 - null and so this is what you need to use
171:41 - the case statement and then experience
171:44 - and exercise and coding challenges will
171:46 - teach you when it's the case to use it
171:49 - pun
171:50 - intended now where does the case
171:52 - statement fit in our logical order of
171:55 - SQL operations and the short answer is
171:57 - that it is defined here at the step when
172:01 - you are selecting your columns that's
172:04 - when you can use the case when statement
172:05 - to create a new column that applies your
172:10 - conditional logic and this is the same
172:13 - as what we've shown in the lecture on
172:16 - SQL calculations you you can use select
172:20 - statement not only to get columns which
172:22 - already exist but to Define new columns
172:25 - based on calculations and
172:29 - logic now let us talk about aggregations
172:32 - which are really a staple of any sort of
172:35 - data analysis and an aggregation is a
172:38 - function that takes any number of values
172:41 - and compresses them down to a single
172:44 - informative
172:46 - value so I'm looking here at at my usual
172:49 - characters table but this is the version
172:51 - that I have in Google Sheets and as you
172:54 - know we have this level column which
172:56 - contains the level of each character and
173:00 - if I select this column in Google Sheets
173:02 - you will see that in the bottom right
173:05 - corner I can see here a number of
173:09 - aggregations on this column and like I
173:11 - said no matter how many values there are
173:14 - in the level columns I can use
173:16 - aggregations to compress them to one
173:18 - value and here you see some of the most
173:21 - important aggregations that you will
173:23 - work with some simply adding up all
173:27 - values together the average which is
173:31 - doing the sum and then dividing by the
173:33 - number of values the minimum value the
173:36 - maximum the count and the count numbers
173:40 - which is the same
173:41 - here so these are basically summaries of
173:45 - my column and you can imagine in cases
173:48 - where where you have thousands or
173:49 - millions of values how useful these
173:52 - aggregations can be for you to
173:54 - understand your
173:55 - data now here's how I can get the exact
173:58 - same result in SQL I simply need to use
174:02 - the functions that SQL provides for this
174:05 - purpose so as you can see here I'm
174:07 - asking for the sum average minimum
174:10 - maximum and count of the column
174:13 - level and you can see the same results
174:16 - down here now now of course I could also
174:20 - give names to this column for example I
174:22 - could take this one and call it max
174:24 - level and in the result I will get a
174:27 - more informative column name and I can
174:29 - do the same for all
174:31 - columns now of course I can run
174:34 - aggregations on any columns that I want
174:37 - for example I could also get the maximum
174:41 - of experience and call this Max
174:44 - experience
174:49 - and I can also run aggregations on
174:53 - calculations that involve multiple
174:56 - columns as well as constants so
174:59 - everything we've seen about applying
175:01 - arithmetic and logic in SQL also
175:06 - applies now of course looking at the
175:09 - characters table we know that our
175:12 - columns have different data types and
175:15 - the behavior of the aggregate functions
175:17 - also is sensitive to the data types of
175:20 - the columns for example let us look at
175:23 - the many text columns that we have such
175:26 - as
175:27 - class now clearly not all of the
175:30 - aggregate functions that we've seen will
175:32 - work on class because how would you take
175:35 - the average of these values it's not
175:37 - possible right however there are some
175:40 - aggregate functions that also work on
175:44 - strings so here's an example of
175:47 - aggregate functions that we can run on a
175:49 - string column such as class first we
175:52 - have count which simply counts the total
175:55 - number of non null values and I will
175:58 - give you a bit more detail about the
176:00 - count functions
176:02 - soon then we have minimum and maximum
176:06 - now the way that strings are ordered in
176:08 - SQL is something called lexicographic
176:12 - order which is basically a fancy word
176:15 - for alphabetical order and basically you
176:18 - can see here that for minimum we get the
176:22 - text value that occurs earlier in uh
176:26 - alphabetical order whereas Warrior
176:28 - occurs
176:30 - last and finally here's an interesting
176:33 - one called string EG and what this does
176:37 - is that this is a function that actually
176:40 - takes two arguments the first argument
176:42 - as usual is the name of the column and
176:45 - the second argument is a separator
176:48 - and what this outputs is now a single
176:51 - string a single piece of text where all
176:54 - of the other pieces of text have been
176:56 - glued together and then separated by
177:00 - this character that we specified over
177:02 - here which in our case is a
177:05 - comma Now if you go to the Google
177:08 - documentation you will find an extensive
177:11 - list of all the aggregate functions that
177:13 - you can use in Google
177:16 - SQL and this includes the ones that
177:18 - we've just seen such as average or Max
177:22 - as well as a few others that we will not
177:24 - explore in detail
177:26 - here so let us select one of them such
177:30 - as average and see what the description
177:33 - looks
177:34 - like now you can see that this function
177:37 - Returns the average of all values that
177:40 - are not null and don't worry about this
177:44 - expression in an aggregated group for
177:46 - now just think about
177:48 - this as meaning all the values that you
177:51 - provide to the function all the values
177:53 - in the
177:55 - column now there is a bit about window
177:58 - functions which we will see later and
178:01 - here there are in the caveat section
178:03 - there are some interesting edge cases
178:05 - for example what happens if you use
178:08 - average on an empty group or if all
178:11 - values are null in that case it returns
178:13 - null and so on you could see what the
178:17 - function does when it finds these edge
178:20 - cases and here is perhaps the most
178:23 - important section which is supported
178:25 - argument types and this tells you what
178:29 - type of columns you can use this
178:31 - aggregation function
178:33 - on so you can see that you can use
178:36 - average on any numeric input type right
178:40 - any column that contains some kind of
178:42 - number and also on interval and interval
178:46 - we haven't examined it in detail but
178:48 - this is actually a data type that
178:50 - specifies a certain span of time so
178:55 - interval could express something like 2
178:58 - hours or 4 days or 3 months it is a
179:02 - quantity of
179:04 - time and finally in this table returned
179:08 - data types you can see what the average
179:11 - function will give you based on the data
179:14 - type that you insert so if you insert uh
179:18 - integer column it will return to you a
179:20 - float column and that makes sense
179:23 - because the average function involves a
179:26 - division and that division will usually
179:29 - give you floating Point
179:31 - values but for any other of the allowed
179:35 - input types such as numeric bit numeric
179:38 - and so on and these are all data types
179:41 - which represent numbers in B query the
179:44 - average function as you can see here
179:47 - will present Reserve that data
179:50 - type and finally we have some examples
179:53 - so whenever you need to use an aggregate
179:57 - function that is whenever you need to
179:59 - take many values a sequence of multiple
180:03 - values and compress them all down to one
180:06 - value but you're not sure about which
180:08 - function to use or what the behavior of
180:10 - the function is you can come to this
180:12 - page and look up the functions that
180:15 - interest you and then read the
180:17 - documentation to see how they
180:19 - work now here's an error that typically
180:23 - occurs when starting out with
180:25 - aggregations so you might say well I
180:29 - want to get the name of each character
180:31 - and their level but I also want to see
180:34 - the average of all levels and because I
180:37 - want to compare those two values I want
180:39 - to compare the level of my character
180:42 - with the average on all levels so I can
180:45 - write a query that looks like this right
180:47 - go to the Fant as a characters table and
180:49 - then select name level and then average
180:52 - level but as you can already see this
180:55 - query is not functioning it's giving me
180:59 - an error and the error says that the
181:01 - select list expression references column
181:04 - name which is neither grouped nor
181:07 - aggregated so what does this actually
181:11 - mean to show you what this means I've
181:14 - gone back to my Google Sheets where I
181:16 - have the same data for my characters
181:18 - table and I have copy pasted our query
181:21 - over here now what this query does it
181:25 - takes the name column so I will copy
181:29 - paste it over here and then it takes the
181:32 - level
181:34 - column copy paste this here as well and
181:37 - then it computes the average over level
181:42 - now I can easily compute this with sheet
181:45 - formula by writing equal
181:48 - and then calling the function which is
181:50 - actually called average and then within
181:53 - the function I can select all these
181:55 - values over
181:57 - here and I get the
182:01 - average now this is the result that SQL
182:05 - computes but SQL is actually not able to
182:09 - return this result and the reason is
182:13 - that there are three columns but they
182:16 - have mismatch number of values
182:20 - specifically these two columns have 15
182:23 - values each whereas this column has a
182:26 - single value and SQL is not able to
182:30 - handle this
182:32 - mismatch because as a rule every SQL
182:36 - query needs to return a table and a
182:40 - table is a series of columns where each
182:44 - column has the same number of values if
182:48 - that constraint is not respected you
182:51 - will get an error in SQL and we will
182:54 - come back to this limitation when we
182:56 - examine Advanced aggregation
183:00 - techniques but for now just remember
183:03 - that you can mix non-aggregated columns
183:07 - with other non-aggregated columns such
183:09 - as name and level and you can mix
183:12 - aggregated columns with aggregated
183:14 - columns such as average level with some
183:18 - level for example so I could simply do
183:22 - this and I would be able to return this
183:26 - as a table because as you can see there
183:28 - are two columns both columns have a
183:31 - single Row the number of rows matches
183:34 - and this is actually
183:36 - valid but you might ask can't I simply
183:40 - take this value over here and just copy
183:44 - it in every
183:46 - row and until I make sure that average
183:50 - level has the same number of values as
183:53 - name and level and so return a table and
183:56 - respect that
183:58 - constraint indeed this is possible you
184:02 - can totally do this and then it would
184:05 - work and then this whole table would
184:08 - become a single table and you would be
184:11 - able to return this
184:14 - result however this requires the use of
184:17 - window functions which is a a feature
184:20 - that we will see in later lectures but
184:23 - yes it is totally possible and it does
184:26 - solve the
184:27 - problem now here's a special aggregation
184:30 - expression that you should know about
184:33 - because it is often used which is the
184:35 - count star and count star is simply
184:38 - counting the total number of rows in a
184:41 - table and as you can see if I say from
184:45 - fantasy characters select count star I
184:48 - get the total count of rows in my
184:53 - results and this is a common expression
184:56 - used across all SQL systems to figure
184:59 - out how many rows a table has and of
185:02 - course you can also combine it with
185:04 - filters with the wear clause in order to
185:08 - get other types of measures for example
185:10 - I could say where is alive equals true
185:15 - and then the count would become actually
185:17 - the count of characters who are alive in
185:20 - my
185:21 - data so this is a universal way to count
185:25 - rows in SQL although you should know
185:27 - that if you're simply interested in the
185:30 - total rows of a table and you are
185:33 - working with bigquery an easy and
185:36 - totally free way to do it is to go to
185:39 - the details Tab and look at the number
185:42 - of rows
185:43 - here so this was all I wanted to tell
185:46 - you about simp Le aggregations for now
185:49 - and last question is why do we call them
185:53 - simple simple as opposed to
185:56 - what I call them simple because the way
185:59 - we've seen them until now the
186:01 - aggregations take all of the values of a
186:04 - column and simply return One summary
186:07 - value for example the sum agregation
186:10 - will take all of the values of the level
186:12 - column and then return a single number
186:15 - which is the sum of all levels
186:18 - and more advanced aggregations involved
186:22 - grouping our data for example a question
186:26 - we might ask is what is the average
186:28 - level for Mages as opposed to the
186:32 - average level for Archers and for
186:34 - Hobbits and for warriors and so on so
186:37 - now you're Computing aggregations not
186:40 - over your whole data but over groups
186:43 - that you find in your data and we will
186:45 - see how to do that in the lecture on
186:48 - groupi but for now you can already find
186:51 - out a lot of interesting stuff about
186:54 - your data by running simple
186:58 - aggregations let us now look at
187:01 - subqueries and Common Table expressions
187:04 - and these are two fundamental
187:06 - functionalities in
187:08 - SQL these functionalities solve a very
187:12 - specific problem and the problem is the
187:16 - following sometimes you just cannot get
187:19 - the result you require with a single
187:22 - query sometimes you have to combine
187:26 - multiple SQL queries to get where you
187:28 - need to
187:29 - go so here's a fun problem that will
187:33 - illustrate my point so we're looking at
187:36 - the characters table and we have this
187:39 - requirement we want to find all those
187:42 - characters whose experience is between
187:45 - the minimum and the maximum maximum
187:47 - value of our
187:49 - experience another way to say this we
187:52 - want characters who are more experienced
187:55 - than the least experienced character but
187:58 - less experienced than the most
188:00 - experienced
188:01 - character in other words we want to find
188:05 - that middle ground that is between the
188:08 - least and the most experienced
188:11 - characters so let us see how we could do
188:13 - that uh I have here A Simple Start where
188:17 - I am getting the name and experience
188:19 - column from the characters
188:21 - table now let us focus on the first half
188:24 - of the problem find characters who have
188:27 - more experience than the least
188:29 - experienced
188:30 - character now because this is a toy data
188:33 - set I can sort of eyeball it so I can
188:36 - scroll down here and I can see that the
188:39 - lowest value of experience is pipin with
188:43 - 2100 and so what I need to do now is to
188:46 - filter out from this table all the rows
188:50 - that have this level of
188:53 - experience but apart from eyeballing how
188:56 - would we find the lowest level of
188:58 - experience in our
189:00 - data if you thought of aggregate
189:02 - functions you are right so we have seen
189:06 - a in a previous lecture that we have
189:09 - aggregated functions that take any
189:11 - number of values and speed out a single
189:14 - value that's a summary for example meing
189:17 - minum and maximum and indeed we need to
189:20 - use a function like that for this
189:23 - problem so your first instinct might be
189:26 - let us take this table and let us filter
189:29 - out rows in this way so let's say where
189:34 - experience is
189:37 - bigger than the minimum of experience
189:41 - and on the surface this makes sense
189:44 - right I am using an aggregation to get
189:48 - the smallest value of experience and
189:50 - then I'm only keeping rows that have a
189:52 - higher value than that however as you
189:55 - see from this red line this actually
189:58 - does not work because it tells us
190:01 - aggregate function is not allowed in the
190:04 - work Clause so what is going on
190:07 - here so if you followed the lecture on
190:11 - aggregation you might have a clue as to
190:14 - why this doesn't work but it is good to
190:16 - go back to to it and understand exactly
190:19 - what the problem is so I'm going back to
190:22 - my Google sheet over here where I have
190:24 - the exact same data and I copied our
190:27 - current query down here and now let's
190:30 - see what happens when SQL tries to run
190:32 - this so SQL goes to the fantasy
190:35 - characters table and the Second Step In
190:38 - The Logical order as you remember is to
190:40 - filter it and for the filter it has to
190:43 - take the column of experience so let me
190:46 - take this column and copy it down
190:50 - here and then it has to compute minimum
190:53 - of experience right so I will Define
190:56 - this column
190:58 - here and I will use Google Sheets
191:01 - function to achieve that result so
191:05 - equals mean and then selecting the
191:09 - numbers and here I get the minimum value
191:12 - of experience and now SQL has to compare
191:16 - these column
191:17 - but this comparison doesn't work right
191:20 - because these are two columns that have
191:23 - a different number of rows they have a
191:25 - different number of values so SQL is not
191:30 - able to do this
191:32 - comparison you cannot do an element by
191:35 - element comparison between a column that
191:37 - has 15 values and a column that has a
191:40 - single value so SQL throws an
191:44 - error but you might say wait there is a
191:47 - simple solution to this just take this
191:50 - value and copy it all over here until
191:53 - you have two columns of the same size
191:56 - and then you can do the comparison
191:58 - indeed that would work that's a solution
192:01 - but SQL doesn't do it
192:03 - automatically whereas if you work with
192:06 - other analytics tools such as pandas in
192:09 - python or npy you will find that um in a
192:13 - situation like this this would be done
192:16 - automatically this would be copied all
192:18 - over here and there's a process called
192:21 - broadcasting for that but SQL does not
192:25 - take so many assumptions and so many
192:28 - risks with your data if it literally
192:31 - doesn't work then SQL will not do
192:34 - it so hopefully now you have a better
192:37 - understanding of why this solution does
192:40 - not work so how could we actually
192:43 - approach this
192:45 - problem now a Insight is that I can run
192:50 - a different
192:51 - query so I will open this on the right
192:54 - to find out the minimum experience right
192:58 - I can go back to the characters table
193:02 - and I can select the minimum of
193:04 - experience this is simply what we've
193:07 - learned to do in the lecture on
193:09 - aggregations and I get the value here
193:12 - that is the minimum value of
193:15 - experience now that I know the minimum
193:18 - value of experience I could simply copy
193:20 - this value and insert it here into a
193:25 - wear
193:29 - filter and if I run this this will
193:32 - actually work it will solve my
193:35 - problem the issue of course is that I do
193:38 - not want to hard code this value first
193:41 - of all it is not very practical to run a
193:44 - separate query and copy paste the value
193:46 - in the code and second the minimum value
193:49 - might change someday and then I might
193:52 - not remember to update it in my code and
193:55 - then this whole query would become
193:56 - invalid to solve this problem I will use
193:59 - a subquery and I will simply delete the
194:03 - hardcoded value and I will open round
194:07 - brackets which is a way to get started
194:10 - on a subquery and I will take the query
194:13 - that I have over here and put them put
194:16 - it in the round brackets and when I run
194:20 - this I get the result that I
194:26 - need so what exactly is going on
194:33 - here we are using a subquery or in other
194:37 - words a query within a query so when SQL
194:40 - looks at this code it says all right so
194:43 - this is the outer query right and it has
194:47 - a inner query inside it a nested query
194:51 - so I have to start with the innermost
194:53 - query I have to start with the nested
194:55 - query so let me compute this and so SQL
194:59 - runs this query first and then it gets a
195:01 - value out of it which in our case we
195:04 - know that is
195:06 - 2100 and after that SQL substitutes this
195:10 - code over here by the value that was
195:13 - computed and we know from before that
195:16 - this works as
195:19 - expected and to compute the other half
195:22 - of our problem we want our character to
195:24 - have less experience than the most
195:27 - experienced character so this is just
195:29 - another condition in the wear filter and
195:32 - so I can add an end
195:35 - here and copy this code over here except
195:39 - that now I want my experience to be
195:42 - smaller than the maximum of EXP
195:47 - experience in my table now you might
195:50 - know this trick that if you select only
195:52 - part of your code like this and then you
195:55 - click run SQL will only execute that
195:58 - part of the code and so here we get the
196:01 - actual maximum for our experience and we
196:04 - can write it here in the comment and now
196:07 - we know that when SQL runs this query
196:11 - all of these will be computed to
196:14 - 15,000 and then experience will will be
196:17 - compared on that and the query will work
196:20 - as intended and here is the solution to
196:24 - our
196:28 - problem now here's the second problem
196:30 - which shows another side of subqueries
196:34 - we want to find the difference between a
196:36 - character's experience and their
196:39 - mentors so let us solve it manually for
196:42 - one case in the characters table so
196:47 - let us look at this character over
196:49 - here which is Saran with
196:53 - id1 and their experience
196:57 - is
197:00 - 8500 and then Saruman has character id6
197:05 - as their Mentor so if I look for id6 we
197:09 - have
197:10 - Gandalf this is not very Canon compared
197:13 - to the story but let's just roll with it
197:16 - and Gandalf has 10,000 of
197:21 - experience and now if we
197:24 - select the experience of Gandalf minus
197:27 - the experience of Saran we can see that
197:30 - there is
197:32 - A500 difference between their experience
197:36 - and this is what I want to find with my
197:38 - query now back to my query I will first
197:41 - Alias my columns in order to make them
197:44 - more informative and this is a great
197:46 - trick trick to make problems clearer in
197:48 - your head assign the right names to
197:51 - things so here instead of ID I will call
197:55 - this
197:56 - mentee
197:57 - ID and here I have Mentor ID and here
198:02 - instead of experience I will call this
198:04 - Mente
198:07 - experience so I have just renamed my
198:11 - columns now the missing piece of the
198:14 - puzzle is the mentor experience right so
198:19 - how can I get the mentor experience for
198:22 - example in the first case I know that
198:25 - character 11 is mentored by character 6
198:28 - how can I get the experience of
198:29 - character
198:31 - six now of course I can take a new tab
198:34 - over here split it to the
198:37 - right go to Fantasy
198:40 - characters filter for ID being equal to
198:44 - six which is the ID of our
198:47 - mentor and get their
198:51 - experience and the experience in this
198:54 - case is 10,000 this is the same example
198:56 - that we saw
198:58 - before but now I would have to write
199:01 - this separate query for each of my rows
199:04 - so here six I've already checked but
199:06 - then I will need to check two and seven
199:08 - and one and this is really not feasible
199:11 - right and the solution of course is to
199:14 - solve it with a subquery
199:17 - so what I'm going to do here is open
199:21 - round brackets and in here I will write
199:24 - the code that I need and here I can
199:27 - simply copy the code that I've written
199:30 - here get experience from the characters
199:35 - where ID equals six now the six part is
199:40 - still hardcoded because in the first row
199:43 - Mentor ID is six
199:47 - to avoid hardcoding this part there are
199:51 - two components to this the first one is
199:55 - noticing that I am referencing the same
199:58 - table fantasy. characters in two
200:01 - different places in my code and this
200:03 - could get buggy and this could get
200:08 - confusing and the solution is to give
200:11 - separate names to these two instances
200:14 - now what are the right names to give so
200:17 - if we look at this outer query right
200:20 - here this is really information about
200:23 - the M te right because we have the Mente
200:27 - ID the ID of their mentor and the Mente
200:30 - experience so I can simply call this
200:33 - Mente
200:36 - table and as you can see I can Alias my
200:40 - table by simply writing it like this or
200:43 - I could also add the as keyword it would
200:46 - work works just the same on the other
200:49 - hand this table will give us the
200:51 - experience of the mentor this is really
200:55 - information about the mentor so we can
200:58 - call this Mentor
201:00 - table now we're not going to get
201:03 - confused anymore because these two
201:05 - instances have different
201:07 - names and now what do we want this ID to
201:11 - be if we're not going to hardcode it we
201:14 - want it to be this value over here we
201:18 - want it to be the mentor ID value from
201:22 - the Mente table we want it to be the M's
201:26 - mentor and to refer to that column I
201:29 - will get the table name dot the column
201:34 - name so this is telling me get the
201:37 - mentor ID value from mentee table and
201:42 - now that I have the
201:44 - subquery which defines a colum with
201:47 - these two brackets I can Alias the
201:50 - result just like I always do and run
201:53 - this and now you will
201:56 - see after making some room here that we
202:00 - have successfully retrieved The
202:02 - Experience value for the
202:07 - mentor now I realize that this is not
202:12 - the
202:12 - simplest process so let us go back to
202:17 - our query over here and make sure that
202:19 - we understand exactly what is
202:21 - happening now first of all we are going
202:24 - to the characters table which contains
202:27 - information about our mentee the person
202:30 - who is being mentored and we label the
202:34 - table so that we remember what it's
202:37 - about we filter it because we're not
202:40 - interested in characters that do not
202:41 - have a mentor and then we're getting a
202:44 - few data right the ID in this case
202:46 - represents the IDE of the mentee and we
202:49 - also have their Mentor ID and we also
202:52 - have the experience which again this is
202:55 - the table about the Mente represents the
202:57 - mentee experience now our goal is to
203:00 - also get the experience of their Mentor
203:03 - our goal is to see that we have a mentor
203:06 - id6 and we want to know that their
203:08 - experience is 10,000 and we do that with
203:11 - a subquery it's a query within a query
203:16 - and in this subquery which is an
203:18 - independent piece of SQL code we are
203:21 - going back to the characters table but
203:24 - this is another instance of the table
203:26 - right that we're looking at so to make
203:28 - sure we remember that we call this
203:30 - Mentor table because it contains
203:33 - information about the mentor and how do
203:35 - we make sure that we get the right value
203:37 - over here that we don't get confused
203:39 - between separate mentors we make sure
203:43 - that for each row the ID
203:47 - of the character in this table is equal
203:50 - to the mentor ID value in the menty
203:54 - table in other words we make sure that
203:56 - we plug in this value over here in this
204:00 - case six into the table to get the right
204:04 - row and then from that row we get the
204:07 - experience value all of these code over
204:11 - here defines a new column which we call
204:14 - Mentor experience
204:19 - and this is basically the same thing
204:21 - that we did manually when we opened a
204:24 - table on the right and queried the table
204:26 - and copy pasted a hardcoded value this
204:29 - is just the way to do it dynamically
204:31 - with a
204:33 - subquery now we are not fully done with
204:35 - the problem right because we wanted to
204:37 - see the difference between the
204:39 - characters experience and their mentors
204:42 - so let's see how to do this and the way
204:45 - to do it is with a column calculation
204:48 - just like the ones we've seen before so
204:51 - given that this column represents the
204:53 - mentor experience I can remove the Alias
204:57 - over here and over here as well and I
205:01 - can subtract the experience from this
205:05 - and a column minus a column gives me
205:08 - another column which I can then
205:11 - Alias as experience
205:14 - difference and if I I run
205:17 - this I will see the value that we
205:20 - originally computed manually which is
205:22 - the difference between the mentor and
205:25 - the Mente
205:29 - experience there's nothing really new
205:31 - about this as long as you realize that
205:34 - this expression over here defines a
205:36 - column and this is the reference to a
205:39 - column and so you can subtract them and
205:42 - then give a name an alias to the result
205:47 - and now we can look at our two examples
205:51 - of nested queries side by side and we
205:55 - can figure out what they have in common
205:58 - and where do they differ so what they
206:01 - have in common is that they're both
206:03 - problem that you cannot resolve with a
206:05 - simple query because you need to use
206:08 - values that you have to compute
206:11 - separately values that you cannot simply
206:14 - refer to by name like we usually do with
206:16 - our
206:18 - columns in this case on the left you
206:21 - need to know what are the minimum and
206:23 - maximum values for experience and in
206:26 - this case on the right you need to know
206:28 - what is the experience of a character's
206:31 - mentor and so we solve that problem by
206:34 - writing a new query a nested query and
206:38 - making sure that SQL solves this query
206:41 - first gets the result and then plugs
206:45 - that result back back into the original
206:47 - query to get the data we need there is
206:51 - however a subtle difference between
206:53 - these two queries that turns out to be
206:56 - pretty important in
207:00 - practice and I can give you a clue to
207:03 - what this difference is by telling you
207:05 - that on the right we have something
207:07 - that's called a correlated
207:10 - subquery and on the left we Define this
207:15 - as uncor related
207:18 - subquery now what does this really mean
207:22 - it means that here on the left our
207:24 - subqueries are Computing the minimum and
207:28 - the maximum experience and these are
207:30 - actually fixed values for all of our
207:33 - characters it doesn't matter which
207:36 - character you're looking at the whole
207:38 - data set has the same values from
207:40 - minimum experience and maximum
207:43 - experience so you could even imagine
207:45 - comp Computing these values first before
207:48 - running your queries for example you
207:50 - could say minimum experience is the
207:53 - minimum and maximum experience is the
207:57 - max and then you could imagine replacing
208:00 - these values over here right this will
208:04 - not actually work because you cannot
208:06 - Define variables like this in in SQL but
208:09 - on a logical level you can imagine doing
208:13 - this right because you only need to
208:15 - compute these two
208:17 - once I will revert this here so we don't
208:20 - get
208:23 - confused on the other hand on the right
208:26 - you will see that the value that is
208:29 - returned by sub by this subquery needs
208:32 - to be computed dynamically for every row
208:36 - this value as you also see in the
208:38 - results is different for every row
208:41 - because every row references a different
208:44 - Mentor ID and so SQL cannot compute this
208:49 - one value here for for all rows at once
208:52 - it has to recompute it for every row and
208:55 - this is why we call it a correlated
208:57 - subquery because it's connected to the
209:00 - value that is in each row and so it must
209:03 - run for each
209:05 - row and an important reason to
209:08 - distinguish between uncorrelated and
209:10 - correlated subqueries is that you can
209:12 - imagine that correlated subqueries are
209:15 - actually slow slower and more expensive
209:17 - to run because you have you're running a
209:20 - SQL query for every row at least At The
209:22 - Logical
209:24 - level so this was our introduction to
209:27 - subqueries they allow you to implement
209:29 - more complex logic and as long as you
209:32 - understand it logically you're off to a
209:35 - great start and then by doing exercises
209:37 - and solving problems you will learn with
209:40 - experience when it's the case to use
209:43 - them in the last lecture we saw that we
209:47 - could use subqueries to retrieve
209:49 - singular values for example what is the
209:52 - minimum value of experience in my data
209:57 - set but we can also use subqueries and
210:01 - Common Table Expressions as well to
210:04 - create new tables all together so here's
210:07 - a motivating example for that so what
210:10 - I'm doing in this query right here is
210:13 - that I am scaling the value of level
210:16 - based on the character's
210:19 - class and you might need this in order
210:22 - to create some balance in your game or
210:24 - for whatever reason now what this does
210:28 - is that if the character is Mage the
210:31 - level gets divided by half or multiplied
210:34 - by
210:35 - 0.5 if the character is Archer or
210:38 - Warrior the level we take the 75% of it
210:43 - and in all other cases the level gains
210:46 - 50% so the details are not very
210:49 - important it's just an example but the
210:51 - point is that we modify the value of
210:53 - level based on the character class and
210:57 - we do this with the case when statement
211:00 - that we saw in a previous
211:02 - lecture and as you can see in the
211:04 - results we get a new value of power
211:07 - level for each character that you can
211:10 - see
211:11 - here but now let's say that I wanted to
211:15 - filter my my characters based on this
211:17 - new column of power level say that I
211:20 - wanted to only keep characters that have
211:23 - a power level of at least 15 how would I
211:26 - do that well we know that the wear
211:29 - filter can be used to filter rows so you
211:33 - might just want to go here and add a
211:36 - wear statement and say where power level
211:40 - is equal or bigger than 15 but this is
211:44 - not going to work
211:47 - right we know this cannot work because
211:50 - we know how the logical order of SQL
211:52 - operations works and so the case when
211:58 - column that we create power level is
212:02 - defined here at the select stage but the
212:07 - wear filter occurs here at the beginning
212:10 - right after we Source our table so due
212:13 - to our rules the wear component cannot
212:16 - know about this power level column that
212:19 - will actually get created
212:21 - later so the query that we just wrote
212:24 - actually violates the logical order of
212:28 - SQL operations and this is why we cannot
212:31 - filter
212:33 - here now there is actually one thing
212:37 - that I could do here to avoid using a
212:42 - subquery and get around this error and
212:46 - that's something would be to avoid using
212:49 - this Alias power level that we assigned
212:52 - here and that the we statement cannot
212:54 - know
212:55 - about and replace it with the whole
212:59 - logic of the case when
213:01 - statement so this is going to look
213:03 - pretty
213:05 - ugly but I'm going to do
213:08 - it and if I run this you will see that
213:12 - we in fact get the result we wanted
213:16 - now in the wear lecture we saw that the
213:20 - wear Clause doesn't just accept simple
213:23 - logical statements you can use all the
213:26 - calculations and all the techniques that
213:29 - are available to you at the select stage
213:34 - and you can also use case when
213:37 - statements and this is why this solution
213:40 - here actually
213:42 - works however this is obviously very
213:45 - ugly and impractical and you should
213:49 - never duplicate code like this so I'm
213:53 - going to remove this wear Clause over
213:57 - here and show you how you can achieve
213:59 - the same result with a
214:02 - subquery so let me first rerun this
214:05 - query over here so that you can see the
214:10 - results and now what I'm going to do I'm
214:13 - going to select this whole logic over
214:15 - here
214:16 - and wrap it in round
214:18 - brackets and then up here I'm going to
214:21 - say select star from and when I run this
214:26 - new query this data that I'm seeing over
214:29 - here should be
214:32 - unchanged so let us run
214:36 - it and you will see that the data has
214:39 - not changed at all but what is actually
214:42 - happening here well it's pretty simple
214:46 - usually we say select star from fantasy
214:50 - characters right and by this we indicate
214:54 - the name of a table that our system can
214:57 - access but now instead of a table name
215:00 - we are showing a
215:03 - subquery and this subquery is a piece of
215:07 - SQL logic that obviously returns a
215:11 - table so SQL will look at this whole
215:14 - code and we'll say say okay there is a
215:17 - outer query which is this one and there
215:20 - is an inner query a nested query which
215:22 - is this one so I will compute this one
215:26 - first and then I will treat this as just
215:29 - another table that I can then select
215:33 - from and now because this is just
215:36 - another table we can actually apply a
215:39 - wear filter on top of it we can say
215:43 - where power level is equal or greater
215:47 - than 15 and you will see that we get the
215:51 - result we wanted just like before but
215:55 - now our code looks actually better and
215:58 - the case when logic is not
216:02 - duplicated if you wanted to visualize
216:04 - this in our schema it would look
216:07 - something like this so the flow of data
216:10 - is the following first we run the inner
216:14 - query that works just like all the other
216:18 - queries we've seen until now it starts
216:21 - with the from component which gets the
216:23 - table from the database and then it goes
216:26 - through the usual pipeline of SQL logic
216:29 - that eventually produces a result which
216:31 - is a table next that table gets piped
216:36 - into the outer query the outer query
216:39 - also starts with the from component but
216:41 - now the from component is not redem
216:44 - directly from the dat database it is
216:46 - reading the result of the inner query
216:50 - and now the outer query goes through the
216:52 - usual pipeline of components and finally
216:55 - it produces a table and that table is
216:58 - our
217:01 - result and this process could have many
217:04 - levels of nesting because the inner
217:07 - query could reference another query
217:09 - which references another query and
217:12 - eventually we would get to the database
217:15 - but it could take many steps to get
217:17 - there and to demonstrate how multiple
217:21 - levels of nesting works I will go back
217:23 - to my query over here and I will go into
217:27 - my inner query which is this one and
217:31 - this is clearly referencing the table in
217:34 - the database but now instead of
217:36 - referencing the table I will reference
217:39 - yet an other
217:41 - subquery which can be something like
217:46 - from fantasy
217:49 - characters where is alive equals true
217:54 - select
217:56 - star so I will now run this
218:00 - and we have added yet another subquery
218:05 - to our code this was actually not
218:08 - necessary at all you could add the wear
218:11 - filter up here but it is just to
218:14 - demonstrate the fact that you can Nest a
218:16 - lot of queries within each
218:19 - other the other reason I wanted to show
218:22 - you this code is that I hope you will
218:26 - recognize that this is also not a great
218:29 - way of writing
218:31 - code it can get quite confusing and it's
218:36 - not something that can be easily read
218:38 - and
218:39 - understood one major issue is that it
218:42 - interrupts the natural flow of reading
218:45 - code
218:46 - because you constantly have to interrupt
218:48 - a query because another nested query is
218:52 - beginning within it so you will read
218:56 - select start from and then here another
218:58 - query starts and this is also querying
219:01 - from another subquery and after reading
219:04 - all of these lines you will find this
219:07 - wear filter that actually refers to the
219:10 - outer query that has started many many
219:13 - lines back
219:15 - and if you find this confusing well I
219:18 - think you're right because it is and the
219:21 - truth is that when you read code on the
219:24 - job or in the wild or when you see
219:27 - solutions that people propose to coding
219:30 - challenges unfortunately this is
219:32 - something that occurs a lot you have
219:35 - subqueries within subqueries within
219:37 - subqueries and very quickly the code
219:40 - becomes impossible to read fortunately
219:44 - there is a better way to handle this and
219:46 - a way that I definitely recommend over
219:49 - this which is to use common table
219:53 - Expressions which we shall see
219:55 - shortly it is however very important
219:59 - that you understand this way of writing
220:01 - subqueries and that you familiarize
220:04 - yourself with it because whether we like
220:06 - it or not a lot of code out there is
220:09 - written like
220:11 - this we've seen that we can use the
220:14 - subquery functionality to define a new
220:18 - table on the Fly just by writing some
220:21 - code a new table that we can then query
220:24 - just like any other SQL
220:27 - table and what this allows us to do is
220:30 - to run jobs that are too complex for a
220:35 - single query and to do that without
220:39 - defining new tables in our database and
220:42 - and storing new tables in our database
220:46 - it is essentially a tool to manage
220:49 - complexity and this is how it works for
220:52 - subqueries so instead of saying from and
220:55 - then the name of a table we open round
220:58 - brackets and then we write a independent
221:01 - SQL query in there and we know that
221:05 - every sqle query returns a table and
221:08 - this is the table that we can then work
221:11 - on what we do here is to select star
221:14 - from this table and then apply a filter
221:17 - on this new column that we created in
221:19 - the subquery power
221:21 - level and now I will show you another
221:24 - way to achieve the same result which is
221:27 - through a functionality called Common
221:30 - Table
221:32 - Expressions to build a Common Table
221:35 - expression I will take the logic of this
221:38 - query right here and I will move it
221:42 - up and next I will give a name to this
221:46 - table I will call it power level
221:51 - table and then all I need to say is with
221:55 - power level table
221:58 - as followed by the
222:03 - logic and now this is just another table
222:08 - that is available in my query and it is
222:11 - defined by the logic of what occurs
222:14 - Within the round
222:17 - brackets and so I can refer to this over
222:21 - here and query it just like I need and
222:25 - when I run this you see that we get the
222:28 - same results as
222:31 - before and this is how a Common Table
222:34 - expression works you start with the
222:36 - keyword with you give an alias to the
222:40 - table that you're going to
222:42 - create you put as open round brackets
222:47 - write an independent query that will of
222:49 - course return a table under this alas
222:52 - over here and then in your code you can
222:55 - query this Alias just like you've done
222:57 - until now for any SQL
223:00 - table and although our data result
223:03 - hasn't changed I would argue that this
223:06 - is a better and more elegant way to
223:08 - achieve the same result because we have
223:11 - separated in the code the logic for the
223:15 - these two different tables instead of
223:17 - putting this logic in between this query
223:20 - and sort of breaking the flow of this
223:22 - table we now have a much cleaner
223:25 - solution where first we Define the
223:27 - virtual table that we will need and by
223:30 - virtual I mean that we treat it like a
223:32 - table but it's not actually saved in our
223:34 - database it's still defined by our
223:37 - code and then below that we have the
223:40 - logic that uses this virtual
223:43 - table we can also have multiple Common
223:47 - Table expressions in our query let me
223:50 - show you what that looks like so in our
223:54 - previous example on subquery we added
223:57 - another part where here instead of
224:00 - querying the fantasy characters table we
224:02 - queried a filter on this characters
224:05 - table and it looked like this we were
224:08 - doing select
224:10 - star
224:12 - where is alive equals true so I'm just
224:16 - reproducing what I did in the previous
224:18 - lecture on subqueries now you will
224:21 - notice that this is really not necessary
224:24 - because all we're doing here is add a
224:26 - wear filter and we could do this in this
224:29 - query directly but please bear with with
224:32 - me because I just want to show you how
224:34 - to handle multiple
224:36 - queries the second thing I want to tell
224:39 - you is although this code actually works
224:42 - and you can verify for yourself I do not
224:45 - recommend doing this meaning mixing
224:48 - Common Table expressions and
224:51 - subqueries it is really not advisable
224:54 - because it adds unnecessary complexity
224:57 - to your code so here we have a common
225:01 - table expression that contains a
225:03 - subquery and I will rather turn this
225:06 - into a situation where we have two
225:08 - common table expressions and no
225:10 - subqueries at all and to do that I will
225:14 - take this logic over
225:17 - here and paste it at the
225:21 - top and I will give this now an
225:25 - alias so I will call it characters alive
225:28 - but you can call it whatever is best for
225:32 - you and then I will do the keyword
225:36 - as add some lines in here to make it
225:39 - more readable and now once we are
225:43 - defining multiple Common Table
225:45 - Expressions we only need to do the with
225:48 - keyword once at the beginning and then
225:51 - we can simply add a comma and please
225:53 - remember this the comma is very
225:55 - important and then we have the Alias of
225:58 - the new table the as keyword and then
226:02 - the logic for that
226:03 - table all that's needed to do now is to
226:06 - fill in this from because we took away
226:08 - the subquery and we need to query the
226:11 - characters
226:13 - alive virtual table here and this is
226:16 - what it looks like and if you run this
226:19 - you will get your
226:21 - result so this is what the syntax looks
226:24 - like when you have multiple Common Table
226:27 - Expressions you start with the keyword
226:30 - with which you're only going to need
226:32 - once and then you give the Alias of your
226:35 - first table as keyword and then the
226:39 - logic between round brackets and then
226:42 - for every extra virtual table that you
226:45 - want to add for every extra Common Table
226:47 - expression you only need to add a comma
226:50 - and then another Alias the ask keyword
226:53 - and then the logic between round
226:56 - brackets and when you are done listing
226:59 - your Common Table Expressions you will
227:02 - omit the comma you will not have a comma
227:05 - here because it will break your
227:07 - code and finally you will run your main
227:12 - query and in each of these queries that
227:15 - you can see here you are totally free to
227:17 - query real tables you know material
227:20 - tables that exist in your database as
227:23 - well as common table Expressions that
227:25 - you have defined in this code and in
227:28 - fact you can see that our second virtual
227:31 - table here is quering the first
227:35 - one however be advised that the order in
227:38 - which you write these Common Table
227:40 - Expressions matters because a Common
227:43 - Table expression can only reference
227:46 - Common Table Expressions that came
227:48 - before it it's not going to be able to
227:50 - see those that came after it so if I say
227:54 - here instead of from fantasy characters
227:57 - I try to query from power level table
228:00 - you will see that I get an error from
228:02 - bigquery because it thinks it doesn't
228:04 - recognize it basically because the code
228:07 - is below so the order in which you write
228:10 - them
228:11 - matters now an important question to ask
228:15 - is when should I use subqueries and when
228:18 - should I use common table expressions
228:20 - and the truth is that they have a
228:23 - basically equivalent functionality what
228:26 - you can do with the subquery you can do
228:28 - with a common table expression my very
228:31 - opinionated advice is that every time
228:35 - you need to define a new table in your
228:38 - code you should use a Common Table
228:41 - expression because they are simpler
228:46 - easier to understand cleaner and they
228:49 - will make your code more
228:51 - professional in fact I can tell you that
228:54 - in the industry it is a best practice to
228:57 - use common table Expressions instead of
228:59 - subqueries and if I were to interview
229:01 - you for a data job I would definitely
229:04 - pay attention to this
229:06 - issue but there is an exception to this
229:10 - and this is the reason why I'm showing
229:12 - you this query which we wrote in a
229:14 - previous lect lecture on
229:15 - subqueries this is a query where you
229:18 - need to get a single specific value
229:22 - right so if you remember we wanted to
229:24 - get characters whose experience is above
229:27 - the minimum experience in the data and
229:30 - also below the maximum experience so
229:33 - characters that are in the
229:35 - middle to do this we need to dynamically
229:38 - find at any point you know when this
229:41 - query is being run what is the minimum
229:43 - experience and the maximum experience
229:46 - and the subquery is actually great for
229:48 - that you will notice here that we don't
229:51 - really need to define a whole new table
229:54 - we just really need to get a specific
229:56 - value and this is where a subquery works
230:00 - well because it implements very simple
230:03 - logic and doesn't actually break the
230:05 - flow of the
230:06 - query but for something more complex
230:09 - like power level table you know this
230:12 - specific query we're using here which
230:14 - takes the name takes the level then
230:16 - applies a case when logic to level to
230:18 - create a new column called power level
230:21 - you could this do this with a subquery
230:23 - but I actually recommend doing it with a
230:26 - common table
230:27 - expression and this is a cool blog post
230:30 - on this topic by the company
230:33 - DBT it talks about common table
230:36 - expressions in SQL why they are so
230:40 - useful for writing complex SQL code and
230:44 - the best best practices for using Common
230:46 - Table
230:47 - expressions and towards the end of the
230:50 - article there's also an interesting
230:53 - comparison between Common Table
230:55 - expressions and
230:57 - subqueries and you can see that of CTE
231:01 - Common Table expressions are more
231:02 - readable whereas subqueries are less
231:05 - readable especially if there there are
231:07 - many nested ones so you know a subquery
231:10 - within a subquery within a subquery
231:12 - quickly becomes unreadable
231:14 - recursiveness is a great advantage of
231:17 - CTE although we won't examine this in
231:20 - detail but basically what this means is
231:23 - that once you define a Common Table
231:25 - expression in your code you can reuse it
231:28 - in any part of your code you can use it
231:31 - in multiple parts right you can use it
231:33 - in other CTE you can use it in your main
231:36 - query and so on on the other hand once
231:40 - you define a subquery you can really
231:43 - only use it in the query in which you
231:45 - defined it you cannot use it in other
231:47 - parts of your code and this is another
231:50 - disadvantage this is a less important
231:52 - factor but when you define a CTE you
231:55 - always need to give it a name whereas
231:57 - subqueries can be anonymous you can see
231:59 - it very well here we of course had to
232:01 - give a name to both of these CTE but the
232:05 - subqueries that we're using here are
232:06 - Anonymous however I don't I wouldn't say
232:09 - that's a huge
232:12 - difference and finally you have that CTE
232:16 - cannot be used in a work Clause whereas
232:19 - subqueries can and this is exactly the
232:23 - example that I've shown you
232:25 - here because this is a simple value that
232:29 - we want to use in our work clause in
232:31 - order to filter our table subqueries are
232:34 - the perfect use case for
232:36 - this whereas CTE are suitable for more
232:40 - complex use cases when you need to
232:42 - Define entire
232:45 - tables in conclusion the article says
232:48 - CTS are essentially temporary views that
232:51 - you can use I've used the term virtual
232:54 - table but temporary view works just as
232:56 - well conveys the same
232:58 - idea they are great to give your SQL
233:01 - more structure and
233:03 - readability and they also allow
233:08 - reusability before we move on to other
233:10 - topics I wanted to show you what an
233:13 - amazing tool to Common Table expressions
233:16 - are to create complex data workflows
233:21 - because Common Table expressions are not
233:23 - just a trick to execute certain SQL
233:26 - queries they're actually a tool that
233:30 - allows us to build data pipelines within
233:33 - our SQL code and that can really give us
233:37 - data
233:39 - superpowers so here I have drawn a
233:42 - typical workflow that you will see in
233:46 - complex SQL queries that make use of
233:49 - Common Table
233:51 - Expressions now what we're looking at
233:53 - here is a single SQL
233:57 - query it's however a complex one because
234:00 - it uses CTE and the query is represented
234:04 - graphically here and in a simple code
234:07 - reference here the blue rectangles
234:11 - represent the Common Table Expressions
234:15 - these virtual tables that you can Define
234:18 - with the CTE syntax whereas the Red
234:21 - Square represents the base query the
234:24 - query at the bottom of your code that
234:27 - ultimately will return the
234:31 - result so a typical flow will look like
234:35 - this you will have a first Common Table
234:38 - expression called T1 that is a query
234:43 - that references a real table a table
234:47 - that actually exists in your data set
234:49 - such as fantasy characters and of course
234:53 - this query will do some work right it
234:56 - can apply filters it can calculate new
234:58 - columns and so on everything that we've
235:01 - seen until
235:02 - now and then the result of this query
235:05 - gets piped in to another Common Table
235:09 - expression this one is T2 that gets the
235:12 - result of whatever happen happened at T1
235:15 - and then apply some further logic to it
235:18 - apply some more
235:20 - Transformations and then again the
235:23 - result gets piped into another table
235:25 - where more Transformations run and this
235:28 - can happen for any number of steps until
235:31 - you get to the final query and in the
235:34 - base query we finally compute the end
235:37 - result that will then be returned to the
235:41 - user so this is effectively a dat
235:44 - pipeline that gets data from the source
235:48 - and then applies a series of complex
235:52 - Transformations and this is similar to
235:54 - The Logical schema that we've been
235:56 - seeing about SQL right except that this
236:00 - is one level
236:02 - further because in our usual schema the
236:05 - steps are done by Clauses by these
236:10 - components of the SQL queries but here
236:13 - every step is actually a query in itself
236:16 - so of course this is a very powerful
236:19 - feature and this data pipeline applies
236:23 - many queries sequentially until it
236:25 - produces the final result and you can do
236:28 - a lot with this
236:30 - capability and also you should now be
236:33 - able to understand how this is
236:35 - implemented in code so we have our usual
236:38 - CTE syntax with and then the first table
236:43 - we
236:44 - call T1 and then here we have the logic
236:48 - within round brackets for T1 and you can
236:50 - see here that in the from we are
236:53 - referencing a table in the data set and
236:56 - then for every successive Common Table
236:58 - expression we just add a comma a new
237:01 - Alias and the logic comma new Alias and
237:04 - the logic and finally when we're done we
237:07 - write our base query and you can see
237:10 - that the base query is selecting from T3
237:13 - T3 is selecting from T2 T2 is selecting
237:17 - from T1 and T1 is selecting from the
237:21 - database but you are not limited to this
237:25 - type of workflow here is another maybe
237:28 - slightly more complex workflow that you
237:30 - will also see in the wild and here you
237:34 - can see that at the top we have two
237:37 - common table Expressions that reference
237:41 - the the database so you can see here
237:43 - like like the first one is getting data
237:46 - from table one and then transforming it
237:48 - the second one is getting data from
237:50 - table two and then transforming it and
237:53 - next we have the third CTE that's
237:56 - actually combining data from these two
237:59 - tables over here so we haven't yet seen
238:02 - how to combine data except through the
238:05 - union um I wrote The Joint here which
238:08 - we're going to see shortly but all you
238:10 - need to know is that T3 is combining
238:13 - data from this these two parent tables
238:16 - and then finally the base query is not
238:20 - only using the data from T3 but also
238:22 - going back to T1 and using that data as
238:26 - well and you remember we said that great
238:29 - thing about ctes is that tables are
238:32 - reusable you define them once and then
238:34 - you can use them anywhere well here's an
238:36 - example with T1 because T1 is defined
238:39 - here at the top of the code and then it
238:41 - is referenced by T3 but it is also
238:44 - referenced by the base
238:46 - query so this is another example of a
238:49 - workflow that you could have and really
238:51 - the limit here is your imagination and
238:54 - the complexity of your needs you can
238:57 - have complex workflows such as this one
239:01 - which can Implement very complex data
239:05 - requirements so this is a short overview
239:09 - of the power of CTE and I hope you're
239:12 - excited to learn about them and to use
239:15 - them in your sequel
239:18 - challenges we now move on to joints
239:22 - which are a powerful way to bring many
239:26 - different tables together and combine
239:29 - their
239:30 - information and I'm going to start us
239:33 - off here with a little motivating
239:37 - example now on the left here I see my
239:41 - characters table and by now we're
239:43 - familiar with this table so let's say
239:46 - that I wanted to know for each character
239:49 - how many items they are carrying in
239:52 - their inventory now you will notice that
239:55 - this information is not available in the
239:59 - characters table however this
240:01 - information is available in the
240:04 - inventory
240:06 - table so how exactly does the inventory
240:09 - table works when you are looking at a
240:11 - table for the first time and you want to
240:14 - understand how it works the best
240:16 - question you can ask is the following
240:18 - what does each row represent so what
240:22 - does each row represent in this table
240:24 - well if we look at the columns we can
240:27 - see that for every row of this table we
240:30 - have a specific character
240:32 - id and an item id as well as a quantity
240:38 - and some other information as well such
240:40 - as whether the item is equipped when it
240:42 - was purchased and and so on so looking
240:46 - at this I realized that each row in this
240:49 - table represents a fact the fact that a
240:54 - character has an item right so I know by
240:58 - looking at this table that character id
241:00 - 2 has item 101 and character ID3 has
241:06 - item six and so
241:09 - on so clearly I can use this in order
241:13 - order to answer my question so how many
241:18 - items is Gandalf
241:21 - carrying to find this out I have to look
241:24 - up the ID of Gandalf which as you can
241:27 - see here is six and then I have to go to
241:30 - the inventory table and in the character
241:34 - id column look for the ID of Gandalf
241:38 - right now unfortunately it's not ordered
241:41 - but I can look for myself here and I can
241:46 - see that at least this row is related to
241:50 - Gandalf because he has character id6 and
241:53 - I can see that Gandalf has item id 16 in
241:57 - his inventory and I'm actually seeing
242:00 - another one now which is this one which
242:03 - is
242:04 - 11 and I'm not seeing anyone uh any
242:07 - other item at the moment so for now
242:10 - based on my imperfect uh visual analysis
242:14 - is I can say that Gandalf has two items
242:17 - in his
242:18 - inventory of course our analysis skills
242:21 - are not limited to eyeballing stuff
242:24 - right we have learned that we can search
242:27 - uh a table for the information we need
242:29 - so I could go here and query the
242:33 - inventory table in a new tab right and I
242:38 - could say give me um from the inventory
242:43 - table
242:44 - where character id equals 6 this should
242:48 - give me all the information for Gandalf
242:50 - and I could say give me all the columns
242:54 - and when I run this I should see that
242:56 - indeed we have uh two rows here and we
243:00 - know that Gandalf has items 16 and 11 in
243:04 - his inventory we don't know exactly what
243:06 - these items are but we know that he's
243:09 - carrying two items so that's a good
243:12 - start
243:14 - okay but uh what if I wanted to know
243:17 - which items Frodo is carrying well again
243:20 - I can go to the characters table and uh
243:23 - look up the name Frodo and I find out
243:25 - that Frodo is id4 so going here I can
243:29 - just plug that uh number into my we
243:33 - filter and I will find out that Frodo is
243:36 - carrying a single type of item which has
243:39 - id9 although it's in a quantity of two
243:44 - and of course I could go on and do this
243:46 - for every character but it is quite
243:48 - impractical to change the filter every
243:51 - time and what if I wanted to know how
243:54 - many items each character is carrying or
243:57 - at least which items each character is
244:00 - carrying all at once well this is where
244:04 - joints come into
244:07 - play what I really want to do in this
244:10 - case is to combine these two tables into
244:15 - one and by bringing them together to
244:18 - create a new table which will have all
244:21 - of the information that I need so let's
244:24 - see how to do
244:25 - this now the first question we must
244:28 - answer is what unites these two tables
244:33 - what connects them what can we use in
244:36 - order to combine them and actually we've
244:39 - already seen this in our
244:42 - example um
244:44 - the inventory table has a character id
244:47 - field which is actually referring to the
244:52 - ID of the character in the character's
244:55 - table so we have two columns here the
244:58 - character id column in inventory and the
245:01 - ID column in characters which actually
245:04 - represent the same thing the identifier
245:07 - for a character and this logical
245:11 - connection the fact that these columns
245:13 - repres repr the same thing can be used
245:16 - in order to combine these
245:19 - tables so let me start a fresh query
245:23 - over here and as usual I will start with
245:26 - the from part now where do I want to get
245:29 - my data
245:31 - from I want to get my data from the
245:34 - characters table just as we've been
245:38 - doing until now however the characters
245:42 - table is not not enough for me anymore I
245:46 - need to join this table on the
245:50 - fantasy. inventory table so I want to
245:53 - join these two
245:56 - tables how do I want to join these two
246:00 - tables well we know that the inventory
246:04 - table has a character id column which is
246:11 - the same as the character
246:14 - tables ID column so like we said before
246:19 - these two columns from the different
246:21 - tables they represent the same thing so
246:24 - there's a logical connection between
246:26 - them and we will use it for the
246:29 - join and I want to draw your attention
246:32 - to the notation that we're using here
246:35 - because in this query we have two tables
246:39 - present and so it is not enough to
246:41 - Simply write the name of columns it is
246:45 - also necessary to specify to which table
246:48 - each column belongs and we do it with
246:50 - this notation so the inventory.
246:54 - character uh is saying that the we are
246:57 - talking about the character id colum in
247:00 - the inventory table and the ID column in
247:03 - the characters table so it's important
247:06 - to write columns with this notation in
247:09 - order to avoid ambiguity when you have
247:11 - more than one table in your your
247:14 - query so until now we have used the from
247:19 - uh Clause to specify where do we want to
247:23 - get data from and normally this was
247:28 - simply specifying the name of a
247:31 - table here we are doing something very
247:34 - similar except that we are creating a
247:38 - new table that is obtained by combining
247:42 - two pre-existing tables okay so we are
247:46 - not getting our data from the characters
247:48 - table and we are not getting it from the
247:50 - inventory table but we are getting it
247:53 - from a brand new table that we have
247:56 - created by combining these two and this
247:59 - is where our data
248:01 - lives and to complete the query for now
248:05 - we can simply add a select star and you
248:08 - will now see the result of this
248:11 - query so let me actually make some room
248:15 - here and expand these results so I can
248:19 - show you what we got and as you can see
248:22 - here we have a brand new table in our
248:27 - result and you will notice if you check
248:31 - the columns that this table includes all
248:34 - of the columns from the characters table
248:37 - and also all of the columns from the
248:40 - inventory table as as you can see here
248:45 - and they have been combined by our join
248:49 - statement now to get a better sense of
248:52 - what's Happening let us get rid of this
248:54 - star and let us actually select the
248:58 - columns that we're interested in and
249:01 - once again I will write columns with
249:04 - this notation in order to avoid
249:07 - ambiguity and in selecting these columns
249:11 - uh I will remind you that we have all of
249:14 - the columns from the characters table
249:16 - and all of the columns from the
249:18 - inventory table to choose from so what I
249:22 - will do here is that I will take the ID
249:25 - columns from
249:28 - characters and I will take the name
249:31 - column from
249:33 - characters and then I will want to see
249:38 - the ID of the item so I will take the
249:45 - inventory table and the item id column
249:49 - from that table and from the inventory
249:52 - table I will also want to see the
249:54 - quantity of each
249:57 - item and to make our results clearer I
250:00 - will order my results
250:04 - by the characters
250:08 - ID and the item ID
250:16 - and you can see here that we get the
250:19 - result that we needed we have all of our
250:23 - characters
250:25 - here with their IDs and their name and
250:29 - then for each character we can tell
250:32 - which items are in their inventory so
250:36 - you can see here that Aragorn has item
250:39 - id4 in his inventory in quantity of two
250:43 - he also has Item 99 so because of this
250:47 - Aragorn has two
250:50 - rows if we look back at Frodo we see the
250:53 - uh information that we retrieved before
250:56 - and the same for Gandalf who has these
250:58 - two items so we have combined the
251:02 - characters table and the inventory table
251:04 - to get the information that we
251:07 - needed what does each row represent in
251:10 - our result well it's the same as the
251:13 - inventory table each row is a fact which
251:17 - is that a certain character possesses a
251:21 - certain item but unlike the inventory
251:24 - table we now have all the information we
251:27 - want for a character and not just the ID
251:30 - so here we've uh we're showing the name
251:33 - of each character but we could of course
251:35 - select more columns and get more
251:38 - information for each character as
251:42 - needed now a short note on notation when
251:46 - you see SQL code in the wild and u a
251:51 - query is joining on two or more tables
251:55 - people uh you know programmers were
251:57 - usually quite lazy and we don't feel
252:00 - like writing the name of the table all
252:02 - all of the time right like we we're
252:05 - doing in this case with characters so
252:07 - what we usually do is that we add an
252:10 - alias um on the table like this
252:14 - so from fantasy characters call it C we
252:18 - will join on inventory call it I and
252:20 - then basically we use this Alias um
252:25 - everywhere in the
252:28 - query both in the instructions for
252:31 - joining and in the column
252:34 - names and the same with
252:38 - characters so I will substitute
252:40 - everything
252:41 - here and and yes maybe it's a bit less
252:45 - readable but it's faster to write and we
252:48 - programmers are quite lazy so we'll
252:50 - often see this notation and you will
252:54 - often also see that in the code we omit
252:57 - the as
252:59 - keyword which can be let's say implicit
253:02 - in SQL code and so we write it like this
253:04 - from fantasy. character C join uh
253:07 - fantasy. inventory i and then C and I
253:11 - refer to the two tables that we're
253:13 - joining and I can run this and show you
253:16 - that the query works just as
253:19 - well now we've seen why join is useful
253:23 - and how it looks like but now I want you
253:26 - to get a detailed understanding of how
253:30 - exactly the logic of join works and for
253:33 - this I'm going to go back to my
253:35 - spreadsheet and what I have here is my
253:39 - characters table and my inventory table
253:43 - these are just like you've seen them in
253:44 - big query except that I'm only taking um
253:49 - four rows each in order to make it
253:52 - simpler for the example and what you see
253:56 - here is the same query that I've just
253:59 - run on big query this is a t a query
254:02 - that takes the characters table joins it
254:05 - on the inventory table on this
254:08 - particular condition and then picks a
254:11 - few columns from this so let us see how
254:14 - to simulate this query in Google
254:18 - Sheets now the first thing I need to do
254:22 - is to build the table that I will run my
254:25 - query on because as we've said before
254:29 - the from part is now referencing not the
254:32 - characters table not the inventory table
254:35 - but the new table which is built by
254:37 - combining these two and so our first job
254:40 - is to build this new table
254:43 - and the first step to building this new
254:46 - table is to take all of the columns from
254:50 - characters and put them in the new table
254:53 - and then take all of the columns from
254:56 - inventory and then put them in the new
254:58 - table and what we've obtained here is
255:01 - the structure of our new table the
255:05 - structure of our new table is uh simply
255:08 - created by taking all of The Columns of
255:12 - the T table on the
255:14 - left along with all of the columns from
255:17 - the table on the
255:20 - right now I will go through each
255:23 - character in turn and consider the join
255:28 - condition the join condition is that the
255:31 - ID of a character is present in the
255:35 - character id column of inventory so let
255:39 - us look at my first
255:41 - character um we have Aragorn and he has
255:44 - ID one now is this ID present in the
255:48 - character id column yes I see it here in
255:52 - the first row so we have a match given
255:56 - that we have a match I will take all of
255:59 - the data that I have in the characters
256:03 - table for
256:04 - Aragorn and then I will take all of the
256:08 - data in the inventory table for the row
256:13 - that matches and I have built here my
256:16 - first
256:19 - row do I have any other Row in the
256:23 - inventory table that
256:25 - matches yes the second row also has a
256:30 - character id of one so because I have
256:33 - another match I will repeat the
256:35 - operation I will will take all of the
256:38 - data that I have in the left table for
256:41 - Aragorn and I will add all of the data
256:45 - from the right column in the row that
256:50 - matches now there are no more matches
256:53 - for
256:54 - id1 uh in the inventory table so I can
256:59 - proceed and I will proceed with Legolas
257:03 - he has character id of two question is
257:06 - there any row that has the value two in
257:10 - the character id column yes I can see it
257:13 - here so I have another match so just
257:16 - like before I will take the information
257:18 - for Legolas and paste it here and then I
257:22 - will take the matching row which is this
257:26 - one and paste it
257:29 - here we move on to gimly because there's
257:34 - no other matches for Legolas now gimly
257:38 - has ID3 and I can see a match over here
257:41 - so I will take the row for
257:44 - gimly paste it here and then take the
257:48 - matching row character id 3 and paste it
257:52 - here
257:53 - great finally we come to Frodo character
257:57 - id for is there any match for this
258:00 - character I can actually find no match
258:03 - at all so I do nothing this row does not
258:08 - come into the resulting table because
258:11 - there is no match
258:13 - and this completes the job of this part
258:16 - of the query over here building the
258:19 - table that comes from joining these two
258:23 - tables this is my resulting
258:27 - table and now to complete the query I
258:30 - simply have to pick the columns that the
258:34 - query asks for so the First Column is
258:38 - character. ID which is this column over
258:42 - here
258:43 - so I will take it and I will put it in
258:46 - my result the second column I want is
258:49 - character. name which is this column
258:52 - over
258:53 - here the third column is the item id
258:56 - column which is this one right here and
258:59 - finally I have
259:02 - quantity which is this one right here
259:05 - and this is the final result of my
259:11 - query and of course this is just like
259:14 - any other SQL table so I can use all of
259:17 - the other things I've learned to run
259:20 - Logic on this table for example I might
259:23 - only want to keep items that are present
259:26 - in a quantity of two and so to do that I
259:29 - will simply add a wear filter here and I
259:33 - will refer uh the inventory table
259:36 - because that's the parent table of the
259:39 - quantity column so I will say I will say
259:41 - i. quantity
259:44 - um bigger or equal to two and then how
259:48 - my query will work is that first it will
259:51 - build this table like we've seen so it
259:53 - will do this stage first and then it
259:57 - will run the wear filter on this table
259:59 - and it will only keep the rows where
260:02 - quantity is at least two and so as a
260:06 - result we will only get this row over
260:10 - here instead of this result that we see
260:13 - right here H except that um we will of
260:17 - course also have to only keep the
260:20 - columns that are specified in the select
260:22 - statement so we will get ID name um Item
260:27 - ID and quantity so this will be the
260:31 - result of my query after I've added a
260:34 - wear
260:36 - filter so let us actually take this and
260:39 - add it to B query and make sure that it
260:41 - works so so I have to add that after the
260:46 - from part and before the order by part
260:50 - right this is the order and after I run
260:54 - this I will see that indeed I get um
260:58 - Aragorn and Frodo is not exactly the
261:01 - same as in our sheet but that's because
261:03 - our sheet has um less data but uh this
261:07 - is what we want to
261:09 - achieve and now let us go back to our
261:12 - super important diagram of the order of
261:16 - SQL operation and let us ask ourselves
261:20 - where does the join fit in in this
261:23 - schema and as you can see I have placed
261:26 - join at the very beginning of our
261:30 - flow together with the from because the
261:34 - truth is that the joint Clause is not
261:38 - really separate from the from CL Clause
261:43 - they are actually one and the same
261:46 - component in The Logical order of
261:49 - operations so as you remember the first
261:53 - stage specifies where our data lives
261:57 - where we do we want to get our data from
262:00 - and until now we were content to answer
262:04 - this question with a single table name
262:07 - with the address of a single table
262:09 - because all the data we needed was in
262:11 - just one table
262:13 - and now instead of doing this we are
262:16 - taking it a step further we are saying
262:20 - our data lives in a particular
262:24 - combination of two or more tables so let
262:28 - me tell you which tables I want to
262:31 - combine and how I want to combine them
262:35 - and the result of this will be of course
262:38 - yet another table and then this table
262:41 - will be the beginning of my flow and
262:45 - after that I can apply all the other
262:48 - operations that I've come to know uh on
262:51 - my table and it will work just like U
262:54 - all our previous examples the result of
262:58 - a join is of course just another
263:03 - table so when you look at a SQL query
263:07 - and this query includes a join you
263:10 - really have to see it as one and the
263:13 - same with the front part it defines the
263:16 - source of your data by combining tables
263:20 - and everything else that you do will be
263:23 - applied not to a single table not to any
263:27 - of the tables that you're combining
263:29 - everything that you do will be applied
263:31 - to the resultant table that comes from
263:35 - this combination and this is why from
263:39 - and join are really the same component
263:41 - and this is why they are the first step
263:44 - in The Logical order of SQL
263:51 - operations let us now briefly look at
263:54 - multiple joints because sometimes the
263:57 - data that you need is in three tables or
263:59 - four tables and you can actually join as
264:03 - many tables as you want uh or at least
264:06 - as many tables as your system uh allows
264:09 - you to join before it becomes too slow
264:12 - so we have our example here from before
264:16 - we have each character and we have their
264:18 - name and we know which items are in
264:21 - their inventory but we actually don't
264:23 - know what the items are we just know
264:26 - their ID so how can I know uh that if
264:30 - Aragorn has item four what item does
264:33 - Aragorn actually have what is the name
264:35 - of this item now obviously this
264:37 - information is available in the items
264:40 - table that you have here on the right
264:42 - and you can see here that we have a name
264:44 - column and just like before I can
264:46 - actually eyeball it I can look for it
264:49 - myself I know that I'm looking for item
264:51 - id 4 and if I go here and uh I go to
264:55 - four I can see that this item is a
264:58 - healing potion and now let us see how we
265:01 - can add this with the
265:03 - join so now I will go to my query and
265:07 - after joining with characters in
265:10 - inventory I will take that result and
265:13 - simply join it on a third table so I
265:16 - will write join on fantasy. items and I
265:22 - can call this it to use a uh brief form
265:29 - uh because I am lazy as all programmers
265:32 - are and now I need to specify the
265:35 - condition on which to
265:38 - join so the condition is that the item
265:42 - ID column which actually came from the
265:45 - inventory table right that's its parent
265:48 - so I'm going to call it inventory.
265:52 - item um ID except that yeah I'm
265:56 - referring to inventory as a simple I
265:58 - that is the brief form is the same as
266:02 - the items table the ID column in the
266:06 - items
266:08 - table and now that I've added my
266:10 - condition
266:13 - the data that I'm searcing is now a
266:16 - combination of these three tables and in
266:21 - my result I now have access to The
266:24 - Columns of the items table and I can
266:28 - access these columns simply by referring
266:30 - to them so I will say it. name and some
266:35 - other thing it. power and after I run
266:39 - this query I should be able for each
266:43 - item to see the name and the power right
266:48 - so Aragorn has a healing potion with
266:51 - power of 50 Legolas has a Elven bow with
266:56 - power of 85 and so
266:59 - on now you may have noticed something a
267:02 - bit curious and it's that name here is
267:05 - actually written as
267:07 - name1 and can you figure out why this is
267:11 - happening well well it's happening
267:13 - because there's an ambiguity right the
267:16 - characters table has a column called
267:18 - name and the items table also has a
267:22 - column called name and because bigquery
267:24 - is not referring to the columns the way
267:26 - we are doing it right by saying the the
267:29 - parent table and then the name of the
267:31 - column it uh it would find itself in a
267:33 - position of having two identically named
267:35 - columns so the second one uh it tries to
267:39 - distinguish it by adding underscore one
267:43 - and how we can remedy this is by
267:45 - renaming the column to something more
267:47 - meaningful for example we could say call
267:49 - this item name which would be a lot
267:52 - clearer for whoever looks at the result
267:55 - of our query and as you can see now the
267:57 - name makes more
267:59 - sense so you can see that the multiple
268:01 - join is actually nothing new because
268:06 - when we join the first time like we did
268:09 - before we have combined two two tables
268:12 - into a new one and then this new table
268:16 - gets joined to a third table so it's
268:19 - simply repeating the join operation
268:22 - twice it's nothing actually
268:25 - new but let us actually simulate a
268:29 - multiple join in our spreadsheet to make
268:31 - sure that we understand it and that it's
268:33 - nothing new so again I have our tables
268:37 - here but I have added the items table
268:39 - which we will combine and I've written
268:42 - here our query right so take the
268:44 - characters table and join it with
268:46 - inventory uh like we did before and then
268:49 - take the result of that table and join
268:51 - it to items and here we have the
268:56 - condition so the first thing we need to
268:59 - do is to process our first join and this
269:02 - is actually exactly what we've done
269:04 - before so let us do it again first of
269:08 - all the combined table uh characters and
269:11 - inventory
269:12 - its structure is obtained by taking all
269:15 - the columns of characters and then all
269:18 - the columns of inventory and putting
269:20 - them side by side and this is the result
269:24 - table now for the logic of this table I
269:27 - will now do it faster because we've done
269:29 - it before but basically we get the first
269:32 - character id1 it has two matches so I'll
269:35 - actually take this values and put them
269:37 - into two rows and for the inventory part
269:41 - I will simply call copy these two rows
269:44 - to um complete my match then we have
269:48 - Legolas there is one match here so I
269:51 - will take the left side and I will take
269:55 - so I'm looking for
269:57 - id2 so I will take this row over here
270:01 - that's all we have and then we have
270:03 - gimle and he also has one match so I'll
270:06 - will take it here and the resulting
270:08 - column and then finally Frodo has no
270:11 - match so I will not add him to my result
270:15 - this is exactly what we've done before
270:17 - so now that we have this new table we
270:20 - can proceed with our next join which is
270:23 - with items okay so the resulting table
270:27 - will be the result of our first join
270:31 - combined with items and to show you that
270:33 - we've already computed uh this and now
270:37 - it's one table I have added round
270:40 - brackets now the rules for joining are
270:43 - just the same so take all of the columns
270:47 - in the left side
270:50 - table and then take all of the columns
270:53 - in the right side table and now we have
270:57 - the resulting structure of our
271:01 - table and then let us go through every
271:04 - row so let us look at the first row what
271:07 - does the joint condition say Item ID
271:11 - needs to be in the ID table of items so
271:16 - I can see a match here so I will simply
271:19 - take this row on the left side and the
271:23 - matching row on the right side and add
271:26 - it
271:28 - here second row the item ID is four do I
271:32 - have a match yes I can see that I have a
271:35 - match so I will
271:38 - paste the row on the left and the mat
271:41 - matching row on the right third column
271:45 - item id 2 do I have a match no I don't
271:48 - so I don't need to do anything and in
271:51 - the final row item id 101 I don't see a
271:55 - match so I don't have to do anything and
271:58 - so this is my final
272:00 - result in short multiple join works just
272:04 - like a normal join combine the first two
272:07 - tables get the resulting table and then
272:09 - keep doing this until you run out of
272:13 - joins now there's another special case
272:15 - of join uh which is the self join and
272:19 - this is something that people who are
272:21 - getting started with SQL tend to find
272:23 - confusing but I want to show you that
272:25 - there's nothing uh confusing about it
272:28 - because really it's just a regular join
272:32 - that works just like all the other
272:33 - joints that we've seen there's nothing
272:36 - actually special about it so we can see
272:39 - here uh the characters table
272:42 - and you might remember that for each
272:45 - character we are we have a column of
272:48 - Mentor ID now in a lot of cases this
272:51 - column has value null so it means that
272:54 - there's nothing there but in some cases
272:56 - there is a value there and what this
272:58 - means is that this particular character
273:01 - so we are looking at number three uh
273:03 - that is Saruman uh this particular
273:06 - character has a mentor and who is this
273:10 - Mentor uh all we know is that their ID
273:13 - is
273:15 - six and it turns out that the ID in this
273:19 - column is referring to the ID in the
273:23 - characters table so to find out who six
273:26 - is I just have to look who has an ID of
273:29 - six and I can see that it is Gandalf so
273:32 - by eyeballing it I know that San has a
273:34 - mentor and that Mentor is Gandalf and
273:37 - then elron also has the same Mentor
273:40 - which is Gand
273:42 - so I can solve this by eyeballing the
273:45 - table but how can I get a table that
273:48 - shows for each character who has a
273:51 - mentor who their Mentor is it turns out
273:54 - that I have to take the character's
273:56 - table and join it on the characters
273:59 - table on itself so let's see how that
274:02 - works in
274:04 - practice so let me start a new query
274:07 - here on the right and so my goal here is
274:12 - to list every character in the table and
274:16 - then to also show their Mentor if they
274:19 - have
274:20 - one so I will of course have to get the
274:25 - characters table for this and the first
274:28 - time I take this table it is simply to
274:30 - list all of the characters right so to
274:34 - remind myself of that I can give it a
274:36 - label which is
274:38 - chars now as you know each
274:42 - character has a mentor ID value and but
274:46 - to find out who like what is the name of
274:50 - this Mentor I actually need to look it
274:52 - up in the characters table so to do this
274:55 - I will join on another instance of the
274:59 - characters table right this is another
275:03 - let's say copy of the same data but now
275:06 - I'm going to use it for a different
275:08 - purpose I will not use it to list my
275:11 - characters I will use it to get the name
275:15 - of the mentor so I will call this
275:18 - mentors to reflect this
275:21 - use now what is The Logical connection
275:25 - between these two copies of the
275:27 - characters
275:30 - table each character in my list of
275:34 - characters has a mentor ID field and I
275:39 - want to match this on the the ID field
275:43 - of my mentor
275:46 - table so this is The Logical connection
275:50 - that I'm looking
275:52 - for and I can now add a select star to
275:56 - quickly complete my query and see the
275:59 - results over here so the resulting table
276:03 - has all of The Columns of the left table
276:05 - and all of The Columns of the right
276:07 - table which means that the columns of
276:09 - the characters table will be repeated uh
276:12 - twice in the result as you can see here
276:15 - but on the left I simply have my list of
276:18 - characters okay so the first one is
276:21 - Saruman and then on the right I have the
276:24 - data about their Mentor so Saran has a
276:28 - mentor ID of six and then here starts
276:30 - the data about the mentor he has ID of
276:33 - six and his name is Gandalf so you can
276:37 - see here that our self jooin has worked
276:40 - as intended
276:43 - but this is actually a bit messy uh we
276:46 - don't need uh all of these columns so
276:48 - let us now select Only The Columns that
276:51 - we need so from my list of characters I
276:54 - want the
276:55 - name and then from the corresponding
277:00 - Mentor I also want the name and I will
277:05 - label these columns so that they make
277:07 - sense to whoever is looking at my data
277:09 - so I will call this character
277:12 - character name and I will call this
277:15 - Mentor name and when I run this query
277:19 - you can see that quite simply we get
277:22 - what we wanted we have the list of all
277:24 - our characters at least the ones who
277:26 - have a mentor and for each character we
277:30 - can see the name of their
277:33 - Mentor so a self join works just like
277:37 - any other join and the key to avoiding
277:41 - confusion is to realize that you are
277:44 - joining on two different copies of the
277:48 - same data okay you're not actually
277:51 - joining on the same exact table so one
277:56 - copy of fantasy characters we call
277:59 - characters and we use for a purpose and
278:01 - then a second copy we call mentors and
278:03 - we use for another purpose and when you
278:05 - realize this you see that you are simply
278:08 - joining two tables uh and all the rules
278:11 - that you've learned about normal joints
278:13 - apply it just so happens that in this
278:15 - case the two tables are identical
278:18 - because you're getting the data from the
278:20 - same
278:22 - source and to drive the point home let
278:25 - us quickly simulate this in our trusty
278:29 - spreadsheet and so as you can see here
278:32 - uh I have the query that I've run in B
278:35 - query and we're now going to simulate it
278:38 - so the important thing to see here is
278:41 - that that we're not actually joining one
278:44 - table to itself although that's what it
278:46 - looks like we're actually joining two
278:49 - tables which just happen to look the
278:51 - same okay and so one is called chars and
278:55 - one is called mentors based on the label
278:58 - that we've given them but then once we
279:01 - join them the rules are just the same as
279:03 - we've seen until now so to create the
279:07 - structure of the resulting table take
279:10 - all the columns from the left left and
279:12 - then take all the columns from the
279:15 - right and then go row by row and look
279:18 - for matches based on on the
279:22 - condition now the condition is that
279:25 - Mentor ID in chars needs to be in the ID
279:29 - column of mentors so first row Aragorn
279:33 - has Mentor 2 is this in the ID column
279:37 - yes I can see a match here so let me
279:39 - take all the values from here and all
279:42 - the values from the matching rows paste
279:45 - them together are there any other
279:48 - matches
279:51 - no second row we're looking for Mentor
279:55 - ID 4 do we have a match yes I can see it
280:00 - here so let me take all of the values
280:02 - from the left and all of the values from
280:05 - the matching row on the
280:07 - right now we have two more rows but but
280:11 - as you can see in both cases Mentor ID
280:14 - is null which means that they have no
280:16 - mentor and basically for the purposes of
280:19 - the join we can ignore these rows we are
280:23 - not going to find a match in these
280:26 - rows in fact as an aside even if there
280:29 - was a character whose ID was
280:32 - null uh we wouldn't match with Mentor ID
280:36 - null on a character whose ID was null
280:38 - because in squl in a sense null does not
280:42 - equal null because null is not a
280:44 - specific value but it represents the
280:47 - absence of data so in short when Mentor
280:50 - ID is null we can be sure that in this
280:53 - case uh there will be no match and the
280:56 - row will not appear in the
280:59 - join now that we have our result we
281:02 - simply need to select the columns that
281:05 - we want and so the first one is name
281:08 - which comes from the charge table which
281:11 - is this one over
281:13 - here and the second one is name that
281:15 - comes from the mentor table which is
281:18 - this one over here and here is our
281:20 - result so that's how a self join
281:25 - works so until now we have seen uh joint
281:29 - conditions which are pretty strict and
281:32 - and straightforward right so there's a
281:34 - column in the left table and there's a
281:37 - column in the right table and they
281:39 - represent the same thing and then you
281:42 - look for an exact match between those
281:44 - two columns and typically they're an ID
281:46 - number right so one table has the item
281:50 - id the other table also has the item ID
281:52 - and then you look for an exact match and
281:55 - if there's an exact match you include
281:57 - the row in the join otherwise not that's
281:59 - pretty straightforward but what I want
282:02 - to show you here is that the join is
282:05 - actually much more flexible and and
282:08 - powerful than that and you don't always
282:10 - need you know two columns that represent
282:13 - the exact same thing or an exact match
282:16 - in order to write a joining
282:18 - condition in fact you can create your
282:21 - own you know complex conditions and
282:25 - combinations that decide how to join two
282:28 - tables and for this you can simply use
282:31 - the Boolean algebra magic that we've
282:34 - learned about in this course and that
282:37 - we've been using for example when
282:39 - working on the wear filter so so let us
282:41 - see how this works in practice now I've
282:45 - tried to come up with an example that
282:46 - will illustrate this so let's say that
282:49 - we have a game you know board game or
282:51 - video game or whatever and we have our
282:54 - characters and we have our items okay
282:57 - and in our game um a character cannot
283:00 - simply use all of the items in the world
283:04 - okay there is a limit to which items a
283:07 - character can use and a limit is based
283:10 - on the following rule um let me write it
283:13 - here as a comment and then we will uh
283:16 - use it in our logic so a character can
283:20 - use any item for which the power
283:25 - level is equal or greater
283:30 - than the
283:34 - characters
283:35 - experience divided by 100 okay so this
283:39 - is just a rule uh that exists in our
283:43 - game and now let us say that we wanted
283:46 - to get a list of all
283:50 - characters and the items that they can
283:54 - use okay and this is clearly uh a case
283:58 - where we would need a
283:59 - join so let us actually write this query
284:03 - I will start by getting my data from
284:06 - fantasy. characters and I will call this
284:09 - c as a shorthand
284:11 - and I will need to join on the items
284:14 - table right and what is the condition of
284:19 - the
284:21 - join the condition of the join is that
284:24 - the character's
284:27 - experience divided by
284:30 - 100 is greater or equal than the items
284:37 - power level and I forgot here to add a
284:41 - short hand I for the items table so this
284:45 - is the condition that refects our
284:48 - Rule and out of this table that I've
284:52 - created I would like to see the
284:55 - characters
284:57 - name and the characters experience
285:00 - divided by
285:02 - 100 and then I would like to see the
285:04 - items name and the items power to make
285:08 - sure that my um join is working as
285:14 - intended so let us run this and look at
285:16 - the result so this looks a bit weird
285:19 - because we haven't given a label to this
285:21 - column but basically I can see um that I
285:25 - have Gandalf and his experience divided
285:27 - by 100 is 100 and he can wear the item
285:32 - Excalibur that has a power of 100 which
285:35 - satisfies our condition let me actually
285:39 - order
285:41 - by
285:43 - character name so that I can
285:46 - see in one place all of the items that a
285:50 - character can wear so we can see that
285:53 - Aragorn is first and his experience
285:55 - divided by 100 is 90 and then uh this is
285:59 - the same in all all of these rows that
286:01 - we see right now but then we see all of
286:03 - the items that Aragorn is allowed to use
286:07 - and we see their power and in each case
286:09 - you will see that their power does not
286:12 - exceed this value on the left so the
286:16 - condition uh that we wrote works as
286:21 - intended so as you can see what we have
286:24 - here is a Boolean expression just like
286:27 - the ones we've seen
286:28 - before which is a logical statement that
286:33 - eventually if you run it it evaluates to
286:37 - either true or false and all of the
286:40 - rules that we've seen for Boolean
286:43 - Expressions apply here as well for
286:46 - example I can decide that this rule over
286:49 - here does not apply to Mages because
286:52 - Mages are special and then I can say
286:55 - that if a character is Mage then I want
286:57 - them to be able to use all of the items
287:00 - well how can I do this in this query can
287:03 - you pause the video and figure it
287:08 - out so what I can do is to Simply expand
287:12 - my Boolean expression by adding an or
287:15 - right and what I want to test for is
287:18 - that character class equals Mage so let
287:23 - me check for a second that I have class
287:26 - and I have Mage so this should work and
287:28 - if I run
287:31 - this going through the result I will not
287:33 - do it but you can uh do it yourself and
287:36 - and verify for yourself that if a
287:39 - character is a Mage you will find out
287:42 - that they can use all of the
287:45 - items and this of course is just a
287:48 - Boolean
287:49 - expression um in which you have two
287:51 - statements connected by an or so if any
287:55 - of this is true if at least one of these
287:58 - two is true then the whole statement
288:01 - will evaluate to true and so the row
288:04 - will match if you have trouble seeing
288:07 - this then go back to the video on the
288:10 - Boolean algebra and uh everything is
288:13 - explained in there so this is just what
288:17 - we did before when we simulated The
288:20 - Joint in the spreadsheet you can imagine
288:23 - taking the left side table which is uh
288:25 - characters and then going row by row and
288:29 - then for the first row you check all of
288:32 - the rows in the right side table which
288:35 - is items all of the rows that have a
288:38 - match but this time you won't check if
288:41 - the ID corresponds you will actually run
288:45 - this expression to see whether there is
288:48 - a match and when this expression
288:51 - evaluates as true you consider that to
288:53 - be a match and you include the row in
288:56 - the join however if this condition does
288:59 - not evaluate to true it's not a match
289:01 - and so the row is not included in the
289:04 - join so this is simply a generalization
289:08 - from the exact match which shows you
289:11 - that you can use any conditions in order
289:13 - to join uh two
289:16 - tables now I've been pretending that
289:19 - there is only one type of join in SQL
289:23 - but that is actually not true there are
289:26 - a few different types of join that we
289:29 - need to know so let us see uh what they
289:33 - are and how they work now this is the
289:36 - query that we wrote before and this is
289:39 - exactly how we've written it before and
289:43 - as you can see we've simply specified
289:45 - join but uh it turns out that what we
289:50 - were doing all the time was something
289:52 - called inner join okay and now that I've
289:57 - written it explicitly you can see that
290:00 - if I rerun the query I will get exactly
290:03 - the same
290:05 - results and this is because the inner
290:08 - join is by far the most common type of
290:11 - join that you find in SQL and so in many
290:16 - uh styles of SQL such as the one used by
290:20 - bigquery they allow you to skip this
290:24 - specification and they allow you to
290:25 - Simply write join and then it is
290:28 - considered as an inner join so when you
290:31 - want to do an inner join you have the
290:33 - choice whether to specify it explicitly
290:36 - or to Simply write join but what I want
290:40 - to show you you now is another type of
290:42 - join called Left join okay and to see
290:47 - how that works I want to show you um how
290:51 - we can simulate this query in the
290:54 - spreadsheet so as you can see this is
290:58 - very similar to what we've done before I
291:00 - have the query uh that I want to
291:02 - simulate and notice the left join and
291:05 - then I have my two tables now what is
291:08 - the purpose of the left join in the
291:12 - previous examples which were featuring
291:14 - the inner join we've seen that when we
291:17 - combine two tables with an inner join
291:20 - the resulting table will only have rows
291:23 - that have a match in both tables okay so
291:28 - what we did is that we went through
291:30 - every Row in the characters table and if
291:32 - it had a match in the inventory table we
291:34 - kept that row but if there was no match
291:37 - we completely discarded that row
291:40 - but what if we wanted in our resulting
291:43 - table to see all of the characters to
291:47 - make sure that our list of characters
291:49 - was complete regardless of whether they
291:52 - had a match in the inventory table this
291:55 - is what left join is for left join
291:58 - exists so that we can keep all of the
292:01 - rows in the left table whether they have
292:04 - a match or
292:06 - not so let us see that in practice okay
292:09 - so when we need to do a left join
292:12 - between characters and inventory so
292:14 - first of all I need to determine the
292:17 - structure of the resulting
292:19 - table and to do this I will take all of
292:22 - the columns from the left table and all
292:25 - of the columns from the right table
292:27 - nothing new
292:29 - there next step let us go row by Row in
292:33 - the left table and look for matches so
292:36 - we have
292:37 - Aragorn and he actually has two
292:41 - matches uh by now we've uh remembered
292:44 - this so these two rows have a match in
292:48 - character id with the ID of characters
292:50 - so I will take these two rows and add
292:53 - them to my resulting table next is
292:57 - Legolas and I see a match here so I will
293:01 - take the rows where Legolas
293:03 - matches and put it here it's only one
293:05 - row actually gimly has also a single
293:09 - match so I will create the row over here
293:15 - um and so this is the match for
293:21 - gimly and of course I can ensure that
293:24 - I'm doing things correctly by looking at
293:26 - this ID column and uh this character id
293:30 - column over here and they have to be
293:32 - identical right if they're not then I've
293:33 - made a mistake and finally we come to
293:37 - Frodo now Frodo you will see does not
293:41 - have a match in this table so before we
293:44 - basically discarded this row because it
293:46 - had no match right now though we are
293:49 - dealing with the left join that means
293:52 - that all of the rows in the characters
293:55 - table need to be included so I don't
293:58 - have a choice I need to take this row
294:00 - and include it and add it here and now
294:03 - the question is what values will I put
294:06 - in here well I cannot put any value from
294:09 - the inventory table because I don't have
294:11 - a match so the only thing that I can do
294:14 - is to put NS in here NS of course
294:19 - represent the absence of data so they're
294:22 - perfect for this use case and that
294:25 - basically completes uh the sourcing part
294:29 - of our left
294:30 - join now you may have noticed that there
294:33 - is an extra row here in inventory which
294:37 - does not have a match right it is
294:39 - referred into character id 10 but there
294:42 - is no character id 10 so here the frao
294:46 - row also did not have a match but we
294:48 - included it so should we include this
294:51 - row as well the answer is no why not
294:54 - because this is a left joint okay so
294:57 - left joint means that we include all of
295:00 - the rows in the left table even if they
295:02 - don't have a match but we do not include
295:05 - rows in the right table when they do not
295:08 - have a match okay this this is why it's
295:10 - a left join so but if you're still
295:14 - confused about this don't worry because
295:16 - it will become clearer once we see the
295:18 - other types of
295:21 - join and of course for the sake of
295:23 - completeness I can actually finish the
295:25 - query by selecting my columns which
295:28 - would be the uh character
295:31 - id and the character
295:35 - name and the item
295:38 - ID and the item
295:41 - quantity and this is my final result and
295:45 - in the case of Frodo we have null values
295:48 - which tells us that this row found no
295:51 - match in the right table which in this
295:53 - case means that Frodo does not have any
295:57 - items now that you understand the left
296:00 - join you can also easily understand the
296:04 - right joint it is simply the symmetrical
296:07 - operation to the left joint right right
296:11 - so whether you do characters left joint
296:15 - inventory or you do inventory right join
296:19 - characters the result will be identical
296:23 - it's just the symmetrical operation
296:25 - right this is why I wrote here
296:28 - that table a left joint b equals table B
296:34 - right joint a so hopefully that's pretty
296:38 - intuitive but of course if I I did
296:41 - characters right join
296:45 - inventory then the results would be
296:49 - reversed because I would have to keep
296:53 - all of the rows of inventory regardless
296:56 - of whether they have a match or
296:58 - not and only keep rows in characters
297:02 - which have a match so if you experiment
297:05 - for yourself on the data you will easily
297:07 - convince yourself of this result
297:11 - let us now see the left joint in
297:13 - practice so remember the query from
297:16 - before um where we take each character
297:19 - and then we see their Mentor this is the
297:22 - code exactly as we've written it before
297:25 - and so now you know that this is an
297:27 - inner join because when you don't
297:29 - specify what type of join you want SQL
297:31 - assumes it's an inner join at least
297:34 - that's what the SQL in bigquery does and
297:37 - you can see that if I write inner join
297:39 - um
297:41 - I think I have a typo there uh the
297:44 - result is absolutely identical and in
297:47 - this case we're only including
297:49 - characters who have a mentor right we
297:53 - are missing out on characters who don't
297:57 - have a mentor meaning that Mentor ID is
297:59 - null because in the inner join there is
298:02 - no match and so they are
298:05 - discarded but what would happen if I
298:07 - went here and instead turn this into a
298:11 - left
298:12 - join what I expect to happen is that I
298:16 - will keep all of my characters so all of
298:19 - the rows from the left side table
298:21 - regardless of whether they have a match
298:23 - or not regardless of whether they have a
298:25 - mentor or not and so let us run this and
298:29 - let us see that this is in fact the case
298:32 - I now have a row for each of my
298:35 - characters and I have a row for Gandalf
298:38 - even though Gandalf does not have mentor
298:40 - and so I have a null value in here so
298:43 - the left join allows me to keep all of
298:45 - the rows of the left
298:48 - table now we've seen the inner join the
298:54 - left join and the right join which are
298:56 - really the same thing just symmetrical
298:58 - to each other and finally I want to show
299:01 - you the full outer join this is the last
299:06 - type of join that I want to that I want
299:08 - to show you
299:11 - now you will see that a full outer joint
299:15 - is like a combination of all of the
299:17 - joints that we've seen until now so a
299:21 - full outer join gives us all of the rows
299:26 - uh that have a match in the two
299:30 - tables plus all of the rows in the left
299:33 - table that don't have a match with the
299:35 - right table plus all of the rows in the
299:39 - right table
299:40 - that don't have a match in the left
299:43 - table so let us see how that works in
299:47 - practice what I have here is our usual
299:51 - query but now as you can see I have
299:54 - specified a full outer join so let us
299:57 - now simulate this join between the two
300:00 - tables now the first step as usual is to
300:04 - take all of the columns from the left
300:06 - table and all of the columns from the
300:08 - right table to get the structure of the
300:11 - resulting table and now I will go row by
300:15 - Row in the left table so as usual we
300:20 - have
300:21 - Aragorn and you know what I'm already
300:24 - going to copy it here because even if
300:27 - there's not a match I still have to keep
300:29 - this row uh because this is a full outer
300:32 - joint and I'm basically not discarding
300:35 - any row now that I've copied it is there
300:39 - a match
300:40 - well I already know from the previous
300:42 - examples that there are two rows uh in
300:46 - the inventory table that match because
300:48 - they have character id one so I'm just
300:50 - going to take them and copy them over
300:52 - here and in the second row I will need
300:55 - to replicate these
300:58 - values perfect let me move on to Legolas
301:01 - and again I can already paste it because
301:03 - there's no way that I'm going to discard
301:06 - this row but of course we know that
301:08 - Legolas has a m match and moving quickly
301:12 - cuz we've already seen this gimly has a
301:15 - match as
301:17 - well and now we come to Frodo now Frodo
301:21 - again I can already copy it because I'm
301:25 - keeping all the rows but Frodo does not
301:28 - have a match so just like before with
301:32 - the left join I'm going to keep this row
301:35 - but I'm going to add null values in the
301:38 - columns that come from the invent
301:41 - table so now I've been through all of
301:44 - the rows in the left table but I'm not
301:47 - done yet with my join because in a full
301:50 - outer join I have to also include all of
301:54 - the rows from the right table so now the
301:58 - question is are there any rows in the
302:01 - inventory table that I have not
302:03 - considered yet and for this I can check
302:07 - the inventory ID from my result
302:10 - 1 2 3 4 and compare it with the ID from
302:14 - my table 1 2 3 4 5 and then I realize
302:18 - that I have not included row number five
302:21 - because it was not selected by any match
302:24 - but since this is a full outer join I
302:27 - will add this row over here I will copy
302:29 - it and of course it has no correspondent
302:33 - uh in the left table so what do I do
302:36 - once again I will insert null values
302:40 - and that completes the first phase of my
302:43 - full outer
302:45 - join the last phase is always the same
302:48 - right pick the columns that are listed
302:51 - in the select so you have the
302:54 - ID the
302:57 - name Item
302:59 - ID and
303:02 - quantity and this completes my full
303:05 - outer
303:06 - join so remember how I said that a full
303:09 - outer join is like an inner join plus a
303:13 - left join plus a right join here is a
303:17 - visualization that
303:19 - demonstrates now in the result the green
303:23 - rows are the rows in which you have a
303:27 - match on the left table and the right
303:30 - table right and these rows correspond to
303:34 - the inner join and if you run an inner
303:37 - join this this will be the only rows
303:41 - that are returned right now the purple
303:46 - row is including a row that is present
303:51 - in the left table but does not have any
303:54 - match in the right table so if you were
303:59 - to run a left join what would the result
304:03 - be a left joint would include all of the
304:06 - green rows because they have a match and
304:09 - and additionally they would also include
304:12 - the purple row because in the left joint
304:14 - you keep all of the rows from the left
304:17 - if on the other hand you were to run a
304:20 - right join and you wouldn't like swap
304:22 - the names of the tables or anything
304:24 - right you would do characters right join
304:28 - inventory you would get of course all of
304:30 - the green rows because they are a
304:33 - match Additionally you would get the
304:36 - blue row at the end because this row is
304:39 - present in the right table even though
304:41 - there's no match and in the right join
304:44 - we want to keep all the rows that are in
304:46 - the right table and finally in a full
304:50 - outer join you will include all of these
304:54 - rows right so first of all all of the
304:56 - rows that have a match and then all of
304:59 - the rows in the left table even though
305:01 - they don't have a match and finally all
305:03 - of the rows in the right table even
305:06 - though they don't have a match and these
305:08 - are the
305:10 - three or four types of joint that you
305:14 - need to know and that you will find
305:16 - useful in solving your
305:18 - problems now here's yet another way to
305:21 - think about joints in SQL and to
305:23 - visualize joints which you might find
305:26 - helpful so one way to think about SQL
305:30 - tables is that a table is a set of
305:35 - rows and that joints correspond to
305:39 - different ways of uh combining sets and
305:44 - you might remember this from school this
305:46 - is a v
305:48 - diagram it represents the relation uh
305:52 - between uh two sets and the elements
305:55 - that are inside these two sets so you
305:58 - can take set a to be our left table uh
306:02 - containing all of the rows from um the
306:06 - left table and set B to be our right
306:09 - table with all of the rows from the
306:10 - right
306:11 - table and in the middle here you can see
306:15 - that there is an intersection between
306:18 - the sets this intersection represents
306:20 - the rows that have a match uh so this
306:25 - would be the rows that I have colored
306:28 - green in our example over here so what
306:32 - will happen if I select if I want to see
306:35 - only the rows that are a match only the
306:39 - rows that belong in both tables let me
306:41 - select this now and you can see that
306:45 - this corresponds to an inner joint
306:48 - because I only want to get the rows that
306:51 - have a
306:52 - match then what would happen if I wanted
306:55 - to include all of the rows in the left
306:58 - table regardless of whether they have a
307:01 - match or not to what type of join does
307:04 - that
307:06 - correspond I will select it here and you
307:09 - can see that that corresponds to a left
307:13 - join the left join produces a complete
307:16 - set of records from table a with the
307:18 - matching records in table B if there is
307:21 - no match the right side will contain
307:24 - null likewise if I wanted to keep all of
307:28 - the rows in uh table B including the
307:32 - ones that match with a I would of course
307:36 - get a right join which is just
307:38 - symmetrical to a left
307:41 - join finally what would I have to do to
307:45 - include all of the rows from both tables
307:49 - regardless of whether they have a match
307:51 - or
307:52 - not if I do this then I will get a full
307:56 - outer join so this is just one way to
308:00 - visualize what we've already
308:03 - seen there is one more thing you can
308:06 - actually realize from this uh tool which
308:10 - is in some cases you might want to get
308:14 - all of the records that are in a except
308:17 - those that match in B so all of the
308:21 - record that records that a does not have
308:23 - in common with b and you can see how you
308:26 - can actually do this this is actually a
308:29 - left
308:30 - join with an added filter where the b
308:34 - key is null so what does that mean the
308:38 - meaning will be clear if I go back to
308:40 - our example for the left join you can
308:44 - see that this is our result for the left
308:47 - join and because Frodo had no match in
308:50 - the right table the ID column over here
308:54 - is null so if I take this table and I
308:56 - apply a filter where ID where inventory
309:00 - ID is null I will only get this result
309:03 - over here and this is exactly the one
309:05 - row in the left table that does not have
309:08 - a match in the right table so this is
309:11 - more of a special case you don't
309:13 - actually see this a lot in practice but
309:16 - I wanted it wanted to show it briefly to
309:18 - you in case you try it and get curious
309:20 - about it likewise the last thing that
309:23 - you can do you could get all of the rows
309:26 - from A and B that do not have a match so
309:31 - the set of Records unique to table a and
309:34 - table B and this is actually very
309:36 - similar you do a full outer join
309:40 - and you check that either key is null so
309:44 - either inventory ID is null or character
309:47 - id is null and if you apply that filter
309:50 - you will get these two rows which is the
309:52 - set of rows that are in a and only in a
309:56 - plus the rows that are in B and only in
309:58 - B once again I've honestly never used
310:01 - this in practice I'm just telling you
310:03 - for the sake of completeness in case you
310:05 - get curious about
310:07 - it now a brief but very important note
310:11 - on how SQL organizes data so you might
310:15 - remember from the start of the course
310:18 - that I've told you that in a way SQL
310:21 - tables are quite similar to spreadsheet
310:23 - tables but there are two fundamental
310:25 - difference one difference is that each
310:28 - SQL table has a fixed schema meaning we
310:30 - always know what the columns are and
310:32 - what type of data they contain and we've
310:35 - seen how this works extensively the
310:37 - second thing was that SQL tables are
310:40 - actually connected with each other which
310:43 - makes SQL very
310:45 - powerful and now we are finally in a
310:47 - position to understand just exactly how
310:51 - SQL tables can be connected with each
310:53 - other and this will allow you to
310:55 - understand how SQL represents data so I
311:00 - came here to DB
311:02 - diagram. which is a very uh nice website
311:06 - for building representations of SQL data
311:10 - and this is uh this type of um of chart
311:14 - of representation that we see here is
311:16 - also known as ER as you can see me
311:19 - writing here which is stands for entity
311:24 - relationship diagram and it's basically
311:26 - a diagram that shows you how your data
311:30 - is organized in your SQL system and so
311:34 - you can see a representation of each
311:36 - table uh this is the example that's
311:38 - shown on the web website and so you have
311:41 - three tables here users follows and
311:43 - posts and then for each table you can
311:45 - see the schema right you can see that
311:47 - the users table has four columns one is
311:50 - the user ID which is an integer the
311:52 - other is the username which is varar
311:54 - this is another way of saying string so
311:56 - this is a piece of text rooll is also a
311:58 - piece of text and then you have a Tim
312:00 - stamp that shows when the user was
312:03 - created and the important thing to
312:05 - notice here is that these tables are
312:08 - actually they're not they don't exist in
312:10 - isolation but they are connected with
312:12 - each other they are connected through
312:15 - these arrows that you see here and what
312:18 - do these arrows
312:20 - represent well let's look at the follows
312:22 - table okay so each row of this table is
312:25 - a fact shows that one user follows
312:28 - another and so in each row you see the
312:31 - ID of the user who follows and the ID of
312:34 - the user who is followed as well as the
312:38 - time when this event happened and what
312:41 - are these uh arrows telling us they're
312:44 - telling us that the IDS in this table
312:47 - are the same thing as the user ID column
312:50 - in this table which means that you can
312:53 - join the follows table with the users
312:56 - table to get the information about the
312:59 - two users that are here the user who is
313:01 - following and the user who is
313:04 - followed so like we've seen before a
313:07 - table has a column which is the same
313:11 - thing as another tables column which
313:14 - means that you can join them to combine
313:16 - their data and this is how in SQL
313:20 - several tables are connected with each
313:22 - other they are connected by logical
313:26 - correspondences that allow you to join
313:29 - those tables and combine their
313:32 - data likewise you have the post table
313:36 - and each row represent a post and each
313:38 - post post has a user ID and what this
313:41 - arrow is telling you is that uh you can
313:44 - join on the user table using this ID to
313:48 - get all the information you need about
313:50 - the user who has created this post now
313:54 - of course as we have seen you are not
313:57 - limited to joining the tables along
314:00 - these lines you can actually join these
314:03 - tables on whatever condition you can
314:05 - think of but this is a guarantee of
314:09 - consistency between these tables that
314:11 - comes from how the data was
314:14 - distributed and it's a guarantee it's a
314:17 - promise that you can get the data you
314:19 - need by joining on these specific
314:23 - columns and that is really all you need
314:25 - to know in order to get started with
314:27 - joints and use them to explore your data
314:30 - and solve SQL
314:32 - problems to conclude this section I want
314:35 - to go back to our diagram and to remind
314:38 - you that from and join are really one
314:43 - and the same they are the way for you to
314:48 - get the data that you need in order to
314:51 - answer your question and so when the
314:54 - data is in one table alone you can get
314:58 - away with just um using the from and
315:01 - then specifying the name of the table
315:03 - but often your data will be distributed
315:06 - in many different tables so you can look
315:11 - at the ER diagram such as this one if
315:13 - you have it to figure out how your uh
315:17 - data works and then once you decided
315:20 - which tables you want to combine you can
315:23 - write a from which combines with a join
315:27 - and so create a new table uh which is a
315:30 - combination of two or more tables and
315:33 - then all of the other operations that
315:35 - you've learned will run on top of that
315:38 - table
315:40 - we are finally ready for a in-depth
315:43 - discussion of grouping and aggregations
315:46 - in
315:47 - SQL and why is this important well as
315:51 - you can see I have asked Chad GPT to
315:55 - show me some typical business questions
315:57 - that can be answered by data
316:01 - aggregation so let's see what we have
316:03 - here What's the total revenue by
316:06 - quarter how many units did did each
316:09 - product sell last month what is the
316:12 - average customer spent per
316:14 - transaction which region has the highest
316:17 - number of sales now as you can see these
316:19 - are some of the most common and
316:22 - fundamental business
316:24 - questions um that you would be asking
316:27 - when you do analytics and this is why
316:30 - grouping and aggregation are so
316:32 - important when we talk about
316:35 - SQL now let's open our date
316:39 - once again in the spreadsheet and see
316:42 - what we might achieve through
316:46 - aggregation so I have copied here four
316:49 - columns from my characters table Guild
316:53 - class level and experience and I'm going
316:56 - to be asking a few questions the first
317:00 - question which you can see here is what
317:03 - are the level measures by class
317:09 - so what does this mean well earlier in
317:12 - the course we looked at
317:15 - aggregations and we call them simple
317:18 - aggregations because we were running
317:20 - them over the whole table so you might
317:23 - remember that if I select the values for
317:26 - level here I will get a few different
317:30 - aggregations in the lower right of my
317:33 - screen so what you can see here is that
317:37 - I have a count of of 15 which means that
317:40 - there are 15 rows for
317:42 - level and that the maximum level is 40
317:47 - the minimum is 11 and then I have an
317:49 - average level of
317:51 - 21.3 more or less and if you sum all the
317:55 - levels you get
317:57 - 319 so this is already some useful
318:01 - information but now I would like to take
318:04 - it a step further and I would like to
318:06 - know this aggregate value
318:09 - within each class so for example what is
318:13 - the maximum level for
318:16 - warriors and what is the maximum level
318:18 - for Hobbits are they different how do
318:20 - they compare this is where aggregation
318:23 - comes into
318:25 - play so let us do just that now let us
318:29 - find the
318:34 - maximum
318:37 - level Within each class and let us see
318:41 - how we might achieve this now to make
318:44 - things quicker I'm going to sort the
318:46 - data to fit my purpose so I will select
318:49 - the range over here and then go to
318:54 - data sort
318:56 - range and then in the in the advanced
318:59 - options I will say that I want to sort
319:01 - by column B because that's my
319:04 - class and now as you can see the data is
319:07 - ordered by class and I can see the
319:10 - different values for each
319:12 - class next I will take all the different
319:16 - values for
319:17 - class
319:19 - and separate them just like this so
319:23 - first I have Archer then I have
319:27 - hobbit then I have Mage and finally I
319:31 - have
319:33 - Warrior so here they are they're all
319:37 - have their own sp
319:39 - right
319:40 - now finally I just need to take to
319:44 - compress each of these ranges so that
319:47 - each of them covers only one
319:51 - row so for Archer I will take the value
319:55 - of the class
319:57 - Archer and then I will have to compress
320:00 - these numbers to a single number and to
320:04 - do that I will use the max function this
320:07 - is the aggregation function that we are
320:09 - using and quite intuitively this
320:12 - function will look at the list of values
320:15 - we'll pick the biggest one and it will
320:17 - reduce everything to the biggest value
320:20 - and you can also see it here in this
320:23 - tool tip over
320:26 - here doing the same for
320:29 - Hobbit compress all of the values to a
320:32 - single value and then compress all of
320:34 - the numbers to a single number by
320:37 - applying a an aggregation
320:40 - function so I've gone ahead and done the
320:43 - same for mage and
320:44 - Warrior and all that's left to do is to
320:48 - take
320:50 - this and bring all these rows
320:58 - together and this is my
321:01 - result this is doing what I have asked
321:04 - for I was looking to find the maximum
321:07 - level Within each class so I have taken
321:11 - all the unique values of
321:13 - class and then all the values of level
321:17 - within each class I have compressed them
321:20 - to a single number by taking the
321:23 - maximum and so here I have a nice
321:25 - summary which shows me what the maximum
321:28 - level is for each class and I can see
321:32 - that mes are much more powerful than
321:34 - everyone and that Hobbits are much more
321:36 - weaker according to this measure I've
321:38 - learned something new about my
321:41 - data now crucially and this is very
321:45 - important in my
321:47 - results I
321:49 - have
321:51 - class which is a grouping
321:54 - field and then level which is an
321:59 - aggregate
322:02 - field okay so what exactly do I mean by
322:06 - this now class is a grouping field
322:10 - because it divides my data in several
322:13 - groups So based on the value of class I
322:16 - have divided my data as you see here so
322:19 - Archer has three values Hobbit has four
322:22 - values and so
322:24 - on level is an aggregate field because
322:29 - it was obtained by taking a list of
322:33 - several values so here we have three
322:36 - here we have four and in the wild we
322:38 - could have a thousand or 100 thousand or
322:40 - Millions it doesn't matter it's a list
322:43 - of multiple values and then I've taken
322:47 - these values and compressed them down to
322:50 - one value I have aggregated them down to
322:54 - one value and this is why level is an
322:57 - aggregate
322:59 - field and whenever you work with groups
323:01 - and aggregations you always have this
323:05 - division okay you are have some fields
323:10 - that you use for grouping you know for
323:12 - subdividing your data and then you have
323:15 - some fields on which you run
323:19 - aggregations and aggregations such as
323:22 - for example looking at a list of value
323:25 - and taking the maximum value or the
323:27 - average or the minimum and so on
323:30 - aggregations are what allow you to
323:32 - understand the differences between
323:34 - groups so after aggregating you can say
323:37 - oh well the the Mages are certainly much
323:39 - more powerful than the hobbits and so on
323:43 - and if you look work with the dashboards
323:46 - like Tableau or other analytical tools
323:50 - you will see that another way to refer
323:53 - to these terms is by calling the
323:56 - grouping Fields dimensions and the
323:59 - aggregate Fields measures okay so I'm
324:03 - just leaving it
324:04 - here you can say grouping field and
324:06 - aggregate field or you can talk about
324:09 - dimensions and measures and they
324:11 - typically refer to the same type of
324:15 - idea now let's see how I can achieve the
324:18 - same result in SQL so I will start a new
324:21 - query here and I want to get data from
324:26 - fantasy.
324:28 - characters and after I've sourced this
324:31 - table I want to Define my groups okay so
324:35 - I will use Group by which is my new
324:38 - clause and then here I will have to
324:43 - specify the grouping field I will have
324:47 - to specify the group that I want to use
324:50 - in order to subdivide the data and that
324:54 - group is class in this
324:57 - case after that I will want to define
325:01 - the columns that I want to see in my
325:05 - result so I will say select
325:09 - and first of all I want to see the
325:11 - class and then I want to see the maximum
325:16 - level within each
325:19 - class so if I run
325:22 - this you will see that I get exactly the
325:26 - same result that I have in Google
325:30 - Sheets so we have seen this before Max
325:34 - is an aggregation function it takes a
325:37 - list of Val vales and then compresses
325:40 - them down to a single value right except
325:44 - that before we were running it on at the
325:48 - level of the whole table right so if I
325:51 - select this query alone and run it what
325:55 - do you expect to
325:57 - see I expect to see a single value
326:01 - because it has looked at all the levels
326:03 - in the table and it has simply selected
326:06 - the biggest one it has reduced all of
326:08 - them to a single
326:09 - value
326:11 - however if I run it after defining a
326:15 - group buy then this will run not on the
326:19 - whole table at once it will run within
326:21 - each group identified by my grouping
326:26 - field and we'll compute the maximum
326:29 - within that group and so the result of
326:33 - this will be that I can see the maximum
326:35 - level for each group
326:39 - now I'm going to delete
326:41 - this and I don't need to limit myself to
326:46 - a single
326:48 - aggregation I can write as many
326:51 - aggregations as I wish so I will put
326:55 - this down here and I'll actually give it
326:58 - a label so that it makes
327:00 - sense and then I will write a bunch of
327:03 - other aggregations such as count
327:06 - star which basically is the number of
327:10 - values within that
327:13 - class um I can also look at the minimum
327:19 - level I can also look at the average
327:26 - level so let's run this and make sure
327:29 - that it
327:31 - works so as you can see we have our
327:34 - unique values for class as usual and
327:38 - then and for each class we can compute
327:40 - as many aggregated values as we want so
327:44 - we have the maximum level the minimum
327:46 - level and we didn't give a label to this
327:50 - so we can call it average
327:53 - level and
327:55 - then number of
327:57 - values n values is not referring to
328:01 - level in itself it's a more General
328:05 - aggregation which is simply counting how
328:07 - many examples I have of each class right
328:10 - so I know I have four Mages three
328:13 - archers four Hobbits and four Warriors
328:16 - by looking at this value over
328:19 - here and here's another thing I am
328:23 - absolutely not limited to the level
328:28 - column as you can see I also have the
328:30 - experience column which is also an
328:32 - integer and the health column which is a
328:34 - floating Point number so I can get the
328:39 - maximum
328:41 - health and I can get the minimum
328:46 - [Music]
328:48 - experience and it all works all the
328:53 - same all the aggregations are
328:56 - computed within each
328:59 - class but one thing I need to be really
329:02 - careful of is the match between the type
329:07 - of aggregation that I want to run and
329:10 - the data type of the field on which I
329:13 - plan to run it so all of these that we
329:15 - show here they're number columns right
329:18 - either integers or
329:20 - floats what would happen if I ran the
329:24 - average aggregation on the name column
329:29 - which is a
329:30 - string what do you expect to
329:32 - happen you can already see that this is
329:36 - an error why no matching signature for
329:39 - aggregate function average for a type
329:43 - string so it's saying this function does
329:45 - not accept the type string it accepts
329:48 - integer float and all types of number
329:51 - columns but if you ask me to find the
329:55 - average between a bunch of strings I
329:57 - have no idea how to do that so I can add
330:01 - as many aggregations as I want within my
330:03 - grouping but the aggregations need to
330:06 - make sense
330:08 - but these Expressions can be as complex
330:11 - as I want them to be so instead of
330:14 - taking the average of the name which is
330:16 - a string it doesn't make sense I could
330:19 - actually run another function instead of
330:21 - this inside of this which is
330:25 - length and what I expect this to do is
330:28 - that for each name it will count how
330:31 - long that name is and then after I've
330:34 - done all these counts I can aggregate
330:38 - them uh I could take the average for
330:40 - them and what I get back is the average
330:44 - name length within each
330:47 - class doesn't sound really helpful as a
330:50 - thing to calculate but this is just to
330:52 - show you that these Expressions can get
330:56 - quite
330:59 - complex now whatever system you're
331:02 - working with it will have a
331:04 - documentation in some place which lists
331:08 - all the aggregate functions that you
331:10 - have at your disposal so here is that
331:13 - page for big query and as you can see
331:16 - here we have our aggregate
331:19 - functions and if you go through the list
331:22 - you will see some of the ones that I've
331:23 - shown you such as count Max mean and
331:27 - some others that uh I haven't shown you
331:30 - in this example such as sum so summing
331:32 - up all the values um any value which
331:37 - simply picks uh one value I think it it
331:40 - happens at
331:41 - random and U array a which actually
331:45 - built a list out of those values and so
331:49 - on so when you need to do an analysis
331:51 - you can start by asking yourself how do
331:54 - I want to subdivide the data what are
331:57 - the different groups that I want to find
331:59 - in the data and then after that you can
332:01 - ask yourself what type of aggregations
332:04 - do I need within each group what do I
332:06 - want to know
332:08 - um about each group and then you can go
332:11 - here and try to find the aggregate
332:14 - function that works best and once you
332:17 - think you found it you can go to the
332:19 - documentation for that function and you
332:22 - can read the description so Returns the
332:24 - average of non-null values in an
332:26 - aggregated
332:30 - group and then you can see what type of
332:33 - argument is supported for example
332:35 - average supports any numeric input type
332:38 - right so any data type that represents a
332:40 - number as well as interval which
332:43 - represents a space of
332:48 - time now in the previous example we have
332:51 - used a single grouping field right so if
332:55 - we go back here we have our grouping
332:58 - field which is class and we only use
333:01 - this one field to subdivide the data but
333:05 - you can actually use multiple grouping
333:07 - Fields so let's see how that
333:11 - works what I have here is my items table
333:14 - and for each item we have an item type
333:18 - and a rarity type uh and then for each
333:23 - item we know the power so what would
333:26 - happen if we wanted to say to see the
333:31 - average
333:33 - Power
333:34 - by item
333:38 - type and
333:39 - Rarity
333:42 - combination one reason we might want to
333:45 - see this is that we might ask
333:48 - ourselves is within every item type is
333:52 - it always true that if you go from
333:54 - common to rare to Legendary the power
333:58 - increases is this true for all item
334:00 - types or only for certain item types let
334:04 - us go and find out so what what I'm
334:07 - going to do now is that I'm going to use
334:10 - two fields to subdivide my data I'm
334:13 - going to use item type and Rarity and to
334:16 - do this as a first step I will sort the
334:21 - data so that it makes it convenient for
334:24 - me so I will go here and I will say sort
334:28 - range Advanced ranged sorting option and
334:32 - first of all I want to sort by column A
334:36 - which is item type and I want to add
334:38 - another sort column which will be column
334:45 - B and you can see that my data has been
334:49 - sorted next I'm going to take each
334:53 - unique
334:54 - combination of the values of my two
334:58 - grouping Fields okay so the first
335:01 - combination is armor common so I'm going
335:04 - to take this
335:05 - here and then I'm going to to write down
335:09 - all the values that come within this
335:10 - combination so in this case we only have
335:13 - one value which is
335:15 - 40 next I have armor
335:19 - legendary and within this combination I
335:21 - only have one value which is
335:24 - 90 next I have armor rare So for armor
335:29 - rare I actually have two
335:32 - values so I'm going to write them here
335:38 - next we have potion and
335:41 - common for this we actually have three
335:45 - values so I'm going to write them
335:49 - here so I've gone ahead and I've done it
335:52 - for each combination and you can see
335:55 - that each unique combination of item
335:57 - type and Rarity I've now copied the re
336:01 - relevant values and now I need to get
336:05 - the average power with in these
336:08 - combinations so I will take the first
336:11 - one put it over here and then I will
336:14 - take the average of the values this is
336:18 - quite easy because there's a single
336:19 - value so I'll simply write
336:21 - 40 next I will take the armor legendary
336:25 - combination and once again I have a
336:27 - single value for armor rare I have two
336:31 - values so I will actually press equal
336:34 - and write average to call the the
336:38 - spreadsheet function and then select the
336:40 - two values in here to compute the
336:45 - average and here we have
336:48 - it and I can go on like this potion
336:53 - common get the average Within These
336:59 - values potion legendary is a single
337:03 - value so I've gone ahead and completed
337:06 - this and this gives me the result of my
337:09 - query here I have all the different
337:13 - combinations for the values of uh what
337:17 - were they item type and Rarity and
337:20 - within each combination the average
337:24 - power so to answer my question is it
337:27 - that within each item type
337:31 - the power grows with the level of Rarity
337:34 - where for armor it goes from 40 to 74 to
337:38 - 90 so yes for potion we don't have um a
337:43 - rare potion but basically it also grows
337:45 - from common to Legendary and in weapon
337:48 - we have uh 74 87 and 98 so I would say
337:53 - yes within each item type power grows
337:57 - with the level of
337:58 - Rarity so what are these three fields in
338:02 - the context of my
338:05 - grouping well item type is grouping
338:09 - field and Rarity is also a grouping
338:13 - field and the average power within each
338:16 - group is a aggregate field right so I am
338:23 - now using two grouping fields to
338:25 - subdivide my data and then I'm Computing
338:27 - this aggregation within those
338:35 - groups so let us now figure figure out
338:38 - how to write this in SQL it's actually
338:40 - quite similar to what we've seen
338:43 - before we have to take our data from
338:46 - the items
338:49 - table and then we want to
338:54 - group by and here I have to list my
338:57 - grouping Fields okay so as I've said I
339:01 - have two grouping Fields they are item
339:06 - type and and
339:08 - Rarity so this defines my groups and
339:12 - then in the select part I
339:15 - will want to see my grouping
339:19 - fields and then within each group I will
339:22 - want to see the
339:26 - average of power I believe we
339:30 - used yes so I will get the average of
339:35 - power
339:41 - and here are our results just like in
339:46 - the
339:48 - sheets now as a tiny detail you may
339:50 - notice that power here is colored in
339:53 - blue and the reason for this is that
339:56 - power is actually a big query function
339:58 - so if you do power of two three you
340:02 - should get uh eight because it
340:05 - calculates the two to to to the power of
340:09 - three so it can be confusing when power
340:12 - is the name of a column because B query
340:13 - might think it's a function but there's
340:16 - an easy way to remedy this you can just
340:17 - use back ticks and that's your way of
340:21 - telling big query hey don't get confused
340:23 - this is not the name of a function this
340:25 - is actually the name of a column and as
340:28 - you can see it also works and it doesn't
340:31 - create
340:33 - issues and just like before we could add
340:37 - as many aggregations as we
340:44 - wanted and for example we could take the
340:47 - sum of power also on other fields not
340:51 - just on Power and everything would be
340:54 - computed within the groups defined by
340:58 - the two grouping fields that I have
341:03 - chosen as
341:06 - expected now now let us see where Group
341:10 - by fits in The Logical order of SQL
341:15 - operations so as you know a SQL query
341:19 - starts with from and join this is where
341:23 - we Source the data this is where we take
341:25 - the data that we need and as we learned
341:27 - in the join section we could either just
341:31 - specify a single table in the from
341:34 - clause or we could specify a join of two
341:39 - or more
341:40 - tables either way the result is the same
341:43 - we have assembled the table where our
341:45 - data leaves and we're going to run our
341:47 - Pipeline on that data we're going to run
341:50 - all the next operations on that
341:53 - data next the work Clause comes into
341:57 - play which we can use in order to filter
342:02 - out rows that we don't need and then
342:05 - finally our group group Pi
342:09 - executes so the group Pi is going to
342:12 - work on the data that we have sourced
342:15 - minus the rows that we have excluded and
342:18 - then the group Pi is going to
342:20 - fundamentally alter the structure of our
342:23 - table because as you have seen in our
342:26 - examples the group I basically
342:29 - compresses down our values or squishes
342:32 - them as I wrote here because in the
342:35 - grouping field you will get a single Row
342:39 - for each distinct value and then in the
342:42 - aggregate field you will get an
342:44 - aggregate value within each class okay
342:47 - so if I use a group bu it's going to
342:50 - alter the structure of my
342:52 - table after doing the group bu I can
342:55 - compute my aggregations like you've seen
342:59 - in our
343:00 - examples so I can compute uh minimum
343:03 - maximum average sum count and and all of
343:06 - that and of course I need to do this
343:09 - after I have applied my
343:13 - grouping
343:15 - and after that after I I've computed my
343:18 - aggregations I can select them right so
343:21 - I can choose which columns to see um and
343:26 - this will include the grouping fields
343:28 - and the aggregated fields we shall see
343:30 - this more in detail in a second and then
343:33 - finally there's all the other oper
343:37 - ations that we have seen in this
343:40 - course and this is where Group by and
343:43 - aggregations fit in our order of SQL
343:48 - operations now I want to show you an
343:51 - error that's extremely common when
343:54 - starting to work with group pi and if
343:58 - you understand this error I promise you
344:01 - you will avoid a lot of headaches when
344:03 - solving SQL problems so I have my IDE
344:07 - items table here again and you can see
344:09 - the preview on the right and I have a
344:12 - simple SQL query okay so take the items
344:15 - table Group by item type and then show
344:19 - me the item type and the average level
344:23 - of power within that item type so so far
344:27 - so
344:29 - good but what if I wanted to see what
344:33 - I'm showing you here in the comments
344:37 - what if I wanted to see each specific
344:40 - item the name of that item the type of
344:44 - that item and then the average Power by
344:48 - the type of that item right so let's
344:51 - look at the first item chain mail
344:55 - armor this is a armor type of item and
344:59 - we know that the average power for
345:01 - armors is
345:02 - 69.5 so I would like to see this row and
345:06 - then let's take Elven bow now Elven baow
345:09 - is a weapon as you can see here the
345:12 - average powerful weapons is 85. 58 and
345:16 - so I would like to see that now stop for
345:20 - a second and think how might I achieve
345:22 - this how might I modify my SQL query to
345:26 - achieve
345:28 - this oh and there is a error in the
345:31 - column name over here because I actually
345:34 - wanted to say name
345:37 - but let's see how to do it in the SQL
345:39 - query so you might be tempted to Simply
345:43 - go to your query and add the name field
345:47 - in order to reproduce What you see here
345:50 - and if I do this and I run it you will
345:53 - see that I get an
345:54 - error select expression references
345:57 - column name which is neither grouped nor
346:02 - aggregated understanding this error is
346:05 - what I want to achieve now because it's
346:07 - very
346:09 - important so can you try to figure out
346:13 - on your own why this query is failing
346:17 - and what exactly this error message
346:22 - means so I'm going to go back to my
346:26 - spreadsheet and get a copy of my items
346:29 - table and as you can see I have copied
346:32 - the query that doesn't work over here so
346:34 - let us now uh go ahead and reproduce
346:38 - this
346:39 - query so I have to take the items table
346:42 - here it is and then I have to group by
346:44 - item type and as you can see I've
346:47 - already sorted by item type to
346:50 - facilitate our
346:51 - work and then for each item we want to
346:55 - select the item type so that would be
346:57 - armor and we want to select the average
347:00 - power so to find that I can run a
347:04 - spreadsheet function like this it's
347:07 - called
347:08 - average and get the power over
347:12 - here and then I am asked to get the name
347:18 - so if I take the name for armor and put
347:21 - it
347:22 - here this is what I have to
347:26 - add and here you can already see the
347:30 - problem that we are
347:33 - facing for this particular class armor
347:37 - there is a mismatch in the number of
347:41 - rows that each column is
347:45 - providing because as an effect of group
347:49 - by item type now there is only one row
347:53 - in which item type is
347:54 - armor and as an effect of applying
347:58 - average to power within the armor group
348:01 - now there is only one row of power
348:05 - corresponding to the armor group
348:08 - but then when it comes to
348:10 - name it's neither present in a group Pi
348:14 - nor is it present in an aggregate
348:17 - function and that means that in the case
348:20 - of name we still have four values four
348:25 - values instead of
348:27 - one and this
348:30 - mismatch is an issue SQL cannot accept
348:34 - it because SQL doesn't know how to
348:38 - combine columns which have different
348:40 - numbers of
348:43 - rows in a way it's like SQL is telling
348:46 - us look you've told me to group the data
348:50 - by item type and I did so I found all
348:55 - the rows that correspond to armor and
348:58 - then you told me to take the average of
349:00 - the power level for those rows and I
349:04 - did but then you asked me for name now
349:09 - the item type armor has four names in
349:13 - it what am I supposed to do with them
349:16 - how am I supposed to combine them how am
349:19 - I supposed to squish them into a single
349:21 - value you haven't explained how to do
349:24 - that so I cannot do
349:28 - it and this takes us to a fundamental
349:32 - rule of SQL something I like to call the
349:36 - law of
349:38 - grouping and the law of grouping is
349:41 - actually quite simple but
349:45 - essential it tells you what type of
349:48 - columns you can select after you've run
349:51 - a group
349:53 - pi and there are basically two types of
349:56 - columns that you can select after
349:59 - running a group
350:00 - bu one is grouping Fields so those those
350:06 - are the columns that appear after the
350:09 - group by Clause those are the columns
350:11 - you are using to group the data and two
350:15 - aggregations of other fields okay so
350:18 - those are fields that go inside a Max
350:21 - function a mean function a sum function
350:24 - a count function and so
350:26 - on now those are the only two types of
350:30 - columns that you can select if you try
350:33 - to select any other column you will get
350:36 - an
350:39 - error and the reason you will get an
350:41 - error is Illustrated here after a group
350:45 - Pi each value in the grouping Fields is
350:49 - repeated exactly once and then for that
350:53 - value the aggregation makes sure that
350:55 - there's only one corresponding value in
350:59 - the aggregated field in this case
351:01 - there's only one average
351:05 - power number within each item
351:10 - type however any other field if it's not
351:14 - a grouping field and you haven't run an
351:16 - aggregation on it you're going to get
351:19 - all of its values and then there's going
351:21 - to be a mismatch so the law of grouping
351:24 - is made to prevent this
351:28 - issue now if we go back to our SQL
351:32 - hopefully you understand now better why
351:35 - this error Isen happening and in fact
351:38 - this error message makes a lot more
351:40 - sense after you've heard about the law
351:42 - of grouping you are referencing a column
351:46 - name which is neither grouped nor
351:51 - aggregated so how could we change this
351:55 - code so that we can include the column
351:59 - name without triggering an
352:02 - error well we have two options either we
352:05 - turn it into a grouping field or we turn
352:09 - it into an
352:11 - aggregation so let's try turning it into
352:14 - an aggregation let's say for example
352:17 - that I said mean of
352:20 - name what do you expect would happen in
352:23 - that
352:25 - case so if I run this you will see that
352:29 - I have my grouping by item
352:32 - type I have the average power within
352:35 - each item type
352:37 - and then I have one name and so when you
352:40 - run mean on a sequence of uh text values
352:47 - what it does is that it gives you the
352:49 - first value in alphabetical order so we
352:51 - are in fact seeing the first name in
352:54 - alphabetical order within each item type
352:58 - so we've overcome the error but this
353:02 - field is actually not very useful we
353:05 - don't really care to see what's the
353:08 - first name in alphabetical order within
353:10 - each
353:12 - type but at least our aggregation is
353:15 - making sure that there's only one value
353:17 - of name for each item type and so the
353:20 - golden rule of grouping is respected and
353:23 - we don't get that error
353:26 - anymore the second alternative is to
353:29 - take name and add it as a grouping field
353:32 - which simply means putting it after item
353:36 - type type in here now what do you expect
353:39 - to happen if I run this
353:45 - query so these results as they show here
353:47 - are a bit misleading because there's
353:49 - actually the name column is hidden so I
353:52 - will also add it
353:53 - here and as you can see I can now refer
353:57 - the name column in select without an
354:00 - aggregation why because it is a grouping
354:03 - field okay and what do we see here in
354:06 - the
354:07 - results well we've seen what happens
354:10 - when you Group by multiple columns that
354:13 - the unique combinations of these columns
354:16 - end up subdividing the data so in fact
354:20 - our values for average power are not
354:24 - divided by item type anymore we don't
354:27 - have the average power for armor potion
354:30 - and weapon anymore we have the average
354:33 - power for an item that's type armor and
354:38 - it's called chain mail armor and that is
354:40 - in fact there's only one row that does
354:43 - that and has power 70 likewise we have
354:47 - the average power for uh any item called
354:50 - cloak of invisibility which is of item
354:52 - type armor and again there's only one
354:55 - example of that so we've overcome our
354:58 - error by adding name as a grouping field
355:01 - but we have lost the original group
355:03 - division by item type and we have
355:06 - subdivided the data to the point that it
355:09 - doesn't make sense
355:11 - anymore so as you surely have noticed by
355:15 - now we made the error Disappear by
355:18 - including name but we haven't actually
355:21 - achieved our original objective which
355:24 - was to show the name of each item the
355:28 - item type and then the average power
355:31 - within that item
355:34 - type well to be honest my original
355:37 - objective was to teach you to spot this
355:40 - error and understand the law of grouping
355:43 - but now you might rightfully ask how do
355:46 - I actually achieve
355:48 - this and the answer unfortunately is
355:51 - that you cannot achieve this with group
355:54 - Pi not in a direct simple way and this
355:57 - is a limitation of group Pi which is a
356:00 - very powerful feature but it doesn't
356:03 - satisfy all the requirements of
356:05 - aggregating data
356:07 - the good news however is that this can
356:09 - be easily achieved with another feature
356:12 - called window
356:14 - functions now window functions are the
356:16 - object of another section of this
356:19 - course so I'm not going to go into depth
356:22 - now but I will write the window function
356:25 - for you just to demonstrate that it can
356:27 - be done easily with that
356:31 - feature so I'm going to go down
356:34 - here and write a a new
356:37 - query I'm going to take the items
356:40 - table and I'm going to select the
356:45 - name and the item
356:48 - type and then I'm going to get the
356:52 - average of
356:54 - power and again I'm going to use back
356:57 - ticks so bigquery doesn't get confused
356:59 - with the function that has the same name
357:02 - and then I'm going to say take the
357:04 - average of power
357:08 - over
357:10 - Partition by item type so this is like
357:15 - saying average of power based on this
357:18 - item type and I will call this average
357:23 - Power by
357:25 - type and if I select this and run the
357:29 - query you will see that I get what I
357:32 - need I have a chain mail armor it's
357:35 - armor
357:36 - and the average power for an armor is
357:40 - 69.5 so this is how we can achieve the
357:42 - original objective unfortunately not
357:45 - with grouping but with window
357:48 - functions now I want to show you how you
357:51 - can filter on aggregated values after a
357:56 - group buy so what I have here is a basic
358:00 - Group by
358:01 - query go to the fantasy characters table
358:04 - group it by class and then show me the
358:07 - class and within each class the average
358:10 - of the experience for all the characters
358:14 - in that class and you can see the
358:16 - results here now what if I wanted to
358:20 - only keep those classes where the
358:23 - average experience is at least
358:26 - 7,000 how could I go and do
358:29 - that one Instinct you might have is to
358:33 - add a wear filter right for example Le I
358:36 - could say where average experience is
358:41 - greater than or equal to 7,000 and if I
358:45 - run this I get an error unrecognized
358:49 - name average
358:51 - experience the wear filter doesn't work
358:54 - here maybe it's a labeling problem what
358:58 - if I actually add the logic instead of
359:00 - the
359:02 - label so what if I say where average of
359:05 - experience is bigger or equal to
359:08 - 7,000 well an aggregate function is
359:12 - actually not allowed in the work Clause
359:14 - so this also doesn't work what's
359:16 - happening
359:18 - here now if we look at the order of SQL
359:21 - operations we can see that the where
359:24 - Clause runs right after sourcing the
359:28 - data and according to our rules over
359:31 - here an operation can only use data
359:34 - produced before it and doesn't know
359:38 - about data produced after
359:41 - it so the wear operation cannot have any
359:46 - way of knowing about aggregations which
359:50 - are computed later after it runs and
359:55 - after running the group bu and this is
359:58 - why it is not allowed to use
360:02 - aggregations inside the wear filter
360:06 - luckily SQL provides us with a having
360:12 - operation which works just like the wear
360:16 - filter except it works on
360:19 - aggregations and it works on
360:21 - aggregations because it happens after
360:23 - the group buy and after the
360:28 - aggregations so to summarize you can
360:32 - Source the table and then drop rows
360:35 - before grouping this is what the wear
360:37 - filter is for and then you can do your
360:40 - grouping and Compu your aggregations and
360:43 - after that you have another chance to
360:45 - drop rows based on a filter that runs on
360:49 - your
360:53 - aggregations so let us see how that
360:55 - works in practice now instead of saying
360:59 - where average experience actually let me
361:02 - just show you what we had before this is
361:06 - our actual result and we want to keep
361:09 - only those rows where average experience
361:11 - is at least
361:12 - 7,000 so after group Pi I will write
361:16 - having and then I will say average
361:19 - experience greater than or equal to
361:24 - 7,000 let me remove this part here run
361:27 - the query and you can see that we get
361:30 - what we need and you might be thinking
361:34 - well why do I have to to write down the
361:36 - function again can't I just use the
361:39 - label that I've
361:40 - assigned well let's try it and see if it
361:44 - still
361:46 - works and the answer is that yes this
361:50 - works in Big query however you should be
361:54 - aware that bigquery is an especially
361:57 - userfriendly and funto use
362:00 - product in many databases however this
362:04 - is actually not allowed
362:06 - in the sense that the database will not
362:09 - be kind enough to recognize your label
362:13 - in the having
362:14 - operation instead you will have to
362:18 - actually repeat the logic as I'm doing
362:21 - now and this is why I write it like this
362:25 - because I want you to be aware of this
362:30 - limitation another thing that you might
362:32 - not realize immediately is that you you
362:35 - can also filter by aggregated columns
362:40 - which you are not selecting so let's say
362:42 - that I wanted to group by class and get
362:45 - the average experience for each class
362:48 - but only keep classes with a high enough
362:51 - average level I am perfectly able to do
362:55 - that I just have to write having average
362:59 - level greater than or equal to
363:04 - 20 and after I run this you will see
363:08 - that instead of four values I actually
363:11 - get three values so I've lost one value
363:15 - and average level is not shown in the
363:17 - results but I can of course show it and
363:21 - you will realize that out of the values
363:23 - that have stayed they all respect this
363:25 - condition they all have at least 20 of
363:27 - average level so in having you are free
363:31 - to write filters on aggregated values
363:35 - regardless of the columns that you are
363:37 - selecting so to summarize once more you
363:41 - get the data that you need you drop rows
363:45 - that are not
363:46 - needed you can then Group by if you want
363:49 - subdivide the data and then compute
363:52 - aggregations within those
363:54 - groups if you've done that you have the
363:57 - option to now filter on the result of
364:00 - those
364:01 - aggregations and then finally you can
364:03 - pick which columns you want to see
364:06 - and then apply all the other operations
364:09 - that we have seen in the
364:11 - course we are now ready to learn about
364:14 - window function a very powerful tool in
364:18 - SQL now window functions allow us to do
364:22 - computations and aggregations on
364:25 - multiple rows in that sense they are
364:29 - similar to what we have seen with
364:32 - aggregations and group bu
364:36 - the fundamental difference between
364:38 - grouping and window function is that
364:42 - grouping is fundamentally altering the
364:46 - structure of the table right because if
364:48 - I go here and I take this items table
364:51 - and I group by item type right now I'm
364:54 - looking at uh about 20 rows right but if
364:58 - I were to group the resulting table
365:00 - would have one two three three rows only
365:05 - because there're only three types of
365:07 - items so that would significantly
365:10 - compress the structure of my
365:12 - table and in fact we have seen with the
365:16 - basic law of grouping that after you
365:19 - apply a group ey you have to work around
365:22 - this fundamental alteration in the
365:25 - structure of a table right because here
365:27 - you can see that the items table has 20
365:31 - rows but how many rows do you expect it
365:34 - to have after you Group by item type I
365:38 - would expect it to have three rows
365:40 - because there's only three types of
365:43 - items and so my table is being
365:46 - compressed my table is changing its
365:48 - structure and the basic law of grouping
365:50 - teaches you how to work with that it
365:52 - tells you that if you want a group by
365:54 - item type you can just select power as
365:57 - is because your table will have three
366:00 - rows but you have 20 values of power so
366:03 - you have to instead select an
366:04 - aggregation on power so that you can
366:07 - compress those values to a single value
366:09 - for each item type and if you want to
366:11 - select name you also cannot select name
366:14 - as is you also have to apply some sort
366:16 - of aggregation for example you could put
366:18 - the names into a list an array uh or so
366:22 - on but window functions are different
366:25 - window functions allow us to do
366:27 - aggregations allow us to work on
366:29 - multiple values without however altering
366:33 - the structure of the table without
366:35 - changing the number of rows of the
366:39 - table so let us see how this works in
366:42 - practice now imagine that I wanted to
366:45 - get the sum of all the power values for
366:50 - my items so what is the total power for
366:52 - all of my items so you should already be
366:56 - aware of how to do this in SQL to just
366:59 - get that sum right I can I can do this
367:03 - by getting my fantasy items table and
367:08 - then selecting the sum over the
367:14 - power so if I take this query and paste
367:18 - it in big
367:19 - query I will get exactly
367:26 - that and this now is a typical
367:29 - aggregation right the sum aggregation
367:32 - has taken 20 different values of power
367:35 - and has compressed them down to one
367:37 - value and it has done the same to my
367:40 - table it's taken 20 different rows to my
367:43 - table and it has squished them it has
367:45 - compressed them down to one row and this
367:49 - is how aggregations
367:51 - work as we've seen in the
367:54 - course but what if I wanted to show the
367:58 - total power without altering the
368:01 - structure of the table what if I wanted
368:03 - to show the total power on every
368:07 - Row in other words I can take the sum of
368:12 - all the values of power and this is the
368:15 - same number that we've seen in B
368:18 - query and I can paste it over
368:23 - here and hopefully I can now expand
368:27 - it and this is exactly what I meant what
368:30 - if I can take that number and put it on
368:32 - every
368:33 - row and why would I want to do this well
368:37 - there's several things that I can do
368:38 - with this setup right for example I
368:41 - could go here um for Phoenix Feather
368:44 - which is power 100 and I could say take
368:47 - this 100 and divide it by the total
368:51 - power in this
368:52 - row and then turn this into a percentage
368:57 - and now I have this 6.5 approximately
369:01 - percentage and thanks to this I can say
369:04 - Hey look
369:05 - um the phoenix feather covers about 6 or
369:10 - 7% of all the power that is in my items
369:14 - of all the power that is in my game and
369:17 - that might be a useful
369:20 - information a more mundane concern uh
369:24 - could be that this is uh your your
369:27 - budget so this is the stuff you're
369:28 - spending on and instead of power you
369:30 - have the the price of everything and
369:32 - then you get the total sum right which
369:34 - is maybe what you spent in a month and
369:36 - then you want to know going at the
369:38 - movies what percent of your budget it
369:41 - covered and so
369:43 - on now I will delete this value because
369:45 - we're not going to use it and let us see
369:48 - what we need to write to obtain this
369:50 - result in SQL so once again we go to the
369:56 - fantasy items table and I'm going to
370:00 - move it a bit down and then we select
370:02 - the sum power just just like
370:06 - before except that now I'm going to add
370:10 - this
370:12 - over open round bracket and close round
370:16 - bracket and this is enough to obtain
370:19 - this
370:20 - result well to be precise when I write
370:24 - this in B query I will want to see a few
370:28 - columns as well so I will want to see
370:32 - the name item tab Ty and
370:36 - power and here I will need a comma at
370:39 - the
370:41 - end as well as the sum power over and I
370:46 - will also want to give a label to this
370:49 - just like I have in the spreadsheet now
370:51 - this is the query that will reproduce
370:54 - What you see here in the
370:56 - spreadsheet so how this works is that
371:00 - the over keyword is signaling to SQL
371:03 - that you want to use a window function
371:06 - and this means that you will get an
371:08 - aggregation you will do a calculation
371:11 - but you're not going to alter the
371:12 - structure of the table you are simply
371:14 - going to take the value and put it in
371:17 - each row this is what the over keyword
371:19 - signals to
371:21 - SQL now because this is a window
371:24 - function we also need to define a window
371:28 - what exactly is a window a window is the
371:32 - part of the table that each row is is
371:35 - able to
371:36 - see now we will understand what this
371:39 - means much more in detail by the end of
371:42 - this lecture so don't worry about
371:44 - it but for now I want to show you that
371:49 - this is the place where we usually
371:51 - specify the window inside these brackets
371:54 - after the over but we have nothing here
371:57 - and what this means is that our window
371:59 - for each row is the entire table so
372:02 - that's pretty simple right each row sees
372:06 - the entire
372:08 - table so to understand how the window
372:12 - function is working we always have to
372:14 - think row by row because the results can
372:17 - always be different on different
372:21 - rows so let us go row by row and figure
372:25 - out how this window function is working
372:28 - so now we have the first row and what is
372:31 - the
372:32 - window in this case meaning what part of
372:36 - the table does does this row see well
372:40 - the answer is that this row sees all of
372:43 - the
372:44 - table given that it sees all of the
372:47 - table it has to do the sum of power and
372:50 - so it will take this thing compute a sum
372:53 - over it put it in the
372:56 - cell now that was the first row moving
373:00 - on to the second
373:02 - row now what's the window here what part
373:06 - of the table does this row see once
373:09 - again it sees all of the
373:12 - table given that it sees all of the
373:14 - table it takes power computes some over
373:17 - it gets the result and puts it in the
373:19 - cell now I hope you can see that the
373:23 - result has to be identical in every cell
373:26 - in every Row in other words because
373:29 - every row sees the same thing and every
373:31 - Row computes the same thing and this is
373:33 - why every Row in here gets the same
373:37 - value and this is probably the simplest
373:41 - possible use of a window function so let
373:45 - us now take this
373:47 - code and bring it to B query and make
373:52 - sure that it runs as
373:55 - intended and like I said in the lecture
373:57 - on grouping you will see that power is
374:00 - blue because bequer is getting confused
374:02 - with its functions so always be best
374:05 - practice to put it into back tis to be
374:08 - very explicit that you are referring to
374:10 - a column but basically what you see here
374:13 - is exactly what we have in our
374:16 - sheet and now of course we have this new
374:19 - field which shows me the total of power
374:22 - on every row and like I said we can use
374:25 - this for several purposes for example I
374:28 - can
374:29 - decide to show for each item what p
374:35 - percentage of total power it covers
374:39 - right that's what I did before in the
374:41 - sheet so to do this I can take the
374:46 - power and I can divide by this window
374:50 - expression which will give me the total
374:53 - power not sure what happened there but
374:57 - let me copy paste here and I can call
375:00 - this percent total power now this is
375:04 - actually just a division so if I want to
375:07 - see the percentage I will have to also
375:10 - multiply by 100 but we know how to do
375:16 - this and once I look at this we can see
375:20 - that when we have power 100 we have
375:22 - almost
375:23 - 6.5% of the total power so this is the
375:28 - same thing that we did before and this
375:30 - goes to show that you can use these
375:33 - fields for your calculations and like I
375:35 - said if this was your budget you could
375:37 - use this to calculate what percentage of
375:40 - your total budget is covered by each
375:43 - item it's a pretty handy thing to know
375:47 - now why do I have to take this uh to
375:50 - repeat uh all of this logic over here
375:53 - why can't I just say give me power
375:55 - divided by some
375:58 - power well as you know from other parts
376:01 - of the
376:03 - course the select part is not aware of
376:06 - these aliases it's not aware of these
376:09 - labels that we are providing so when I
376:11 - try to do this it won't recognize the
376:13 - label so unfortunately if I want to show
376:16 - both I have to repeat the
376:20 - logic and of course I'm not limited to
376:24 - just taking the sum right what I have
376:27 - here is an aggregation function just
376:30 - like the ones we've seen with simple
376:32 - aggregations and grouping in aggregation
376:36 - so instead of sum I could use something
376:39 - like
376:40 - average using the back TI over right I
376:45 - need to remember uh to add the over
376:48 - otherwise it won't work because it won't
376:50 - know it's a window function and I can
376:53 - give it a
376:54 - label and now for each row I will see
376:57 - the same value which is the average of
377:00 - power over the whole data set and you
377:04 - you can basically use any aggregation
377:07 - function that you need it will work all
377:10 - the
377:11 - same few more btics to put in here just
377:14 - to be
377:16 - precise but the result is what we
377:19 - expect now let us proceed with our
377:22 - Explorations so I would like now to see
377:25 - the total power for each row but now I'm
377:29 - not interested in the total power of the
377:32 - data set I'm interested of in the total
377:36 - Power by item type okay so if my item is
377:40 - an armor I want to see the total power
377:43 - of all armors if my have item is a
377:45 - potion I want to see the total power of
377:47 - all potions and so
377:50 - on because I want to compare items
377:53 - within their category I don't want to
377:55 - compare every item with every item so
377:58 - how can I achieve this in the
378:01 - spreadsheet well let us start with the
378:04 - first r row so I need to check what item
378:07 - type I have and conveniently I have
378:09 - sorted this so we can be
378:11 - quicker now we have an armor so I want
378:15 - to see the total power for armor so what
378:18 - I can do is to get the sum function and
378:22 - be careful to select only rows where the
378:25 - item type is
378:27 - armor and this is what I
378:31 - get and then the next step would be to
378:34 - Simply copy this
378:36 - value and then fill in all of the rows
378:40 - which are armor because for all of the
378:43 - rows but again you have to be careful
378:46 - because the spreadsheet wants to
378:48 - complete the pattern but what I want is
378:50 - the exact same number and then all of
378:52 - the rows that have item type armor will
378:55 - have this value because I'm looking
378:57 - within the item
378:59 - type now I will do it for potion so here
379:03 - I need to get the sum of power for all
379:06 - items that are
379:07 - potions
379:09 - 239 and then make sure to co copy the
379:13 - exact same value and to extend it to all
379:20 - potions and next we have weapons so sum
379:24 - of all power by
379:27 - weapon which is here then copy it and
379:31 - copy it and then let's see if it tries
379:34 - to complete the pattern it does so I'm
379:37 - just going to go ahead and paste it and
379:41 - now make this a bit nicer and now I have
379:45 - what I wanted to get each row is showing
379:49 - the total power within the items that
379:52 - are the same as the one that we see in
379:55 - the
379:56 - row now how can I write this in SQL so
380:00 - let me go ahead and write it here now
380:03 - two parts of this query will be the same
380:04 - same because we want to get the items
380:07 - table and see these columns but we need
380:09 - to change how we write the window
380:11 - function so once again I want to get the
380:14 - sum of
380:17 - power and I will need now to define a
380:21 - specific
380:23 - window now remember the window defines
380:26 - what each row sees so what do I want
380:29 - each row to see when it takes the sum of
380:32 - power for example what do I want the
380:34 - first row to see when it takes the sum
380:36 - of power I wanted to see only rows which
380:41 - have the item type
380:44 - armor or in other words all the rows
380:47 - with the same item
380:49 - type and I can achieve this in the
380:51 - window function by
380:54 - writing Partition by item type by adding
381:00 - a partition
381:05 - defining the window as a partition by
381:08 - item type means that each row will look
381:13 - at its item type and then we'll
381:15 - partition the table so that it only sees
381:18 - rows which have the same item type so
381:22 - this row over here will see only these
381:26 - four rows and then you will take the sum
381:28 - of power and then you will put it in the
381:30 - cell and for this uh the second third
381:34 - third and fourth row the result will be
381:37 - the same because they will each see this
381:40 - part of the table when we come to potion
381:43 - so this row over here will say hey what
381:46 - is my item type it's potion okay then I
381:49 - I will only look at rows that have item
381:52 - type potion and so this will be the
381:54 - window for these four rows and then in
381:58 - those rows I'm going to take power and
382:00 - I'm going to
382:02 - Summit and finally when we come to to
382:04 - these rows over here so starting with
382:07 - this row it will look at its item type
382:10 - and say okay I have item type uh weapon
382:14 - let me look at all the rows that share
382:16 - the same item type and so each window
382:19 - will look like
382:22 - this so let me color it properly its
382:26 - window will look like this and then it
382:28 - will take the sum of these values of
382:30 - power that fit in the window and put it
382:33 - in the cell second cell sees the same
382:35 - window sums over these values of power
382:38 - puts it in the cell and this is how we
382:41 - get the required result this is how we
382:44 - use partitioning in window
382:47 - functions so let's go now to Big query
382:50 - and make sure that this actually
382:54 - works and when I run this I didn't put a
382:57 - label but you can see that I'm basically
383:01 - getting the same result when I have a
383:03 - weapon I see a certain value when I have
383:07 - a potion I see uh another one and when I
383:10 - have an armor I see the third value so
383:14 - now for each item I am seeing the total
383:18 - power not over the whole table but
383:21 - within the item
383:24 - type now next task find the cumulative
383:29 - sum of
383:31 - power which is this column over here
383:34 - what is a cumulative
383:37 - sum it's the sum of the powers of this
383:43 - item plus all of the items that are less
383:47 - powerful so to do this in the
383:50 - spreadsheet I will first want to reorder
383:54 - my data because I want to see it simply
383:58 - in order of power so I will actually
384:01 - take this whole range
384:04 - and I will go to data sort
384:08 - range Advance options and I will say
384:11 - that the data has a header row so that I
384:14 - can see the names of the columns and
384:16 - then I will order by power
384:22 - ascending so as you can see my records
384:25 - have now been sorted in direction of
384:29 - ascending
384:31 - power now how do I compute the
384:34 - cumulative sum of
384:36 - power in the first row all we have is 30
384:40 - so the sum will be 30 in the second row
384:44 - I have 40 in this row plus 30 before so
384:50 - E I will have 70 when it comes here I
384:54 - have 50 in this row and then the sum up
384:57 - to now was actually 70 which I can see
385:00 - by looking at these two cells or I can
385:03 - see more simply by looking at the last
385:05 - cell so 50 + 70 will be
385:08 - 820 and proceeding like this I could
385:11 - compute the cumulative power over the
385:13 - whole
385:14 - column now for your reference I have
385:18 - figured out the correct Google Sheets
385:21 - formula that will allow you to compute
385:24 - the cumulative sum of power for our
385:27 - example and I went ahead and computed it
385:30 - so that we have it for all our data now
385:33 - this is is the formula right here and
385:36 - I'm not going to go in depth into it
385:38 - because this is not a course on on
385:40 - spreadsheets but I will show you the
385:42 - formula just in case you're
385:44 - curious so the sum IF function will take
385:49 - the sum over a range only but it will
385:53 - only consider values that satisfy a
385:55 - certain logical condition so the first
385:59 - argument is the range that we want to
386:00 - sum over and this is the power and the
386:04 - Criterion so what needs to be true for a
386:07 - value to be um to be considered is that
386:11 - this value is lesser than or equal to
386:16 - the level of power in this row so what
386:19 - this formula is saying is take the level
386:21 - of power in this row and then take all
386:25 - the values of power which are lesser or
386:27 - equal and then sum them up this is
386:29 - exactly what our window function does
386:32 - and so our formula reproduces this now
386:36 - if you go and look what's the way to do
386:38 - a cumulative sum in Google Sheets or
386:41 - what's the way to do a running total
386:43 - there are other Solutions but they do
386:46 - come with some um pitfalls they do come
386:49 - with some Corner cases so this is a
386:51 - Formula that's actually reproducing the
386:54 - behavior of
386:56 - SQL now let us go back to actually SQL
386:59 - and see how we would write
387:01 - this so I'm going to take
387:04 - the fantasy items table and I'm still
387:07 - going to select the columns and now I
387:09 - have to write my window function now the
387:12 - aggregation is just the same so take the
387:15 - sum of power and now I have to Define my
387:21 - window now my window is not defined Now
387:24 - by a partition but it is defined by an
387:29 - ordering order by power
387:35 - and when I say order by power in a
387:37 - window function what's implicit in this
387:41 - is the keyword ask for ascending so this
387:45 - means that the window will order power
387:48 - from the smallest to the biggest and I
387:51 - can choose to write this keyword or not
387:53 - because just like in order by in SQL
387:57 - when you don't specify it the default
387:59 - value is ascending from smallest to
388:01 - biggest so how does this window work
388:05 - work let's start with the first row and
388:08 - let's say we need to fill in this value
388:10 - so I'm going to look at my power level
388:12 - it is 30 and then the window says that I
388:16 - can only see rows where the power level
388:21 - is equal or
388:24 - smaller and what are the rows where the
388:27 - power level is equal or smaller to 30
388:30 - there're these rows over here so
388:32 - effectively this this is the only part
388:36 - of the table that this window sees on
388:38 - the first row and then take the sum over
388:41 - power so sum over 30 is
388:43 - 30 move on to the second row the power
388:47 - level is 40 the window says I only see
388:50 - rows where the power level is smaller uh
388:53 - or equal and this includes these two
388:56 - rows over here now take the sum of power
389:00 - over here you get 70 put it in the cell
389:04 - third row I have power level 50 I'm only
389:07 - seeing these rows so take the sum of
389:10 - power over this it's 120 put it in the
389:13 - cell and I can continue like this until
389:17 - I get to the highest value in my data
389:20 - set it's 100 never mind that is not the
389:23 - last row because both of the last two
389:26 - rows they have the highest value and
389:29 - when you look at this um when you come
389:31 - to this row and you look at 100 and and
389:34 - you say what's the window what rows can
389:36 - I see I can see all rows where power is
389:40 - 100 or less and that basically includes
389:44 - all of the table right it includes all
389:46 - of the table so when you take the sum of
389:51 - power you will get the total sum and in
389:55 - fact you can see that in this case the
389:57 - cumulative power is equal to the total
390:00 - power that we computed before just as we
390:03 - would
390:04 - expect so this is easy to see here
390:07 - because we have ordered um our data
390:11 - conveniently but it works in any case
390:14 - and so what the order by does in a
390:17 - window function is that it makes sure
390:21 - that each row only sees rows which come
390:26 - before it given your
390:29 - ordering so if I want to order from the
390:32 - smallest power to the biggest power each
390:34 - row will only see rows that come before
390:37 - it in this ordering so they have the
390:39 - same level of power or lower but they
390:42 - don't have a higher level of
390:45 - power so let us now take it to Big query
390:50 - and make sure it works as intended and I
390:54 - will add an ordering by
390:57 - power and here I will see the same thing
391:01 - that I've shown you in the spreadsheet
391:05 - I notice now that some numbers are
391:07 - different that these two items have 90
391:10 - instead of 100 but never mind that the
391:13 - logic is the same and the numbers make
391:17 - sense now I'm also able to change the
391:21 - direction of the ordering right so let's
391:23 - say that I take this
391:26 - field and copy it just the same except
391:30 - that instead of ordering by power
391:32 - ascending I order by power descending so
391:36 - what do you expect to see in this case
391:39 - let's take a
391:43 - look now what I see here is that each
391:48 - item is going to look at its level of
391:52 - power and then it's only going to
391:54 - consider items that are just as powerful
391:58 - or more powerful right so it's the exact
392:01 - same logic but it's reversed so when you
392:04 - look at the weakest item potion it has
392:09 - 30 and so it is looking at all the items
392:12 - because there's no weaker
392:14 - item and so it finds the total level of
392:17 - power in our data
392:19 - set but if you go to the strongest item
392:22 - like Excalibur it has a power level of
392:26 - 100 and there's only two items in the
392:29 - whole data set that have this power
392:31 - level itself and the phoenix feather
392:34 - so if you sum the power over this you
392:37 - get 200 so you can see it's the exact
392:40 - same logic but now each row only sees
392:43 - items that have the same level of power
392:45 - or higher so when you order inside a
392:49 - window function you can decide the
392:52 - direction of this ordering by using
392:55 - descending or ascending or if you are a
392:58 - lazy programmer you can omit the um
393:02 - ascending key word and it will work just
393:05 - the same because that's the
393:09 - default and finally we want to compute
393:13 - the cumulative sum of Power by type and
393:16 - you might notice that it is in a way the
393:19 - combination of these two uh requirements
393:22 - so let us see how to do that now the
393:25 - first thing I want to do is to sort our
393:28 - data in order to help us so I'm going to
393:31 - get this whole thing
393:34 - and I'm going to say sort range I'm
393:37 - going to need the advanced options I
393:39 - have a heading row and so first of all I
393:43 - want to order by type and then within
393:46 - each type I want to order by
393:49 - power and this is our data
393:52 - now now for
393:54 - each item I want to show the cumulative
393:58 - sum of
394:00 - power just like I did here except that
394:03 - now I only want to do it within the same
394:06 - item
394:07 - type so if we look at Armor it's already
394:11 - sorted right so I have power 40 and this
394:15 - is the smallest one so I will just put
394:17 - 40 over here next I have uh this item
394:22 - with power 70 it's still armor has power
394:25 - 70 and so I'm going to look at these two
394:28 - values and sum them
394:31 - up now I have uh 7 8 so I will take this
394:36 - plus
394:37 - 78 which is the sum of these three
394:41 - values and finally I have um 90 which
394:47 - is the sum of those
394:50 - values and now I'm done with armor right
394:54 - I'm beginning with a new item type so I
394:57 - have to start all over again I'm looking
394:59 - at potions now so we start with 30 that
395:02 - is the smallest value then we move to uh
395:05 - 50 so this is now seeing 30 and
395:09 - 50 uh which is
395:11 - 80 add 60 to 80 that is
395:16 - 140 and finally we want to add we want
395:21 - to add
395:23 - 99 plus 140 which is another way of
395:27 - saying that we want to add these values
395:30 - all the values for potion
395:34 - so this is what we want cumulative sum
395:39 - of power within item type so we do it
395:43 - within the item type and then when we
395:45 - find a new type we start over so to
395:49 - calculate it for weapon I could copy my
395:52 - function from
395:53 - here paste it in
395:56 - weapon and then I would need to modify
395:58 - it right I would need the range to only
396:01 - include weapon so that's from
396:05 - C10 so go here C10 is the first
396:11 - one and the value that I want to look at
396:18 - here would have to be C10 as well
396:23 - because I want to start by looking at
396:27 - the power level for the for
396:32 - weapon and for some reason it's
396:36 - purple however it should be
396:39 - correct it should always be the sum of
396:43 - the previous value so we start with 65
396:46 - then we have 65 + 75 66 75 65 and so on
396:52 - so this is our result it's cumulative
396:56 - power within the item
396:59 - type and to write this in SQL
397:04 - I will take my previous query over here
397:07 - and now when we Define the window we can
397:10 - simply combine what we've done before we
397:13 - can combine the partition buy with the
397:16 - order bu and you need to write them in
397:19 - the following order first the partition
397:23 - and then the order so I will Partition
397:26 - by item
397:30 - type and I will order by power
397:34 - ascending and this will achieve the
397:37 - required
397:39 - result so for each row in this field the
397:43 - window will be defined as follows first
397:48 - Partition by item type right so first of
397:51 - all you can only see rows which have the
397:54 - same item type as you
397:56 - have but then within this partition you
397:59 - can you have to keep only rows where the
398:02 - power is equal or smaller than what you
398:05 - have so in the case of the first item
398:07 - you only get this
398:10 - row likewise in the case of the first
398:13 - potion item you only get this row if you
398:16 - look at the second armor item again it
398:19 - looks it partitions right so it looks at
398:22 - all the items which have armor but then
398:25 - it has to discard those that have a
398:26 - bigger power than
398:28 - itself so it will be looking at these
398:31 - two rows and if for example example we
398:33 - look at the last row over here so this
398:37 - row will say oh okay I'm a weapon so I
398:40 - can only see those that are weapon and
398:43 - then I can only see those that have a
398:45 - level of power that's equal or smaller
398:47 - than mine and that checks out those are
398:50 - all the rows and in fact the sum over
398:54 - here is equal to the sum of Power by
398:57 - type which is what we would
399:00 - expect so once again let us verify that
399:03 - this works in Big
399:07 - query and I will actually want to order
399:11 - by item type and power just so I have
399:16 - the same ordering as in my
399:18 - sheet and I should be able to see that
399:21 - within armor you have this like
399:24 - growing uh cumulative
399:27 - sum and then once the item changes it
399:31 - starts all over right it starts again at
399:33 - the value it grows it grows it
399:35 - accumulates and then we're done with
399:38 - potions and then we have weapons and
399:40 - then again it starts and then it grows
399:43 - and it goes all the way to include the
399:46 - total sum of all powers in the weapon
399:48 - item
399:51 - type so here's a summary of all the
399:54 - variants of Windows that we've seen we
399:58 - have seen four variants now in all of
400:01 - those for clarity we've kept the
400:03 - aggregation identical right we are doing
400:05 - some over the power field but of course
400:08 - you know that you can use any aggregate
400:10 - function here on any column which is
400:13 - compatible with that aggregate function
400:16 - and then we have defined four different
400:18 - Windows the first one is the simplest
400:21 - one there's actually nothing in the
400:22 - definition we just say over and this
400:25 - means that it will just look at all the
400:27 - table so every row will see the whole
400:29 - table and so every row will show you the
400:33 - total of power for the whole table
400:36 - simple as
400:39 - that the second window is introducing a
400:42 - partition by item type and what this
400:46 - means in practice is that each row will
400:50 - uh look at its own item type and then
400:53 - only consider rows which share the same
400:56 - exact item
400:57 - type and So within those rows it will
401:00 - calculate the sum of power
401:05 - third window we have an ordering
401:09 - field so what this means is that each
401:14 - row is going to look at its level of
401:17 - power because we are ordering by
401:20 - power and then it's going to only see
401:23 - rows where the power level is equal or
401:27 - smaller and the reason why we're looking
401:30 - in this direction is that when we order
401:33 - by power is implicitly uh understood
401:37 - that we want to order by power
401:39 - ascending If instead we ordered by power
401:42 - descending it would be the same just in
401:44 - the opposite direction each row would
401:47 - would look at its level of power and
401:49 - then only consider rows where power is
401:51 - equal or
401:56 - bigger and then finally we have a
402:00 - combination of these two right a we have
402:03 - a window where we use both a partition
402:07 - and an
402:08 - order and so what this means is that uh
402:12 - each row is going to look at its item
402:15 - type and discard all of the rows which
402:18 - don't have the same item type but then
402:21 - within the rows that remain it's going
402:23 - to apply that ordering it's going to
402:24 - only consider rows which have the same
402:27 - level of power or lesser so it's simply
402:30 - a combination of these two conditions
402:34 - and this is the gist of how window
402:37 - functions
402:39 - work first thing to remember window
402:42 - function provide aggregation but they
402:45 - don't change the structure of the table
402:48 - they just insert a specific value at
402:52 - each row but after applying a window
402:55 - function the number of rows in your
402:58 - table is the
403:01 - same second thing thing to remember is
403:04 - that in the window definition you get to
403:08 - Define what each row is able to see when
403:13 - Computing the
403:14 - aggregation so when you are thinking
403:17 - about window function you should be
403:20 - asking yourself what part of the table
403:23 - does each row see what's the perspective
403:26 - that each row
403:28 - has and there are two dimensions on
403:31 - which you can work in order to Define
403:34 - these
403:35 - windows one is the partition Dimension
403:38 - and the other is the ordering
403:40 - Dimension the partition Dimension Cuts
403:43 - up the table based on the value of a
403:47 - column so you will only keep rows that
403:49 - have the same
403:50 - value the order
403:53 - Dimension Cuts up the table based on the
403:56 - ordering of a field and then depending
403:59 - on ascending or descending depending on
404:02 - the direction that you choose you can
404:04 - you can look at rows that are after you
404:07 - in the ordering or you can look at rows
404:10 - that are before you in the
404:13 - ordering and you can pick either of
404:17 - these right either partitioning or
404:19 - ordering or you can combine them and by
404:21 - using this you can Define all of the
404:23 - windows that you might need to get your
404:27 - data now as a quick extension of this I
404:31 - want to show you that you're not limited
404:33 - to defining windows on single fields on
404:37 - single columns you can list as many
404:39 - columns as you want so in this example
404:43 - I'm going to the fantasy characters
404:45 - table I'm getting a few columns and then
404:47 - I'm defining an
404:49 - aggregation uh on a window function so
404:52 - I'm taking the level uh field and I'm
404:54 - summing it up and then I'm partitioning
404:57 - by two Fields actually by Guild and is
405:01 - alive so what do you expect to happen if
405:04 - I do
405:05 - this this is actually the exact same
405:08 - logic as grouping by multiple fields
405:11 - which we've seen in the group ey now the
405:15 - data is not going to be divided by Guild
405:18 - and is not going to be divided by
405:20 - whether the character is alive or not
405:23 - but by the all the mutual combinations
405:25 - between these fields okay so um merkwood
405:30 - and true is one combin ation and so the
405:34 - people in here are going to fit together
405:37 - right so in fact we have two characters
405:39 - here 22 and 26 and their sum is 48 so
405:43 - you can see here that they both get 48
405:46 - for sum of level and likewise when you
405:49 - look at Sher folk true these three they
405:52 - all end up in the same group and so they
405:55 - all share the same sum of level which is
405:58 - 35 but sh Fulk fals this is another
406:01 - group and they're actually alone right
406:04 - it's 12 and then the sum is 12 so again
406:09 - when you Partition by multiple
406:11 - Fields the data is divided in groups
406:13 - that are obtained by all the
406:16 - combinations between the values that
406:20 - these fields can have and if you
406:22 - experiment a bit by yourself you should
406:25 - have an easier time to convince yourself
406:28 - of
406:29 - this likewise the same idea applies to
406:33 - the
406:33 - order uh part of a window we have until
406:38 - now for Simplicity ordered by one field
406:40 - to be honest most times you will only
406:42 - need to order by one field but sometimes
406:45 - you might want to order by different
406:46 - fields so in this example you can see
406:49 - that we are defining our ordering based
406:53 - on two Fields power and then weight and
406:56 - then based on that ordering we calculate
406:59 - the sum of power and this is again a
407:01 - case of cumulative
407:03 - sum however now the ordering is
407:05 - different and you will realize this if
407:08 - we go to the most powerful items in our
407:11 - data these last two which are both at
407:15 - 100 so if you remember when we were
407:18 - ordering by power alone these two uh
407:21 - Fields had the same value in this um
407:25 - window function because when you order
407:28 - just by power they are actually the same
407:30 - they both have 100 but because now we're
407:33 - ordering by weight and again we're
407:35 - ordering by weight ascending so from the
407:38 - smallest weight to the biggest weight
407:40 - now the phoenix feather comes
407:42 - first because although it has the same
407:46 - power as Excalibur the Phoenix weather
407:48 - is
407:50 - lighter and because it comes first it
407:52 - has a different value for this
407:56 - aggregation and of course we have the
407:59 - power to to say ascending or descending
408:03 - on each of the fields by which we order
408:06 - so if I wanted to reverse this I could
408:08 - simply write descending after the weight
408:12 - and be careful that in this case
408:15 - descending is only referring to weight
408:18 - it's not referring to power so this is
408:20 - just as if I've wrote this right so the
408:24 - this one can be omitted um because it's
408:27 - ascending by default but I would write
408:30 - both to be clear and now if I run this
408:32 - you will see that our result is reversed
408:36 - right Excalibur comes first because we
408:38 - have weight descending so it's heavier
408:40 - and then last we have the phoenix
408:43 - feather which is
408:45 - lighter and
408:47 - again understanding this theoretically
408:49 - is one thing but I do encourage you to
408:51 - experiment with this with your data with
408:54 - exercises and then you will um you will
408:58 - be able to internalize
409:00 - it and now we are back to our schema for
409:04 - The Logical order of SQL
409:07 - operations and it is finally complete
409:10 - again because we've seen all of the
409:13 - components that we can use to assemble
409:15 - our SQL query and now the question is
409:18 - where do window functions fit into
409:22 - this well as you can see uh we have
409:25 - placed them right here so what happens
409:28 - is that again you get your data and then
409:31 - the we filter runs dropping rows which
409:33 - you don't need and then you have a
409:35 - choice whether to do a group by right
409:39 - now if you do a group by you're going to
409:42 - change the structure of your table it's
409:44 - not going to have the same number of
409:46 - rows it's going to have a number of rows
409:49 - that depends of the unique values of
409:53 - your grouping field or the unique
409:56 - combinations of values of your Fields if
409:59 - you have used more than one
410:02 - if you group you will probably want to
410:05 - compute some aggregations and then you
410:08 - may want to filter on those aggregations
410:10 - meaning dropping rows uh based on the
410:13 - values of those aggregations and here is
410:16 - where window functions come into play it
410:19 - is on this result that window functions
410:22 - work so if you haven't done a group bu
410:26 - then window functions will work on your
410:28 - data after the wear filter runs if you
410:31 - have done a group buy we window
410:32 - functions will work on the result of
410:36 - your
410:38 - aggregation and then after applying the
410:40 - window function you can select which
410:42 - columns you want to show give them uh
410:46 - labels and then all the other parts run
410:49 - right so you can choose to drop
410:51 - duplicates from your result meaning
410:54 - duplicate rows rows which have the same
410:56 - value on every column you can stack
410:59 - together different tables right you can
411:01 - put them on top of each other and then
411:03 - finally when you have your result you
411:05 - can apply some ordering and also you can
411:08 - cut the result you can limit it so you
411:10 - only show a few uh
411:14 - rows and this is where window functions
411:18 - fit into the big scheme of things and
411:21 - there are some other implications of
411:23 - this ordering one interesting one is
411:26 - that if you have computed aggregations
411:29 - such as the sum of a value Within within
411:33 - a um a class um you can actually use
411:37 - those aggregations in the window
411:39 - function so you can sort of do an
411:41 - aggregation of an aggregation but this
411:44 - is uh in my opinion an advanced topic
411:47 - and it doesn't fit into this um
411:51 - fundamentals course it may fit uh
411:53 - someday in a later more advanced
411:56 - course I want to show you another type
411:59 - of window functions which are very
412:02 - commonly used and very useful in SQL
412:05 - challenges and SQL interviews and these
412:08 - are numbering
412:11 - functions numbering functions are
412:14 - functions that we use in order to number
412:16 - the rows in our data according to our
412:21 - needs and there are several numbering
412:24 - functions but the three most important
412:26 - ones are without any doubt row number
412:29 - dense Rank and rank so let's let's see
412:32 - how they work in
412:34 - practice now what I have here is a part
412:38 - of my uh inventory table I'm basically
412:41 - showing you the item ID and the value of
412:44 - each number and conveniently I have
412:46 - ordered our
412:48 - rows uh by value ascending okay and now
412:53 - we are going to number our rows
412:56 - according to the value by using these
412:58 - window
412:59 - functions now I've already written the
413:02 - query that I want to reproduce so I'm
413:06 - going to the fantasy inventory table and
413:09 - then I'm selecting the item ID and the
413:11 - item value as you see here and then I'm
413:13 - using uh three window
413:17 - functions so the syntax is the same as
413:20 - what we've seen uh in the previous
413:23 - exercise except that now I'm not using
413:26 - an aggregation function over a field
413:29 - like I did before when I was doing a sum
413:32 - of power and so on but I'm using another
413:35 - type of function this is a numbering
413:38 - function okay so this functions over
413:41 - here they don't actually take a
413:43 - parameter as you can see that there's
413:45 - nothing between these round brackets
413:47 - because I don't need to provide it an
413:49 - argument or a parameter all I need to do
413:51 - is to call the function but what really
413:55 - uh what's really important here is to
413:57 - define the correct window and as you can
414:00 - see in the three examples here the
414:03 - windows are all the same I am simply
414:07 - ordering my rows by value
414:12 - ascending which means that when it's
414:15 - going to compute the window function
414:17 - every row will look at its own value and
414:20 - then say okay I'm only going to see rows
414:24 - where the value is the same or smaller
414:28 - I'm not going to be able to visualize
414:31 - rows where the value is bigger than mine
414:34 - and this is what the window
414:36 - does so the first row over here will'll
414:39 - only see value of 30 the second row will
414:43 - see this the third row will see these
414:46 - and so on up until the last row which
414:49 - will see itself and all the other rows
414:51 - as
414:53 - well now let us start with row number so
414:56 - row number is going to use this ordering
414:59 - over here in order to number my rows and
415:02 - it's as simple as saying putting one in
415:05 - the first row two in the second one 3
415:08 - four and so on so if I extend this
415:11 - pattern I'm going to get a number for
415:14 - every row and that's it that's all that
415:17 - row number does it assigns a unique
415:20 - integer number to every row based on the
415:23 - ordering that's defined by the window
415:28 - function and you might think oh big deal
415:31 - why do I need this don't I already have
415:34 - like row numbers over here in the
415:37 - spreadsheet well in Pro SQ problems you
415:41 - often need to order things based on
415:44 - different values and um row number
415:48 - allows you to do this you can also have
415:50 - many different orderings coexisting in
415:52 - the same table based on different
415:54 - conditions and that can come in handy as
415:57 - you will discover if you do SQL
416:00 - problems now let's move on to ranking so
416:03 - first of all we have dense rank okay and
416:07 - ranking is another way of counting but
416:10 - is slightly
416:13 - different sometimes you just want to
416:15 - count things you know sometimes uh like
416:17 - we did here in row number like I don't
416:20 - know you are a dog sitter and you're
416:22 - given 20 dogs and you getting confused
416:26 - between all their their names and then
416:28 - you assign a unique number to every dog
416:30 - so that you can identify them uh and you
416:32 - can sort them by I don't know by age or
416:35 - by how much you're getting paid to docit
416:38 - them sometimes on the other hand you
416:41 - want to rank things like when choosing
416:44 - which product to buy or expressing the
416:47 - results of a race right if and the
416:51 - difference between ranking and Counting
416:53 - can be seen when you have the same value
416:56 - right so when you want to Simply number
417:00 - like we did here when you want to Simply
417:01 - assign assign a different number to each
417:04 - element and two things have the same
417:07 - value then you don't really care right
417:10 - you need to sort of arbitrarily decide
417:13 - that okay one of them will be a number
417:14 - two and one of them will be number three
417:18 - but you cannot do the same for ranking
417:20 - if two students in a classroom get the
417:22 - best score you can't just randomly
417:24 - choose that one of them is number one
417:26 - and the other is number two they have to
417:29 - both be number one right and if two
417:31 - people finish a race at at the same time
417:33 - and is the best time you can't say that
417:35 - one uh won the race and the other didn't
417:38 - that because one is number one the other
417:40 - is arbitrarily number two they both have
417:42 - to be number one right they have to
417:43 - share that Rank and this is where
417:45 - ranking differs so let's go in here and
417:48 - apply our rank now we are ordering by
417:51 - value ascending which means that the
417:53 - smallest value will have rank number
417:58 - one and so 30 has rank number one
418:03 - now we go to the second row and again
418:05 - remember window functions that you
418:07 - always have to think row by row you have
418:09 - to think what each row sees and what
418:11 - each row decides so again the row is
418:14 - going to order by uh value so it's only
418:17 - going to see these values over here and
418:20 - it has to decide its rank so this row
418:24 - says uh oh I'm not actually number one
418:27 - because there is a value which is
418:29 - smaller than me so that means I have to
418:31 - be number number
418:33 - two and then we get to the third row and
418:37 - this row is uh seeing all the values
418:41 - that come before it right they're equal
418:43 - or or or smaller and now it's saying oh
418:47 - I'm not number one because there's
418:49 - something smaller but then uh the value
418:52 - 50 which uh this guy has uh is rank two
418:56 - and I have the same value number 50 we
418:59 - arrived in the same spot so I must have
419:01 - the same rank okay and this is the
419:04 - difference between row number and rank
419:07 - that identical values get the same rank
419:09 - but they don't get the same row
419:12 - number and now we come to this row which
419:16 - is 60 so it's going to look back and
419:19 - it's going to say oh from what I see 30
419:22 - is the smallest one so it has a rank of
419:24 - one and then you have 50 and 50 they
419:27 - both share a rank of two but I am bigger
419:30 - so I need a new rank and so what am I
419:33 - going to pick now as a new rank well I'm
419:36 - going to pick three because it's the
419:38 - next uh number in the
419:41 - sequence then the next one is going to
419:44 - pick four the next one is going to pick
419:47 - five and then we have six and then it
419:50 - proceeds in the following way so I'll do
419:53 - it quickly now so 7
419:56 - 8
419:58 - 9 10 11 and again careful here we're
420:03 - sharing the same value so they are both
420:06 - 11 next we can proceed to 12
420:10 - 13 again the same value right so they
420:13 - have to share the 13th spot 14
420:19 - so 14 for 1700 and then 14 again and
420:24 - then 15 and then 16 and this is what we
420:28 - expect to see when we compute the dense
420:31 - rank
420:33 - and finally we come to rank now rank is
420:37 - very similar to dense rank but there is
420:39 - one important difference so let's do
420:42 - this
420:43 - again smallest value has rank number one
420:48 - like before and then we have 50 which
420:50 - has rank number two and then 50 is once
420:54 - more sharing rank number
420:57 - two and now we move from 50 to 60 so we
421:00 - need a new rank but instead of three we
421:03 - put four over here why do we put four
421:07 - because the previous rank covered uh two
421:12 - rows and it sort of at the three it sort
421:15 - of expanded to eight the three So based
421:18 - on the rules of Simply rank we have to
421:21 - lose the three and put four over
421:25 - here so this is just another way of
421:28 - managing
421:29 - ranking and you will notice that it
421:32 - conveys another piece of information
421:35 - compared to dense rank because not only
421:39 - I see that um this row over here has a
421:43 - different rank than the previous row but
421:46 - I can only I can also see how many
421:50 - members were covered by the previous uh
421:53 - ranks I can see that in the previous
421:56 - ranks uh they must have involved three
422:00 - members because I'm at four already and
422:03 - this piece of information was not
422:05 - available for dence rank so I will
422:09 - continue over here and so I have a new
422:12 - value which is uh rank five and then I
422:15 - have rank six rank seven rank 8 rank
422:21 - n Rank 10 rank
422:27 - 11 now I have rank 12 and again I have
422:31 - to share the rank 12 because two
422:33 - identical values but now because 12 has
422:37 - eaten up two spots I can't use the 13
422:40 - anymore the second 12 has like eaten the
422:43 - 13 and so I need to jump straight to
422:45 - 14 15 15 again and now I have to jump to
422:51 - 17 because 15 had two spots 17
422:55 - again and now I have to jump to 19 and
422:59 - then finally I have 20
423:02 - so you can see that the final number uh
423:07 - is 20 for rank just as with row number
423:11 - because it's not only differentiating
423:14 - between ranks but it's also counting for
423:16 - me how many elements have come before me
423:19 - how many rows are contained in the
423:22 - previous ranks I can tell that there's
423:24 - 19 rows in the previous ranks uh because
423:27 - of how rank Works whereas with 10 rank
423:31 - we end ended up using only 16 uh ended
423:35 - up being only up to 16 so we sort of
423:38 - lost information on how many records we
423:41 - have and this might be one of the
423:43 - reasons why by default you have this
423:46 - method of ranking instead of this method
423:48 - of ranking even though dense rank seems
423:50 - more intuitive when you are uh building
423:53 - the ranking yourself so we can now take
423:57 - this query and hopefully I've written it
423:59 - correctly and go to big query
424:02 - and try to run
424:05 - it and as you can see we have our items
424:09 - they are sorted by value and then we
424:12 - have our numbering functions so row
424:16 - number should go from one to 20 without
424:19 - any surprises CU it's just numbering the
424:23 - rows this dense rank should have rank
424:26 - one for the first and then these two
424:29 - should share the same rank because they
424:31 - have both have 50 and then the next rank
424:33 - is three so just as I've shown you in
424:35 - the spreadsheet similarly here you have
424:38 - 11 11 and then
424:40 - 12 rank uh instead starts off uh just
424:44 - the same uh smallest value has rank
424:48 - number one and the next two values have
424:52 - rank number two but then after using up
424:55 - two and two it's like you've used up the
424:57 - three so you jump straight to four and
425:00 - after doing 15 and 15 you jump straight
425:02 - to 17 after doing 17 17 you jump
425:05 - straight to 19 and then the the highest
425:09 - number here is 20 which tells you how
425:12 - many rows you're dealing
425:14 - with of course what you see here are
425:17 - window functions they work just the same
425:20 - as we I've shown you and so you could
425:24 - pick up Rank and you could order by
425:26 - value descending and then you will see
425:28 - you will find the inverse of that rank
425:32 - in the sense that the highest value item
425:36 - will give you rank one and it will go
425:39 - from there and the lowest value item
425:41 - will have sort of the the biggest rank
425:45 - number and and rank is often used like
425:48 - this you know the thing that has the
425:50 - most of what we want you know the
425:51 - biggest salary the biggest value the
425:54 - most successful product we rank it we
425:57 - make it so that it's rank one it's like
425:59 - the first in our race and then everyone
426:01 - else goes from there and so we often
426:04 - have actually we order by something
426:06 - descending when we calculate the
426:09 - rank and of course because these
426:12 - numbering functions are window functions
426:15 - they can also be combined with Partition
426:17 - by if you want to cut the data into
426:20 - subgroups so here's an example on the
426:23 - fantasy characters table we are
426:26 - basically uh partitioning by class
426:29 - meaning that each row only sees the
426:32 - other rows that share the same class so
426:35 - archers only care about archers Warriors
426:37 - only care about Warriors and so forth
426:40 - and then within the class we are
426:42 - ordering by level descending okay so the
426:44 - highest levels come first and using this
426:48 - to rank the characters okay so if I go
426:51 - here then I can see that within the
426:53 - archers the highest level Archer has
426:56 - level 26 so they get the first Rank and
426:59 - then all the others is go down down from
427:02 - there and then we have our Warriors and
427:07 - the highest level Warrior is 25 and they
427:09 - also get rank one because they are being
427:12 - ranked within Warriors so this is like
427:14 - when you have races and there are
427:16 - categories this like when you have a
427:18 - race and there are categories within the
427:20 - race so there are like many people who
427:23 - arrive first because they arrive first
427:25 - in their category it's not that everyone
427:27 - competes with everyone and so on and so
427:30 - forth you can see that each uh class of
427:34 - character has their own dedicated
427:38 - ranking and you can check the uh
427:41 - bigquery page on numbering function if
427:43 - you want to learn more about these
427:44 - functions you can see here the ones
427:47 - we've talked about rank row number and
427:50 - dense rank there are a few more but
427:52 - these are the ones that are most
427:54 - commonly used in SQL
427:57 - problems and because I know that it can
428:00 - be a bit confusing um to distinguish
428:03 - between row number dense Rank and rank
428:05 - here's a visualization that you might
428:08 - find useful so let's say that we have a
428:11 - list of values uh which are these ones
428:13 - and we are ordering them in descending
428:16 - order so you can see that there's quite
428:19 - some repetition in these values and
428:21 - given this list of values how would
428:23 - these different numbering functions work
428:25 - on them right so here's row number row
428:28 - number is easy it just um
428:33 - assigns a unique number to to each of
428:34 - them so it doesn't matter that the
428:37 - values are sometimes the same you sort
428:39 - of arbitrarily pick um one to be one the
428:42 - other to be two and then you have three
428:44 - and then here you have 10 10 10 but it
428:46 - doesn't matter you just want to order
428:48 - them so you uh do four five six and then
428:50 - finally
428:52 - seven dense rank is actually cares about
428:56 - the values being the same so 50 and 50
428:58 - they both get one uh 40 gets two and
429:03 - then uh the 10 get three and then five
429:06 - gets four so easy the rank just grows uh
429:10 - using all the integer
429:12 - numbers dense rank is also assigning
429:15 - rank one to 50 and 50 but it's also
429:19 - throwing away the two because there are
429:23 - two elements in here then the next one
429:26 - is getting rank three because the two
429:28 - has already been used and then the next
429:32 - batch 1011 is getting rank four but it's
429:35 - also burning five and six and the next
429:39 - one then can only get rank seven so
429:42 - these are the differences between row
429:46 - number dance Rank and rank
429:50 - visualized we have now reached the end
429:53 - of our journey through the SQL
429:55 - fundamentals I hope you enjoyed it and I
429:58 - hoped that you learned something new
430:01 - you hopefully now have some
430:03 - understanding of the different
430:05 - components of SQL queries and the order
430:08 - in which they work and how they come
430:10 - together to allow us to do what we need
430:13 - with the
430:15 - data now of course learning the
430:18 - individual components and understanding
430:20 - how they work is only half the battle
430:23 - the other half of the battle is how do I
430:27 - put these pieces together how do I use
430:30 - them to solve real
430:32 - problems and in my opinion the response
430:35 - to that is not more Theory but it's
430:40 - exercises go out there and do SQL
430:44 - challenges do SQL interviews find
430:47 - exercises or even better find some data
430:51 - that you're interested
430:52 - in upload it in big query and then try
430:56 - to analyze it with
430:59 - SQL I should let you know that I have
431:02 - another playlist where I am solving 42
431:06 - SQL exercises in postrest SQL and I
431:11 - think this can be really useful to get
431:14 - the other half of the course which is
431:16 - doing exercises and knowing how to face
431:19 - real problems with
431:21 - SQL and I really like this playlist
431:24 - because I'm using a free website a
431:26 - website that doesn't require any sign up
431:28 - or any login uh it just works works and
431:32 - you get a chance to go there and do all
431:34 - of these exercises that cover all the
431:36 - theory that we've seen in this course
431:39 - and then after trying it yourself you
431:41 - get to see me solving it and my thought
431:43 - process and my explanation and I think
431:45 - it could be really useful if you want to
431:47 - deepen your SQL
431:51 - skills but in terms of uh how do I put
431:54 - it all together how do I combine all of
431:56 - this stuff I do want to leave you with
431:59 - another resource that I have created
432:01 - which is this table and this table shows
432:03 - you the fundamental moves that you will
432:07 - need to do whenever you do any type of
432:09 - data
432:10 - analytics and I believe that every sort
432:14 - of analytics that you might work on no
432:17 - matter how simple or complicated can
432:21 - ultimately be reduced to these few basic
432:26 - moves and what are these moves they
432:29 - should actually be quite familiar to you
432:30 - by now so we have joining and this is
432:33 - where we combine data from multiple
432:36 - tables based on some connections between
432:39 - columns and in SQL you can do that with
432:42 - the
432:43 - join then we have filtering filtering is
432:47 - when we pick certain rows and discard
432:50 - others so you know let's look only at
432:53 - customers that joined after
432:56 - 2022 now how do you do that in
432:58 - SQL there are a few tools tools that you
433:01 - can use to do
433:03 - that the most important one is the wear
433:06 - filter and the wear
433:08 - filter comes in action right after
433:11 - you've loaded your data and it decides
433:13 - which rows to keep which rows to
433:16 - discard having does just the same except
433:19 - that it works on aggregated fields it
433:22 - works on fields that you've obtained
433:24 - after a group by qualify we actually
433:28 - haven't seen it in this course because
433:30 - it's not a universal component of SQL
433:33 - certain systems have it others don't but
433:35 - qualify is basically also a filter and
433:38 - it works on the result of window
433:41 - functions and finally you have distinct
433:43 - which runs quite at the end of your
433:46 - query and it's basically removing all
433:48 - duplicate
433:50 - rows and then of course you have
433:52 - grouping and aggregation and we've seen
433:54 - this in detail in the course you
433:56 - subdivide the data um on certain
434:00 - dimensions and then you calculate
434:02 - aggregate values within those Dimensions
434:05 - fundamental for analytics how do we
434:08 - aggregate in SQL we have the group by we
434:11 - have the window functions and for both
434:13 - of them we use aggregate functions such
434:15 - as sum average and so
434:19 - on and then we have column
434:22 - Transformations so this is where you
434:24 - apply logic uh arithmetic to transform
434:28 - columns combine column values and take
434:31 - take the data that you have in order to
434:33 - compute data that you
434:36 - need and we do this where we write the
434:39 - select right we can write calculations
434:43 - that involve our columns we have the
434:45 - case when which allows us to have a sort
434:48 - of branching logic and decide what to do
434:51 - based on some conditions and of course
434:53 - we have a lot of functions that make our
434:55 - life easier by doing specific next we
434:59 - have Union Union is pretty simp simple
435:01 - take tables that have the same columns
435:03 - and stack them together meaning put
435:05 - their rows together and combine
435:08 - them and finally we have sorting which
435:12 - can change how your data is sorted when
435:15 - you get the result of your analysis and
435:17 - can be also used in window functions in
435:20 - order to number or rank our data and
435:24 - these are really the fundamental
435:26 - elements of every analysis and every
435:31 - equal problem that you will need to
435:32 - solve so one way to face a problem even
435:36 - if you are finding it difficult is to
435:38 - come back to these fundamental
435:40 - components and try to think of how do
435:42 - you need to combine them in order to
435:44 - solve your problem and how can you take
435:48 - your problem and break it down to
435:51 - simpler operations that involve these
435:55 - steps now at the beginning of the course
435:58 - I promised you that uh we we would be
436:01 - solving a hard squl challenge together
436:04 - at the end of the course so here it is
436:07 - let us try now to solve this challenge
436:10 - applying the concepts in this course now
436:13 - as a quick disclaimer I'm picking a hard
436:16 - challenge because it's sort of fun and
436:19 - it gives us um a playground to Showcase
436:23 - several Concepts that we've seen in the
436:25 - course and also because I would like to
436:27 - show you that even big hard scary ch
436:30 - Alles that are marked as hard and even
436:34 - have advanced in their name can be
436:36 - tackled by applying the basic concepts
436:39 - of
436:40 - SQL however I do not intend for you to
436:42 - jump into these hard challenges um from
436:47 - the very start it would be much better
436:49 - to start with basic exercises and do
436:51 - them step by step and be sure that you
436:53 - are confident with the basic steps
436:56 - before you move on to more advanced
436:58 - steps so if you have trouble
437:02 - uh approaching this problem or even
437:04 - understanding my solution don't worry
437:06 - about it just go back to your exercises
437:09 - and start from the simple ones and then
437:11 - gradually build your way
437:13 - up that being said let's look at the
437:16 - challenge marketing campaign success
437:19 - Advanced on strata scratch so first of
437:23 - all we have one table that we will work
437:26 - on for this challenge marketing campaign
437:29 - so marketing campaign
437:31 - has a few
437:34 - columns and it actually looks like this
437:37 - okay so there's a user ID created that
437:39 - product ID quantity price now when I'm
437:43 - looking at the new table the one
437:45 - question that I must ask to understand
437:47 - it is what does each row represent and
437:52 - just by looking at this table I can have
437:54 - some hypotheses but I'm actually not
437:56 - sure what each row represents so I
437:59 - better go and read the text until I can
438:01 - get a sense of that so let's scroll up
438:05 - and read you have a table of inapp
438:09 - purchases by user okay so this explains
438:13 - my
438:14 - table what does each row represent it
438:18 - represents an event that is a purchase
438:21 - okay so it means that user ID 10 bought
438:26 - product ID 101 in a quantity of three at
438:30 - the price of 55 and created that tells
438:33 - me when this happened so this happened
438:36 - 1st of January
438:38 - 2019 so great now I understand my table
438:42 - and now I can see what the problem wants
438:44 - from
438:45 - me let's go on and read the question so
438:48 - I have a table of inapp purchases by
438:50 - users users that make their first inapp
438:53 - purchase are placed in a marketing
438:55 - campaign where they see call to actions
438:57 - for more Ina purchases find the number
439:00 - of users that made additional purchases
439:03 - due to the success of the marketing
439:05 - campaign the marketing campaign doesn't
439:08 - start until one day after the initial
439:10 - app purchase so users that made one or
439:13 - multiple purchases on the first day do
439:15 - not count nor do we count users that
439:17 - over time purchase only the products
439:19 - they purchased on the first day all
439:21 - right so that was a mouthful okay so
439:24 - this on the first run it's actually a
439:27 - pretty complicated problem
439:31 - so our next task now is to understand
439:34 - this text and to simplify it to the
439:37 - point that we can convert it into code
439:40 - okay and a good intermediate step before
439:43 - jumping into the code is to write some
439:45 - notes and we can use the SQL commenting
439:48 - feature for that so what I understand
439:51 - from this text is that users make
439:57 - purchases and we are interested in users
440:01 - that make additional
440:05 - purchases we're interested in users who
440:08 - make additional purchases thanks to this
440:12 - marketing
440:13 - campaign how do we Define additional
440:17 - purchases additional purchase is defined
440:22 - as and the fundamental sentence is this
440:27 - one users that made one or multiple Pur
440:30 - purchases on the first day do not count
440:33 - so additional purchase happens after the
440:37 - first day
440:40 - right nor do we count users that over
440:44 - time purchase only the products they
440:46 - purchased on the first
440:48 - day so the other condition that we're
440:51 - interested
440:53 - in is that it involves a
440:58 - product that was not
441:01 - bought the first
441:04 - day and finally what we want is the
441:08 - number of
441:09 - users so get the number of these
441:14 - users that should be a good start for us
441:18 - to begin writing the code so let us look
441:22 - at the marketing campaign table again
441:24 - and I remind you that each row
441:26 - represents a purchase so what do we need
441:30 - to find First in this
441:35 - table so we want to compare purchases
441:39 - that happen on the first day with
441:42 - purchases that happen the following day
441:46 - so we need a way to count days and what
441:49 - do we mean first day and following days
441:52 - do we mean the first day that the shop
441:54 - was uh open no we actually mean the
441:58 - first day that the user ordered right
442:02 - because the user signs up does the first
442:05 - order and then after that the marketing
442:07 - campaign starts so we're interested in
442:11 - numbering days for each user such that
442:15 - we know what purchases happened on the
442:17 - first day what purchases happened on the
442:19 - second day third day and so on and what
442:23 - can we use to run a numbering by
442:28 - user we can use a window function with a
442:33 - numbering function
442:37 - right so I can go to my
442:42 - marketing
442:45 - campaign
442:48 - table and I can select the user
442:53 - ID and the date in which they bought
442:58 - something and the product ID for
443:03 - now now I said that I need a window
443:06 - function so let me start and Define the
443:10 - window now I want to count the days
443:13 - within each user so I will actually need
443:16 - to Partition by user ID so that each row
443:21 - only looks at the rows that correspond
443:25 - to that same
443:27 - user and then there is an ordering right
443:30 - there is a a sequence from the first day
443:33 - uh in which the user bought something to
443:34 - the second and the third and so on so my
443:36 - window will also need an
443:39 - ordering and what column in my table can
443:43 - provide an ordering it is created
443:47 - at and then what counting
443:50 - function do I need to use here well the
443:54 - the way to choose is to say what happens
443:57 - when the same user made two two
444:00 - different purchases on the same date
444:04 - what do I want my function to Output do
444:08 - I want it to Output two different
444:10 - numbers as a simple count or do I want
444:12 - them want it to Output the same number
444:16 - and the answer is that I wanted to
444:18 - Output the same number because all of
444:22 - the purchases that happened on day one
444:25 - need to be marked as day one and all the
444:28 - purchases that have happened on day two
444:31 - need to be marked as day two and so on
444:34 - and so the numbering function that
444:36 - allows us to achieve this is
444:39 - Rank and if you remember ranking is
444:43 - works just like ranking the winners of a
444:45 - race everyone who shares the same spot
444:48 - gets the same number right and this is
444:51 - what we want to achieve
444:53 - here so let us see what this looks like
444:55 - now and let us order by user ID and
444:59 - created
445:00 - at let us now see our
445:04 - purchases now user 10 started buying
445:08 - stuff on this day they bought one
445:10 - product and the rank is one Let's us
445:13 - actually give a better name to this
445:15 - column so that it's not just
445:17 - rank and we can call it user
445:21 - day all right
445:24 - so this user id10 had first user day on
445:30 - the this date and they brought one
445:31 - product then at a later date they had
445:34 - their second user day and they bought
445:35 - another product and then they had a
445:38 - third now user 14 started buying on this
445:43 - date this was their first user day they
445:45 - bought product 109 and then the same day
445:48 - they bought product 107 and this is also
445:51 - marked as user day one so this is what
445:54 - we
445:55 - want and then at a later day they bought
445:59 - another product and this is marked as
446:01 - user day three remember with rank you
446:05 - can go from 1 one to three because this
446:10 - the F the spot marked as one has eaten
446:13 - the spot Mark as two that's not an issue
446:16 - in this problem so we are happy with
446:20 - this now if we go back to our notes we
446:23 - see that we are interested in users who
446:26 - made additional purchases and additional
446:29 - means that it happen s after the first
446:33 - day and how can we identify purchases
446:36 - that happened after the first
446:38 - day well there's a simple solution for
446:41 - this we can simply filter out rows that
446:45 - have a user day
446:48 - one right all of the rows where the user
446:51 - day is one represent purchases that the
446:53 - user made on their first day so we can
446:58 - discard this and keep only purchase that
447:00 - happened on the following
447:03 - days now I don't really have a way to
447:06 - filter on this uh window function
447:10 - because as you recall from the order of
447:12 - SQL operation the window function
447:14 - happens here and the wear filter happens
447:17 - before that so the wear filter cannot be
447:19 - aware of what happens in the window
447:22 - function and the having also happens
447:24 - before
447:26 - it so I need a different solution to
447:29 - filter on this
447:30 - field what I need to do is to use a
447:33 - Common Table expression so that I can
447:35 - break this query in two steps so I'm
447:39 - going to wrap this logic into a table
447:43 - called
447:45 - T1 or I can call it
447:48 - purchases for it to be more meaningful
447:51 - and if I do select star from
447:55 - purchases you will see that the result
447:58 - does not change
448:00 - but what I can do now is to use the wear
448:04 - filter and make sure that the user day
448:08 - is bigger than
448:09 - one and if I look here you will see that
448:12 - I have all purchases which happened
448:15 - after the users first day but there is
448:19 - yet one last requirement that I have to
448:22 - deal with which is
448:25 - that the purchase must happen after the
448:27 - first day and it must involve a product
448:31 - that the user didn't buy on the first
448:35 - day so how can I comply with this
448:40 - requirement now for all of the
448:43 - rows that represent a
448:47 - purchase I need to drop the rows that
448:51 - involve a product ID that the user
448:53 - bought the first day so if I find out
448:56 - that user 10 bought product 119 on day
448:59 - one
449:01 - this purchase does not count I'm not
449:03 - interested in
449:05 - it so how can I achieve this in code I'm
449:09 - already getting all the purchases that
449:11 - didn't happen on day one and then I want
449:13 - another condition so I will say and
449:17 - product
449:19 - ID not
449:22 - in and here I will
449:25 - say products that this user bought on
449:30 - day one right it makes sense so this is
449:34 - all the filters I need to complete my
449:37 - problem show me all the
449:39 - purchases that happened not on day one
449:42 - and also make sure that the user didn't
449:45 - buy this product on day
449:47 - one so what I need to do is to add a
449:51 - subquery in here and before I do that
449:55 - let me give a Alias to this table so so
450:00 - that I don't get confused when I call it
450:02 - again in the subquery so this first
450:05 - version of purchases that we're using we
450:07 - could call it next days because we're
450:10 - only looking at purchases that happen
450:13 - after the first day whereas in the
450:16 - subquery we want to look at
450:19 - purchases but we're interested in the
450:22 - ones that actually happened on day one
450:25 - so we could call this first
450:28 - day and and we can use a wear filter to
450:32 - say that first day user
450:35 - day needs to be equal to one so this is
450:39 - a way that we can use to look at the
450:42 - purchases that happened on the first day
450:45 - now when we make this list we need to
450:48 - make sure that we are use looking at the
450:51 - same user right and to do that we can
450:55 - say end first day user ID needs to be
451:00 - the same as next day's user ID and this
451:04 - ensures that we're looking at the same
451:07 - user and we're not getting confused
451:10 - between users and finally what do we
451:13 - need from the list of first day
451:15 - purchases we need the list of
451:22 - products so let me first see if the
451:24 - query
451:25 - runs so it runs there's no mistakes and
451:29 - now let us review the logic of this
451:32 - query we have purchases which is
451:36 - basically a list of purchases with the
451:38 - added value that we know if it happened
451:41 - on day one on day two on day three and
451:43 - so on and then we are getting all of
451:47 - these
451:49 - purchases the ones that happened after
451:52 - day one and we are also getting the the
451:57 - list of products that they this user
452:01 - bought on day one and we are making sure
452:04 - to exclude those products from our final
452:09 - list and this is a correlated subquery
452:14 - because it is a specific SQL query that
452:19 - provides different results for every row
452:21 - that must run for every row because in
452:24 - the first row we need to get the list of
452:27 - products that user ID 10 has bought on
452:30 - day one and make sure that this product
452:33 - is not in it um and then when we go to
452:37 - another row such as this one we need to
452:39 - get the list of all products that user
452:41 - 13 bought on day one and make sure that
452:44 - 118 is not in those
452:47 - products so this is why it's a
452:49 - correlated
452:50 - subquery and the final step in our
452:53 - problem is to get the number of these
452:55 - users so instead of selecting star and
452:58 - getting all of the C columns I can say
453:01 - count distinct user
453:04 - ID and if I run
453:07 - this I get
453:10 - 23 checking and this is indeed the right
453:16 - solution so this is one way to solve the
453:19 - problem and hopefully it's not too
453:21 - confusing but if it is don't worry it is
453:25 - after all an advanced problem
453:30 - if you go to solution here I do think
453:33 - however that my solution is a bit
453:36 - clearer than what strata scratch
453:39 - provides this is actually a bit of a
453:41 - weird solution but that's ultimately up
453:44 - to you to decide and I am grateful to
453:47 - strata scratch for providing problems
453:50 - that I can solve for free such as this
453:54 - one welcome to postgress SQL exercises
453:58 - the website that we will use to exercise
454:01 - our SQL skills now I am not the author
454:05 - of this website I'm not the author of
454:07 - these
454:08 - exercises the author is Alis D Owens and
454:13 - he has generously created this website
454:16 - for anyone to use and it's free you
454:19 - don't even need to sign up you can go
454:21 - here right away and start working on
454:24 - it I believe it is a truly awesome
454:28 - website in fact the best at uh what it
454:31 - does and I'm truly grateful to Alis
454:34 - there for making this available to
454:37 - all the way the website works is pretty
454:40 - simple you have a few categories of
454:43 - exercises here and you can select a
454:46 - session and once you select a session
454:49 - you have a list of exercises you can
454:52 - click on an exercise and then here in
454:55 - the exercise view you have a question
454:58 - that you need to solve and you see a
455:02 - representation of your three tables
455:04 - we're going to go into this shortly and
455:07 - then you see your expected results and
455:10 - here in this text box over here you can
455:13 - write your uh
455:15 - answer and then hit run to see if it's
455:17 - the correct one the results will appear
455:20 - in this lower quadrant over here and if
455:23 - you get stock you can ask for a
455:26 - hint um and uh here there are also a few
455:31 - keyboard shortcuts that you can use and
455:33 - then after you submit your answer uh or
455:37 - if you are completely stuck you can go
455:40 - here and see the answers and and
455:45 - discussion and that's basically all
455:47 - there is to
455:48 - it now let's have a brief look at the
455:51 - data and see what that's about and the
455:54 - data is the same for all exercises and
455:57 - what we have here is the data about a
456:01 - newly opened Country Club and we have
456:04 - three tables here members represents the
456:08 - members of the country club so we have
456:10 - their surname and first name their
456:12 - address their telephone and uh the the
456:15 - date that which they joined and so on
456:18 - and then we have the bookings so
456:20 - whenever a member makes a booking into a
456:23 - facility that event is stored into this
456:26 - table and then finally we have a table
456:28 - of facility where we have information
456:31 - about each facility and U in there we
456:34 - have some some tennis courts some
456:36 - badminton courts uh massage rooms uh and
456:40 - so
456:41 - on now as you may know this is a
456:45 - standard way of representing how data is
456:48 - stored in a SQL system so you have um
456:52 - the tables and for each table you see
456:55 - the columns and for each column you see
456:58 - the name and then the data type right so
457:01 - the data type is the type of data that
457:04 - is allowed into this column and as you
457:07 - know each column has a single data type
457:09 - and you are not allowed to mix multiple
457:12 - data types within each column so we have
457:15 - a few different data types here and they
457:18 - have the postgress um name so in
457:22 - postgress an integer is a whole number
457:25 - like 1 2 3 and a numeric is actually a
457:29 - FL floating Point number such as 2.5 or
457:33 - 3.2 character varying is the same as
457:36 - string it represents a piece of text and
457:39 - if you wonder about this number in round
457:41 - brackets 200 it represents the maximum
457:44 - limit of characters that you can put
457:46 - into this piece of text so you cannot
457:48 - have a surname that's bigger than 200
457:51 - characters and then you have a time
457:53 - stamp which represents a specific point
457:56 - in time and this is actually all the
457:59 - data types that we have
458:01 - here and finally you can see that the
458:05 - tables are connected so in the booking
458:08 - table every entry every row of this
458:11 - table represent an event where a certain
458:14 - facility ID was booked by a certain
458:17 - member ID at a certain time for a
458:20 - certain number of slots and the facility
458:23 - ID is the same as the facility ID field
458:26 - in facilities and the M ID field field
458:29 - is the same as the M ID or member ID
458:32 - field in
458:34 - members therefore the booking table is
458:37 - connecting to to both of these table and
458:40 - these logical connections will allow us
458:42 - to use joins in order to build queries
458:47 - that work on all of these three tables
458:49 - together and we shall see in detail how
458:51 - that
458:52 - works finally we have an interesting
458:55 - Arrow over here which represents a self
458:58 - relation meaning that the members table
459:01 - has a relation to itself and if you and
459:04 - if you look here this is actually very
459:06 - similar to the example that I have shown
459:09 - in my U mental models course um for each
459:13 - member we can have a recommended bu
459:18 - field which is the ID of another member
459:21 - the member who recommended them into the
459:24 - club and this basically means that you
459:27 - can join the members table to itself in
459:30 - order to get at the same time
459:33 - information about a specific member and
459:35 - about the member who recommended them
459:38 - and we shall see that in the
459:41 - exercises and clearly the exercises run
459:46 - on post SQL and postgress is one of the
459:49 - most popular open-source SQL systems out
459:53 - there postgress SQL is a specific
459:56 - dialect of SQL which has some minor
459:58 - difference es from other dialects such
460:01 - as my SQL or Google SQL that used is
460:05 - used by bigquery but it is mostly the
460:08 - same as all the others if you've learned
460:11 - SQL with another dialect you're going to
460:13 - be just fine postgress sqle does have a
460:16 - couple of quirks that you should be
460:18 - aware about but I will address them
460:20 - specifically as we solve these
460:24 - exercises now if you want to rock these
460:27 - exercises I recommend keep keeping in
460:29 - mind The Logical order of SQL operations
460:33 - and this is a chart that I have
460:35 - introduced and explained extensively in
460:39 - my mental models course where we
460:41 - actually start with this chart being
460:43 - mostly empty and then we add one element
460:46 - at a time making sure that we understand
460:48 - it in
460:49 - detail so I won't go in depth on this
460:51 - chart now but in short this chart
460:54 - represents the logical order of SQL
460:57 - operations these are are all the
461:00 - components that we can assemble to build
461:03 - our SQL queries they're like our Lego
461:06 - building blocks for for SQL and these
461:09 - components when they're assembled they
461:12 - run in a specific order right so the
461:15 - chart represents this order it goes from
461:18 - top to bottom so first you have from
461:20 - then you have where and then you have
461:22 - all the others and there are two very
461:26 - important rules that each operation can
461:29 - only use data produced above
461:32 - it and an operation doesn't know
461:36 - anything about data produced below it so
461:40 - if you can keep this in mind and keep
461:42 - this chart as a reference it will
461:44 - greatly help you with the
461:46 - exercises and as I solve the exercises
461:50 - you will see that I put a lot of
461:52 - emphasis on coming back to this order
461:55 - and actually thinking in this order in
461:57 - order to write effective
461:59 - queries let us now jump in and get
462:03 - started with our basic exercises so I
462:06 - will jump into the first exercise which
462:10 - is retrieve everything from a table so
462:14 - here I have my question and how can I
462:17 - get all the information I need from the
462:20 - facilities table and as you know all my
462:23 - data is represented here so I can check
462:26 - here to see where I can find the data
462:28 - that I need
462:30 - now as I write my query I aim to always
462:34 - start with the front part why start with
462:37 - the front
462:39 - part first of all it is the first
462:43 - component that runs in The Logical order
462:47 - so again if I go back to my chart over
462:50 - here I can see that the from component
462:52 - is the
462:53 - first and that makes sense right because
462:56 - before I do any work I need to get my
462:58 - data so I need to tell SQL where my data
463:01 - is so in this case the data is in the
463:05 - facilities
463:06 - table next I need to retrieve all the
463:10 - information from this table so that
463:12 - means I'm not going to drop any rows and
463:15 - I'm going to select all the
463:17 - columns and so I can simply write select
463:22 - star and if I hit
463:25 - run I get the result that I need here in
463:29 - this quadrant I can see my result and it
463:32 - fits the expected
463:36 - results now the star is a shortcut for
463:39 - saying give me all of The Columns of
463:42 - this table so I could have listed each
463:44 - column in turn but instead I took a
463:46 - shortcut and used a
463:49 - star retrieve specific columns from a
463:53 - table I want to print a list of all the
463:56 - facilities and their cost to members
464:00 - so as always let's start with the front
464:02 - part where is the data that we need it's
464:05 - in the facilities table
464:10 - again and now the question is actually
464:13 - not super clear but luckily I can check
464:16 - the expected results so what I need are
464:18 - two columns from this table which is
464:21 - name and member cost so to get those two
464:24 - columns I can write select name member
464:28 - cost
464:30 - hit run and I get the result that I
464:33 - need so if I write select star I'm going
464:36 - to get all the columns of the table but
464:39 - if I write the name of specific columns
464:41 - separated by comma I will get uh only
464:46 - those columns
464:48 - specifically control which rows are
464:51 - retrieved we need a list of facilities
464:54 - that charge a fee to members so we know
464:59 - that we're going to work with the
465:00 - facilities
465:02 - table and now we need to keep certain
465:06 - rows and drop others we need to keep
465:09 - only the rows that charge a fee to
465:12 - members so what component can we use in
465:15 - order to do
465:18 - this if I go back to my components chart
465:21 - I can see that right after from we have
465:24 - the we component and the we component is
465:27 - used to drop rows that we don't need
465:31 - right so in after getting the facilities
465:34 - table I can see I can say where member
465:39 - cost is bigger than zero meaning that
465:43 - they charge a fee to members and finally
465:47 - I can get all of the columns from
465:53 - this control which rows are retrieved
465:56 - part two so like before we want the list
466:00 - of facilities that charge a fee to
466:01 - members but our filtering condition is
466:04 - now a bit more complex because we need
466:06 - that fee to be less than 150th of the
466:08 - monthly maintenance
466:12 - cost so I copied over the code from the
466:15 - last exercise we're getting the data
466:17 - from our facilities list and we're
466:19 - filtering for those where the member
466:21 - cost is bigger than zero and now we need
466:24 - to add a new condition which is that
466:27 - that fee which is member cost is less
466:31 - than 150th of the monthly maintenance
466:35 - cost so I can take monthly maintenance
466:37 - over
466:40 - here and divide it by
466:42 - 50 and I have my condition now when I
466:46 - have multiple logical conditions in the
466:49 - wear I need to link them with the
466:51 - logical operator so SQL can figure out
466:55 - how to combine them because the final
466:57 - result of all my conditions needs to be
467:00 - a single value which is either true or
467:03 - false right
467:05 - so let's see how to do
467:08 - this in my mental models course I
467:11 - introduced the Boolean operators and how
467:14 - they work so you can go there for more
467:16 - detail but can you figure out which
467:18 - logical operator do we need here to
467:21 - chain these two
467:23 - conditions as suggested in the question
467:26 - the operator that I need is end so I can
467:28 - put it here here and what end does is
467:30 - that both of these conditions need to be
467:33 - true for the whole expression to
467:36 - evaluate to true and for the row to be
467:38 - kept so only the rows where both of
467:43 - these conditions are true will be kept
467:45 - and all other rows will be
467:48 - discarded now to complete my exercise I
467:51 - just need to select a few specific
467:54 - columns because we don't want to return
467:55 - all the columns here and I think that I
467:57 - will cheat a bit by copying them from
468:00 - the expected
468:03 - results and putting them
468:06 - here but normally you would look at the
468:09 - table schema and figure out which
468:11 - columns you
468:16 - need and that completes our
468:19 - exercise basic string searches produce a
468:22 - list of all facilities with the word
468:25 - tennis in their name so where is the
468:27 - data we need it's in the CD facilities
468:30 - table next question do I need all the
468:33 - rows from this table or do I need to
468:35 - filter out some rows well I only want
468:38 - facilities with the word tennis in their
468:40 - name so clearly I need a filter
468:43 - therefore I need to use the wear
468:46 - statement how can I write the wear
468:48 - statement I need to check the
468:50 - name and I need to keep only facilities
468:54 - which have tennis in their name so I can
468:57 - use the like
469:00 - statement
469:01 - here to say
469:05 - that the facility needs to have tennis
469:09 - in its name but what this wild card
469:12 - signify is that we don't care what
469:15 - precedes tennis and what follows tennis
469:17 - it could be zero or more characters
469:20 - before it and after it we just care to
469:22 - check that they have tennis in their
469:25 - name and finally we need to select all
469:28 - all the columns from these
469:31 - facilities and that's our
469:34 - result beware like I said before of your
469:37 - use of the quotes So what you have here
469:40 - is a string it's a piece of text that uh
469:43 - allows you to do your match therefore
469:46 - you need single quotes if you as it's
469:49 - likely to happen used double quotes you
469:53 - would get an error here and the error
469:55 - tells you that the column tenis the does
469:58 - not exist because double quotes are used
470:02 - to represent column names and not pieces
470:05 - of text so be careful with
470:08 - that matching against multiple possible
470:11 - values can we get the details of
470:13 - facilities with id1 and
470:16 - id5 so where is my data is in the
470:20 - facilities
470:21 - table and do I need all the rows from
470:25 - this table or only certain ones I need
470:28 - only certain rows because I want those
470:31 - that have id1 and id5 so I need to use a
470:35 - wear
470:36 - statement Now what are my conditions
470:39 - here their ID actually facility ID
470:44 - equals 1 and facility ID equals 5 so I
470:49 - have my two logical conditions now what
470:52 - operator do I need to use in order to
470:55 - chain
470:57 - them I need to use the or operator right
471:01 - because only one of these need needs to
471:04 - be true in order for the whole
471:06 - expression to evaluate to true and in
471:09 - fact only one of them can be true
471:12 - because it's impossible for the idea of
471:14 - a facility to be equal to one and five
471:17 - at the same time therefore the end
471:19 - operator would not work and what we need
471:22 - is the or
471:23 - operator and finally we need to get all
471:26 - the data meaning all the columns about
471:29 - this facility so I will use select
471:33 - star the problem is now solved but now
471:36 - let's imagine that tomorrow we need this
471:39 - query again and we need to include
471:41 - another id id 10 so what we can do is
471:44 - put or facility ID equals 10 but this is
471:49 - becoming a bit unwieldy right because
471:51 - imagine having a list of 10 IDs and then
471:53 - writing or every time and it's it's not
471:56 - very scalable as an approach approach so
471:59 - as an alternative we can say facility ID
472:03 - in and then list the values like one and
472:07 - five so if I take this and make it into
472:12 - my
472:14 - condition I will again get the same
472:16 - result I will get the the solution but
472:20 - this is a more elegant approach and it's
472:22 - also more scalable because it's much
472:24 - easier to come back and insert other IDs
472:26 - inside this list so this is a preferred
472:29 - solution in this
472:31 - case and logically what in is doing is
472:35 - looking at the facility ID for each row
472:39 - and then checking whether that ID is
472:41 - included in this list if it is it
472:44 - returns true therefore it keeps the row
472:46 - if it's not returns false therefore it
472:48 - drops the
472:50 - row and we shall see a bit later that
472:53 - the in uh notation is also powerful
472:57 - because in this case we have a static
472:59 - list of IDs we know that we want IDs one
473:01 - and five but in more advanced use cases
473:05 - instead of a static list we could
473:07 - provide another query a SQL query or a
473:11 - subquery that would dynamically retrieve
473:15 - a certain list and then we could use
473:17 - that in our query so we shall see that
473:19 - in later
473:21 - exercises classify result into buckets
473:24 - produce a list of facilities and label
473:27 - them cheap or expensive based on their
473:30 - monthly maintenance
473:33 - so we want to get our facilities do we
473:38 - need a filter do we need to drop certain
473:40 - rows no we actually don't we want to get
473:43 - all facilities and then we want to label
473:47 - them and we need to select the name of
473:49 - the facility and then here we need to
473:52 - provide the label so what SQL statement
473:55 - can we use to provide a text level label
473:59 - according to the value of a certain
474:03 - column what we need here is a case
474:07 - statement which
474:09 - implements conditional logic which
474:11 - implements a branching right it's
474:14 - similar to the if else statements in
474:16 - other programming languages because if
474:19 - the monthly maintenance cost is more
474:22 - than 100 then it's expensive otherwise
474:26 - it's cheap so this call for a case
474:30 - statement now I always start with case
474:34 - and end with end and I always write
474:36 - these at the beginning so I don't forget
474:38 - them and then for each condition I write
474:42 - when and what is the condition that I'm
474:44 - interested in monthly
474:51 - maintenance being
474:54 - above
474:57 - 100 that's my first
474:59 - condition what do I do in that case I
475:03 - output a piece of text which
475:06 - says
475:08 - expensive and remember single quotes for
475:11 - test
475:14 - text next I could write the next
475:18 - condition explicitly but actually if
475:20 - it's not 100 then it's less than 100 so
475:23 - all I need here is an
475:24 - else and in that case I need to Output
475:28 - the piece of text which says
475:31 - cheap and finally I have a new column
475:35 - and I can give it a label I can call it
475:41 - cost and I get my
475:44 - result so whenever you need to put
475:46 - values into buckets or you need to label
475:49 - values according to certain rules that's
475:52 - usually when you need a case
475:55 - statement working with dates
475:58 - let's get a list of members who joined
476:01 - after the start of September 2012 so
476:06 - looking at these tables where is our
476:08 - data it's in the members
476:10 - table so I will start writing this and
476:15 - now do I need to filter this table yes I
476:17 - only want to keep members that joined
476:19 - after a certain time and now how can I
476:22 - run this the condition on this table I
476:26 - can say where join date
476:29 - is bigger
476:32 - than
476:34 - 2012
476:37 - September 01 so luckily in SQL and in
476:41 - postgress filtering on dates is quite
476:43 - intuitive even though here we have a
476:46 - time stamp that represents a specific
476:49 - moment in time up to the second we can
476:51 - say bigger or equal actually because we
476:55 - also want to include those who joined on
476:56 - the first day we can write bigger or
476:59 - equal and just specify the the date and
477:02 - SQL will fill in the the rest of the
477:04 - remaining values and the filter will
477:07 - work and next we want to
477:10 - get a few columns for these
477:14 - members so I will copy paste here
477:22 - select and this solves our
477:25 - query removing duplicates and ordering
477:27 - results
477:28 - we want an ordered list of the first 10
477:31 - surnames in the members table and the
477:34 - list must not contain duplicates so
477:36 - let's start by getting our table which
477:39 - is the members
477:41 - table now we want to see the
477:45 - surnames so if I write this I will see
477:50 - that there are surnames which are shared
477:54 - by members so there are actually
477:56 - duplicates here so what what can we do
477:58 - in SQL in order to remove
478:01 - duplicates we have seen in the mental
478:04 - models course that we have the distinct
478:06 - keyword and the distinct is going to
478:10 - remove all duplicate rows based on the
478:14 - columns that we have
478:16 - selected so if I run this again I will
478:18 - not see any duplicates
478:21 - anymore now the list needs to be ordered
478:24 - alphabetically as I see here in the
478:26 - expected results and we can do that with
478:29 - the order by
478:32 - statement and when you use order by on a
478:34 - piece of text the default behavior is
478:37 - that the text is ordered
478:40 - alphabetically and uh if I were to use
478:42 - Des sending then it would be ordered in
478:45 - Reverse alphabetical order however
478:48 - that's not what I need I need it in
478:50 - alphabetical
478:52 - order so now I see that they are ordered
478:55 - and finally I want the first 10 surnames
478:59 - so how can I return the first 10 rows of
479:02 - my
479:03 - result I can do that with the limit
479:06 - statement so if I say limit 10 I will
479:09 - get the first 10 surnames and since I
479:11 - have ordered alphabetically I will get
479:13 - the first 10 surnames in alphabetical
479:17 - order and this is my
479:20 - result now going back to our map over
479:24 - here we have the from which gets a table
479:28 - we have a where which drops rows that we
479:30 - don't need from that table and then all
479:33 - the way down here we have the select
479:35 - which gets the columns that we need and
479:37 - then we have the distinct right and the
479:39 - distinct needs to know which columns we
479:42 - need because it's it drops duplicates
479:44 - based on these columns so in this
479:47 - example over here we're only taking a
479:49 - single column surname so the distinct is
479:52 - going to drop duplicate surnames and
479:56 - then at the end of it all when all the
479:58 - processing is done we can order our
480:00 - results and then finally once our
480:03 - results are ordered we can do a limit to
480:06 - limit the number of rows that we return
480:09 - so I hope this makes
480:11 - sense combining results from multiple
480:13 - queries so let's get a combined list of
480:16 - all surnames and all facility names so
480:21 - where are the surnames there in CD
480:25 - members and from CD m mbers I can select
480:29 - surname right and this will give me the
480:32 - list of all
480:34 - surnames and where are the facility
480:37 - names there are in CD facilities and I
480:41 - could say
480:42 - select name from CD facilities and I
480:45 - would get a list of all the
480:48 - facilities now we have two distinct
480:50 - queries and they both produce a list or
480:54 - a column of text values and we want to
480:57 - combine them what does it mean we want
480:59 - to stack them on top of each other right
481:03 - and how does that work well if I just
481:06 - say run query like this I will get an
481:08 - error because I have two distinct query
481:12 - here queries here and they're not
481:13 - connected in any way but when I have two
481:18 - queries or more defining tables and I
481:21 - want to stack them on top of each other
481:23 - I can use the union statement right
481:29 - and if I do Union here I will uh get
481:32 - what I want because all the surnames
481:35 - will be stacked uh vertically with all
481:39 - the names and I will get a unique list
481:42 - containing both of these
481:44 - columns now as I mentioned in the mental
481:47 - models course typically when you have
481:50 - just
481:51 - Union uh it means Union distinct and
481:54 - actually other systems like bigquery
481:57 - don't allow you to write just Union they
481:59 - want you to specify Union
482:01 - distinct and what this actually does is
482:03 - that after stacking together these two
482:06 - tables it removes all duplicate
482:10 - rows and uh the alternative to this is
482:13 - Union
482:14 - all which um does not do this it
482:18 - actually keeps all the rows and as you
482:20 - know we have some duplicate surnames and
482:23 - then we get them here and it doesn't fit
482:25 - with our result but if you write just
482:27 - Union it will be Union distinct and you
482:29 - won't have any
482:31 - duplicates and if you look at our map
482:33 - for The Logical order of SQL operations
482:36 - we are getting the data from a certain
482:39 - table and uh filtering it and then doing
482:42 - all sorts of operations and um on on
482:45 - this data and then we are selecting The
482:48 - Columns that we need and then we can uh
482:51 - remove the the duplicates from this one
482:53 - table and then what comes next is that
482:56 - we could combine this table U with other
483:00 - tables right we can tell SQL that we
483:02 - want to Stack this table on top of
483:05 - another table so this is where the union
483:07 - comes into play and only after we have
483:10 - combined all the tables only after we
483:12 - have stacked them all up on top of each
483:14 - other we can order the results and limit
483:17 - the
483:18 - results also remember and I showed this
483:21 - in detail in the mental models course um
483:24 - when I combine two or more table tables
483:27 - with a union what I need is for them to
483:31 - have the exact same number of columns
483:34 - and all of the columns need to have the
483:36 - same data type so in this case both
483:39 - tables have one column and this column
483:42 - is a text so the the union works but if
483:45 - I were to add another column here and
483:48 - it's an integer
483:49 - column it would not work because the
483:52 - union query must have the same number of
483:54 - columns right I will get an error
483:56 - however if I were to add an integer
483:59 - column in the second position in both
484:01 - tables they would work again because
484:03 - again I have the same number of columns
484:06 - and they have the same data
484:08 - type simple aggregation I need the sign
484:11 - up date of my last member so I need to
484:14 - work with the members table and we have
484:17 - a field here which is join date and I
484:20 - need to get the latest value of this
484:22 - date the time when a member last joined
484:26 - right
484:28 - so how can I do that I can take my join
484:32 - date field and run an aggregation on top
484:35 - of it what is the correct aggregation in
484:37 - this case it is Max
484:41 - because when it comes to dates Max will
484:44 - take the latest date whereas mean will
484:48 - take the earliest date and I can label
484:51 - this as
484:53 - latest and get the result I need
484:57 - now how aggregations work they are uh
485:01 - functions that look like this you write
485:03 - the name of the function and then in
485:05 - round brackets you provide the arguments
485:07 - the first argument is always the column
485:10 - on which to run the aggregation and what
485:13 - the aggregation does is that it takes a
485:15 - list of values could be 10 100 a million
485:18 - 10 million it doesn't matter it takes a
485:21 - long list of values and it compresses
485:23 - this list to a single value it um
485:28 - does like we've seen in this case taking
485:31 - all of the dates and then returning the
485:32 - latest date now to place this in our map
485:36 - we get the data from the table we filter
485:38 - it and then sometimes we do a grouping
485:42 - which we we shall see later in the
485:44 - exercises but whether we do grouping or
485:46 - not here we have aggregations and if we
485:50 - haven't done any grouping the
485:52 - aggregation works at the level of all
485:55 - the rows so in the absence of grouping
485:58 - as in this case the aggregation will
486:00 - look at all the rows in my table except
486:02 - for the rows that I filtered away but
486:05 - otherwise it will look at all the rows
486:07 - and then it will compress them into a
486:08 - single
486:11 - value more aggregation we need the first
486:14 - and last name of the last member who
486:16 - signed up not just the date so in the
486:19 - previous exercise we saw that we can say
486:22 - select Max join date from members and we
486:27 - would get the last join date the date
486:31 - when the last member signed up right so
486:34 - given that I want the first and the last
486:37 - name you might think that you can say
486:40 - first name and
486:41 - surname in here but this actually
486:44 - doesn't work this gives an
486:47 - error the error is that the column first
486:51 - name must appear in the group by clause
486:53 - or be used in a aggregate function
486:57 - now the meaning behind this error and
487:00 - how to avoid it is described in detail
487:03 - in the mental models course in the group
487:05 - by section but the short version of it
487:09 - is that what you're doing here is that
487:11 - with this aggregation you're compressing
487:14 - join date to a single value but you're
487:16 - doing no such compression or aggregation
487:19 - for first name and surname and so SQL is
487:22 - left with the um instruction to return
487:26 - something like this and as you can see
487:29 - here we have a single value but for
487:31 - these columns we have multiple values
487:33 - and this does not work in SQL because
487:35 - you need all columns to have the same
487:37 - number of values and so it it throws an
487:41 - error and what we really need to do here
487:45 - is to take this maximum join date and
487:49 - use it in a wear filter because we only
487:52 - want to keep that row which corresponds
487:55 - to the latest join date so we can take
487:58 - the members table and get the row where
488:01 - join date is equal to the max join date
488:05 - and from that select the name and the
488:10 - surname unfortunately this also doesn't
488:13 - work so what we saw in the course is
488:17 - that you cannot you're not allowed to
488:20 - use aggregations inside wear so you
488:24 - cannot use max inside where and the
488:26 - reason why is that actually pretty clear
488:29 - because aggregations happen at this
488:32 - stage in the in the process and
488:35 - aggregations need to know whether a
488:38 - group ey has occurred or not they need
488:41 - to know whether they have to happen over
488:43 - all the rows in the table or only within
488:46 - the groups defined by the group ey and
488:49 - when we are at the where stage the
488:51 - groupy hasn't happened yet so we don't
488:53 - know at which level to execute the
488:56 - aggregations and because of this we are
488:58 - not allowed to do aggregations inside
489:00 - the where
489:01 - statement so how can we solve the
489:04 - problem now well a a sort of cheating
489:07 - solution would be if we knew the exact
489:10 - value of join date we could place it
489:14 - here and then our filter would work
489:16 - we're not using an
489:18 - aggregation and we could put join date
489:20 - in here to display it as well and that
489:23 - would would
489:24 - work however this is a bit cheating
489:27 - right because um the maximum join date
489:31 - is actually a dynamic value it will
489:33 - change with time so we don't want to
489:35 - hardcode it we want to actually um
489:38 - compute it but because this is not
489:41 - allowed what we actually need is a
489:44 - subquery and the subquery is a SQL query
489:48 - that runs within a query to return a
489:50 - certain result and we can have a
489:53 - subquery by opening round brackets here
489:56 - and write writing
489:58 - a a
490:00 - query and in this query we need to go to
490:03 - the members table and select the maximum
490:06 - join
490:08 - date and this is our actual
490:11 - solution so in this execution you can
490:15 - imagine that SQL will go here inside the
490:18 - subquery run this get the maximum
490:21 - jointed place it in the
490:24 - filter uh keep only the row for the
490:27 - latest member who has joined and then
490:30 - retrieve what we need about this
490:33 - member let us now move to the joints and
490:36 - subqueries
490:41 - exercises the first exercise retrieve
490:44 - the start times of members
490:47 - bookings now we can see that the
490:50 - information we need is spread out into
490:53 - tables because we want the start time
490:55 - for bookings that and that information
490:58 - is in the bookings table but we want to
491:00 - filter to only get members named David
491:05 - farel and the name of the member is
491:08 - contained in the members table so
491:11 - because of that we will need a
491:13 - join so if we briefly look at the map
491:16 - for the order of SQL operations we we
491:19 - can see here that from and join are
491:21 - really the same uh step um and how this
491:25 - works is that in the from statement
491:29 - sometimes uh all my data is in one table
491:31 - and then I just provide the name of that
491:32 - table but sometimes I need to combine
491:35 - two or more different
491:37 - tables in order to get my data and in
491:40 - that case I would use the join but
491:43 - everything in SQL works with tables
491:45 - right so when I when I take two or more
491:47 - tables and combine them together at the
491:49 - end all I get is just another table and
491:52 - this is why from and join are actually
491:55 - the same component and they are the same
491:58 - step so as usual let us start with the
492:01 - front part and we need to take the
492:03 - booking table and we need to join it on
492:06 - the members
492:07 - table and I can give an alas to each
492:11 - table to make my life easier so I will
492:13 - call this book and I will call this mem
492:18 - and then I need to specify The Logical
492:20 - condition for joining this table and The
492:22 - Logical condition is that the M ID
492:25 - column in the booking table is really
492:29 - the same thing as the M ID
492:32 - column in the members
492:37 - table concretely you can imagine um SQL
492:41 - going row by Row in the booking table
492:44 - and looking at the M ID and then
492:46 - checking whether this m ID is present in
492:48 - the members table and if it's present it
492:50 - combines the row uh the current Row from
492:53 - bookings with the matching Row for
492:55 - members does this with all the matching
492:57 - rows and then drops rows which don't
492:59 - have a match and we saw that in detail
493:02 - in the mental models course so I'm not
493:04 - going to go in depth into
493:06 - it now that we have our table which is
493:11 - uh comes from the joint of members and
493:12 - bookings we can properly properly filter
493:15 - it and what we want is that the first
493:20 - name column is David in the column which
493:26 - comes from the members table right so m.
493:29 - first name is indicating the parent
493:31 - table and then the column
493:33 - name
493:35 - and the surname is equal to
493:39 - FAL and remember single quotes when
493:42 - using pieces of text this is a where
493:45 - filter you have two logical conditions
493:47 - and then we use the operator end because
493:49 - both of them need to be true so now we
493:52 - have uh filtered our data and finally we
493:55 - need to select the start time
493:59 - and that's our
494:01 - query now remember that when we use join
494:06 - in a query what's implied is that we are
494:09 - using inner join and there are several
494:11 - types of join but inner joint is the
494:14 - most common so it's the default one and
494:16 - what inner joint means is that it's
494:18 - going to return uh from the two tables
494:21 - that we're joining is going to return
494:23 - only the rows that have a match and all
494:26 - the row that don't have a match are
494:28 - going to be dropped so if there's a row
494:31 - in bookings and it has a m ID that
494:33 - doesn't exist in the members table that
494:36 - row will be dropped and conversely if
494:38 - there's a row in the members table and
494:40 - it has a m ID that is not referenced in
494:43 - the booking table that row will also be
494:46 - dropped and that's an inner
494:50 - join work out the start times of
494:52 - bookings for tennis courts so we need to
494:57 - get the facilities that are actually
495:00 - tennis courts and then for each of the
495:03 - facility we'll have several bookings and
495:06 - we need to get the start time for those
495:08 - uh bookings and it will be in a specific
495:12 - date so we know that we need the data
495:16 - from these two tables because the name
495:18 - of the facility is here but the data
495:20 - about the bookings is here so I will go
495:24 - from CD facilities
495:27 - join CD bookings on what are the fields
495:32 - that we can join on logically now let me
495:35 - first give an alias to these tables so I
495:38 - will call this fox and this I will call
495:43 - book and now what I need to see is
495:48 - that the facility ID matches on both
495:55 - sides now we can work work on our
495:57 - filters so first of all I only want to
495:59 - look at tennis courts and if you look at
496:01 - the result here um it means that in the
496:04 - name of the facility we want to see
496:06 - tennis and so we can filter on uh string
496:12 - patterns on text patterns by using the
496:15 - like uh command so I can take
496:20 - facilities
496:22 - name and get it
496:24 - like tennis
496:27 - and the percentage signs are um wild
496:30 - cards which means that tennis could be
496:33 - preceded and followed by zero or more
496:36 - characters we don't care we just want to
496:39 - get those strings that have tennis in
496:42 - them but that's not enough as a
496:44 - condition we also need the booking to
496:47 - have happened on a specific date so I
496:49 - will put an end here so end is the
496:52 - operator we need because we're providing
496:56 - two logical conditions and they both
496:58 - need to be true so end is what we
497:02 - need and then I can take the start
497:07 - time from the booking
497:10 - table and um say that it should be equal
497:14 - to the date provided in the
497:18 - instructions because I want the booking
497:20 - to have happened in this particular
497:23 - date however this will not work so I can
497:28 - actually complete the query and show you
497:31 - that it will not
497:35 - work because here we get zero results so
497:38 - can you figure out why this um command
497:41 - here did not
497:43 - work now I'm going to write a few
497:45 - comments here and uh this is how you
497:48 - write them and they are just pieces of
497:50 - text they're not actually executed as
497:52 - code and I'll just use them to show you
497:55 - what's going on so the value for start
497:58 - time looks like this so this is a time
498:02 - stamp and is showing a specific point in
498:04 - time but the date that we are providing
498:07 - for the comparison looks like this so as
498:11 - you can
498:12 - see we have something that is uh less
498:16 - granular because we we're not showing
498:19 - all of this data about hour minute and
498:22 - uh and
498:24 - second now in order to compare these two
498:27 - things which are different SQL
498:29 - automatically fills in uh this date over
498:33 - here and what it
498:35 - does is that since there's nothing there
498:37 - it puts zeros in there and now that it
498:41 - has made this um extension it's going to
498:45 - actually compare
498:47 - them so when you look at this uh
498:50 - comparison over
498:52 - here between these two
498:54 - elements this comparison is false false
498:57 - because the hour is
498:59 - different now when we write this uh
499:01 - filter command over here SQL is looking
499:05 - at every single start time and then
499:07 - comparing it with this value over here
499:11 - which is the very first moment of that
499:14 - date but there's no start time that is
499:16 - exactly like this one so basically this
499:19 - is always false and thus we get uh zero
499:23 - rows in our result
499:26 - so what is the solution to
499:29 - this before when we take a start time
499:32 - from the data before comparing it we can
499:36 - put it into the date function and if I
499:40 - take my example here if I put it into
499:42 - the date function it's going to drop
499:46 - that extra information about hour minute
499:49 - and second and it's only going to keep
499:52 - uh the information about the date so
499:58 - once I do this if I uh if I pass it to
500:02 - the date function before comparing it to
500:05 - my reference
500:08 - date now this one is going to
500:12 - become the result which is this one and
500:16 - then I'm going to compare it with my
500:18 - reference date and then this is going to
500:20 - be
500:21 - true so all this to say
500:25 - that before we compare start time with
500:28 - our reference date we need to reduce its
500:30 - granularity and we need to reduce it to
500:33 - its uh to its date so if I run the query
500:36 - now I will actually get my start
500:41 - times and after this I just need to add
500:43 - the
500:45 - name and finally I need to order by time
500:51 - so I need to order
500:53 - bu um book start
501:00 - time there is still a small error here
501:02 - so sometimes you just have to look at
501:04 - what you get and what's expected and if
501:06 - you notice here we are returning data
501:09 - about the table tennis facility but
501:12 - we're actually just interested in tennis
501:13 - court so what are we missing here the
501:18 - string filter is not precise enough and
501:20 - we need to change this into tennis
501:24 - court and now we get our results
501:29 - produce a list of all members who have
501:31 - recommended another
501:33 - member now if we look at the members
501:37 - table we have all these data about each
501:39 - member and then we know if they were
501:41 - recommended by another member and
501:44 - recommended by is the ID of the member
501:49 - who has recommended them and because of
501:52 - this the members table like we said has
501:56 - a relation to itself because one of its
501:59 - column references its ID column so let's
502:03 - see how to put this in
502:05 - practice so to be clear I simply want a
502:08 - list of members who appear to have
502:12 - recommended another member so if I
502:15 - wanted just the IDS of these people my
502:19 - task would be much simpler right I would
502:22 - go to the members table and then I could
502:26 - select
502:27 - recommended
502:29 - by and then I will put a distinct in
502:32 - here to avoid
502:35 - repetitions and what I would get here is
502:38 - the IDS of all members who have
502:40 - recommended another
502:43 - member however the problem does not want
502:46 - this because the problem wants the first
502:49 - name and Sur name of these uh of these
502:52 - people so in order to get the first name
502:55 - and the name of these people I need to
502:58 - plug this ID back into the members table
503:01 - and get the the data there so for
503:04 - example if I went to the members table
503:08 - and I selected
503:11 - everything where the M ID is
503:15 - 11 then I would get the data for this
503:18 - first
503:24 - member but now I need to do this for all
503:26 - members so what I will have to do is to
503:31 - take the members
503:34 - table and join it to
503:39 - itself and the first time I take the
503:42 - table I'm looking at the members quite
503:45 - simply but the second time I take the
503:47 - members table I'm looking at data about
503:50 - the recommenders of the members so I
503:54 - will call this second instance re
503:57 - so both of these they come from the same
503:59 - table but they're now two separate
504:00 - instances and what is the logic to join
504:03 - these two
504:04 - tables the members table has this
504:09 - recommended by field and we take the ID
504:13 - from recommended by and we plug it back
504:16 - into the table into M
504:18 - ID to get the data about the
504:23 - recommenders and now we can go into the
504:26 - recommenders which we got by plugging
504:28 - that ID and get their first name and
504:33 - surname I want to avoid repetition
504:36 - because a member may have been
504:38 - recommending multiple members but I want
504:41 - to avoid repetition so I will put a
504:44 - distinct to make sure that I don't get
504:46 - any uh repeated rows at the end and then
504:50 - finally I can order by surname and first
504:54 - name
504:56 - and I get my
504:58 - result so I encourage you to play with
505:01 - this and experiment a bit until it is
505:03 - clear and in my U mental models course I
505:07 - go into depth into the self joint and uh
505:10 - do a visualization in Google Sheets that
505:13 - also makes it uh much
505:16 - clearer produce a list of all members
505:18 - along with a
505:20 - recommender now if we look at the
505:22 - members table we have a few column and
505:26 - then we have the recommended by column
505:28 - and sometimes we have the ID of another
505:30 - member who recommended this
505:32 - member um it can be repeated because the
505:36 - same member may have recommended
505:37 - multiple people and then sometimes this
505:41 - is empty and when this is empty we have
505:43 - a null in here which is the value that
505:45 - SQL uses to represent absence of
505:48 - data now let us count the rows in
505:52 - members so you might know that to count
505:56 - the rows of a table we can do a simple
505:58 - aggregation which is Count
506:01 - star and we get 31 and let's just make a
506:05 - note of
506:06 - this that members has 31 rows because in
506:12 - the result we want a list of all members
506:15 - so we must ensure that we return 31 rows
506:19 - in our
506:21 - results now I'm going to delete this
506:24 - select and as before I want want to go
506:26 - for each member and check the ID they
506:28 - have here in recommended bu and then
506:31 - plug this back into the table into M ID
506:34 - so I can get the data about the
506:35 - recommender as well and I can do that
506:37 - with a self jooin so let me take members
506:41 - and join on itself and the first time I
506:44 - will call it Ms and the second time I
506:46 - will call it Rex and the logic for
506:49 - joining is that in Ms recommended by is
506:53 - the same um is connected to to Rex M ID
506:57 - so this is taking the ID in the
507:00 - recommended by field and plugging it
507:02 - back into me ID to get the data about
507:04 - the
507:05 - recommender now what do I want from this
507:08 - I want to get the first name of the
507:13 - member and the last
507:15 - name uh
507:17 - surname and then the first name and last
507:20 - name of the recommender
507:28 - uh
507:32 - surname
507:34 - great so it's starting to look like the
507:36 - right result but how many rows do we
507:38 - think we have here and in order to count
507:42 - the rows I can do select count
507:46 - star from and then if I simply take this
507:50 - table uh if I simply take this query and
507:53 - en close it in Brackets now this becomes
507:55 - a a
507:56 - subquery
508:00 - so I
508:02 - can ah the subquery must have an alias
508:06 - so I can give it an alias like
508:10 - this and I get 22 so how this works is
508:15 - that first SQL will compute the content
508:19 - of the subquery which is the table that
508:21 - we saw before and then it will uh we
508:24 - need to assign it an alas otherwise it
508:26 - doesn't work this changes a bit by
508:28 - System but in post you need to do this
508:31 - so we we call it simply
508:33 - T1 and then we run a count star on this
508:37 - table to get the number of rows and we
508:40 - see that the result has 22
508:42 - rows and this is an issue because we saw
508:46 - before that members has 31 rows and that
508:49 - we want to return all of the members
508:51 - therefore our result should also have 31
508:54 - rows so can you figure figure out why
508:57 - are we missing some rows
508:59 - here now the issue here is that we are
509:02 - using an inner join so remember when we
509:05 - don't specify the type of joint it's an
509:07 - inner joint and what does an inner joint
509:10 - do it keeps only rows that have matches
509:14 - so if you we saw before that in members
509:18 - sometimes this field is empty it has a
509:21 - null value because U you know maybe the
509:23 - member wasn't recommended by anyone
509:26 - maybe they just apply it themselves and
509:29 - what happens when we use this in an
509:31 - inner joint and it has a null value the
509:34 - row for that me member will be dropped
509:37 - because obviously it cannot have a match
509:39 - with M ID because null cannot match with
509:42 - with anything with any ID and so that
509:44 - row is dropped and we lose it however
509:47 - that's not what we want to do therefore
509:50 - instead of an inner join we need to use
509:52 - a left join
509:54 - here the the left join will look at the
509:57 - left table so the table that is left of
510:00 - the join command and it will make sure
510:03 - to keep all the rows in that table even
510:07 - the rows that don't have a match in the
510:09 - rows that don't have a match it will not
510:11 - drop them it will just put a null in the
510:14 - values that correspond to the right
510:17 - table and if I run the count again
510:26 - uh I will get 31 so now I have I'm
510:30 - keeping all the members and I have the
510:33 - number of rows that I
510:35 - need so now I can get rid of all of
510:38 - these because I know I have the right
510:40 - amount of of
510:41 - rows and I can um get my selection over
510:48 - here and it would actually help if we
510:51 - could make this a bit uh more ordered
510:55 - and a assign aliases to the columns so I
510:59 - will follow the expected results here
511:02 - and call this m first
511:06 - name me
511:10 - surname W first
511:14 - name Rec
511:18 - surname now we have the proper
511:21 - labels and you can see here that we
511:25 - always have the name of the member but
511:28 - some member weren't recommended by
511:30 - anyone and therefore for the first and
511:32 - last name of the recommender we simply
511:34 - have null values and this is what the
511:36 - left join
511:40 - does the last step here is to order and
511:44 - we want to order by the last name and
511:51 - the first name of each
511:54 - member and we finally get our
511:57 - result so typically you use inner joints
512:01 - which is the default joint because
512:03 - you're only interested in the rows from
512:05 - both tables that actually have a match
512:08 - but sometimes you want to keep all the
512:10 - data about one table and then you would
512:13 - put that table on the left side and do a
512:15 - left join as we did in this
512:19 - case produce a list of all members who
512:21 - have used a tennis
512:24 - court now now for this problem we need
512:28 - to combine data from all our tables
512:31 - because we need to get look at the
512:34 - members and we need to look at their
512:36 - bookings and we need to check what's the
512:40 - name of the facility for their
512:44 - bookings so as always let us start with
512:47 - the front part and let us start by
512:49 - joining together all of these tables
512:55 - CD
512:57 - facilities on facility
513:01 - ID and then I want to also
513:04 - join on
513:12 - members and that is my
513:15 - join so we can always join two or more
513:19 - tables in this case we're joining three
513:21 - tables and how this works is that the
513:24 - first join
513:26 - creates a new table and then this new
513:29 - table is joined with the with the next
513:31 - one over here and this is how multiple
513:35 - joints are
513:37 - managed now I have my table which is the
513:41 - join of all of these
513:44 - tables and
513:47 - um we we're only interested in members
513:51 - who have used the tennis court if a
513:53 - member has made no bookings
513:56 - um we are we don't we're not interested
513:59 - in that member and so it's okay to have
514:02 - a join and not a left join and we're for
514:06 - each booking we want to see the name of
514:08 - the facility and if there was a booking
514:10 - who didn't have the name of the facility
514:12 - we wouldn't be interested in that
514:14 - booking anyway and so um this joint here
514:18 - also can be an inner join and doesn't
514:20 - need to be a left join this is how you
514:22 - can think about whether to have a join
514:25 - or left
514:26 - join now we want the booking to include
514:30 - a tennis court so we can filter on this
514:34 - table and we will look at the name of
514:37 - the
514:38 - facility and
514:40 - uh make sure that it has tennis court in
514:44 - it with the like
514:48 - operator and now that we have filtered
514:50 - we can get
514:52 - the first name
514:56 - and the surname of the
514:58 - member and we can get the facility
515:04 - name so here we have a starting
515:07 - result now in the expected result we
515:11 - have merged the first name and the
515:14 - surname into a single string and um in
515:18 - SQL you can do this with a concatenation
515:21 - operator which is basically taking two
515:24 - strings and putting them together into
515:26 - one string now if I do this here I will
515:30 - get
515:31 - um something like
515:34 - this and so this looks a bit weird and
515:38 - what I want to do here is to add an
515:40 - empty space in between and again
515:42 - concatenate it and now the names will
515:45 - look uh will look fine I also want to
515:49 - label this as
515:52 - member and this other column as facility
515:56 - to match the expected
516:00 - results next I need to ensure that there
516:05 - is no duplicate data so at the end of it
516:07 - all I will want
516:09 - to have distinct in order to remove
516:12 - duplicate
516:14 - rows and then I want to order the final
516:18 - result by member name and facility name
516:22 - so order by member
516:26 - and then
516:28 - facility and this will work because the
516:30 - order bu coming second to last coming at
516:34 - the end of our logical order of SQL
516:38 - operations over here the order by is
516:41 - aware of the alas is aware of the label
516:43 - that I have that I have put on the
516:47 - columns and here I get the results that
516:50 - I
516:53 - needed not a lot happening here to be
516:55 - honest it's just that we're joining
516:57 - three tables instead of two but it still
517:00 - works um just like uh any other join and
517:04 - then concatenating the
517:06 - strings filtering according to the
517:09 - facility
517:11 - name and then removing duplicate rows
517:14 - and finally
517:17 - ordering produce a list of costly
517:20 - bookings so we want to see all bookings
517:24 - that occurred in this particular
517:25 - particular day and we want to see how
517:27 - much they cost the member and we want to
517:29 - keep the bookings that cost more than
517:33 - $30
517:35 - so clearly in this case we also need the
517:38 - information from from all tables because
517:41 - if you look at the expected results we
517:43 - want the name of the member which is in
517:47 - the members table the name of the
517:49 - facility which is in the facilities
517:51 - table and the cost for which we will
517:54 - need the booking table so we need to
517:57 - start with a join of these three tables
517:59 - and since we did it already in the last
518:02 - exercise I have copied the code for that
518:05 - uh join so if you want more detail on
518:07 - this go and check the last exercise as
518:10 - well as I have copied the code to get
518:13 - the first name of the member by
518:14 - concatenating strings and the name of
518:17 - the of the
518:19 - facility now we need to calculate the
518:23 - cost of each booking so how does it work
518:26 - looking at our data so we have here a
518:28 - list of bookings and um a booking is
518:32 - defined as a number of slots and a slot
518:35 - is a one uh is a 30 minute usage of that
518:39 - facility and then we also have mid which
518:43 - tells us whether the member is a guest
518:46 - or not I mean whether the person is a
518:48 - guest or a member because if mid is zero
518:51 - then that person is a guest otherwise
518:53 - that person is a member and
518:55 - then I also know the facility that this
518:59 - person booked and if I go and look at
519:02 - the
519:03 - facility it has uh two different prices
519:06 - right one price uh is for members the
519:09 - other price is for guests and the price
519:12 - applies to the
519:14 - slots so we have all of the ingredients
519:18 - that we need for the cost in our join
519:21 - right and to convince ourselves of that
519:23 - let us actually select the here so in
519:27 - Booking I can see facility
519:29 - ID member ID and
519:33 - then
519:35 - slots and then in facility I can see the
519:40 - member cost the guest
519:43 - cost and I guess that's all I need
519:45 - really to calculate the
519:47 - cost and as you can see after the join
519:51 - I'm in a really good position because
519:53 - for each row I do have all of these
519:57 - values placed on each row so now I just
520:01 - have to figure out how to combine all of
520:03 - these values in order to get the
520:07 - cost now the way that I can get the cost
520:11 - is that I can look at the number of
520:14 - slots and then I need to multiply this
520:16 - by the right cost which is either member
520:19 - cost or guest cost and how do I know
520:23 - which of these to pick if it depends on
520:25 - the M ID if the M id M ID is zero then I
520:30 - will use the guess cost otherwise I will
520:33 - use the member
520:34 - cost so let me go back to my code
520:39 - here and after this I can
520:43 - say I want to take the
520:46 - slots and I want to multiply it by
520:49 - either member cost or guest cost now how
520:53 - can I put some logic in here that will
520:56 - choose uh either member cost or guest
520:59 - cost based on the ID of this person what
521:04 - can I use in order to make this Choice
521:07 - whenever I have such a choice to make I
521:10 - need to use a case statement so I can
521:13 - start with a case statement
521:16 - here and I will already write the end of
521:20 - it so that I don't forget
521:22 - it and then in the case statement M what
521:25 - do I need to check for I need to check
521:28 - that the member ID is
521:31 - zero in that case I will
521:35 - use the guest
521:37 - cost and in all other
521:40 - cases I will use the member cost so I'm
521:44 - taking slots and then I'm using this
521:46 - case
521:47 - when to decide by which column I'm going
521:51 - to multiply
521:53 - it and this is actually my
521:57 - cost now let's take a look at
522:00 - this and so I get this error that the
522:03 - column reference M ID is ambiguous so
522:06 - can you figure out why I got this
522:09 - error what's happening is that I have
522:11 - joined U multiple tables and the M ID
522:15 - column appears twice now in my join and
522:19 - so I cannot refer to it just by name
522:21 - because SQL doesn't know which column I
522:24 - want so I have to to reference the
522:25 - parent of the column every time I use it
522:28 - so here I will say that it comes from
522:31 - the booking
522:34 - table and now I get my
522:38 - result so if I see
522:41 - here then um I can see that I have
522:44 - successfully calculated my cost and
522:47 - let's look at the first row uh first
522:50 - it's um the me ID is not zero therefore
522:54 - it's a member and here the member cost
522:56 - is zero meaning that this facility is
522:58 - free for members so regardless of the
523:00 - slots the cost will be zero and let's
523:04 - look at one who is a guest so this one
523:07 - uh is clearly a a guest and they have uh
523:10 - taken one slot and the member cost is
523:12 - zero but uh so it's free for members but
523:15 - it costs five per slot for guests so the
523:18 - total cost is five So based on this
523:20 - sanity check the cost looks
523:23 - good now I need to actually filter my
523:26 - table because we have um we should
523:29 - consider only bookings that occurred in
523:31 - a certain day so after creating my new
523:35 - table uh and joining I can write aware
523:38 - filter to drop the rows that I don't
523:40 - need and I can say this is the the time
523:44 - column that I'm interested
523:46 - in the start time needs to be equal to
523:49 - this date over here
523:55 - and we have seen before that this will
523:57 - not work because start time is a Tim
524:00 - stamp it also shows hour um minute and
524:04 - seconds whereas here is just a date so
524:06 - this comparison will fail and so before
524:09 - I do the comparison I need to take this
524:12 - and reduce it to a date so that I'm
524:14 - comparing Apples to Apples on the
524:18 - time check that that didn't break
524:20 - anything now we should have
524:22 - significantly fewer rows
524:27 - so now what we need to do is to only
524:30 - keep rows that have a cost which is
524:32 - higher than 30 so can I go here and say
524:37 - end cost bigger than
524:41 - 30 no I cannot do it column cost does
524:44 - not exist right typical mistake but if
524:47 - you look at the logical order of SQL
524:50 - operations first you have the sourcing
524:52 - of the data then you have the wear
524:54 - filter and then all of the logic um by
524:58 - which we calculate the cost happens here
525:00 - and the label cost happens here as well
525:03 - so we cannot um filter on this column on
525:07 - the column cost because the we component
525:10 - has no idea about the uh column
525:13 - cost so this will now
525:16 - work but what we can do is to take all
525:20 - of the logic we've done until now and
525:24 - wrap it in round
525:25 - brackets and then introduce a Common
525:28 - Table
525:29 - expression and call this T1 so I will
525:33 - say with T1 as and then I can from T1
525:38 - and now I can use my filter right so
525:42 - cost bigger than 30 I can
525:47 - select star from this
525:53 - table and I'm starting to get somewhere
525:56 - because the cost has been successfully
526:00 - filtered now I have a lot of columns
526:02 - that I don't want in my final result
526:04 - that I used to help me reason about the
526:06 - cost so I want to keep member
526:11 - and I want to keep the
526:15 - facility but I don't want to keep any of
526:18 - these
526:27 - great now as a final step I need to
526:30 - order by cost descending and there's
526:35 - actually
526:36 - a issue that I have because I copy
526:39 - pasted code from the previous exercise I
526:42 - kept a distinct and you have to be very
526:44 - careful with this especially if you copy
526:47 - paste code anyway for learning it would
526:49 - be best to write it always from scratch
526:51 - but the distinct will remove uh rows
526:54 - that are duplicate and can actually
526:56 - cause an issue now I remove the distinct
526:59 - and I get the um solution that I want
527:03 - and if you look here we have if you look
527:06 - at the last two rows you can see that
527:08 - they're absolutely identical and so the
527:10 - distinct would remove them but there are
527:13 - two uh bookings that happen to be just
527:16 - the same uh in our data and we want to
527:19 - keep them we don't want to delete them
527:21 - so having distinct was a mistake in this
527:23 - case
527:25 - to summarize what we did here first we
527:30 - joined all the tables so we could have
527:32 - all the columns uh that we needed side
527:34 - by side and then we filtered on on the
527:37 - date pretty
527:39 - straightforward and then we took the
527:42 - first name and surname and um
527:44 - concatenated them together as well as
527:46 - the facility name and then we computed
527:49 - the cost and to compute the cost we got
527:52 - the number of slots and we used used a
527:55 - case when to multiply this by either the
527:58 - guest cost or the member cost according
528:00 - to the member's ID and at the end we
528:03 - wrapped everything in a Common Table
528:06 - expression so that we could filter on
528:08 - this newly computed value of cost and
528:10 - keep only those bookings that had a cost
528:14 - higher than
528:16 - 30 now I am aware that the question said
528:19 - not to use any
528:20 - subqueries technically I didn't because
528:23 - this is a common table expression but if
528:25 - you look at the author solution it is
528:27 - slightly different than ours so here
528:30 - they did basically the same thing that
528:32 - we did to compute the the cost except
528:35 - that in the case when they inserted the
528:38 - whole uh expression which is fine works
528:41 - just the
528:42 - same the difference is that um in this
528:46 - case they added a lot of logic in the we
528:50 - filter so that they could use a we
528:52 - filter in the first query so clearly
528:55 - they didn't use any columns that were
528:57 - added at the stage of the select they
529:00 - didn't use cost for example because like
529:02 - we said that wouldn't be possible
529:04 - so what they did is that they added the
529:08 - date filter over here and then in this
529:11 - case they added a um logical
529:16 - expression and in this logical
529:19 - expression either one of these two
529:22 - needed to be true for us to keep the row
529:26 - either the M ID is zero meaning that
529:29 - it's a it's a guest and so the
529:33 - calculation based on Guess cost ends up
529:35 - being bigger than 30 or the M ID is not
529:38 - zero which means it's a member and then
529:41 - this calculation based on the member
529:44 - cost ends up being bigger than
529:47 - 30 so this works I personally think that
529:52 - there's quite some repetition of the
529:55 - cost
529:56 - calculation both by putting it in the we
529:59 - filter and by uh putting it inside the
530:02 - case when and so I think that uh the
530:05 - solution we have here is a bit cleaner
530:08 - because we're only calculating cost once
530:11 - uh in this case and then we're simply
530:14 - referencing it thanks to the Common
530:16 - Table expression so if you look at the
530:19 - mental models course you will see that I
530:21 - warmly recommend not repeating logic in
530:24 - the code and using Common Table
530:26 - Expressions as often as possible because
530:28 - I think that they made the code uh
530:31 - clearer and um simpler to to
530:35 - understand produce a list of all members
530:38 - and the recommender without any
530:40 - joins now we have already Sol solved
530:43 - this
530:44 - problem and we have solved it with a
530:46 - self join as you remember we take the
530:49 - members table and join it on itself so
530:52 - that we can get this uh recommend by ID
530:56 - and plug it into members ID and then see
530:58 - the names of both the member and the
531:01 - recommender side by side but here we are
531:04 - challenged to do it without a
531:06 - join so let us go to the members
531:11 - table and let us select the first
531:16 - name and the
531:22 - surname now we actually want want to
531:24 - concatenate these two into a single
531:29 - string and call this
531:37 - member now how can we get data about the
531:40 - recommender without a self-join
531:43 - typically when you have to combine data
531:46 - you always have a choice between a join
531:49 - in a subquery right
531:53 - so what we we can do is to have a
531:55 - subquery here which looks at
531:59 - the recommended by ID from this
532:03 - table and um goes back to the members
532:07 - table and gets the the data that we need
532:10 - so let's see how that would
532:13 - look let us give an alias to this table
532:16 - and call it Ms and now we need to go
532:19 - back to this table inside the subquery
532:22 - and we can call it Rex
532:25 - and we want to select
532:27 - again the first name and surname like
532:31 - we're doing
532:32 - here and how are we able to identify the
532:36 - right row inside this
532:40 - subquery we can use aware
532:43 - filter and we want the Rex M ID to be
532:48 - equal to the Mims
532:51 - recommended
532:53 - by value
532:56 - and once we get this value we can call
532:58 - this
533:03 - recommender and now we want to avoid
533:05 - duplicates so after our outer select we
533:08 - can say distinct which will remove any
533:10 - duplicates from the
533:12 - result and then we want to
533:17 - sort I guess
533:19 - by member and recommender
533:25 - and here we get our
533:26 - result so replacing a join with a
533:31 - subquery so we go row by Row in members
533:36 - and
533:37 - then we take the recommended by ID and
533:42 - then we query the members table again
533:45 - inside the subquery and we use the wear
533:47 - filter to plug in that recommended by
533:51 - and find the row where the mem ID is
533:53 - equal to it
533:54 - and then getting first name and surname
533:56 - we get the data about the
533:58 - recommender and uh and that's how we can
534:00 - do
534:01 - it in the mental models course we
534:04 - discuss the subqueries and um and this
534:08 - particular case we talk about a
534:10 - correlated
534:12 - subquery why is this a correlated
534:14 - subquery because you can imagine that
534:17 - the the query that is in
534:20 - here it runs again for every row because
534:23 - for every row row I have a different
534:25 - value recommended by and I need and I
534:28 - need to plug this value into the members
534:30 - table to get the data about the
534:32 - recommender so this is a correlated
534:35 - subquery because it runs uh every time
534:38 - and it is different for every row of the
534:40 - members
534:42 - table produce a list of costly bookings
534:44 - using a subquery so this is the
534:47 - exact exercise that we did before and as
534:51 - you will remember uh we actually ignored
534:54 - it instructions a bit and we did use not
534:56 - a subquery but a Common Table expression
534:59 - and by reference this is the code that
535:00 - we used and this code works with that
535:03 - exercise as well and we get the
535:06 - result so you can go back to that
535:08 - exercise to see the logic behind this
535:11 - code and why this
535:14 - works and if we look at the author's uh
535:17 - solution they are actually using a
535:19 - subquery instead of a common table
535:22 - expression so they have an outer quer
535:24 - query which is Select member facility
535:26 - cost from and then instead of the from
535:30 - instead of telling the name of the table
535:32 - they have all of this logic here in this
535:34 - subquery which they call bookings and
535:37 - finally they they add a filter and
535:40 - order now this is technically correct it
535:43 - works but I'm not a fan of uh of writing
535:46 - queries like this I prefer writing them
535:50 - like this as a common table expression
535:52 - and I explain this in detail in my
535:54 - mental models
535:56 - course the reason I prefer this is
535:58 - because U it doesn't break queries apart
536:03 - so in my case this is one query and this
536:07 - is another query and it's pretty easy
536:09 - and simple to read however in this case
536:12 - you will start reading this query and
536:14 - then it is broken uh in in two by
536:18 - another query and when people do this
536:21 - sometimes they go even further and here
536:23 - when you have the from instead of a
536:26 - table you have yet another subquery it
536:28 - gets really complicated um so because of
536:32 - these uh two approaches are equivalent I
536:35 - definitely recommend going for a Common
536:37 - Table expression every time and avoiding
536:39 - subqueries unless they are really
536:42 - Compact and you can fit them in one
536:45 - row let us now get started with
536:48 - aggregation exercises and the first
536:51 - problem count the number of facilities
536:55 - so I can go to the facilities
536:58 - table and
536:59 - then when I want to count the number of
537:02 - rows in a table and here every row is a
537:06 - facility I can use the countar
537:09 - aggregation and we get the count of
537:13 - facilities so what we see here is a
537:16 - global
537:18 - aggregation and when you run an
537:20 - aggregation without having done any
537:22 - grouping it runs on the whole table
537:25 - therefore it will take all the rows of
537:28 - this table no matter how many compress
537:31 - them into one number which is determined
537:34 - by the aggregation function in this case
537:36 - we have a count and it returns a total
537:39 - of nine
537:41 - rows so in our map aggregation happens
537:46 - right here so we Source the table we
537:49 - filtered it if needed and then we might
537:52 - do a grouping which we didn't do in this
537:54 - case but whether we do it or not
537:57 - aggregations happen here and if grouping
537:59 - didn't happen the aggregation is at the
538:01 - level of the whole
538:04 - table count the number of expensive
538:06 - facilities this is similar to the
538:08 - previous exercise we can go to the
538:12 - facilities table but here we can add a
538:15 - filtering because we're only interested
538:18 - in facilities that have guest
538:20 - cost greater than or equal to 10
538:24 - and now once again I
538:27 - can get my aggregation count star to
538:31 - count the number of rows of this
538:32 - resulting
538:34 - table looking again at our map why does
538:37 - this work because with the from We're
538:40 - sourcing the table and immediately after
538:43 - the wear runs and it drops unneeded rows
538:46 - and then we can decide whether to group
538:47 - by or not and in our case in this case
538:49 - we're not doing it um but then the
538:52 - aggregations Run so by the time the
538:55 - aggregations run I've already dropped
538:58 - the rows in the wear and this is why in
539:00 - this case after dropping some rows the
539:03 - aggregation only sees six rows which is
539:06 - what we
539:08 - want count the number of recommendations
539:11 - each member makes so in the members
539:15 - table we have a field which is
539:17 - recommended by and here is the ID of the
539:21 - member who recommended the member that
539:24 - that this row is
539:25 - about so now we want to get all these uh
539:29 - recommended by values and count how many
539:32 - times they
539:34 - appear so I can go to my members
539:38 - table and what I need to do here is to
539:41 - group by recommended
539:44 - by so what this will do is that it will
539:48 - take all the unique values of this
539:50 - column recommended
539:52 - by and then you will allow me to do an
539:55 - aggregation on all of the rows in which
539:58 - those values
540:00 - occur so now I can go here to
540:03 - select and call this column again and if
540:07 - I run this query I get all the unique
540:10 - values of recommended buy without any
540:16 - repetitions and now I can run an
540:18 - aggregation like count
540:20 - star what this will do is that for
540:23 - recomend recomended by value 11 it will
540:26 - run this aggregation on all the rows in
540:29 - which recommended by is
540:32 - 11 and the aggregation in this case is
540:36 - Count star which means that it will
540:38 - return the number of rows in which 11
540:41 - appears which in the result happens to
540:43 - be one and so on for all the
540:47 - values what I also want to do is to
540:50 - order by recommended buy to match the
540:52 - expected results
540:55 - now what we get here is almost correct
540:59 - we see all the unique values of this
541:00 - column and we see the number of times
541:03 - that it appears in our
541:04 - data but there's one discrepancy which
541:07 - is this last row over
541:10 - here so in this last row you cannot see
541:13 - anything which means that it's a null
541:15 - value so it's a value that represents
541:17 - absence of data and why does this occur
541:21 - if you look at the original recommended
541:23 - by column
541:24 - there is a bunch of null values in this
541:26 - column because there's a bunch of member
541:28 - that have null in recommended by so
541:31 - maybe we don't know who recommended them
541:33 - or maybe they weren't recommended they
541:35 - just applied
541:37 - independently when you group bu you take
541:40 - all the unique values of the recommended
541:41 - by column and that includes the null
541:43 - value the null value defines a group of
541:46 - its own and the count works as expected
541:49 - because we can see that there are nine
541:51 - members for whom we don't have the
541:54 - recommended by
541:56 - value but the solution does not want to
542:00 - see this because we only want to see the
542:02 - number of recommendations each member
542:05 - has made so we actually need to drop
542:08 - this row therefore how how can I drop
542:11 - this
542:13 - row well it's as simple as going to uh
542:17 - after the from and putting a simple
542:20 - filter and saying recommended by is not
542:23 - not
542:26 - null and this will drop all of the rows
542:29 - in which in which that value is null
542:32 - therefore we won't appear in the
542:33 - grouping and now our results are
542:37 - correct remember when you're checking
542:39 - whether a value is null or not you need
542:42 - to use the is null or is not null you
542:45 - cannot actually do equal or um not
542:50 - equal because um null is not an act ual
542:54 - value it's just a notation for the
542:57 - absence of a value so you cannot say
542:59 - that something is equal or not equal to
543:01 - null you have to say that it is not
543:06 - null let's list the total slots booked
543:10 - per facility now first question where is
543:13 - the information that I need the number
543:16 - of slots booked in the is in the CD
543:18 - bookings and there I also have the
543:20 - facility ID so I can work with that
543:24 - table and now how can I get the total
543:28 - slots for each
543:30 - facility I can Group by facility ID and
543:34 - then I can select that facility ID and
543:37 - within each unique facility ID what type
543:40 - of uh aggregation might I want to do in
543:44 - every booking we have a certain number
543:46 - of slots right and so we want to find
543:50 - all the bookings for a certain facility
543:52 - ID and then sum all the slots that are
543:56 - being booked so I can write sum of slots
544:01 - over
544:02 - here and then I want to name this column
544:06 - total slots uh looking at the expected
544:09 - results but this will actually not work
544:13 - because um it's it's two two separate
544:17 - words so I actually need to use quotes
544:19 - for
544:20 - this and remember I have to use double
544:23 - quotes because it's a column name so
544:25 - it's always double quotes for the column
544:27 - name and single quotes for pieces of
544:30 - text and finally I need to order by
544:33 - facility
544:36 - ID and I get the
544:39 - results so for facility ID zero we
544:43 - looked at all the rows where facility ID
544:45 - was zero and we squished all of this to
544:49 - a single value which is the unique
544:50 - facility ID and then we looked at all
544:53 - the slots that were occurring in these
544:55 - rows and then we compress them we
544:57 - squished them to a single value as well
545:00 - using the sum aggregation so summing
545:02 - them all up and then we get the slum the
545:04 - sum of the total
545:08 - slots list the total slots booked per
545:11 - facility in a given month so this is
545:14 - similar to the previous problem except
545:17 - that we are now isolating a specific
545:18 - time period And so let's us think about
545:22 - how we can um select bookings that
545:25 - happened in the month of September
545:29 - 2012 now we can go to the bookings
545:35 - table and select the start time
545:38 - column and to help our exercise I will
545:41 - order by start time uh
545:45 - descending and I will limit our results
545:47 - to
545:49 - 20 and you can see here that start time
545:52 - is a time stamp call and it goes down to
545:54 - the second because we have year month
545:57 - day hour minutes second so how can we
546:01 - check whether any of these dates is
546:04 - corresponds to September
546:09 - 2012 we could add a logical check here
546:12 - we could say that start time needs to be
546:14 - greater than or equal to 2012 September
546:20 - 1st and it needs to be strictly
546:24 - smaller than 2012
546:29 - October
546:33 - 1st and this will actually
546:37 - work as an alternative there is a nice
546:40 - function that we could
546:42 - use which is the
546:45 - following date
546:47 - trunk
546:51 - month start time let's see what that
546:54 - looks
546:56 - like so what do you think this function
546:59 - does like the name suggests it truncates
547:03 - the date to a specific U granularity
547:06 - that we choose here and so all of the
547:09 - months are reduced to the very first
547:12 - moment of the month in which they occur
547:17 - so it is sort of cutting that date and
547:20 - removing some information and reducing
547:22 - the granularity
547:24 - I could of course uh have other values
547:26 - here such as day and then every um time
547:31 - stem here would be reduced to its day
547:34 - but I actually want to use
547:37 - month and now that I have this I can set
547:40 - an equality and I can say that I want
547:42 - this to be equal to September
547:47 - 2012 and this will actually work and I
547:50 - also think it's nicer than the range
547:52 - that we showed before
547:55 - now I've taken the code for the previous
547:57 - exercise and copied it here because it's
547:59 - actually pretty similar except that now
548:02 - after we get bookings we need to insert
548:05 - a filter to isolate our time range and
548:09 - actually we can use this logical
548:12 - condition directly I'll delete all the
548:15 - rest and now what I need to do is to
548:18 - change the
548:20 - ordering and I actually need to order by
548:23 - the the total slots
548:25 - here and I get my
548:29 - result to summarize I get the booking
548:32 - table and then I uh take the start time
548:36 - time stamp and I truncate it because I'm
548:39 - only interested in the month of that of
548:42 - that time and then I make sure that the
548:44 - month is the one I actually
548:47 - need and then I'm grouping by facility
548:50 - ID and then I'm getting the facility ID
548:52 - and within each of those groups I'm
548:54 - summing all the
548:56 - slots and finally I'm ordering by this
548:59 - uh
549:00 - column list the total slots booked per
549:03 - facility per month in the year 2012 so
549:09 - again our data is in
549:12 - bookings and now we want to see how we
549:15 - how can we isolate the time period of
549:18 - the year 2012 for this
549:21 - table now once again I am looking at the
549:25 - start time column from bookings uh to
549:28 - see how we can extract the the year so
549:31 - in the previous exercise we we saw the
549:34 - date trunk function and we could apply
549:36 - it here as well so we could say date
549:39 - trunk start
549:43 - time um Year from start time right
549:46 - because we want to see it at the Year
549:49 - resolution and then we will get
549:50 - something like this and then we could
549:53 - check that this is equal to
549:56 - 2012
549:59 - 0101 and this would actually work but
550:02 - there's actually a better way to do
550:04 - it what we could do here is that we
550:07 - could say
550:09 - extract Year from start
550:13 - time and when we look at here we got a
550:18 - integer that actually represents the
550:21 - year and it will be easy now to just say
550:24 - equal to
550:26 - 2012 and make that
550:28 - test so if we look at what happened
550:34 - here extract is a different function
550:37 - than date time because extract is
550:39 - getting the year and outputting it as an
550:41 - integer whereas date time is still
550:44 - outputting a time stamp or a date just
550:46 - with lower granularity so you have to
550:50 - use one or another according to your
550:52 - needs
550:54 - now to proceed with our
550:56 - query we can get CD
550:59 - bookings and add a filter here and
551:03 - insert this expression in the filter and
551:06 - we want the year to be
551:09 - 2012 so this will take care of isolating
551:12 - our desired time
551:15 - period next we want to check the total
551:19 - slots within groups defined by facility
551:23 - fac ID and month so we want a total for
551:25 - each facility for each month as you can
551:28 - see here in the respected results such
551:31 - that we can say that for facility ID
551:34 - zero in the month of July in the year
551:37 - 2012 we uh booked to 170 slots so let's
551:41 - see how we can do that this basically
551:44 - means that we have to group by multiple
551:46 - values right and facility ID is easy we
551:50 - have it however we do not have the month
551:53 - so how can we extract the month from the
551:56 - start time over
551:58 - here well we can use the extract
552:00 - function right which is which we just
552:03 - saw so if we write it like this and we
552:06 - put month
552:08 - here um this function will look at the
552:10 - month and then we'll output the month as
552:13 - an actual
552:15 - integer and um the thing is that I can
552:19 - Group by uh the names of columns but I
552:22 - can also Group by Transformations on
552:25 - columns it works just as well SQL will
552:28 - compute uh this expression over here and
552:32 - then it will get the value and then it
552:33 - will Group by that
552:35 - value now when it comes to getting the
552:37 - columns what I usually do is that when I
552:40 - group by I want to see the The Columns
552:44 - in which I grouped so I just copy what I
552:46 - had
552:47 - here and I add it to my query and then
552:53 - what aggregation do I want to do within
552:55 - the groups defined by these two
552:58 - columns I have seen it in the previous
553:00 - exercise I want to sum over the the
553:05 - slots and get the
553:08 - total
553:10 - slots I also want to take this column
553:15 - over here and rename it as
553:18 - month and now I have to order by
553:24 - ID and
553:27 - month and we get the data that we
553:32 - needed so what did we learn with this
553:35 - exercise we learned to use the extract
553:38 - function to get a number out of a date
553:42 - and we use that we have used uh grouping
553:45 - by multiple
553:48 - columns which simply defines a group as
553:51 - the combination of the unique values of
553:54 - two or more columns that's what multiple
553:57 - grouping
553:58 - does we have also seen that not only you
554:02 - can Group by providing a column name but
554:05 - you can also Group by a logical
554:09 - operation and you should then reference
554:11 - that same operation in the select
554:14 - statement so that you can get the uh
554:17 - value that was
554:20 - obtained find the count of members who
554:23 - have made at least one booking so where
554:26 - is the data that we need it's in the
554:30 - bookings table and for every booking we
554:33 - have the ID of the member who has made
554:35 - the
554:36 - booking so I can
554:39 - select this
554:42 - column and clearly I can run a count on
554:46 - this column and the count will return
554:49 - the number of nonnull values
554:57 - however this count as you can see is
555:00 - quite
555:03 - inflated What's Happening Here is that
555:06 - uh a single member can make any number
555:09 - of bookings and now we're basically
555:11 - counting all the bookings in
555:14 - here but if I put distinct in
555:19 - here then I'm only going to count
555:23 - the unique values of
555:27 - mid in my booking
555:29 - table and this give me gives me the
555:33 - total number of members who have made at
555:36 - least one
555:39 - booking so count will get you the count
555:42 - of non-null values and count distinct
555:45 - will get you the count of unique nonnull
555:50 - values list the facilities with more
555:53 - than 1,000 slots
555:55 - booked
555:57 - so what do we need to do here we need to
556:01 - look at each facility and how many slots
556:05 - they each booked so where is the data
556:07 - for this as you can see again the data
556:10 - is in the bookings
556:13 - table now I don't need to do any filter
556:15 - so I don't need the wear statement but I
556:18 - need to count the total slots within
556:20 - each facility so I need a group pi
556:24 - and I can Group by the facility
556:26 - ID and once I do that I can select the
556:29 - facility
556:32 - ID and to get the total slots I can
556:35 - simply do sum of
556:37 - slots and I can call
556:39 - this total slots it's double quotes for
556:43 - a column
556:44 - name now I need to add the filter I want
556:47 - to keep those that have some of slots
556:50 - bigger than 1,000 and I cannot do it in
556:53 - a where statement right so if I were to
556:56 - write this in a where statement I would
557:00 - get that aggregate functions are not
557:02 - allowed in wear and if I look at my map
557:06 - uh we have been through this again the
557:08 - wear runs first right after we Source
557:11 - the data whereas aggregations happens
557:14 - happen later so the wear cannot be aware
557:17 - of any aggregations that I've
557:19 - done for this purpose we actually have
557:22 - the having component so the having
557:25 - component works just like wear it's a
557:28 - filter it drops rows based on logical
557:31 - conditions the difference is that having
557:34 - runs after the aggregations and it works
557:36 - on the aggregations so I get the data do
557:40 - my first filtering then do the grouping
557:43 - compute an aggregation and then I can
557:45 - filter it again based on the result of
557:47 - the
557:49 - aggregation so I can now now go to my
557:51 - query and take this and put having
557:55 - instead of
557:56 - where
557:59 - and place it after the group
558:03 - pi and we get our
558:06 - result and all we need to do is to order
558:11 - bu facility
558:15 - ID and we get our
558:19 - result find the total revenue of each
558:22 - facility
558:23 - so we want a list of facilities by name
558:27 - along with their total
558:31 - revenue first question as always where
558:34 - is my data so if I want facility's name
558:38 - it's in the facilities
558:40 - table but to calculate the revenue I
558:43 - need to know about the bookings so I'll
558:45 - actually need to join on both of these
558:49 - tables so I will write from CD bookings
558:53 - book join CD
558:56 - facilities
558:58 - fact on facility
559:05 - ID next I will want the total revenue of
559:09 - the
559:10 - facilities but I don't even have the
559:14 - revenue yet so my first priority should
559:17 - be to compute the
559:20 - revenue let us first select the facility
559:26 - name and here I will now need to add the
559:31 - revenue so to do that I will need to
559:34 - have something like cost times slots and
559:39 - that determines the revenue of each
559:42 - booking however I don't have a single
559:45 - value for cost I have two values member
559:49 - cost and guest cost and as you remember
559:52 - from previous exercises I need to choose
559:55 - every time which of them to apply and
559:58 - the way that I can choose is by looking
560:00 - at the member ID and if it's zero then I
560:05 - need to use the guest cost otherwise I
560:08 - need to use the member cost so what can
560:10 - we use now in order to choose between
560:13 - these two variants for each
560:16 - booking we can use the case statement
560:19 - for this so I will say
560:24 - case and then immediately close it with
560:26 - end and I'll say
560:30 - when uh book M ID equals zero
560:34 - then Fox guest cost I always need to
560:37 - reference the parent Table after a join
560:40 - to avoid
560:42 - confusion else
560:46 - fax member cost so this will allow me to
560:52 - get the C cost dynamically it allows me
560:54 - to choose between two
560:57 - columns and I can multiply this by slots
561:03 - and get the
561:05 - revenue now if I run this I get this
561:09 - result which is the name of the facility
561:12 - and the
561:13 - revenue but I need to ask myself at what
561:17 - level am I working here in other words
561:21 - what does each row represent
561:23 - well I haven't grouped yet so each row
561:26 - here represents a single
561:29 - booking having joined bookings and
561:33 - facilities and not having grouped
561:35 - anything we are still at the level of
561:37 - this table where every row represent a
561:40 - single booking so to find the total
561:43 - revenue for each facility I now need to
561:45 - do an
561:47 - aggregation I need to group by facility
561:50 - name and then sum all all the
561:53 - revenue I can actually do this within
561:56 - the same query by saying Group
562:01 - by facility
562:04 - name and if I run this I will now get an
562:09 - error can you figure out why I'm getting
562:13 - this error
562:15 - now so I have grouped by facility name
562:19 - and then I'm selecting by facility name
562:22 - and that works well because now this
562:24 - column has been squished has been
562:26 - compressed to show only the unique names
562:29 - for each
562:30 - facility however I am then adding
562:33 - another column which is revenue which I
562:36 - have not compressed in any way therefore
562:40 - this column has a different number of
562:42 - rows than than this
562:44 - column and the general rule of grouping
562:47 - is that after I group by one or more
562:51 - columns I can
562:53 - select by The Columns that are in the
562:57 - grouping and
562:59 - aggregations right so nothing else is
563:01 - allowed so fax name is good because it's
563:05 - in the grouping revenue is not good
563:08 - because it's not in the grouping and
563:09 - it's not an
563:11 - aggregation and to solve this I can
563:13 - simply turn it into an
563:15 - aggregation by doing sum over here
563:22 - and when I run this this actually
563:26 - works and now all I need to do is to
563:29 - sort by Revenue so if I say order by
563:36 - Revenue I will get the result that I
563:39 - need so there's a few things going on
563:42 - here but I can understand it by looking
563:44 - at my map now what I'm doing is that I'm
563:48 - first sourcing the data and I'm actually
563:51 - joining two tables
563:53 - in order to create a new table where my
563:55 - data
563:56 - is then I'm grouping by a c a column
564:01 - which is the facility name so this
564:03 - compresses the column to all the unique
564:05 - facility
564:06 - name and next I run the aggregation
564:10 - right so the aggregation can be a sum
564:14 - over an existing column but as we saw in
564:17 - the mental models course the aggregation
564:19 - can also be a sum over a calculation I
564:23 - can actually run logic in there it's
564:25 - very
564:26 - flexible so if I had a revenue column
564:29 - here I would just say sum Revenue as
564:32 - revenue and it would be simpler but I
564:34 - need to do some to put some logic in
564:37 - there and uh this logic involves uh
564:40 - choosing whether to get guest cost or
564:42 - member cost but I'm perfectly able to
564:44 - put that logic inside the sum and so SQL
564:47 - will first evaluate this Logic for each
564:50 - row and then um it will sum up all the
564:54 - results and it will give me
564:57 - Revenue finally after Computing that
565:00 - aggregation I uh select the columns that
565:03 - I
565:04 - need and then I do an order buy at the
565:10 - end find facilities with a total revenue
565:14 - of less than
565:15 - 1,000 so the the question is pretty
565:19 - clear but wait a second we calculate
565:22 - ated the total revenue by facility in
565:24 - the previous exercise so we can probably
565:27 - just adapt that code here's the code
565:29 - from the previous exercise so check that
565:31 - out if you want to know how I wrote
565:34 - this and if I run this code I do indeed
565:37 - get the total revenue for for each
565:40 - facility and now I just need to keep
565:42 - those with a revenue less than 1,000 so
565:45 - how can I do that it's a filter right I
565:48 - need to filter on this Revenue column um
565:52 - I cannot use a wear
565:54 - filter because this uh revenue is an
565:57 - aggregation and it was computed after
565:59 - the group buy after the wear so the wear
566:01 - wouldn't be aware of that uh column but
566:05 - as we have seen there is a
566:08 - keyword there is a statement called
566:10 - having which does the same job as where
566:12 - it filters based on logical conditions
566:15 - however it works on aggregations so I
566:17 - could say having Revenue smaller than
566:21 - 1,000
566:24 - unfortunately this doesn't
566:26 - work can you figure out why this doesn't
566:30 - work in our query we do a grouping and
566:34 - then we compute an
566:36 - aggregation and then we give it a
566:39 - label and then we try to run a having
566:43 - filter on this
566:45 - label if you look now at our map for The
566:48 - Logical order of SQL
566:50 - operations this is where the group by
566:53 - happens this is where we compute our
566:56 - aggregation and this is where having
566:59 - runs and now having is trying to use the
567:02 - Alias that comes at this step but
567:06 - according to our rules having does not
567:08 - know of the Alias that's assigned at
567:11 - this step because it hasn't happened
567:17 - yet now as the discussion for this
567:20 - exercise says there are in fact database
567:24 - systems that try to make your life
567:25 - easier by allowing you to use labels in
567:28 - having but that's not the case with
567:30 - postgress so we need a slightly
567:32 - different solution
567:34 - here note that if I repeated all of my
567:37 - logic in here instead of using the label
567:40 - it would work so if I do
567:44 - this I will get my result I just need to
567:48 - order by Revenue
567:53 - and you see that I get the correct
567:56 - result why does it work when I put the
567:58 - whole logic in there instead of using
568:00 - the label once again the logic happens
568:06 - here and so the having is aware of this
568:08 - logic having
568:10 - happened but the having is just not
568:13 - aware of the
568:16 - Alias however I do not recommend
568:20 - repeating logic like this in your
568:22 - queries because it increases the chances
568:25 - of errors and it also makes them less
568:28 - elegant less readable so the simpler
568:30 - solution we can do here is to take this
568:34 - original query and put it in round
568:37 - brackets and then create a virtual table
568:40 - using a Common Table
568:43 - expression here and call this all of
568:46 - these
568:47 - T1 and then we can treat T1 like any
568:51 - other table
568:52 - so I can say from
568:55 - T1 select
568:58 - everything
569:01 - where
569:03 - revenue is smaller than
569:06 - 1,000 and then order by
569:09 - Revenue remove all
569:13 - this and we get the correct
569:16 - answer to summarize you can use having
569:19 - to filter on the result of aggregation
569:22 - ations unfortunately in postest you
569:24 - cannot use the labels that you assign to
569:26 - aggregations in having so if it's a
569:29 - really small
569:31 - aggregation like if it's select some
569:35 - revenue and then all of the rest then
569:38 - it's fine to say sum Revenue smaller
569:41 - than 1,000 there's a small repetition
569:44 - but it's not an issue however if your
569:47 - aggregation is more complex as in this
569:49 - case you don't really want to repeat it
569:51 - and then your forced to add an extra
569:53 - step to your query which you can do with
569:55 - a common table
569:57 - expression output the facility ID that
570:00 - has the highest number of slots
570:03 - booked so first of all we need to get
570:07 - the number of slots booked by facility
570:11 - and we've actually done it before but
570:13 - let's do it again where is our data the
570:16 - data is in the booking
570:20 - table and uh we don't need to filter
570:23 - this table but we need we do need to
570:26 - group by the facility ID and then once
570:30 - we do this we can select the facility ID
570:33 - this will isolate all the unique values
570:35 - of this column and within each unique
570:37 - value we can sum the number of slots and
570:41 - call this total slots and if we do this
570:46 - we get the total slots for each
570:49 - facility now to get the top one the
570:51 - quickest solution really would be to
570:54 - order
570:55 - by total slots and then limit the result
570:59 - to
571:03 - one however this would give me the one
571:06 - with the smallest number of slots
571:08 - because order is ascending by default so
571:11 - I need to turn this into descending and
571:14 - here I would get my
571:15 - solution but given that this is a simple
571:19 - solution and it solved our exercise can
571:21 - you imagine a situation in which this
571:23 - query would not achieve what we wanted
571:26 - it
571:27 - to let us say that there were multiple
571:31 - facilities that had the top number of
571:34 - total slots so the top number of slots
571:38 - in our data set is 1404 that's all good
571:41 - but let's say that there were two
571:42 - facilities that had this uh this top
571:45 - number and we wanted to see both of them
571:48 - for our business
571:50 - purposes what would happen here is that
571:53 - limit one so the everything else would
571:56 - work correctly and the ordering would
571:58 - work correctly but inevitably in the
572:01 - ordering one of them would get the first
572:03 - spot and the other would get the second
572:04 - spot and limit one is always cutting the
572:07 - output to a single row therefore in this
572:11 - query we would only ever see one
572:14 - facility ID even if there were more that
572:17 - had the same number of top slots so how
572:20 - can we solve
572:23 - this clearly in instead of combining
572:26 - order by and limit we need to figure out
572:29 - a filter we need to filter our table
572:34 - such that only the facilities with the
572:37 - top number of slots are
572:40 - returned but we cannot really get the
572:43 - maximum of some
572:46 - slots in this query
572:52 - because if I tried to
572:55 - do having some slots equals maximum of
572:59 - some
573:03 - slots I would be told that aggregate
573:06 - function calls cannot be
573:08 - nested and if I go back to my map I can
573:12 - see that having can only run after all
573:16 - the aggregations have completed but what
573:18 - we're trying to do here is to add a new
573:21 - aggregation inside having and that
573:23 - basically doesn't
573:25 - work so the simplest solution here is to
573:29 - just wrap all of this into a Common
573:33 - Table
573:34 - expression and then get this uh table
573:37 - that we've just defined and then select
573:42 - star
573:44 - where the total slots is equal to the
573:48 - maximum number of slots which we know to
573:50 - be 1404
573:54 - however we cannot hardcode the maximum
573:56 - number of slots because for one we might
573:59 - not know what it is and for and second
574:02 - it uh it will change with time so this
574:04 - won't work when the data changes so
574:07 - what's the alternative to hardcoding
574:09 - this we actually need some logic here to
574:12 - get the maximum value and we can put
574:15 - that logic inside the
574:17 - subquery and the subquery will go back
574:20 - to my table T1
574:22 - and you will actually find the maximum
574:24 - of total
574:27 - slots from T1 so first this query will
574:31 - run it will get the maximum and then the
574:34 - filter will check for that
574:36 - maximum and then I will get uh the
574:39 - required result and this won't break if
574:42 - there are many facilities that share the
574:44 - same top spot because we're using a
574:46 - filter all of them will be
574:49 - returned so this is a perfectly good
574:52 - solution for your information you can
574:55 - also solve this with a window
574:58 - function and um which is a sort of row
575:02 - level aggregation that doesn't change
575:03 - the structure of the data we've seen it
575:05 - in detail in the mental models course so
575:08 - what I can do here is to use a window
575:13 - function to get the maximum value over
575:16 - the sum of
575:18 - slots and then I can I will say over to
575:22 - make it clear that this is a window
575:24 - function but I won't put anything in the
575:27 - window definition because I I just want
575:29 - to look at my whole data set here and I
575:32 - can label this Max
575:35 - slots and if I look at the data
575:38 - here you can see that I will get the
575:41 - maximum for every row and then to get
575:44 - the correct result I can add a simple
575:46 - filter here saying that
575:49 - total slots should be equal to Max
575:53 - slots and I will only want to return
575:57 - facility ID and total
576:00 - slots so this also solves the
576:03 - problem what's interesting to note here
576:06 - for the sake of understanding window
576:08 - functions more deeply is that the
576:10 - aggregation function for this uh window
576:15 - Clause works over an aggregation as well
576:19 - so here we sum the total slots over each
576:23 - facility and then the window function
576:26 - gets the maximum of all of those uh
576:30 - value and this is quite a powerful
576:33 - feature um and if I look at my map over
576:36 - here I can see that it makes perfect
576:39 - sense because here is where we Group by
576:42 - facility ID and here is where we compute
576:45 - the
576:45 - aggregation and then the window comes
576:47 - later so the window is aware of the
576:49 - aggregation and the window can work on
576:51 - on
576:54 - that so A few different solutions here
576:57 - and overall um a really interesting
577:01 - exercise list the total slots booked per
577:04 - facility per month part two so this is a
577:08 - bit of a complex query but the easiest
577:11 - way to get it is to look at the expected
577:13 - results so what we see here is a
577:17 - facility ID and then within each month
577:20 - of the year
577:22 - 2012 we get the total number of
577:26 - slots and um at the end of it we have a
577:32 - null value here and for facility zero
577:35 - and what we get is the sum of all slots
577:38 - booked in
577:40 - 2012 and then the same pattern repeats
577:43 - repeats with every facility we have the
577:46 - total within each month and then finally
577:48 - we have the total for that facility in
577:51 - the year here so there's two level of
577:54 - aggregations here and then if I go at
577:56 - the end there's a third level of
577:58 - aggregation which is the total for all
578:02 - facilities within that
578:05 - year so there are three levels of
578:07 - aggregation here by increasing
578:09 - granularity it's total over the year
578:13 - then total by facility over the year and
578:17 - then finally total by Facility by month
578:22 - within that
578:23 - year so this is a bit breaking the mold
578:27 - of what SQL usually does in the sense
578:29 - that SQL is not designed to return a
578:33 - single result with multiple levels of
578:35 - aggregation so we will need to be a bit
578:38 - creative around that but let us start
578:41 - now with the lowest level of granularity
578:46 - let's get this uh this part right
578:48 - facility ID and month and and then we'll
578:51 - build on top of that so the table that I
578:54 - need is in the bookings
578:57 - table and first question do I need to
579:00 - filter this table yes because I'm only
579:04 - interested in the year
579:06 - 2012 so we have seen that we can use the
579:10 - extract
579:11 - function to get the
579:17 - year out of a Tim
579:19 - stamp which would be start
579:23 - time and we can use this function in a
579:26 - wear filter and what this function will
579:29 - do is that it will go to that time stamp
579:31 - and then we will get an integer out of
579:33 - it it will get a number and then we can
579:36 - check that this is uh the year that
579:38 - we're interested
579:40 - in and let's do a quick sanity check to
579:43 - make sure this
579:45 - worked so I will get some bookings here
579:48 - and they will all be in the year 2012
579:53 - next I need to Define my grouping right
579:55 - so I will need to group by facility ID
579:59 - but then I will also need to group by
580:02 - month however I don't actually have a
580:05 - column named uh month in this table so I
580:10 - need to calculate it I can calculate it
580:14 - once again with the extract function so
580:16 - I can say
580:18 - extract extract
580:21 - month from start
580:25 - time and once again this will go to the
580:28 - start time and sped out a integer which
580:32 - for this first row would be
580:37 - seven and uh as you know in the group bu
580:40 - I can select a column but I can also
580:43 - select an operation over a column which
580:46 - works just as well now after grouping I
580:49 - cannot do select star anymore but I want
580:51 - to see The Columns that I have grouped
580:56 - by and so let us do a quick sanity check
580:59 - on that it looks pretty good I get the
581:02 - facility ID and the month and I can
581:05 - actually label this
581:09 - month and next I simply need to take the
581:13 - sum over the slots within each facility
581:15 - and within each month
581:21 - and when I look at this I have my first
581:23 - level of granularity and you can see
581:26 - that the first row corresponds to the
581:28 - expected
581:29 - result now I need to add the next level
581:32 - of
581:33 - granularity which is the total within
581:36 - each
581:38 - facility so can you think of how can I
581:41 - add that next level of granularity to my
581:46 - results the key Insight is to look at
581:49 - this uh expected results table and to
581:51 - see it as multiple tables stacked on top
581:55 - of each
581:56 - other one table is the one that we have
581:59 - here and this
582:01 - is uh total
582:04 - by
582:06 - facility
582:09 - month a second table that we will need
582:13 - is the total by
582:15 - facility and then the third table that
582:17 - we will need is the overall total which
582:21 - you could see here at the
582:25 - bottom and how can we stack multiple
582:27 - tables on top of each each other with a
582:30 - union statement
582:33 - right Union will stack all the rows from
582:38 - my tables on top of each
582:41 - other so now let us compute the table
582:44 - which has the total by facility and I
582:47 - will actually copy paste what I have
582:49 - here and and I just need to remove a
582:53 - level of grouping right so if I do this
582:57 - I I will not Group by month
583:00 - anymore and I will not Group by month
583:04 - anymore and once I do this I get an
583:08 - error Union query must have the same
583:12 - number of columns so do you understand
583:15 - this error
583:16 - here so I will write a bit to show you
583:19 - what's happening so how does it work
583:22 - when we Union two
583:24 - tables let's say the first table in our
583:27 - case is facility
583:29 - ID month and then slots and then the
583:33 - second table if you look here it's
583:36 - facility ID and then
583:40 - slots now when you Union these two
583:43 - tables SQL assumes that you have the
583:46 - same number of columns and that the
583:48 - ordering is also identical so here we
583:52 - are failing because the first table has
583:54 - three columns and the second table has
583:57 - only two and not only We are failing
584:00 - because there's a numbers mismatch but
584:02 - we are also mixing the values of month
584:04 - and Slots now this might work because
584:07 - they're both integers so SQL won't
584:09 - necessarily complain about this but it
584:11 - is logically wrong so what we need to do
584:15 - is to make sure that when we're unioning
584:17 - these two tables we have the same number
584:20 - of columns and the same ordering as
584:22 - well but how can we do this given that
584:25 - the second table does indeed have one
584:28 - column less it does have less
584:30 - information so what I can do is to put
584:34 - null over here so what happens if I do
584:38 - select null this will create a column of
584:41 - a of constant value which is a column of
584:44 - all NS and then the structure will
584:48 - become like this
584:52 - now when I Union first of all I'm going
584:55 - to have the same number of columns so
584:57 - I'm not going to see this uh this error
585:00 - again that we have here and second in u
585:05 - the facility ID is going to be mixed
585:07 - with the facility ID slots is going to
585:09 - be mixed with slots which is all good
585:12 - and then month is going to be mixed with
585:14 - null which is what we want because in
585:16 - some cases we will have the actual month
585:19 - and in some cases we won't have any
585:22 - anything so I have added uh null over
585:26 - here and I am unioning the tables and if
585:30 - I run the query I can see that I don't
585:33 - get any error anymore and this is what I
585:37 - want so I can tell that this row is
585:40 - coming from the second table because it
585:43 - has null in the value of month and so
585:46 - it's showing the total slots for
585:49 - facility um zero in every month whereas
585:52 - this row came from the upper table
585:55 - because it's showing the sum of slots
585:56 - for a facility within a certain month so
585:59 - this achieves the desired result next we
586:03 - want to compute the last level of
586:05 - granularity which is the total so once
586:08 - again I will select my query over
586:12 - here and and I don't even need to group
586:17 - by anymore right because it's the total
586:20 - number of slots over the whole year so I
586:24 - can simply say sum of slots as
586:28 - slots and remove the
586:32 - grouping next I can add the Union as
586:36 - well so that that I can keep stacking
586:38 - these tables and if I run this I get the
586:41 - same error as
586:43 - before so going back to our little uh
586:46 - text over here we are now adding a third
586:49 - table and this table only has slots and
586:53 - of course I cannot this doesn't work
586:56 - because there's a mismatch in the number
586:57 - of columns and so the solution here is
587:01 - to also add a null column here and a
587:05 - null column here and so I have the same
587:09 - number of columns and Slots gets
587:11 - combined with slots and everything else
587:13 - gets filled with null
587:15 - values and I can do it here making sure
587:18 - that the ordering is correct so I will
587:20 - select
587:21 - null null and then sum of
587:26 - slots and if I run this query I can see
587:29 - that the result
587:32 - works the final step is to add
587:37 - ordering sorted by ID and month so at
587:42 - the end of all of these unions I can
587:46 - say order by facility ID
587:52 - one and I finally get my
587:58 - result so this is now the combination of
588:03 - three different tables stacked on top of
588:05 - each other that show different levels of
588:08 - granularity and as you can see here in
588:10 - the schema we added null columns to uh
588:14 - two of these tables just to make sure
588:17 - that they have the same number of
588:18 - columns and that they can stack up
588:21 - correctly and now if we look again at
588:24 - the whole
588:27 - query we can see that there are actually
588:30 - three select statements in this query
588:34 - meaning three tables which are
588:36 - calculated and then finally stack with
588:38 - Union and all of them they do some
588:41 - pretty straightforward aggregation the
588:44 - first one um Aggregates by facility ad
588:48 - and month after extracting the month the
588:51 - second one simply Aggregates by facility
588:53 - ID and the third one gets the sum of
588:56 - slots over the whole data without any
588:58 - grouping and then we are adding the null
589:01 - uh constant columns here to make the the
589:04 - column count
589:04 - [Music]
589:06 - match and it's also worth it to see this
589:10 - in our map of the SQL
589:13 - operations so here um you can see that
589:17 - this order is actually repeating for
589:19 - every table so for each of our three
589:22 - tables we are getting our data and then
589:24 - we are running a filter to keep the year
589:27 - 2012 and then we do a grouping and
589:30 - compute an aggregation and select the
589:33 - columns that we need adding null columns
589:35 - when necessary and then it repeats all
589:38 - over right so for the second table again
589:40 - the same process for the third table the
589:43 - same process except that in the third
589:45 - table we don't Group by and then when
589:48 - all three tables are done the union r
589:50 - runs the union runs and stacks them all
589:53 - up together and now instead of three
589:55 - tables I only have one table and after
589:58 - the union has run now I can finally
590:01 - order my table and return the
590:05 - result list the total hours booked per
590:08 - named
590:10 - facility so we want to get the facility
590:13 - ID and the facility name and the total
590:17 - hours that they've been booked keeping
590:20 - keeping in mind that what we have here
590:22 - are number of slots for each booking and
590:25 - a slot represents 30 minutes of
590:29 - booking now to get my data I will need
590:32 - both the booking table and the
590:34 - facilities table because I need both the
590:37 - information on the bookings and the
590:38 - facility name so I will get the bookings
590:46 - table and the facilities table
590:54 - and join them
590:55 - together next I don't really need to
590:58 - filter on anything but I need to group
591:01 - by facility so I will
591:04 - Group
591:07 - by facility ID and then I also need to
591:11 - group by facility name otherwise I won't
591:14 - be able to use this in the select part
591:17 - and now I can select these two columns
591:27 - and to get the total hours I will need
591:30 - to get the sum of the slots so I can get
591:33 - the total number of slots within each
591:36 - facility and I will need to divide this
591:38 - by two right so let's see what that
591:41 - looks
591:42 - like now superficially this looks
591:45 - correct but there's actually a pitfall
591:47 - in here and to realize a pitfall I will
591:51 - take some slots as well before dividing
591:54 - it by
591:56 - two and you can see it already in the
591:59 - first row
592:01 - 9911 ided by 2 is not quite
592:07 - 455 so what is happening
592:11 - here the thing is that in postgress when
592:14 - you take an integer number such as some
592:18 - slots the sum of the slots is an integer
592:21 - number and you divide by another
592:24 - integer postgress assumes that you you
592:27 - are doing integer Division and since you
592:29 - are dividing two integers it returns uh
592:32 - an integer as well so that means that um
592:36 - that the solution is not exact if you
592:39 - are thinking in floating Point
592:41 - numbers and the solution for this is
592:44 - that at least one of the two numbers
592:46 - needs to be a Flo floating Point number
592:50 - and so we can turn two into
592:52 - 2.0 and if I run this I now get the
592:56 - correct
592:57 - result so it's important to be careful
593:00 - with integer division in postest it is a
593:02 - potential
593:05 - Pitfall now what I need to do is to
593:07 - reduce the number of zeros after the
593:09 - comma so I need some sort of
593:11 - rounding and for this I can use the
593:14 - round
593:17 - function which looks like this
593:22 - and this is a typical function in uh in
593:26 - SQL and how it works is that it takes
593:29 - two
593:30 - arguments the first argument is a
593:33 - column and actually this is the column
593:37 - right this whole operation and then the
593:39 - second argument is how many uh figures
593:43 - do you want to see after the zero after
593:46 - the comma
593:48 - sorry so now I can clean this up a
593:53 - bit label this as total
593:58 - hours and then I will need to order
594:04 - by facility
594:09 - ID and I get my
594:12 - result so nothing crazy here really we
594:15 - Source our data from a
594:18 - join which is this part over here and
594:22 - then we Group by two
594:25 - columns we select those columns and U
594:29 - then we sum over the slots divide making
594:32 - sure to not have integer
594:34 - division so we use one of the numbers
594:37 - becomes a floating Point number and we
594:40 - round the result of this
594:44 - column list each Member's First booking
594:47 - after September 1st 2012
594:52 - so in order to get our data where does
594:55 - our data leave we need the data about
594:58 - the member and we also need data about
595:00 - their bookings so the data is actually
595:03 - in the members and bookings
595:05 - table so I will quickly join on these
595:16 - [Music]
595:18 - tables and we now have our data do we
595:23 - need a filter on our data yes because we
595:26 - only want to look after September 1st
595:29 - 2012 so we can say
595:32 - where start time is bigger
595:36 - than and it should be enough to just
595:41 - provide the date like
595:44 - this now in the result we need the
595:48 - members surname and first name and their
595:52 - memory ID and then we get to we need to
595:55 - see the first booking in our
596:01 - data meaning the earliest time so again
596:07 - we have an aggregation
596:09 - here so in order to implement this
596:11 - aggregation I need to group by all of
596:14 - these columns that I want to call so
596:17 - surname
596:20 - first
596:23 - name and member
596:25 - ID now that I have grouped by this
596:28 - columns I can select
596:34 - them so now I am I have grouped by each
596:38 - member and now I have all the dates for
596:42 - all their
596:43 - bookings after
596:46 - September 1st
596:48 - 2012 and now how can I look at all these
596:51 - dates and get the earliest
596:54 - date what type of aggregation do I need
596:57 - to
596:58 - use I can use the mean aggregation which
597:02 - will look at all of the dates and then
597:05 - compress them to a single date which is
597:09 - the smallest
597:10 - date and I can call this start
597:15 - time finally I need to order by member
597:19 - ID
597:22 - and I get the result that I
597:25 - needed so this is actually quite
597:27 - straightforward I get my data by joining
597:31 - two tables I make sure I only have the
597:34 - data that I need by filtering on the on
597:37 - the time
597:38 - period and then I group by all the
597:41 - information that I want to see for each
597:43 - member and then within each member I use
597:46 - mean to get the smallest date meaning
597:48 - the earliest date
597:51 - now I wanted to give you a bit of an
597:53 - insight into the subtleties of how SQL
597:57 - Compares timestamps and dates because
598:00 - the results here can be a bit
598:03 - surprising so I wrote three logical
598:06 - Expressions here for you and your job is
598:09 - to try to guess if either of these three
598:13 - Expressions will be true or false so
598:17 - take a look at them and try to answer
598:19 - that
598:21 - as you can see what we have here is a
598:23 - time
598:24 - stamp uh that indicates the 1st of
598:27 - September
598:28 - 8:00 whereas here we have uh simply the
598:32 - indication of the date the 1st of
598:34 - September and the values are the same in
598:36 - all three but my question is are they
598:39 - equal is this uh greater or is this
598:44 - smaller so what do you
598:47 - think I think the intuitive answer is to
598:50 - say that in the first case we have
598:53 - September 1st on one side September 1st
598:55 - on the other they are the same day so
598:58 - this ought to be
599:02 - true whereas here we have again the same
599:05 - day on both sides so this is not
599:08 - strictly bigger than the other one so
599:10 - this should be
599:12 - false and it is also not strictly
599:16 - smaller so this would be false as well
599:23 - now let's run the query and see what's
599:26 - actually
599:29 - happening right so what we see here is
599:33 - that we thought this would be true but
599:36 - it's actually false we thought this
599:38 - would be false but it's actually uh true
599:42 - and this one is indeed false so are you
599:46 - surprised by this result or is it what
599:49 - you expected
599:50 - if you are surprised can you figure out
599:53 - what's going on
599:55 - here now what is happening here is that
599:59 - you are running a comparison between two
600:02 - expressions which have a different level
600:06 - of
600:07 - granularity the one on the left is
600:09 - showing you day hour minute seconds and
600:13 - the one on the right is showing you the
600:16 - date only
600:20 - in other words the value on the left is
600:23 - a Tim
600:24 - stamp whereas the value on the right is
600:28 - a date so different levels of precision
600:32 - here now to make the comparison work SQL
600:35 - needs to convert one into the other it
600:38 - needs to do something that is known
600:40 - technically as implicit type coercion
600:44 - what does it mean type is the data type
600:46 - right so either time stamp or date type
600:49 - coercion is when you take a value and
600:51 - you convert it to a different type and
600:54 - it's
600:55 - implicit uh because we haven't ask for
600:58 - it and SQL has to do it on its own
601:00 - behind the
601:02 - scenes and so how does SQL choose which
601:05 - one to convert to the other the choice
601:08 - is based on let's keep the one with the
601:10 - highest precision and convert the other
601:14 - so we have the time stamp with the
601:16 - higher Precision on the left and we need
601:20 - to convert the date into the timestamp
601:22 - this is how SQL is going to handle this
601:25 - situation it's going to favor the one
601:28 - with the highest
601:29 - Precision now in order to convert a date
601:32 - to a time stamp what SQL will do is that
601:34 - it will add all zeros here so this will
601:39 - basically represent the very first
601:42 - second of the day of uh September 1st
601:48 - 2012 now we can verify which I just
601:51 - showed you I'm going to comment this
601:55 - line and I'm going to add another
601:58 - logical expression here which
602:02 - is taking the Tim stamp representing all
602:08 - zeros here and then setting it equal to
602:13 - the date right
602:18 - here so what do we we expect to happen
602:21 - now we have two different types there
602:24 - will be a type coercion and then SQL
602:28 - will take this value on the right and
602:31 - turn it into exactly this value on the
602:34 - left therefore after I check whether
602:37 - they're equal I should get true
602:41 - here turns out that this is true but I
602:45 - need to add another step which is to
602:48 - convert this to a Tim
602:50 - stamp and after I do this I get what I
602:55 - expected which is that this comparison
602:58 - here is
603:00 - true so what this notation does in
603:03 - postest is that it does the type
603:06 - coercion taking this date and forcing it
603:09 - into a time stamp and I'll be honest
603:12 - with you I don't understand exactly why
603:14 - I need to to do this here I thought that
603:17 - this would work simply by taking this
603:19 - part over here but u i I also need to
603:23 - somehow explicitly tell SQL that I want
603:26 - this to be a time
603:28 - stamp nonetheless this is the Insight
603:31 - that we needed here and it allows us to
603:34 - understand why this comparison is
603:39 - actually
603:40 - false because we are comparing a time
603:44 - stamp for the very first second of
603:47 - September 1st with a time stamp that is
603:50 - the first second of the eighth hour of
603:53 - September 1st and so it
603:55 - fails and we can also see why on on this
604:00 - line the left side is bigger than the
604:03 - right hand
604:04 - side and uh and this one did not
604:07 - actually fool us so we're good with
604:10 - that so long story
604:13 - short if you're just getting started you
604:16 - might not know that SQL does this uh
604:20 - implicit type coercion in the background
604:23 - and this dates comparison might leave
604:25 - you quite
604:27 - confused now I've cleaned the code up a
604:30 - bit and now the question is what do we
604:33 - need to do with the code in order to
604:35 - match our initial intuition so what do
604:38 - we need to do such that this line is
604:41 - true and the second line is false and
604:45 - this one is still false so we don't have
604:47 - to worry about it well since the
604:50 - implicit coercion turns the date into a
604:53 - time stamp we actually want to do the
604:55 - opposite we want to turn the time stamp
604:57 - into a date so it will be enough to do
605:01 - the type coion
605:03 - ourselves and transform this into
605:13 - dates like
605:15 - this and when I run this new query I get
605:19 - exactly what I
605:21 - expected so now I'm comparing at the
605:23 - level of precision or granularity that I
605:26 - wanted I'm only looking at the at the
605:30 - date so I hope this wasn't too confusing
605:34 - I hope it was a bit insightful and that
605:36 - you have a new appreciation for the
605:38 - complexities that can arise when you
605:40 - work with dates and time stamps in
605:44 - SQL produce a list of member names with
605:47 - each row containing the total member
605:50 - count let's look at the expected results
605:53 - we have the first name and the surname
605:55 - for each member and then every single
605:58 - row shows the total count of members
606:02 - there are 31 members in our table now if
606:05 - I want to get the total count of
606:07 - members I can take the members
606:11 - table and then select the
606:16 - count and this will give me 31 right
606:20 - but I cannot add first name and surname
606:22 - to this I will get uh an error because
606:27 - count star is an aggregation and it
606:29 - takes all the 31 rows and produces a
606:32 - single number which is
606:34 - 31 while I'm not aggregating first name
606:37 - and surname so the standard aggregation
606:41 - doesn't work here I need an aggregation
606:44 - that doesn't change the structure of my
606:46 - table and that works at the level of the
606:48 - row
606:50 - and to have an aggregation that works at
606:52 - the level of the row I can use a window
606:54 - function and the window function looks
606:57 - like having an aggregation followed by
607:00 - the keyword over and then the definition
607:02 - of the
607:04 - window so if I do this I get the count
607:07 - at the level of the row and to respect
607:10 - the results I need to change the order a
607:12 - bit
607:14 - here and I get the result that I wanted
607:19 - so a window function has these two main
607:21 - components an
607:23 - aggregation and a window
607:26 - definition in this case the aggregation
607:29 - counts the number of rows and the window
607:31 - definition is empty meaning that our
607:33 - window is the entire
607:36 - table and so this aggregation will be
607:39 - computed over the entire table and then
607:41 - added to each
607:42 - row there are far more details about the
607:46 - window functions and how they work in my
607:48 - mental model
607:51 - course produce a numbered list of
607:54 - members ordered by their date of joining
607:57 - so I will take the members
608:00 - table and I will select the first name
608:04 - and
608:04 - surname and to to produce a numbered
608:07 - list I can use a window function with
608:11 - the row number
608:14 - aggregation so I'll say row number
608:18 - over so row number is a special
608:21 - aggregation that works only for window
608:23 - functions and what it does is that it
608:26 - numbers the rows um monotonically giving
608:30 - a number to each starting from one and
608:32 - going uh forward and it never assigns
608:36 - the same number to two rows and in the
608:39 - window you need to define the
608:41 - ordering uh for for the numbering so
608:44 - what is the ordering in this case it's
608:47 - um defined by the join date and by
608:51 - default it's
608:53 - ascending so that's
608:56 - good and we can call
608:59 - this row
609:05 - number and we get the results we
609:08 - wanted and again you can find a longer
609:10 - explanation for this with much more
609:13 - detail about the window functions and
609:16 - and row number in the mental models
609:20 - course output the facility ID that has
609:23 - the highest number of slots booked again
609:26 - so we've we've already solved this
609:28 - problem in a few different ways let's
609:30 - see a new way to to solve
609:34 - it so we can go to our bookings
609:40 - table and we can Group by facility
609:44 - ID and then we can get the facility ID
609:47 - in our select
609:49 - and then we could sum on slots to get
609:54 - the total slots booked for each
609:58 - facility and since we're dealing with
610:00 - window functions we can also rank
610:04 - facilities based on the total slots that
610:07 - they have booked and this would look
610:10 - like
610:12 - rank
610:14 - over order by some slots descending
610:20 - and we can call this RK for
610:23 - Rank and if I order by some
610:29 - slots uh
610:31 - descending I should see that my rank
610:34 - works as
610:36 - intended so we've seen this in the
610:38 - mental models course you can think of
610:41 - rank as U deciding the outcome of a Race
610:45 - So the person who did the most in this
610:48 - case gets ranked one and then everyone
610:50 - else gets rank two 3 four but if there
610:53 - were two um candidates that got the same
610:57 - score the highest score they would both
610:59 - get rank one because they would both
611:02 - have won the race so to
611:05 - speak and the rank here is defined over
611:08 - the window
611:09 - of the sum of slots descending so that
611:13 - is what we
611:14 - need and next to get all the facilities
611:18 - that have the highest score or we could
611:20 - wrap this into a Common Table
611:24 - expression and then take that table and
611:29 - then
611:31 - select the facility
611:33 - ID and we can label this column total
611:37 - then we will get total and filter for
611:40 - where ranking is equal to
611:43 - one and we get our
611:46 - result aside from how rank works the the
611:49 - other thing to note in this exercise is
611:52 - that we can Define the window based on
611:56 - an aggregation so in this case we are
611:59 - ordering the elements of our
612:01 - window based on the sum of slots and if
612:06 - we look at our map over here we can see
612:10 - that uh we get the data we have our
612:13 - group ey we have the aggregation and
612:16 - then we have the window so the window
612:18 - follows the aggregation
612:19 - and So based on our rules the window has
612:22 - access to the aggregation and it's able
612:23 - to use
612:25 - it rank members by rounded hours used so
612:31 - the expected results are quite
612:32 - straightforward we have the first name
612:35 - and the surname of each member we have
612:37 - the total hours that they have used and
612:40 - then we are ranking them based on that
612:42 - so the information for this result where
612:46 - is it uh we can see that it's in the
612:49 - members and bookings
612:52 - tables and so we will need to join on
612:54 - these two tables members Ms join
612:59 - bookings book on M
613:05 - ID and that's our join now we need to
613:08 - get the total hours so we can
613:11 - Group by our first name and we also need
613:15 - to group by the surname because we will
613:17 - want to display it
613:22 - and now we can select these two
613:26 - columns and we need to compute the total
613:29 - hours so how can we get that for each
613:33 - member we know the slots that they got
613:37 - uh at every booking so we need to get
613:40 - all those those uh slots sum them
613:43 - up and uh every slot represents a 30
613:47 - minute interval right so to get the
613:50 - hours we need to divide this value by
613:54 - two and remember if I take an integer
613:58 - like sum of slots and divide by two
614:01 - which is also an integer I'm going to
614:03 - have integer division so I won't have
614:06 - the part after the comma in the result
614:08 - of the division and that's not what I
614:10 - want so instead of saying divide by two
614:13 - I will say divide by 2.0 so let's check
614:17 - um how the data looks
614:21 - like this is looking good now but um if
614:26 - we read the question we want to round to
614:29 - the nearest 10 hours so 19 should
614:34 - probably be 20 115 should probably be
614:38 - 120 because I think that we round up
614:40 - when we have
614:42 - 15 and so on as you can see here in the
614:45 - result so how can we do this rounding
614:49 - well we have a Nifty round function
614:53 - which as the first argument takes the
614:55 - column with all the values and the
614:57 - second argument we can specify how do we
614:59 - want the rounding and to round to the
615:01 - nearest 10 you can put -1
615:05 - here so actually let's keep displaying
615:08 - the the total
615:10 - hours as well as the rounded value to
615:13 - make sure that we're doing it
615:17 - correctly so as you can see we are
615:19 - indeed um rounding to to the nearest 10
615:22 - so this is looking good and for the to
615:26 - understand the reason why I used minus
615:28 - one here and how the rounding function
615:30 - works I will have a small section about
615:33 - it when we're done with this exercise
615:36 - but meanwhile Let's uh finish this
615:40 - exercise so now I want to rank all of my
615:44 - rows based on this value here that I
615:48 - have comped
615:49 - computed and since this is an
615:52 - aggregation it will already be available
615:55 - to a window function right because in
615:58 - The Logical order of operations
616:00 - aggregation happen
616:01 - here and then Windows happen afterward
616:05 - and they have access to the data uh from
616:08 - the aggregation so it should be possible
616:11 - to transform this into a window function
616:14 - so think for a moment uh of how we could
616:17 - do that so window function has its own
616:20 - aggregation which in this case is a
616:22 - simple Rank and then we have the over
616:27 - part which defines the window and what
616:31 - do we want to put in our window in this
616:33 - case we want to order by let's say our
616:38 - um rounded
616:42 - hours and we want to order descending
616:45 - because we want the guest the member
616:48 - with the high hours to have the best
616:52 - rank but uh clearly we don't have a
616:55 - column called rounded hours what we have
616:58 - here is this logic over here so I will
617:03 - substitute this name with my actual
617:11 - logic and I will get my actual
617:15 - Rank and now I can delete this column
617:18 - here that I was was just looking
617:23 - at and I can
617:26 - sort by rank
617:29 - surname first
617:33 - name small error here I actually do need
617:37 - to show the hour as
617:40 - well so I need to take this logic over
617:43 - here
617:45 - again and call this ours
617:50 - and I finally get my
617:53 - result so to summarize what we are doing
617:56 - in this exercise we're getting our data
618:00 - by joining these two tables and then
618:02 - we're grouping by the first name and the
618:04 - surname of the member and then we are
618:08 - summing over the slots for each member
618:12 - dividing by 2.0 to make sure we have an
618:14 - exact
618:15 - Division and uh using the rounding
618:18 - function to round down to the nearest
618:21 - hour and so we get the
618:24 - hours and we use the same logic inside a
618:29 - window function to have a ranking such
618:32 - that the members with the with most
618:36 - hours get rank of one and then the one
618:39 - with the second most hours get rank of
618:40 - two and so on as you can see here in the
618:45 - result and I am perfectly able to use
618:48 - use this logic to Define The Ordering of
618:52 - my window because window functions can
618:56 - use uh aggregations as seen in The
618:59 - Logical order of SQL operations here
619:03 - because window functions occur after
619:06 - aggregations and um and that's it then
619:09 - we just order by the required values and
619:12 - get our
619:14 - results now here's a brief overview of
619:16 - how rounding Works in
619:19 - SQL now rounding is a function that
619:22 - takes a certain number and then returns
619:25 - an approximation of that number which is
619:28 - usually easier to parse and easier to
619:30 - read and you have the round function and
619:33 - it works like this the first argument is
619:36 - a value and it can be a constant as in
619:39 - this case so we just have a number or it
619:41 - can be a column um in which case it will
619:45 - apply the round function to every
619:47 - element of the column
619:49 - and the second argument specifies how we
619:52 - want the rounding to
619:55 - occur so here you can see the number
619:59 - from which we start and the first
620:02 - rounding we apply has an argument of two
620:05 - so this means that we really just want
620:07 - to see two uh numbers after the decimal
620:12 - so this is what the first rounding does
620:14 - as you can see
620:17 - here and we we round down or up based on
620:21 - whether the values are equal or greater
620:24 - than five in which case we round up or
620:27 - smaller than five in which case we round
620:29 - down so in this first example two is
620:32 - lesser than five so we just get rid of
620:35 - it and then we have eight eight is
620:37 - greater than five so we have to round up
620:40 - and so when we round up this 79 becomes
620:44 - an 80 and this is how we get to this
620:47 - first round over here
620:49 - here then we have round with an argument
620:51 - of one which leaves one place after the
620:55 - decimal and which is this result over
620:57 - here and then we have round without any
621:00 - argument which is actually the same as
621:03 - providing an argument of zero which
621:06 - means that we really just want to see
621:08 - the whole
621:10 - number and then what's interesting to
621:13 - note is that the rounding function can
621:16 - be generalized to continue even after we
621:20 - got rid of all the decimal part by
621:22 - providing negative arguments so round
621:25 - with the argument of-1 really means that
621:28 - I want to round uh round this number to
621:31 - the nearest 10 so you can see here that
621:34 - from
621:35 - 48,2 192 we end up at 48,2 190 going to
621:41 - the nearest
621:43 - 10 rounding with a value of -2 means
621:46 - going to the nearest 100 so uh
621:50 - 290 the nearest 100 is 300 right so we
621:54 - have to round up and so we get
621:59 - this minus 3 means uh round to the
622:03 - nearest thousand so if you look at here
622:06 - we have
622:09 - 48,3 and so the nearest thousand to that
622:12 - is
622:14 - 48,000 minus 4 means the nearest 10,000
622:18 - ,000 so given that we have
622:21 - 48,000 the nearest 10,000 is
622:27 - 50,000 and finally round minus 5 means
622:32 - round to the nearest
622:35 - 100,000 and um the given that we have
622:40 - 48,000 the nearest 100,000 is actually
622:45 - zero and from now on as we keep going
622:49 - negatively we will always get zero on
622:51 - this
622:52 - number so this is how rounding Works in
622:55 - brief it's a pretty useful function not
622:58 - everyone knows that you can provide it
623:00 - negative arguments actually I didn't
623:03 - know and then when I did the first
623:05 - version of this course um commenter
623:08 - pointed it out so shout out to him U
623:12 - don't know if he wants me to say his
623:15 - name but hopefully now you understand
623:17 - how rounding works and you can use it in
623:21 - your
623:23 - problems find the top three Revenue
623:27 - generating
623:28 - facilities so we want a list of the
623:32 - facilities that have the top three
623:34 - revenues including ties this is
623:38 - important and if you look at the
623:40 - expected results we simply have a the
623:43 - facility name and a bit of a giveaway of
623:46 - what we will need to use the rank of
623:49 - these
623:52 - facilities so there's this other
623:54 - exercise that we did a while back which
623:57 - is find the total revenue of each
623:59 - facility and from this exercise I have
624:02 - taken the code that uh allows us to get
624:05 - to this point where we see the name of
624:07 - the facility and the total revenue for
624:10 - that
624:11 - facility and you can go back there to
624:14 - that exercise to see in detail how this
624:16 - code works but in brief we are joining
624:20 - the bookings and Facilities tables and
624:24 - we are grouping by facility name and
624:27 - then we are getting that facility name
624:30 - and then within each booking we are
624:33 - Computing the revenue by taking the
624:35 - slots and using a case when to choose
624:39 - whether to use guest cost or member cost
624:42 - and so this is how we get the revenue
624:43 - for each single booking and now given
624:46 - that we grouped by facility we can sum
624:48 - all of these revenues to get the total
624:50 - revenue of each facility and this is how
624:53 - we get to this
624:55 - point given this partial result all
624:58 - that's left to do now is to rank these
625:02 - facilities based on their
625:04 - revenue so what I need here is a window
625:07 - function that will allow me to implement
625:10 - this
625:11 - ranking and this window function would
625:14 - look something like this I have a rank
625:18 - and why is rank the right function even
625:20 - though they sort of uh gave it away
625:24 - because if you want the facilities who
625:27 - have the top revenues including ties you
625:30 - can think of it as a race all facilities
625:32 - are racing to get to the top revenue and
625:36 - then if two or three or four facilities
625:40 - get that top Revenue if there are more
625:43 - in the top position you can't
625:45 - arbitrarily say oh you are first and
625:47 - they are second second you have to give
625:49 - them all the rank one because you have
625:51 - to tell them um recognize that they are
625:54 - all first so these type of problems uh
625:58 - call for a ranking solution so our
626:01 - window function would use rank as the
626:03 - aggregation and then we need to Define
626:06 - our window and how do we Define our
626:08 - window we Define the ordering for the
626:11 - ranking here so we can say order by
626:15 - Revenue descending such that the high
626:18 - highest revenue will get rank one the
626:20 - next highest will get rank two and so
626:24 - on now this will not work because I
626:27 - don't have the revenue column
626:31 - right I do have something here that is
626:34 - labeled as Revenue but the ranking part
626:38 - will not be aware of this label however
626:41 - I do have the logic to compute the
626:44 - revenue so I could take the
626:47 - logic right here and paste it over
626:51 - here and I will add a
626:54 - comma now this is not the most elegant
626:56 - looking code but let's see if it
627:01 - works and we need to order by Revenue
627:05 - descending to see it in action and if I
627:07 - order by Revenue descending you can in
627:10 - fact see that the facility with the
627:13 - highest revenue gets rank one and then
627:15 - it goes down from there
627:18 - so now I just need to clean this up a
627:20 - bit first I will remove the revenue
627:25 - column and then I will remove the
627:28 - ordering and what I need here for the
627:31 - result is to keep only the facilities
627:35 - that have rank of three or smaller so
627:38 - ranks 1 2 three and there's actually no
627:41 - way to do it in this query so I actually
627:44 - have to wrap this query into a common
627:47 - table expr
627:48 - expression and then take that
627:51 - table and say select star from
627:56 - T1 where rank is smaller or equal to
628:02 - three and I will need to order by rank
628:06 - ascending
628:07 - here and I get the result I
628:10 - needed so what happened here we built
628:14 - upon the logic of getting the total
628:17 - revenue for each facility and again we
628:20 - saw that in the previous
628:22 - exercise and um then what we did here is
628:27 - that we added a rank window function and
628:34 - within this rank we order by this total
628:39 - revenue so this might look a bit complex
628:43 - but you have to remember that when we
628:45 - have many operations that are nested you
628:48 - always start with the innermost
628:51 - operation and move your way up from
628:53 - there so the innermost operation is a
628:57 - case when which chooses between guest
629:00 - cost and member cost and then multiplies
629:02 - it by slots and this inner operation
629:05 - over here is calculating the revenue for
629:08 - each single
629:10 - booking the next operation is an
629:12 - aggregation that takes that revenue for
629:15 - each single booking and sums this these
629:18 - revenues up to get the total revenue by
629:20 - each
629:22 - facility and finally the outermost
629:25 - operation is taking the total revenue
629:27 - for each facility and it's ordering them
629:30 - in descending order in order to figure
629:33 - out the
629:35 - ranking and the reason all of this works
629:38 - we can go back to our map of SQL
629:42 - operations you can see here that after
629:44 - getting the
629:45 - table the first thing that happens here
629:48 - is the group buy and then the
629:50 - aggregations and here is where
629:53 - we sum over the total of of
629:57 - Revenue and after the aggregation is
630:00 - completed we have the window function so
630:03 - the window function has access to the
630:05 - aggregation and can use them when
630:07 - defining the
630:10 - window and finally after we get the
630:13 - ranking we we have no way of isolating
630:17 - only the first three ranks in this query
630:19 - so we need to do it with a common table
630:22 - expression and if you look here back to
630:25 - our map this makes sense because what
630:28 - components do we have in order to filter
630:30 - our table in order to only keep certain
630:32 - rows we have the wear which happens here
630:35 - very early and we have the having and
630:38 - they both happen before the window so
630:40 - after the window function you actually
630:42 - don't have another filter so you need to
630:45 - use a Common Table expression
630:48 - classify facilities by value so we want
630:51 - to classify facilities into equally
630:54 - sized groups of high average and low
630:57 - based on their
630:59 - revenue and the result you can see it
631:02 - here for each facility it's classified
631:06 - as high average or low and the point is
631:09 - that we decid decided uh at the
631:12 - beginning that we want three groups and
631:14 - this is arbitrary we could have said we
631:16 - want two groups or five or six or seven
631:19 - and then but we have three and then all
631:22 - the facilities that we have are
631:24 - distributed equally Within These groups
631:27 - so because we have nine facilities we
631:29 - get uh three facilities within each
631:32 - group and I can already tell you that
631:35 - there is a spe special function that
631:37 - will do this for us so we will not go
631:39 - through the trouble of implementing this
631:41 - manually which could be pretty
631:44 - complex so I have copied here the code
631:47 - that allow allows me to get the total
631:49 - revenue for each facility and we have
631:51 - seen this code more than one time in
631:54 - past exercises so if you're still not
631:57 - clear about how we can get to this point
632:00 - uh check out the the previous
632:03 - exercises so what we did in the previous
632:06 - exercise was rank the facilities based
632:09 - on the revenue and how we did that is
632:11 - that we took the ranking window function
632:15 - and then we def defined our window as
632:18 - order by Revenue
632:22 - descending except that we don't have a
632:25 - revenue column here but we do have the
632:29 - logic to compute the revenue so we can
632:31 - just get this logic and paste it in
632:35 - here and when I run this I will get a
632:38 - rank for each of my facilities where the
632:41 - biggest Revenue gets rank one and then
632:43 - it goes up from
632:46 - there now the whole trick to solve this
632:49 - exercise is to replace the rank
632:51 - aggregation with a antile aggregation
632:55 - and provide here the number of groups in
632:58 - which we want to divide our
633:00 - facilities and if I run this you see
633:02 - that I get what I need the facilities
633:05 - have been equally distributed into three
633:08 - groups where group number one has the
633:12 - facilities with the highest revenue and
633:14 - then we have group number two and
633:16 - finally group number three which has the
633:18 - facilities with the lowest
633:21 - revenue and to see how this function
633:23 - works I will simply go to Google and say
633:26 - postest
633:27 - antile and the second link here is the
633:30 - postest documentation and this is the
633:33 - page for window
633:34 - functions so if I scroll down here I can
633:37 - see all of the functions that I can use
633:39 - in window functions and you will
633:41 - recognize some of our old friends here
633:44 - row number rank dance rank
633:48 - uh and here we have antile and what we
633:51 - see here is that antile returns an
633:54 - integer ranging from one to the argument
633:58 - value and the argument value is what we
634:01 - have here which is the number of buckets
634:04 - dividing the partition as equally as
634:07 - possible so we call the enti function
634:10 - and we provide how many buckets we want
634:13 - to divide our data into and then the
634:17 - function
634:18 - divides the data as equally as possible
634:21 - into our buckets and how will this
634:24 - division take place that depends on the
634:28 - definition of the window in this case we
634:31 - are ordering by Revenue
634:33 - descending and so this is how the ntile
634:36 - function
634:37 - works so we just need to clean this up a
634:40 - bit I will remove the revenue part
634:43 - because that's not required from
634:45 - us and I will call this uh enti quite
634:51 - simply and now I need to add a label on
634:55 - top of this enti value as you can see in
634:58 - the results so to do that I will wrap
635:02 - this into a Common Table
635:06 - expression and when I have a common
635:08 - table expression I don't need the
635:09 - ordering
635:11 - anymore and then I can select from the
635:14 - table that I have just
635:16 - defined and what what do I want to get
635:18 - from this table I want to get the name
635:20 - of the facility and then I want to get
635:23 - the enti value with a label on top of it
635:28 - so I will use a case when statement to
635:31 - assign this
635:33 - label so case when NTI equals 1 then I
635:38 - will have
635:41 - high when anti equals 2 then I will have
635:47 - average
635:48 - else I will have
635:52 - low uh and the case and call
635:57 - this
635:59 - revenue and finally I want to order by
636:03 - antile so the results are first showing
636:07 - High then average then low and also by
636:11 - facility
636:13 - name and I get the result that I wanted
636:18 - so to summarize uh this is just like the
636:21 - previous exercise except that we use a
636:24 - different window function because
636:26 - instead of rank we use end tile so that
636:28 - we can pocket our data and in the window
636:32 - like we said in the previous exercise
636:34 - there's a few nested operations and you
636:36 - can figure it out by going to the
636:38 - deepest one and moving upwards so the
636:41 - first one picks up the guest cost or
636:44 - member cost multiplies it by slots gets
636:47 - the revenue Vue for each single booking
636:49 - the next one Aggregates on top of this
636:51 - within each facility so we get the total
636:53 - revenue by facility and then we use this
636:57 - we order by Revenue descending this
637:00 - defines our window and this is what the
637:03 - bucketing system uses to distribute the
637:07 - facilities uh in each bucket based on
637:09 - their
637:10 - revenue and then finally we need to add
637:13 - another layer of logic uh here we need
637:15 - to use a common table expression so that
637:18 - we can label our our percentile with the
637:22 - required um text
637:25 - labels calculate the payback time for
637:28 - each facility
637:31 - so this requires some understanding of
637:35 - the business reality that this data
637:38 - represents so if we look at the
637:40 - facilities
637:41 - table we have an initial outlay which
637:45 - represents the initial investment that
637:47 - was put into getting this
637:50 - facility and then we also have a value
637:53 - of monthly maintenance which is what we
637:56 - pay each month to keep this facility
637:59 - running and of course we will also have
638:02 - a value of monthly revenue for each
638:05 - facility so how can we
638:09 - calculate the amount of time that each
638:12 - facility will take to repay its cost of
638:16 - ownership
638:19 - let's actually write it down so we don't
638:21 - lose track of
638:22 - it we can get the monthly revenue of
638:26 - each
638:27 - facility but what we're actually
638:29 - interested in is the monthly profit
638:32 - right um and to get the profit we can
638:35 - subtract the monthly
638:38 - maintenance for each facility so Revenue
638:42 - minus expenses equals
638:44 - profit and when we know how much profit
638:48 - we make for the facility each month we
638:51 - can take the initial
638:55 - investment and divided by the monthly
638:59 - profit and then we can see how many
639:03 - months it will take to repay the initial
639:07 - investment so let us do that now and
639:12 - what I have done here once again I
639:14 - copied the code to calculate the total
639:17 - revenue for each
639:19 - facility and um we have seen this in the
639:22 - previous exercises so you can check
639:24 - those out if you still have some
639:26 - questions about
639:27 - this and now that we have the total
639:31 - revenue for each facility we know that
639:34 - we have three complete months of data so
639:37 - far so how do we get to this to the
639:40 - monthly Revenue it's as simple as
639:43 - dividing all of this by three and I will
639:46 - say 3.0 so we don't have integer
639:48 - division but we have proper division you
639:50 - know and I can call this
639:55 - monthly
639:59 - revenue and now the revenue column does
640:03 - not exist anymore so I can remove the
640:05 - order
640:06 - buy and here I can see the monthly
640:09 - revenue for each
640:11 - facility and now from the monthly
640:14 - revenue for each facility I can subtract
640:17 - the monthly
640:20 - maintenance and this will give me the
640:23 - monthly
640:28 - profit but now we get this error and can
640:31 - you figure out what this is about
640:34 - monthly maintenance does not appear in
640:36 - the group by Clause so what we did here
640:39 - is that we grouped by facility name and
640:42 - then we selected that which is fine and
640:45 - all the rest was gation so remember as a
640:49 - rule when you Group by you can only
640:51 - select the columns that you have grouped
640:53 - by and aggregations and monthly
640:56 - maintenance uh is not an aggregation so
640:59 - in order to make it work we need to add
641:01 - it to the group by statement over
641:05 - here and now I get the monthly
641:08 - profit and finally the last step that I
641:10 - need to take is to take the initial
641:14 - outlay and divide it by by all of the
641:18 - rest that we have computed until
641:21 - now and we can call this months because
641:24 - this will give us the number of months
641:27 - that we need in order to repay our
641:29 - initial
641:31 - investment and again we get the same
641:34 - issue initial outlay is not an
641:36 - aggregation does not appear in the group
641:39 - by clause and easy solution we can just
641:42 - add it to the group by clause
641:47 - so something is pretty wrong here the
641:49 - values look pretty weird so looking at
641:53 - all this calculation that we have done
641:55 - until
641:56 - now can you figure out why the value is
642:04 - wrong the issue here is related to the
642:08 - order of
642:09 - operations because we have no round
642:12 - brackets here the order of
642:14 - operation will be the following initial
642:18 - outlay will be divided by the total
642:22 - revenue then it will be divided by 3.0
642:25 - and then out of all of these we will
642:27 - subtract monthly maintenance but that's
642:30 - not what we want to do right what we
642:32 - want to do is to take initial outlay and
642:35 - divide it by everything
642:38 - else which is the profit so I will add
642:41 - round brackets
642:44 - here and here
642:50 - and now we get something that makes much
642:52 - more sense because first we execute
642:55 - everything that's Within These round
642:57 - brackets and we get the monthly profit
642:59 - and then all of it we divide initial
643:02 - outlay
643:03 - by and then what we want to do is to
643:07 - order by facility
643:11 - name so I will add it here
643:17 - and we get the
643:19 - result so quite a representative
643:22 - business
643:23 - problem calculating a revenue and
643:27 - profits and time to repay initial
643:30 - investment and uh overall is just a
643:33 - bunch of
643:34 - calculations starting from the group bu
643:37 - that allows us to get the total revenue
643:40 - for each
643:41 - booking we sum those revenues to get the
643:45 - total revenue for each facility
643:47 - divide by three to get the monthly
643:49 - Revenue subtract the monthly expenses to
643:52 - get the monthly profit and then take the
643:54 - initial investment and divide by the
643:56 - monthly profit and then we get the
643:58 - number of months that it will take to
644:00 - repay the
644:02 - facility calculate a rolling average of
644:05 - total revenue so for each day in August
644:09 - 2012 we want to see a rolling average of
644:13 - total revenue over the previous 15 days
644:17 - rolling averages are quite common in
644:19 - business analytics and how it works is
644:22 - that if you look at August 1st this
644:25 - value over here is the average of daily
644:28 - revenue for all facilities over the past
644:32 - 15 days including the day of August 1st
644:35 - and then this average is rolling by one
644:39 - win one day or sliding by one day every
644:42 - time so that the next average is the uh
644:47 - same one except it has shifted by one
644:49 - day because now it includes the 2nd of
644:53 - August so let's see how to calculate
644:55 - this and in here I have basic code that
645:00 - calculates the revenue for each booking
645:03 - and I've taken this from previous
645:05 - exercises so if you have any questions
645:08 - uh check those out and what we have here
645:11 - is the name of each facility and um and
645:14 - the revenue for each booking so each row
645:17 - here represents just a single
645:19 - booking so this is what we had until now
645:22 - but if you think about it we're not
645:24 - actually interested in seeing the name
645:27 - of the facility because we're going to
645:29 - be uh summing up over all facilities
645:31 - we're not interested in the revenue by
645:33 - each facility but we are interested in
645:36 - seeing the date in which each booking
645:38 - occurs because we want to aggregate
645:41 - within the date here so to get the date
645:45 - I can get the start part time field from
645:48 - bookings and because this is a time
645:51 - stamp so it shows hours minutes seconds
645:54 - I need to reduce it to a
646:01 - date and what I get here is that for
646:05 - again each row is a booking and for each
646:07 - booking I know the date on which it
646:09 - occurred and the revenue that it
646:12 - generated now for the next step I need
646:15 - to see the total revenue over each
646:19 - facility within the date right so this
646:23 - is a simple
646:24 - grouping so if I group
646:29 - by this calculation over here which
646:33 - gives me my date I can then get the date
646:37 - and now I have um I have compressed all
646:41 - the different occurrences of dates to
646:44 - Unique values right one row for every
646:47 - date and now I need to compress as well
646:51 - all these different revenues for each
646:54 - date to a single value and for that I
646:57 - can put this logic inside the sum
647:00 - aggregation as we have done
647:02 - before and this will give me the total
647:06 - revenue across all facilities for each
647:10 - day and we have it
647:12 - here for the next step my question for
647:15 - you is how can I see the global average
647:19 - over all revenues on each of these
647:25 - rows so that is a roow level aggregation
647:29 - that doesn't change the structure of the
647:30 - table and that's a window function right
647:33 - so I can have a window function here
647:36 - that gets the average of
647:39 - Revenue over and for now I can leave my
647:43 - window definition open because I will
647:45 - look at the whole
647:48 - table however um Revenue will not work
647:53 - because revenue is just a label that
647:54 - I've given on this column and but but
647:57 - this part here is not aware of the label
648:00 - I don't actually have a revenue column
648:02 - at this
648:03 - point but instead of saying Revenue I
648:06 - could actually copy this logic over
648:10 - here and it would work because the
648:13 - window function occurs after Computing
648:15 - the aggregation so the window function
648:17 - is aware of it so this should
648:21 - work and now for every row I see the
648:25 - global average over all the revenues by
648:31 - day now for the next step I would like
648:34 - to first order by date
648:37 - ascending so we have it here in
648:40 - order and my next question of for you is
648:44 - how can we make this a cumulative
648:46 - average average so let's say that our
648:49 - rows are already ordered by date and how
648:52 - can I get the average to grow by date so
648:55 - in the first case the average would be
648:58 - equal to the revenue because we only
648:59 - have one
649:01 - value on the second day the average
649:03 - would be the average of these two values
649:06 - so all the values we've seen until now
649:08 - on the third day it would be the average
649:10 - of the first three values and so on how
649:13 - can I do
649:15 - that the way that I can do that is that
649:18 - I can go to my window definition over
649:21 - here and I can add an
649:24 - ordering and I can order by
649:33 - date but of course the column date does
649:36 - not exist because that's a label that
649:40 - will be assigned after all this part is
649:42 - done uh window function is not aware of
649:45 - labels but again window function works
649:48 - great with logic so I will take the
649:51 - logic and put it in
649:53 - here and now you can see that I get
649:56 - exactly what I wanted on the first row I
649:59 - get the average is equal to the revenue
650:02 - and then as it grows we only look at the
650:06 - current revenue and all the previous
650:08 - revenues to compute the
650:11 - average and but we don't look at all of
650:13 - the revenues so on the second row
650:17 - uh we have the average between this
650:19 - Revenue over here and this one over here
650:22 - and then on the third row we have the
650:24 - average between these three revenues and
650:26 - so
650:27 - on now you will realize that we are
650:30 - almost done with our
650:33 - problem and the only piece that's
650:36 - missing is that right now if I pick a
650:40 - random day within my data set say this
650:45 - one the the average here is computed
650:49 - over all the revenues from the previous
650:52 - days so all the days in my data that
650:55 - lead up to this one they get averaged
650:57 - and we compute this revenue and what I
651:00 - want to do to finish this problem is
651:02 - that instead of looking at all the days
651:05 - I only want to look 15 days back so I
651:09 - need to to reduce the maximum length
651:13 - that this window can extend in time from
651:16 - limited to 15 days
651:19 - back now here is where it gets
651:22 - interesting so what we need to do is to
651:26 - fine-tune the window
651:28 - definition in order to only look 15 days
651:32 - back and with window functions we do
651:36 - have the option to fine-tune the
651:41 - window and it turns out that there's a
651:44 - another element to the window definition
651:47 - which is usually implicit it's usually
651:50 - not written explicitly but it's there in
651:52 - the
651:53 - background and it's the rows part so I
651:56 - will now write
651:57 - rows
651:59 - between unbounded preceding and current
652:06 - rows
652:08 - row now what the rose part does is that
652:14 - it defines how far back the window can
652:18 - look and how far forward the the window
652:22 - can
652:22 - look and what we see in this command is
652:26 - actually the standard Behavior it's the
652:29 - thing that happens by default which is
652:31 - why we usually don't need to write it
652:34 - and what this means is that it says look
652:38 - as far back in the past as you can look
652:42 - as far back as you can based on the
652:44 - ordering
652:46 - and the current
652:49 - row so this is what we've been seeing
652:51 - until now and if I now run the query
652:55 - again after adding this part you will
652:57 - see that the values don't change at all
653:00 - because this is what we have been doing
653:02 - until now so now instead of unbounded
653:05 - proceeding I want to
653:07 - look 14 rows back plus the current row
653:13 - which together makes 15
653:17 - and if I run this my averages change
653:20 - because I'm now looking
653:23 - um I'm now averaging over the current
653:26 - row and the 14 previous rows so the last
653:29 - 15
653:31 - values and now what's left to do to
653:33 - match our result is to remove the actual
653:38 - Revenue over
653:40 - here and call this Revenue
653:48 - and finally we're only interested in
653:50 - values for the month of August
653:53 - 2012 so we need to add a filter but we
653:57 - cannot add a filter in this table
653:59 - definition here because if we added a
654:03 - wear filter here um isolating the period
654:07 - for August
654:09 - 2012 can you see what the problem would
654:13 - be
654:14 - um if my data could only see um Revenue
654:19 - starting from the 1st of August he
654:21 - wouldn't be able to compute the rolling
654:24 - average here because to get the rolling
654:27 - average for this value you need to look
654:29 - two weeks back and so you need to look
654:31 - into July so you need all the data to
654:33 - compute the rolling revenue and we must
654:36 - filter after getting our result so what
654:39 - that looks like is that we can wrap all
654:42 - of this into a Common Table expression
654:47 - and we can we won't need the order
654:50 - within the Common Table expression
654:52 - anymore and then selecting
654:56 - this we can
654:58 - filter to make sure that the date fits
655:01 - in the required period so we could
655:04 - truncate this date at the month
655:10 - level and make sure that it is equal
655:15 - that the truncated value value is equal
655:18 - to the month of August and we have seen
655:21 - how day trunk works in the previous
655:25 - exercises and then we could select all
655:27 - of our columns and order by
655:34 - date I believe we may have an
655:38 - extra small error here because I kept
655:40 - the partial wear
655:43 - statement and if I run this I finally
655:45 - get the result that I
655:50 - wanted so a query that was a bit more
655:52 - complex it was the final boss of our
655:56 - exercises um so let's summarize it we
656:00 - get the data we need by joining booking
656:03 - and
656:04 - facility um and then we are getting the
656:09 - revenue for each booking that is this um
656:13 - multiply slots by either guest cost or
656:15 - member cost cost depending on whether
656:17 - the member is a guest or not this is
656:19 - getting the revenue within each
656:21 - booking then we are grouping by
656:25 - date which you see uh over here and
656:29 - summing all of these revenues that we
656:31 - computed so that we get the total
656:34 - revenue within each day for all
656:38 - facilities then the total revenue for
656:41 - each day goes into a window function
656:44 - which computes an aggre ation at the
656:46 - level of each row and the window
656:49 - function computes the average for these
656:52 - total revenues within a specific
656:55 - window and the window is defines an
656:58 - ordering based on time so the the
657:01 - ordering based on date and the default
657:05 - behavior of the window would be to look
657:07 - at the average for the current day and
657:10 - all the days that precede up until the
657:13 - earliest
657:14 - date and we're doing here is that we are
657:17 - fine-tuning the behavior of this
657:19 - function by saying hey don't look all
657:22 - the way back in the past uh only look at
657:26 - 14 rows preceding plus the current row
657:30 - which means that given the time ordering
657:32 - we compute the average over the last 15
657:36 - values of total
657:38 - revenue and then finally we wrap this in
657:41 - a Common Table expression and we filter
657:44 - so that we only see the rolling average
657:47 - for the month of August and we order by
657:51 - date and that were all the exercises
657:54 - that I wanted to do with you I hope you
657:56 - enjoyed it I hope you learned something
657:58 - new as you know there are more sections
658:01 - in here that go more into depth into
658:03 - date functions and string functions and
658:06 - how you can modify data I really think
658:08 - you can tackle those on your own these
658:10 - were the uh Essentials ones that I
658:12 - wanted to address
658:16 - and once again thank you to the author
658:19 - of this website aliser Owens who created
658:22 - this and made it available for free I
658:24 - did not create this website um so you
658:27 - can just go here and without signing up
658:30 - or paying anything you can just do these
658:33 - exercises my final advice for you don't
658:36 - be afraid of repetition we live in the
658:39 - age of endless content so there's always
658:43 - something new to do but there's a lot of
658:45 - value to um repeating the same exercises
658:49 - over and over again when I Was preparing
658:52 - for interviews when I began as a date
658:55 - engineer I did these exercises and
658:58 - altogether I did them like maybe three
659:01 - or four times um and um I found that it
659:05 - was really helpful to do the same
659:07 - exercises over and over again because
659:09 - often I did not remember the solution
659:11 - and I had to think through it all over
659:13 - again and it strengthened those those uh
659:16 - those learning patterns for
659:19 - me so now that you've gone through all
659:21 - the exercises and seen my Solutions uh
659:24 - let it rest for a bit and then come back
659:26 - here and try to do them again I think it
659:29 - will be really
659:30 - beneficial in my course I start from the
659:33 - very Basics and I show you in depth how
659:37 - each of the SQL components work I um
659:40 - explore the logical order of of SQL
659:43 - operations
659:46 - and I spend a lot of time in Google
659:48 - Sheets um simulating SQL operations in
659:51 - the spreadsheet coloring cells moving
659:54 - them around making some drawings in
659:57 - excal draw uh so that I can help you
660:00 - understand in depth what's happening and
660:03 - build those mental models for how SQL
660:05 - operations work this course was actually
660:09 - intended as a complement to that so be
660:13 - sure to check it out

Cleaned transcript:

learn SQL for analytics Vlad is a data engineer and in this course he covers both the theory and the practice so you can confidently solve hard SQL challenges on your own no previous experience required and you'll do everything in your browser using big query hi everyone my name is Vlad and I'm a date engineer welcome to intuitive SQL for analytics this here is the main web page for the course you will find it in the video description and this will get updated over time with links and resources so be sure to bookmark it now the goal of this course is to quickly enable you to use SQL to analyze and manipulate data this is arguably the most important use case for SQL and the Practical objective is that by the end of this course you should be able to confidently solve hard SQL problems of the kind that are suggested during data interviews the course assumes no previous knowledge of SQL or programming although it will be helpful if you've work with spreadsheets such as Microsoft Excel or Google Sheets because there's a lot of analogies between manipulating data in spreadsheets and doing it in SQL and I also like to use spreadsheets to explain SQL Concepts now there are two parts to this course theory and practice the theory part is a series of short and sweet explainers about the fundamental concepts in SQL and for this part we will use Google bigquery bigquery which you can see here is a Google service that allows you to upload your own data and run SQL on top of it so in the course I will teach you how to do that and how to do it for free you won't have to to spend anything and then we will load our data and we will run SQL code and besides this there will be drawings and we will also be working with spreadsheets and anything it takes to make the SQL Concepts as simple and understandable as possible the practice part involves doing SQL exercises and for this purpose I recommend this website postest SQL exercises this is a free and opensource website where you will find plenty of exercises and you will be able to run SQL code to solve these exercises check your answer and then see a suggested way to do it so I will encourage you to go here and attempt to solve these exercises on your own however I have also solved 42 of these exercises the most important ones and I have filmed explainers where I solve the exercise break it apart and then connect it to the concepts of the course so after you've attempted the exercise you will be able to see me solving it and connect it to the rest of the course so how should you take this course there are actually many ways to do it and you're free to choose the one that works best if you are a total beginner I recommend doing the following you should watch the theory lectures and try to understand everything and then once you are ready you should attempt to do the exercises on your own on the exercise uh website that I've shown you here and if you get stuck or after you're done you can Watch How I solved the exercise but like I said this is just a suggestion and uh you can combine theory and practice as you wish and for example a more aggressive way of doing this course would be to jump straight into the exercises and try to do them and every time that you are stuck you can actually go to my video and see how I solved the exercise and then if you struggle to understand the solution that means that maybe there's a theoretical Gap and then you can go to the theory and see how the fundamental concepts work so feel free to experiment and find the way that works best for you now let us take a quick look at the syllabus for the course so one uh getting started this is a super short explainer on what SQL actually is and then I teach you how to set up bigquery the Google service where we will load our data and run SQL for the theory part the second uh chapter writing your first query so here I explained to you how big query works and how you can use it um and how you are able to take your own data and load it in big query so you can run SQL on top of it and at the end of it we finally run our first SQL query chapter 3 is about exploring some ESS IAL SQL Concepts so this is a short explainer of how data is organized in SQL how the SQL statement Works meaning how we write code in SQL and here is actually the most important concept of the whole course the order of SQL operations this is something that is not usually taught properly and a lot of beginners Miss and this causes a lot of trouble when you're you're trying to work with SQL so once you learn this from the start you will be empowered to progress much faster in your SQL knowledge and then finally we get into the meat of the course this is where we learn all the different components in SQL how they work and how to combine them together so this happens in a few phases in the first phase we look at the basic components of SQL so these are uh there's a few of them uh there's select and from uh there's learning how to transform columns the wear filter the distinct Union order by limit and then finally we see how to do simple aggregations at the end of this part you will be empowered to do the first batch of exercises um don't worry about the fact that there's no links yet I will I will add them but this is basically involves going to this post SQL exercises website and going here and doing this uh first batch of exercises and like I said before after you've done the exercises you can watch the video of me also solving them and breaking them down next we take a look at complex queries and this involves learning about subqueries and Common Table expressions and then we look at joining tables so here is where we understand how SQL tables are connected uh with each other and how we can use different types of joints to bring them together and then you are ready for the second batch of exercises which are those that involve joints and subqueries and here there are eight exercises the next step is learning about aggregations in SQL so this involves the group bu the having and window functions and then finally you are ready for the final batch of exercises which actually bring together all the concepts that we've learned in this course and these are 22 exercises and like before for each exercise you have a video for me solving it and breaking it apart and then finally we have the conclusion in the conclusion we see how we can put all of this knowledge together and then we take a look at how to use this knowledge to actually go out there and solve SQL challenges such as the ones that are done in data interviews and then here you'll find uh all the resources that are connected to the course so you have the files with our data you have the link to the spreadsheet that we will use the exercises and all the drawings that we will do this will definitely evolve over over time as the course evolves so bookmark this page and keep an eye on it that was that was all you needed to know to get started so I will see you in the course if you are working with SQL or you are planning to work with SQL you're certainly a great company in the 2023 developer survey by stack Overflow there is a ranking of the most popular Technologies out there if we look at professional developers where we have almost 70,000 responses we can see that SQL is ranked as the third most popular technology SQL is certainly one of the most in demand skills out there not just for developers but for anyone who works with data in any capacity and in this course I'm going to help you learn SQL the way I wish I would have learned it when I started out on my journey since this is a practical course we won't go too deep into the theory all you need to know for our purposes is that SQL is a language for working with data like most languages SQL has several dialects you may have heard of post SQL or my sqil for example you don't need to worry about these dialects because they're all very similar so if you learn SQL in any one of the dialects you'll do well on all the others in this course we will be working with B query and thus we will write SQL in the Google SQL dialect here is the documentation for Google big query the service that we will use to write SQL code in this course you can see that big query uses Google SQL a dialect of SQL which is an compliant an compliant means that Google SQL respects the generally recognized standard for creating SQL dialects and so it is highly compatible with with all other common SQL dialects as you can read here Google SQL supports many types of statements and statements are the building blocks that we use in order to get work done with SQL and there are several types of statements listed here for example query statements allow us to retrieve and analyze and transform data data definition language statements allow us to create and modify database objects such as tables and Views whereas data manipulation language statements allows us to update and insert and delete data from our tables now in this course we focus exclusively on query statements statements that allow us to retrieve and process data and the reason for this is that if you're going to start working with big query you will most likely start working with this family of statements furthermore query statements are in a sense the foundation for all other families of statements so if you understand uh query statements you'll have no trouble learning the others on your own why did I pick big query for this course I believe that the best way to learn is to load your own data and follow questions that interest you and play around with your own projects and P query is a great tool to do just that first of all it is free at least for the purposes of learning and for the purposes of this course it has a great interface that will give you U really good insights into your data and most importantly it is really easy to get started you don't have to install anything on your computer you don't have to deal with complex software you just sign up for Google cloud and you're ready to go and finally as you will see next big query gives you many ways to load your own data easily and quickly and get started writing SQL right away I will now show you how you can sign up for Google cloud and get started with bigquery so it all starts with this link which I will share in the resources and this is the homepage of Google cloud and if you don't have an account with Google Cloud you can go here and select sign in and here you need to sign in with your Google account which you probably have but if you don't you can go here and select create account so I have now signed in with my Google account which you can see here in the upper right corner and now I get a button that says start free so I'm going to click that and now I get taken to this page and on the right you see that the first time you sign up for Google Cloud you get $300 of free credits so that you can try the services and that's pretty neat and here I have to enter some extra information about myself so I will keep it as is and agree to the terms of service and continue finally I need to do the payment information verification so unfortunately this is something I need to do even though I'm not going to be charged for the services and this is for Google to be able to verify my my identity so I will pick individual as account type and insert my address and finally I need to add a payment method and again uh I need to do this even though I'm not going to pay I will actually not do it here because I don't intend to sign up but after you are done you can click Start my free trial and then you should be good to go now your interface may look a bit different but essentially after you've signed up for Google Cloud you will need to create a project and the project is a tool that organizes all your work in Google cloud and essentially every work that you do in Google cloud has to happen inside a specific project now as you can see here there is a limited quota of projects but that's not an issue because we will only need one project to work in this course and of course creating a new project is totally free so I will go ahead and give it a name and I don't need any organization and I will simply click on create once that's done I can go back back to the homepage for Google cloud and here as you can see I can select a project and here I find the project that I have created before and once I select it the rest of the page won't change but you will see the name of the project in the upper bar here now although I've created this project as an example for you for the rest of the course you will see me working within this other project which was the one that I had originally now I will show you how you can avoid paying for Google cloud services if you don't want to so from the homepage you have the search bar over here and you can go here and write billing and click payment overview to go to the billing service now here on the left you will see your billing account account which could be called like this or have another name and clicking here I can go to manage billing accounts now here I can go to my projects Tab and I see a list of all of my projects in Google cloud and a project might or might not be connected to a billing account if a project is not connected to a billing account then then Google won't be able to charge you for this project although keep in mind that if you link your project with a billing account and then you incur some expenses if you then remove the billing account you will still owe Google Cloud for those uh expenses so what I can do here is go to my projects and on actions I can select disabled building in case I have a billing account connected now while this is probably the shest way to avoid incurring any charges you will see that you will be severely limited in what you can do in your project if that project is not linked to any billing account however you should still be able to do most of what you need to do in B query at least for this course and we can get more insight into how that works by by going to the big query pricing table so this page gives us an overview of how pricing works for big query I will not analyze this in depth but what you need to know is that when you work with bigquery you can fundamentally be charged for two things one is compute pricing and this basically means all the data that bigquery scans in order to return the results that you need when you write your query and then you have storage pricing which is the what you pay in order to store your data inside bigquery now if I click on compute pricing I will go to the pricing table and here you can select the region that uh most reflects where you are located and I have selected Europe here and as you can see you are charged $625 at the time of this video for scanning a terabyte of data however the first terabyte per month is free so every month you can write queries that scan one terabyte of data and not pay for them and as you will see more in detail this is more than enough for what we will be doing in this course and also for for what you'll be doing on your own in order to experiment with SQL and if I go back to the top of the page and then click on storage pricing you can see here that again you can select your region and see um several pricing uh units but here you can see that the first 10 gab of storage per month is free so you can put up to 10 gigabytes of data in B query and you won't need a billing account you won't pay for storage and this is more than enough for our needs in order to learn SQL in short bigquery gives us a pretty generous free allowance for us to load data and play with it and we should be fine however I do urge you to come back to this page and read it again because things may have changed since I recorded this video video to summarize go to the billing service check out your billing account and you have the option to decouple your project from the billing account to avoid incurring any charges and you should still be able to use B query but as a disclaimer I cannot guarantee that things will work just the same uh at the time that you are watching this video so be sure to check the documentation or maybe discuss with Google Cloud support to um avoid incurring any unexpected expenses please do your research and be careful in your usage of these services for this course I have created an imaginary data set with the help of chat GPT the data set is about a group of fantasy characters as well as their items and inventories I then proceed proed to load this data into bigquery which is our SQL system I also loaded it into Google Sheets which is a spreadsheet system similar to Microsoft Excel this will allow me to manipulate the data visually and help you develop a strong intuition about SQL operations I'm going to link a separate video which explains how you can also use chat PT to generate imaginary data according to your needs and then load this data in Google Sheets or bigquery I will also link the files for this data in the description which you can use to reproduce this data on your side next I will show you how we can load the data for this course into bigquery so I'm on the homepage of Google cloud and I have a search bar up here and I can write big query and select it from here and this will take me to the big query page now there is a panel on the left side that appears here if I hover or it could be fixed and this is showing you several tools that you can use within bigquery and you can see that we are in the SQL workspace and this is actually the only tool that we will need for this course so if you if you're seeing this panel on the left I recommend going to this arrow in the upper left corner and clicking it so you can disable it and make more room for yourself now I want to draw your attention to the Explorer tab which shows us where our data is and how it is organized so I'm going to expand it here now data in bigquery and in SQL in general exists in the form of tables and a table looks just like this as you can see here the customer's table it is a collection of rows and columns and it is quite similar to a spreadsheet so this will be familiar to you if you've ever worked with Microsoft Excel or Google Sheets or any spreadsheet program so your data is actually living in a table and you could have as many tables as you need in B query there could be quite a lot of them so in order to organize our tables we use data sets for example in this case my data is a data set which contains the table customers and employee data and a data set is is just that it's a collection of tables and it's similar to how a folder Works in a file setem system it is like a for folder for tables finally in bigquery each data set belongs to a project so you can see here that we have two data sets SQL course and my data and they both belong to this project idelic physics and so on and this is actually the ID of my project this is the ID of the project that I'm working in right now the reason the Explorer tab shows the project as well is that in big query I'm not limited to working with data that leaves in my project I could also from within my project query data that leaves in another project for example the bigquery public data is a project that is not mine but it's actually a public project by bigquery and if I expand this you will see that it contains a collection of of several data sets which are in themselves um collections of tables and I would be able to query these uh tables as well but you don't need to worry about that now because in this course we will only focus on our own data that lives in our own project so this in short is how data is organized in big query now for the purpose of this course I recommend creating a new data set so so that our tables can be neatly organized and to do that I can click the three dots next to the project uh ID over here and select create data set and here I need to pick a name for the data set so I will call this fantasy and I suggest you use the same name because if you do then the code that I share with you will work immediately then as for the location you can select the multi region and choose the region that is closest to you and finally click on create data set so now the data set fantasy has been created and if I try to expand it here I will see that it is empty because I haven't loaded any data yet the next step is to load our tables so I assume that you have downloaded the zip file with the tables and extracted it on your local computer and then we can select the action point here next to the fantasy data set and select create table now as a source I will select upload and here I will click on browse and access the files that I have downloaded and I will select the first table here here which is the characters table the file format is CSV so Google has already understood that and scrolling down here I need to choose a name for my table so I will call it just like the file uh which is characters and very important under schema I need to select autodetect and we will see what this means in a bit but basically this is all we need so now I will select create table and now you will see that the characters table has appeared under the fantasy data set and if I click on the table and then go on preview I will should be able to see my data I will now do the same for the other two tables so again create table source is upload file is inventory repeat the name and select autod detect and I have done the same with the third table so at the end of this exercise the fantasy data set should have three tables and you can select them and go on preview to make sure that the data looks as expected now our data is fully loaded and we are ready to start querying it within big query now let's take a look at how the bigquery interface works so on the left here you can see the Explorer which shows all the data that I have access to and so to get a table in big query first of all you open the name of the project and then you look at the data sets that are available within this project you open a data set and finally you see a table such as characters and if I click now on characters I will open the table view now in the table view I will find a lot of important information about my table in these tabs over here so let's look at the first tab schema the schema tab shows me the structure of my table which as we shall see is very important and the schema is defined essentially by two things the name of each column in my table and the data type of each column so here we see that the characters table contains a few columns such as ID name Guild class and so on and these columns have different data types for example ID is an integer which means that it contains natural numbers whereas name is string which means that it contains text and as we shall see the schema is very important because it defines what you can do with the table and next we have the details tab which contains a few things first of all is the table ID and this ID represents the full address of the table and this address is made up of three components first of all you have the ID of the project which is as you can see the project in which I'm working and it's the same that you see here on the left in the Explorer tab the next component is the data set that contains the table and again you see it in the Explorer Tab and finally you have the name of the table this address is important because it's what we use to reference the table and it's what we use to get data from this table and then we see a few more things about the table such as when it was created when it was last modified and here we can see the storage information so we can see here that this table has 15 rows and on the dis it occupies approximately one kilobyte if you work extensively with P query this information will be important for two reasons number one it defines how much you are paying every month to store this table and number two it defines how much you would pay for a query that scans all the data in this table and as we have seen in the lecture on bigquery pricing these are the two determinants of bigquery costs however for the purpose of this course you don't need to worry about this because the tables we are working with are so small that they won't put a dent in your free month monthly allowance for using big query next we have the preview tab which is really cool to get a sense of the data and this basically shows you a graphical representation of your table and as you will notice it looks very similar to a spreadsheet so you can see our columns the same ones that we saw in the schema tab ID name Guild and so on and as you remember we saw that ID is an integer column so you can only contain numbers name is a text column and then you see that this table has 15 rows and because it's such a small table all of it fits into this graphical representation but in the real world you may have tables with millions of rows and in this case the preview will show you only a small portion of that table table but still enough to get a good sense of the data now there are a few more tabs in the table view we have lineage data profile data quality but I'm not going to look at them now because they are like Advanced features in bigquery and you won't need them in this course instead I will run a very basic query on this table and this is not for the purpose of understanding query that will come soon it is for the purpose of showing you what the interface looks like after you run a query so I have a very basic query here that will run on my table and you can see that the interface is telling me how much data this query will process and this is important because this is the main determinant of cost in bigquery every query scans a certain amount of data and you have to pay for that but as we saw in the lecture of bigquery pricing this table is so small that you could run a million or more of these queries and not exhaust your monthly allowance so if you see 1 kilobyte you don't have to worry about that so now I will click run and my query will execute and here I get the query results view this is the view that that appears after you have successfully run a query so we have a few tabs here and the first step that you see is results and this shows you graphically the table that was returned by your query so as we shall see every query in SQL runs on a table and returns a table and just like the preview tab showed you a graphical view of your table the results tab shows you a graphical view of the table that your query has returned and this is really the only tab in the query results view that you will need on this course the other ones show different features or more advanced features that we won't look at but feel free to explore them on your own if you are curious but what's also important in this view is this button over here save results which you can use to EXP report the result of your query towards several different destinations such as Google drive or local files on your computer in different formats or another big query table a spreadsheet in Google Sheets or even copying them to your clipboard so that you can paste them somewhere else but we shall discuss this more in detail in the lecture on getting data in and out of big query finally if you click on this little keyboard icon up here you can see a list of shortcuts that you can use in the big query interface and if you end up running a lot of queries and you want to be fast this is a nice way to improve your experience with big query so be sure to check these out we are finally ready to write our first query and in the process we will keep exploring the Fantastic bigquery interface so one way to get started would be to click this plus symbol over here so that we can open a new tab now to write the query the first thing I will do is to tell big query where the data that I want leaves and to do that I will use the from Clause so I will simply write from and my data lives in the fantasy data set and in the characters table next I will tell SQL what data I actually want from this table and the simplest thing to ask for is to get all the data and I can do this by writing select star now my query is ready and I can either click run up here or I can press command enter on my Mac keyboard and the query will run and here I get a new tab which shows me the results now the results here are displayed as a table just as uh we saw in the preview tab of the table and I can get an idea of uh my results and this is actually the whole table because this is what I asked for in the query there are also other ways to see the results which are provided by bigquery such as Json which shows the same data but in a different format but we're not going to be looking into that for this course one cool option that the interface provides is if I click on this Arrow right here in my tab I can select split tab to right and now I have a bit of less room in my interface but I am seeing the table on the left and the query on the right so that I can look at the structure of the table while writing my query for example if I click on schema here I could see which columns I'm able to um reference in my query and that can be pretty handy I could also click this toggle to close the Explorer tab temporarily if I don't need to look look at those tables so I can make a bit more room or I can reactivate it when needed I will now close this tab over here go back to the characters table and show you another way that I can write a query which is to use this query command over here so if I click here I can select whether I want my query in a new tab or in a split tab let let me say in new tab and now bigquery has helpfully uh written a temp template for a query that I can easily modify in order to get my data and to break down this template as you can see we have the select Clause that we used before we have the from clause and then we have a new one called limit now the from Clause is doing the same job as before it is telling query where we want to get our data but you will notice that the address looks a bit different from the one that I had used specifically I used the address fantasy. characters so what's happening here is that fantasy. characters is a useful shorthand for the actual address of the table and what we see here that big query provided is the actual full address of the table or in other words it is the table ID and as you remember the table ID indicates the project ID the data set name and the table name and importantly this ID is usually enclosed by back ticks which are a quite specific character long story short if you want to be 100% sure you can use the full address of the table and bigquery will provide it for you but if you are working within the same project where the data lives so you don't need to reference the project you can also use this shorthand here to make writing the address easier and in this course I will use these two ways to reference a table interchangeably I will now keep the address that bigquery provided now the limit statement as we will see is simply limiting the number of rows that will be returned by this query no more than 1,000 rows will be returned and next to the select we have to say what data we want to get from this table and like before I can write star and now my query will be complete before we run our query I want to draw your attention to this message over here this query will process 1 kilobyte when run so this is very important because here big query is telling you how much data will be scanned in order to give you the results of this query in this case we are returning um all the data in the table therefore all of the table will be scanned and actually limit does not have any influence on that it doesn't reduce how much data is scanned so this query will scan 1 kilobyte of data and the amount of data that scanned by the query is the primary determinant of bigquery costs now as you remember we are able to scan up to one terabyte of data each month within the sandbox program and if we wanted to scan more data then we would have to pay so the question is how many of these queries could we run before running out of our free allowance well to answer that we could check how many kilobytes are in a terabyte and if you Google this the conversion says it's one to um multipli by 10 to the power of 9 which ends up being 1 billion therefore we can run 1 billion of these queries each month before running out of our allowance now you understand why I've told you that as long as you work with small tables you won't really run out of your allowance and you don't really have to worry about costs however here's an example of a query that will scan a large amount of data and what I've done here is I've taken one of the public tables provided by big query which I've seen to be quite large and I have told big query to get me all the data for this table and as you can see here big query says that 120 gabt of data will be processed once this query runs now you would need about eight of these queries to get over your free allowance and if you had connected to B query you could also be charged money for any extra work that you do so be very careful about this and if you work with large tables always check this message over here before running the query and remember you won't actually be charged until you actually hit run on the query and there you have it we learned how the big query interface works and wrote our first SQL query it is important that we understand how data is organized in SQL so we've already seen a a preview of the characters table and we've said that this is quite similar to how you would see data in a spreadsheet namely you have a table which is a collection of rows and columns and then in this case on every row you have a character and for every character you have a number of information points such as their ID their name their class level and so on the first fundamental difference with the spreadsheet is that if I want to have some data in a spreadsheet I can just open a new one and uh insert some data in here right so ID level name and so on then I could say that I have a character id one who is level 10 and his name is Gandalf and this looks like the data I have in SQL and I can add some more data as well well a new character id 2 level five and the name is frao now I will save this spreadsheet and then some days later someone else comes in let's say a colleague and they want to add some new data and they say oh ID uh is unknown level is um 20.3 and the name here and then I also want to uh show their class so I will just add another column here and call this Mage now spreadsheets are of course extremely flexible because you can always um add another column and write in more cells and you can basically write wherever you want but this flexibility comes at a price because the more additions we make to this uh to the data model that is represented here the more complex it will get with time and the more likely it will be that we make confusions or mistakes which is what actually happens in real life when people work with spreadsheets SQL takes a different approach in SQL before we insert any actual data we have to agree on the data model that we are going to use and the data model is essentially defined by two elements the name of our columns and the data type that each column will contain for example we can agree that we will use three columns in our table ID level and name and then we can agree that ID will be an integer meaning that it will contain contain whole numbers level will be a integer as well and name will be a string meaning that it contains text now that we've agreed on this structure we can start inserting data on the table and we have a guarantee that the structure will not change with time and so any queries that we write on top of this table any sort of analysis that we create for this table will also be durable in time because it will have the guarantee that the data model of the table will not change and then if someone else comes in and wants to insert this row they will actually not be allowed to first of all because they are trying to insert text into an integer column and so they're violating the data type of the column and they are not allowed to do that in level they are also violating the data type of the column because this column only accepts whole numbers and they're trying to put a floating Point number in there and then finally there are also violating the column definition because they're they're trying to add a column class that was not actually included in our data model and that we didn't agree on so the most important difference between spreadsheets and SQL is that for each SQL table you have a schema and as we've seen before the schema defines exactly which columns our table has and what is the data type of each column so in this case for the characters table we have several columns uh and here we can see their names and then each column has a specific data types and all the most important data types are actually represented here specifically by integer we mean a whole number and by float we mean a floating Point number string is a piece of text Boolean is a value that is either true or false and time stamp is a value that represents a specific point in time all of this information so the number of columns the name of each column and the type of each column they constitute the schema of the table and like we've said the schema is as assumed as a given when working in SQL and it is assumed that will not change over time now in special circumstances there are ways to alter the schema of a table but it is generally assumed as a given when writing queries and we shall do the same in this course and why is it important to keep track of the data type why is it important to distinguish between integer string Boolean the simple answer is that the data type defines the type of operations that you you can do to a column for example if you have an integer or a float you can multiply the value by two or divide it and so on if you have a string you can turn that string to uppercase or lowercase if you have a time stamp you can subtract 30 days from that specific moment in time and so on so by looking at the data type you can find out what type of work you can do with a column the second fundamental difference from spreadsheets is that spreadsheets are usually disconnected but SQL has a way to define connections between tables so what we see here is a representation of our three tables and for each table you can see the schema meaning the list of columns and their types but the extra information that we have here is the connection between the tables so you can see that the inventory table is connected to the items table and also to the character table moreover the characters table is connected with itself now we're not going to explore this in depth now because I don't want to add too much Theory we will see this in detail in the chapter on joints but it is a fundamental difference from spreadsheets that SQL tables can be clearly connected with each other and that's basically all you need to understand how data is organized in SQL for now you create a table and when creating that table you define the schema the schema is the list of columns and their names and their data types you then insert data into this table and finally you have a way to define how the tables are connected with each other I will now show you how SQL code is structured and give you the most important concept that you need to understand in order to succeed at SQL now this is a SQL statement it is like a complete sentence in the SQL language the statement defines where we want to get our data from and how we want to receive these data including any processing that we want to apply to it and once we have a statement we can select run and it will give us our data now the statement is made up of building block blocks which we call Clauses and in this statement we have a clause for every line so the Clauses that we see here are select from where Group by having order and limit and clauses are really the building blocks that we assemble in order to build statements what this course is about is understanding what each Clause is and how it works and then understanding how we can put together these Clauses in order to write effective statements now the first thing that you need to understand is that there is an order to write in these Clauses you have to write them in the correct order and there is no flexibility there if you write them in the wrong order you will simply get an error for example if I I were to take the work clause and put it below the group Clause you can see that I'm already getting an error here which is a syntax error but you don't have to worry about memorizing this now because you will pick up this order as we learn each clause in turn now the essential thing that you need to understand and that slows down so many SQL Learners is that while we are forced by SQL to write Clauses in this specific order this is not actually the order in which the Clauses are executed if you've interacted with another programming language such as python or or JavaScript you're used to the fact that each line of your program is executed in turn from top to bottom generally speaking and that is pretty transparent to understand but this is not what is happening here in SQL to give you a sense of the order in which these Clauses are run on a logical level what SQL does is that first it reads the from then it does the wear then the group by then the having then it does the select part after the select part is do it does the order by and finally the limit all of this just to show that the order in which operations are executed is not the same as the order in which they're written in fact we can distinguish three orders that pertain to SQL Clauses and this distinction is so important to help you master SQL the first level is what we call the lexical order and this is simply what I've just shown you it's the order in which you have to write these Clauses so that SQL can actually execute the statement and not throw you an error then there's the logical order and this is the order in which the clause are actually run logically in the background and understanding this logical order is crucial for accelerating your learning of SQL and finally for the sake of completeness I had to include the effective order here because what happens in practice is that your statement is executed by a SQL engine and that engine will usually try to take shortcuts and optimize things and save on processing power and memory and so the actual order might be a bit different because the Clauses might be moved around um in the process of optimization but like I said I've only included it for the sake of completeness and we're not going to worry about that level in this course with we are going to focus on mastering the lexical order and The Logical order of SQL Clauses and to help you master The Logical order of SQL Clauses or SQL operations I have created this schema and this is the fundamental tool that you will use in this course this schema as you learn it progressively will allow you to build a powerful mental model of SQL that will allow you to tackle even the most complex and tricky SQL problems now what this schema shows you is all of the Clauses that you will work with when writing SQL statements so these are the building blocks that you will use in order to assemble your logic and then the sequence in which they're shown is corresponding to The Logical order in which they are actually executed and there are three simple rules for you to understand this schema the first rule is that operations are EX executed sequentially from left to right the second rule is that each operation can only use data that was produced by operations that came before it and the third rule is that each operation cannot know anything about data that is produced by operations that follow it what this means in practice is that if you take any of these components for example the having component you already know that having will have access to data that was produced by the operations that are to to its left so aggregations Group by where and from however having will have absolutely no idea of information that is produced by the operations that follow for example window or select or Union and so on of course you don't have to worry about understanding this and memorizing it now because we will tackle this gradually throughout the course and throughout the course we will go back to the schema again and again in order to make sense of the work we're doing and understand the typical errors and Pitfall that happen when working with SQL now you may be wondering why there are these two cases where you actually see two components stacked on top of each other that being from and join and then select an alas these are actually components that are tightly coupled together and they occur at the same place in The Logical ordering which is why I have stacked them like this in this section we tackle the basic components that you need to master in order to write simple but powerful SQL queries and we are back here with our schema of The Logical order of SQL operations which is also our map for everything that we learn in this course but as you can see there is now some empty space in the schema because to help us manage the complexity I have removed all of the components that we will not be tackling in this section let us now learn about from and select which are really the two essential components that you need in order to write the simplest SQL queries going back now to our data let's say that we wanted to retrieve all of the data from the characters table in the fantasy data set now when you have to write a SQL query the first question you need to ask yourself is where is the data that I need because the first thing that you have to do is to retrieve the data which you can then process and display as needed so in this case it's pretty simple we know that the data we want leaves in the characters table once you figured out where your data leaves you can write the from Clause so I always suggest starting queries with the from clause and to get the table that we need we can write the name of the data set followed by a DOT followed by the name of the table and you can see that bigquery has recognized the table here so I have written the from clause and I have specified the address of the table which is where the data leaves and now I can write the select clause and in the select Clause I can specify which Columns of the table I want to see so if I click on the characters table here it will open in a new tab in my panel and as you remember the it shows me here the schema of the table and the schema includes the list of all the columns now I can simply decide that I want to see the name and the guilt and so in the select statement here I will write name and guilt and when I run this I get the table with the two columns that I need and one neat thing about this I could write the columns in any order it doesn't have to be the original order of the schema and the result will show that order and if I I wanted to get all of The Columns of the table I could write them here one by one or I could write star with which is a shorthand for saying please give me all of the columns so this is the corresponding data to our table in Google Sheets and if you want to visualize select in your mind you can imagine it as vertically selecting the parts of the table that you need for example if I were to write select Guild and level this would be equivalent to taking these two columns over here and selecting them let us now think of The Logical order of these operations so first comes the from and then comes the select and this makes logical sense right because the first thing you need to do is to Source the data and later you can select the parts of the data that you need in fact if we look at our schema over here from is the very first component in The Logical order of operations because the first thing that we need to do is to get our data we have seen that the select Clause allows us to get any columns from our table in any order but the select Clause has many other powers so let's see what else we can do with it one useful thing to know about SQL is that you can add comments in the code and comments are parts of text which are not uh executed as code they're just there for you to um keep track track of things or or explain what you are doing so I'm going to write a few comments now and the way we do comments is by doing Dash Dash and now I'm going to show you aliasing aliasing is simply renaming a column so I could take the level column and say as character level provided a new name and after I run this we can see that the name of the colum has changed now one thing that's important to understand as we now start transforming the data with our queries is that any sort of change that we apply such as in this case we change the name of the column it only affects our results it does not affect the original table that we are querying so no matter what we do here moving forward Ward the actual table fantasy characters will not change all that will change are the results that we get after running our query and of course there are ways to go back to Fantasy characters and permanently change it but that is outside the scope for us and going back to our schema you will see that Alias has its own component and it happens happens at the same time as the select component and this is important because as we will see in a bit that it's a common temptation to use these aliases these labels that we give to columns in the phases that precede this stage which typically fails because as our rules say um every component does not have access to data that is computed after it so something that we will come back to now another power of Select that we want to show is constants and constants is the ability of creating new columns which have a constant value for example let's say that I wanted to implement a versioning system for my characters and I would say that right now all the characters I have are version one but then in the future every time I change a character I will increase that version and so that will allow me to keep track of changes I can do that by simply writing one over here in the column definition and when I run this you will see that SQL has created a new column and it has put one for every Row in that column this is why we call it a constant column so if I scroll down down all of it will be one and this column has a weird name because we haven't provided a name for it yet but we already know how to do this we can use the alas sync command to say to call it version and here we go so in short when you write a column name in the select statement SQL looks for that column in the table and gives you that column but when instead you write a value SQL creates a new column and puts that value in every Row the next thing that SQL allows me to do is calculations so let me call the experience column here as well and get my data now one thing I could do is to take experience and divide it by 100 so what we see here is a new column which is the result of this calculation now 100 is a constant value right so you can imagine in the background SQL has created a new column and put 100 in every row and then it has done the calculation between experience and that new column and we get this result and and in short we can do any sort of calculation we want combining current columns and constants as well for example although this doesn't make any sense I could take experience add 100 to it divided by character level and then multiply it by two and and we see that we got an error can you understand why we got this error pause the video and think for a second I am referring to my column as character level but what is character level really it is a label that I have assigned over here now if we go back to our schema we can see that select and Alias happen at the same time so so this is the phase in which we assign our label and this is also the phase in which we try to call our label now if you look at our rules this is not supposed to work because an operation can only use data produced by operations before it and Alias does not happen before select it happens at the same time in other words this part part over here when we say character level is attempting to use information that was produced right here when we assigned the label but because these parts happen at the same time it's not aware of the label all this to say that the logical order of operations matters and that what we want here is to actually call it level because that is the name of the column in the table and now when I run this I get a resulting number and so going back to our original point we are able to combine columns and constants with any sort of arithmetic operations another very powerful thing that SQL can do is to apply functions and a function is a prepackaged piece of logic that you can apply to our data and it works like this there is a function called sqrt which stands for square root which takes a number and computes the square root so you call the function by name and then you open round brackets and in round brackets you provide the argument and the argument can be a constant such as 16 or it can be a column such as level and when I run this you can see that in this case the square root of 16 is calculated as four and this creates a constant column and then here for each value of level we have also computed the square root there are many functions in SQL and they vary according to the data type which you provide as you remember we said that knowing the data types of columns such as distinguishing between numbers and text is important because it it allows us to know which operations we can apply and so there are functions that work only on certain data types for example here we see square root which only works on numbers but we also have text functions or string functions which only work on text one of them is upper so if I take upper and provide Guild as an argument what do you expect will happen we have created a new column where the G is shown in all uppercase so how can I remember which functions there are and how to use them the short answer is I don't uh there are many many functions in SQL and here in the documentation you can see a very long list of all the functions that you can use in big query and as we said the functions vary according to the data that they can work on so if you look look here on the left we have array functions um date functions mathematical functions numbering functions time functions and so on and so on it is impossible to remember all of these functions so all you need to know is how to look them up when you need them for example if I know I need to work with numbers I could scroll down here and go to mathematic iCal functions and here I have a long list of all the mathematical functions and I can see them all on the right and I should be able to find the square root function that I have showed you and here the description tells me what the function does and it also provides some examples to summarize these are some of the most powerful things you can do with a select statement not only you can retrieve every column you need in any order you can rename columns according to your needs you can Define constant columns with a value that you choose you can combine columns and constant columns in all sorts of calculations and you can apply functions to do more complex work I definitely invite you to go ahead and put your own data in big query as a I've shown you and then start playing around with select and see how you can transform your data with it one thing worth knowing is that I can also write queries that only include select without the front part that is queries that do not reference a table let's see how that works now after I write select I clearly cannot reference any columns because there is no table but I can still reference constant for example I could say hello one and false and if I run this I get this result so remember in SQL we always query tables and we always get back tables in this case we didn't reference any previous table we've just created constants so what we have here are three columns with constant values and there is only one row in the resulting table this is useful mainly to test stuff so let's say I wanted to make sure that the square root function does what I expect it to do so I could call it right here and uh look at the result let's use this capability to look into the order of arithmetic operations in SQL so if I write an expression like this would you be able to compute the final result in order to do that you should be able to figure out the order in which all these operations are done and you might remember this from arithmetic in school because SQL applies the same order that is taught in school and we could Define the order as follows first you would execute any specific functions that take a number as Target and uh then you have multi multiplication and division then you have addition and subtraction and finally brackets go first so you first execute things that are within brackets so pause the video and apply these rules and see if you can figure out what this result will give us now let's do this operation and do it in stages like we were doing in school so first of all we want to worry about what's in the brackets right so I will now consider this bracket over here and in this bracket we have the multiplication and addition multiplication goes first so first I will execute this which will give me four and then I will have 3 + 4 + 1 which should give me 8 next I will copy the rest of the operation and here here I reach another bracket to execute what is in these brackets I need to First execute the function so this is the power function so it takes two and exponentiate it to the power of two which gives four and then 4 minus 2 will give me two and this is what we get now we can solve this line and first of all we need to execute multiplication and division in the order in which they occur so the first operation that occurs here is 4 / 2 which is 2 and I will just copy this for clarity 8 2 * 2 / 2 the next operation that occurs now is 2 * 2 which is 4 so that would be 8 4 / 2 and the next operation is 4 / 2 which is two so I will have 8 2 and all of these will give me a six now all of these are comments and we only have one line of code here and to see whether I was right I just need to execute this code and indeed I get six so that's how you can use the select Clause only to test your assumptions and uh your operations and a short refresher on the order of arithmetic operations which will be important for solving certain sequal problems let us now see how the where statement works now looking at the characters table I see that there is a field which is called is alive and this field is of type Boolean that means that the value can be either true or false so if I go to the preview here and scroll to the right I can see that for some characters this is true and for others it is false now let's say I only wanted to get those characters which are actually alive and so to write my query I would first write the address of the table which is fantasy characters next I could use the where Clause to get those rows where is a five is true and finally I could do a simple select star to get all the columns and here I see that I only get the rows where is alive is equal to true so where is effectively a tool for filtering table rows it filters them because it only keeps rows where a certain condition is true and discards all of the other rows so if you want to visualize how the wear Filter Works you can see it as a horizontal selection of certain slices of the table like in this case where I have colored all of the rows in which is alive is true now the we statement is not limited to Boolean Fields it's not limited to columns that can only be true or false we can run the we filter on any column by making a logical statement about it for example I could ask to keep all the rows where Health number is bigger than 50 this is a logical statement Health bigger than 50 because it is either true or fals for every row and of course the wh filter will only keep those rows where this statement evaluates to true and if I run this I can see that in all of my results health will be bigger than 50 and I can also combine smaller logical statements with each other to make more complex logical statements for example I could say that I want all the rows where health is bigger than 50 and is a live is equal to true now all of this becomes one big logical statement and again this will be true or false for every row and we will only keep the rows where it is true and if I run this you will see that in the resulting table the health value is always above 50 and is alive is always true in the next lecture we will see in detail how these logical statements work and how we can combine them effectively but now let us focus on the order of operations and how the wear statement fits in there when it comes to the lexical order the order in which we write things it is pretty clear from this example first you have select then from and after from you have the WHERE statement and you have to respect this order when it comes to The Logical order you can see that the where Clause follows right after the from Clause so it is second actually in The Logical order if you think about it this makes a lot of sense because the first thing that I need to do is to get the data from where it Lees and then the first thing I want to do after that is is that I'm going to drop all the rows that I don't need so that my table becomes actually smaller and easier to deal with there is no reason why I should carry over rows that I don't actually need data that I don't actually want and waste memory and processing power on it so I want to drop those unneeded rows as soon as possible and now that you know that where happens at this stage in The Logical order you can avoid many of the pitfalls that happen when you're just learning SQL let's see an example now take a look at this query I'm going to the fantasy characters table and then I'm getting the name and the level and then I'm defining a new column this is simply level divided by 10 and I'm calling this level scaled now let's say that I wanted to only keep the rows that have at at least three as level scaled so I would go here and write aware filter where level scaled bigger than three and if I run this I get an error unrecognized name can you figure out why we get this error level scaled is an alas that we assign in the select stage but the we Clause occurs before the select stage so the we Clause has no way to know about this alias in other words the we Clause is at this point and our rules say that an operation can only use data produced by operations before it so the we Clause has no way of knowing about the label which is a sign at this stage so how can we solve this problem right here the solution is to not use the Alias and to instead repeat the logic of the transformation and this actually works because it turns out that when you write logical statements in the we filter you can not only reference The Columns of the tables but you can also reference operations on columns and this way of writing operations of on columns and combinations between columns works just as what we have shown in the select part so that was all you need to know to get started with the wear clause which is a powerful Clause that allows us to filter out the row that we don't need and keep the rows that we need based on logical conditions now let's delve a bit deeper into how exactly these logical statements work in SQL and here is a motivating example for you this is a selection from the characters table and we have a wear filter and this we filter is needlessly complicated and I did this intentionally because by the end of this lecture you should have no trouble at all interpreting this statement and figuring out for which rows it will be true and likewise you will have no problem writing complex statements yourself or deciphering them when you encounter them in the wild the way that these logical statements work is through something called Boolean algebra which is an essential theory for working with SQL but also for working with any other programming language and is indeed fundamental to the way that computers work and though the name may sound a bit scary it is really easy to understand the fundamentals of Boolean algebra now let's look back at socalled normal algebra meaning the common form that is taught in schools in this algebra you have a bunch of elements which in this case I'm only showing a few positive numbers such as 0 25 100 you also have operators that act on these elements for example the square root symbol the plus sign the minus sign the division sign or the multiplication sign and finally you have operations right so in operations you apply The Operators to your elements and then you get some new elements out of them so here are two different types of operation in one case we take this operator the square root and we apply it to a single element and out of this we get another element in the second kind of operation we use this operator the plus sign to actually combine two elements and again we get another element in return Boolean algebra is actually very similar except that it's simpler in a way because you can only have two elements either true or false those are all the elements that you are working with and of course this is why when there's a Boolean field in SQL it is a column that can only have these two values which are true and false now just like normal algebra Boolean algebra has several operators that we can use to transform the elements and for now we will only focus on the three most important ones which are not and and or and finally in Boolean algebra we also have operations and in operations we combine operators and elements and get back elements now we need to understand how these operators work so let us start with the not operator to figure out how a Boolean operator works we have to look at something that's called a truth table so let me look up the truth table for the not operator and in this Wikipedia article this is available here at logical negation now first of all we see that logical negation is an operation on one logical value what does this mean it means that the not operator works on a single element such as not true or not false and this this is similar to the square root operator in algebra that works on a single element a single number next we can see how exactly this works so given an element that we call P and of course P can only be true or false the negation of p is simply the opposite value so not true is false and not false is true and we can easily test this in our SQL code so if I say select not true what do you expect to get we get false and if I do select not false I will of course get true next let's see how the end operator works so we've seen that the not operator works on a single element on the other hand the end operator connects two elements such as writing true and false and in this sense the end operator is more similar to the plus sign here which is connecting two elements so what is the result of true and false to figure this out we have to go back to our truth tables and I can see here at The Logical conjunction function section which is another word for the end operator now the end operator combines two elements and each element can either be true or false so this creates four combinations that we see here in this table and what we see here is that only if both elements are true then the end operator will return true in any other case it will return false so going back here if I select true and false what do you expect to see I am going to get false and it's only in the case when I do true and true that the result here will be true and finally we can look at the or operator which is also known as a logical disjunction it's also combining two elements it also has four combinations but in this case if at least one of the two elements is true then you get true and only if both elements are false then you get false and so going back to our SQL true or true will of course be true but but even if one of them is false we will still get true and only if both are false we will get false so now you know how the three most important operators in Boolean algebra work now the next step is to be able to solve long and complex Expressions such as this one and you already know how operators work the only information you're missing is the order of operations and just like in arithmetic we have an agreed upon order of operations that helps us solve complex expressions and the Order of Operations is written here first you solve for not then you solve for and and finally for or and as with arithmetic you first solve for the brackets so let's see how that works in practice let us now simplify this expression so the first thing I want to do is to deal with the brackets so if I copy all of this part over here as a comment so it doesn't run as code you will see that this is the most nested bracket the innermost bracket in our expression and we have to solve for this so what is true or true this is true right and now I can copy the rest of my EXP expression up to here and here I can solve the innermost bracket as well so I can say true and what I have here is false and true so this is false right because when you have end both of them need to be true for you to return true otherwise it's false so I will write false moving on to the next line I need to solve what's in the bracket so I can copy the knot and now I have to solve what's in this bracket over here now there are several operators here but we have seen that not has the Precedence right so I will copy true and here I have not false which becomes true and then I can copy the last of the bracket I'm not going to do any more at this step to avoid confusion and then I have or and I can solve for this bracket over here and true and false is actually false moving on I can keep working on my bracket and so I have a lot of operations here but I need to give precedence to the ends so the first end that occurs is this one and that means I have to start with this expression over here true and and true results in true and then moving on I will copy the or over here and now I have another end which means that I have to isolate this expression false and true results in false and finally I can copy the final end because I'm not able to compute it yet because I needed to compute the left side and I can copy the remaining part as well moving on to the next line um I need to still do the end because the end takes precedence and so this is the expression that I have to compute so I will say true or and then this expression false and true computes to false and then copy the rest now let me make some rul over here and go to the next line and I can finally compute this bracket we have true or false which we know is true next I need to invert this value because I have not true which is false and then I have or false and finally this computes to false and now for the Moment of Truth F intended I can run my code and see if the result actually corresponds to what we got and the result is false so in short this is how you can solve complex expressions in Boolean algebra you just need to understand how these three operators work and you can use truth tables like like this one over here to help you with that and then you need to remember to respect the order of operations and then if you proceed step by step you will have no problem solving this but now let's go back to the query with which we started because what we have here is a complex logical statement that is plugged into the wear filter and it isolates only certain rows and we want to understand exactly how this statement works so let us apply what we've just learned about Boolean algebra to decipher this statement now what I've done here is to take the first row of our results which you see here and just copi the values in a comment and then I've taken our logical statement and copied it here as well so let us see what SQL does when it checks for this Row the first thing that we need to do is to take all of these statements in our wear filter and convert them to true or false and to do that we have to look at our data let us start with the first component which is level bigger than 20 so for the row that we are considering level is 12 so this comes out as false next I will copy this end and here we have is alive equals true now for our row is alive equals false so this statement computes as false Mentor ID is not null with null representing absence of data in our case Mentor ID is one so it is indeed not null so here we have true and finally what we have in here is class in Mage Archer so we have not seen this before but it should be pretty intuitive this is a membership test this is looking at class which in this case is Hobbit and checking whether it can be found in this list and in our case this is now false so now that we've plugged in all the values for our row what we have here is a classic Boolean algebra expression and we are able to solve this based on what we've learned so let us go and solve this and first I need to deal with the brackets and what I have here I have an end and an or and the end TR takes precedence so false and false is false and I will copy the rest and here I have not false which is true next we have false or true which is true and true and in the end this computes to true now in this case we sort of knew that the result was meant to come out as true because we started from a row that survived the wear filter and that means that for this particular row this statement had to compute as true but it's still good to know exactly how SQL has computed this and understand exactly what's going on and this is how SQL deals with complex logical statements for each row it looks at the relevant values in the row so that it can convert the statement to a Boolean algebra expression and then it uses the Boolean algebra rules to compute a final result which is either true or false and then if this computes as true for the row then the row is kept and otherwise the row is discarded and this is great to know because this way of resoling solving logical statements applies not only to the word component but to all components in SQL which use logical statements and which we shall see in this course let us now look at the distinct clause which allows me to remove duplicate rows so let's say that I wanted to examine the class column in my data so I could simply select it and check out the results so what if I simply wanted to see all the unique types of class that I have in my data this is where distinct comes in handy if I write distinct here I will see that there are only four unique classes in my data now what if I was interested in the combinations between class and guilt in my data so let me remove the distinct from now and add guilt here and for us to better understand the results I'm going to add an ordering and here are the combinations of class and Guild in my data there is a character who is an Archer and belongs to Gondor and there are actually two characters who are archers and belong belong to mirkwood and there are many Hobbits from sholk and so on but again what if I was interested in the unique combinations of class and Guild in my data I could add the distinct keyword here and as you can see there are no more repetitions here Archer and merkwood occurs only once Hobbit and Shar f occurs only once because I'm only looking at unique combinations and of course I could go on and on and add more columns and expand the results to show the unique combinations between these columns so here Hobbit and sherol has expanded again because some Hobbits are alive and others unfortunately are not at the limit I could have a star here and what I would get back is actually my whole data all the 15 rows because what we're doing here is looking at rows that have the same value on all columns rows that are complete duplicates and there are no such rows in the data so when I do select star in this case distinct has no effect so in short how distinct works it looks at the columns that you've selected only those which you have selected and then it looks at all the rows and two rows are duplicate if they have the exact same values on every column that you have selected and then duplicate rows are removed and only unique values are preserved so just like the wear filter the distinct is a clause that removes certain rows but it is more strict and less flexible in a sense it only want does one job and that job is to remove duplicate rows based on your selection and if we look at our map of SQL operations we can place distinct it occurs right after select right and and this makes sense because we have seen that distinct Works only on the columns that you have selected and so it has to wait for select to choose the columns that we're interested in and then we can D duplicate based on those for the following lecture on unions I wanted to have a very clear example so I decided to go to the characters table and split it in two and create two new tables out of it and then I thought that I should show you how I'm doing this because it's a pretty neat thing to know and it will help you when you are working with SQL in bigquery so here's a short primer on yet another way to create a table in bigquery you can use your newly acquired power of writing cql queries to turn those queries into permanent tables so here's how you can do it first I've written a simple query here and you should have no trouble understanding it by now go to the fantasy characters table keep only rows where is alive is true and then get all the columns next we need to choose where the new table will live and how it will be called so I'm placing it also in the fantasy data set and I'm calling it characters alive and finally I have a simple command which is create table now what you see here is a single statement in SQL it's a single command that will create the table and you can have in fact multiple statements within the same code and you can run all the statements together when you hit run the trick is to separate all of them with this semicolon over here the semicolon tell SQL hey this command over here is over and and uh next I might add another one so here we have the second statement that we're going to run and this looks just like the one above except that our query has changed because we're getting rows where is alive is false and then we are calling these table characters dead so I have my two statements they're separated by semicolons and I can just hit run and I will see over here that bigquery is showing me the two statements on two different rows and you can see that they are both done now so if I open my Explorer over here I will see that I have two new tables characters alive and characters dead and if I go here for characters alive is alive will of course be true on every row now what do you think would happen if I ran this script again let's try it so I get an error the error says that the table already exists and this makes sense because I've told SQL to create a table but SQL says that table already exists I cannot create it again so there are ways that we can tell SQL what to do if the table already exists again so that we specify the behavior we want and we are not going to just get an error one way is to say create or replace table fantasy characters alive and what this will do is that if the table already exists uh big query will delete it and then create it again or in other words it will overwrite the data so let's write it down to and let's make sure that this query actually works so when I run this I will get no errors even if the table already existed because bigquery was able to remove the previous table and create a new one alternatively we may want to create the table only if it doesn't exist yet and leave it untouched otherwise so in that case we could say create table if not exists so what this will do is that if this table is already existing big query won't touch it and it won't throw an error but if it doesn't exist it will create it so let us write it down two and make sure that this query runs without errors and we see that also here we get no errors and that in short is how you can save the results of your queries in big query and make them into fullfledged tables that you can save and and create query at will and I think this is a really useful feature if you're analyzing data in big query because any results of your queries that you would like to keep you can just save them and then come back and find them later let's learn about unions now to show you how this works I have taken our characters table and I have split it into two parts and I believe the name is quite self descriptive there is a separate table now for characters who are alive and a separate table for characters who are dead and you can look at the previous lecture to see how I've done this how I've used a query to create two new tables but this is exactly the characters table with you know the same schema the same columns the same times is just split in two based on the E alive column now now let us imagine that we do not have the fantasy. characters table anymore we do not have the table with all the characters because it was deleted or we never had it in the first place and let's pretend that we only have these two tables now characters alive and characters dead and we want to reconstruct the characters table out of it we want to create a table with all the characters how can we do that now what I have here are two simple queries select star from fantasy characters alive and select star from fantasy characters dead so these are two separate queries but actually in big query there are ways to run multiple queries at the same time so I'm going to show you first how to do that now an easy way to do that is to write your queries and then add a semicolon at the end and so what you have here is basically a SQL script which contains multiple SQL statements in this case two and if you hit run all of these will be executed sequentially and when you look at the results so you're not just getting a table anymore because it's not just a single query that has been executed but you can see that there have been two commands uh that have been executed which are here and then for each of those two you can simply click View results and you will get to the familiar results tab for that and if I want to see the other one I will click on the back arrow here and click on the other view results and then I can see the other one another way to handle this is that I can select the query that I'm interested in and then click run and here I see the results so big query has only executed the part that I have selected or I can decide to run the other query in my script select it click run and then I will see the results for that query and this is a pretty handy functionality in big query it's also functionality that might give you some headaches if you don't know about it because if for some reason you selected a part of the code uh during your work and then you just want to run everything you might hit run and get an error here because B queer is only seeing the part that you selected and cannot make sense of it so it's good to know about this but our problem has not been solved yet because remember we want to reconstruct the characters table and what we have here are two queries and we can run them separately and we can look at the results separately but we still don't have a single table with all the results and this is where Union comes into play Union allows me to stack the results from these two tables so so if I take first I will take off the semic columns because this will become a single statement and then in between these two queries I will write Union distinct and when I run this you can verify for yourself we have 15 rows and we have indeed reconstructed the characters table so what exactly is going on here well it's actually pretty simple SQL is taking all of the rows from this first query and then all of the rows for the second query and then it's stacking them on top of each other so you can really imagine the act of horizontally stacking a table on top of the other to create a new table which contains all of the rows of these two queries combined and that in short is what union does now there are a few details that we need to know when working with Union and to figure them out let us look at a toy example so I've created two very simple tables toy one and toy two and you can see how they look in these comments over here they all have three columns which are called imaginatively call One Call two call three and then this is the uh Toy one table and then this is the toy 2 table now just like before we can combine this table tabls by selecting all of them and then writing a union in between them now in B query you're not allow to write Union without the further qualifier a keyword and it has to be either all or distinct so you have to choose one of these two and what is the choice about well if you do Union all you will get all of the rows that are in the first table and those that are in the second table regardless of whether they are duplicate okay but with Union distinct you will get again all of the rows from the two tables but you will only consider unique rows you will not get any duplicates now we can see that these two table share a column which is actually identical one through yes over here and the same row over here now if I write Union all I expect the result to include this row twice so let us verify that and you can see that here you have one true yes and at the end you also have one true yes and in total you get four rows which are all the rows in the two tables however if I do Union distinct I expect to get three rows and I expect this row to appear only once and not to be duplicated again you need to make sure you're not selecting any little part of your script before you run it so the whole script will be run and as you can see we have three rows and there are no duplicates now it's interesting that big query actually forces you to choose between all or distinct because in many SQL systems for your information you are able to write Union without any qualifier and in that case it means Union distinct so in other SQL systems when you write Union it is understood that you want Union distinct and if you actually want to keep the duplicate rows you will explicitly write Union all but in big query you always have to explicitly say whether you want Union all or Union distinct now the reason this command is called Union and not like stack or or something else is is that this is a set terminology right this comes from the mathematical theory of sets which you might remember from school and the idea is that a table is simply a set of rows so this table over here is a set of two rows and this table over here is a set of two rows and once you have two sets you can do various set operations between them and the most common operation that we do in SQL is unioning and unioning means combining the values of two sets so you might remember from school the V diagram which is a typical way to visualize the relations between sets so in this simple vent diagram we have two circles A and B which represent two sets and in our case a represents the collection of rows in the first table and B represents the all the rows that are in the second table so what does it mean to Union these sets it means taking all of the elements that are in both sets so taking all of the rows that are in both tables and what is the difference here between union distinct and Union all where you can see that the rows of a are this part over here plus this part over here and the rows of B are this part over here plus this part over here and so when we combine them we're actually counting the intersection twice we are counting this part twice and so what do you do with this double counting do you keep it or do you discard it if you do Union all you will keep it so rows that are in common between A and B will duplicate you will see them twice twice but if you do Union distinct you will discard it and so um you won't have any duplicates in the results so that's one way to think about it in terms of sets but we also know that Union is not the only set operation right there are other set operations a very popular one is the intersect operation now the intersect looks like this right it it says take only the El elements that are in common between these two sets so can we do that in SQL can we say give me only the rows that are in common between the two tables and the answer is yes we can do this and if we go back here we can instead of Union write intersect and then distinct and what do you expect to see after I run this command take a minute to think about it so what I expect to see is to get only the rows that are shared between the two tables now there is one row which is shared between these two tables which is uh the one true yes row which we have seen and if I run this I will get exactly this row so intersect distinct gives me the rows that are shared between the two tables and I have to write intersect distinct I cannot write intersect all because actually doesn't mean anything so it's not going to work and here's another set operation which you might consider which is subtraction so what if I told you give me all of the elements in a except the elements that a shares with B so what would that look on the drawing it would look like this right so this is taking all of the elements that are in a except these ones over here because they are in a but they're also in B and I don't want the elements shared with b and yes I can also do that in squl I can come here and I could say give me everything from Toy one except distinct everything from Toy two and what this means is that I want to get all of my rows from Toy one except the rows that are shared with toy two so what do you expect to see when I run this let's hit run and I expect to see only this row over here because this other row is actually shared with b and this is what I get again you have to write accept distinct you cannot write accept all because it's actually actually doesn't mean anything and keep in mind that unlike the previous two operations which are union and distinct the accept operation is not symmetric right so if I swap the tables over here I actually expect to get a different result right I expect to see this row over here selected because I'm saying give me everything from this table uh Toy 2 except the rows that are shared with toy one so so let us run this and make sure and in fact I get the three through uh maybe row so careful that the accept operation is not symmetric the order in which you put the two tables matters so that was a short overview of Union intersect except and I will link this here which is the bigquery documentation on this and you can see that they're actually called set operators in fact in real life you almost always see Union very rarely you will see somebody using intersect or accept a lot of people also don't know about them but I think it's worth it that we briefly looked at all three and it's especially good for you to get used to thinking about tables as sets of rows and thinking about SQL operations in terms of set set operations and that will also come in handy when we study joints but let us quickly go back to our toy example and there are two essential prerequisites for you to be able to do a union or any type of sort operations number one the tables must have the same number of columns and number two the columns must have the same same data type so as you can see here we are combining toy 2 and toy 1 and both of them have three columns and the First Column is an integer the second is a Boolean and the third is a string in both tables and this is how we are able to combine them so what would happen if I went to the first table and I got only the first two columns and then I tried to combine it you guessed it I would get an error because I have a mismatched column count so if I want to select only the first two columns in a table I need to select only the first two columns in another table and then the union will work now what would happen if I messed up the order of the columns so let's say that here I will select uh column one and column 3 and here I will select column one and column two let me run this and I will get an error because of incompatible types string and bull so what's happening here is that SQL is trying to get the values of call three over here and put it into call two over here and it's trying to get a string and put it into a Boolean column and that simply doesn't work because as you know SQL enforces streak Types on columns and so this will not work but of course I could select call three in here as well and now again we will have a string column going into a string column and of course this will work so so to summarize you can Union or intersect or accept any two tables as long as they have the same number of columns and the columns have the same data types let us now illustrate a union with a more concrete example so we have our items table here and our characters table here so the items table repres represents like magical items right while the characters table we're familiar with it represents actual characters so let's say that you are managing a video game and someone asks you for a single table that contains all of the entities in that video game right and the entities include both characters and items so you want to create a table which combines these two tables into one we know we can use Union to do that we know we can use Union to stack all the rows but we cannot directly Union these two tables be because they have a different schema right they have a different number of columns and then those columns have different data types but let's analyze what these two tables have in common and how we could maybe combine that so first of all they both have an ID and in both cases it's an integer so that's already pretty good they both have a name and in both cases the name is a string so we can combine that as as well the item type can be thought of being similar to the class and then each item has a level of power which is expressed as an integer and each character has a level of experience which is expressed as an integer and you can think that they are kind of similar and then finally we have a timestamp field representing a moment in time for both columns which are date added and last active so looking at this columns that the two have sort of in common we can find a way to combine them and here's how we can translate this into SQL right so I'm went to the fantasy items table and I selected The Columns that I wanted and then I went to the characters table and I selected the columns that I wanted to combine with those um in in the right order so we have ID with ID name with name class with item type level with power and last active with date added so I have my columns they're in the right order I wrote Union distinct and if I run this you will see that I have successfully combined the rows from these two tables by finding out which columns they have in common and then writing them in the right order and then adding Union distinct now all the columns that we've chosen for the combination have the same type but what would happen if I wanted to combine two columns that are not actually the same type so let's say what if we wanted to combine Rarity which is a string with experience which is an integer as you know I cannot do this directly but I can go around it by either taking Rarity and turning it into an integer or taking um experience and turning it into a string I just have to make sure that they both have the same data type now the easiest way is usually to take um any other data type and turn it into a string because we you just turn it into text so let's say that for the sake of this demonstration we will take integer experience which is an integer and turn it into a string which is text and then combine that with Rarity so I will go back to my code and I will make some room over here and here in items I will add Rarity and here in characters I will add experience and you can see that I already get an error here saying that the union distinct has incompatible types just like expected so what I want to do here is to take experience and turn it into string and I can do that with the cast function so I can do cast experience as string and what this will do is basically take these values and convert them to string and if I run this you can see that this has worked so we combined two tables into one and now the result is a single table it has a column called Rarity the reason it's called Rarity is that um it's it's taking the name from the first table in the in the operation but we could of course rename it to whatever we need and this is now a text column because we have combined a text column with also a text column thanks to the casting function so what we see here are a bunch of numbers which came originally from The Experience uh column from the character table but they're now converted to text and if I scroll down then I will also see the original values of Rarity from the items table finally let us examine Union in the context of The Logical order of SQL operations so you can see here that we have our logical map but it looks a bit different than usual and the reason it's different is that we are considering what happens when you un two tables and here the blue represents one table and the red represents the other table so I wanted to show you that all of the ordering that we have seen until now so first get the table then use the filter with where then select the columns you want and if you want use this thing to remove duplicates all of these happens in the same order separately for the two tables that you are unioning and this applies to all of the other operations like joining and grouping which we will see um later in the course so at first the two tables are working on two separate tracks and SQL is doing all this operations on them in this specific order and only at the end of all this only after all of these operations have run then we have the union and in the Union these two tables are combined into one and only after that only after the tables have been combined into one you apply the last two operations which are order by and limit and actually nothing forces you to combine only two tables you could actually have any number of tables that you are combining in Union but then the logic doesn't change at all all of these operations will happen separately for each of the tables and then only when all of these operations are done only when all of the tables are ready then they will be combined into one and if you think about it it makes a lot of sense because first of all you need the select to have run in order to know what is the schema of the tables that you are combining and then you also also need to know if distinct has run on each uh table because you need to know which rows you need to combine in the union and that is all you need to know to get started with Union this very powerful statement that allows us to combine rows from different tables let us now look at order by so I'm looking at the characters table here and as you can see we have an ID column that goes from one to 15 which assigns an ID to every character but you will see that the IDS don't appear in any particular order and in fact this is a general rule for SQL there is absolutely no order guarantee for your data your data is not stored in any specific order and your data is not going to be returned in any specific order and the reason for this is fun fundamentally one of efficiency because if we had to always make sure that our data was perfectly ordered that would add a lot of work it would add a lot of overhead to the engine that makes the queries work and uh there's really no reason to do this however we do often want to order our data when we are querying it we want to order the way that it is displayed and this is why the order by clause is here so let us see how it works I am selecting everything from fantasy characters and again I'm going to get the results in no particular order but let's say I wanted to see them in uh ordered by name so then I would do order by name and as you can see the rows are now ordered alphabetically according to the name I could also invert the order by writing desk which stands for descending and that means U descending alphabetical order which means from the last letter in the alphabet to the first I can of course also order by number columns such as level and we would see that the level is increasing here and of course that could also be descending to to go in the opposite direction and the corresponding keyword here is ask which stands for ascending and this is actually the default Behavior so even if you omit this you will get the same going from the smallest to the largest I can also order by multiple columns so I could say order by class and then level and what that looks like is that first of all the rows are ordered by class so as you can see this is done alphabetically so first Archer and then the last is Warrior and then within each class the values within the class are ordered according to the level going from the smallest level to the biggest level and I can invert the order of one of them for example class and in this case we will start with Warriors and then within the warrior class we will still will order the level in ascending order so I can for every column uh that's in the ordering I can decide whether that ordering is in ascending order or descending order now let us remove this and select the name and the class and once again I get my rows in no particular order and I'm seeing the name and the class so I wanted to show you that you can also order by columns which you have not selected Ed so I could order these elements by level even though I'm not looking at at level and it will work all the same and finally I can also order by operations so I could say take level divide it by experience and then multiply that by two for some reason and it would also work in the order ordering even though I am not seeing that calculation that calculation is being done in the background and used for the ordering so I could actually take this here and copy it create a new column call it calc for calculation and if I show you this you will see the results are not uh very meaningful but you will see that they are in ascending order so we have ordered by that and sometimes you will see this notation over here order by 21 for example and as you can see what we've done here is that we've ordered by class first of all because we starting with archers and going to Warriors and then within each class we are ordering by name uh also in ascending order so this is basically referring to the columns that are referenced in the select two means order by the second column which you have referenced which in this case is class and one means order by the First Column that you referenced so it's basically a shortcut that people sometimes use to avoid rewriting the names of columns that they have selected and finally when we go back to the order of operations we can see that order bu is happening really at the end of all of this process so as you will recall I have created this diagram that's a bit more complex to show show what happens when we Union different tables together what happens is that basically all these operations they run independently on each table and then finally the tables get uh unioned together and after all of this is done SQL knows the final list of rows that we will include in our results and that's the right moment to order those rows it would not be possible to do that before so it makes sense that order is located here let us now look at the limit Clause so what I have here is a simple query it goes to the characters table it filters for the rows where the character is alive and then it gets three columns out of this so let's run this query and you can see that this query returns 11 rows now let us say that I only wanted to see five of those rows and this is where limit comes into place limit will look at the final results and then pick five rows out of those results reducing the size of my output and here you can see that we get five rows now as we said in the lecture of ordering by default there is no guarantee of order in a SQL system so when you are getting all your data with a query and then you run limit five on top of it you have no way of kn knowing which of the rows will be selected to fit amongst those five you're basically saying that you're okay with getting any five of all of the rows from your result because of this people often will use limit in combination with order by for example I could say order by level and then limit five and what I would get here is essenti the first five most inexperienced characters in my data set and let us say that you have a problem of finding the least experienced character in your data the character with the lowest level so of course you could say order by level and then limit one and you would get the character with the lowest level right and this works however it is not ideal there is a problem with this solution so can you figure out what the problem with this solution is the problem will be obvious once I go back to limit 5 and I look here and I see that I actually have two characters which have the lowest level in my data set so in theory I should be able to return both of them because they both have the lowest level however when I write limit one it simply cuts the rows in my output and it is unaware of that uh further information that is here in this second row and in the further lectures we will see how we can solve this better and get results which are more precise and if we look at The Logical order of operations we can see that limit is the very last operation and so all of the logic of our query is executed all our data is computed and then based on that final result we sometimes decide to not output all of it but to Output a limited number of rows so a common mistake for someone who is starting with SQL is thinking that they can use limit in order to have a cheaper query for example you could say oh this is a really large table this table has two terabytes of data it would cost a lot to scan the whole table so I will say select star but then I will put limit 20 because I only want to see the first 20 rows and that will means that I will only scan 20 rows and my query will be very cheap right no that is actually wrong that doesn't save you anything and you can understand this if you look at the map because all of the logic is going to execute before you get to limit so you're going to scan the whole table when you say select star and you're going to apply all of the logic and the limit is actually just changing the way your result is displayed it's not actually changing the way the your result is computed if you did want to write your query so that it scans less rows one thing you should do is focus on the where statement actually because the where statement is the one that runs in the beginning right after getting the table and it is able to actually eliminate rows which usually saves you on computation and money and so on however I do need to say that there are systems where writing limit may actually turn into savings because different systems are optimized in different ways and um allow you to do different things with the commands but as a rule usually with SQL limit is just changing the way your result is displayed and doesn't actually change anything in the logic of execution let us now look at the case clause which allows us to apply conditional logic in SQL so you can see here a simple query I am getting the data from the characters table I am filtering it so that we only look at characters who are alive and then for each character we're getting the name and the level now when you have a column that contains numbers such as level one typical thing that you do in data analysis is bucketing and bucketing basically means that I look at all these multiple values that level can have and and I reduce them to a smaller number of values so that whoever looks at the data can make sense of it uh more easily now the simplest form of bucketing that you can have is the one that has only two buckets right so looking at level our two buckets for example could be uh in one bucket we put values that are equal or bigger than 20 so characters who have a level that's at least 20 and in the other bucket we put all the characters that have a level that is less than 20 for example now how could I Define those two buckets so we know that we can Define new columns in the select statement and that we can use calculations and logical statements to define those columns so one thing that I could do would be to go here and then write level bigger than bigger or equal than 20 and then call this new column level at least 20 for example and when I run this I get my column now of course this is a logical statement and for each row this will be true or false and then you can see that our new column here gives us true or false on every column and this is a really basic form of bucketing because it allows us to take you know level has basically 11 different values in our data and it can be complicated to look at this many values at once and now we've taken these 11 values and reduced them to two uh to two buckets so that we have um organized our data better and it's easier to read but there are two limitations with this approach one I might not want to call my buckets true or false I might want to give more informative names to my buckets such as experienced or inexperienced for example the other limitation is that with this approach I can effectively only divide my data in two buckets because once I write a logical statement it's either either true or false so my data gets divided in two but often it's the case that I want to use multiple buckets for my use case now bucketing is a typical use case for the case when statement so let's see it in action now so let me first write a comment not any actual code where I Define what I want to do and then I will do it with the code so I have written here the buckets that I want to use to classify the characters level so up to 15 they are considered low experience between 15 and 25 they are considered mid and anything above 25 we will classify as super now let us apply the case Clause to make this work so the case Clause Is Always bookended by these two parts case and end so it starts with case it ends with end and a typical error when you're getting started is to forget about the end part so my recommendation is to always start by writing both of these and then going in the middle to write the rest now in the middle we're going to Define all the conditions that we're interested in and each condition starts with the keyword when and is Then followed by a logical condition so our logical condition here is level smaller than 15 now we have to Define what to do when this condition is true and it follows with the keyword then and when this condition is true we want to return the value low which is a string a piece of text that says low next we proceed with the following condition so when level is bigger and equal to 15 and level is lower than 25 so if you have trouble understanding this logical statement I suggest you go back to the lecture about Boolean algebra but what we have here there are two micro statements right Level under 25 and level equal or bigger than 15 they are conect connected by end which means that both of these statements have to be true in order for the whole statement to be true which is what we want in this case right and what do we want to return in this case we will return the value mid and the last condition that we want to apply when level is bigger or equal than 25 then we will return super now all of this that you see here this is the case Clause right or the case statement and all of this is basically defining a new column in my table and given that it's a new column I can use the alas sync to also give it a name and I can call this level bucket now let's run this and see what we get and as you can see we have our level bucket and the characters that are above 25 are super and then we have a few Ms and then everyone who's under 15 is low so we got the results we wanted and now let us see exactly how the case statement works so I'm going to take Gandalf over here and he has level 30 so I'm going to write over here level equals 30 because we're looking at the first low row and that is the value of level and then I'm going to take the conditions for the case statement that we are examining and add them here as a comment now because in our first row level equals 30 I'm going to take the value and substitute it here for level now what we have here is a sequence of logical statements and we have seen how to work with these logical statements in the lecture on Boolean algebra now our job is to go through each of these logical statements in turn and evaluate them and then as soon as we find one that's true we will stop so the first one is 30 smaller than 50 now this is false so we continue the second one is a more complex statement we have 30 greater or equal to 15 which is actually true and 30 Oops I did not substitute it there but I will do it now and 30 smaller than 25 which is false and we know from our Boolean algebra that true and false evaluates to false therefore the second statement is also false so we continue and now we have 30 greater or equal than 25 which is true so we finally found a line which evaluates as true and that means that we return the value super and as you can see for Gandalf we have indeed gotten the value super let us look very quickly at one more example we get Legolas which is level 22 and so I will once again copy this whole thing and comment it and I will substitute 22 for every value of level cuz that's the row we're looking at and then looking at the first row 22 small than 15 is false so we proceed and then looking at the second row 22 bigger than 15 is true and 22 smaller than 25 is also true so we get true and true which evaluates to true and so we return mid and then looking at Legolas we get mid so this is how the case when statement Works in short for each row you insert the values that correspond to your Row in this case the value of level and then you evaluate each of these logical conditions in turn and as soon as one of them returns true then you return the value that corresponds to that condition and then you move on to the next row now I will clean this up a bit and now looking at this statement now and knowing what we know about the way way it works can we think of a way to optimize it to make it nicer to remove redundancies think about it for a minute now one thing we could do to improve it is to remove this little bit over here because if you think about it this part that I have highlighted is making sure that the character is not under 15 so that it can be classified as meat but actually we already have the first condition that makes sure that if the character is under 15 then the statement will output low and then move on so if the character is under 15 we will never end up in the second statement but if we do end up in the second statement we already know that the character is not under 15 this is due to the fact that case when proceeds condition by condition and exits as soon as the condition is true so effectively I can remove this part over here and then at the second condition only make sure that the level is below 25 and you will see if you run this that our bucketing system works just the same and the other Improvement that I can add is to replace this last line with an else CL Clause so the else Clause takes care of all the cases that did not meet any of the conditions that we specified so the case statement will go condition by condition and look for a condition that's true but in the end if none of the conditions were true it will return what the else Clause says so it's like a fallback for the cases when none of our conditions turned out to be true and if you look at our logic you will see that if this has returned false and this has returned false all that's left is characters that have a level which is either 25 or bigger than 25 so it is sufficient to use an else and to call those super and if I run this you will see that our bucketing works just the same for example Gandalf is still marked as super because in the case of Gandalf this condition has returned false and this condition has returned false and so the else output has been written there now what do you think would happen if I completely removed the else what do you think would happen if I only had two conditions but it can be the the case that none of them is true what will SQL do in that case let us try it and see what happens so the typical response in SQL when it doesn't know what to do is to select the null value right and if you think about it it makes sense because we have specified what happens when level is below 15 and when level is is below 25 but none of these are true and we haven't specified what we want to do when none of these are true and because we have been silent on this issue SQL has no choice but to put a null value in there so this is practically equivalent to saying else null this is the default behavior for SQL when you don't specify an else Clause now like every other piece of SQL the case statement is quite flexible for instance you are not forced to always create a text column out of it you can also create an integer column so you could Define a simpler leveling system for your characters by using one and two else three for the higher level characters and uh this of course will also work as you can see here however one thing that you cannot do is to mix types right because what this does is that it results in one column in a new column and as you know in SQL you're not allowed to mix types between columns so always keep it consistent when it comes to typing and then when it comes to writing the when condition all the computational power of SQL is available so you can reference columns that you are not selecting you can run calculations as I am doing here and you can change logical statements right Boolean statements in complex ways you can really do anything you want although I generally suggest to keep it as simple as possible for your sake and the sake of the people who use your code and that is really all you need to know to get started with the case statement to summarize the case statement allows us to define a new columns whose values are changing conditional on the other values of my row this is also called conditional logic which means that we consider several conditions and then we do have different behaviors based on which condition is true and the way it works is that in the select statement when you are mentioning all your columns you create a new column which in our case is this one and you bookend it with a case and end and then between those you write your actual conditions so every condition starts with a when is followed by a logical statement which needs to evaluate to true or false and then has the keyword then and then a value and then the case when statement will go through each of these conditions in turn and as soon as one of them evaluates to true you will output the value that you have specified if none of the conditions evaluate to true then it will output the value that you specify in the else keyword and if the lse keyword is missing it will output null and so this is what you need to use the case statement and then experience and exercise and coding challenges will teach you when it's the case to use it pun intended now where does the case statement fit in our logical order of SQL operations and the short answer is that it is defined here at the step when you are selecting your columns that's when you can use the case when statement to create a new column that applies your conditional logic and this is the same as what we've shown in the lecture on SQL calculations you you can use select statement not only to get columns which already exist but to Define new columns based on calculations and logic now let us talk about aggregations which are really a staple of any sort of data analysis and an aggregation is a function that takes any number of values and compresses them down to a single informative value so I'm looking here at at my usual characters table but this is the version that I have in Google Sheets and as you know we have this level column which contains the level of each character and if I select this column in Google Sheets you will see that in the bottom right corner I can see here a number of aggregations on this column and like I said no matter how many values there are in the level columns I can use aggregations to compress them to one value and here you see some of the most important aggregations that you will work with some simply adding up all values together the average which is doing the sum and then dividing by the number of values the minimum value the maximum the count and the count numbers which is the same here so these are basically summaries of my column and you can imagine in cases where where you have thousands or millions of values how useful these aggregations can be for you to understand your data now here's how I can get the exact same result in SQL I simply need to use the functions that SQL provides for this purpose so as you can see here I'm asking for the sum average minimum maximum and count of the column level and you can see the same results down here now now of course I could also give names to this column for example I could take this one and call it max level and in the result I will get a more informative column name and I can do the same for all columns now of course I can run aggregations on any columns that I want for example I could also get the maximum of experience and call this Max experience and I can also run aggregations on calculations that involve multiple columns as well as constants so everything we've seen about applying arithmetic and logic in SQL also applies now of course looking at the characters table we know that our columns have different data types and the behavior of the aggregate functions also is sensitive to the data types of the columns for example let us look at the many text columns that we have such as class now clearly not all of the aggregate functions that we've seen will work on class because how would you take the average of these values it's not possible right however there are some aggregate functions that also work on strings so here's an example of aggregate functions that we can run on a string column such as class first we have count which simply counts the total number of non null values and I will give you a bit more detail about the count functions soon then we have minimum and maximum now the way that strings are ordered in SQL is something called lexicographic order which is basically a fancy word for alphabetical order and basically you can see here that for minimum we get the text value that occurs earlier in uh alphabetical order whereas Warrior occurs last and finally here's an interesting one called string EG and what this does is that this is a function that actually takes two arguments the first argument as usual is the name of the column and the second argument is a separator and what this outputs is now a single string a single piece of text where all of the other pieces of text have been glued together and then separated by this character that we specified over here which in our case is a comma Now if you go to the Google documentation you will find an extensive list of all the aggregate functions that you can use in Google SQL and this includes the ones that we've just seen such as average or Max as well as a few others that we will not explore in detail here so let us select one of them such as average and see what the description looks like now you can see that this function Returns the average of all values that are not null and don't worry about this expression in an aggregated group for now just think about this as meaning all the values that you provide to the function all the values in the column now there is a bit about window functions which we will see later and here there are in the caveat section there are some interesting edge cases for example what happens if you use average on an empty group or if all values are null in that case it returns null and so on you could see what the function does when it finds these edge cases and here is perhaps the most important section which is supported argument types and this tells you what type of columns you can use this aggregation function on so you can see that you can use average on any numeric input type right any column that contains some kind of number and also on interval and interval we haven't examined it in detail but this is actually a data type that specifies a certain span of time so interval could express something like 2 hours or 4 days or 3 months it is a quantity of time and finally in this table returned data types you can see what the average function will give you based on the data type that you insert so if you insert uh integer column it will return to you a float column and that makes sense because the average function involves a division and that division will usually give you floating Point values but for any other of the allowed input types such as numeric bit numeric and so on and these are all data types which represent numbers in B query the average function as you can see here will present Reserve that data type and finally we have some examples so whenever you need to use an aggregate function that is whenever you need to take many values a sequence of multiple values and compress them all down to one value but you're not sure about which function to use or what the behavior of the function is you can come to this page and look up the functions that interest you and then read the documentation to see how they work now here's an error that typically occurs when starting out with aggregations so you might say well I want to get the name of each character and their level but I also want to see the average of all levels and because I want to compare those two values I want to compare the level of my character with the average on all levels so I can write a query that looks like this right go to the Fant as a characters table and then select name level and then average level but as you can already see this query is not functioning it's giving me an error and the error says that the select list expression references column name which is neither grouped nor aggregated so what does this actually mean to show you what this means I've gone back to my Google Sheets where I have the same data for my characters table and I have copy pasted our query over here now what this query does it takes the name column so I will copy paste it over here and then it takes the level column copy paste this here as well and then it computes the average over level now I can easily compute this with sheet formula by writing equal and then calling the function which is actually called average and then within the function I can select all these values over here and I get the average now this is the result that SQL computes but SQL is actually not able to return this result and the reason is that there are three columns but they have mismatch number of values specifically these two columns have 15 values each whereas this column has a single value and SQL is not able to handle this mismatch because as a rule every SQL query needs to return a table and a table is a series of columns where each column has the same number of values if that constraint is not respected you will get an error in SQL and we will come back to this limitation when we examine Advanced aggregation techniques but for now just remember that you can mix nonaggregated columns with other nonaggregated columns such as name and level and you can mix aggregated columns with aggregated columns such as average level with some level for example so I could simply do this and I would be able to return this as a table because as you can see there are two columns both columns have a single Row the number of rows matches and this is actually valid but you might ask can't I simply take this value over here and just copy it in every row and until I make sure that average level has the same number of values as name and level and so return a table and respect that constraint indeed this is possible you can totally do this and then it would work and then this whole table would become a single table and you would be able to return this result however this requires the use of window functions which is a a feature that we will see in later lectures but yes it is totally possible and it does solve the problem now here's a special aggregation expression that you should know about because it is often used which is the count star and count star is simply counting the total number of rows in a table and as you can see if I say from fantasy characters select count star I get the total count of rows in my results and this is a common expression used across all SQL systems to figure out how many rows a table has and of course you can also combine it with filters with the wear clause in order to get other types of measures for example I could say where is alive equals true and then the count would become actually the count of characters who are alive in my data so this is a universal way to count rows in SQL although you should know that if you're simply interested in the total rows of a table and you are working with bigquery an easy and totally free way to do it is to go to the details Tab and look at the number of rows here so this was all I wanted to tell you about simp Le aggregations for now and last question is why do we call them simple simple as opposed to what I call them simple because the way we've seen them until now the aggregations take all of the values of a column and simply return One summary value for example the sum agregation will take all of the values of the level column and then return a single number which is the sum of all levels and more advanced aggregations involved grouping our data for example a question we might ask is what is the average level for Mages as opposed to the average level for Archers and for Hobbits and for warriors and so on so now you're Computing aggregations not over your whole data but over groups that you find in your data and we will see how to do that in the lecture on groupi but for now you can already find out a lot of interesting stuff about your data by running simple aggregations let us now look at subqueries and Common Table expressions and these are two fundamental functionalities in SQL these functionalities solve a very specific problem and the problem is the following sometimes you just cannot get the result you require with a single query sometimes you have to combine multiple SQL queries to get where you need to go so here's a fun problem that will illustrate my point so we're looking at the characters table and we have this requirement we want to find all those characters whose experience is between the minimum and the maximum maximum value of our experience another way to say this we want characters who are more experienced than the least experienced character but less experienced than the most experienced character in other words we want to find that middle ground that is between the least and the most experienced characters so let us see how we could do that uh I have here A Simple Start where I am getting the name and experience column from the characters table now let us focus on the first half of the problem find characters who have more experience than the least experienced character now because this is a toy data set I can sort of eyeball it so I can scroll down here and I can see that the lowest value of experience is pipin with 2100 and so what I need to do now is to filter out from this table all the rows that have this level of experience but apart from eyeballing how would we find the lowest level of experience in our data if you thought of aggregate functions you are right so we have seen a in a previous lecture that we have aggregated functions that take any number of values and speed out a single value that's a summary for example meing minum and maximum and indeed we need to use a function like that for this problem so your first instinct might be let us take this table and let us filter out rows in this way so let's say where experience is bigger than the minimum of experience and on the surface this makes sense right I am using an aggregation to get the smallest value of experience and then I'm only keeping rows that have a higher value than that however as you see from this red line this actually does not work because it tells us aggregate function is not allowed in the work Clause so what is going on here so if you followed the lecture on aggregation you might have a clue as to why this doesn't work but it is good to go back to to it and understand exactly what the problem is so I'm going back to my Google sheet over here where I have the exact same data and I copied our current query down here and now let's see what happens when SQL tries to run this so SQL goes to the fantasy characters table and the Second Step In The Logical order as you remember is to filter it and for the filter it has to take the column of experience so let me take this column and copy it down here and then it has to compute minimum of experience right so I will Define this column here and I will use Google Sheets function to achieve that result so equals mean and then selecting the numbers and here I get the minimum value of experience and now SQL has to compare these column but this comparison doesn't work right because these are two columns that have a different number of rows they have a different number of values so SQL is not able to do this comparison you cannot do an element by element comparison between a column that has 15 values and a column that has a single value so SQL throws an error but you might say wait there is a simple solution to this just take this value and copy it all over here until you have two columns of the same size and then you can do the comparison indeed that would work that's a solution but SQL doesn't do it automatically whereas if you work with other analytics tools such as pandas in python or npy you will find that um in a situation like this this would be done automatically this would be copied all over here and there's a process called broadcasting for that but SQL does not take so many assumptions and so many risks with your data if it literally doesn't work then SQL will not do it so hopefully now you have a better understanding of why this solution does not work so how could we actually approach this problem now a Insight is that I can run a different query so I will open this on the right to find out the minimum experience right I can go back to the characters table and I can select the minimum of experience this is simply what we've learned to do in the lecture on aggregations and I get the value here that is the minimum value of experience now that I know the minimum value of experience I could simply copy this value and insert it here into a wear filter and if I run this this will actually work it will solve my problem the issue of course is that I do not want to hard code this value first of all it is not very practical to run a separate query and copy paste the value in the code and second the minimum value might change someday and then I might not remember to update it in my code and then this whole query would become invalid to solve this problem I will use a subquery and I will simply delete the hardcoded value and I will open round brackets which is a way to get started on a subquery and I will take the query that I have over here and put them put it in the round brackets and when I run this I get the result that I need so what exactly is going on here we are using a subquery or in other words a query within a query so when SQL looks at this code it says all right so this is the outer query right and it has a inner query inside it a nested query so I have to start with the innermost query I have to start with the nested query so let me compute this and so SQL runs this query first and then it gets a value out of it which in our case we know that is 2100 and after that SQL substitutes this code over here by the value that was computed and we know from before that this works as expected and to compute the other half of our problem we want our character to have less experience than the most experienced character so this is just another condition in the wear filter and so I can add an end here and copy this code over here except that now I want my experience to be smaller than the maximum of EXP experience in my table now you might know this trick that if you select only part of your code like this and then you click run SQL will only execute that part of the code and so here we get the actual maximum for our experience and we can write it here in the comment and now we know that when SQL runs this query all of these will be computed to 15,000 and then experience will will be compared on that and the query will work as intended and here is the solution to our problem now here's the second problem which shows another side of subqueries we want to find the difference between a character's experience and their mentors so let us solve it manually for one case in the characters table so let us look at this character over here which is Saran with id1 and their experience is 8500 and then Saruman has character id6 as their Mentor so if I look for id6 we have Gandalf this is not very Canon compared to the story but let's just roll with it and Gandalf has 10,000 of experience and now if we select the experience of Gandalf minus the experience of Saran we can see that there is A500 difference between their experience and this is what I want to find with my query now back to my query I will first Alias my columns in order to make them more informative and this is a great trick trick to make problems clearer in your head assign the right names to things so here instead of ID I will call this mentee ID and here I have Mentor ID and here instead of experience I will call this Mente experience so I have just renamed my columns now the missing piece of the puzzle is the mentor experience right so how can I get the mentor experience for example in the first case I know that character 11 is mentored by character 6 how can I get the experience of character six now of course I can take a new tab over here split it to the right go to Fantasy characters filter for ID being equal to six which is the ID of our mentor and get their experience and the experience in this case is 10,000 this is the same example that we saw before but now I would have to write this separate query for each of my rows so here six I've already checked but then I will need to check two and seven and one and this is really not feasible right and the solution of course is to solve it with a subquery so what I'm going to do here is open round brackets and in here I will write the code that I need and here I can simply copy the code that I've written here get experience from the characters where ID equals six now the six part is still hardcoded because in the first row Mentor ID is six to avoid hardcoding this part there are two components to this the first one is noticing that I am referencing the same table fantasy. characters in two different places in my code and this could get buggy and this could get confusing and the solution is to give separate names to these two instances now what are the right names to give so if we look at this outer query right here this is really information about the M te right because we have the Mente ID the ID of their mentor and the Mente experience so I can simply call this Mente table and as you can see I can Alias my table by simply writing it like this or I could also add the as keyword it would work works just the same on the other hand this table will give us the experience of the mentor this is really information about the mentor so we can call this Mentor table now we're not going to get confused anymore because these two instances have different names and now what do we want this ID to be if we're not going to hardcode it we want it to be this value over here we want it to be the mentor ID value from the Mente table we want it to be the M's mentor and to refer to that column I will get the table name dot the column name so this is telling me get the mentor ID value from mentee table and now that I have the subquery which defines a colum with these two brackets I can Alias the result just like I always do and run this and now you will see after making some room here that we have successfully retrieved The Experience value for the mentor now I realize that this is not the simplest process so let us go back to our query over here and make sure that we understand exactly what is happening now first of all we are going to the characters table which contains information about our mentee the person who is being mentored and we label the table so that we remember what it's about we filter it because we're not interested in characters that do not have a mentor and then we're getting a few data right the ID in this case represents the IDE of the mentee and we also have their Mentor ID and we also have the experience which again this is the table about the Mente represents the mentee experience now our goal is to also get the experience of their Mentor our goal is to see that we have a mentor id6 and we want to know that their experience is 10,000 and we do that with a subquery it's a query within a query and in this subquery which is an independent piece of SQL code we are going back to the characters table but this is another instance of the table right that we're looking at so to make sure we remember that we call this Mentor table because it contains information about the mentor and how do we make sure that we get the right value over here that we don't get confused between separate mentors we make sure that for each row the ID of the character in this table is equal to the mentor ID value in the menty table in other words we make sure that we plug in this value over here in this case six into the table to get the right row and then from that row we get the experience value all of these code over here defines a new column which we call Mentor experience and this is basically the same thing that we did manually when we opened a table on the right and queried the table and copy pasted a hardcoded value this is just the way to do it dynamically with a subquery now we are not fully done with the problem right because we wanted to see the difference between the characters experience and their mentors so let's see how to do this and the way to do it is with a column calculation just like the ones we've seen before so given that this column represents the mentor experience I can remove the Alias over here and over here as well and I can subtract the experience from this and a column minus a column gives me another column which I can then Alias as experience difference and if I I run this I will see the value that we originally computed manually which is the difference between the mentor and the Mente experience there's nothing really new about this as long as you realize that this expression over here defines a column and this is the reference to a column and so you can subtract them and then give a name an alias to the result and now we can look at our two examples of nested queries side by side and we can figure out what they have in common and where do they differ so what they have in common is that they're both problem that you cannot resolve with a simple query because you need to use values that you have to compute separately values that you cannot simply refer to by name like we usually do with our columns in this case on the left you need to know what are the minimum and maximum values for experience and in this case on the right you need to know what is the experience of a character's mentor and so we solve that problem by writing a new query a nested query and making sure that SQL solves this query first gets the result and then plugs that result back back into the original query to get the data we need there is however a subtle difference between these two queries that turns out to be pretty important in practice and I can give you a clue to what this difference is by telling you that on the right we have something that's called a correlated subquery and on the left we Define this as uncor related subquery now what does this really mean it means that here on the left our subqueries are Computing the minimum and the maximum experience and these are actually fixed values for all of our characters it doesn't matter which character you're looking at the whole data set has the same values from minimum experience and maximum experience so you could even imagine comp Computing these values first before running your queries for example you could say minimum experience is the minimum and maximum experience is the max and then you could imagine replacing these values over here right this will not actually work because you cannot Define variables like this in in SQL but on a logical level you can imagine doing this right because you only need to compute these two once I will revert this here so we don't get confused on the other hand on the right you will see that the value that is returned by sub by this subquery needs to be computed dynamically for every row this value as you also see in the results is different for every row because every row references a different Mentor ID and so SQL cannot compute this one value here for for all rows at once it has to recompute it for every row and this is why we call it a correlated subquery because it's connected to the value that is in each row and so it must run for each row and an important reason to distinguish between uncorrelated and correlated subqueries is that you can imagine that correlated subqueries are actually slow slower and more expensive to run because you have you're running a SQL query for every row at least At The Logical level so this was our introduction to subqueries they allow you to implement more complex logic and as long as you understand it logically you're off to a great start and then by doing exercises and solving problems you will learn with experience when it's the case to use them in the last lecture we saw that we could use subqueries to retrieve singular values for example what is the minimum value of experience in my data set but we can also use subqueries and Common Table Expressions as well to create new tables all together so here's a motivating example for that so what I'm doing in this query right here is that I am scaling the value of level based on the character's class and you might need this in order to create some balance in your game or for whatever reason now what this does is that if the character is Mage the level gets divided by half or multiplied by 0.5 if the character is Archer or Warrior the level we take the 75% of it and in all other cases the level gains 50% so the details are not very important it's just an example but the point is that we modify the value of level based on the character class and we do this with the case when statement that we saw in a previous lecture and as you can see in the results we get a new value of power level for each character that you can see here but now let's say that I wanted to filter my my characters based on this new column of power level say that I wanted to only keep characters that have a power level of at least 15 how would I do that well we know that the wear filter can be used to filter rows so you might just want to go here and add a wear statement and say where power level is equal or bigger than 15 but this is not going to work right we know this cannot work because we know how the logical order of SQL operations works and so the case when column that we create power level is defined here at the select stage but the wear filter occurs here at the beginning right after we Source our table so due to our rules the wear component cannot know about this power level column that will actually get created later so the query that we just wrote actually violates the logical order of SQL operations and this is why we cannot filter here now there is actually one thing that I could do here to avoid using a subquery and get around this error and that's something would be to avoid using this Alias power level that we assigned here and that the we statement cannot know about and replace it with the whole logic of the case when statement so this is going to look pretty ugly but I'm going to do it and if I run this you will see that we in fact get the result we wanted now in the wear lecture we saw that the wear Clause doesn't just accept simple logical statements you can use all the calculations and all the techniques that are available to you at the select stage and you can also use case when statements and this is why this solution here actually works however this is obviously very ugly and impractical and you should never duplicate code like this so I'm going to remove this wear Clause over here and show you how you can achieve the same result with a subquery so let me first rerun this query over here so that you can see the results and now what I'm going to do I'm going to select this whole logic over here and wrap it in round brackets and then up here I'm going to say select star from and when I run this new query this data that I'm seeing over here should be unchanged so let us run it and you will see that the data has not changed at all but what is actually happening here well it's pretty simple usually we say select star from fantasy characters right and by this we indicate the name of a table that our system can access but now instead of a table name we are showing a subquery and this subquery is a piece of SQL logic that obviously returns a table so SQL will look at this whole code and we'll say say okay there is a outer query which is this one and there is an inner query a nested query which is this one so I will compute this one first and then I will treat this as just another table that I can then select from and now because this is just another table we can actually apply a wear filter on top of it we can say where power level is equal or greater than 15 and you will see that we get the result we wanted just like before but now our code looks actually better and the case when logic is not duplicated if you wanted to visualize this in our schema it would look something like this so the flow of data is the following first we run the inner query that works just like all the other queries we've seen until now it starts with the from component which gets the table from the database and then it goes through the usual pipeline of SQL logic that eventually produces a result which is a table next that table gets piped into the outer query the outer query also starts with the from component but now the from component is not redem directly from the dat database it is reading the result of the inner query and now the outer query goes through the usual pipeline of components and finally it produces a table and that table is our result and this process could have many levels of nesting because the inner query could reference another query which references another query and eventually we would get to the database but it could take many steps to get there and to demonstrate how multiple levels of nesting works I will go back to my query over here and I will go into my inner query which is this one and this is clearly referencing the table in the database but now instead of referencing the table I will reference yet an other subquery which can be something like from fantasy characters where is alive equals true select star so I will now run this and we have added yet another subquery to our code this was actually not necessary at all you could add the wear filter up here but it is just to demonstrate the fact that you can Nest a lot of queries within each other the other reason I wanted to show you this code is that I hope you will recognize that this is also not a great way of writing code it can get quite confusing and it's not something that can be easily read and understood one major issue is that it interrupts the natural flow of reading code because you constantly have to interrupt a query because another nested query is beginning within it so you will read select start from and then here another query starts and this is also querying from another subquery and after reading all of these lines you will find this wear filter that actually refers to the outer query that has started many many lines back and if you find this confusing well I think you're right because it is and the truth is that when you read code on the job or in the wild or when you see solutions that people propose to coding challenges unfortunately this is something that occurs a lot you have subqueries within subqueries within subqueries and very quickly the code becomes impossible to read fortunately there is a better way to handle this and a way that I definitely recommend over this which is to use common table Expressions which we shall see shortly it is however very important that you understand this way of writing subqueries and that you familiarize yourself with it because whether we like it or not a lot of code out there is written like this we've seen that we can use the subquery functionality to define a new table on the Fly just by writing some code a new table that we can then query just like any other SQL table and what this allows us to do is to run jobs that are too complex for a single query and to do that without defining new tables in our database and and storing new tables in our database it is essentially a tool to manage complexity and this is how it works for subqueries so instead of saying from and then the name of a table we open round brackets and then we write a independent SQL query in there and we know that every sqle query returns a table and this is the table that we can then work on what we do here is to select star from this table and then apply a filter on this new column that we created in the subquery power level and now I will show you another way to achieve the same result which is through a functionality called Common Table Expressions to build a Common Table expression I will take the logic of this query right here and I will move it up and next I will give a name to this table I will call it power level table and then all I need to say is with power level table as followed by the logic and now this is just another table that is available in my query and it is defined by the logic of what occurs Within the round brackets and so I can refer to this over here and query it just like I need and when I run this you see that we get the same results as before and this is how a Common Table expression works you start with the keyword with you give an alias to the table that you're going to create you put as open round brackets write an independent query that will of course return a table under this alas over here and then in your code you can query this Alias just like you've done until now for any SQL table and although our data result hasn't changed I would argue that this is a better and more elegant way to achieve the same result because we have separated in the code the logic for the these two different tables instead of putting this logic in between this query and sort of breaking the flow of this table we now have a much cleaner solution where first we Define the virtual table that we will need and by virtual I mean that we treat it like a table but it's not actually saved in our database it's still defined by our code and then below that we have the logic that uses this virtual table we can also have multiple Common Table expressions in our query let me show you what that looks like so in our previous example on subquery we added another part where here instead of querying the fantasy characters table we queried a filter on this characters table and it looked like this we were doing select star where is alive equals true so I'm just reproducing what I did in the previous lecture on subqueries now you will notice that this is really not necessary because all we're doing here is add a wear filter and we could do this in this query directly but please bear with with me because I just want to show you how to handle multiple queries the second thing I want to tell you is although this code actually works and you can verify for yourself I do not recommend doing this meaning mixing Common Table expressions and subqueries it is really not advisable because it adds unnecessary complexity to your code so here we have a common table expression that contains a subquery and I will rather turn this into a situation where we have two common table expressions and no subqueries at all and to do that I will take this logic over here and paste it at the top and I will give this now an alias so I will call it characters alive but you can call it whatever is best for you and then I will do the keyword as add some lines in here to make it more readable and now once we are defining multiple Common Table Expressions we only need to do the with keyword once at the beginning and then we can simply add a comma and please remember this the comma is very important and then we have the Alias of the new table the as keyword and then the logic for that table all that's needed to do now is to fill in this from because we took away the subquery and we need to query the characters alive virtual table here and this is what it looks like and if you run this you will get your result so this is what the syntax looks like when you have multiple Common Table Expressions you start with the keyword with which you're only going to need once and then you give the Alias of your first table as keyword and then the logic between round brackets and then for every extra virtual table that you want to add for every extra Common Table expression you only need to add a comma and then another Alias the ask keyword and then the logic between round brackets and when you are done listing your Common Table Expressions you will omit the comma you will not have a comma here because it will break your code and finally you will run your main query and in each of these queries that you can see here you are totally free to query real tables you know material tables that exist in your database as well as common table Expressions that you have defined in this code and in fact you can see that our second virtual table here is quering the first one however be advised that the order in which you write these Common Table Expressions matters because a Common Table expression can only reference Common Table Expressions that came before it it's not going to be able to see those that came after it so if I say here instead of from fantasy characters I try to query from power level table you will see that I get an error from bigquery because it thinks it doesn't recognize it basically because the code is below so the order in which you write them matters now an important question to ask is when should I use subqueries and when should I use common table expressions and the truth is that they have a basically equivalent functionality what you can do with the subquery you can do with a common table expression my very opinionated advice is that every time you need to define a new table in your code you should use a Common Table expression because they are simpler easier to understand cleaner and they will make your code more professional in fact I can tell you that in the industry it is a best practice to use common table Expressions instead of subqueries and if I were to interview you for a data job I would definitely pay attention to this issue but there is an exception to this and this is the reason why I'm showing you this query which we wrote in a previous lect lecture on subqueries this is a query where you need to get a single specific value right so if you remember we wanted to get characters whose experience is above the minimum experience in the data and also below the maximum experience so characters that are in the middle to do this we need to dynamically find at any point you know when this query is being run what is the minimum experience and the maximum experience and the subquery is actually great for that you will notice here that we don't really need to define a whole new table we just really need to get a specific value and this is where a subquery works well because it implements very simple logic and doesn't actually break the flow of the query but for something more complex like power level table you know this specific query we're using here which takes the name takes the level then applies a case when logic to level to create a new column called power level you could this do this with a subquery but I actually recommend doing it with a common table expression and this is a cool blog post on this topic by the company DBT it talks about common table expressions in SQL why they are so useful for writing complex SQL code and the best best practices for using Common Table expressions and towards the end of the article there's also an interesting comparison between Common Table expressions and subqueries and you can see that of CTE Common Table expressions are more readable whereas subqueries are less readable especially if there there are many nested ones so you know a subquery within a subquery within a subquery quickly becomes unreadable recursiveness is a great advantage of CTE although we won't examine this in detail but basically what this means is that once you define a Common Table expression in your code you can reuse it in any part of your code you can use it in multiple parts right you can use it in other CTE you can use it in your main query and so on on the other hand once you define a subquery you can really only use it in the query in which you defined it you cannot use it in other parts of your code and this is another disadvantage this is a less important factor but when you define a CTE you always need to give it a name whereas subqueries can be anonymous you can see it very well here we of course had to give a name to both of these CTE but the subqueries that we're using here are Anonymous however I don't I wouldn't say that's a huge difference and finally you have that CTE cannot be used in a work Clause whereas subqueries can and this is exactly the example that I've shown you here because this is a simple value that we want to use in our work clause in order to filter our table subqueries are the perfect use case for this whereas CTE are suitable for more complex use cases when you need to Define entire tables in conclusion the article says CTS are essentially temporary views that you can use I've used the term virtual table but temporary view works just as well conveys the same idea they are great to give your SQL more structure and readability and they also allow reusability before we move on to other topics I wanted to show you what an amazing tool to Common Table expressions are to create complex data workflows because Common Table expressions are not just a trick to execute certain SQL queries they're actually a tool that allows us to build data pipelines within our SQL code and that can really give us data superpowers so here I have drawn a typical workflow that you will see in complex SQL queries that make use of Common Table Expressions now what we're looking at here is a single SQL query it's however a complex one because it uses CTE and the query is represented graphically here and in a simple code reference here the blue rectangles represent the Common Table Expressions these virtual tables that you can Define with the CTE syntax whereas the Red Square represents the base query the query at the bottom of your code that ultimately will return the result so a typical flow will look like this you will have a first Common Table expression called T1 that is a query that references a real table a table that actually exists in your data set such as fantasy characters and of course this query will do some work right it can apply filters it can calculate new columns and so on everything that we've seen until now and then the result of this query gets piped in to another Common Table expression this one is T2 that gets the result of whatever happen happened at T1 and then apply some further logic to it apply some more Transformations and then again the result gets piped into another table where more Transformations run and this can happen for any number of steps until you get to the final query and in the base query we finally compute the end result that will then be returned to the user so this is effectively a dat pipeline that gets data from the source and then applies a series of complex Transformations and this is similar to The Logical schema that we've been seeing about SQL right except that this is one level further because in our usual schema the steps are done by Clauses by these components of the SQL queries but here every step is actually a query in itself so of course this is a very powerful feature and this data pipeline applies many queries sequentially until it produces the final result and you can do a lot with this capability and also you should now be able to understand how this is implemented in code so we have our usual CTE syntax with and then the first table we call T1 and then here we have the logic within round brackets for T1 and you can see here that in the from we are referencing a table in the data set and then for every successive Common Table expression we just add a comma a new Alias and the logic comma new Alias and the logic and finally when we're done we write our base query and you can see that the base query is selecting from T3 T3 is selecting from T2 T2 is selecting from T1 and T1 is selecting from the database but you are not limited to this type of workflow here is another maybe slightly more complex workflow that you will also see in the wild and here you can see that at the top we have two common table Expressions that reference the the database so you can see here like like the first one is getting data from table one and then transforming it the second one is getting data from table two and then transforming it and next we have the third CTE that's actually combining data from these two tables over here so we haven't yet seen how to combine data except through the union um I wrote The Joint here which we're going to see shortly but all you need to know is that T3 is combining data from this these two parent tables and then finally the base query is not only using the data from T3 but also going back to T1 and using that data as well and you remember we said that great thing about ctes is that tables are reusable you define them once and then you can use them anywhere well here's an example with T1 because T1 is defined here at the top of the code and then it is referenced by T3 but it is also referenced by the base query so this is another example of a workflow that you could have and really the limit here is your imagination and the complexity of your needs you can have complex workflows such as this one which can Implement very complex data requirements so this is a short overview of the power of CTE and I hope you're excited to learn about them and to use them in your sequel challenges we now move on to joints which are a powerful way to bring many different tables together and combine their information and I'm going to start us off here with a little motivating example now on the left here I see my characters table and by now we're familiar with this table so let's say that I wanted to know for each character how many items they are carrying in their inventory now you will notice that this information is not available in the characters table however this information is available in the inventory table so how exactly does the inventory table works when you are looking at a table for the first time and you want to understand how it works the best question you can ask is the following what does each row represent so what does each row represent in this table well if we look at the columns we can see that for every row of this table we have a specific character id and an item id as well as a quantity and some other information as well such as whether the item is equipped when it was purchased and and so on so looking at this I realized that each row in this table represents a fact the fact that a character has an item right so I know by looking at this table that character id 2 has item 101 and character ID3 has item six and so on so clearly I can use this in order order to answer my question so how many items is Gandalf carrying to find this out I have to look up the ID of Gandalf which as you can see here is six and then I have to go to the inventory table and in the character id column look for the ID of Gandalf right now unfortunately it's not ordered but I can look for myself here and I can see that at least this row is related to Gandalf because he has character id6 and I can see that Gandalf has item id 16 in his inventory and I'm actually seeing another one now which is this one which is 11 and I'm not seeing anyone uh any other item at the moment so for now based on my imperfect uh visual analysis is I can say that Gandalf has two items in his inventory of course our analysis skills are not limited to eyeballing stuff right we have learned that we can search uh a table for the information we need so I could go here and query the inventory table in a new tab right and I could say give me um from the inventory table where character id equals 6 this should give me all the information for Gandalf and I could say give me all the columns and when I run this I should see that indeed we have uh two rows here and we know that Gandalf has items 16 and 11 in his inventory we don't know exactly what these items are but we know that he's carrying two items so that's a good start okay but uh what if I wanted to know which items Frodo is carrying well again I can go to the characters table and uh look up the name Frodo and I find out that Frodo is id4 so going here I can just plug that uh number into my we filter and I will find out that Frodo is carrying a single type of item which has id9 although it's in a quantity of two and of course I could go on and do this for every character but it is quite impractical to change the filter every time and what if I wanted to know how many items each character is carrying or at least which items each character is carrying all at once well this is where joints come into play what I really want to do in this case is to combine these two tables into one and by bringing them together to create a new table which will have all of the information that I need so let's see how to do this now the first question we must answer is what unites these two tables what connects them what can we use in order to combine them and actually we've already seen this in our example um the inventory table has a character id field which is actually referring to the ID of the character in the character's table so we have two columns here the character id column in inventory and the ID column in characters which actually represent the same thing the identifier for a character and this logical connection the fact that these columns repres repr the same thing can be used in order to combine these tables so let me start a fresh query over here and as usual I will start with the from part now where do I want to get my data from I want to get my data from the characters table just as we've been doing until now however the characters table is not not enough for me anymore I need to join this table on the fantasy. inventory table so I want to join these two tables how do I want to join these two tables well we know that the inventory table has a character id column which is the same as the character tables ID column so like we said before these two columns from the different tables they represent the same thing so there's a logical connection between them and we will use it for the join and I want to draw your attention to the notation that we're using here because in this query we have two tables present and so it is not enough to Simply write the name of columns it is also necessary to specify to which table each column belongs and we do it with this notation so the inventory. character uh is saying that the we are talking about the character id colum in the inventory table and the ID column in the characters table so it's important to write columns with this notation in order to avoid ambiguity when you have more than one table in your your query so until now we have used the from uh Clause to specify where do we want to get data from and normally this was simply specifying the name of a table here we are doing something very similar except that we are creating a new table that is obtained by combining two preexisting tables okay so we are not getting our data from the characters table and we are not getting it from the inventory table but we are getting it from a brand new table that we have created by combining these two and this is where our data lives and to complete the query for now we can simply add a select star and you will now see the result of this query so let me actually make some room here and expand these results so I can show you what we got and as you can see here we have a brand new table in our result and you will notice if you check the columns that this table includes all of the columns from the characters table and also all of the columns from the inventory table as as you can see here and they have been combined by our join statement now to get a better sense of what's Happening let us get rid of this star and let us actually select the columns that we're interested in and once again I will write columns with this notation in order to avoid ambiguity and in selecting these columns uh I will remind you that we have all of the columns from the characters table and all of the columns from the inventory table to choose from so what I will do here is that I will take the ID columns from characters and I will take the name column from characters and then I will want to see the ID of the item so I will take the inventory table and the item id column from that table and from the inventory table I will also want to see the quantity of each item and to make our results clearer I will order my results by the characters ID and the item ID and you can see here that we get the result that we needed we have all of our characters here with their IDs and their name and then for each character we can tell which items are in their inventory so you can see here that Aragorn has item id4 in his inventory in quantity of two he also has Item 99 so because of this Aragorn has two rows if we look back at Frodo we see the uh information that we retrieved before and the same for Gandalf who has these two items so we have combined the characters table and the inventory table to get the information that we needed what does each row represent in our result well it's the same as the inventory table each row is a fact which is that a certain character possesses a certain item but unlike the inventory table we now have all the information we want for a character and not just the ID so here we've uh we're showing the name of each character but we could of course select more columns and get more information for each character as needed now a short note on notation when you see SQL code in the wild and u a query is joining on two or more tables people uh you know programmers were usually quite lazy and we don't feel like writing the name of the table all all of the time right like we we're doing in this case with characters so what we usually do is that we add an alias um on the table like this so from fantasy characters call it C we will join on inventory call it I and then basically we use this Alias um everywhere in the query both in the instructions for joining and in the column names and the same with characters so I will substitute everything here and and yes maybe it's a bit less readable but it's faster to write and we programmers are quite lazy so we'll often see this notation and you will often also see that in the code we omit the as keyword which can be let's say implicit in SQL code and so we write it like this from fantasy. character C join uh fantasy. inventory i and then C and I refer to the two tables that we're joining and I can run this and show you that the query works just as well now we've seen why join is useful and how it looks like but now I want you to get a detailed understanding of how exactly the logic of join works and for this I'm going to go back to my spreadsheet and what I have here is my characters table and my inventory table these are just like you've seen them in big query except that I'm only taking um four rows each in order to make it simpler for the example and what you see here is the same query that I've just run on big query this is a t a query that takes the characters table joins it on the inventory table on this particular condition and then picks a few columns from this so let us see how to simulate this query in Google Sheets now the first thing I need to do is to build the table that I will run my query on because as we've said before the from part is now referencing not the characters table not the inventory table but the new table which is built by combining these two and so our first job is to build this new table and the first step to building this new table is to take all of the columns from characters and put them in the new table and then take all of the columns from inventory and then put them in the new table and what we've obtained here is the structure of our new table the structure of our new table is uh simply created by taking all of The Columns of the T table on the left along with all of the columns from the table on the right now I will go through each character in turn and consider the join condition the join condition is that the ID of a character is present in the character id column of inventory so let us look at my first character um we have Aragorn and he has ID one now is this ID present in the character id column yes I see it here in the first row so we have a match given that we have a match I will take all of the data that I have in the characters table for Aragorn and then I will take all of the data in the inventory table for the row that matches and I have built here my first row do I have any other Row in the inventory table that matches yes the second row also has a character id of one so because I have another match I will repeat the operation I will will take all of the data that I have in the left table for Aragorn and I will add all of the data from the right column in the row that matches now there are no more matches for id1 uh in the inventory table so I can proceed and I will proceed with Legolas he has character id of two question is there any row that has the value two in the character id column yes I can see it here so I have another match so just like before I will take the information for Legolas and paste it here and then I will take the matching row which is this one and paste it here we move on to gimly because there's no other matches for Legolas now gimly has ID3 and I can see a match over here so I will take the row for gimly paste it here and then take the matching row character id 3 and paste it here great finally we come to Frodo character id for is there any match for this character I can actually find no match at all so I do nothing this row does not come into the resulting table because there is no match and this completes the job of this part of the query over here building the table that comes from joining these two tables this is my resulting table and now to complete the query I simply have to pick the columns that the query asks for so the First Column is character. ID which is this column over here so I will take it and I will put it in my result the second column I want is character. name which is this column over here the third column is the item id column which is this one right here and finally I have quantity which is this one right here and this is the final result of my query and of course this is just like any other SQL table so I can use all of the other things I've learned to run Logic on this table for example I might only want to keep items that are present in a quantity of two and so to do that I will simply add a wear filter here and I will refer uh the inventory table because that's the parent table of the quantity column so I will say I will say i. quantity um bigger or equal to two and then how my query will work is that first it will build this table like we've seen so it will do this stage first and then it will run the wear filter on this table and it will only keep the rows where quantity is at least two and so as a result we will only get this row over here instead of this result that we see right here H except that um we will of course also have to only keep the columns that are specified in the select statement so we will get ID name um Item ID and quantity so this will be the result of my query after I've added a wear filter so let us actually take this and add it to B query and make sure that it works so so I have to add that after the from part and before the order by part right this is the order and after I run this I will see that indeed I get um Aragorn and Frodo is not exactly the same as in our sheet but that's because our sheet has um less data but uh this is what we want to achieve and now let us go back to our super important diagram of the order of SQL operation and let us ask ourselves where does the join fit in in this schema and as you can see I have placed join at the very beginning of our flow together with the from because the truth is that the joint Clause is not really separate from the from CL Clause they are actually one and the same component in The Logical order of operations so as you remember the first stage specifies where our data lives where we do we want to get our data from and until now we were content to answer this question with a single table name with the address of a single table because all the data we needed was in just one table and now instead of doing this we are taking it a step further we are saying our data lives in a particular combination of two or more tables so let me tell you which tables I want to combine and how I want to combine them and the result of this will be of course yet another table and then this table will be the beginning of my flow and after that I can apply all the other operations that I've come to know uh on my table and it will work just like U all our previous examples the result of a join is of course just another table so when you look at a SQL query and this query includes a join you really have to see it as one and the same with the front part it defines the source of your data by combining tables and everything else that you do will be applied not to a single table not to any of the tables that you're combining everything that you do will be applied to the resultant table that comes from this combination and this is why from and join are really the same component and this is why they are the first step in The Logical order of SQL operations let us now briefly look at multiple joints because sometimes the data that you need is in three tables or four tables and you can actually join as many tables as you want uh or at least as many tables as your system uh allows you to join before it becomes too slow so we have our example here from before we have each character and we have their name and we know which items are in their inventory but we actually don't know what the items are we just know their ID so how can I know uh that if Aragorn has item four what item does Aragorn actually have what is the name of this item now obviously this information is available in the items table that you have here on the right and you can see here that we have a name column and just like before I can actually eyeball it I can look for it myself I know that I'm looking for item id 4 and if I go here and uh I go to four I can see that this item is a healing potion and now let us see how we can add this with the join so now I will go to my query and after joining with characters in inventory I will take that result and simply join it on a third table so I will write join on fantasy. items and I can call this it to use a uh brief form uh because I am lazy as all programmers are and now I need to specify the condition on which to join so the condition is that the item ID column which actually came from the inventory table right that's its parent so I'm going to call it inventory. item um ID except that yeah I'm referring to inventory as a simple I that is the brief form is the same as the items table the ID column in the items table and now that I've added my condition the data that I'm searcing is now a combination of these three tables and in my result I now have access to The Columns of the items table and I can access these columns simply by referring to them so I will say it. name and some other thing it. power and after I run this query I should be able for each item to see the name and the power right so Aragorn has a healing potion with power of 50 Legolas has a Elven bow with power of 85 and so on now you may have noticed something a bit curious and it's that name here is actually written as name1 and can you figure out why this is happening well well it's happening because there's an ambiguity right the characters table has a column called name and the items table also has a column called name and because bigquery is not referring to the columns the way we are doing it right by saying the the parent table and then the name of the column it uh it would find itself in a position of having two identically named columns so the second one uh it tries to distinguish it by adding underscore one and how we can remedy this is by renaming the column to something more meaningful for example we could say call this item name which would be a lot clearer for whoever looks at the result of our query and as you can see now the name makes more sense so you can see that the multiple join is actually nothing new because when we join the first time like we did before we have combined two two tables into a new one and then this new table gets joined to a third table so it's simply repeating the join operation twice it's nothing actually new but let us actually simulate a multiple join in our spreadsheet to make sure that we understand it and that it's nothing new so again I have our tables here but I have added the items table which we will combine and I've written here our query right so take the characters table and join it with inventory uh like we did before and then take the result of that table and join it to items and here we have the condition so the first thing we need to do is to process our first join and this is actually exactly what we've done before so let us do it again first of all the combined table uh characters and inventory its structure is obtained by taking all the columns of characters and then all the columns of inventory and putting them side by side and this is the result table now for the logic of this table I will now do it faster because we've done it before but basically we get the first character id1 it has two matches so I'll actually take this values and put them into two rows and for the inventory part I will simply call copy these two rows to um complete my match then we have Legolas there is one match here so I will take the left side and I will take so I'm looking for id2 so I will take this row over here that's all we have and then we have gimle and he also has one match so I'll will take it here and the resulting column and then finally Frodo has no match so I will not add him to my result this is exactly what we've done before so now that we have this new table we can proceed with our next join which is with items okay so the resulting table will be the result of our first join combined with items and to show you that we've already computed uh this and now it's one table I have added round brackets now the rules for joining are just the same so take all of the columns in the left side table and then take all of the columns in the right side table and now we have the resulting structure of our table and then let us go through every row so let us look at the first row what does the joint condition say Item ID needs to be in the ID table of items so I can see a match here so I will simply take this row on the left side and the matching row on the right side and add it here second row the item ID is four do I have a match yes I can see that I have a match so I will paste the row on the left and the mat matching row on the right third column item id 2 do I have a match no I don't so I don't need to do anything and in the final row item id 101 I don't see a match so I don't have to do anything and so this is my final result in short multiple join works just like a normal join combine the first two tables get the resulting table and then keep doing this until you run out of joins now there's another special case of join uh which is the self join and this is something that people who are getting started with SQL tend to find confusing but I want to show you that there's nothing uh confusing about it because really it's just a regular join that works just like all the other joints that we've seen there's nothing actually special about it so we can see here uh the characters table and you might remember that for each character we are we have a column of Mentor ID now in a lot of cases this column has value null so it means that there's nothing there but in some cases there is a value there and what this means is that this particular character so we are looking at number three uh that is Saruman uh this particular character has a mentor and who is this Mentor uh all we know is that their ID is six and it turns out that the ID in this column is referring to the ID in the characters table so to find out who six is I just have to look who has an ID of six and I can see that it is Gandalf so by eyeballing it I know that San has a mentor and that Mentor is Gandalf and then elron also has the same Mentor which is Gand so I can solve this by eyeballing the table but how can I get a table that shows for each character who has a mentor who their Mentor is it turns out that I have to take the character's table and join it on the characters table on itself so let's see how that works in practice so let me start a new query here on the right and so my goal here is to list every character in the table and then to also show their Mentor if they have one so I will of course have to get the characters table for this and the first time I take this table it is simply to list all of the characters right so to remind myself of that I can give it a label which is chars now as you know each character has a mentor ID value and but to find out who like what is the name of this Mentor I actually need to look it up in the characters table so to do this I will join on another instance of the characters table right this is another let's say copy of the same data but now I'm going to use it for a different purpose I will not use it to list my characters I will use it to get the name of the mentor so I will call this mentors to reflect this use now what is The Logical connection between these two copies of the characters table each character in my list of characters has a mentor ID field and I want to match this on the the ID field of my mentor table so this is The Logical connection that I'm looking for and I can now add a select star to quickly complete my query and see the results over here so the resulting table has all of The Columns of the left table and all of The Columns of the right table which means that the columns of the characters table will be repeated uh twice in the result as you can see here but on the left I simply have my list of characters okay so the first one is Saruman and then on the right I have the data about their Mentor so Saran has a mentor ID of six and then here starts the data about the mentor he has ID of six and his name is Gandalf so you can see here that our self jooin has worked as intended but this is actually a bit messy uh we don't need uh all of these columns so let us now select Only The Columns that we need so from my list of characters I want the name and then from the corresponding Mentor I also want the name and I will label these columns so that they make sense to whoever is looking at my data so I will call this character character name and I will call this Mentor name and when I run this query you can see that quite simply we get what we wanted we have the list of all our characters at least the ones who have a mentor and for each character we can see the name of their Mentor so a self join works just like any other join and the key to avoiding confusion is to realize that you are joining on two different copies of the same data okay you're not actually joining on the same exact table so one copy of fantasy characters we call characters and we use for a purpose and then a second copy we call mentors and we use for another purpose and when you realize this you see that you are simply joining two tables uh and all the rules that you've learned about normal joints apply it just so happens that in this case the two tables are identical because you're getting the data from the same source and to drive the point home let us quickly simulate this in our trusty spreadsheet and so as you can see here uh I have the query that I've run in B query and we're now going to simulate it so the important thing to see here is that that we're not actually joining one table to itself although that's what it looks like we're actually joining two tables which just happen to look the same okay and so one is called chars and one is called mentors based on the label that we've given them but then once we join them the rules are just the same as we've seen until now so to create the structure of the resulting table take all the columns from the left left and then take all the columns from the right and then go row by row and look for matches based on on the condition now the condition is that Mentor ID in chars needs to be in the ID column of mentors so first row Aragorn has Mentor 2 is this in the ID column yes I can see a match here so let me take all the values from here and all the values from the matching rows paste them together are there any other matches no second row we're looking for Mentor ID 4 do we have a match yes I can see it here so let me take all of the values from the left and all of the values from the matching row on the right now we have two more rows but but as you can see in both cases Mentor ID is null which means that they have no mentor and basically for the purposes of the join we can ignore these rows we are not going to find a match in these rows in fact as an aside even if there was a character whose ID was null uh we wouldn't match with Mentor ID null on a character whose ID was null because in squl in a sense null does not equal null because null is not a specific value but it represents the absence of data so in short when Mentor ID is null we can be sure that in this case uh there will be no match and the row will not appear in the join now that we have our result we simply need to select the columns that we want and so the first one is name which comes from the charge table which is this one over here and the second one is name that comes from the mentor table which is this one over here and here is our result so that's how a self join works so until now we have seen uh joint conditions which are pretty strict and and straightforward right so there's a column in the left table and there's a column in the right table and they represent the same thing and then you look for an exact match between those two columns and typically they're an ID number right so one table has the item id the other table also has the item ID and then you look for an exact match and if there's an exact match you include the row in the join otherwise not that's pretty straightforward but what I want to show you here is that the join is actually much more flexible and and powerful than that and you don't always need you know two columns that represent the exact same thing or an exact match in order to write a joining condition in fact you can create your own you know complex conditions and combinations that decide how to join two tables and for this you can simply use the Boolean algebra magic that we've learned about in this course and that we've been using for example when working on the wear filter so so let us see how this works in practice now I've tried to come up with an example that will illustrate this so let's say that we have a game you know board game or video game or whatever and we have our characters and we have our items okay and in our game um a character cannot simply use all of the items in the world okay there is a limit to which items a character can use and a limit is based on the following rule um let me write it here as a comment and then we will uh use it in our logic so a character can use any item for which the power level is equal or greater than the characters experience divided by 100 okay so this is just a rule uh that exists in our game and now let us say that we wanted to get a list of all characters and the items that they can use okay and this is clearly uh a case where we would need a join so let us actually write this query I will start by getting my data from fantasy. characters and I will call this c as a shorthand and I will need to join on the items table right and what is the condition of the join the condition of the join is that the character's experience divided by 100 is greater or equal than the items power level and I forgot here to add a short hand I for the items table so this is the condition that refects our Rule and out of this table that I've created I would like to see the characters name and the characters experience divided by 100 and then I would like to see the items name and the items power to make sure that my um join is working as intended so let us run this and look at the result so this looks a bit weird because we haven't given a label to this column but basically I can see um that I have Gandalf and his experience divided by 100 is 100 and he can wear the item Excalibur that has a power of 100 which satisfies our condition let me actually order by character name so that I can see in one place all of the items that a character can wear so we can see that Aragorn is first and his experience divided by 100 is 90 and then uh this is the same in all all of these rows that we see right now but then we see all of the items that Aragorn is allowed to use and we see their power and in each case you will see that their power does not exceed this value on the left so the condition uh that we wrote works as intended so as you can see what we have here is a Boolean expression just like the ones we've seen before which is a logical statement that eventually if you run it it evaluates to either true or false and all of the rules that we've seen for Boolean Expressions apply here as well for example I can decide that this rule over here does not apply to Mages because Mages are special and then I can say that if a character is Mage then I want them to be able to use all of the items well how can I do this in this query can you pause the video and figure it out so what I can do is to Simply expand my Boolean expression by adding an or right and what I want to test for is that character class equals Mage so let me check for a second that I have class and I have Mage so this should work and if I run this going through the result I will not do it but you can uh do it yourself and and verify for yourself that if a character is a Mage you will find out that they can use all of the items and this of course is just a Boolean expression um in which you have two statements connected by an or so if any of this is true if at least one of these two is true then the whole statement will evaluate to true and so the row will match if you have trouble seeing this then go back to the video on the Boolean algebra and uh everything is explained in there so this is just what we did before when we simulated The Joint in the spreadsheet you can imagine taking the left side table which is uh characters and then going row by row and then for the first row you check all of the rows in the right side table which is items all of the rows that have a match but this time you won't check if the ID corresponds you will actually run this expression to see whether there is a match and when this expression evaluates as true you consider that to be a match and you include the row in the join however if this condition does not evaluate to true it's not a match and so the row is not included in the join so this is simply a generalization from the exact match which shows you that you can use any conditions in order to join uh two tables now I've been pretending that there is only one type of join in SQL but that is actually not true there are a few different types of join that we need to know so let us see uh what they are and how they work now this is the query that we wrote before and this is exactly how we've written it before and as you can see we've simply specified join but uh it turns out that what we were doing all the time was something called inner join okay and now that I've written it explicitly you can see that if I rerun the query I will get exactly the same results and this is because the inner join is by far the most common type of join that you find in SQL and so in many uh styles of SQL such as the one used by bigquery they allow you to skip this specification and they allow you to Simply write join and then it is considered as an inner join so when you want to do an inner join you have the choice whether to specify it explicitly or to Simply write join but what I want to show you you now is another type of join called Left join okay and to see how that works I want to show you um how we can simulate this query in the spreadsheet so as you can see this is very similar to what we've done before I have the query uh that I want to simulate and notice the left join and then I have my two tables now what is the purpose of the left join in the previous examples which were featuring the inner join we've seen that when we combine two tables with an inner join the resulting table will only have rows that have a match in both tables okay so what we did is that we went through every Row in the characters table and if it had a match in the inventory table we kept that row but if there was no match we completely discarded that row but what if we wanted in our resulting table to see all of the characters to make sure that our list of characters was complete regardless of whether they had a match in the inventory table this is what left join is for left join exists so that we can keep all of the rows in the left table whether they have a match or not so let us see that in practice okay so when we need to do a left join between characters and inventory so first of all I need to determine the structure of the resulting table and to do this I will take all of the columns from the left table and all of the columns from the right table nothing new there next step let us go row by Row in the left table and look for matches so we have Aragorn and he actually has two matches uh by now we've uh remembered this so these two rows have a match in character id with the ID of characters so I will take these two rows and add them to my resulting table next is Legolas and I see a match here so I will take the rows where Legolas matches and put it here it's only one row actually gimly has also a single match so I will create the row over here um and so this is the match for gimly and of course I can ensure that I'm doing things correctly by looking at this ID column and uh this character id column over here and they have to be identical right if they're not then I've made a mistake and finally we come to Frodo now Frodo you will see does not have a match in this table so before we basically discarded this row because it had no match right now though we are dealing with the left join that means that all of the rows in the characters table need to be included so I don't have a choice I need to take this row and include it and add it here and now the question is what values will I put in here well I cannot put any value from the inventory table because I don't have a match so the only thing that I can do is to put NS in here NS of course represent the absence of data so they're perfect for this use case and that basically completes uh the sourcing part of our left join now you may have noticed that there is an extra row here in inventory which does not have a match right it is referred into character id 10 but there is no character id 10 so here the frao row also did not have a match but we included it so should we include this row as well the answer is no why not because this is a left joint okay so left joint means that we include all of the rows in the left table even if they don't have a match but we do not include rows in the right table when they do not have a match okay this this is why it's a left join so but if you're still confused about this don't worry because it will become clearer once we see the other types of join and of course for the sake of completeness I can actually finish the query by selecting my columns which would be the uh character id and the character name and the item ID and the item quantity and this is my final result and in the case of Frodo we have null values which tells us that this row found no match in the right table which in this case means that Frodo does not have any items now that you understand the left join you can also easily understand the right joint it is simply the symmetrical operation to the left joint right right so whether you do characters left joint inventory or you do inventory right join characters the result will be identical it's just the symmetrical operation right this is why I wrote here that table a left joint b equals table B right joint a so hopefully that's pretty intuitive but of course if I I did characters right join inventory then the results would be reversed because I would have to keep all of the rows of inventory regardless of whether they have a match or not and only keep rows in characters which have a match so if you experiment for yourself on the data you will easily convince yourself of this result let us now see the left joint in practice so remember the query from before um where we take each character and then we see their Mentor this is the code exactly as we've written it before and so now you know that this is an inner join because when you don't specify what type of join you want SQL assumes it's an inner join at least that's what the SQL in bigquery does and you can see that if I write inner join um I think I have a typo there uh the result is absolutely identical and in this case we're only including characters who have a mentor right we are missing out on characters who don't have a mentor meaning that Mentor ID is null because in the inner join there is no match and so they are discarded but what would happen if I went here and instead turn this into a left join what I expect to happen is that I will keep all of my characters so all of the rows from the left side table regardless of whether they have a match or not regardless of whether they have a mentor or not and so let us run this and let us see that this is in fact the case I now have a row for each of my characters and I have a row for Gandalf even though Gandalf does not have mentor and so I have a null value in here so the left join allows me to keep all of the rows of the left table now we've seen the inner join the left join and the right join which are really the same thing just symmetrical to each other and finally I want to show you the full outer join this is the last type of join that I want to that I want to show you now you will see that a full outer joint is like a combination of all of the joints that we've seen until now so a full outer join gives us all of the rows uh that have a match in the two tables plus all of the rows in the left table that don't have a match with the right table plus all of the rows in the right table that don't have a match in the left table so let us see how that works in practice what I have here is our usual query but now as you can see I have specified a full outer join so let us now simulate this join between the two tables now the first step as usual is to take all of the columns from the left table and all of the columns from the right table to get the structure of the resulting table and now I will go row by Row in the left table so as usual we have Aragorn and you know what I'm already going to copy it here because even if there's not a match I still have to keep this row uh because this is a full outer joint and I'm basically not discarding any row now that I've copied it is there a match well I already know from the previous examples that there are two rows uh in the inventory table that match because they have character id one so I'm just going to take them and copy them over here and in the second row I will need to replicate these values perfect let me move on to Legolas and again I can already paste it because there's no way that I'm going to discard this row but of course we know that Legolas has a m match and moving quickly cuz we've already seen this gimly has a match as well and now we come to Frodo now Frodo again I can already copy it because I'm keeping all the rows but Frodo does not have a match so just like before with the left join I'm going to keep this row but I'm going to add null values in the columns that come from the invent table so now I've been through all of the rows in the left table but I'm not done yet with my join because in a full outer join I have to also include all of the rows from the right table so now the question is are there any rows in the inventory table that I have not considered yet and for this I can check the inventory ID from my result 1 2 3 4 and compare it with the ID from my table 1 2 3 4 5 and then I realize that I have not included row number five because it was not selected by any match but since this is a full outer join I will add this row over here I will copy it and of course it has no correspondent uh in the left table so what do I do once again I will insert null values and that completes the first phase of my full outer join the last phase is always the same right pick the columns that are listed in the select so you have the ID the name Item ID and quantity and this completes my full outer join so remember how I said that a full outer join is like an inner join plus a left join plus a right join here is a visualization that demonstrates now in the result the green rows are the rows in which you have a match on the left table and the right table right and these rows correspond to the inner join and if you run an inner join this this will be the only rows that are returned right now the purple row is including a row that is present in the left table but does not have any match in the right table so if you were to run a left join what would the result be a left joint would include all of the green rows because they have a match and and additionally they would also include the purple row because in the left joint you keep all of the rows from the left if on the other hand you were to run a right join and you wouldn't like swap the names of the tables or anything right you would do characters right join inventory you would get of course all of the green rows because they are a match Additionally you would get the blue row at the end because this row is present in the right table even though there's no match and in the right join we want to keep all the rows that are in the right table and finally in a full outer join you will include all of these rows right so first of all all of the rows that have a match and then all of the rows in the left table even though they don't have a match and finally all of the rows in the right table even though they don't have a match and these are the three or four types of joint that you need to know and that you will find useful in solving your problems now here's yet another way to think about joints in SQL and to visualize joints which you might find helpful so one way to think about SQL tables is that a table is a set of rows and that joints correspond to different ways of uh combining sets and you might remember this from school this is a v diagram it represents the relation uh between uh two sets and the elements that are inside these two sets so you can take set a to be our left table uh containing all of the rows from um the left table and set B to be our right table with all of the rows from the right table and in the middle here you can see that there is an intersection between the sets this intersection represents the rows that have a match uh so this would be the rows that I have colored green in our example over here so what will happen if I select if I want to see only the rows that are a match only the rows that belong in both tables let me select this now and you can see that this corresponds to an inner joint because I only want to get the rows that have a match then what would happen if I wanted to include all of the rows in the left table regardless of whether they have a match or not to what type of join does that correspond I will select it here and you can see that that corresponds to a left join the left join produces a complete set of records from table a with the matching records in table B if there is no match the right side will contain null likewise if I wanted to keep all of the rows in uh table B including the ones that match with a I would of course get a right join which is just symmetrical to a left join finally what would I have to do to include all of the rows from both tables regardless of whether they have a match or not if I do this then I will get a full outer join so this is just one way to visualize what we've already seen there is one more thing you can actually realize from this uh tool which is in some cases you might want to get all of the records that are in a except those that match in B so all of the record that records that a does not have in common with b and you can see how you can actually do this this is actually a left join with an added filter where the b key is null so what does that mean the meaning will be clear if I go back to our example for the left join you can see that this is our result for the left join and because Frodo had no match in the right table the ID column over here is null so if I take this table and I apply a filter where ID where inventory ID is null I will only get this result over here and this is exactly the one row in the left table that does not have a match in the right table so this is more of a special case you don't actually see this a lot in practice but I wanted it wanted to show it briefly to you in case you try it and get curious about it likewise the last thing that you can do you could get all of the rows from A and B that do not have a match so the set of Records unique to table a and table B and this is actually very similar you do a full outer join and you check that either key is null so either inventory ID is null or character id is null and if you apply that filter you will get these two rows which is the set of rows that are in a and only in a plus the rows that are in B and only in B once again I've honestly never used this in practice I'm just telling you for the sake of completeness in case you get curious about it now a brief but very important note on how SQL organizes data so you might remember from the start of the course that I've told you that in a way SQL tables are quite similar to spreadsheet tables but there are two fundamental difference one difference is that each SQL table has a fixed schema meaning we always know what the columns are and what type of data they contain and we've seen how this works extensively the second thing was that SQL tables are actually connected with each other which makes SQL very powerful and now we are finally in a position to understand just exactly how SQL tables can be connected with each other and this will allow you to understand how SQL represents data so I came here to DB diagram. which is a very uh nice website for building representations of SQL data and this is uh this type of um of chart of representation that we see here is also known as ER as you can see me writing here which is stands for entity relationship diagram and it's basically a diagram that shows you how your data is organized in your SQL system and so you can see a representation of each table uh this is the example that's shown on the web website and so you have three tables here users follows and posts and then for each table you can see the schema right you can see that the users table has four columns one is the user ID which is an integer the other is the username which is varar this is another way of saying string so this is a piece of text rooll is also a piece of text and then you have a Tim stamp that shows when the user was created and the important thing to notice here is that these tables are actually they're not they don't exist in isolation but they are connected with each other they are connected through these arrows that you see here and what do these arrows represent well let's look at the follows table okay so each row of this table is a fact shows that one user follows another and so in each row you see the ID of the user who follows and the ID of the user who is followed as well as the time when this event happened and what are these uh arrows telling us they're telling us that the IDS in this table are the same thing as the user ID column in this table which means that you can join the follows table with the users table to get the information about the two users that are here the user who is following and the user who is followed so like we've seen before a table has a column which is the same thing as another tables column which means that you can join them to combine their data and this is how in SQL several tables are connected with each other they are connected by logical correspondences that allow you to join those tables and combine their data likewise you have the post table and each row represent a post and each post post has a user ID and what this arrow is telling you is that uh you can join on the user table using this ID to get all the information you need about the user who has created this post now of course as we have seen you are not limited to joining the tables along these lines you can actually join these tables on whatever condition you can think of but this is a guarantee of consistency between these tables that comes from how the data was distributed and it's a guarantee it's a promise that you can get the data you need by joining on these specific columns and that is really all you need to know in order to get started with joints and use them to explore your data and solve SQL problems to conclude this section I want to go back to our diagram and to remind you that from and join are really one and the same they are the way for you to get the data that you need in order to answer your question and so when the data is in one table alone you can get away with just um using the from and then specifying the name of the table but often your data will be distributed in many different tables so you can look at the ER diagram such as this one if you have it to figure out how your uh data works and then once you decided which tables you want to combine you can write a from which combines with a join and so create a new table uh which is a combination of two or more tables and then all of the other operations that you've learned will run on top of that table we are finally ready for a indepth discussion of grouping and aggregations in SQL and why is this important well as you can see I have asked Chad GPT to show me some typical business questions that can be answered by data aggregation so let's see what we have here What's the total revenue by quarter how many units did did each product sell last month what is the average customer spent per transaction which region has the highest number of sales now as you can see these are some of the most common and fundamental business questions um that you would be asking when you do analytics and this is why grouping and aggregation are so important when we talk about SQL now let's open our date once again in the spreadsheet and see what we might achieve through aggregation so I have copied here four columns from my characters table Guild class level and experience and I'm going to be asking a few questions the first question which you can see here is what are the level measures by class so what does this mean well earlier in the course we looked at aggregations and we call them simple aggregations because we were running them over the whole table so you might remember that if I select the values for level here I will get a few different aggregations in the lower right of my screen so what you can see here is that I have a count of of 15 which means that there are 15 rows for level and that the maximum level is 40 the minimum is 11 and then I have an average level of 21.3 more or less and if you sum all the levels you get 319 so this is already some useful information but now I would like to take it a step further and I would like to know this aggregate value within each class so for example what is the maximum level for warriors and what is the maximum level for Hobbits are they different how do they compare this is where aggregation comes into play so let us do just that now let us find the maximum level Within each class and let us see how we might achieve this now to make things quicker I'm going to sort the data to fit my purpose so I will select the range over here and then go to data sort range and then in the in the advanced options I will say that I want to sort by column B because that's my class and now as you can see the data is ordered by class and I can see the different values for each class next I will take all the different values for class and separate them just like this so first I have Archer then I have hobbit then I have Mage and finally I have Warrior so here they are they're all have their own sp right now finally I just need to take to compress each of these ranges so that each of them covers only one row so for Archer I will take the value of the class Archer and then I will have to compress these numbers to a single number and to do that I will use the max function this is the aggregation function that we are using and quite intuitively this function will look at the list of values we'll pick the biggest one and it will reduce everything to the biggest value and you can also see it here in this tool tip over here doing the same for Hobbit compress all of the values to a single value and then compress all of the numbers to a single number by applying a an aggregation function so I've gone ahead and done the same for mage and Warrior and all that's left to do is to take this and bring all these rows together and this is my result this is doing what I have asked for I was looking to find the maximum level Within each class so I have taken all the unique values of class and then all the values of level within each class I have compressed them to a single number by taking the maximum and so here I have a nice summary which shows me what the maximum level is for each class and I can see that mes are much more powerful than everyone and that Hobbits are much more weaker according to this measure I've learned something new about my data now crucially and this is very important in my results I have class which is a grouping field and then level which is an aggregate field okay so what exactly do I mean by this now class is a grouping field because it divides my data in several groups So based on the value of class I have divided my data as you see here so Archer has three values Hobbit has four values and so on level is an aggregate field because it was obtained by taking a list of several values so here we have three here we have four and in the wild we could have a thousand or 100 thousand or Millions it doesn't matter it's a list of multiple values and then I've taken these values and compressed them down to one value I have aggregated them down to one value and this is why level is an aggregate field and whenever you work with groups and aggregations you always have this division okay you are have some fields that you use for grouping you know for subdividing your data and then you have some fields on which you run aggregations and aggregations such as for example looking at a list of value and taking the maximum value or the average or the minimum and so on aggregations are what allow you to understand the differences between groups so after aggregating you can say oh well the the Mages are certainly much more powerful than the hobbits and so on and if you look work with the dashboards like Tableau or other analytical tools you will see that another way to refer to these terms is by calling the grouping Fields dimensions and the aggregate Fields measures okay so I'm just leaving it here you can say grouping field and aggregate field or you can talk about dimensions and measures and they typically refer to the same type of idea now let's see how I can achieve the same result in SQL so I will start a new query here and I want to get data from fantasy. characters and after I've sourced this table I want to Define my groups okay so I will use Group by which is my new clause and then here I will have to specify the grouping field I will have to specify the group that I want to use in order to subdivide the data and that group is class in this case after that I will want to define the columns that I want to see in my result so I will say select and first of all I want to see the class and then I want to see the maximum level within each class so if I run this you will see that I get exactly the same result that I have in Google Sheets so we have seen this before Max is an aggregation function it takes a list of Val vales and then compresses them down to a single value right except that before we were running it on at the level of the whole table right so if I select this query alone and run it what do you expect to see I expect to see a single value because it has looked at all the levels in the table and it has simply selected the biggest one it has reduced all of them to a single value however if I run it after defining a group buy then this will run not on the whole table at once it will run within each group identified by my grouping field and we'll compute the maximum within that group and so the result of this will be that I can see the maximum level for each group now I'm going to delete this and I don't need to limit myself to a single aggregation I can write as many aggregations as I wish so I will put this down here and I'll actually give it a label so that it makes sense and then I will write a bunch of other aggregations such as count star which basically is the number of values within that class um I can also look at the minimum level I can also look at the average level so let's run this and make sure that it works so as you can see we have our unique values for class as usual and then and for each class we can compute as many aggregated values as we want so we have the maximum level the minimum level and we didn't give a label to this so we can call it average level and then number of values n values is not referring to level in itself it's a more General aggregation which is simply counting how many examples I have of each class right so I know I have four Mages three archers four Hobbits and four Warriors by looking at this value over here and here's another thing I am absolutely not limited to the level column as you can see I also have the experience column which is also an integer and the health column which is a floating Point number so I can get the maximum health and I can get the minimum experience and it all works all the same all the aggregations are computed within each class but one thing I need to be really careful of is the match between the type of aggregation that I want to run and the data type of the field on which I plan to run it so all of these that we show here they're number columns right either integers or floats what would happen if I ran the average aggregation on the name column which is a string what do you expect to happen you can already see that this is an error why no matching signature for aggregate function average for a type string so it's saying this function does not accept the type string it accepts integer float and all types of number columns but if you ask me to find the average between a bunch of strings I have no idea how to do that so I can add as many aggregations as I want within my grouping but the aggregations need to make sense but these Expressions can be as complex as I want them to be so instead of taking the average of the name which is a string it doesn't make sense I could actually run another function instead of this inside of this which is length and what I expect this to do is that for each name it will count how long that name is and then after I've done all these counts I can aggregate them uh I could take the average for them and what I get back is the average name length within each class doesn't sound really helpful as a thing to calculate but this is just to show you that these Expressions can get quite complex now whatever system you're working with it will have a documentation in some place which lists all the aggregate functions that you have at your disposal so here is that page for big query and as you can see here we have our aggregate functions and if you go through the list you will see some of the ones that I've shown you such as count Max mean and some others that uh I haven't shown you in this example such as sum so summing up all the values um any value which simply picks uh one value I think it it happens at random and U array a which actually built a list out of those values and so on so when you need to do an analysis you can start by asking yourself how do I want to subdivide the data what are the different groups that I want to find in the data and then after that you can ask yourself what type of aggregations do I need within each group what do I want to know um about each group and then you can go here and try to find the aggregate function that works best and once you think you found it you can go to the documentation for that function and you can read the description so Returns the average of nonnull values in an aggregated group and then you can see what type of argument is supported for example average supports any numeric input type right so any data type that represents a number as well as interval which represents a space of time now in the previous example we have used a single grouping field right so if we go back here we have our grouping field which is class and we only use this one field to subdivide the data but you can actually use multiple grouping Fields so let's see how that works what I have here is my items table and for each item we have an item type and a rarity type uh and then for each item we know the power so what would happen if we wanted to say to see the average Power by item type and Rarity combination one reason we might want to see this is that we might ask ourselves is within every item type is it always true that if you go from common to rare to Legendary the power increases is this true for all item types or only for certain item types let us go and find out so what what I'm going to do now is that I'm going to use two fields to subdivide my data I'm going to use item type and Rarity and to do this as a first step I will sort the data so that it makes it convenient for me so I will go here and I will say sort range Advanced ranged sorting option and first of all I want to sort by column A which is item type and I want to add another sort column which will be column B and you can see that my data has been sorted next I'm going to take each unique combination of the values of my two grouping Fields okay so the first combination is armor common so I'm going to take this here and then I'm going to to write down all the values that come within this combination so in this case we only have one value which is 40 next I have armor legendary and within this combination I only have one value which is 90 next I have armor rare So for armor rare I actually have two values so I'm going to write them here next we have potion and common for this we actually have three values so I'm going to write them here so I've gone ahead and I've done it for each combination and you can see that each unique combination of item type and Rarity I've now copied the re relevant values and now I need to get the average power with in these combinations so I will take the first one put it over here and then I will take the average of the values this is quite easy because there's a single value so I'll simply write 40 next I will take the armor legendary combination and once again I have a single value for armor rare I have two values so I will actually press equal and write average to call the the spreadsheet function and then select the two values in here to compute the average and here we have it and I can go on like this potion common get the average Within These values potion legendary is a single value so I've gone ahead and completed this and this gives me the result of my query here I have all the different combinations for the values of uh what were they item type and Rarity and within each combination the average power so to answer my question is it that within each item type the power grows with the level of Rarity where for armor it goes from 40 to 74 to 90 so yes for potion we don't have um a rare potion but basically it also grows from common to Legendary and in weapon we have uh 74 87 and 98 so I would say yes within each item type power grows with the level of Rarity so what are these three fields in the context of my grouping well item type is grouping field and Rarity is also a grouping field and the average power within each group is a aggregate field right so I am now using two grouping fields to subdivide my data and then I'm Computing this aggregation within those groups so let us now figure figure out how to write this in SQL it's actually quite similar to what we've seen before we have to take our data from the items table and then we want to group by and here I have to list my grouping Fields okay so as I've said I have two grouping Fields they are item type and and Rarity so this defines my groups and then in the select part I will want to see my grouping fields and then within each group I will want to see the average of power I believe we used yes so I will get the average of power and here are our results just like in the sheets now as a tiny detail you may notice that power here is colored in blue and the reason for this is that power is actually a big query function so if you do power of two three you should get uh eight because it calculates the two to to to the power of three so it can be confusing when power is the name of a column because B query might think it's a function but there's an easy way to remedy this you can just use back ticks and that's your way of telling big query hey don't get confused this is not the name of a function this is actually the name of a column and as you can see it also works and it doesn't create issues and just like before we could add as many aggregations as we wanted and for example we could take the sum of power also on other fields not just on Power and everything would be computed within the groups defined by the two grouping fields that I have chosen as expected now now let us see where Group by fits in The Logical order of SQL operations so as you know a SQL query starts with from and join this is where we Source the data this is where we take the data that we need and as we learned in the join section we could either just specify a single table in the from clause or we could specify a join of two or more tables either way the result is the same we have assembled the table where our data leaves and we're going to run our Pipeline on that data we're going to run all the next operations on that data next the work Clause comes into play which we can use in order to filter out rows that we don't need and then finally our group group Pi executes so the group Pi is going to work on the data that we have sourced minus the rows that we have excluded and then the group Pi is going to fundamentally alter the structure of our table because as you have seen in our examples the group I basically compresses down our values or squishes them as I wrote here because in the grouping field you will get a single Row for each distinct value and then in the aggregate field you will get an aggregate value within each class okay so if I use a group bu it's going to alter the structure of my table after doing the group bu I can compute my aggregations like you've seen in our examples so I can compute uh minimum maximum average sum count and and all of that and of course I need to do this after I have applied my grouping and after that after I I've computed my aggregations I can select them right so I can choose which columns to see um and this will include the grouping fields and the aggregated fields we shall see this more in detail in a second and then finally there's all the other oper ations that we have seen in this course and this is where Group by and aggregations fit in our order of SQL operations now I want to show you an error that's extremely common when starting to work with group pi and if you understand this error I promise you you will avoid a lot of headaches when solving SQL problems so I have my IDE items table here again and you can see the preview on the right and I have a simple SQL query okay so take the items table Group by item type and then show me the item type and the average level of power within that item type so so far so good but what if I wanted to see what I'm showing you here in the comments what if I wanted to see each specific item the name of that item the type of that item and then the average Power by the type of that item right so let's look at the first item chain mail armor this is a armor type of item and we know that the average power for armors is 69.5 so I would like to see this row and then let's take Elven bow now Elven baow is a weapon as you can see here the average powerful weapons is 85. 58 and so I would like to see that now stop for a second and think how might I achieve this how might I modify my SQL query to achieve this oh and there is a error in the column name over here because I actually wanted to say name but let's see how to do it in the SQL query so you might be tempted to Simply go to your query and add the name field in order to reproduce What you see here and if I do this and I run it you will see that I get an error select expression references column name which is neither grouped nor aggregated understanding this error is what I want to achieve now because it's very important so can you try to figure out on your own why this query is failing and what exactly this error message means so I'm going to go back to my spreadsheet and get a copy of my items table and as you can see I have copied the query that doesn't work over here so let us now uh go ahead and reproduce this query so I have to take the items table here it is and then I have to group by item type and as you can see I've already sorted by item type to facilitate our work and then for each item we want to select the item type so that would be armor and we want to select the average power so to find that I can run a spreadsheet function like this it's called average and get the power over here and then I am asked to get the name so if I take the name for armor and put it here this is what I have to add and here you can already see the problem that we are facing for this particular class armor there is a mismatch in the number of rows that each column is providing because as an effect of group by item type now there is only one row in which item type is armor and as an effect of applying average to power within the armor group now there is only one row of power corresponding to the armor group but then when it comes to name it's neither present in a group Pi nor is it present in an aggregate function and that means that in the case of name we still have four values four values instead of one and this mismatch is an issue SQL cannot accept it because SQL doesn't know how to combine columns which have different numbers of rows in a way it's like SQL is telling us look you've told me to group the data by item type and I did so I found all the rows that correspond to armor and then you told me to take the average of the power level for those rows and I did but then you asked me for name now the item type armor has four names in it what am I supposed to do with them how am I supposed to combine them how am I supposed to squish them into a single value you haven't explained how to do that so I cannot do it and this takes us to a fundamental rule of SQL something I like to call the law of grouping and the law of grouping is actually quite simple but essential it tells you what type of columns you can select after you've run a group pi and there are basically two types of columns that you can select after running a group bu one is grouping Fields so those those are the columns that appear after the group by Clause those are the columns you are using to group the data and two aggregations of other fields okay so those are fields that go inside a Max function a mean function a sum function a count function and so on now those are the only two types of columns that you can select if you try to select any other column you will get an error and the reason you will get an error is Illustrated here after a group Pi each value in the grouping Fields is repeated exactly once and then for that value the aggregation makes sure that there's only one corresponding value in the aggregated field in this case there's only one average power number within each item type however any other field if it's not a grouping field and you haven't run an aggregation on it you're going to get all of its values and then there's going to be a mismatch so the law of grouping is made to prevent this issue now if we go back to our SQL hopefully you understand now better why this error Isen happening and in fact this error message makes a lot more sense after you've heard about the law of grouping you are referencing a column name which is neither grouped nor aggregated so how could we change this code so that we can include the column name without triggering an error well we have two options either we turn it into a grouping field or we turn it into an aggregation so let's try turning it into an aggregation let's say for example that I said mean of name what do you expect would happen in that case so if I run this you will see that I have my grouping by item type I have the average power within each item type and then I have one name and so when you run mean on a sequence of uh text values what it does is that it gives you the first value in alphabetical order so we are in fact seeing the first name in alphabetical order within each item type so we've overcome the error but this field is actually not very useful we don't really care to see what's the first name in alphabetical order within each type but at least our aggregation is making sure that there's only one value of name for each item type and so the golden rule of grouping is respected and we don't get that error anymore the second alternative is to take name and add it as a grouping field which simply means putting it after item type type in here now what do you expect to happen if I run this query so these results as they show here are a bit misleading because there's actually the name column is hidden so I will also add it here and as you can see I can now refer the name column in select without an aggregation why because it is a grouping field okay and what do we see here in the results well we've seen what happens when you Group by multiple columns that the unique combinations of these columns end up subdividing the data so in fact our values for average power are not divided by item type anymore we don't have the average power for armor potion and weapon anymore we have the average power for an item that's type armor and it's called chain mail armor and that is in fact there's only one row that does that and has power 70 likewise we have the average power for uh any item called cloak of invisibility which is of item type armor and again there's only one example of that so we've overcome our error by adding name as a grouping field but we have lost the original group division by item type and we have subdivided the data to the point that it doesn't make sense anymore so as you surely have noticed by now we made the error Disappear by including name but we haven't actually achieved our original objective which was to show the name of each item the item type and then the average power within that item type well to be honest my original objective was to teach you to spot this error and understand the law of grouping but now you might rightfully ask how do I actually achieve this and the answer unfortunately is that you cannot achieve this with group Pi not in a direct simple way and this is a limitation of group Pi which is a very powerful feature but it doesn't satisfy all the requirements of aggregating data the good news however is that this can be easily achieved with another feature called window functions now window functions are the object of another section of this course so I'm not going to go into depth now but I will write the window function for you just to demonstrate that it can be done easily with that feature so I'm going to go down here and write a a new query I'm going to take the items table and I'm going to select the name and the item type and then I'm going to get the average of power and again I'm going to use back ticks so bigquery doesn't get confused with the function that has the same name and then I'm going to say take the average of power over Partition by item type so this is like saying average of power based on this item type and I will call this average Power by type and if I select this and run the query you will see that I get what I need I have a chain mail armor it's armor and the average power for an armor is 69.5 so this is how we can achieve the original objective unfortunately not with grouping but with window functions now I want to show you how you can filter on aggregated values after a group buy so what I have here is a basic Group by query go to the fantasy characters table group it by class and then show me the class and within each class the average of the experience for all the characters in that class and you can see the results here now what if I wanted to only keep those classes where the average experience is at least 7,000 how could I go and do that one Instinct you might have is to add a wear filter right for example Le I could say where average experience is greater than or equal to 7,000 and if I run this I get an error unrecognized name average experience the wear filter doesn't work here maybe it's a labeling problem what if I actually add the logic instead of the label so what if I say where average of experience is bigger or equal to 7,000 well an aggregate function is actually not allowed in the work Clause so this also doesn't work what's happening here now if we look at the order of SQL operations we can see that the where Clause runs right after sourcing the data and according to our rules over here an operation can only use data produced before it and doesn't know about data produced after it so the wear operation cannot have any way of knowing about aggregations which are computed later after it runs and after running the group bu and this is why it is not allowed to use aggregations inside the wear filter luckily SQL provides us with a having operation which works just like the wear filter except it works on aggregations and it works on aggregations because it happens after the group buy and after the aggregations so to summarize you can Source the table and then drop rows before grouping this is what the wear filter is for and then you can do your grouping and Compu your aggregations and after that you have another chance to drop rows based on a filter that runs on your aggregations so let us see how that works in practice now instead of saying where average experience actually let me just show you what we had before this is our actual result and we want to keep only those rows where average experience is at least 7,000 so after group Pi I will write having and then I will say average experience greater than or equal to 7,000 let me remove this part here run the query and you can see that we get what we need and you might be thinking well why do I have to to write down the function again can't I just use the label that I've assigned well let's try it and see if it still works and the answer is that yes this works in Big query however you should be aware that bigquery is an especially userfriendly and funto use product in many databases however this is actually not allowed in the sense that the database will not be kind enough to recognize your label in the having operation instead you will have to actually repeat the logic as I'm doing now and this is why I write it like this because I want you to be aware of this limitation another thing that you might not realize immediately is that you you can also filter by aggregated columns which you are not selecting so let's say that I wanted to group by class and get the average experience for each class but only keep classes with a high enough average level I am perfectly able to do that I just have to write having average level greater than or equal to 20 and after I run this you will see that instead of four values I actually get three values so I've lost one value and average level is not shown in the results but I can of course show it and you will realize that out of the values that have stayed they all respect this condition they all have at least 20 of average level so in having you are free to write filters on aggregated values regardless of the columns that you are selecting so to summarize once more you get the data that you need you drop rows that are not needed you can then Group by if you want subdivide the data and then compute aggregations within those groups if you've done that you have the option to now filter on the result of those aggregations and then finally you can pick which columns you want to see and then apply all the other operations that we have seen in the course we are now ready to learn about window function a very powerful tool in SQL now window functions allow us to do computations and aggregations on multiple rows in that sense they are similar to what we have seen with aggregations and group bu the fundamental difference between grouping and window function is that grouping is fundamentally altering the structure of the table right because if I go here and I take this items table and I group by item type right now I'm looking at uh about 20 rows right but if I were to group the resulting table would have one two three three rows only because there're only three types of items so that would significantly compress the structure of my table and in fact we have seen with the basic law of grouping that after you apply a group ey you have to work around this fundamental alteration in the structure of a table right because here you can see that the items table has 20 rows but how many rows do you expect it to have after you Group by item type I would expect it to have three rows because there's only three types of items and so my table is being compressed my table is changing its structure and the basic law of grouping teaches you how to work with that it tells you that if you want a group by item type you can just select power as is because your table will have three rows but you have 20 values of power so you have to instead select an aggregation on power so that you can compress those values to a single value for each item type and if you want to select name you also cannot select name as is you also have to apply some sort of aggregation for example you could put the names into a list an array uh or so on but window functions are different window functions allow us to do aggregations allow us to work on multiple values without however altering the structure of the table without changing the number of rows of the table so let us see how this works in practice now imagine that I wanted to get the sum of all the power values for my items so what is the total power for all of my items so you should already be aware of how to do this in SQL to just get that sum right I can I can do this by getting my fantasy items table and then selecting the sum over the power so if I take this query and paste it in big query I will get exactly that and this now is a typical aggregation right the sum aggregation has taken 20 different values of power and has compressed them down to one value and it has done the same to my table it's taken 20 different rows to my table and it has squished them it has compressed them down to one row and this is how aggregations work as we've seen in the course but what if I wanted to show the total power without altering the structure of the table what if I wanted to show the total power on every Row in other words I can take the sum of all the values of power and this is the same number that we've seen in B query and I can paste it over here and hopefully I can now expand it and this is exactly what I meant what if I can take that number and put it on every row and why would I want to do this well there's several things that I can do with this setup right for example I could go here um for Phoenix Feather which is power 100 and I could say take this 100 and divide it by the total power in this row and then turn this into a percentage and now I have this 6.5 approximately percentage and thanks to this I can say Hey look um the phoenix feather covers about 6 or 7% of all the power that is in my items of all the power that is in my game and that might be a useful information a more mundane concern uh could be that this is uh your your budget so this is the stuff you're spending on and instead of power you have the the price of everything and then you get the total sum right which is maybe what you spent in a month and then you want to know going at the movies what percent of your budget it covered and so on now I will delete this value because we're not going to use it and let us see what we need to write to obtain this result in SQL so once again we go to the fantasy items table and I'm going to move it a bit down and then we select the sum power just just like before except that now I'm going to add this over open round bracket and close round bracket and this is enough to obtain this result well to be precise when I write this in B query I will want to see a few columns as well so I will want to see the name item tab Ty and power and here I will need a comma at the end as well as the sum power over and I will also want to give a label to this just like I have in the spreadsheet now this is the query that will reproduce What you see here in the spreadsheet so how this works is that the over keyword is signaling to SQL that you want to use a window function and this means that you will get an aggregation you will do a calculation but you're not going to alter the structure of the table you are simply going to take the value and put it in each row this is what the over keyword signals to SQL now because this is a window function we also need to define a window what exactly is a window a window is the part of the table that each row is is able to see now we will understand what this means much more in detail by the end of this lecture so don't worry about it but for now I want to show you that this is the place where we usually specify the window inside these brackets after the over but we have nothing here and what this means is that our window for each row is the entire table so that's pretty simple right each row sees the entire table so to understand how the window function is working we always have to think row by row because the results can always be different on different rows so let us go row by row and figure out how this window function is working so now we have the first row and what is the window in this case meaning what part of the table does does this row see well the answer is that this row sees all of the table given that it sees all of the table it has to do the sum of power and so it will take this thing compute a sum over it put it in the cell now that was the first row moving on to the second row now what's the window here what part of the table does this row see once again it sees all of the table given that it sees all of the table it takes power computes some over it gets the result and puts it in the cell now I hope you can see that the result has to be identical in every cell in every Row in other words because every row sees the same thing and every Row computes the same thing and this is why every Row in here gets the same value and this is probably the simplest possible use of a window function so let us now take this code and bring it to B query and make sure that it runs as intended and like I said in the lecture on grouping you will see that power is blue because bequer is getting confused with its functions so always be best practice to put it into back tis to be very explicit that you are referring to a column but basically what you see here is exactly what we have in our sheet and now of course we have this new field which shows me the total of power on every row and like I said we can use this for several purposes for example I can decide to show for each item what p percentage of total power it covers right that's what I did before in the sheet so to do this I can take the power and I can divide by this window expression which will give me the total power not sure what happened there but let me copy paste here and I can call this percent total power now this is actually just a division so if I want to see the percentage I will have to also multiply by 100 but we know how to do this and once I look at this we can see that when we have power 100 we have almost 6.5% of the total power so this is the same thing that we did before and this goes to show that you can use these fields for your calculations and like I said if this was your budget you could use this to calculate what percentage of your total budget is covered by each item it's a pretty handy thing to know now why do I have to take this uh to repeat uh all of this logic over here why can't I just say give me power divided by some power well as you know from other parts of the course the select part is not aware of these aliases it's not aware of these labels that we are providing so when I try to do this it won't recognize the label so unfortunately if I want to show both I have to repeat the logic and of course I'm not limited to just taking the sum right what I have here is an aggregation function just like the ones we've seen with simple aggregations and grouping in aggregation so instead of sum I could use something like average using the back TI over right I need to remember uh to add the over otherwise it won't work because it won't know it's a window function and I can give it a label and now for each row I will see the same value which is the average of power over the whole data set and you you can basically use any aggregation function that you need it will work all the same few more btics to put in here just to be precise but the result is what we expect now let us proceed with our Explorations so I would like now to see the total power for each row but now I'm not interested in the total power of the data set I'm interested of in the total Power by item type okay so if my item is an armor I want to see the total power of all armors if my have item is a potion I want to see the total power of all potions and so on because I want to compare items within their category I don't want to compare every item with every item so how can I achieve this in the spreadsheet well let us start with the first r row so I need to check what item type I have and conveniently I have sorted this so we can be quicker now we have an armor so I want to see the total power for armor so what I can do is to get the sum function and be careful to select only rows where the item type is armor and this is what I get and then the next step would be to Simply copy this value and then fill in all of the rows which are armor because for all of the rows but again you have to be careful because the spreadsheet wants to complete the pattern but what I want is the exact same number and then all of the rows that have item type armor will have this value because I'm looking within the item type now I will do it for potion so here I need to get the sum of power for all items that are potions 239 and then make sure to co copy the exact same value and to extend it to all potions and next we have weapons so sum of all power by weapon which is here then copy it and copy it and then let's see if it tries to complete the pattern it does so I'm just going to go ahead and paste it and now make this a bit nicer and now I have what I wanted to get each row is showing the total power within the items that are the same as the one that we see in the row now how can I write this in SQL so let me go ahead and write it here now two parts of this query will be the same same because we want to get the items table and see these columns but we need to change how we write the window function so once again I want to get the sum of power and I will need now to define a specific window now remember the window defines what each row sees so what do I want each row to see when it takes the sum of power for example what do I want the first row to see when it takes the sum of power I wanted to see only rows which have the item type armor or in other words all the rows with the same item type and I can achieve this in the window function by writing Partition by item type by adding a partition defining the window as a partition by item type means that each row will look at its item type and then we'll partition the table so that it only sees rows which have the same item type so this row over here will see only these four rows and then you will take the sum of power and then you will put it in the cell and for this uh the second third third and fourth row the result will be the same because they will each see this part of the table when we come to potion so this row over here will say hey what is my item type it's potion okay then I I will only look at rows that have item type potion and so this will be the window for these four rows and then in those rows I'm going to take power and I'm going to Summit and finally when we come to to these rows over here so starting with this row it will look at its item type and say okay I have item type uh weapon let me look at all the rows that share the same item type and so each window will look like this so let me color it properly its window will look like this and then it will take the sum of these values of power that fit in the window and put it in the cell second cell sees the same window sums over these values of power puts it in the cell and this is how we get the required result this is how we use partitioning in window functions so let's go now to Big query and make sure that this actually works and when I run this I didn't put a label but you can see that I'm basically getting the same result when I have a weapon I see a certain value when I have a potion I see uh another one and when I have an armor I see the third value so now for each item I am seeing the total power not over the whole table but within the item type now next task find the cumulative sum of power which is this column over here what is a cumulative sum it's the sum of the powers of this item plus all of the items that are less powerful so to do this in the spreadsheet I will first want to reorder my data because I want to see it simply in order of power so I will actually take this whole range and I will go to data sort range Advance options and I will say that the data has a header row so that I can see the names of the columns and then I will order by power ascending so as you can see my records have now been sorted in direction of ascending power now how do I compute the cumulative sum of power in the first row all we have is 30 so the sum will be 30 in the second row I have 40 in this row plus 30 before so E I will have 70 when it comes here I have 50 in this row and then the sum up to now was actually 70 which I can see by looking at these two cells or I can see more simply by looking at the last cell so 50 + 70 will be 820 and proceeding like this I could compute the cumulative power over the whole column now for your reference I have figured out the correct Google Sheets formula that will allow you to compute the cumulative sum of power for our example and I went ahead and computed it so that we have it for all our data now this is is the formula right here and I'm not going to go in depth into it because this is not a course on on spreadsheets but I will show you the formula just in case you're curious so the sum IF function will take the sum over a range only but it will only consider values that satisfy a certain logical condition so the first argument is the range that we want to sum over and this is the power and the Criterion so what needs to be true for a value to be um to be considered is that this value is lesser than or equal to the level of power in this row so what this formula is saying is take the level of power in this row and then take all the values of power which are lesser or equal and then sum them up this is exactly what our window function does and so our formula reproduces this now if you go and look what's the way to do a cumulative sum in Google Sheets or what's the way to do a running total there are other Solutions but they do come with some um pitfalls they do come with some Corner cases so this is a Formula that's actually reproducing the behavior of SQL now let us go back to actually SQL and see how we would write this so I'm going to take the fantasy items table and I'm still going to select the columns and now I have to write my window function now the aggregation is just the same so take the sum of power and now I have to Define my window now my window is not defined Now by a partition but it is defined by an ordering order by power and when I say order by power in a window function what's implicit in this is the keyword ask for ascending so this means that the window will order power from the smallest to the biggest and I can choose to write this keyword or not because just like in order by in SQL when you don't specify it the default value is ascending from smallest to biggest so how does this window work work let's start with the first row and let's say we need to fill in this value so I'm going to look at my power level it is 30 and then the window says that I can only see rows where the power level is equal or smaller and what are the rows where the power level is equal or smaller to 30 there're these rows over here so effectively this this is the only part of the table that this window sees on the first row and then take the sum over power so sum over 30 is 30 move on to the second row the power level is 40 the window says I only see rows where the power level is smaller uh or equal and this includes these two rows over here now take the sum of power over here you get 70 put it in the cell third row I have power level 50 I'm only seeing these rows so take the sum of power over this it's 120 put it in the cell and I can continue like this until I get to the highest value in my data set it's 100 never mind that is not the last row because both of the last two rows they have the highest value and when you look at this um when you come to this row and you look at 100 and and you say what's the window what rows can I see I can see all rows where power is 100 or less and that basically includes all of the table right it includes all of the table so when you take the sum of power you will get the total sum and in fact you can see that in this case the cumulative power is equal to the total power that we computed before just as we would expect so this is easy to see here because we have ordered um our data conveniently but it works in any case and so what the order by does in a window function is that it makes sure that each row only sees rows which come before it given your ordering so if I want to order from the smallest power to the biggest power each row will only see rows that come before it in this ordering so they have the same level of power or lower but they don't have a higher level of power so let us now take it to Big query and make sure it works as intended and I will add an ordering by power and here I will see the same thing that I've shown you in the spreadsheet I notice now that some numbers are different that these two items have 90 instead of 100 but never mind that the logic is the same and the numbers make sense now I'm also able to change the direction of the ordering right so let's say that I take this field and copy it just the same except that instead of ordering by power ascending I order by power descending so what do you expect to see in this case let's take a look now what I see here is that each item is going to look at its level of power and then it's only going to consider items that are just as powerful or more powerful right so it's the exact same logic but it's reversed so when you look at the weakest item potion it has 30 and so it is looking at all the items because there's no weaker item and so it finds the total level of power in our data set but if you go to the strongest item like Excalibur it has a power level of 100 and there's only two items in the whole data set that have this power level itself and the phoenix feather so if you sum the power over this you get 200 so you can see it's the exact same logic but now each row only sees items that have the same level of power or higher so when you order inside a window function you can decide the direction of this ordering by using descending or ascending or if you are a lazy programmer you can omit the um ascending key word and it will work just the same because that's the default and finally we want to compute the cumulative sum of Power by type and you might notice that it is in a way the combination of these two uh requirements so let us see how to do that now the first thing I want to do is to sort our data in order to help us so I'm going to get this whole thing and I'm going to say sort range I'm going to need the advanced options I have a heading row and so first of all I want to order by type and then within each type I want to order by power and this is our data now now for each item I want to show the cumulative sum of power just like I did here except that now I only want to do it within the same item type so if we look at Armor it's already sorted right so I have power 40 and this is the smallest one so I will just put 40 over here next I have uh this item with power 70 it's still armor has power 70 and so I'm going to look at these two values and sum them up now I have uh 7 8 so I will take this plus 78 which is the sum of these three values and finally I have um 90 which is the sum of those values and now I'm done with armor right I'm beginning with a new item type so I have to start all over again I'm looking at potions now so we start with 30 that is the smallest value then we move to uh 50 so this is now seeing 30 and 50 uh which is 80 add 60 to 80 that is 140 and finally we want to add we want to add 99 plus 140 which is another way of saying that we want to add these values all the values for potion so this is what we want cumulative sum of power within item type so we do it within the item type and then when we find a new type we start over so to calculate it for weapon I could copy my function from here paste it in weapon and then I would need to modify it right I would need the range to only include weapon so that's from C10 so go here C10 is the first one and the value that I want to look at here would have to be C10 as well because I want to start by looking at the power level for the for weapon and for some reason it's purple however it should be correct it should always be the sum of the previous value so we start with 65 then we have 65 + 75 66 75 65 and so on so this is our result it's cumulative power within the item type and to write this in SQL I will take my previous query over here and now when we Define the window we can simply combine what we've done before we can combine the partition buy with the order bu and you need to write them in the following order first the partition and then the order so I will Partition by item type and I will order by power ascending and this will achieve the required result so for each row in this field the window will be defined as follows first Partition by item type right so first of all you can only see rows which have the same item type as you have but then within this partition you can you have to keep only rows where the power is equal or smaller than what you have so in the case of the first item you only get this row likewise in the case of the first potion item you only get this row if you look at the second armor item again it looks it partitions right so it looks at all the items which have armor but then it has to discard those that have a bigger power than itself so it will be looking at these two rows and if for example example we look at the last row over here so this row will say oh okay I'm a weapon so I can only see those that are weapon and then I can only see those that have a level of power that's equal or smaller than mine and that checks out those are all the rows and in fact the sum over here is equal to the sum of Power by type which is what we would expect so once again let us verify that this works in Big query and I will actually want to order by item type and power just so I have the same ordering as in my sheet and I should be able to see that within armor you have this like growing uh cumulative sum and then once the item changes it starts all over right it starts again at the value it grows it grows it accumulates and then we're done with potions and then we have weapons and then again it starts and then it grows and it goes all the way to include the total sum of all powers in the weapon item type so here's a summary of all the variants of Windows that we've seen we have seen four variants now in all of those for clarity we've kept the aggregation identical right we are doing some over the power field but of course you know that you can use any aggregate function here on any column which is compatible with that aggregate function and then we have defined four different Windows the first one is the simplest one there's actually nothing in the definition we just say over and this means that it will just look at all the table so every row will see the whole table and so every row will show you the total of power for the whole table simple as that the second window is introducing a partition by item type and what this means in practice is that each row will uh look at its own item type and then only consider rows which share the same exact item type and So within those rows it will calculate the sum of power third window we have an ordering field so what this means is that each row is going to look at its level of power because we are ordering by power and then it's going to only see rows where the power level is equal or smaller and the reason why we're looking in this direction is that when we order by power is implicitly uh understood that we want to order by power ascending If instead we ordered by power descending it would be the same just in the opposite direction each row would would look at its level of power and then only consider rows where power is equal or bigger and then finally we have a combination of these two right a we have a window where we use both a partition and an order and so what this means is that uh each row is going to look at its item type and discard all of the rows which don't have the same item type but then within the rows that remain it's going to apply that ordering it's going to only consider rows which have the same level of power or lesser so it's simply a combination of these two conditions and this is the gist of how window functions work first thing to remember window function provide aggregation but they don't change the structure of the table they just insert a specific value at each row but after applying a window function the number of rows in your table is the same second thing thing to remember is that in the window definition you get to Define what each row is able to see when Computing the aggregation so when you are thinking about window function you should be asking yourself what part of the table does each row see what's the perspective that each row has and there are two dimensions on which you can work in order to Define these windows one is the partition Dimension and the other is the ordering Dimension the partition Dimension Cuts up the table based on the value of a column so you will only keep rows that have the same value the order Dimension Cuts up the table based on the ordering of a field and then depending on ascending or descending depending on the direction that you choose you can you can look at rows that are after you in the ordering or you can look at rows that are before you in the ordering and you can pick either of these right either partitioning or ordering or you can combine them and by using this you can Define all of the windows that you might need to get your data now as a quick extension of this I want to show you that you're not limited to defining windows on single fields on single columns you can list as many columns as you want so in this example I'm going to the fantasy characters table I'm getting a few columns and then I'm defining an aggregation uh on a window function so I'm taking the level uh field and I'm summing it up and then I'm partitioning by two Fields actually by Guild and is alive so what do you expect to happen if I do this this is actually the exact same logic as grouping by multiple fields which we've seen in the group ey now the data is not going to be divided by Guild and is not going to be divided by whether the character is alive or not but by the all the mutual combinations between these fields okay so um merkwood and true is one combin ation and so the people in here are going to fit together right so in fact we have two characters here 22 and 26 and their sum is 48 so you can see here that they both get 48 for sum of level and likewise when you look at Sher folk true these three they all end up in the same group and so they all share the same sum of level which is 35 but sh Fulk fals this is another group and they're actually alone right it's 12 and then the sum is 12 so again when you Partition by multiple Fields the data is divided in groups that are obtained by all the combinations between the values that these fields can have and if you experiment a bit by yourself you should have an easier time to convince yourself of this likewise the same idea applies to the order uh part of a window we have until now for Simplicity ordered by one field to be honest most times you will only need to order by one field but sometimes you might want to order by different fields so in this example you can see that we are defining our ordering based on two Fields power and then weight and then based on that ordering we calculate the sum of power and this is again a case of cumulative sum however now the ordering is different and you will realize this if we go to the most powerful items in our data these last two which are both at 100 so if you remember when we were ordering by power alone these two uh Fields had the same value in this um window function because when you order just by power they are actually the same they both have 100 but because now we're ordering by weight and again we're ordering by weight ascending so from the smallest weight to the biggest weight now the phoenix feather comes first because although it has the same power as Excalibur the Phoenix weather is lighter and because it comes first it has a different value for this aggregation and of course we have the power to to say ascending or descending on each of the fields by which we order so if I wanted to reverse this I could simply write descending after the weight and be careful that in this case descending is only referring to weight it's not referring to power so this is just as if I've wrote this right so the this one can be omitted um because it's ascending by default but I would write both to be clear and now if I run this you will see that our result is reversed right Excalibur comes first because we have weight descending so it's heavier and then last we have the phoenix feather which is lighter and again understanding this theoretically is one thing but I do encourage you to experiment with this with your data with exercises and then you will um you will be able to internalize it and now we are back to our schema for The Logical order of SQL operations and it is finally complete again because we've seen all of the components that we can use to assemble our SQL query and now the question is where do window functions fit into this well as you can see uh we have placed them right here so what happens is that again you get your data and then the we filter runs dropping rows which you don't need and then you have a choice whether to do a group by right now if you do a group by you're going to change the structure of your table it's not going to have the same number of rows it's going to have a number of rows that depends of the unique values of your grouping field or the unique combinations of values of your Fields if you have used more than one if you group you will probably want to compute some aggregations and then you may want to filter on those aggregations meaning dropping rows uh based on the values of those aggregations and here is where window functions come into play it is on this result that window functions work so if you haven't done a group bu then window functions will work on your data after the wear filter runs if you have done a group buy we window functions will work on the result of your aggregation and then after applying the window function you can select which columns you want to show give them uh labels and then all the other parts run right so you can choose to drop duplicates from your result meaning duplicate rows rows which have the same value on every column you can stack together different tables right you can put them on top of each other and then finally when you have your result you can apply some ordering and also you can cut the result you can limit it so you only show a few uh rows and this is where window functions fit into the big scheme of things and there are some other implications of this ordering one interesting one is that if you have computed aggregations such as the sum of a value Within within a um a class um you can actually use those aggregations in the window function so you can sort of do an aggregation of an aggregation but this is uh in my opinion an advanced topic and it doesn't fit into this um fundamentals course it may fit uh someday in a later more advanced course I want to show you another type of window functions which are very commonly used and very useful in SQL challenges and SQL interviews and these are numbering functions numbering functions are functions that we use in order to number the rows in our data according to our needs and there are several numbering functions but the three most important ones are without any doubt row number dense Rank and rank so let's let's see how they work in practice now what I have here is a part of my uh inventory table I'm basically showing you the item ID and the value of each number and conveniently I have ordered our rows uh by value ascending okay and now we are going to number our rows according to the value by using these window functions now I've already written the query that I want to reproduce so I'm going to the fantasy inventory table and then I'm selecting the item ID and the item value as you see here and then I'm using uh three window functions so the syntax is the same as what we've seen uh in the previous exercise except that now I'm not using an aggregation function over a field like I did before when I was doing a sum of power and so on but I'm using another type of function this is a numbering function okay so this functions over here they don't actually take a parameter as you can see that there's nothing between these round brackets because I don't need to provide it an argument or a parameter all I need to do is to call the function but what really uh what's really important here is to define the correct window and as you can see in the three examples here the windows are all the same I am simply ordering my rows by value ascending which means that when it's going to compute the window function every row will look at its own value and then say okay I'm only going to see rows where the value is the same or smaller I'm not going to be able to visualize rows where the value is bigger than mine and this is what the window does so the first row over here will'll only see value of 30 the second row will see this the third row will see these and so on up until the last row which will see itself and all the other rows as well now let us start with row number so row number is going to use this ordering over here in order to number my rows and it's as simple as saying putting one in the first row two in the second one 3 four and so on so if I extend this pattern I'm going to get a number for every row and that's it that's all that row number does it assigns a unique integer number to every row based on the ordering that's defined by the window function and you might think oh big deal why do I need this don't I already have like row numbers over here in the spreadsheet well in Pro SQ problems you often need to order things based on different values and um row number allows you to do this you can also have many different orderings coexisting in the same table based on different conditions and that can come in handy as you will discover if you do SQL problems now let's move on to ranking so first of all we have dense rank okay and ranking is another way of counting but is slightly different sometimes you just want to count things you know sometimes uh like we did here in row number like I don't know you are a dog sitter and you're given 20 dogs and you getting confused between all their their names and then you assign a unique number to every dog so that you can identify them uh and you can sort them by I don't know by age or by how much you're getting paid to docit them sometimes on the other hand you want to rank things like when choosing which product to buy or expressing the results of a race right if and the difference between ranking and Counting can be seen when you have the same value right so when you want to Simply number like we did here when you want to Simply assign assign a different number to each element and two things have the same value then you don't really care right you need to sort of arbitrarily decide that okay one of them will be a number two and one of them will be number three but you cannot do the same for ranking if two students in a classroom get the best score you can't just randomly choose that one of them is number one and the other is number two they have to both be number one right and if two people finish a race at at the same time and is the best time you can't say that one uh won the race and the other didn't that because one is number one the other is arbitrarily number two they both have to be number one right they have to share that Rank and this is where ranking differs so let's go in here and apply our rank now we are ordering by value ascending which means that the smallest value will have rank number one and so 30 has rank number one now we go to the second row and again remember window functions that you always have to think row by row you have to think what each row sees and what each row decides so again the row is going to order by uh value so it's only going to see these values over here and it has to decide its rank so this row says uh oh I'm not actually number one because there is a value which is smaller than me so that means I have to be number number two and then we get to the third row and this row is uh seeing all the values that come before it right they're equal or or or smaller and now it's saying oh I'm not number one because there's something smaller but then uh the value 50 which uh this guy has uh is rank two and I have the same value number 50 we arrived in the same spot so I must have the same rank okay and this is the difference between row number and rank that identical values get the same rank but they don't get the same row number and now we come to this row which is 60 so it's going to look back and it's going to say oh from what I see 30 is the smallest one so it has a rank of one and then you have 50 and 50 they both share a rank of two but I am bigger so I need a new rank and so what am I going to pick now as a new rank well I'm going to pick three because it's the next uh number in the sequence then the next one is going to pick four the next one is going to pick five and then we have six and then it proceeds in the following way so I'll do it quickly now so 7 8 9 10 11 and again careful here we're sharing the same value so they are both 11 next we can proceed to 12 13 again the same value right so they have to share the 13th spot 14 so 14 for 1700 and then 14 again and then 15 and then 16 and this is what we expect to see when we compute the dense rank and finally we come to rank now rank is very similar to dense rank but there is one important difference so let's do this again smallest value has rank number one like before and then we have 50 which has rank number two and then 50 is once more sharing rank number two and now we move from 50 to 60 so we need a new rank but instead of three we put four over here why do we put four because the previous rank covered uh two rows and it sort of at the three it sort of expanded to eight the three So based on the rules of Simply rank we have to lose the three and put four over here so this is just another way of managing ranking and you will notice that it conveys another piece of information compared to dense rank because not only I see that um this row over here has a different rank than the previous row but I can only I can also see how many members were covered by the previous uh ranks I can see that in the previous ranks uh they must have involved three members because I'm at four already and this piece of information was not available for dence rank so I will continue over here and so I have a new value which is uh rank five and then I have rank six rank seven rank 8 rank n Rank 10 rank 11 now I have rank 12 and again I have to share the rank 12 because two identical values but now because 12 has eaten up two spots I can't use the 13 anymore the second 12 has like eaten the 13 and so I need to jump straight to 14 15 15 again and now I have to jump to 17 because 15 had two spots 17 again and now I have to jump to 19 and then finally I have 20 so you can see that the final number uh is 20 for rank just as with row number because it's not only differentiating between ranks but it's also counting for me how many elements have come before me how many rows are contained in the previous ranks I can tell that there's 19 rows in the previous ranks uh because of how rank Works whereas with 10 rank we end ended up using only 16 uh ended up being only up to 16 so we sort of lost information on how many records we have and this might be one of the reasons why by default you have this method of ranking instead of this method of ranking even though dense rank seems more intuitive when you are uh building the ranking yourself so we can now take this query and hopefully I've written it correctly and go to big query and try to run it and as you can see we have our items they are sorted by value and then we have our numbering functions so row number should go from one to 20 without any surprises CU it's just numbering the rows this dense rank should have rank one for the first and then these two should share the same rank because they have both have 50 and then the next rank is three so just as I've shown you in the spreadsheet similarly here you have 11 11 and then 12 rank uh instead starts off uh just the same uh smallest value has rank number one and the next two values have rank number two but then after using up two and two it's like you've used up the three so you jump straight to four and after doing 15 and 15 you jump straight to 17 after doing 17 17 you jump straight to 19 and then the the highest number here is 20 which tells you how many rows you're dealing with of course what you see here are window functions they work just the same as we I've shown you and so you could pick up Rank and you could order by value descending and then you will see you will find the inverse of that rank in the sense that the highest value item will give you rank one and it will go from there and the lowest value item will have sort of the the biggest rank number and and rank is often used like this you know the thing that has the most of what we want you know the biggest salary the biggest value the most successful product we rank it we make it so that it's rank one it's like the first in our race and then everyone else goes from there and so we often have actually we order by something descending when we calculate the rank and of course because these numbering functions are window functions they can also be combined with Partition by if you want to cut the data into subgroups so here's an example on the fantasy characters table we are basically uh partitioning by class meaning that each row only sees the other rows that share the same class so archers only care about archers Warriors only care about Warriors and so forth and then within the class we are ordering by level descending okay so the highest levels come first and using this to rank the characters okay so if I go here then I can see that within the archers the highest level Archer has level 26 so they get the first Rank and then all the others is go down down from there and then we have our Warriors and the highest level Warrior is 25 and they also get rank one because they are being ranked within Warriors so this is like when you have races and there are categories this like when you have a race and there are categories within the race so there are like many people who arrive first because they arrive first in their category it's not that everyone competes with everyone and so on and so forth you can see that each uh class of character has their own dedicated ranking and you can check the uh bigquery page on numbering function if you want to learn more about these functions you can see here the ones we've talked about rank row number and dense rank there are a few more but these are the ones that are most commonly used in SQL problems and because I know that it can be a bit confusing um to distinguish between row number dense Rank and rank here's a visualization that you might find useful so let's say that we have a list of values uh which are these ones and we are ordering them in descending order so you can see that there's quite some repetition in these values and given this list of values how would these different numbering functions work on them right so here's row number row number is easy it just um assigns a unique number to to each of them so it doesn't matter that the values are sometimes the same you sort of arbitrarily pick um one to be one the other to be two and then you have three and then here you have 10 10 10 but it doesn't matter you just want to order them so you uh do four five six and then finally seven dense rank is actually cares about the values being the same so 50 and 50 they both get one uh 40 gets two and then uh the 10 get three and then five gets four so easy the rank just grows uh using all the integer numbers dense rank is also assigning rank one to 50 and 50 but it's also throwing away the two because there are two elements in here then the next one is getting rank three because the two has already been used and then the next batch 1011 is getting rank four but it's also burning five and six and the next one then can only get rank seven so these are the differences between row number dance Rank and rank visualized we have now reached the end of our journey through the SQL fundamentals I hope you enjoyed it and I hoped that you learned something new you hopefully now have some understanding of the different components of SQL queries and the order in which they work and how they come together to allow us to do what we need with the data now of course learning the individual components and understanding how they work is only half the battle the other half of the battle is how do I put these pieces together how do I use them to solve real problems and in my opinion the response to that is not more Theory but it's exercises go out there and do SQL challenges do SQL interviews find exercises or even better find some data that you're interested in upload it in big query and then try to analyze it with SQL I should let you know that I have another playlist where I am solving 42 SQL exercises in postrest SQL and I think this can be really useful to get the other half of the course which is doing exercises and knowing how to face real problems with SQL and I really like this playlist because I'm using a free website a website that doesn't require any sign up or any login uh it just works works and you get a chance to go there and do all of these exercises that cover all the theory that we've seen in this course and then after trying it yourself you get to see me solving it and my thought process and my explanation and I think it could be really useful if you want to deepen your SQL skills but in terms of uh how do I put it all together how do I combine all of this stuff I do want to leave you with another resource that I have created which is this table and this table shows you the fundamental moves that you will need to do whenever you do any type of data analytics and I believe that every sort of analytics that you might work on no matter how simple or complicated can ultimately be reduced to these few basic moves and what are these moves they should actually be quite familiar to you by now so we have joining and this is where we combine data from multiple tables based on some connections between columns and in SQL you can do that with the join then we have filtering filtering is when we pick certain rows and discard others so you know let's look only at customers that joined after 2022 now how do you do that in SQL there are a few tools tools that you can use to do that the most important one is the wear filter and the wear filter comes in action right after you've loaded your data and it decides which rows to keep which rows to discard having does just the same except that it works on aggregated fields it works on fields that you've obtained after a group by qualify we actually haven't seen it in this course because it's not a universal component of SQL certain systems have it others don't but qualify is basically also a filter and it works on the result of window functions and finally you have distinct which runs quite at the end of your query and it's basically removing all duplicate rows and then of course you have grouping and aggregation and we've seen this in detail in the course you subdivide the data um on certain dimensions and then you calculate aggregate values within those Dimensions fundamental for analytics how do we aggregate in SQL we have the group by we have the window functions and for both of them we use aggregate functions such as sum average and so on and then we have column Transformations so this is where you apply logic uh arithmetic to transform columns combine column values and take take the data that you have in order to compute data that you need and we do this where we write the select right we can write calculations that involve our columns we have the case when which allows us to have a sort of branching logic and decide what to do based on some conditions and of course we have a lot of functions that make our life easier by doing specific next we have Union Union is pretty simp simple take tables that have the same columns and stack them together meaning put their rows together and combine them and finally we have sorting which can change how your data is sorted when you get the result of your analysis and can be also used in window functions in order to number or rank our data and these are really the fundamental elements of every analysis and every equal problem that you will need to solve so one way to face a problem even if you are finding it difficult is to come back to these fundamental components and try to think of how do you need to combine them in order to solve your problem and how can you take your problem and break it down to simpler operations that involve these steps now at the beginning of the course I promised you that uh we we would be solving a hard squl challenge together at the end of the course so here it is let us try now to solve this challenge applying the concepts in this course now as a quick disclaimer I'm picking a hard challenge because it's sort of fun and it gives us um a playground to Showcase several Concepts that we've seen in the course and also because I would like to show you that even big hard scary ch Alles that are marked as hard and even have advanced in their name can be tackled by applying the basic concepts of SQL however I do not intend for you to jump into these hard challenges um from the very start it would be much better to start with basic exercises and do them step by step and be sure that you are confident with the basic steps before you move on to more advanced steps so if you have trouble uh approaching this problem or even understanding my solution don't worry about it just go back to your exercises and start from the simple ones and then gradually build your way up that being said let's look at the challenge marketing campaign success Advanced on strata scratch so first of all we have one table that we will work on for this challenge marketing campaign so marketing campaign has a few columns and it actually looks like this okay so there's a user ID created that product ID quantity price now when I'm looking at the new table the one question that I must ask to understand it is what does each row represent and just by looking at this table I can have some hypotheses but I'm actually not sure what each row represents so I better go and read the text until I can get a sense of that so let's scroll up and read you have a table of inapp purchases by user okay so this explains my table what does each row represent it represents an event that is a purchase okay so it means that user ID 10 bought product ID 101 in a quantity of three at the price of 55 and created that tells me when this happened so this happened 1st of January 2019 so great now I understand my table and now I can see what the problem wants from me let's go on and read the question so I have a table of inapp purchases by users users that make their first inapp purchase are placed in a marketing campaign where they see call to actions for more Ina purchases find the number of users that made additional purchases due to the success of the marketing campaign the marketing campaign doesn't start until one day after the initial app purchase so users that made one or multiple purchases on the first day do not count nor do we count users that over time purchase only the products they purchased on the first day all right so that was a mouthful okay so this on the first run it's actually a pretty complicated problem so our next task now is to understand this text and to simplify it to the point that we can convert it into code okay and a good intermediate step before jumping into the code is to write some notes and we can use the SQL commenting feature for that so what I understand from this text is that users make purchases and we are interested in users that make additional purchases we're interested in users who make additional purchases thanks to this marketing campaign how do we Define additional purchases additional purchase is defined as and the fundamental sentence is this one users that made one or multiple Pur purchases on the first day do not count so additional purchase happens after the first day right nor do we count users that over time purchase only the products they purchased on the first day so the other condition that we're interested in is that it involves a product that was not bought the first day and finally what we want is the number of users so get the number of these users that should be a good start for us to begin writing the code so let us look at the marketing campaign table again and I remind you that each row represents a purchase so what do we need to find First in this table so we want to compare purchases that happen on the first day with purchases that happen the following day so we need a way to count days and what do we mean first day and following days do we mean the first day that the shop was uh open no we actually mean the first day that the user ordered right because the user signs up does the first order and then after that the marketing campaign starts so we're interested in numbering days for each user such that we know what purchases happened on the first day what purchases happened on the second day third day and so on and what can we use to run a numbering by user we can use a window function with a numbering function right so I can go to my marketing campaign table and I can select the user ID and the date in which they bought something and the product ID for now now I said that I need a window function so let me start and Define the window now I want to count the days within each user so I will actually need to Partition by user ID so that each row only looks at the rows that correspond to that same user and then there is an ordering right there is a a sequence from the first day uh in which the user bought something to the second and the third and so on so my window will also need an ordering and what column in my table can provide an ordering it is created at and then what counting function do I need to use here well the the way to choose is to say what happens when the same user made two two different purchases on the same date what do I want my function to Output do I want it to Output two different numbers as a simple count or do I want them want it to Output the same number and the answer is that I wanted to Output the same number because all of the purchases that happened on day one need to be marked as day one and all the purchases that have happened on day two need to be marked as day two and so on and so the numbering function that allows us to achieve this is Rank and if you remember ranking is works just like ranking the winners of a race everyone who shares the same spot gets the same number right and this is what we want to achieve here so let us see what this looks like now and let us order by user ID and created at let us now see our purchases now user 10 started buying stuff on this day they bought one product and the rank is one Let's us actually give a better name to this column so that it's not just rank and we can call it user day all right so this user id10 had first user day on the this date and they brought one product then at a later date they had their second user day and they bought another product and then they had a third now user 14 started buying on this date this was their first user day they bought product 109 and then the same day they bought product 107 and this is also marked as user day one so this is what we want and then at a later day they bought another product and this is marked as user day three remember with rank you can go from 1 one to three because this the F the spot marked as one has eaten the spot Mark as two that's not an issue in this problem so we are happy with this now if we go back to our notes we see that we are interested in users who made additional purchases and additional means that it happen s after the first day and how can we identify purchases that happened after the first day well there's a simple solution for this we can simply filter out rows that have a user day one right all of the rows where the user day is one represent purchases that the user made on their first day so we can discard this and keep only purchase that happened on the following days now I don't really have a way to filter on this uh window function because as you recall from the order of SQL operation the window function happens here and the wear filter happens before that so the wear filter cannot be aware of what happens in the window function and the having also happens before it so I need a different solution to filter on this field what I need to do is to use a Common Table expression so that I can break this query in two steps so I'm going to wrap this logic into a table called T1 or I can call it purchases for it to be more meaningful and if I do select star from purchases you will see that the result does not change but what I can do now is to use the wear filter and make sure that the user day is bigger than one and if I look here you will see that I have all purchases which happened after the users first day but there is yet one last requirement that I have to deal with which is that the purchase must happen after the first day and it must involve a product that the user didn't buy on the first day so how can I comply with this requirement now for all of the rows that represent a purchase I need to drop the rows that involve a product ID that the user bought the first day so if I find out that user 10 bought product 119 on day one this purchase does not count I'm not interested in it so how can I achieve this in code I'm already getting all the purchases that didn't happen on day one and then I want another condition so I will say and product ID not in and here I will say products that this user bought on day one right it makes sense so this is all the filters I need to complete my problem show me all the purchases that happened not on day one and also make sure that the user didn't buy this product on day one so what I need to do is to add a subquery in here and before I do that let me give a Alias to this table so so that I don't get confused when I call it again in the subquery so this first version of purchases that we're using we could call it next days because we're only looking at purchases that happen after the first day whereas in the subquery we want to look at purchases but we're interested in the ones that actually happened on day one so we could call this first day and and we can use a wear filter to say that first day user day needs to be equal to one so this is a way that we can use to look at the purchases that happened on the first day now when we make this list we need to make sure that we are use looking at the same user right and to do that we can say end first day user ID needs to be the same as next day's user ID and this ensures that we're looking at the same user and we're not getting confused between users and finally what do we need from the list of first day purchases we need the list of products so let me first see if the query runs so it runs there's no mistakes and now let us review the logic of this query we have purchases which is basically a list of purchases with the added value that we know if it happened on day one on day two on day three and so on and then we are getting all of these purchases the ones that happened after day one and we are also getting the the list of products that they this user bought on day one and we are making sure to exclude those products from our final list and this is a correlated subquery because it is a specific SQL query that provides different results for every row that must run for every row because in the first row we need to get the list of products that user ID 10 has bought on day one and make sure that this product is not in it um and then when we go to another row such as this one we need to get the list of all products that user 13 bought on day one and make sure that 118 is not in those products so this is why it's a correlated subquery and the final step in our problem is to get the number of these users so instead of selecting star and getting all of the C columns I can say count distinct user ID and if I run this I get 23 checking and this is indeed the right solution so this is one way to solve the problem and hopefully it's not too confusing but if it is don't worry it is after all an advanced problem if you go to solution here I do think however that my solution is a bit clearer than what strata scratch provides this is actually a bit of a weird solution but that's ultimately up to you to decide and I am grateful to strata scratch for providing problems that I can solve for free such as this one welcome to postgress SQL exercises the website that we will use to exercise our SQL skills now I am not the author of this website I'm not the author of these exercises the author is Alis D Owens and he has generously created this website for anyone to use and it's free you don't even need to sign up you can go here right away and start working on it I believe it is a truly awesome website in fact the best at uh what it does and I'm truly grateful to Alis there for making this available to all the way the website works is pretty simple you have a few categories of exercises here and you can select a session and once you select a session you have a list of exercises you can click on an exercise and then here in the exercise view you have a question that you need to solve and you see a representation of your three tables we're going to go into this shortly and then you see your expected results and here in this text box over here you can write your uh answer and then hit run to see if it's the correct one the results will appear in this lower quadrant over here and if you get stock you can ask for a hint um and uh here there are also a few keyboard shortcuts that you can use and then after you submit your answer uh or if you are completely stuck you can go here and see the answers and and discussion and that's basically all there is to it now let's have a brief look at the data and see what that's about and the data is the same for all exercises and what we have here is the data about a newly opened Country Club and we have three tables here members represents the members of the country club so we have their surname and first name their address their telephone and uh the the date that which they joined and so on and then we have the bookings so whenever a member makes a booking into a facility that event is stored into this table and then finally we have a table of facility where we have information about each facility and U in there we have some some tennis courts some badminton courts uh massage rooms uh and so on now as you may know this is a standard way of representing how data is stored in a SQL system so you have um the tables and for each table you see the columns and for each column you see the name and then the data type right so the data type is the type of data that is allowed into this column and as you know each column has a single data type and you are not allowed to mix multiple data types within each column so we have a few different data types here and they have the postgress um name so in postgress an integer is a whole number like 1 2 3 and a numeric is actually a FL floating Point number such as 2.5 or 3.2 character varying is the same as string it represents a piece of text and if you wonder about this number in round brackets 200 it represents the maximum limit of characters that you can put into this piece of text so you cannot have a surname that's bigger than 200 characters and then you have a time stamp which represents a specific point in time and this is actually all the data types that we have here and finally you can see that the tables are connected so in the booking table every entry every row of this table represent an event where a certain facility ID was booked by a certain member ID at a certain time for a certain number of slots and the facility ID is the same as the facility ID field in facilities and the M ID field field is the same as the M ID or member ID field in members therefore the booking table is connecting to to both of these table and these logical connections will allow us to use joins in order to build queries that work on all of these three tables together and we shall see in detail how that works finally we have an interesting Arrow over here which represents a self relation meaning that the members table has a relation to itself and if you and if you look here this is actually very similar to the example that I have shown in my U mental models course um for each member we can have a recommended bu field which is the ID of another member the member who recommended them into the club and this basically means that you can join the members table to itself in order to get at the same time information about a specific member and about the member who recommended them and we shall see that in the exercises and clearly the exercises run on post SQL and postgress is one of the most popular opensource SQL systems out there postgress SQL is a specific dialect of SQL which has some minor difference es from other dialects such as my SQL or Google SQL that used is used by bigquery but it is mostly the same as all the others if you've learned SQL with another dialect you're going to be just fine postgress sqle does have a couple of quirks that you should be aware about but I will address them specifically as we solve these exercises now if you want to rock these exercises I recommend keep keeping in mind The Logical order of SQL operations and this is a chart that I have introduced and explained extensively in my mental models course where we actually start with this chart being mostly empty and then we add one element at a time making sure that we understand it in detail so I won't go in depth on this chart now but in short this chart represents the logical order of SQL operations these are are all the components that we can assemble to build our SQL queries they're like our Lego building blocks for for SQL and these components when they're assembled they run in a specific order right so the chart represents this order it goes from top to bottom so first you have from then you have where and then you have all the others and there are two very important rules that each operation can only use data produced above it and an operation doesn't know anything about data produced below it so if you can keep this in mind and keep this chart as a reference it will greatly help you with the exercises and as I solve the exercises you will see that I put a lot of emphasis on coming back to this order and actually thinking in this order in order to write effective queries let us now jump in and get started with our basic exercises so I will jump into the first exercise which is retrieve everything from a table so here I have my question and how can I get all the information I need from the facilities table and as you know all my data is represented here so I can check here to see where I can find the data that I need now as I write my query I aim to always start with the front part why start with the front part first of all it is the first component that runs in The Logical order so again if I go back to my chart over here I can see that the from component is the first and that makes sense right because before I do any work I need to get my data so I need to tell SQL where my data is so in this case the data is in the facilities table next I need to retrieve all the information from this table so that means I'm not going to drop any rows and I'm going to select all the columns and so I can simply write select star and if I hit run I get the result that I need here in this quadrant I can see my result and it fits the expected results now the star is a shortcut for saying give me all of The Columns of this table so I could have listed each column in turn but instead I took a shortcut and used a star retrieve specific columns from a table I want to print a list of all the facilities and their cost to members so as always let's start with the front part where is the data that we need it's in the facilities table again and now the question is actually not super clear but luckily I can check the expected results so what I need are two columns from this table which is name and member cost so to get those two columns I can write select name member cost hit run and I get the result that I need so if I write select star I'm going to get all the columns of the table but if I write the name of specific columns separated by comma I will get uh only those columns specifically control which rows are retrieved we need a list of facilities that charge a fee to members so we know that we're going to work with the facilities table and now we need to keep certain rows and drop others we need to keep only the rows that charge a fee to members so what component can we use in order to do this if I go back to my components chart I can see that right after from we have the we component and the we component is used to drop rows that we don't need right so in after getting the facilities table I can see I can say where member cost is bigger than zero meaning that they charge a fee to members and finally I can get all of the columns from this control which rows are retrieved part two so like before we want the list of facilities that charge a fee to members but our filtering condition is now a bit more complex because we need that fee to be less than 150th of the monthly maintenance cost so I copied over the code from the last exercise we're getting the data from our facilities list and we're filtering for those where the member cost is bigger than zero and now we need to add a new condition which is that that fee which is member cost is less than 150th of the monthly maintenance cost so I can take monthly maintenance over here and divide it by 50 and I have my condition now when I have multiple logical conditions in the wear I need to link them with the logical operator so SQL can figure out how to combine them because the final result of all my conditions needs to be a single value which is either true or false right so let's see how to do this in my mental models course I introduced the Boolean operators and how they work so you can go there for more detail but can you figure out which logical operator do we need here to chain these two conditions as suggested in the question the operator that I need is end so I can put it here here and what end does is that both of these conditions need to be true for the whole expression to evaluate to true and for the row to be kept so only the rows where both of these conditions are true will be kept and all other rows will be discarded now to complete my exercise I just need to select a few specific columns because we don't want to return all the columns here and I think that I will cheat a bit by copying them from the expected results and putting them here but normally you would look at the table schema and figure out which columns you need and that completes our exercise basic string searches produce a list of all facilities with the word tennis in their name so where is the data we need it's in the CD facilities table next question do I need all the rows from this table or do I need to filter out some rows well I only want facilities with the word tennis in their name so clearly I need a filter therefore I need to use the wear statement how can I write the wear statement I need to check the name and I need to keep only facilities which have tennis in their name so I can use the like statement here to say that the facility needs to have tennis in its name but what this wild card signify is that we don't care what precedes tennis and what follows tennis it could be zero or more characters before it and after it we just care to check that they have tennis in their name and finally we need to select all all the columns from these facilities and that's our result beware like I said before of your use of the quotes So what you have here is a string it's a piece of text that uh allows you to do your match therefore you need single quotes if you as it's likely to happen used double quotes you would get an error here and the error tells you that the column tenis the does not exist because double quotes are used to represent column names and not pieces of text so be careful with that matching against multiple possible values can we get the details of facilities with id1 and id5 so where is my data is in the facilities table and do I need all the rows from this table or only certain ones I need only certain rows because I want those that have id1 and id5 so I need to use a wear statement Now what are my conditions here their ID actually facility ID equals 1 and facility ID equals 5 so I have my two logical conditions now what operator do I need to use in order to chain them I need to use the or operator right because only one of these need needs to be true in order for the whole expression to evaluate to true and in fact only one of them can be true because it's impossible for the idea of a facility to be equal to one and five at the same time therefore the end operator would not work and what we need is the or operator and finally we need to get all the data meaning all the columns about this facility so I will use select star the problem is now solved but now let's imagine that tomorrow we need this query again and we need to include another id id 10 so what we can do is put or facility ID equals 10 but this is becoming a bit unwieldy right because imagine having a list of 10 IDs and then writing or every time and it's it's not very scalable as an approach approach so as an alternative we can say facility ID in and then list the values like one and five so if I take this and make it into my condition I will again get the same result I will get the the solution but this is a more elegant approach and it's also more scalable because it's much easier to come back and insert other IDs inside this list so this is a preferred solution in this case and logically what in is doing is looking at the facility ID for each row and then checking whether that ID is included in this list if it is it returns true therefore it keeps the row if it's not returns false therefore it drops the row and we shall see a bit later that the in uh notation is also powerful because in this case we have a static list of IDs we know that we want IDs one and five but in more advanced use cases instead of a static list we could provide another query a SQL query or a subquery that would dynamically retrieve a certain list and then we could use that in our query so we shall see that in later exercises classify result into buckets produce a list of facilities and label them cheap or expensive based on their monthly maintenance so we want to get our facilities do we need a filter do we need to drop certain rows no we actually don't we want to get all facilities and then we want to label them and we need to select the name of the facility and then here we need to provide the label so what SQL statement can we use to provide a text level label according to the value of a certain column what we need here is a case statement which implements conditional logic which implements a branching right it's similar to the if else statements in other programming languages because if the monthly maintenance cost is more than 100 then it's expensive otherwise it's cheap so this call for a case statement now I always start with case and end with end and I always write these at the beginning so I don't forget them and then for each condition I write when and what is the condition that I'm interested in monthly maintenance being above 100 that's my first condition what do I do in that case I output a piece of text which says expensive and remember single quotes for test text next I could write the next condition explicitly but actually if it's not 100 then it's less than 100 so all I need here is an else and in that case I need to Output the piece of text which says cheap and finally I have a new column and I can give it a label I can call it cost and I get my result so whenever you need to put values into buckets or you need to label values according to certain rules that's usually when you need a case statement working with dates let's get a list of members who joined after the start of September 2012 so looking at these tables where is our data it's in the members table so I will start writing this and now do I need to filter this table yes I only want to keep members that joined after a certain time and now how can I run this the condition on this table I can say where join date is bigger than 2012 September 01 so luckily in SQL and in postgress filtering on dates is quite intuitive even though here we have a time stamp that represents a specific moment in time up to the second we can say bigger or equal actually because we also want to include those who joined on the first day we can write bigger or equal and just specify the the date and SQL will fill in the the rest of the remaining values and the filter will work and next we want to get a few columns for these members so I will copy paste here select and this solves our query removing duplicates and ordering results we want an ordered list of the first 10 surnames in the members table and the list must not contain duplicates so let's start by getting our table which is the members table now we want to see the surnames so if I write this I will see that there are surnames which are shared by members so there are actually duplicates here so what what can we do in SQL in order to remove duplicates we have seen in the mental models course that we have the distinct keyword and the distinct is going to remove all duplicate rows based on the columns that we have selected so if I run this again I will not see any duplicates anymore now the list needs to be ordered alphabetically as I see here in the expected results and we can do that with the order by statement and when you use order by on a piece of text the default behavior is that the text is ordered alphabetically and uh if I were to use Des sending then it would be ordered in Reverse alphabetical order however that's not what I need I need it in alphabetical order so now I see that they are ordered and finally I want the first 10 surnames so how can I return the first 10 rows of my result I can do that with the limit statement so if I say limit 10 I will get the first 10 surnames and since I have ordered alphabetically I will get the first 10 surnames in alphabetical order and this is my result now going back to our map over here we have the from which gets a table we have a where which drops rows that we don't need from that table and then all the way down here we have the select which gets the columns that we need and then we have the distinct right and the distinct needs to know which columns we need because it's it drops duplicates based on these columns so in this example over here we're only taking a single column surname so the distinct is going to drop duplicate surnames and then at the end of it all when all the processing is done we can order our results and then finally once our results are ordered we can do a limit to limit the number of rows that we return so I hope this makes sense combining results from multiple queries so let's get a combined list of all surnames and all facility names so where are the surnames there in CD members and from CD m mbers I can select surname right and this will give me the list of all surnames and where are the facility names there are in CD facilities and I could say select name from CD facilities and I would get a list of all the facilities now we have two distinct queries and they both produce a list or a column of text values and we want to combine them what does it mean we want to stack them on top of each other right and how does that work well if I just say run query like this I will get an error because I have two distinct query here queries here and they're not connected in any way but when I have two queries or more defining tables and I want to stack them on top of each other I can use the union statement right and if I do Union here I will uh get what I want because all the surnames will be stacked uh vertically with all the names and I will get a unique list containing both of these columns now as I mentioned in the mental models course typically when you have just Union uh it means Union distinct and actually other systems like bigquery don't allow you to write just Union they want you to specify Union distinct and what this actually does is that after stacking together these two tables it removes all duplicate rows and uh the alternative to this is Union all which um does not do this it actually keeps all the rows and as you know we have some duplicate surnames and then we get them here and it doesn't fit with our result but if you write just Union it will be Union distinct and you won't have any duplicates and if you look at our map for The Logical order of SQL operations we are getting the data from a certain table and uh filtering it and then doing all sorts of operations and um on on this data and then we are selecting The Columns that we need and then we can uh remove the the duplicates from this one table and then what comes next is that we could combine this table U with other tables right we can tell SQL that we want to Stack this table on top of another table so this is where the union comes into play and only after we have combined all the tables only after we have stacked them all up on top of each other we can order the results and limit the results also remember and I showed this in detail in the mental models course um when I combine two or more table tables with a union what I need is for them to have the exact same number of columns and all of the columns need to have the same data type so in this case both tables have one column and this column is a text so the the union works but if I were to add another column here and it's an integer column it would not work because the union query must have the same number of columns right I will get an error however if I were to add an integer column in the second position in both tables they would work again because again I have the same number of columns and they have the same data type simple aggregation I need the sign up date of my last member so I need to work with the members table and we have a field here which is join date and I need to get the latest value of this date the time when a member last joined right so how can I do that I can take my join date field and run an aggregation on top of it what is the correct aggregation in this case it is Max because when it comes to dates Max will take the latest date whereas mean will take the earliest date and I can label this as latest and get the result I need now how aggregations work they are uh functions that look like this you write the name of the function and then in round brackets you provide the arguments the first argument is always the column on which to run the aggregation and what the aggregation does is that it takes a list of values could be 10 100 a million 10 million it doesn't matter it takes a long list of values and it compresses this list to a single value it um does like we've seen in this case taking all of the dates and then returning the latest date now to place this in our map we get the data from the table we filter it and then sometimes we do a grouping which we we shall see later in the exercises but whether we do grouping or not here we have aggregations and if we haven't done any grouping the aggregation works at the level of all the rows so in the absence of grouping as in this case the aggregation will look at all the rows in my table except for the rows that I filtered away but otherwise it will look at all the rows and then it will compress them into a single value more aggregation we need the first and last name of the last member who signed up not just the date so in the previous exercise we saw that we can say select Max join date from members and we would get the last join date the date when the last member signed up right so given that I want the first and the last name you might think that you can say first name and surname in here but this actually doesn't work this gives an error the error is that the column first name must appear in the group by clause or be used in a aggregate function now the meaning behind this error and how to avoid it is described in detail in the mental models course in the group by section but the short version of it is that what you're doing here is that with this aggregation you're compressing join date to a single value but you're doing no such compression or aggregation for first name and surname and so SQL is left with the um instruction to return something like this and as you can see here we have a single value but for these columns we have multiple values and this does not work in SQL because you need all columns to have the same number of values and so it it throws an error and what we really need to do here is to take this maximum join date and use it in a wear filter because we only want to keep that row which corresponds to the latest join date so we can take the members table and get the row where join date is equal to the max join date and from that select the name and the surname unfortunately this also doesn't work so what we saw in the course is that you cannot you're not allowed to use aggregations inside wear so you cannot use max inside where and the reason why is that actually pretty clear because aggregations happen at this stage in the in the process and aggregations need to know whether a group ey has occurred or not they need to know whether they have to happen over all the rows in the table or only within the groups defined by the group ey and when we are at the where stage the groupy hasn't happened yet so we don't know at which level to execute the aggregations and because of this we are not allowed to do aggregations inside the where statement so how can we solve the problem now well a a sort of cheating solution would be if we knew the exact value of join date we could place it here and then our filter would work we're not using an aggregation and we could put join date in here to display it as well and that would would work however this is a bit cheating right because um the maximum join date is actually a dynamic value it will change with time so we don't want to hardcode it we want to actually um compute it but because this is not allowed what we actually need is a subquery and the subquery is a SQL query that runs within a query to return a certain result and we can have a subquery by opening round brackets here and write writing a a query and in this query we need to go to the members table and select the maximum join date and this is our actual solution so in this execution you can imagine that SQL will go here inside the subquery run this get the maximum jointed place it in the filter uh keep only the row for the latest member who has joined and then retrieve what we need about this member let us now move to the joints and subqueries exercises the first exercise retrieve the start times of members bookings now we can see that the information we need is spread out into tables because we want the start time for bookings that and that information is in the bookings table but we want to filter to only get members named David farel and the name of the member is contained in the members table so because of that we will need a join so if we briefly look at the map for the order of SQL operations we we can see here that from and join are really the same uh step um and how this works is that in the from statement sometimes uh all my data is in one table and then I just provide the name of that table but sometimes I need to combine two or more different tables in order to get my data and in that case I would use the join but everything in SQL works with tables right so when I when I take two or more tables and combine them together at the end all I get is just another table and this is why from and join are actually the same component and they are the same step so as usual let us start with the front part and we need to take the booking table and we need to join it on the members table and I can give an alas to each table to make my life easier so I will call this book and I will call this mem and then I need to specify The Logical condition for joining this table and The Logical condition is that the M ID column in the booking table is really the same thing as the M ID column in the members table concretely you can imagine um SQL going row by Row in the booking table and looking at the M ID and then checking whether this m ID is present in the members table and if it's present it combines the row uh the current Row from bookings with the matching Row for members does this with all the matching rows and then drops rows which don't have a match and we saw that in detail in the mental models course so I'm not going to go in depth into it now that we have our table which is uh comes from the joint of members and bookings we can properly properly filter it and what we want is that the first name column is David in the column which comes from the members table right so m. first name is indicating the parent table and then the column name and the surname is equal to FAL and remember single quotes when using pieces of text this is a where filter you have two logical conditions and then we use the operator end because both of them need to be true so now we have uh filtered our data and finally we need to select the start time and that's our query now remember that when we use join in a query what's implied is that we are using inner join and there are several types of join but inner joint is the most common so it's the default one and what inner joint means is that it's going to return uh from the two tables that we're joining is going to return only the rows that have a match and all the row that don't have a match are going to be dropped so if there's a row in bookings and it has a m ID that doesn't exist in the members table that row will be dropped and conversely if there's a row in the members table and it has a m ID that is not referenced in the booking table that row will also be dropped and that's an inner join work out the start times of bookings for tennis courts so we need to get the facilities that are actually tennis courts and then for each of the facility we'll have several bookings and we need to get the start time for those uh bookings and it will be in a specific date so we know that we need the data from these two tables because the name of the facility is here but the data about the bookings is here so I will go from CD facilities join CD bookings on what are the fields that we can join on logically now let me first give an alias to these tables so I will call this fox and this I will call book and now what I need to see is that the facility ID matches on both sides now we can work work on our filters so first of all I only want to look at tennis courts and if you look at the result here um it means that in the name of the facility we want to see tennis and so we can filter on uh string patterns on text patterns by using the like uh command so I can take facilities name and get it like tennis and the percentage signs are um wild cards which means that tennis could be preceded and followed by zero or more characters we don't care we just want to get those strings that have tennis in them but that's not enough as a condition we also need the booking to have happened on a specific date so I will put an end here so end is the operator we need because we're providing two logical conditions and they both need to be true so end is what we need and then I can take the start time from the booking table and um say that it should be equal to the date provided in the instructions because I want the booking to have happened in this particular date however this will not work so I can actually complete the query and show you that it will not work because here we get zero results so can you figure out why this um command here did not work now I'm going to write a few comments here and uh this is how you write them and they are just pieces of text they're not actually executed as code and I'll just use them to show you what's going on so the value for start time looks like this so this is a time stamp and is showing a specific point in time but the date that we are providing for the comparison looks like this so as you can see we have something that is uh less granular because we we're not showing all of this data about hour minute and uh and second now in order to compare these two things which are different SQL automatically fills in uh this date over here and what it does is that since there's nothing there it puts zeros in there and now that it has made this um extension it's going to actually compare them so when you look at this uh comparison over here between these two elements this comparison is false false because the hour is different now when we write this uh filter command over here SQL is looking at every single start time and then comparing it with this value over here which is the very first moment of that date but there's no start time that is exactly like this one so basically this is always false and thus we get uh zero rows in our result so what is the solution to this before when we take a start time from the data before comparing it we can put it into the date function and if I take my example here if I put it into the date function it's going to drop that extra information about hour minute and second and it's only going to keep uh the information about the date so once I do this if I uh if I pass it to the date function before comparing it to my reference date now this one is going to become the result which is this one and then I'm going to compare it with my reference date and then this is going to be true so all this to say that before we compare start time with our reference date we need to reduce its granularity and we need to reduce it to its uh to its date so if I run the query now I will actually get my start times and after this I just need to add the name and finally I need to order by time so I need to order bu um book start time there is still a small error here so sometimes you just have to look at what you get and what's expected and if you notice here we are returning data about the table tennis facility but we're actually just interested in tennis court so what are we missing here the string filter is not precise enough and we need to change this into tennis court and now we get our results produce a list of all members who have recommended another member now if we look at the members table we have all these data about each member and then we know if they were recommended by another member and recommended by is the ID of the member who has recommended them and because of this the members table like we said has a relation to itself because one of its column references its ID column so let's see how to put this in practice so to be clear I simply want a list of members who appear to have recommended another member so if I wanted just the IDS of these people my task would be much simpler right I would go to the members table and then I could select recommended by and then I will put a distinct in here to avoid repetitions and what I would get here is the IDS of all members who have recommended another member however the problem does not want this because the problem wants the first name and Sur name of these uh of these people so in order to get the first name and the name of these people I need to plug this ID back into the members table and get the the data there so for example if I went to the members table and I selected everything where the M ID is 11 then I would get the data for this first member but now I need to do this for all members so what I will have to do is to take the members table and join it to itself and the first time I take the table I'm looking at the members quite simply but the second time I take the members table I'm looking at data about the recommenders of the members so I will call this second instance re so both of these they come from the same table but they're now two separate instances and what is the logic to join these two tables the members table has this recommended by field and we take the ID from recommended by and we plug it back into the table into M ID to get the data about the recommenders and now we can go into the recommenders which we got by plugging that ID and get their first name and surname I want to avoid repetition because a member may have been recommending multiple members but I want to avoid repetition so I will put a distinct to make sure that I don't get any uh repeated rows at the end and then finally I can order by surname and first name and I get my result so I encourage you to play with this and experiment a bit until it is clear and in my U mental models course I go into depth into the self joint and uh do a visualization in Google Sheets that also makes it uh much clearer produce a list of all members along with a recommender now if we look at the members table we have a few column and then we have the recommended by column and sometimes we have the ID of another member who recommended this member um it can be repeated because the same member may have recommended multiple people and then sometimes this is empty and when this is empty we have a null in here which is the value that SQL uses to represent absence of data now let us count the rows in members so you might know that to count the rows of a table we can do a simple aggregation which is Count star and we get 31 and let's just make a note of this that members has 31 rows because in the result we want a list of all members so we must ensure that we return 31 rows in our results now I'm going to delete this select and as before I want want to go for each member and check the ID they have here in recommended bu and then plug this back into the table into M ID so I can get the data about the recommender as well and I can do that with a self jooin so let me take members and join on itself and the first time I will call it Ms and the second time I will call it Rex and the logic for joining is that in Ms recommended by is the same um is connected to to Rex M ID so this is taking the ID in the recommended by field and plugging it back into me ID to get the data about the recommender now what do I want from this I want to get the first name of the member and the last name uh surname and then the first name and last name of the recommender uh surname great so it's starting to look like the right result but how many rows do we think we have here and in order to count the rows I can do select count star from and then if I simply take this table uh if I simply take this query and en close it in Brackets now this becomes a a subquery so I can ah the subquery must have an alias so I can give it an alias like this and I get 22 so how this works is that first SQL will compute the content of the subquery which is the table that we saw before and then it will uh we need to assign it an alas otherwise it doesn't work this changes a bit by System but in post you need to do this so we we call it simply T1 and then we run a count star on this table to get the number of rows and we see that the result has 22 rows and this is an issue because we saw before that members has 31 rows and that we want to return all of the members therefore our result should also have 31 rows so can you figure figure out why are we missing some rows here now the issue here is that we are using an inner join so remember when we don't specify the type of joint it's an inner joint and what does an inner joint do it keeps only rows that have matches so if you we saw before that in members sometimes this field is empty it has a null value because U you know maybe the member wasn't recommended by anyone maybe they just apply it themselves and what happens when we use this in an inner joint and it has a null value the row for that me member will be dropped because obviously it cannot have a match with M ID because null cannot match with with anything with any ID and so that row is dropped and we lose it however that's not what we want to do therefore instead of an inner join we need to use a left join here the the left join will look at the left table so the table that is left of the join command and it will make sure to keep all the rows in that table even the rows that don't have a match in the rows that don't have a match it will not drop them it will just put a null in the values that correspond to the right table and if I run the count again uh I will get 31 so now I have I'm keeping all the members and I have the number of rows that I need so now I can get rid of all of these because I know I have the right amount of of rows and I can um get my selection over here and it would actually help if we could make this a bit uh more ordered and a assign aliases to the columns so I will follow the expected results here and call this m first name me surname W first name Rec surname now we have the proper labels and you can see here that we always have the name of the member but some member weren't recommended by anyone and therefore for the first and last name of the recommender we simply have null values and this is what the left join does the last step here is to order and we want to order by the last name and the first name of each member and we finally get our result so typically you use inner joints which is the default joint because you're only interested in the rows from both tables that actually have a match but sometimes you want to keep all the data about one table and then you would put that table on the left side and do a left join as we did in this case produce a list of all members who have used a tennis court now now for this problem we need to combine data from all our tables because we need to get look at the members and we need to look at their bookings and we need to check what's the name of the facility for their bookings so as always let us start with the front part and let us start by joining together all of these tables CD facilities on facility ID and then I want to also join on members and that is my join so we can always join two or more tables in this case we're joining three tables and how this works is that the first join creates a new table and then this new table is joined with the with the next one over here and this is how multiple joints are managed now I have my table which is the join of all of these tables and um we we're only interested in members who have used the tennis court if a member has made no bookings um we are we don't we're not interested in that member and so it's okay to have a join and not a left join and we're for each booking we want to see the name of the facility and if there was a booking who didn't have the name of the facility we wouldn't be interested in that booking anyway and so um this joint here also can be an inner join and doesn't need to be a left join this is how you can think about whether to have a join or left join now we want the booking to include a tennis court so we can filter on this table and we will look at the name of the facility and uh make sure that it has tennis court in it with the like operator and now that we have filtered we can get the first name and the surname of the member and we can get the facility name so here we have a starting result now in the expected result we have merged the first name and the surname into a single string and um in SQL you can do this with a concatenation operator which is basically taking two strings and putting them together into one string now if I do this here I will get um something like this and so this looks a bit weird and what I want to do here is to add an empty space in between and again concatenate it and now the names will look uh will look fine I also want to label this as member and this other column as facility to match the expected results next I need to ensure that there is no duplicate data so at the end of it all I will want to have distinct in order to remove duplicate rows and then I want to order the final result by member name and facility name so order by member and then facility and this will work because the order bu coming second to last coming at the end of our logical order of SQL operations over here the order by is aware of the alas is aware of the label that I have that I have put on the columns and here I get the results that I needed not a lot happening here to be honest it's just that we're joining three tables instead of two but it still works um just like uh any other join and then concatenating the strings filtering according to the facility name and then removing duplicate rows and finally ordering produce a list of costly bookings so we want to see all bookings that occurred in this particular particular day and we want to see how much they cost the member and we want to keep the bookings that cost more than $30 so clearly in this case we also need the information from from all tables because if you look at the expected results we want the name of the member which is in the members table the name of the facility which is in the facilities table and the cost for which we will need the booking table so we need to start with a join of these three tables and since we did it already in the last exercise I have copied the code for that uh join so if you want more detail on this go and check the last exercise as well as I have copied the code to get the first name of the member by concatenating strings and the name of the of the facility now we need to calculate the cost of each booking so how does it work looking at our data so we have here a list of bookings and um a booking is defined as a number of slots and a slot is a one uh is a 30 minute usage of that facility and then we also have mid which tells us whether the member is a guest or not I mean whether the person is a guest or a member because if mid is zero then that person is a guest otherwise that person is a member and then I also know the facility that this person booked and if I go and look at the facility it has uh two different prices right one price uh is for members the other price is for guests and the price applies to the slots so we have all of the ingredients that we need for the cost in our join right and to convince ourselves of that let us actually select the here so in Booking I can see facility ID member ID and then slots and then in facility I can see the member cost the guest cost and I guess that's all I need really to calculate the cost and as you can see after the join I'm in a really good position because for each row I do have all of these values placed on each row so now I just have to figure out how to combine all of these values in order to get the cost now the way that I can get the cost is that I can look at the number of slots and then I need to multiply this by the right cost which is either member cost or guest cost and how do I know which of these to pick if it depends on the M ID if the M id M ID is zero then I will use the guess cost otherwise I will use the member cost so let me go back to my code here and after this I can say I want to take the slots and I want to multiply it by either member cost or guest cost now how can I put some logic in here that will choose uh either member cost or guest cost based on the ID of this person what can I use in order to make this Choice whenever I have such a choice to make I need to use a case statement so I can start with a case statement here and I will already write the end of it so that I don't forget it and then in the case statement M what do I need to check for I need to check that the member ID is zero in that case I will use the guest cost and in all other cases I will use the member cost so I'm taking slots and then I'm using this case when to decide by which column I'm going to multiply it and this is actually my cost now let's take a look at this and so I get this error that the column reference M ID is ambiguous so can you figure out why I got this error what's happening is that I have joined U multiple tables and the M ID column appears twice now in my join and so I cannot refer to it just by name because SQL doesn't know which column I want so I have to to reference the parent of the column every time I use it so here I will say that it comes from the booking table and now I get my result so if I see here then um I can see that I have successfully calculated my cost and let's look at the first row uh first it's um the me ID is not zero therefore it's a member and here the member cost is zero meaning that this facility is free for members so regardless of the slots the cost will be zero and let's look at one who is a guest so this one uh is clearly a a guest and they have uh taken one slot and the member cost is zero but uh so it's free for members but it costs five per slot for guests so the total cost is five So based on this sanity check the cost looks good now I need to actually filter my table because we have um we should consider only bookings that occurred in a certain day so after creating my new table uh and joining I can write aware filter to drop the rows that I don't need and I can say this is the the time column that I'm interested in the start time needs to be equal to this date over here and we have seen before that this will not work because start time is a Tim stamp it also shows hour um minute and seconds whereas here is just a date so this comparison will fail and so before I do the comparison I need to take this and reduce it to a date so that I'm comparing Apples to Apples on the time check that that didn't break anything now we should have significantly fewer rows so now what we need to do is to only keep rows that have a cost which is higher than 30 so can I go here and say end cost bigger than 30 no I cannot do it column cost does not exist right typical mistake but if you look at the logical order of SQL operations first you have the sourcing of the data then you have the wear filter and then all of the logic um by which we calculate the cost happens here and the label cost happens here as well so we cannot um filter on this column on the column cost because the we component has no idea about the uh column cost so this will now work but what we can do is to take all of the logic we've done until now and wrap it in round brackets and then introduce a Common Table expression and call this T1 so I will say with T1 as and then I can from T1 and now I can use my filter right so cost bigger than 30 I can select star from this table and I'm starting to get somewhere because the cost has been successfully filtered now I have a lot of columns that I don't want in my final result that I used to help me reason about the cost so I want to keep member and I want to keep the facility but I don't want to keep any of these great now as a final step I need to order by cost descending and there's actually a issue that I have because I copy pasted code from the previous exercise I kept a distinct and you have to be very careful with this especially if you copy paste code anyway for learning it would be best to write it always from scratch but the distinct will remove uh rows that are duplicate and can actually cause an issue now I remove the distinct and I get the um solution that I want and if you look here we have if you look at the last two rows you can see that they're absolutely identical and so the distinct would remove them but there are two uh bookings that happen to be just the same uh in our data and we want to keep them we don't want to delete them so having distinct was a mistake in this case to summarize what we did here first we joined all the tables so we could have all the columns uh that we needed side by side and then we filtered on on the date pretty straightforward and then we took the first name and surname and um concatenated them together as well as the facility name and then we computed the cost and to compute the cost we got the number of slots and we used used a case when to multiply this by either the guest cost or the member cost according to the member's ID and at the end we wrapped everything in a Common Table expression so that we could filter on this newly computed value of cost and keep only those bookings that had a cost higher than 30 now I am aware that the question said not to use any subqueries technically I didn't because this is a common table expression but if you look at the author solution it is slightly different than ours so here they did basically the same thing that we did to compute the the cost except that in the case when they inserted the whole uh expression which is fine works just the same the difference is that um in this case they added a lot of logic in the we filter so that they could use a we filter in the first query so clearly they didn't use any columns that were added at the stage of the select they didn't use cost for example because like we said that wouldn't be possible so what they did is that they added the date filter over here and then in this case they added a um logical expression and in this logical expression either one of these two needed to be true for us to keep the row either the M ID is zero meaning that it's a it's a guest and so the calculation based on Guess cost ends up being bigger than 30 or the M ID is not zero which means it's a member and then this calculation based on the member cost ends up being bigger than 30 so this works I personally think that there's quite some repetition of the cost calculation both by putting it in the we filter and by uh putting it inside the case when and so I think that uh the solution we have here is a bit cleaner because we're only calculating cost once uh in this case and then we're simply referencing it thanks to the Common Table expression so if you look at the mental models course you will see that I warmly recommend not repeating logic in the code and using Common Table Expressions as often as possible because I think that they made the code uh clearer and um simpler to to understand produce a list of all members and the recommender without any joins now we have already Sol solved this problem and we have solved it with a self join as you remember we take the members table and join it on itself so that we can get this uh recommend by ID and plug it into members ID and then see the names of both the member and the recommender side by side but here we are challenged to do it without a join so let us go to the members table and let us select the first name and the surname now we actually want want to concatenate these two into a single string and call this member now how can we get data about the recommender without a selfjoin typically when you have to combine data you always have a choice between a join in a subquery right so what we we can do is to have a subquery here which looks at the recommended by ID from this table and um goes back to the members table and gets the the data that we need so let's see how that would look let us give an alias to this table and call it Ms and now we need to go back to this table inside the subquery and we can call it Rex and we want to select again the first name and surname like we're doing here and how are we able to identify the right row inside this subquery we can use aware filter and we want the Rex M ID to be equal to the Mims recommended by value and once we get this value we can call this recommender and now we want to avoid duplicates so after our outer select we can say distinct which will remove any duplicates from the result and then we want to sort I guess by member and recommender and here we get our result so replacing a join with a subquery so we go row by Row in members and then we take the recommended by ID and then we query the members table again inside the subquery and we use the wear filter to plug in that recommended by and find the row where the mem ID is equal to it and then getting first name and surname we get the data about the recommender and uh and that's how we can do it in the mental models course we discuss the subqueries and um and this particular case we talk about a correlated subquery why is this a correlated subquery because you can imagine that the the query that is in here it runs again for every row because for every row row I have a different value recommended by and I need and I need to plug this value into the members table to get the data about the recommender so this is a correlated subquery because it runs uh every time and it is different for every row of the members table produce a list of costly bookings using a subquery so this is the exact exercise that we did before and as you will remember uh we actually ignored it instructions a bit and we did use not a subquery but a Common Table expression and by reference this is the code that we used and this code works with that exercise as well and we get the result so you can go back to that exercise to see the logic behind this code and why this works and if we look at the author's uh solution they are actually using a subquery instead of a common table expression so they have an outer quer query which is Select member facility cost from and then instead of the from instead of telling the name of the table they have all of this logic here in this subquery which they call bookings and finally they they add a filter and order now this is technically correct it works but I'm not a fan of uh of writing queries like this I prefer writing them like this as a common table expression and I explain this in detail in my mental models course the reason I prefer this is because U it doesn't break queries apart so in my case this is one query and this is another query and it's pretty easy and simple to read however in this case you will start reading this query and then it is broken uh in in two by another query and when people do this sometimes they go even further and here when you have the from instead of a table you have yet another subquery it gets really complicated um so because of these uh two approaches are equivalent I definitely recommend going for a Common Table expression every time and avoiding subqueries unless they are really Compact and you can fit them in one row let us now get started with aggregation exercises and the first problem count the number of facilities so I can go to the facilities table and then when I want to count the number of rows in a table and here every row is a facility I can use the countar aggregation and we get the count of facilities so what we see here is a global aggregation and when you run an aggregation without having done any grouping it runs on the whole table therefore it will take all the rows of this table no matter how many compress them into one number which is determined by the aggregation function in this case we have a count and it returns a total of nine rows so in our map aggregation happens right here so we Source the table we filtered it if needed and then we might do a grouping which we didn't do in this case but whether we do it or not aggregations happen here and if grouping didn't happen the aggregation is at the level of the whole table count the number of expensive facilities this is similar to the previous exercise we can go to the facilities table but here we can add a filtering because we're only interested in facilities that have guest cost greater than or equal to 10 and now once again I can get my aggregation count star to count the number of rows of this resulting table looking again at our map why does this work because with the from We're sourcing the table and immediately after the wear runs and it drops unneeded rows and then we can decide whether to group by or not and in our case in this case we're not doing it um but then the aggregations Run so by the time the aggregations run I've already dropped the rows in the wear and this is why in this case after dropping some rows the aggregation only sees six rows which is what we want count the number of recommendations each member makes so in the members table we have a field which is recommended by and here is the ID of the member who recommended the member that that this row is about so now we want to get all these uh recommended by values and count how many times they appear so I can go to my members table and what I need to do here is to group by recommended by so what this will do is that it will take all the unique values of this column recommended by and then you will allow me to do an aggregation on all of the rows in which those values occur so now I can go here to select and call this column again and if I run this query I get all the unique values of recommended buy without any repetitions and now I can run an aggregation like count star what this will do is that for recomend recomended by value 11 it will run this aggregation on all the rows in which recommended by is 11 and the aggregation in this case is Count star which means that it will return the number of rows in which 11 appears which in the result happens to be one and so on for all the values what I also want to do is to order by recommended buy to match the expected results now what we get here is almost correct we see all the unique values of this column and we see the number of times that it appears in our data but there's one discrepancy which is this last row over here so in this last row you cannot see anything which means that it's a null value so it's a value that represents absence of data and why does this occur if you look at the original recommended by column there is a bunch of null values in this column because there's a bunch of member that have null in recommended by so maybe we don't know who recommended them or maybe they weren't recommended they just applied independently when you group bu you take all the unique values of the recommended by column and that includes the null value the null value defines a group of its own and the count works as expected because we can see that there are nine members for whom we don't have the recommended by value but the solution does not want to see this because we only want to see the number of recommendations each member has made so we actually need to drop this row therefore how how can I drop this row well it's as simple as going to uh after the from and putting a simple filter and saying recommended by is not not null and this will drop all of the rows in which in which that value is null therefore we won't appear in the grouping and now our results are correct remember when you're checking whether a value is null or not you need to use the is null or is not null you cannot actually do equal or um not equal because um null is not an act ual value it's just a notation for the absence of a value so you cannot say that something is equal or not equal to null you have to say that it is not null let's list the total slots booked per facility now first question where is the information that I need the number of slots booked in the is in the CD bookings and there I also have the facility ID so I can work with that table and now how can I get the total slots for each facility I can Group by facility ID and then I can select that facility ID and within each unique facility ID what type of uh aggregation might I want to do in every booking we have a certain number of slots right and so we want to find all the bookings for a certain facility ID and then sum all the slots that are being booked so I can write sum of slots over here and then I want to name this column total slots uh looking at the expected results but this will actually not work because um it's it's two two separate words so I actually need to use quotes for this and remember I have to use double quotes because it's a column name so it's always double quotes for the column name and single quotes for pieces of text and finally I need to order by facility ID and I get the results so for facility ID zero we looked at all the rows where facility ID was zero and we squished all of this to a single value which is the unique facility ID and then we looked at all the slots that were occurring in these rows and then we compress them we squished them to a single value as well using the sum aggregation so summing them all up and then we get the slum the sum of the total slots list the total slots booked per facility in a given month so this is similar to the previous problem except that we are now isolating a specific time period And so let's us think about how we can um select bookings that happened in the month of September 2012 now we can go to the bookings table and select the start time column and to help our exercise I will order by start time uh descending and I will limit our results to 20 and you can see here that start time is a time stamp call and it goes down to the second because we have year month day hour minutes second so how can we check whether any of these dates is corresponds to September 2012 we could add a logical check here we could say that start time needs to be greater than or equal to 2012 September 1st and it needs to be strictly smaller than 2012 October 1st and this will actually work as an alternative there is a nice function that we could use which is the following date trunk month start time let's see what that looks like so what do you think this function does like the name suggests it truncates the date to a specific U granularity that we choose here and so all of the months are reduced to the very first moment of the month in which they occur so it is sort of cutting that date and removing some information and reducing the granularity I could of course uh have other values here such as day and then every um time stem here would be reduced to its day but I actually want to use month and now that I have this I can set an equality and I can say that I want this to be equal to September 2012 and this will actually work and I also think it's nicer than the range that we showed before now I've taken the code for the previous exercise and copied it here because it's actually pretty similar except that now after we get bookings we need to insert a filter to isolate our time range and actually we can use this logical condition directly I'll delete all the rest and now what I need to do is to change the ordering and I actually need to order by the the total slots here and I get my result to summarize I get the booking table and then I uh take the start time time stamp and I truncate it because I'm only interested in the month of that of that time and then I make sure that the month is the one I actually need and then I'm grouping by facility ID and then I'm getting the facility ID and within each of those groups I'm summing all the slots and finally I'm ordering by this uh column list the total slots booked per facility per month in the year 2012 so again our data is in bookings and now we want to see how we how can we isolate the time period of the year 2012 for this table now once again I am looking at the start time column from bookings uh to see how we can extract the the year so in the previous exercise we we saw the date trunk function and we could apply it here as well so we could say date trunk start time um Year from start time right because we want to see it at the Year resolution and then we will get something like this and then we could check that this is equal to 2012 0101 and this would actually work but there's actually a better way to do it what we could do here is that we could say extract Year from start time and when we look at here we got a integer that actually represents the year and it will be easy now to just say equal to 2012 and make that test so if we look at what happened here extract is a different function than date time because extract is getting the year and outputting it as an integer whereas date time is still outputting a time stamp or a date just with lower granularity so you have to use one or another according to your needs now to proceed with our query we can get CD bookings and add a filter here and insert this expression in the filter and we want the year to be 2012 so this will take care of isolating our desired time period next we want to check the total slots within groups defined by facility fac ID and month so we want a total for each facility for each month as you can see here in the respected results such that we can say that for facility ID zero in the month of July in the year 2012 we uh booked to 170 slots so let's see how we can do that this basically means that we have to group by multiple values right and facility ID is easy we have it however we do not have the month so how can we extract the month from the start time over here well we can use the extract function right which is which we just saw so if we write it like this and we put month here um this function will look at the month and then we'll output the month as an actual integer and um the thing is that I can Group by uh the names of columns but I can also Group by Transformations on columns it works just as well SQL will compute uh this expression over here and then it will get the value and then it will Group by that value now when it comes to getting the columns what I usually do is that when I group by I want to see the The Columns in which I grouped so I just copy what I had here and I add it to my query and then what aggregation do I want to do within the groups defined by these two columns I have seen it in the previous exercise I want to sum over the the slots and get the total slots I also want to take this column over here and rename it as month and now I have to order by ID and month and we get the data that we needed so what did we learn with this exercise we learned to use the extract function to get a number out of a date and we use that we have used uh grouping by multiple columns which simply defines a group as the combination of the unique values of two or more columns that's what multiple grouping does we have also seen that not only you can Group by providing a column name but you can also Group by a logical operation and you should then reference that same operation in the select statement so that you can get the uh value that was obtained find the count of members who have made at least one booking so where is the data that we need it's in the bookings table and for every booking we have the ID of the member who has made the booking so I can select this column and clearly I can run a count on this column and the count will return the number of nonnull values however this count as you can see is quite inflated What's Happening Here is that uh a single member can make any number of bookings and now we're basically counting all the bookings in here but if I put distinct in here then I'm only going to count the unique values of mid in my booking table and this give me gives me the total number of members who have made at least one booking so count will get you the count of nonnull values and count distinct will get you the count of unique nonnull values list the facilities with more than 1,000 slots booked so what do we need to do here we need to look at each facility and how many slots they each booked so where is the data for this as you can see again the data is in the bookings table now I don't need to do any filter so I don't need the wear statement but I need to count the total slots within each facility so I need a group pi and I can Group by the facility ID and once I do that I can select the facility ID and to get the total slots I can simply do sum of slots and I can call this total slots it's double quotes for a column name now I need to add the filter I want to keep those that have some of slots bigger than 1,000 and I cannot do it in a where statement right so if I were to write this in a where statement I would get that aggregate functions are not allowed in wear and if I look at my map uh we have been through this again the wear runs first right after we Source the data whereas aggregations happens happen later so the wear cannot be aware of any aggregations that I've done for this purpose we actually have the having component so the having component works just like wear it's a filter it drops rows based on logical conditions the difference is that having runs after the aggregations and it works on the aggregations so I get the data do my first filtering then do the grouping compute an aggregation and then I can filter it again based on the result of the aggregation so I can now now go to my query and take this and put having instead of where and place it after the group pi and we get our result and all we need to do is to order bu facility ID and we get our result find the total revenue of each facility so we want a list of facilities by name along with their total revenue first question as always where is my data so if I want facility's name it's in the facilities table but to calculate the revenue I need to know about the bookings so I'll actually need to join on both of these tables so I will write from CD bookings book join CD facilities fact on facility ID next I will want the total revenue of the facilities but I don't even have the revenue yet so my first priority should be to compute the revenue let us first select the facility name and here I will now need to add the revenue so to do that I will need to have something like cost times slots and that determines the revenue of each booking however I don't have a single value for cost I have two values member cost and guest cost and as you remember from previous exercises I need to choose every time which of them to apply and the way that I can choose is by looking at the member ID and if it's zero then I need to use the guest cost otherwise I need to use the member cost so what can we use now in order to choose between these two variants for each booking we can use the case statement for this so I will say case and then immediately close it with end and I'll say when uh book M ID equals zero then Fox guest cost I always need to reference the parent Table after a join to avoid confusion else fax member cost so this will allow me to get the C cost dynamically it allows me to choose between two columns and I can multiply this by slots and get the revenue now if I run this I get this result which is the name of the facility and the revenue but I need to ask myself at what level am I working here in other words what does each row represent well I haven't grouped yet so each row here represents a single booking having joined bookings and facilities and not having grouped anything we are still at the level of this table where every row represent a single booking so to find the total revenue for each facility I now need to do an aggregation I need to group by facility name and then sum all all the revenue I can actually do this within the same query by saying Group by facility name and if I run this I will now get an error can you figure out why I'm getting this error now so I have grouped by facility name and then I'm selecting by facility name and that works well because now this column has been squished has been compressed to show only the unique names for each facility however I am then adding another column which is revenue which I have not compressed in any way therefore this column has a different number of rows than than this column and the general rule of grouping is that after I group by one or more columns I can select by The Columns that are in the grouping and aggregations right so nothing else is allowed so fax name is good because it's in the grouping revenue is not good because it's not in the grouping and it's not an aggregation and to solve this I can simply turn it into an aggregation by doing sum over here and when I run this this actually works and now all I need to do is to sort by Revenue so if I say order by Revenue I will get the result that I need so there's a few things going on here but I can understand it by looking at my map now what I'm doing is that I'm first sourcing the data and I'm actually joining two tables in order to create a new table where my data is then I'm grouping by a c a column which is the facility name so this compresses the column to all the unique facility name and next I run the aggregation right so the aggregation can be a sum over an existing column but as we saw in the mental models course the aggregation can also be a sum over a calculation I can actually run logic in there it's very flexible so if I had a revenue column here I would just say sum Revenue as revenue and it would be simpler but I need to do some to put some logic in there and uh this logic involves uh choosing whether to get guest cost or member cost but I'm perfectly able to put that logic inside the sum and so SQL will first evaluate this Logic for each row and then um it will sum up all the results and it will give me Revenue finally after Computing that aggregation I uh select the columns that I need and then I do an order buy at the end find facilities with a total revenue of less than 1,000 so the the question is pretty clear but wait a second we calculate ated the total revenue by facility in the previous exercise so we can probably just adapt that code here's the code from the previous exercise so check that out if you want to know how I wrote this and if I run this code I do indeed get the total revenue for for each facility and now I just need to keep those with a revenue less than 1,000 so how can I do that it's a filter right I need to filter on this Revenue column um I cannot use a wear filter because this uh revenue is an aggregation and it was computed after the group buy after the wear so the wear wouldn't be aware of that uh column but as we have seen there is a keyword there is a statement called having which does the same job as where it filters based on logical conditions however it works on aggregations so I could say having Revenue smaller than 1,000 unfortunately this doesn't work can you figure out why this doesn't work in our query we do a grouping and then we compute an aggregation and then we give it a label and then we try to run a having filter on this label if you look now at our map for The Logical order of SQL operations this is where the group by happens this is where we compute our aggregation and this is where having runs and now having is trying to use the Alias that comes at this step but according to our rules having does not know of the Alias that's assigned at this step because it hasn't happened yet now as the discussion for this exercise says there are in fact database systems that try to make your life easier by allowing you to use labels in having but that's not the case with postgress so we need a slightly different solution here note that if I repeated all of my logic in here instead of using the label it would work so if I do this I will get my result I just need to order by Revenue and you see that I get the correct result why does it work when I put the whole logic in there instead of using the label once again the logic happens here and so the having is aware of this logic having happened but the having is just not aware of the Alias however I do not recommend repeating logic like this in your queries because it increases the chances of errors and it also makes them less elegant less readable so the simpler solution we can do here is to take this original query and put it in round brackets and then create a virtual table using a Common Table expression here and call this all of these T1 and then we can treat T1 like any other table so I can say from T1 select everything where revenue is smaller than 1,000 and then order by Revenue remove all this and we get the correct answer to summarize you can use having to filter on the result of aggregation ations unfortunately in postest you cannot use the labels that you assign to aggregations in having so if it's a really small aggregation like if it's select some revenue and then all of the rest then it's fine to say sum Revenue smaller than 1,000 there's a small repetition but it's not an issue however if your aggregation is more complex as in this case you don't really want to repeat it and then your forced to add an extra step to your query which you can do with a common table expression output the facility ID that has the highest number of slots booked so first of all we need to get the number of slots booked by facility and we've actually done it before but let's do it again where is our data the data is in the booking table and uh we don't need to filter this table but we need we do need to group by the facility ID and then once we do this we can select the facility ID this will isolate all the unique values of this column and within each unique value we can sum the number of slots and call this total slots and if we do this we get the total slots for each facility now to get the top one the quickest solution really would be to order by total slots and then limit the result to one however this would give me the one with the smallest number of slots because order is ascending by default so I need to turn this into descending and here I would get my solution but given that this is a simple solution and it solved our exercise can you imagine a situation in which this query would not achieve what we wanted it to let us say that there were multiple facilities that had the top number of total slots so the top number of slots in our data set is 1404 that's all good but let's say that there were two facilities that had this uh this top number and we wanted to see both of them for our business purposes what would happen here is that limit one so the everything else would work correctly and the ordering would work correctly but inevitably in the ordering one of them would get the first spot and the other would get the second spot and limit one is always cutting the output to a single row therefore in this query we would only ever see one facility ID even if there were more that had the same number of top slots so how can we solve this clearly in instead of combining order by and limit we need to figure out a filter we need to filter our table such that only the facilities with the top number of slots are returned but we cannot really get the maximum of some slots in this query because if I tried to do having some slots equals maximum of some slots I would be told that aggregate function calls cannot be nested and if I go back to my map I can see that having can only run after all the aggregations have completed but what we're trying to do here is to add a new aggregation inside having and that basically doesn't work so the simplest solution here is to just wrap all of this into a Common Table expression and then get this uh table that we've just defined and then select star where the total slots is equal to the maximum number of slots which we know to be 1404 however we cannot hardcode the maximum number of slots because for one we might not know what it is and for and second it uh it will change with time so this won't work when the data changes so what's the alternative to hardcoding this we actually need some logic here to get the maximum value and we can put that logic inside the subquery and the subquery will go back to my table T1 and you will actually find the maximum of total slots from T1 so first this query will run it will get the maximum and then the filter will check for that maximum and then I will get uh the required result and this won't break if there are many facilities that share the same top spot because we're using a filter all of them will be returned so this is a perfectly good solution for your information you can also solve this with a window function and um which is a sort of row level aggregation that doesn't change the structure of the data we've seen it in detail in the mental models course so what I can do here is to use a window function to get the maximum value over the sum of slots and then I can I will say over to make it clear that this is a window function but I won't put anything in the window definition because I I just want to look at my whole data set here and I can label this Max slots and if I look at the data here you can see that I will get the maximum for every row and then to get the correct result I can add a simple filter here saying that total slots should be equal to Max slots and I will only want to return facility ID and total slots so this also solves the problem what's interesting to note here for the sake of understanding window functions more deeply is that the aggregation function for this uh window Clause works over an aggregation as well so here we sum the total slots over each facility and then the window function gets the maximum of all of those uh value and this is quite a powerful feature um and if I look at my map over here I can see that it makes perfect sense because here is where we Group by facility ID and here is where we compute the aggregation and then the window comes later so the window is aware of the aggregation and the window can work on on that so A few different solutions here and overall um a really interesting exercise list the total slots booked per facility per month part two so this is a bit of a complex query but the easiest way to get it is to look at the expected results so what we see here is a facility ID and then within each month of the year 2012 we get the total number of slots and um at the end of it we have a null value here and for facility zero and what we get is the sum of all slots booked in 2012 and then the same pattern repeats repeats with every facility we have the total within each month and then finally we have the total for that facility in the year here so there's two level of aggregations here and then if I go at the end there's a third level of aggregation which is the total for all facilities within that year so there are three levels of aggregation here by increasing granularity it's total over the year then total by facility over the year and then finally total by Facility by month within that year so this is a bit breaking the mold of what SQL usually does in the sense that SQL is not designed to return a single result with multiple levels of aggregation so we will need to be a bit creative around that but let us start now with the lowest level of granularity let's get this uh this part right facility ID and month and and then we'll build on top of that so the table that I need is in the bookings table and first question do I need to filter this table yes because I'm only interested in the year 2012 so we have seen that we can use the extract function to get the year out of a Tim stamp which would be start time and we can use this function in a wear filter and what this function will do is that it will go to that time stamp and then we will get an integer out of it it will get a number and then we can check that this is uh the year that we're interested in and let's do a quick sanity check to make sure this worked so I will get some bookings here and they will all be in the year 2012 next I need to Define my grouping right so I will need to group by facility ID but then I will also need to group by month however I don't actually have a column named uh month in this table so I need to calculate it I can calculate it once again with the extract function so I can say extract extract month from start time and once again this will go to the start time and sped out a integer which for this first row would be seven and uh as you know in the group bu I can select a column but I can also select an operation over a column which works just as well now after grouping I cannot do select star anymore but I want to see The Columns that I have grouped by and so let us do a quick sanity check on that it looks pretty good I get the facility ID and the month and I can actually label this month and next I simply need to take the sum over the slots within each facility and within each month and when I look at this I have my first level of granularity and you can see that the first row corresponds to the expected result now I need to add the next level of granularity which is the total within each facility so can you think of how can I add that next level of granularity to my results the key Insight is to look at this uh expected results table and to see it as multiple tables stacked on top of each other one table is the one that we have here and this is uh total by facility month a second table that we will need is the total by facility and then the third table that we will need is the overall total which you could see here at the bottom and how can we stack multiple tables on top of each each other with a union statement right Union will stack all the rows from my tables on top of each other so now let us compute the table which has the total by facility and I will actually copy paste what I have here and and I just need to remove a level of grouping right so if I do this I I will not Group by month anymore and I will not Group by month anymore and once I do this I get an error Union query must have the same number of columns so do you understand this error here so I will write a bit to show you what's happening so how does it work when we Union two tables let's say the first table in our case is facility ID month and then slots and then the second table if you look here it's facility ID and then slots now when you Union these two tables SQL assumes that you have the same number of columns and that the ordering is also identical so here we are failing because the first table has three columns and the second table has only two and not only We are failing because there's a numbers mismatch but we are also mixing the values of month and Slots now this might work because they're both integers so SQL won't necessarily complain about this but it is logically wrong so what we need to do is to make sure that when we're unioning these two tables we have the same number of columns and the same ordering as well but how can we do this given that the second table does indeed have one column less it does have less information so what I can do is to put null over here so what happens if I do select null this will create a column of a of constant value which is a column of all NS and then the structure will become like this now when I Union first of all I'm going to have the same number of columns so I'm not going to see this uh this error again that we have here and second in u the facility ID is going to be mixed with the facility ID slots is going to be mixed with slots which is all good and then month is going to be mixed with null which is what we want because in some cases we will have the actual month and in some cases we won't have any anything so I have added uh null over here and I am unioning the tables and if I run the query I can see that I don't get any error anymore and this is what I want so I can tell that this row is coming from the second table because it has null in the value of month and so it's showing the total slots for facility um zero in every month whereas this row came from the upper table because it's showing the sum of slots for a facility within a certain month so this achieves the desired result next we want to compute the last level of granularity which is the total so once again I will select my query over here and and I don't even need to group by anymore right because it's the total number of slots over the whole year so I can simply say sum of slots as slots and remove the grouping next I can add the Union as well so that that I can keep stacking these tables and if I run this I get the same error as before so going back to our little uh text over here we are now adding a third table and this table only has slots and of course I cannot this doesn't work because there's a mismatch in the number of columns and so the solution here is to also add a null column here and a null column here and so I have the same number of columns and Slots gets combined with slots and everything else gets filled with null values and I can do it here making sure that the ordering is correct so I will select null null and then sum of slots and if I run this query I can see that the result works the final step is to add ordering sorted by ID and month so at the end of all of these unions I can say order by facility ID one and I finally get my result so this is now the combination of three different tables stacked on top of each other that show different levels of granularity and as you can see here in the schema we added null columns to uh two of these tables just to make sure that they have the same number of columns and that they can stack up correctly and now if we look again at the whole query we can see that there are actually three select statements in this query meaning three tables which are calculated and then finally stack with Union and all of them they do some pretty straightforward aggregation the first one um Aggregates by facility ad and month after extracting the month the second one simply Aggregates by facility ID and the third one gets the sum of slots over the whole data without any grouping and then we are adding the null uh constant columns here to make the the column count match and it's also worth it to see this in our map of the SQL operations so here um you can see that this order is actually repeating for every table so for each of our three tables we are getting our data and then we are running a filter to keep the year 2012 and then we do a grouping and compute an aggregation and select the columns that we need adding null columns when necessary and then it repeats all over right so for the second table again the same process for the third table the same process except that in the third table we don't Group by and then when all three tables are done the union r runs the union runs and stacks them all up together and now instead of three tables I only have one table and after the union has run now I can finally order my table and return the result list the total hours booked per named facility so we want to get the facility ID and the facility name and the total hours that they've been booked keeping keeping in mind that what we have here are number of slots for each booking and a slot represents 30 minutes of booking now to get my data I will need both the booking table and the facilities table because I need both the information on the bookings and the facility name so I will get the bookings table and the facilities table and join them together next I don't really need to filter on anything but I need to group by facility so I will Group by facility ID and then I also need to group by facility name otherwise I won't be able to use this in the select part and now I can select these two columns and to get the total hours I will need to get the sum of the slots so I can get the total number of slots within each facility and I will need to divide this by two right so let's see what that looks like now superficially this looks correct but there's actually a pitfall in here and to realize a pitfall I will take some slots as well before dividing it by two and you can see it already in the first row 9911 ided by 2 is not quite 455 so what is happening here the thing is that in postgress when you take an integer number such as some slots the sum of the slots is an integer number and you divide by another integer postgress assumes that you you are doing integer Division and since you are dividing two integers it returns uh an integer as well so that means that um that the solution is not exact if you are thinking in floating Point numbers and the solution for this is that at least one of the two numbers needs to be a Flo floating Point number and so we can turn two into 2.0 and if I run this I now get the correct result so it's important to be careful with integer division in postest it is a potential Pitfall now what I need to do is to reduce the number of zeros after the comma so I need some sort of rounding and for this I can use the round function which looks like this and this is a typical function in uh in SQL and how it works is that it takes two arguments the first argument is a column and actually this is the column right this whole operation and then the second argument is how many uh figures do you want to see after the zero after the comma sorry so now I can clean this up a bit label this as total hours and then I will need to order by facility ID and I get my result so nothing crazy here really we Source our data from a join which is this part over here and then we Group by two columns we select those columns and U then we sum over the slots divide making sure to not have integer division so we use one of the numbers becomes a floating Point number and we round the result of this column list each Member's First booking after September 1st 2012 so in order to get our data where does our data leave we need the data about the member and we also need data about their bookings so the data is actually in the members and bookings table so I will quickly join on these tables and we now have our data do we need a filter on our data yes because we only want to look after September 1st 2012 so we can say where start time is bigger than and it should be enough to just provide the date like this now in the result we need the members surname and first name and their memory ID and then we get to we need to see the first booking in our data meaning the earliest time so again we have an aggregation here so in order to implement this aggregation I need to group by all of these columns that I want to call so surname first name and member ID now that I have grouped by this columns I can select them so now I am I have grouped by each member and now I have all the dates for all their bookings after September 1st 2012 and now how can I look at all these dates and get the earliest date what type of aggregation do I need to use I can use the mean aggregation which will look at all of the dates and then compress them to a single date which is the smallest date and I can call this start time finally I need to order by member ID and I get the result that I needed so this is actually quite straightforward I get my data by joining two tables I make sure I only have the data that I need by filtering on the on the time period and then I group by all the information that I want to see for each member and then within each member I use mean to get the smallest date meaning the earliest date now I wanted to give you a bit of an insight into the subtleties of how SQL Compares timestamps and dates because the results here can be a bit surprising so I wrote three logical Expressions here for you and your job is to try to guess if either of these three Expressions will be true or false so take a look at them and try to answer that as you can see what we have here is a time stamp uh that indicates the 1st of September 800 whereas here we have uh simply the indication of the date the 1st of September and the values are the same in all three but my question is are they equal is this uh greater or is this smaller so what do you think I think the intuitive answer is to say that in the first case we have September 1st on one side September 1st on the other they are the same day so this ought to be true whereas here we have again the same day on both sides so this is not strictly bigger than the other one so this should be false and it is also not strictly smaller so this would be false as well now let's run the query and see what's actually happening right so what we see here is that we thought this would be true but it's actually false we thought this would be false but it's actually uh true and this one is indeed false so are you surprised by this result or is it what you expected if you are surprised can you figure out what's going on here now what is happening here is that you are running a comparison between two expressions which have a different level of granularity the one on the left is showing you day hour minute seconds and the one on the right is showing you the date only in other words the value on the left is a Tim stamp whereas the value on the right is a date so different levels of precision here now to make the comparison work SQL needs to convert one into the other it needs to do something that is known technically as implicit type coercion what does it mean type is the data type right so either time stamp or date type coercion is when you take a value and you convert it to a different type and it's implicit uh because we haven't ask for it and SQL has to do it on its own behind the scenes and so how does SQL choose which one to convert to the other the choice is based on let's keep the one with the highest precision and convert the other so we have the time stamp with the higher Precision on the left and we need to convert the date into the timestamp this is how SQL is going to handle this situation it's going to favor the one with the highest Precision now in order to convert a date to a time stamp what SQL will do is that it will add all zeros here so this will basically represent the very first second of the day of uh September 1st 2012 now we can verify which I just showed you I'm going to comment this line and I'm going to add another logical expression here which is taking the Tim stamp representing all zeros here and then setting it equal to the date right here so what do we we expect to happen now we have two different types there will be a type coercion and then SQL will take this value on the right and turn it into exactly this value on the left therefore after I check whether they're equal I should get true here turns out that this is true but I need to add another step which is to convert this to a Tim stamp and after I do this I get what I expected which is that this comparison here is true so what this notation does in postest is that it does the type coercion taking this date and forcing it into a time stamp and I'll be honest with you I don't understand exactly why I need to to do this here I thought that this would work simply by taking this part over here but u i I also need to somehow explicitly tell SQL that I want this to be a time stamp nonetheless this is the Insight that we needed here and it allows us to understand why this comparison is actually false because we are comparing a time stamp for the very first second of September 1st with a time stamp that is the first second of the eighth hour of September 1st and so it fails and we can also see why on on this line the left side is bigger than the right hand side and uh and this one did not actually fool us so we're good with that so long story short if you're just getting started you might not know that SQL does this uh implicit type coercion in the background and this dates comparison might leave you quite confused now I've cleaned the code up a bit and now the question is what do we need to do with the code in order to match our initial intuition so what do we need to do such that this line is true and the second line is false and this one is still false so we don't have to worry about it well since the implicit coercion turns the date into a time stamp we actually want to do the opposite we want to turn the time stamp into a date so it will be enough to do the type coion ourselves and transform this into dates like this and when I run this new query I get exactly what I expected so now I'm comparing at the level of precision or granularity that I wanted I'm only looking at the at the date so I hope this wasn't too confusing I hope it was a bit insightful and that you have a new appreciation for the complexities that can arise when you work with dates and time stamps in SQL produce a list of member names with each row containing the total member count let's look at the expected results we have the first name and the surname for each member and then every single row shows the total count of members there are 31 members in our table now if I want to get the total count of members I can take the members table and then select the count and this will give me 31 right but I cannot add first name and surname to this I will get uh an error because count star is an aggregation and it takes all the 31 rows and produces a single number which is 31 while I'm not aggregating first name and surname so the standard aggregation doesn't work here I need an aggregation that doesn't change the structure of my table and that works at the level of the row and to have an aggregation that works at the level of the row I can use a window function and the window function looks like having an aggregation followed by the keyword over and then the definition of the window so if I do this I get the count at the level of the row and to respect the results I need to change the order a bit here and I get the result that I wanted so a window function has these two main components an aggregation and a window definition in this case the aggregation counts the number of rows and the window definition is empty meaning that our window is the entire table and so this aggregation will be computed over the entire table and then added to each row there are far more details about the window functions and how they work in my mental model course produce a numbered list of members ordered by their date of joining so I will take the members table and I will select the first name and surname and to to produce a numbered list I can use a window function with the row number aggregation so I'll say row number over so row number is a special aggregation that works only for window functions and what it does is that it numbers the rows um monotonically giving a number to each starting from one and going uh forward and it never assigns the same number to two rows and in the window you need to define the ordering uh for for the numbering so what is the ordering in this case it's um defined by the join date and by default it's ascending so that's good and we can call this row number and we get the results we wanted and again you can find a longer explanation for this with much more detail about the window functions and and row number in the mental models course output the facility ID that has the highest number of slots booked again so we've we've already solved this problem in a few different ways let's see a new way to to solve it so we can go to our bookings table and we can Group by facility ID and then we can get the facility ID in our select and then we could sum on slots to get the total slots booked for each facility and since we're dealing with window functions we can also rank facilities based on the total slots that they have booked and this would look like rank over order by some slots descending and we can call this RK for Rank and if I order by some slots uh descending I should see that my rank works as intended so we've seen this in the mental models course you can think of rank as U deciding the outcome of a Race So the person who did the most in this case gets ranked one and then everyone else gets rank two 3 four but if there were two um candidates that got the same score the highest score they would both get rank one because they would both have won the race so to speak and the rank here is defined over the window of the sum of slots descending so that is what we need and next to get all the facilities that have the highest score or we could wrap this into a Common Table expression and then take that table and then select the facility ID and we can label this column total then we will get total and filter for where ranking is equal to one and we get our result aside from how rank works the the other thing to note in this exercise is that we can Define the window based on an aggregation so in this case we are ordering the elements of our window based on the sum of slots and if we look at our map over here we can see that uh we get the data we have our group ey we have the aggregation and then we have the window so the window follows the aggregation and So based on our rules the window has access to the aggregation and it's able to use it rank members by rounded hours used so the expected results are quite straightforward we have the first name and the surname of each member we have the total hours that they have used and then we are ranking them based on that so the information for this result where is it uh we can see that it's in the members and bookings tables and so we will need to join on these two tables members Ms join bookings book on M ID and that's our join now we need to get the total hours so we can Group by our first name and we also need to group by the surname because we will want to display it and now we can select these two columns and we need to compute the total hours so how can we get that for each member we know the slots that they got uh at every booking so we need to get all those those uh slots sum them up and uh every slot represents a 30 minute interval right so to get the hours we need to divide this value by two and remember if I take an integer like sum of slots and divide by two which is also an integer I'm going to have integer division so I won't have the part after the comma in the result of the division and that's not what I want so instead of saying divide by two I will say divide by 2.0 so let's check um how the data looks like this is looking good now but um if we read the question we want to round to the nearest 10 hours so 19 should probably be 20 115 should probably be 120 because I think that we round up when we have 15 and so on as you can see here in the result so how can we do this rounding well we have a Nifty round function which as the first argument takes the column with all the values and the second argument we can specify how do we want the rounding and to round to the nearest 10 you can put 1 here so actually let's keep displaying the the total hours as well as the rounded value to make sure that we're doing it correctly so as you can see we are indeed um rounding to to the nearest 10 so this is looking good and for the to understand the reason why I used minus one here and how the rounding function works I will have a small section about it when we're done with this exercise but meanwhile Let's uh finish this exercise so now I want to rank all of my rows based on this value here that I have comped computed and since this is an aggregation it will already be available to a window function right because in The Logical order of operations aggregation happen here and then Windows happen afterward and they have access to the data uh from the aggregation so it should be possible to transform this into a window function so think for a moment uh of how we could do that so window function has its own aggregation which in this case is a simple Rank and then we have the over part which defines the window and what do we want to put in our window in this case we want to order by let's say our um rounded hours and we want to order descending because we want the guest the member with the high hours to have the best rank but uh clearly we don't have a column called rounded hours what we have here is this logic over here so I will substitute this name with my actual logic and I will get my actual Rank and now I can delete this column here that I was was just looking at and I can sort by rank surname first name small error here I actually do need to show the hour as well so I need to take this logic over here again and call this ours and I finally get my result so to summarize what we are doing in this exercise we're getting our data by joining these two tables and then we're grouping by the first name and the surname of the member and then we are summing over the slots for each member dividing by 2.0 to make sure we have an exact Division and uh using the rounding function to round down to the nearest hour and so we get the hours and we use the same logic inside a window function to have a ranking such that the members with the with most hours get rank of one and then the one with the second most hours get rank of two and so on as you can see here in the result and I am perfectly able to use use this logic to Define The Ordering of my window because window functions can use uh aggregations as seen in The Logical order of SQL operations here because window functions occur after aggregations and um and that's it then we just order by the required values and get our results now here's a brief overview of how rounding Works in SQL now rounding is a function that takes a certain number and then returns an approximation of that number which is usually easier to parse and easier to read and you have the round function and it works like this the first argument is a value and it can be a constant as in this case so we just have a number or it can be a column um in which case it will apply the round function to every element of the column and the second argument specifies how we want the rounding to occur so here you can see the number from which we start and the first rounding we apply has an argument of two so this means that we really just want to see two uh numbers after the decimal so this is what the first rounding does as you can see here and we we round down or up based on whether the values are equal or greater than five in which case we round up or smaller than five in which case we round down so in this first example two is lesser than five so we just get rid of it and then we have eight eight is greater than five so we have to round up and so when we round up this 79 becomes an 80 and this is how we get to this first round over here here then we have round with an argument of one which leaves one place after the decimal and which is this result over here and then we have round without any argument which is actually the same as providing an argument of zero which means that we really just want to see the whole number and then what's interesting to note is that the rounding function can be generalized to continue even after we got rid of all the decimal part by providing negative arguments so round with the argument of1 really means that I want to round uh round this number to the nearest 10 so you can see here that from 48,2 192 we end up at 48,2 190 going to the nearest 10 rounding with a value of 2 means going to the nearest 100 so uh 290 the nearest 100 is 300 right so we have to round up and so we get this minus 3 means uh round to the nearest thousand so if you look at here we have 48,3 and so the nearest thousand to that is 48,000 minus 4 means the nearest 10,000 ,000 so given that we have 48,000 the nearest 10,000 is 50,000 and finally round minus 5 means round to the nearest 100,000 and um the given that we have 48,000 the nearest 100,000 is actually zero and from now on as we keep going negatively we will always get zero on this number so this is how rounding Works in brief it's a pretty useful function not everyone knows that you can provide it negative arguments actually I didn't know and then when I did the first version of this course um commenter pointed it out so shout out to him U don't know if he wants me to say his name but hopefully now you understand how rounding works and you can use it in your problems find the top three Revenue generating facilities so we want a list of the facilities that have the top three revenues including ties this is important and if you look at the expected results we simply have a the facility name and a bit of a giveaway of what we will need to use the rank of these facilities so there's this other exercise that we did a while back which is find the total revenue of each facility and from this exercise I have taken the code that uh allows us to get to this point where we see the name of the facility and the total revenue for that facility and you can go back there to that exercise to see in detail how this code works but in brief we are joining the bookings and Facilities tables and we are grouping by facility name and then we are getting that facility name and then within each booking we are Computing the revenue by taking the slots and using a case when to choose whether to use guest cost or member cost and so this is how we get the revenue for each single booking and now given that we grouped by facility we can sum all of these revenues to get the total revenue of each facility and this is how we get to this point given this partial result all that's left to do now is to rank these facilities based on their revenue so what I need here is a window function that will allow me to implement this ranking and this window function would look something like this I have a rank and why is rank the right function even though they sort of uh gave it away because if you want the facilities who have the top revenues including ties you can think of it as a race all facilities are racing to get to the top revenue and then if two or three or four facilities get that top Revenue if there are more in the top position you can't arbitrarily say oh you are first and they are second second you have to give them all the rank one because you have to tell them um recognize that they are all first so these type of problems uh call for a ranking solution so our window function would use rank as the aggregation and then we need to Define our window and how do we Define our window we Define the ordering for the ranking here so we can say order by Revenue descending such that the high highest revenue will get rank one the next highest will get rank two and so on now this will not work because I don't have the revenue column right I do have something here that is labeled as Revenue but the ranking part will not be aware of this label however I do have the logic to compute the revenue so I could take the logic right here and paste it over here and I will add a comma now this is not the most elegant looking code but let's see if it works and we need to order by Revenue descending to see it in action and if I order by Revenue descending you can in fact see that the facility with the highest revenue gets rank one and then it goes down from there so now I just need to clean this up a bit first I will remove the revenue column and then I will remove the ordering and what I need here for the result is to keep only the facilities that have rank of three or smaller so ranks 1 2 three and there's actually no way to do it in this query so I actually have to wrap this query into a common table expr expression and then take that table and say select star from T1 where rank is smaller or equal to three and I will need to order by rank ascending here and I get the result I needed so what happened here we built upon the logic of getting the total revenue for each facility and again we saw that in the previous exercise and um then what we did here is that we added a rank window function and within this rank we order by this total revenue so this might look a bit complex but you have to remember that when we have many operations that are nested you always start with the innermost operation and move your way up from there so the innermost operation is a case when which chooses between guest cost and member cost and then multiplies it by slots and this inner operation over here is calculating the revenue for each single booking the next operation is an aggregation that takes that revenue for each single booking and sums this these revenues up to get the total revenue by each facility and finally the outermost operation is taking the total revenue for each facility and it's ordering them in descending order in order to figure out the ranking and the reason all of this works we can go back to our map of SQL operations you can see here that after getting the table the first thing that happens here is the group buy and then the aggregations and here is where we sum over the total of of Revenue and after the aggregation is completed we have the window function so the window function has access to the aggregation and can use them when defining the window and finally after we get the ranking we we have no way of isolating only the first three ranks in this query so we need to do it with a common table expression and if you look here back to our map this makes sense because what components do we have in order to filter our table in order to only keep certain rows we have the wear which happens here very early and we have the having and they both happen before the window so after the window function you actually don't have another filter so you need to use a Common Table expression classify facilities by value so we want to classify facilities into equally sized groups of high average and low based on their revenue and the result you can see it here for each facility it's classified as high average or low and the point is that we decid decided uh at the beginning that we want three groups and this is arbitrary we could have said we want two groups or five or six or seven and then but we have three and then all the facilities that we have are distributed equally Within These groups so because we have nine facilities we get uh three facilities within each group and I can already tell you that there is a spe special function that will do this for us so we will not go through the trouble of implementing this manually which could be pretty complex so I have copied here the code that allow allows me to get the total revenue for each facility and we have seen this code more than one time in past exercises so if you're still not clear about how we can get to this point uh check out the the previous exercises so what we did in the previous exercise was rank the facilities based on the revenue and how we did that is that we took the ranking window function and then we def defined our window as order by Revenue descending except that we don't have a revenue column here but we do have the logic to compute the revenue so we can just get this logic and paste it in here and when I run this I will get a rank for each of my facilities where the biggest Revenue gets rank one and then it goes up from there now the whole trick to solve this exercise is to replace the rank aggregation with a antile aggregation and provide here the number of groups in which we want to divide our facilities and if I run this you see that I get what I need the facilities have been equally distributed into three groups where group number one has the facilities with the highest revenue and then we have group number two and finally group number three which has the facilities with the lowest revenue and to see how this function works I will simply go to Google and say postest antile and the second link here is the postest documentation and this is the page for window functions so if I scroll down here I can see all of the functions that I can use in window functions and you will recognize some of our old friends here row number rank dance rank uh and here we have antile and what we see here is that antile returns an integer ranging from one to the argument value and the argument value is what we have here which is the number of buckets dividing the partition as equally as possible so we call the enti function and we provide how many buckets we want to divide our data into and then the function divides the data as equally as possible into our buckets and how will this division take place that depends on the definition of the window in this case we are ordering by Revenue descending and so this is how the ntile function works so we just need to clean this up a bit I will remove the revenue part because that's not required from us and I will call this uh enti quite simply and now I need to add a label on top of this enti value as you can see in the results so to do that I will wrap this into a Common Table expression and when I have a common table expression I don't need the ordering anymore and then I can select from the table that I have just defined and what what do I want to get from this table I want to get the name of the facility and then I want to get the enti value with a label on top of it so I will use a case when statement to assign this label so case when NTI equals 1 then I will have high when anti equals 2 then I will have average else I will have low uh and the case and call this revenue and finally I want to order by antile so the results are first showing High then average then low and also by facility name and I get the result that I wanted so to summarize uh this is just like the previous exercise except that we use a different window function because instead of rank we use end tile so that we can pocket our data and in the window like we said in the previous exercise there's a few nested operations and you can figure it out by going to the deepest one and moving upwards so the first one picks up the guest cost or member cost multiplies it by slots gets the revenue Vue for each single booking the next one Aggregates on top of this within each facility so we get the total revenue by facility and then we use this we order by Revenue descending this defines our window and this is what the bucketing system uses to distribute the facilities uh in each bucket based on their revenue and then finally we need to add another layer of logic uh here we need to use a common table expression so that we can label our our percentile with the required um text labels calculate the payback time for each facility so this requires some understanding of the business reality that this data represents so if we look at the facilities table we have an initial outlay which represents the initial investment that was put into getting this facility and then we also have a value of monthly maintenance which is what we pay each month to keep this facility running and of course we will also have a value of monthly revenue for each facility so how can we calculate the amount of time that each facility will take to repay its cost of ownership let's actually write it down so we don't lose track of it we can get the monthly revenue of each facility but what we're actually interested in is the monthly profit right um and to get the profit we can subtract the monthly maintenance for each facility so Revenue minus expenses equals profit and when we know how much profit we make for the facility each month we can take the initial investment and divided by the monthly profit and then we can see how many months it will take to repay the initial investment so let us do that now and what I have done here once again I copied the code to calculate the total revenue for each facility and um we have seen this in the previous exercises so you can check those out if you still have some questions about this and now that we have the total revenue for each facility we know that we have three complete months of data so far so how do we get to this to the monthly Revenue it's as simple as dividing all of this by three and I will say 3.0 so we don't have integer division but we have proper division you know and I can call this monthly revenue and now the revenue column does not exist anymore so I can remove the order buy and here I can see the monthly revenue for each facility and now from the monthly revenue for each facility I can subtract the monthly maintenance and this will give me the monthly profit but now we get this error and can you figure out what this is about monthly maintenance does not appear in the group by Clause so what we did here is that we grouped by facility name and then we selected that which is fine and all the rest was gation so remember as a rule when you Group by you can only select the columns that you have grouped by and aggregations and monthly maintenance uh is not an aggregation so in order to make it work we need to add it to the group by statement over here and now I get the monthly profit and finally the last step that I need to take is to take the initial outlay and divide it by by all of the rest that we have computed until now and we can call this months because this will give us the number of months that we need in order to repay our initial investment and again we get the same issue initial outlay is not an aggregation does not appear in the group by clause and easy solution we can just add it to the group by clause so something is pretty wrong here the values look pretty weird so looking at all this calculation that we have done until now can you figure out why the value is wrong the issue here is related to the order of operations because we have no round brackets here the order of operation will be the following initial outlay will be divided by the total revenue then it will be divided by 3.0 and then out of all of these we will subtract monthly maintenance but that's not what we want to do right what we want to do is to take initial outlay and divide it by everything else which is the profit so I will add round brackets here and here and now we get something that makes much more sense because first we execute everything that's Within These round brackets and we get the monthly profit and then all of it we divide initial outlay by and then what we want to do is to order by facility name so I will add it here and we get the result so quite a representative business problem calculating a revenue and profits and time to repay initial investment and uh overall is just a bunch of calculations starting from the group bu that allows us to get the total revenue for each booking we sum those revenues to get the total revenue for each facility divide by three to get the monthly Revenue subtract the monthly expenses to get the monthly profit and then take the initial investment and divide by the monthly profit and then we get the number of months that it will take to repay the facility calculate a rolling average of total revenue so for each day in August 2012 we want to see a rolling average of total revenue over the previous 15 days rolling averages are quite common in business analytics and how it works is that if you look at August 1st this value over here is the average of daily revenue for all facilities over the past 15 days including the day of August 1st and then this average is rolling by one win one day or sliding by one day every time so that the next average is the uh same one except it has shifted by one day because now it includes the 2nd of August so let's see how to calculate this and in here I have basic code that calculates the revenue for each booking and I've taken this from previous exercises so if you have any questions uh check those out and what we have here is the name of each facility and um and the revenue for each booking so each row here represents just a single booking so this is what we had until now but if you think about it we're not actually interested in seeing the name of the facility because we're going to be uh summing up over all facilities we're not interested in the revenue by each facility but we are interested in seeing the date in which each booking occurs because we want to aggregate within the date here so to get the date I can get the start part time field from bookings and because this is a time stamp so it shows hours minutes seconds I need to reduce it to a date and what I get here is that for again each row is a booking and for each booking I know the date on which it occurred and the revenue that it generated now for the next step I need to see the total revenue over each facility within the date right so this is a simple grouping so if I group by this calculation over here which gives me my date I can then get the date and now I have um I have compressed all the different occurrences of dates to Unique values right one row for every date and now I need to compress as well all these different revenues for each date to a single value and for that I can put this logic inside the sum aggregation as we have done before and this will give me the total revenue across all facilities for each day and we have it here for the next step my question for you is how can I see the global average over all revenues on each of these rows so that is a roow level aggregation that doesn't change the structure of the table and that's a window function right so I can have a window function here that gets the average of Revenue over and for now I can leave my window definition open because I will look at the whole table however um Revenue will not work because revenue is just a label that I've given on this column and but but this part here is not aware of the label I don't actually have a revenue column at this point but instead of saying Revenue I could actually copy this logic over here and it would work because the window function occurs after Computing the aggregation so the window function is aware of it so this should work and now for every row I see the global average over all the revenues by day now for the next step I would like to first order by date ascending so we have it here in order and my next question of for you is how can we make this a cumulative average average so let's say that our rows are already ordered by date and how can I get the average to grow by date so in the first case the average would be equal to the revenue because we only have one value on the second day the average would be the average of these two values so all the values we've seen until now on the third day it would be the average of the first three values and so on how can I do that the way that I can do that is that I can go to my window definition over here and I can add an ordering and I can order by date but of course the column date does not exist because that's a label that will be assigned after all this part is done uh window function is not aware of labels but again window function works great with logic so I will take the logic and put it in here and now you can see that I get exactly what I wanted on the first row I get the average is equal to the revenue and then as it grows we only look at the current revenue and all the previous revenues to compute the average and but we don't look at all of the revenues so on the second row uh we have the average between this Revenue over here and this one over here and then on the third row we have the average between these three revenues and so on now you will realize that we are almost done with our problem and the only piece that's missing is that right now if I pick a random day within my data set say this one the the average here is computed over all the revenues from the previous days so all the days in my data that lead up to this one they get averaged and we compute this revenue and what I want to do to finish this problem is that instead of looking at all the days I only want to look 15 days back so I need to to reduce the maximum length that this window can extend in time from limited to 15 days back now here is where it gets interesting so what we need to do is to finetune the window definition in order to only look 15 days back and with window functions we do have the option to finetune the window and it turns out that there's a another element to the window definition which is usually implicit it's usually not written explicitly but it's there in the background and it's the rows part so I will now write rows between unbounded preceding and current rows row now what the rose part does is that it defines how far back the window can look and how far forward the the window can look and what we see in this command is actually the standard Behavior it's the thing that happens by default which is why we usually don't need to write it and what this means is that it says look as far back in the past as you can look as far back as you can based on the ordering and the current row so this is what we've been seeing until now and if I now run the query again after adding this part you will see that the values don't change at all because this is what we have been doing until now so now instead of unbounded proceeding I want to look 14 rows back plus the current row which together makes 15 and if I run this my averages change because I'm now looking um I'm now averaging over the current row and the 14 previous rows so the last 15 values and now what's left to do to match our result is to remove the actual Revenue over here and call this Revenue and finally we're only interested in values for the month of August 2012 so we need to add a filter but we cannot add a filter in this table definition here because if we added a wear filter here um isolating the period for August 2012 can you see what the problem would be um if my data could only see um Revenue starting from the 1st of August he wouldn't be able to compute the rolling average here because to get the rolling average for this value you need to look two weeks back and so you need to look into July so you need all the data to compute the rolling revenue and we must filter after getting our result so what that looks like is that we can wrap all of this into a Common Table expression and we can we won't need the order within the Common Table expression anymore and then selecting this we can filter to make sure that the date fits in the required period so we could truncate this date at the month level and make sure that it is equal that the truncated value value is equal to the month of August and we have seen how day trunk works in the previous exercises and then we could select all of our columns and order by date I believe we may have an extra small error here because I kept the partial wear statement and if I run this I finally get the result that I wanted so a query that was a bit more complex it was the final boss of our exercises um so let's summarize it we get the data we need by joining booking and facility um and then we are getting the revenue for each booking that is this um multiply slots by either guest cost or member cost cost depending on whether the member is a guest or not this is getting the revenue within each booking then we are grouping by date which you see uh over here and summing all of these revenues that we computed so that we get the total revenue within each day for all facilities then the total revenue for each day goes into a window function which computes an aggre ation at the level of each row and the window function computes the average for these total revenues within a specific window and the window is defines an ordering based on time so the the ordering based on date and the default behavior of the window would be to look at the average for the current day and all the days that precede up until the earliest date and we're doing here is that we are finetuning the behavior of this function by saying hey don't look all the way back in the past uh only look at 14 rows preceding plus the current row which means that given the time ordering we compute the average over the last 15 values of total revenue and then finally we wrap this in a Common Table expression and we filter so that we only see the rolling average for the month of August and we order by date and that were all the exercises that I wanted to do with you I hope you enjoyed it I hope you learned something new as you know there are more sections in here that go more into depth into date functions and string functions and how you can modify data I really think you can tackle those on your own these were the uh Essentials ones that I wanted to address and once again thank you to the author of this website aliser Owens who created this and made it available for free I did not create this website um so you can just go here and without signing up or paying anything you can just do these exercises my final advice for you don't be afraid of repetition we live in the age of endless content so there's always something new to do but there's a lot of value to um repeating the same exercises over and over again when I Was preparing for interviews when I began as a date engineer I did these exercises and altogether I did them like maybe three or four times um and um I found that it was really helpful to do the same exercises over and over again because often I did not remember the solution and I had to think through it all over again and it strengthened those those uh those learning patterns for me so now that you've gone through all the exercises and seen my Solutions uh let it rest for a bit and then come back here and try to do them again I think it will be really beneficial in my course I start from the very Basics and I show you in depth how each of the SQL components work I um explore the logical order of of SQL operations and I spend a lot of time in Google Sheets um simulating SQL operations in the spreadsheet coloring cells moving them around making some drawings in excal draw uh so that I can help you understand in depth what's happening and build those mental models for how SQL operations work this course was actually intended as a complement to that so be sure to check it out
