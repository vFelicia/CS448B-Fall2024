With timestamps:

00:00 - this deep learning course is designed to
00:02 - take you from beginner to proficient in
00:05 - deep learning IU sing created this
00:07 - course he's an experienced data
00:09 - scientist and popular course creator
00:12 - aush will teach you the fundamental
00:14 - concepts architectures and applications
00:17 - of deep learning in a clear and
00:19 - practical way so get ready to build
00:22 - train and deploy models that can tackle
00:24 - real world problems across various
00:26 - Industries what if I tell you that
00:28 - there's a deep learning course that
00:30 - teaches you deep learning from very
00:32 - scratch to a core level so what I really
00:35 - mean by scratch is teaching you the core
00:38 - and the Crux of the mathematics which is
00:40 - required for deep learning like linear
00:42 - algebra single variable calculus and
00:44 - much more and not only this we give you
00:46 - detailed lecture notes along with the
00:48 - Practical assignments on data Wars for
00:51 - absolutely free so that you can follow
00:53 - through this course and become the
00:55 - master hi this is aayush I'm co-founder
00:57 - of second brain laps and in past worked
01:00 - as a lead data scientist at triplate a
01:02 - UK based esteemed organizations working
01:04 - on large scale career economy product as
01:06 - well as I've worked as an emops engineer
01:08 - in a core ziml team in order to
01:11 - streamline mlops Frameworks and
01:12 - furthermore I worked in several us-based
01:14 - companies as a contractual roles as a
01:16 - data scientist and not only this I love
01:18 - teaching my courses has got millions and
01:21 - millions of views throughout the
01:23 - internet and I've helped several
01:25 - thousands of students in order to get
01:27 - their first paycheck or their first job
01:29 - and but why you should consider learning
01:31 - deep learning and what is the core
01:33 - problem which is coming into the deep
01:34 - learning content throughout the internet
01:36 - now it is all of the companies are
01:38 - asking for a deep learning skill set
01:40 - into a particular candidate and to be
01:43 - honest people think deep learning
01:45 - extremely hard and pretty hard to
01:47 - understand and it's my personal opinion
01:49 - I feel that because of the instructors
01:50 - on YouTube it's becoming a little bit
01:52 - hard to understand it's not because it
01:54 - is very complex I agree that the
01:56 - whatever things are little bit difficult
01:58 - or hard to interpret it but if taught in
02:01 - the right way deep learning is the most
02:03 - I think easiest subject as compared to
02:06 - even core machine learning I will teach
02:08 - you deep learning in a very core way
02:10 - from very scratch we will mathematically
02:12 - do every iteration by our mathematics so
02:15 - that you understand okay this is how the
02:17 - flow is going and this is how each steps
02:20 - is helping your model to learn or become
02:22 - better let's get started with our course
02:26 - [Music]
02:37 - hey everyone welcome to this first
02:39 - lecture on linear algebra so today we
02:43 - are going to talk about uh vectors and
02:46 - we'll be exploring vectors A bit okay so
02:50 - uh first we'll start off with what is
02:53 - linear algebra why do we even bother to
02:57 - study this as some of you all already
03:00 - are familiar with algebra you you just
03:02 - want to uh just re refresh your memory
03:06 - of your algebra or you wanted to uh
03:09 - you're from very scratch and then s so
03:11 - that's why let's let's start with the
03:13 - definition of linear algebra so I've
03:16 - written one definition of linear algebra
03:19 - is it's the mathematics of the data yeah
03:22 - you heard me correct I saw this
03:24 - definition online and I found this a
03:27 - very basic definition to tell you uh
03:29 - rather than taking too much of uh
03:32 - mathematical terms is algebra L linear
03:36 - algebra is the mathematics of data and
03:39 - why I'm saying it's mathematics for data
03:41 - because uh L linear algebra contains of
03:44 - matrices and vectors so so these uh
03:47 - these these two are the language of the
03:50 - data so whatever we are going to study
03:53 - you which you will see in your machine
03:55 - learning or deep Learning Journey
03:57 - whatever you are going to study so
03:59 - that's why we we just say that is it's
04:01 - the mathematics of the data because in
04:03 - machine learning data is so much of uh
04:07 - basic component or or a mandatory
04:10 - component the same way the we use
04:13 - algebra or L lar algebra to work with
04:17 - the data mathematically okay so this is
04:21 - a simple definition of linear algebra
04:24 - over here so let's get started with the
04:27 - first uh first uh the component which
04:30 - first thing which we'll study in this
04:32 - video is vectors okay so so we'll start
04:36 - with the vectors so we starting with
04:39 - vectors so if you if if you have any
04:42 - definition of a vectors please please
04:45 - please stop stop this video pause this
04:47 - video and then go down in the comment
04:49 - and please tell me what are vectors so
04:52 - the the we will start with very scratch
04:55 - uh you can assume a vectors as an arrows
04:59 - as an arrows we we have an arrow so
05:02 - these These are the geometric inition of
05:04 - a vectors so the definition of a vector
05:06 - can be it's it's an arrays of a numbers
05:10 - okay so vectors are arrays of a number
05:14 - or a tuple of a numbers okay or or or
05:18 - you can you can take it as an arrays
05:20 - okay so you can consider a vectors
05:23 - vectors can be arrays of a numbers
05:27 - arrays of number
05:30 - or you can consider this vectors uh as
05:33 - an as an arrow you can consider vectors
05:36 - as an arrows as an arrows or you can
05:41 - consider a vector as a tuple of a
05:44 - numbers you can consider ve Vector as a
05:47 - tuple of numbers okay so these are you
05:52 - you can just imagine this an an array is
05:56 - maybe an array can be 1 2 2 3 this is
06:01 - called this is this is called the draw
06:02 - Vector which you'll study you can safely
06:04 - ignore this so this is this is also a
06:06 - vector which is special type of vector
06:08 - which is called the row Vector this can
06:10 - be tle of a numbers or it can be arrows
06:13 - okay so so so the way I like to
06:16 - represent uh vectors or to make you very
06:19 - very much comfortable with it is to to
06:22 - make you familiar with an arrows so this
06:24 - is the geometric division of a
06:27 - two-dimensional Vector so let's see how
06:29 - the vector looks numerically okay uh in
06:33 - in terms of mathematical so let's see
06:35 - let's let's see how the vector look so
06:37 - let's name the vector as U okay so let's
06:41 - let's name the vector as U equals so
06:45 - let's store 2 and four so U is a vector
06:50 - where you have the elements so the the
06:52 - numbers inside the vector you you
06:54 - enclose into a a square bracket so here
06:58 - here is two and here is four okay so why
07:01 - I'm saying we have this is this this is
07:03 - like a an arrow so the first element is
07:07 - called X
07:08 - component and the first element is
07:11 - called X component and uh second element
07:14 - is called a y component it's called a y
07:17 - component so let's let's see how this
07:20 - looks on on a graph paper or or to or an
07:25 - X and Y plane so let me plot let me plot
07:29 - that so 1 2 3 4 5 1 2 3 4 and five okay
07:41 - so this is my y plane and this is my
07:44 - x-axis okay so let's plot this Vector
07:47 - onto this uh X and Y plane so so X
07:50 - component is 2 and Y component is four
07:53 - okay so X component is two and Y
07:55 - component is four so here's the point
07:57 - and it passes through the origin so so
08:00 - this is your vector U okay so this is
08:03 - your vector U where you have 2x 4 where
08:07 - two indicates the X component and four
08:10 - indicates the Y component okay so this
08:13 - makes sense I hope so okay so and
08:17 - vectors are arrays of a numbers or you
08:20 - can say the the Tuple of a numbers which
08:24 - which which consist of numbers where it
08:27 - is it has only it is here here our
08:31 - Vector is two dimensional okay but but
08:35 - uh here you can have M number of a rows
08:38 - here you can have in vectors you can
08:40 - have M number of rows you can have M
08:43 - number of rows but and you can and in
08:47 - vectors you have only one number you
08:49 - have only one column okay so so so you
08:53 - can have any number of you can have M
08:55 - number of rows for for for example for
08:58 - example let let me show you a vector U
09:02 - Can Be A B all the way around to the N
09:07 - okay so it can be n dimensional Vector
09:10 - so here our this Vector is two
09:12 - dimensional Vector this Vector is 2D
09:16 - Vector okay and geometrically I showed
09:19 - you by plotting on this XY plane that
09:22 - this is the the two two dimensional plot
09:24 - means first of all X we we we we we go
09:26 - through x x is the four the two units on
09:30 - the x-axis and the four units on the Y
09:33 - AIS and then we and then we uh taken the
09:36 - from the origin and that point okay so
09:39 - so this is this a graph for you but
09:42 - let's take an example eight so this is
09:43 - our Vector a where you going store n
09:46 - dimensional Vector uh it's not key that
09:48 - you should only have two two dimensional
09:51 - Vector you can have n dimensional Vector
09:53 - okay so so but but showing you
09:57 - geometrically you can show three three
09:59 - dimensional Vector geometrically so you
10:01 - can just draw a straight line over here
10:05 - and this is your Z okay so you can plot
10:08 - a three-dimensional Vector so so so you
10:10 - can plot it for example you taken K as a
10:13 - vector and you can plot 2 32 on this
10:17 - threedimensional plane or the three
10:18 - threedimensional graph but you can but
10:21 - you can kind plot
10:25 - your you cannot plot your
10:27 - four-dimensional or or five dimens
10:30 - Vector over here so for geometrically
10:32 - understanding I have just just showed
10:34 - you how this Vector looks like but it's
10:36 - not a manner that you can only have a
10:39 - two-dimensional vector or or only three
10:41 - dimensional Vector you can have nend
10:43 - dimensional Vector because scientist or
10:45 - researchers most care about your uh or
10:48 - nend dimensional uh your numerically
10:50 - rather than uh most of of course they
10:52 - care about geometrically as well but uh
10:54 - but I just showed you it's not possible
10:56 - for me to draw a FL four dimensional and
10:58 - show you how this how how how we are
11:00 - going to plot but but geometric
11:02 - contribution of a vectors are are we can
11:04 - plot it like this and for example for
11:07 - example you you you have a 2 4 3 so here
11:11 - your K here your K is 3x 1 so first is
11:16 - what are the number of columns which
11:17 - usually denote as three number of
11:19 - columns and you have only one row of
11:20 - course in vectors you can only have one
11:22 - row okay uh okay so so so here you have
11:26 - X component here you have X component
11:29 - here you have y component and here three
11:33 - is your Z component which is in
11:34 - threedimensional and so on okay so this
11:37 - is how you represent vectors so the
11:40 - whole so the whole intuition about
11:43 - vectors so I hope that you understood
11:45 - what I'm trying to convey you over here
11:47 - okay so so so let's so let's see so
11:50 - let's see uh so let's let's go further
11:53 - into understanding uh some more
11:55 - intuitively one last examples of a
11:57 - vectors to to get us what is trying to
12:00 - convey and and and it's it's it's it's
12:02 - much better for for for us to understand
12:04 - okay so I'm going to just just just draw
12:06 - an X and Y plane over here so I'm just
12:09 - just going to draw an X and Y uh X and Y
12:12 - plane like this X and Y and I'm going to
12:16 - take one I'm going to take 2 3 4 5 and
12:22 - six 1 2 3 4 5 okay so this is our X and
12:29 - Y plane now now what what I'm going to
12:31 - do is make it for for example you can
12:34 - you want to plot the vector one 2 okay
12:38 - so how so here you go one unit or X's
12:42 - unit or one unit on the X because this
12:45 - this is your X component this is your X
12:47 - component this is your X component so
12:49 - you go on X unit over here so we'll
12:51 - we'll go till here okay and then two
12:54 - units above so this is your this is your
12:57 - final vector v okay is a final vector v
13:01 - and and to for denoting the Y always the
13:03 - name of the vector should be in lower
13:05 - case with one Arrow Above So this this
13:08 - indicates that it's it's a vector okay
13:11 - so this is this is how you can you have
13:13 - to practice so just try to plot a vector
13:16 - U where X component to be 4x4 okay and a
13:20 - vector can be nend dimensional it can be
13:23 - 6 7 8 9 it can be in dimensional so this
13:27 - is this is how you this is what the
13:29 - vectors are okay so vectors are a two
13:31 - polar array of numbers which we which we
13:34 - just shown shown you today okay so now
13:36 - let's see how we can take out the length
13:39 - of a vector so for example you just draw
13:42 - this Vector so the v v Vector so how do
13:45 - you how can you take out the length of
13:48 - this Vector it's it's a good good good
13:50 - good good way to think about this okay
13:52 - so what I'm going to do now what I'm
13:54 - going to do now is to just remove this
13:56 - and show you uh so take another another
13:59 - example so I'm I'm talking about how do
14:01 - you take out so let's take one example
14:04 - that you have a vector U you have a
14:06 - vector U you have Vector U we have a
14:09 - vector U where your X component is four
14:12 - and 4 by 4 and four okay so here you
14:15 - have here you have a two-dimensional
14:16 - Vector two-dimensional Vector so you can
14:19 - you can also write w with the member of
14:21 - R2 okay so uh so this is the this is the
14:26 - w u is the member of two-dimensional
14:29 - real numbers okay so this is this is
14:31 - this this is your example so what what
14:33 - you want to do is plot the or just let's
14:37 - plot the vector on this okay so 4X 4 I
14:41 - should see over here so X unit four over
14:45 - here and four units of Earth okay and
14:48 - then let's touch this point I think it's
14:50 - wrong bit but no problem this is so how
14:53 - how are you going to take out the length
14:56 - of this Vector this is a good good
14:57 - question to ask to you so for taking out
15:00 - the length of this Vector okay so what
15:02 - you can do here you can see here you can
15:04 - see here you can see that this is also
15:07 - four units this this is also four units
15:11 - and this is also four units so I'm just
15:13 - just going to change the this this is
15:14 - also four units this is also four units
15:17 - okay and you can see this forms a right
15:19 - triangle right triangle at a 90° okay so
15:23 - this forms a right triangle so you know
15:25 - this this is so you know this so which
15:28 - is four units
15:29 - you know this which is four units and
15:31 - you know the this x which is which is
15:33 - here the base four units so here height
15:36 - is 4 units and your base is four units
15:39 - can't you take out the
15:41 - hypotenuse okay or the vector length by
15:45 - using Pythagorean theorem of course you
15:48 - can take out so you can use the
15:49 - Pythagorean theorem you can use the
15:52 - pytha Pythagorean theorem Pythagorean
15:56 - theorem to take out the hypotenuse so
15:59 - for taking the hypoten so the the u²
16:03 - equal to b² + h² okay so you don't know
16:08 - U Square you know b square which is 4
16:11 - you know the H Square which is 4 okay so
16:15 - U ^2 = 16 + 16 which = 32 and then U ² =
16:21 - 32 now want to U equals to square < TK
16:25 - of 32 that is actually 5.6
16:29 - 5 and and and nearest 100 okay so this
16:32 - is your length of the vector this is the
16:35 - length 5.65 is the length of the vector
16:38 - so so the norm you you usually say the
16:41 - norm okay so in in linear algebra terms
16:44 - so the norm of the vector which is
16:46 - equivalent to length of vector is equals
16:48 - to
16:49 - 5.65 which is your an which is your the
16:53 - length of the vector okay so I hope that
16:55 - you understood what I'm trying to convey
16:56 - with geometric intuition over here okay
16:59 - so what if if we have n dimensional
17:01 - Vector okay so what if we have n
17:04 - dimensional Vector so we will we will
17:05 - see what if we have n dimensional Vector
17:08 - but but let's see let's let's go further
17:11 - let's understand a bit more intuition
17:13 - about um the the how many number of
17:15 - elements what what are the terminologies
17:17 - and then we'll see how do we take out
17:19 - the length of an N dimensional Vector
17:22 - okay so uh just as a notation or
17:24 - terminology or to to remember so that
17:26 - everything is clear everything is clear
17:30 - uh the elements in the vectors are the
17:31 - dimension of the vectors I'm not saying
17:34 - that uh elements it's the the number of
17:36 - elements in the vector is equivalent to
17:38 - the dimensions of your vector okay so
17:40 - the number so the number of elements
17:45 - elements here is numbers okay so
17:47 - elements are usually the numbers like
17:48 - four is an element four is an element
17:51 - okay the dimension are the dimension
17:54 - dimension of the vector of the vector
17:58 - okay so this is the first terminology so
18:01 - here here you can see that you have
18:03 - Vector U which has a and b so here it
18:06 - has two two elements so this so so here
18:09 - is two elements so here it the U the
18:12 - vector U is a 2d Vector is so I I am
18:15 - forgetting is a 2d Vector is a 2d Vector
18:18 - two dimensional Vector so you can plot
18:20 - it if it is three threedimensional you
18:23 - you'll be having a bit difficulty in
18:24 - plotting in a threedimensional plane but
18:26 - you can plot it if it is
18:27 - four-dimensional you cannot plot it on
18:30 - on over here okay so this is a number of
18:33 - elements are the the are the dimensions
18:36 - of your vector okay the next
18:39 - terminology vectors can be n dimensional
18:42 - as I stated the vectors can be vectors
18:47 - can be can be n dimensional Vector n
18:52 - Dimension and dimensional so you can
18:54 - have the U Vector as a b or all the way
18:59 - around to the N okay so here it can be n
19:02 - dimensional Vector so as I stated that
19:04 - it should not be only 2D Vector it can
19:06 - have a n dimensional Vector okay so so
19:10 - as as I left you hey hey how you're
19:13 - going to take out so you take out the
19:14 - length of this Vector U by just uh by
19:17 - just using the Pythagorean theorem but
19:19 - how you how you are going to take out
19:21 - the length of vector U which is an N
19:23 - dimensional so how do you take out the
19:25 - length so the norm of U so I'm talking
19:27 - about this U the norm of U is a square
19:31 - root of U1 2 + U2 2 +
19:37 - u3 squar all the way around to the U N
19:41 - squ okay so it's it's just equals that
19:44 - not nothing much deeper which which we
19:47 - have talked
19:49 - or
19:51 - i u i
19:54 - s okay so this is the this this this is
19:57 - what I want to convey over here this is
20:00 - what I'm going to convey okay so you can
20:02 - actually actually think think about it
20:05 - and in in that way that you want to take
20:08 - for taking the length you just Square U
20:10 - square plus u Square all the way around
20:12 - to the whatever the number of elements
20:13 - in your okay so over here what we were
20:15 - doing we can simply do like this we can
20:18 - simply do over here if if you want to
20:20 - take out you can simply use this of
20:22 - course you are it's just related to some
20:24 - Pythagorean theorem okay so over here
20:27 - you can just add a square root of your
20:31 - b² + h² which is b square is U1 H square
20:36 - is U2 okay
20:38 - means this one four and four okay and
20:41 - then you take out so it's just
20:43 - equivalent whatever whatever we had seen
20:45 - over here okay so so the same way we
20:47 - take out the the dimens or the length of
20:50 - our Vector uh U okay which is an N
20:53 - dimensional Vector I hope that
20:55 - understood till now whatever I taught
20:57 - Okay so so so so let's so let's see so
21:00 - let's see uh bit more so we have studied
21:03 - the how what are the vectors how do we
21:05 - take the length how do we represent the
21:07 - vector v vectors can be n dimensional so
21:09 - now let's see some of the let's do some
21:12 - of the operations on our Vector okay so
21:16 - so we'll start doing the operations on
21:18 - our Vector so I'm just just going to
21:20 - give a headline uh doing operations
21:23 - doing
21:25 - operations on Vector okay on Vector so
21:29 - it's a it's a very great it's it just
21:32 - not too much hard it's very very easy so
21:34 - so the first operation which you want to
21:36 - do is addition of a vector so how are
21:39 - how are we going to do the addition of a
21:41 - vector the first component is
21:46 - addition so you are given you are given
21:49 - so let me State the problem you given
21:52 - the vector the vector a which is 2x 2
21:57 - and you're given the vector B
21:59 - which is 4X 4 got it you want to what
22:02 - you want to do you want to calculate you
22:04 - want to calculate you want to
22:06 - calculate Vector C by adding a + b means
22:13 - you want to calculate the vector C by
22:15 - taking out the the the uh adding by by
22:19 - summing this to Vector okay so how are
22:21 - we going to sum some this 2x two and 4x
22:24 - 4 means so what you can do you can do
22:27 - the element wise you can do the element
22:29 - so so you can what you can do c c will
22:33 - will be equals to 2 + 4 and 2 + 4 2 + 4
22:39 - 2 + 4 and then also 2 + 4 okay so two
22:43 - element wise addition so it will be
22:45 - nothing but equals to 6 by 6 okay where
22:50 - you add the one you when when you add
22:54 - the
22:55 - vector 2 by 2 by 1 + 2 by 1 2 * 1 which
23:01 - is the the dimension of the resulting
23:03 - Vector will be also 2x 1 okay so what
23:07 - what do you do you you 2x 1 uh which is
23:10 - the dimension of your a vector and then
23:12 - 2 2x 2 * one where the there is two
23:16 - elements and the one one one column so
23:18 - that is also B and then the resulting
23:21 - Vector is 2 * 1 okay so it is the 6X 6
23:25 - so you get you calculate C to be 66
23:29 - where your X component is 6 and Y
23:30 - component is 6 so the 66 is your
23:33 - resulting Vector so your resulting
23:35 - Vector C is a two-dimensional Vector
23:37 - okay some of the things which I want to
23:39 - highlight is your dimensions of your
23:42 - both the vector which is a and b should
23:46 - match okay if for example for
23:50 - example if your a vector a vector a
23:54 - vector is 2x two and your V vector
23:59 - is
24:00 - 446 then you try to add it the resulting
24:04 - Vector will be
24:06 - undefined will be undefined okay so your
24:09 - dimensions of the addition of vector
24:11 - should match okay otherwise it is
24:13 - undefined operation okay so so this this
24:17 - is this is what I to convey over here so
24:19 - let's see how you add the vectors how
24:22 - you add the vectors uh how you add the
24:25 - vectors geometrically so we seen the
24:27 - numerically how we add the vectors so
24:29 - let's see how you how you add the
24:31 - vectors geometrically okay so let's
24:35 - let's do something let's do something is
24:38 - uh we will do now some geometric because
24:41 - people tend to understand more
24:43 - geometrically rather than numerically
24:45 - okay so we'll understand geometrically
24:48 - so here I'm drawing I'm going to draw
24:49 - One X and Y plane like this okay X and Y
24:53 - plane okay so so let's see so let's see
24:57 - that you that your vector a that this is
25:00 - this this this is this is your vector a
25:03 - so let's let's let's keep uh let's make
25:06 - it total okay so this is so this this
25:10 - this is your vector a and this is your
25:12 - vector B this is your vector v okay this
25:16 - this is your vector v and this this is
25:18 - your vector a you want to add this two
25:20 - Vector geometrically speaking you want
25:22 - to add add this Vector a plus Vector B
25:26 - okay you want to add this so when you do
25:28 - this geometrically okay so what what you
25:30 - going to do you're going to take this
25:32 - Vector a so I'm just just just going to
25:34 - say you take this Vector a take this
25:36 - Vector a and put the tail of this Vector
25:40 - the tail of this Vector onto the onto
25:42 - the top of the vector B okay you put the
25:45 - tail of this Vector onto the top of the
25:47 - vector B okay and then you put this this
25:51 - this one onto the head of this okay what
25:55 - what you want to do is uh take this VOR
25:58 - whatever whatever the length okay
26:00 - whatever may be the length you just put
26:03 - it like this I'm I'm not drawing correct
26:06 - but no problem in that so the length of
26:08 - the length of this should should be same
26:10 - as whatever you are doing you're just
26:12 - taking this a vector and putting it over
26:15 - here okay you just um you're just taking
26:18 - this Vector a and putting this tail onto
26:20 - the head and and then you are and then
26:22 - you are putting it over here and then
26:24 - what what you do you take this vector v
26:26 - you take this Vector B and put the ta
26:28 - onto the top of a and put the and and
26:31 - just match this uh with this okay so
26:33 - take a vector B and then you just put it
26:36 - like this I think it's not correct too
26:38 - much but this is how you are going to do
26:40 - you first to take the vector a put it on
26:43 - the the the tail of that on the head of
26:45 - B and then you whatever the length you
26:47 - just uh you just put put that over here
26:49 - and then you take this and that and then
26:52 - put put put this state onto the head of
26:53 - this and then you match the heads okay
26:56 - and then and then and then the head the
26:59 - the the resulting Vector so the
27:01 - resulting Vector so the resulting Vector
27:05 - I'm just just just going to take this so
27:07 - so you just take you just this is this
27:09 - will be this will
27:12 - be this will be your resulting Vector we
27:16 - are blue one in the case A+ b a vector
27:19 - plus b Vector okay so this is this this
27:22 - this is how you add the vector
27:23 - geometrically and then it makes sense as
27:25 - well okay if Ison don't worry let's see
27:27 - one more example to make more clarity
27:30 - okay so over here which you let's plot
27:33 - so this is how you just take this and
27:34 - then you draw and then you are done with
27:36 - this and then you attach the the here's
27:39 - the tail on the origin okay and then you
27:41 - attach and then you draw a straight line
27:43 - on like that okay so this this is called
27:45 - the parallelogram method okay so this is
27:47 - called the parallelogram this this is
27:49 - called the parallelogram method for
27:52 - showing the geometrically for showing
27:54 - the geometrically let's see triangle
27:56 - method that that will make make more
27:58 - even sense okay so that will make more
28:01 - even sense so here is your vector a here
28:05 - is your vector a and here is your vector
28:08 - v here is your vector v okay here is
28:11 - your vector v you want to add this up so
28:14 - what do you do you simply you simply
28:16 - what you what you don't do anything
28:19 - extra you simply attach this to make a
28:22 - triangle to make a triangle like
28:25 - this and then this
28:28 - resulting Vector is your addition of the
28:31 - two vectors okay so this the resulting
28:34 - Vector which is which I'm going to
28:35 - highlight with this is your resulting
28:38 - vector and this is this is the this this
28:40 - this is how you do the addition
28:42 - geometrically speaking okay so this this
28:44 - is called the triangle method this is
28:46 - called the triangle method for addition
28:49 - of two vectors okay try to play with it
28:52 - a much much more better way so that it
28:55 - could make sense to you as well okay so
28:57 - try try try to play with it try to draw
28:59 - some diagrams of it and then show and
29:02 - then try try try
29:04 - to little bit juggle with it and then
29:06 - then and then you will better better
29:08 - understanding rather than uh just seeing
29:11 - okay so you can you will be getting some
29:13 - assignments on this as well so you can
29:14 - approach the assignments uh in
29:16 - geometrically speaking okay so so let's
29:19 - see one more operations which is the
29:22 - vector subtraction Vector subtraction
29:26 - okay so I'm just just going to show you
29:28 - Vector subtraction subtraction so what
29:31 - this Vector sub subtraction will do so
29:33 - let's say you want to subtract Vector U
29:36 - minus minus vctor V okay so that will be
29:40 - simply we can frame it as an addition of
29:42 - a vector U plus minus B okay so then it
29:47 - will be much easier to show it
29:49 - geometrically speaking okay got it so
29:53 - what I'm going to say over here is you
29:56 - have a vector so you can simply do like
29:58 - this for showing it geometrically now it
30:00 - will very easy to show it geometrically
30:02 - like this by adding of the vector okay
30:05 - so what what what what you do you just
30:07 - make this and then just for for showing
30:10 - it geometrically speaking okay so for
30:13 - example you have a vector U which is 2x3
30:17 - and we have a vector and and you have
30:19 - vector v which is 1x 1 subtracted so
30:23 - resulting Vector the resulting Vector
30:25 - will be 2 - 1 and 3 - 1 that will be
30:29 - nothing but equals to 2 okay it's 1 2
30:33 - okay so X component y component so this
30:36 - is your resulting Vector okay so let's
30:39 - let's see how it looks like uh so it it
30:42 - would make more sense to you as well
30:45 - okay so let's assume 1 2 3 okay 1 1 okay
30:52 - so let's plot the U Vector so I'm just
30:54 - just going to plot the U Vector which is
30:57 - 23 okay okay so okay it's two now so
31:00 - it's 2
31:01 - three so okay I think I done wrong it's
31:05 - 2 three so just going to okay so this is
31:09 - your vector U so this is your vector U
31:12 - this this this is your vector U and you
31:14 - want to plot the vector v v so this is
31:17 - your vector v this this is your vector v
31:20 - okay this is your vector v and the
31:22 - resulting Vector is 1 by 2 okay so this
31:25 - is your resulting Vector which is your
31:28 - after subtracting okay so after you
31:30 - subtract uh uh this from this the vector
31:35 - v from U okay so that will be your
31:38 - resulting Vector that makes sense
31:40 - geometrically speaking as well okay so
31:42 - this is how you do the vector
31:44 - subtraction geometrically if you don't
31:46 - understand geometrically it's no worries
31:48 - but it's not more than tough which which
31:50 - which is is very very easy not more than
31:52 - tough okay so this is your vector
31:54 - addition and Vector subtraction so let's
31:56 - see the last concept which you which I
31:58 - to make you familiar with is is is
32:01 - Vector scalar multiplication Vector
32:04 - scalar multiplication Vector scalar
32:07 - multiplication so I'm just but but first
32:09 - of all what is in scalar what is an
32:12 - scalar so scalers are give scalar such
32:15 - as constant or a numbers for for for
32:18 - example four is in scalar two is in
32:20 - scalar one is in scaler or or or or
32:23 - anything okay so this is scalers are
32:25 - just constant okay so it is it's a
32:28 - number but in algebra or linear algebra
32:31 - term we call it as a scalar okay so it
32:34 - plays an important role when we stud
32:36 - about L linear combinations or linear
32:39 - Transformations it plays a very
32:41 - important role so so so this is so what
32:44 - if if you multiply a scalar for example
32:48 - you have you want to multiply scalar a
32:50 - times the vector U you times the vector
32:53 - U so let's let's take the you you take
32:55 - the scalar a to be two and the Vector U
32:58 - as a 2x2 okay so multiply 2 * 2x 2 so it
33:04 - 2x2 so it do the element wise Product 2
33:07 - * 2 4 by 4 that will be equals to 2 * 1
33:11 - okay so Dimensions will be 2 * 1 that
33:14 - that is simply the vector scalar
33:16 - multiplication so what it actually does
33:18 - in geometrically speaking it stretches
33:20 - the vector so it doubles the vector so
33:22 - for example you have this Vector 2x two
33:25 - okay and then you multiply with two then
33:27 - it will be doubled then then it will be
33:29 - stretched okay then then it will be
33:31 - stretched means transformed the vector
33:34 - okay or stretched the vector by by
33:37 - following the linear structure it
33:39 - stretched the vector like like this so
33:41 - this this this was your initial Vector
33:43 - so after applying the a times the vector
33:46 - U that is stretch which is your final
33:48 - Vector after applying your uh the scalar
33:50 - and Vector multiplication okay so this
33:52 - is your stretch stretched Vector so
33:54 - please please please please ensure that
33:56 - it's stretched so before applying it was
33:58 - U and after applying is doubled okay so
34:01 - this is this is this is the vector
34:02 - scalar multiplication and the reason why
34:04 - we are studying these because this helps
34:06 - to build a very good foundation the
34:08 - stretching the geometrical speaking this
34:10 - helps a very good foundation when when
34:13 - we talk about Transformations con
34:15 - combination combinations uh igen values
34:18 - I vectors these these plays an important
34:20 - role in that so so this was uh so so so
34:24 - we have seen a bit about vector and
34:26 - scalar multiplication the last concept
34:28 - the last two concept which I want to
34:30 - introduce to you is the unit vectors is
34:33 - the unit vectors is the unit vectors and
34:36 - the zero Vector okay so the unit Vector
34:40 - so unit Vector is any Vector with a
34:43 - length one so so the definition States
34:45 - the unit Vector the unit Vector is any
34:49 - Vector is any Vector for for example any
34:52 - Vector okay any Vector with length one
34:55 - whose length is whose length is one
34:58 - whose
34:59 - length is one that's a unit Vector okay
35:03 - what is a Zero's Vector Z's Vector is
35:07 - whose length is zero whose length is
35:09 - whose length is whose length is zero
35:13 - whose length is zero okay so that is the
35:17 - zero Vector you usually denote the z z
35:20 - Vector in very bold way okay that is the
35:22 - zero vector and that is the unit vector
35:25 - vector and vector
35:28 - okay so these are the two basic basic
35:30 - very very basic ter terminology which
35:32 - you need to know about okay so so we
35:35 - have seen a lot about our vectors in
35:37 - this video so I hope that you understood
35:40 - every Everything whatever we had have
35:42 - have have a talk on this so just just
35:44 - just to make sure that everyone
35:46 - understood this so what what what you
35:48 - actually do so let me show you one one
35:50 - more example of that so so for example
35:53 - you have this okay so you have this
35:55 - Vector a and you have this vector and
35:58 - you have this Vector B okay so what do
36:01 - you do for addition of these two vectors
36:03 - you can just you can what you can do you
36:04 - can just uh make this make this a
36:08 - triangle okay but let's approach within
36:10 - using a parallelogram method okay so
36:13 - what do you do you put you take this you
36:15 - take this vector and put on the this
36:17 - tail out of the head and then you just
36:19 - draw a parallel to this and you take
36:21 - take this and then you take this and
36:23 - then you join the and this this A+ B is
36:27 - a result in Vector okay so this is how
36:30 - you go further into approaching these
36:32 - stuffs and I hope and maybe you can
36:34 - solve it using a triangle method to show
36:37 - you how it works geometrically
36:40 - speaking okay so I hope that you
36:42 - understood whatever I'm trying to convey
36:44 - you over here okay so I hope that you
36:46 - understood geometric intuition the
36:48 - triangle method what the unit what are
36:50 - zeros what are scalers what are what are
36:53 - what are vectors and etc etc etc okay so
36:56 - I think that we are done with this
36:57 - lecture um on it's 30 minutes so I hope
37:00 - that you understood vectors what are
37:02 - vectors uh so you you can find the notes
37:05 - of this the lecture notes maybe all of
37:07 - these in the description round box below
37:09 - uh the the or in LMS and the assignments
37:12 - will will be also related to this will
37:14 - will be released at the end of this week
37:16 - and I hope and I really really really
37:18 - hope that you understood this and if if
37:21 - not please feel free to ask the question
37:23 - in description down box below or in our
37:25 - Discord server we are we will be very
37:27 - really happy to answer your questions
37:29 - the next announcement is you can simply
37:31 - uh the next the next lecture will will
37:33 - be based on matrices okay so we'll talk
37:36 - about a bit about matrices okay and then
37:38 - and then we'll talk about after after
37:40 - completing a bit of mat matrices we'll
37:42 - talk about linear combinations then
37:45 - trans linear Transformations okay so we
37:48 - will be studying these things so don't
37:50 - worry we'll gohe very slow pie and very
37:52 - easy way okay so thanks for seeing this
37:54 - video I'll be catching up you in the
37:57 - next video till then bye-bye and have a
37:59 - good
38:00 - [Music]
38:10 - day hey everyone in this lecture we'll
38:13 - be talking about mates in our pre
38:15 - previous lecture we talked about vectors
38:17 - and I really really really hope that tat
38:19 - to understood about vectors I know this
38:22 - these are very very easy concept for you
38:25 - but but uh let me tell you these sets a
38:28 - foundation when we study about
38:30 - combinations or Transformations and
38:33 - other other stuffs so that's how we are
38:35 - prely focusing on this so from this
38:37 - video we'll be starting stepping up a
38:39 - bit difficult note from matrices and
38:41 - then we'll talking about some some
38:43 - matrices operations and then we'll talk
38:45 - about some properties of matrices
38:48 - multiplication and then I will just end
38:50 - up with this video with Matrix Vector
38:52 - products and then I will show you the
38:54 - wide results at the end of the video
38:57 - that the linear combination of the
38:59 - column Vector okay so we'll be talking
39:01 - about I'll just introducing a notion of
39:03 - a linear combination so that in the next
39:05 - video is totally based upon your linear
39:08 - combination so that's why I will just
39:09 - give you a taste of linear combination
39:12 - at the end of the video so let's get
39:15 - started with matrices so today we'll be
39:17 - talking about matrices so let's let's
39:20 - recall a bit about vectors so so vectors
39:23 - are uh n dimensional where where it can
39:26 - have n and where where it can have n
39:29 - number of rows but only one column okay
39:31 - so we were having this it can have one
39:34 - two all the way around to the n and here
39:38 - this is n * 1 so the shape of this
39:40 - vector v the shape of this vector v is n
39:44 - * 1 and it can have n number of a rows
39:47 - and one number of a column and this is a
39:49 - vector specifically we can call this as
39:52 - a column Vector okay so so so we
39:56 - specifically call this as a column
39:57 - Vector okay so so it is given a new name
40:00 - and when we when I will introduce you a
40:02 - notion of a matrices then then we will
40:05 - use extensively in the in the later
40:08 - videos but but this is also called the
40:11 - column vector and and for example your
40:13 - vector can be in this 1 2 3 all the way
40:17 - around to the end okay so this is called
40:20 - the row Vector this is called the row
40:23 - Vector okay okay so this is called the
40:26 - row Vector so this for this these are
40:28 - the two two things which I want to in
40:29 - introduce to you so we'll be covering
40:31 - this again just after we complete the
40:33 - matrices okay so let's start with what
40:36 - are matrices so matrices are a are a set
40:40 - of numbers or or a multi-dimensional l
40:43 - okay where it can have n number of rows
40:46 - or n number of rows and M number of
40:48 - columns it can have a multiple columns
40:50 - and multiple rows okay so in vectors we
40:54 - we in vectors in vectors we were have
40:57 - having only we were having only n n rows
41:01 - and one column which is the example of
41:04 - the example will be 1 2 3 okay so this
41:09 - this this is an example of a vector v
41:11 - okay so the matrices so the
41:14 - matrices can have can have n number of
41:17 - rows as usual but n number of columns
41:20 - but can have M number for column so as
41:23 - an example we can make that Matrix a
41:26 - equals to 1 2 three so one column 2 3 4
41:30 - second column 3 4 6 third column so it
41:34 - can have M number of M number of a row M
41:37 - number of columns and and and N number
41:41 - of a rows and N number of a rows okay so
41:44 - here the shape of this a is 3x 3 uh
41:50 - where three is number of a rows number
41:52 - of a rows and this one is number of a
41:56 - columns number of columns okay so so
41:59 - over here this this first first indate
42:01 - the 3X3 Matrix so this is the example is
42:04 - 3x3 Matrix so the formal definition of a
42:08 - matrix is is matrices are a set of
42:12 - numbers okay mates are a set of numbers
42:16 - arranged in a rows and a columns which
42:20 - is n rows and N columns so to form a
42:23 - rectangular array okay so here here it
42:27 - on the rectangular array in other words
42:30 - matrices can have n number of columns
42:32 - and M number of rows okay so let let me
42:35 - write a formal definition of matrices
42:37 - over here so and the definition which is
42:42 - just let me write the a good definition
42:44 - of this so that everyone can can Define
42:47 - what a matrix is so
42:50 - matri so here's the definition of a
42:53 - matrices where it it it is arranged in a
42:56 - rows and a column so I'm going to give
42:58 - the name of a rows going to give the
43:00 - name of a rows to n and columns to M so
43:04 - to form a rectangular l so for for for
43:06 - example we can have here it can have a b
43:10 - c d e f g h i okay so it's can have any
43:15 - here we have this is this this is an
43:16 - example of 3x3 Matrix where we already
43:18 - have one two three rows and one two
43:23 - three columns okay or we can say in
43:25 - other words it's going to have n number
43:27 - of rows and N number of columns M number
43:29 - of columns okay so I'm again I'm saying
43:32 - n is for number of rows number of rows
43:36 - and M is for number of columns number of
43:39 - columns all those are in small small
43:42 - letters okay so this is your form formal
43:45 - definition of a matris and the lecture
43:46 - notes is in description box below please
43:49 - go there and assess your lecture notes
43:51 - for you to better to to just revise in
43:54 - the meantime Okay so
43:57 - so so just let's write a formal notation
44:00 - of how the maor matrices are so that so
44:03 - that it it is uh easily inter
44:06 - interpretable so I'm going to make a
44:07 - matrix a going to make a matrix a where
44:11 - I'm going to make a matrix a where it
44:13 - I'm going to make this
44:16 - a11 so the how do we assess the the
44:19 - first element so here it is in first row
44:21 - and First Column so this a is in first
44:24 - row and First Column so that's why I
44:26 - written I and Z okay so a i and Z
44:30 - indicates I what is the row number and g
44:33 - z what is the column number so for
44:36 - example for assessing the elements so
44:38 - over here you have this you have this
44:40 - Matrix we have this Matrix a and what
44:42 - you going to do is assess three so how
44:45 - do you want to assess so the you the
44:47 - formal the formal assessing things is a
44:50 - i j or or yeah so a i j where I
44:54 - indicates the row number and J indicates
44:56 - the the column numb okay so over here
44:58 - you can see the three is on is on we
45:02 - start with 1 2 3 okay not from zero so 2
45:06 - 2 A 2 means a A2 means the row number is
45:10 - two and the column number is also two
45:13 - column number is two that is nothing but
45:14 - equals to three okay so here's how you
45:16 - assist so first of all you write the row
45:18 - number then you write the column number
45:20 - so I indicates the row number and J
45:24 - indicates the column number okay so I
45:26 - hope that you understood what I'm trying
45:28 - to say so it is telling go and this is
45:30 - the first element where is first row and
45:32 - First Column then it is second first row
45:35 - and second column then uh all the way
45:37 - down to the first row and N column okay
45:41 - or M column okay and over here it can
45:45 - have 8 2 1 okay so second row First
45:47 - Column second row second column all the
45:50 - way down to the A2 m m second row M
45:54 - column okay uh so
45:58 - uh this can be
45:59 - a31
46:02 - a32
46:03 - a3m here all the way around to the a N1
46:08 - so n is number of rows or a N2 all the
46:13 - way down to
46:15 - the all the way down to the a n m okay
46:19 - so here it is the formal notation or or
46:24 - a definition which we can write over
46:25 - here which is the formal notation for
46:27 - writing uh so so this is a matrix okay
46:30 - so here's how I developed this so it can
46:32 - have M number M number of a column A
46:36 - rows n number of rows so 1 2 3 4 all the
46:41 - way down to the N which is n number of
46:42 - rows and it can have only it can have
46:45 - only M number of columns okay it can
46:48 - have only M number of columns so this is
46:50 - your formal formal formal definition
46:53 - which you have which I have given to you
46:55 - for uh matrices okay so I hope that you
46:58 - understood now what I'm going to talk
46:59 - about is I introduced a notion of a row
47:02 - vector or a column Vector is it so I
47:05 - introduce you so can you just go and
47:07 - just type me what is a row vector and
47:11 - what is a column Vector so let's take
47:13 - one example so let's take one example is
47:16 - you have a you have a matrix but just
47:19 - just just one thing that the vectors are
47:21 - subset of matrices okay so the the the
47:24 - vectors are a n * 1 Matrix or n * 1
47:29 - matrices okay so the vectors are a
47:31 - subset of matrices okay so if you if you
47:34 - extract this extract this extract this
47:36 - row that this is just a vector okay so
47:40 - what I'm going to do now is uh is going
47:43 - to just make a vector make a matrix a
47:45 - make a matrix a which contains just just
47:48 - don't relate I'm just taking examples
47:50 - I'm just taking examples you have a
47:53 - vector a I'm going to store 1 2 3 4 5 6
47:57 - okay so this is my this is my 2x3 Matrix
48:01 - okay so so what you do you take the
48:04 - first row okay and then stored in
48:07 - another so C okay that is 1 2 3 okay so
48:13 - what is C over here C is called the the
48:16 - the the the the the Matrix with one row
48:20 - okay the Matrix with one row is called
48:22 - the row Vector this is called the row
48:25 - Vector this is called called the row
48:27 - Vector The Matrix which has only one row
48:31 - is called row vector and the Matrix with
48:34 - only one colum which is nothing but
48:35 - called a column Vector okay so so so if
48:39 - we if you take this
48:41 - 1x4 okay in D that is 1x 4 it has only
48:45 - one column that is nothing but equals to
48:47 - column Vector which is nothing but is
48:50 - column Vector okay I I hope that you are
48:55 - understanding whatever I'm trying trying
48:56 - to tell here so so these are the size of
48:58 - a matrix over of size of a matrix where
49:01 - we have row vector and what is row
49:03 - Vector row Vector are nothing but the
49:05 - Matrix with one row with one row is
49:08 - called the row vector and the matrices
49:11 - with one column is called the column
49:13 - Vector okay so if you take one example
49:15 - so just just just just assume that this
49:17 - is your this is so this is your first of
49:20 - all row Vector sorry column column
49:22 - Vector because you have uh you have so
49:25 - so you have sorry column vector so this
49:27 - this this this one is a column Vector
49:29 - which is V1 so V V1 over here is a
49:31 - column Vector okay so if we take this
49:33 - one if we take this one so here you have
49:36 - only one row so that is B2 which is your
49:40 - row Vector okay so this is this is what
49:43 - the notation of the notion of a row row
49:45 - vector and column Vector means and I
49:47 - really really hope that you understood
49:49 - about this okay so if not please please
49:51 - feel free to ask ask a question below
49:53 - where you're stu please please please
49:55 - use Discord server or whatever that's
49:57 - the doubt support which is provided to
49:59 - you so that you can get most out of out
50:02 - of this course and if you need any
50:03 - guidance for absolutely free please feel
50:05 - free to reach out to me via email
50:07 - Discord comment we can get on a meet to
50:11 - help you solve the doubts it's it's for
50:14 - Okay cool so let's so we have we have
50:18 - seen what some matrices are so just just
50:20 - want to Recaps youate everything so
50:22 - matrices are a set of numbers arranged
50:24 - in a number of rows and number of column
50:26 - s to form a rectangular array where it
50:29 - can have an m n number for rows and M
50:31 - for mango number for columns okay so so
50:34 - the notation for for the definition of
50:37 - geometrically over here is the a so I I
50:40 - have made this as an example the show
50:41 - showcase shoe and we have we have seen
50:44 - some of the Matrix size where you where
50:46 - the the the terminology which is where
50:48 - the Matrix has only one row that's the
50:50 - row vector and with the Matrix has only
50:53 - one column that is a column Vector okay
50:56 - so I hope that you understood what I'm
50:57 - trying to convey over here cool so now
51:00 - let's talk about so now let's talk about
51:03 - so now let's talk about some of the
51:06 - operations because in pre previous video
51:07 - we talked about vectors and then
51:09 - operations so the same day I'm going to
51:11 - talk about operations on a matrices okay
51:15 - so operations on matrices just just just
51:18 - just just going to show you show it to
51:19 - you operations
51:22 - operations I think my handwriting is not
51:24 - good too much no problem
51:28 - operations
51:30 - operations uh on matrices okay so you
51:33 - want to do the operations on matrices so
51:36 - the first operation which you're going
51:38 - to do is Matrix and a scalar
51:42 - multiplication okay so what I'm going to
51:44 - do is
51:47 - Matrix
51:49 - Matrix
51:51 - scalar multiplication okay not a not a
51:54 - big deal it's very very easy to
51:56 - understand okay so assume that you have
51:59 - a matrix a you have a matrix
52:02 - a you have a matrix a 10 6 4 3 which is
52:09 - your nothing but a 2x2
52:11 - matrix okay 2 rows and two
52:13 - columns okay and you have a scalar you
52:16 - have a scalar a which is nothing but two
52:19 - okay so you're given these two now what
52:21 - you want to do is uh you want to
52:24 - multiply uh a a
52:27 - okay so we multiply a scaler with a
52:29 - matrix which nothing but 2 * 10 6
52:34 - 43 okay that is so what will be the
52:38 - result please anyone please please
52:40 - please feel free to to pause this video
52:42 - so what you're going to do is
52:44 - multiplying a scaler with a 2x2 matrix
52:48 - so what will be the result anyone please
52:50 - please please in the comment okay how
52:53 - how do you tell okay no problem but
52:55 - please please free to ask say in the
52:57 - comment box I will be very happy to see
52:58 - the if you're still here so the answer
53:01 - of this is first of all what do you do
53:03 - you you simply do the element wise
53:05 - product with this scalar okay so 2 * 10
53:09 - which is nothing but 20 which is nothing
53:11 - but 20 2 * 6 which is nothing but 12 2 *
53:16 - 4 what it is 9 no it's 8 okay 2 * 3
53:20 - which is nothing but six okay so the
53:23 - resulting the resulting Matrix is 2 * 2
53:26 - Matrix so what you do you transform or
53:30 - or or not a transform I would say you
53:32 - just multiply a scalar with a a matrix
53:35 - and and then you get a 2x2 matrix which
53:38 - is the same size of your a okay so 2 * 2
53:42 - = to 2 * 2 which result when you
53:45 - multiply with any scaler okay so what it
53:48 - does is simply do the element wise produ
53:50 - product okay so the so the formul
53:52 - notation for this is you you multiply C
53:56 - with with
53:57 - a okay and a have I and J row okay so
54:02 - what it will do it will simply M do the
54:05 - element wise a c * a i j that is nothing
54:08 - but what it will do first of all it will
54:10 - uh so in more more notational terms it
54:14 - will just for for for example you have a
54:16 - vector you have a a scaled a and you
54:19 - multiply with c d e f okay so what do
54:24 - you do you simply uh
54:26 - a * C A * d a * E and A * F okay so this
54:35 - this will be the form this is this this
54:37 - will thing and then you'll be getting
54:39 - some values 2 by two okay which with
54:42 - your values okay the same this is this
54:44 - is what it is doing so this is your
54:46 - formal definition formal definition of
54:49 - your Matrix Vector multi multiplication
54:51 - sorry uh scalar matrix multiplication
54:54 - and I really really hope that you
54:55 - understood this let's go on to the next
54:57 - operation which want to see let's go on
54:59 - the next operation which want to see is
55:02 - addition of our matrices okay so the
55:05 - next operation which we are going to
55:06 - cover is addition addition of matrices
55:13 - okay additions of a matrices so let's
55:16 - Zoom so we are you are you are given a
55:18 - matrix a you give a matrix a which is 1
55:22 - 3 1 uh 1 0 0
55:26 - okay and you have and you are given
55:29 - Matrix B okay and Matrix are always
55:31 - written in a capital letters make sure
55:34 - and the vectors are always in small
55:36 - letters okay with one over here uh and
55:39 - the scalers are also in like this yeah
55:42 - so that's B is
55:46 - 05
55:48 - 75 okay so what you want to do you want
55:50 - to add these two matrices A + B add this
55:55 - two matrices so this is the operations
55:57 - which you want to do this is the
55:58 - operation which you want to perform so
56:01 - how how are you going to perform this
56:03 - operation how are you going to perform
56:05 - this operation so for performing this
56:09 - operation so for performing this
56:11 - operation you will just what what you
56:13 - will do you wanted to just have this 13
56:17 - 1 1 0 0 plus 05
56:23 - 750 what you what you will do you will
56:26 - nothing but uh 1 +
56:30 - 0 at element y some okay in scaler you
56:34 - doing so you will do the same 1 + 0 3 +
56:38 - 0 1 + 5 okay element wise sum okay 1 + 7
56:45 - 0 + 5 0 +
56:49 - 0 okay so this is this this is what you
56:52 - do and then you'll be getting your
56:54 - answer which is 1 3 6 6 and then you
56:57 - getting 8 5 0 that is a resulting U
57:02 - Matrix okay which is also 3x3 Matrix
57:06 - okay so what you do you simply do do
57:08 - this and you're getting a 3X3 Matrix
57:10 - okay so so when you what you do you you
57:13 - just added a 3X3 Matrix 3x3 Matrix and
57:17 - then and then you and the resulting
57:19 - Matrix is also 3x3 okay so so this is
57:23 - how you do the addition of a matrix and
57:24 - the formal definition for this I which
57:26 - which which which I can state so because
57:28 - definition is very very important for
57:30 - fundamentals for for making your
57:32 - fundamental
57:33 - strong okay so the definition is your
57:36 - given a matrix a which can have
57:39 - a uh b c d e f okay that is nothing and
57:46 - your B is also some some kind of k g i h
57:55 - o p okay so add these two so you what
57:59 - what you will do a + k b + G C + I D+ D
58:05 - + h e + o e + O and F + P that will be
58:12 - nothing but equals
58:14 - to 3x3 Matrix okay so I have just taken
58:18 - example your your Matrix can have n
58:21 - dimensional the can have any number of
58:23 - rows and any number of columns but there
58:25 - are some constraints which you you need
58:26 - to there are some properties like
58:28 - Dimension property which you have to
58:30 - take care while adding the matrices okay
58:32 - so it can have it can have uh ABC you
58:34 - can it can have a 10 x 10 matrix the
58:37 - size of the Matrix and then you are
58:38 - adding the 10 x 10 matrix with another
58:40 - 10 by 10 matrix and that would resulting
58:42 - in another 10 by 10 metri okay so that
58:45 - is the that is the thing uh so some of
58:47 - the property which I want to highlight
58:48 - which you all have to focus on so some
58:53 - of the properties of a very very very
58:54 - very uh simp simp simple property that
58:57 - commutative property commutative
58:59 - property so addition of a matrix is
59:01 - commutative
59:02 - commutative and all of these is written
59:05 - in your notes please please please feel
59:07 - free to write see from there if you if
59:10 - you wanted to just just just revise it
59:12 - up okay uh in in future but I would
59:15 - highly highly recommend to complete this
59:16 - video V plus so you so you have a matx
59:19 - say you have a matrix you can do B+ a
59:22 - okay uh the next thing is associated
59:24 - property your your your your addition of
59:26 - a matrices are associ associative as
59:29 - well okay so associative
59:33 - associative property associated property
59:36 - I'm not writing a lot of properties over
59:37 - here but the one who are important I'm
59:40 - writing over here A +
59:42 - B+ C which is nothing but equals to A +
59:46 - B plus C and all of these can be
59:50 - proved very very easy the proof is very
59:52 - very easy not a hard please feel free to
59:55 - search on internet about the proof it is
59:57 - very very easy associative commutative
59:59 - it's it's the proof are available on
60:00 - internet okay so so the last thing what
60:03 - I'm which the first property is
60:05 - commutative the second is associative
60:07 - the last one is dimension property the
60:09 - dimensions are be the dimensions the
60:13 - dimensions the dimensions of the
60:16 - dimensions of your of your uh uh the two
60:21 - two two Matrix would be same The Matrix
60:24 - the matrices would be same the matri
60:26 - should be same okay the dimensions okay
60:29 - if it is not then then it will be
60:30 - undefined your operations will be
60:32 - undefined Okay cool so so now we have we
60:37 - have we have talked about one one of the
60:38 - operation which is addition of a matrix
60:40 - and I really really hope that you
60:41 - understood addition you you understood a
60:43 - scaler but one thing which I'm going to
60:45 - spend some some two minutes talking
60:47 - about is you may be thinking yeah you do
60:50 - we really really need to know about
60:51 - these stuffs uh so I would say yeah you
60:54 - need to know about although you don't
60:56 - need to just worry about how I'm going
60:59 - to code it you can actually develop it
61:01 - from very very scratch not a big deal
61:03 - but there are some libraries like
61:05 - numai which would be in this course
61:08 - we'll be using in this course or or
61:09 - pytorch that'll be doing using uh the LI
61:13 - library for S computation for addition
61:15 - of a matrix for multiplication of a
61:16 - matrix which they handle which they are
61:19 - very efficient okay because in real
61:21 - world your Matrix are not 3x3 Matrix
61:24 - they are they are they are billions size
61:26 - size is millions okay Millions by
61:28 - millions so so they are they are very
61:30 - very large so so so Maj mag
61:33 - multiplication with your own for Loops
61:34 - are very very time taken okay so that's
61:38 - that's that that's a big deal okay so
61:40 - your your time complexity will increase
61:42 - as your input size increases okay so
61:44 - that's a big deal so you we are learning
61:46 - this to understand the inner workings of
61:48 - our of our function so that we can we
61:50 - can know how how our algorithm is doing
61:54 - and how everything is working behind so
61:56 - that it it becomes very easy to debug
61:58 - something or you get some error or or or
62:00 - to to have very good or decent knowledge
62:02 - of what your code is performing Okay
62:04 - cool so addition of a matrix is also
62:06 - done now let's you can do you can do the
62:08 - same with sub subtraction of a matrices
62:10 - please try it out by your own the next
62:12 - thing which which I'm going to spend
62:13 - some time talking about is matrix
62:16 - multiplication okay bit bit I'll spend
62:20 - some time talking on this okay so the
62:23 - first the next next thing which I'm
62:25 - going to to talk about is
62:30 - Matrix uh Matrix Matrix multiplication
62:34 - Matrix
62:36 - Matrix multiplication okay so this is
62:39 - your uh next operation which is which is
62:41 - one of the most important important uh
62:44 - important what do you say uh the
62:46 - operations which you which you need to
62:47 - learn okay so you given a matrix a
62:50 - you're given a matrix a I'm going to
62:52 - take very easy example 1 7 2
62:57 - 4 okay and you're given a matrix B given
63:00 - a matrix B which is 3 5 3 2 okay now you
63:07 - need to now what you need to do you want
63:09 - to multiply Matrix a matrix B okay so
63:13 - how do you do you may be thinking here
63:16 - you here is 2x two here is 2x two is 2
63:20 - by 2 1 * 3 7 * 3 2 * 5 that's that's
63:25 - that's that's that's not how you do okay
63:27 - so the matrix multiplication the way you
63:29 - do is like this you take the first row
63:34 - of that a matrix you take the first row
63:37 - I'm going to change my pen you take the
63:39 - first row of that a matrix and multiply
63:42 - with the multiply with the First Column
63:45 - of the b b Matrix multiply with the
63:48 - First Column with the B Matrix okay so
63:51 - so here's here's how you do so resulting
63:54 - resulting M Matrix c will
63:56 - 1 * 3 okay
63:59 - plus taking the dot product of
64:03 - your of your row Vector times the column
64:07 - Vector so what you what you are actually
64:08 - doing is taking out the dot product dot
64:12 - product which you'll see in our later
64:14 - videos don't don't worry about that dot
64:16 - product of this this is your because if
64:19 - if you see this is your it this it it
64:22 - has only one row okay so of a vector
64:26 - of a vector of a of a vector of a row
64:29 - Vector of a row row vector and column
64:33 - Vector column Vector so this this is
64:34 - your column Vector where it has only one
64:36 - column so specifically you're taking out
64:38 - a DOT product of a row vector and a
64:42 - column Vector what is dot product so dot
64:45 - product is element one so what what you
64:47 - do you simply multiply and add it up but
64:50 - element wise adding and add it up okay
64:53 - so over here what what what you want to
64:54 - do you one * 3 and 7 * 5 1 * 3 7 * 5 and
64:59 - then you will add it okay now take this
65:02 - again this row again this row with this
65:04 - row row vector and multiply with this
65:07 - column Vector multiply with this column
65:10 - Vector so that that will be nothing but
65:12 - equals to 1 * 3 + 7 * 2 okay now what do
65:19 - you do now what do you do you you now
65:21 - you have this The Taking of the dot
65:23 - product of row row vector and the second
65:25 - column VOR which is V2 okay now what do
65:27 - you do you go go go further into this
65:31 - the second row Vector which is 2x4 and
65:33 - then you do the same 2 * 3 dot product
65:36 - of the 2 4 with row Vector with the
65:39 - column Vector okay 4 * 5 okay and then 2
65:43 - * 3 4 * 5 2 * 3 2 * 2 okay so 2 * 3 + 4
65:50 - * 2 okay that will be nothing but equals
65:53 - to C which is nothing equals to uh 1 * 3
65:58 - which which will be how much 1 1 * 3
66:00 - which will be 3
66:02 - +
66:04 - 5 35s okay 1 * 3 3 + 14 2 * 3 6 + uh 20
66:14 - which is 26 2 * 3 6 + 8 which is nothing
66:18 - but
66:20 - 14 okay
66:26 - cool so this is your resulting and then
66:28 - what you do you simply make
66:31 - your
66:34 - 38 17 26 14 will be your resulting
66:39 - Vector which is 2x2 matrix which is the
66:41 - 2x2 matrix okay so when you multiply
66:44 - with this you will be getting uh your
66:46 - favorite the after after multiplication
66:48 - of the Matrix this this is your answer
66:50 - of this of your particular
66:53 - question got it and I really really hope
66:55 - that you are that you understanding what
66:57 - I'm what what whatever I'm trying to say
66:59 - but you it may you you can use you it
67:02 - can have any dimensional but some
67:04 - properties are there okay so let's let's
67:07 - visit the property I'm just going to
67:09 - constraint that Dimension property is
67:11 - very important in this so what
67:13 - dimensions should match to be so that
67:15 - the matrix multiplication is not
67:17 - undefined okay so so the properties of
67:20 - for MRI multiplication so the first
67:22 - property which I'm going to talk about
67:24 - property the first property is for for
67:27 - example you're given a matrix a b and c
67:31 - so these are three Matrix which are n
67:34 - byn Matrix which are n by n Matrix where
67:38 - you have n n byn Matrix okay so where
67:42 - you have n rows and N columns okay so
67:46 - the first thing which holds is
67:48 - commutative property of multiplication
67:50 - of this multiplication does does not
67:52 - hold okay so M multiplication is not
67:54 - commutative
67:56 - is not commutative is
67:59 - not is not okay so when you multiply a b
68:04 - ba a which is which will be totally
68:06 - wrong it can be proved it can prove it
68:10 - it can prove very easily then the next
68:13 - property associative property of matrix
68:16 - multiplication is is is there okay so
68:19 - associative property
68:21 - associative associative property is
68:24 - there okay so a b + C which is nothing
68:28 - but uh a okay so I think it's I have
68:32 - written for this distributive I written
68:35 - for
68:37 - a c which is nothing but equals to A B C
68:41 - okay it can prove rigorously and of
68:43 - course you're multiplying it over here
68:45 - okay it can it can prove very very
68:46 - easily it is distributive property
68:48 - distributive
68:50 - property this this it is also
68:52 - distributive property so what you want
68:54 - to do a B+ C that that will be A+ a c
68:59 - and you can prove these you can prove
69:02 - this very very easily which which can be
69:04 - found on internet the next thing is the
69:07 - most important Dimension
69:10 - property Dimension
69:12 - property Dimension property so the
69:16 - dimension property is you can have M
69:19 - number of a rows you can have M number
69:21 - of rows you can have any number of rows
69:24 - but an N number of columns okay this is
69:26 - for the dimension of a matrix a your
69:29 - dimension of a matrix B should be your
69:31 - your your number of a rows should be
69:34 - same as uh number of columns in that
69:39 - Matrix okay times K any okay so your
69:44 - resulting will be M * K okay so so it
69:49 - makes sense as well if you have you can
69:50 - have M number of rows at the a a matrix
69:54 - but you can and N number of columns okay
69:58 - but with here you can have only n number
70:00 - of columns n number of a n number of
70:03 - rows sorry n number of rows okay so here
70:06 - you have two 2 by two so here it has two
70:10 - and the resulting will be the resulting
70:12 - will be 2 by 2 Vector sorry Matrix okay
70:17 - so that the the the resulting size will
70:19 - be this okay so this is your dimension
70:22 - property of your matrix multiplication
70:25 - cool so we have talked a lot about
70:27 - Matrix and and and and you seeing that
70:29 - you're are going and then and you are
70:31 - seeing that that you are going bit up
70:33 - bit bit little bit little bit up so the
70:35 - last thing which I will end this video
70:36 - which I promised you is is talking about
70:40 - Matrix Vector product is a matrix Vector
70:42 - multiplication okay so what I'm going to
70:45 - talk about is Matrix Vector
70:49 - multiplication yeah so let's let's let's
70:53 - let's do that then so let's do that so
70:56 - you have a matrix a you have a matrix a
71:00 - which is nothing but which is nothing
71:01 - but so I'm just just going to define
71:03 - rigorous very very definition of Matrix
71:05 - Vector product so I'm just just just
71:07 - going to write it very very fast a11 a12
71:12 - all the way around to the A1
71:14 - n and a12
71:18 - a22 A2
71:20 - n
71:22 - uh a M1 which is is N1
71:31 - okay
71:34 - N2 okay that be A and M okay so this is
71:40 - your Matrix this this is your Matrix so
71:44 - it is having M number of columns and N
71:46 - number of a rows okay this is your
71:48 - Matrix a and you want to multiply with
71:51 - multiply with uh a vector a column a row
71:55 - L row a column Vector okay
71:59 - X2 XM XM okay so to do the
72:03 - multiplication of it so how do we do it
72:05 - how you how you how you will will will
72:07 - you do it so you want to multiply a
72:10 - matrix which is a which is M * n Matrix
72:13 - times the vector a scalar uh sorry
72:15 - Vector col color Vector X which will be
72:18 - the definition will be so so what what
72:21 - it be the answer of
72:22 - this so how will you perform the
72:26 - so for performing the operations is
72:27 - nothing but equals to a11 X1 so what you
72:30 - are actually doing you can take this a11
72:33 - okay and you multiply this with this
72:36 - multiply this with this okay so what you
72:39 - actually doing what you what you are
72:41 - actually doing you're are multiplying
72:43 - you're multiplying the the column Vector
72:46 - the column sorry row Vector sorry row
72:49 - Vector to the column Vector okay element
72:52 - wise and adding it up okay so the dot
72:56 - product between
72:58 - your row vector and a column vector and
73:01 - adding it up okay so a11 plus you're
73:04 - adding plus over here see
73:06 - a12 X2 plus A1 3 X3 all the way around
73:12 - to the A1 M A1 M * x m XM okay then you
73:20 - do the same A1 2 X1 + a uh
73:26 - plus
73:29 - A2 all the way around to the
73:32 - A2 uh n okay so you're multiplying with
73:36 - X whatever the X so what what you're
73:38 - actually doing you're multiplying you're
73:39 - taking
73:41 - this column Vector taking this column
73:44 - Vector so row vector and multiplying
73:46 - with this so you are taking on the dot
73:47 - product between taking all the dot
73:50 - product taking all the dot Dr taking out
73:52 - the dot product and giving your answer
73:54 - that's it okay taking taking out the dot
73:56 - product of between the row vector and
73:58 - the column Vector so let's see with one
74:00 - of one of the example so it it it would
74:03 - make more sense so here you have - 3 0 3
74:06 - 2 1 7 - 1 9 okay and you have
74:12 - a vector column Vector 2 - 3 4 1 okay so
74:18 - what what will be the output so the
74:20 - answer will be uh minus 3 * so so we
74:24 - taking this you're taking this and
74:27 - multiplying with this okay - 3 * 2 + 0 *
74:36 - 3 okay - 3 it's minus + 3 * 4 + 2 *
74:44 - 1 it's yeah it's 1 okay now you go for
74:48 - the 1 * 2 1 * 2 + 7 * - 3 + - 1 * * 4 +
74:56 - 9 *
74:57 - 1 that will be nothing but equals to
75:00 - after after you add it everything after
75:03 - calculating okay after calculating that
75:06 - will be a and b which is 2x 2 2x 2 sorry
75:11 - 2x one vector so which is a column
75:13 - Vector which is 2x one so after after
75:16 - after doing the Matrix Vector you
75:18 - transform you what what you do you have
75:21 - this R4 R4 we is four dimensional vector
75:25 - you transform it to D after doing the
75:27 - after using this V using this Matrix say
75:29 - you transform it into a a and b which is
75:32 - the 2x1 okay which is from R4 to
75:37 - R2 okay so here you transformed using
75:40 - this Matrix a so so so we'll see in our
75:43 - four videos that matrix multiplication
75:46 - are a linear transformation okay so
75:50 - we'll see in our later videos but but
75:52 - now as of now I I hope that you
75:54 - understood the mutrix Vector product
75:56 - okay so in the next video which what
75:58 - I'll be showing you a wired thing over
76:00 - here or not a vired thing a very useful
76:02 - thing over here is the linear
76:06 - combination using the help of Matrix
76:08 - Vector product so I will take an example
76:10 - of matric vector product as a linear
76:13 - combination of uh of so that it would
76:15 - make more sense and then we'll complete
76:17 - the linear combination now in in our
76:19 - next video and then and then we'll end
76:22 - up uh this uh uh so we'll be completing
76:24 - and then then we talking about the
76:25 - linear transformation I really really
76:27 - hope that that you understood this I'll
76:29 - be catching up you in the next video
76:32 - till then bye-bye have a great day and
76:35 - and please please and one more thing
76:37 - attendance is you have to mark your
76:39 - attendance so please please feel you to
76:41 - do so bye-bye have have a great day
76:47 - [Music]
76:56 - okay so welcome to this lecture in this
76:58 - lecture we we'll be discussing about
77:00 - linear combination of a vector so and
77:03 - this is this is one of the most
77:04 - important concept as you will go further
77:07 - and you will understand okay so it sets
77:09 - up a very strong fundamentals to
77:12 - mathematically understand or to see the
77:14 - Deep learning or machine learning into a
77:17 - linear algebra point of view so this is
77:19 - one of the most important concept which
77:21 - we'll focus on so in this video we'll
77:22 - talk about that specifically so uh as so
77:26 - I'll in this video I'll just give you
77:28 - I'll be giving you some definition of
77:30 - linear combination and then I will
77:31 - giving you some some examples and then
77:33 - we're talking about a matrix Vector
77:35 - product as a l linear combination
77:38 - because in previous video at last we
77:39 - talked about Matrix Vector product so
77:42 - that's why we are in this video we'll be
77:43 - talking about the as an example taking
77:46 - that as an example for linear
77:48 - combination of our Matrix Vector problem
77:51 - okay so so the definition States so the
77:53 - definition of a linear combination
77:55 - States uh so let's let's do something
77:59 - let's start with an example let's start
78:02 - with an example and example States and
78:04 - an example States key that you have so
78:08 - first example that I want take is you
78:10 - have a scalar you have a scalar and you
78:13 - multiply with the sum Vector U okay and
78:16 - then you have and then plus two with
78:18 - some vector v okay and that will be some
78:21 - resulting Vector that will be some
78:23 - resulting vector which is just the scale
78:26 - version of u and v which is nothing but
78:29 - uh for example D okay so that is the
78:32 - resulting Vector of our uh after
78:35 - applying off first of all we we added it
78:37 - up sorry mult M multiplied and added the
78:40 - vector okay so that that will be the
78:43 - resulting Vector so the resulting Vector
78:46 - is a linear combination of vector U and
78:50 - a v okay so again listen me that what
78:54 - you we we have taken this example and
78:56 - and this in this example we have one
78:59 - scalar three and then we multiply with
79:01 - the vector U so for example we can have
79:03 - a vector U to to be 2x two so when when
79:07 - you multiply three times okay that would
79:11 - be nothing but 3 six six that would be
79:14 - the six okay 6 six okay plus you have
79:18 - some some Vector B it is nothing but 4 4
79:22 - okay so 2 * 4 so when when you multiply
79:25 - or do the scalar scalar Vector
79:28 - multiplication that will be simply
79:29 - element wise product so 2 * 4 which is 8
79:32 - and 2 * 4 which is 8 okay so when you uh
79:36 - now you add 8 by 8 okay so that will be
79:39 - 8 by 8 that will be nothing but 8 by 8
79:43 - okay so the resulting Vector will be 14
79:46 - by 14 okay so that the the resulting
79:49 - Vector will be 14 by 14 which is your D
79:52 - which is your vector D so this the
79:55 - vector D is a linear combination of your
79:58 - vector U of your vector U and of your
80:02 - vector v okay so the the the resulting
80:04 - Vector is a linear combination of these
80:08 - two Vector which is u and v so I hope
80:10 - that you that that you are understanding
80:12 - what what whatever I'm trying to tell
80:14 - and this 14 is just the the 6 + 8 which
80:18 - is okay so this is the D is a result
80:22 - resulting Vector uh after after you do
80:24 - so what you specifically done is
80:27 - multiply there is some scalar so there
80:29 - is some scaler so you are given any
80:32 - number of a vector so the I'm writing
80:34 - the definition so what you specifically
80:35 - done you're given you're given any
80:39 - number any number of any number of
80:44 - vectors we given any number of vectors
80:46 - and the linear combination linear
80:51 - combination linear combination of the
80:54 - vector
80:55 - of the vectors so you are given any
80:58 - number of a vector but the linear
81:00 - combination of this Vector means these
81:03 - vectors are simply the result of are
81:06 - simply the result when we multiply when
81:10 - we when we multiply when we
81:15 - multiply each Vector each vector by a
81:21 - scalar scalar and add the vector
81:25 - vectors so so how you get the linear
81:29 - combination is you are given any number
81:31 - of vectors okay so you are given any
81:33 - numbers of vectors and the linear
81:36 - combination of those vectors the given
81:38 - vectors is simply the result is this in
81:41 - this case the result of when the the
81:45 - result when you multiply when you
81:48 - multiply each Vector which is you are
81:51 - you are given u and v over here so when
81:53 - you multiply each each vector by a given
81:56 - scalar which in in this case is three
81:59 - and two okay and add them up that the
82:02 - resulting Vector is your linear
82:04 - combination of those vectors okay the
82:08 - formal notation which I can write is you
82:10 - are given a vector a you are given you
82:14 - are you are given a vector B you are
82:16 - given a vector C you you given a vector
82:19 - D okay so you want to find out the
82:21 - linear combination of these vectors so
82:25 - so the linear combination of these
82:27 - vectors will be simply uh when you
82:30 - multiply with some scalar okay with some
82:32 - scalar so I'm just going to write it uh
82:36 - uh a okay so this so let me write a
82:39 - different name of this so I will just so
82:41 - your given given vectors given vectors
82:44 - are are I think uh v u g okay so this
82:50 - these these three are your uh given
82:53 - Vector okay uh now what you do uh you
82:56 - simply multiply with some scalar a okay
82:59 - so so what what will be the so we want
83:01 - to ask what will be the linear
83:04 - combination
83:06 - linear combination linear combination of
83:10 - these vectors so what is linear
83:13 - combinations so the definition states
83:15 - that a linear combination is the result
83:18 - when you multiply the given vectors by
83:20 - some scalar and add the vector okay so
83:23 - that is the resulting vector so what you
83:24 - do simply multiply a scalar with a
83:27 - vector plus b with the U Vector plus C
83:31 - with a z Vector the resulting Vector D
83:34 - which will be nothing but your linear
83:36 - combination of v u and G okay so that
83:41 - will be the resulting that will be the
83:43 - linear combination so let's see more of
83:45 - the example to get comfortable with this
83:47 - so that you could get uh you could get a
83:49 - good feeling okay this is the linear
83:52 - combination Okay so so another example
83:56 - can be you can have a vector U you can
83:59 - have a vector U and you can have a
84:01 - vector v you can have a vector v so so
84:04 - what you do you it can be like this any
84:07 - number minus one * U Vector which is a
84:10 - scaler plus z b the answer whatever the
84:13 - resulting Vector will be will be the
84:15 - linear combination of these two vectors
84:19 - which are the given vectors okay so it
84:22 - can be fraction as well your
84:24 - it for the the the scalars can be
84:26 - fraction and fraction as well 51 7 by 11
84:29 - it can multiply with some some some some
84:31 - some Vector U and 1 195 by 2 with some
84:35 - vector v the resulting Vector the
84:37 - resulting Vector the resulting Vector
84:40 - will be your linear combination of these
84:42 - two vectors which is U and which is V
84:46 - okay so whatever the resulting Vector is
84:48 - will be the linear combination of u and
84:51 - v Vector okay so let's take one formal
84:54 - example so that we we could understand
84:56 - this much much better okay so one formal
85:00 - example of linear combination say you
85:03 - given a u uh say you given a vector U
85:05 - say you given a vector u u which is
85:09 - which is uh which is two two dimensional
85:11 - Vector which is a 2x1 vector or we can
85:13 - say it's a column Vector minus 5 by 0
85:16 - because column Vector is what what the
85:18 - the vector the the matjes is with only
85:21 - one column and over here this is we have
85:23 - only one column so so so so that's why
85:25 - it's called a column Vector so that's
85:27 - why we are telling it to the column
85:30 - Vector please see the previous video to
85:31 - help us understand to help you
85:33 - understand much more better way okay uh
85:35 - and you have a vector and you have a
85:37 - vector v which is 02 okay so you so you
85:41 - given a vector u and v okay this is also
85:44 - a column Vector this this is also a
85:45 - column Vector so what you want to do is
85:48 - to take out the linear combination of
85:51 - these two vector and and and the linear
85:55 - combination can be uh we can multiply
85:58 - this this this is this uh U Vector so
86:03 - what we can do we can multiply any
86:05 - scalar with this U Vector plus any
86:07 - scalar with this V Vector we will'll be
86:09 - getting the linear combination of these
86:11 - two vectors so let's take an example so
86:13 - let's let's take one example you
86:15 - multiply 1 with - 5 0 okay - 5 0 plus uh
86:21 - you multiply you you you have a scalar
86:23 - one which is place of v and you have 0
86:26 - by 02 okay that is your VV and the
86:29 - resulting Vector which will be nothing
86:30 - but - 5x2 okay and this is the linear
86:34 - combination of these two vectors and you
86:38 - can you can you can go ahead you can try
86:39 - it at different different scalers and
86:41 - and the same will be so you triy first
86:43 - you can try different different scalar
86:45 - with two with the a - 5 0 + 2 it it can
86:50 - be any number 02 that will be nothing
86:53 - but when you do do this to which is - 10
86:56 - and then - 10 0 + 0 4 that will be
87:01 - nothing what equals to - 10 4 so this is
87:05 - - 104 is a linear combination of these
87:09 - two Vector yeah you heard me correct
87:11 - this is the lar combination of these two
87:12 - Vector as well as this is the lar
87:15 - combination of these two vectors you
87:16 - heard me correct it can be you can
87:19 - multiply with some scalar 4 okay with
87:22 - -50 0 Plus plus I think okay you can do
87:25 - with one uh 02 the resulting Vector
87:30 - whatever the resulting Vector after
87:31 - doing this a will be the linear
87:33 - combination of these two vectors you
87:35 - heard me correct yeah exactly so your
87:38 - linear combination will be is it it it
87:41 - can be it it can be anything okay after
87:45 - the whatever means you take any scaler
87:48 - multiply with a given Vector uh you will
87:50 - and add it up you will be getting a
87:52 - linear combination so linear combination
87:55 - cannot be over uh some some finite over
87:58 - here okay except some exceptions which
88:01 - are there okay so over here linear
88:03 - combinations are said to be a vector if
88:06 - there exist a scalar a b okay and then
88:11 - whatever the resulting Vector will be
88:13 - will be the linear combination of those
88:16 - two vectors okay do not think that
88:18 - linear combinations can be one one is
88:21 - line combination of any two Vector can
88:23 - be only one no it can be anything it it
88:26 - can be any number of a linear
88:28 - combination of that two Vector what you
88:31 - need to do simply multiply the vector
88:33 - with some scalar A or B whatever and add
88:36 - it up the resulting Vector will be your
88:39 - linear combination of those two vectors
88:41 - so again I'm writing one formal formal
88:44 - definition so we have already written
88:46 - one formal definition but let's write
88:47 - one definition which will give you a
88:49 - more idea about what we have seen so
88:52 - far so the so the definition States so
88:55 - the definition States a vector R okay so
88:58 - a vector r a vector a vector r a vector
89:04 - R is said to be the linear combination
89:08 - is said to be a linear combination a
89:12 - linear
89:13 - combination a
89:15 - linear combination a linear
89:19 - combination of a b C okay uh a a vector
89:26 - is said to be a linear combination of a
89:28 - vector a b and
89:31 - c Etc okay so a a vector R is said to be
89:38 - the linear combination of these given
89:40 - vectors A B C which in this case this
89:43 - was u and v in this case this this was u
89:46 - and v these are the given these are the
89:48 - vectors okay these are the given vectors
89:50 - so the same way a b c d these are the
89:54 - the given vectors okay if there exists
89:58 - if if there exists if there exists
90:02 - scalers if there exist scalers x y z Etc
90:08 - whatever the means whatever how whatever
90:10 - the number of your vector is such that
90:14 - such that your resulting Vector is
90:17 - equals to x a plus
90:21 - YB zc all the way around to the n so so
90:25 - a vector R is said to be the linear
90:28 - combination of these vectors of these
90:31 - vectors if you multiply the scalar with
90:34 - a given vectors respectively
90:37 - respectively and the then then the r is
90:40 - said to be the linear combination of
90:43 - these two vectors of these all the
90:45 - vectors so again I'm repeating what's
90:47 - the linear combination means it simply
90:50 - means that it simply means that the the
90:53 - vector R is is is is we can call it as a
90:57 - linear combination okay how we can call
91:00 - we we are given a vector we are given
91:02 - these vectors we are given these vectors
91:04 - and if there exists exists some scalar
91:07 - and such that in such a way that your
91:10 - that the resulting Vector is equals to
91:13 - the multi the the the r is the is the
91:17 - multi when you multiply a scalar with a
91:19 - vector and add add the add the vector
91:21 - the resulting Vector will be your linear
91:24 - combination of that uh given vectors
91:26 - okay so let's take one example one
91:28 - simple simple example is uh is let's you
91:33 - want to you want to say is 1x 4 so
91:36 - you're you're you're given a vector U so
91:38 - I'm just going to take take one example
91:40 - so that everything is um make you
91:43 - understandable so your your your example
91:45 - you you're given a vector U you're given
91:47 - a vector U which is - 5 which is a two
91:51 - two dimensional vector and you given a
91:54 - vector v you're given a vector v you're
91:56 - G given a vector v which is 02 which is
91:59 - 02 so you want to show you want to
92:02 - show is is your 1x4 which is your R the
92:08 - vector R is the is also a linear
92:12 - combination is also the linear
92:15 - combination combination of vector U and
92:19 - vector v so you want to show is this
92:22 - Vector is the linear combination of
92:24 - these two Vector okay you want to show
92:26 - this you want to show this this is a
92:28 - problem so you want to show this is this
92:30 - the resulting which is r 1 by4 is the
92:33 - linear combination of these two u and v
92:35 - Vector is you want to show it so how you
92:38 - going to show it so for showing it we we
92:41 - will see the systems of equations
92:42 - solving the systems of equations later
92:45 - on but we can I will just give you a
92:47 - tool so so 1X 4 so this will be the
92:51 - resulting Vector so if there exists some
92:53 - scalar which which is 1x 5 so this is
92:55 - this this is my scalar times your vector
92:58 - - 5 0 plus there exists the second
93:02 - scalar times the 02 so your resulting
93:06 - Vector is 1x4 and hence and hence this
93:10 - 1x4 is the linear combination of this U
93:14 - and of this V Vector okay so I hope that
93:17 - you that that you are able to understand
93:19 - what's I'm talking about the linear
93:21 - combination of these vectors I hope so
93:25 - okay for a linear combination of a
93:28 - vector U and a vector v and I have given
93:31 - you also the formal definition of a
93:33 - linear combination so so let's let's see
93:37 - one terminology the terminology States
93:40 - terminology States terminology states
93:44 - that terminology states that the
93:46 - constants or the scalers which is here
93:49 - 1.5 this is two so these These are
93:52 - called the weights of the given Vector
93:55 - so in we we don't call it as a scalar we
93:58 - we call it as a weights rather than
94:00 - calling the scaler so if you have seen
94:04 - your if if you have seen your uh your
94:07 - hypothesis function so you have a Theta
94:09 - so that's so so so that is so that is
94:11 - the weights weights of your vector so
94:13 - the same way over here we don't call it
94:15 - as a scalar we call it as a weight of
94:18 - that u and v Vector okay so I hope so
94:21 - that you are getting a point uh to
94:23 - towards linear algebra and viewing
94:25 - machine learning or deep learning into
94:27 - the view of linear algebra okay so I
94:31 - hope that you are understanding so let's
94:33 - take one example to understand the
94:34 - weights so for example for example for
94:38 - example 1 by - 5 is your linear
94:43 - combination of vector 1
94:47 - 4+ 1 one when we multiply with the some
94:51 - weight this is called the weight so
94:53 - three that will be the this is the
94:55 - linear combination of these two vectors
94:57 - these two vectors okay and over here
95:01 - over here that 1 4 1 14 which is a
95:04 - vector and vector and 1 1 with weights
95:10 - with weights which is - 2 and 3 okay so
95:14 - 1 by - 5 is the linear combination of a
95:18 - vector 1 4 and 1 one with weights Min -
95:21 - 2 and 3 we don't don't we we don't call
95:24 - it as a scaler we even call it as a
95:26 - weights of that thing okay so I hope so
95:29 - that you are understanding whatever I'm
95:30 - trying to say you over here so I'm just
95:34 - going to talk about one last thing if I
95:36 - have a time I do have a 10 minutes time
95:38 - so what I what what I can talk about
95:40 - today is is the next concept which is
95:44 - the span of a vector okay so I'm just
95:47 - just just going to just make you
95:48 - familiar this is this this is just very
95:50 - easy so the we be talking about a span
95:53 - of a Vector I'll just giving you a small
95:55 - introduction to span and in the next
95:57 - video if possible I can show you some
95:59 - some some geometric intuition of a span
96:02 - okay so what is a span so let me let me
96:06 - write it span of okay so what is span
96:11 - span is a set of all the possible linear
96:14 - combination of a given group of vectors
96:18 - so for example we showed you over here
96:21 - we showed you you can have a multiple
96:23 - linear combination of a given Vector u
96:26 - and v you can have a multiple L your
96:28 - combination are you getting me so you
96:30 - can you can you can have multiple linear
96:32 - L linear combination of that given
96:34 - vectory U and B or u and v so the same
96:38 - so the group of all the or the set of
96:41 - all the linear combination of a given
96:43 - group of Vector in this case u and v is
96:46 - the span of those vectors I hope so that
96:50 - you're getting me okay so let me show
96:52 - you within help of example or or before
96:55 - that I'm going to give you a formal
96:56 - formal definition of this span because I
96:58 - think I I I TR some definitions also so
97:01 - SP definitions gives you a clear way of
97:04 - thinking this so I'm going to just give
97:05 - you a definition this this definition of
97:07 - a span the definition of a span the
97:10 - definition of a span is the set of all
97:14 - the possible the set
97:16 - of the set
97:20 - of all the possible all the possible
97:24 - possible linear combinations I'm just
97:27 - just going to write
97:29 - linear combinations linear
97:32 - combinations L linear combinations of
97:35 - given of given group of vectors okay so
97:39 - group of vectors what do I mean with
97:41 - this that uh how the the like in in this
97:45 - for example you have a vector U and you
97:47 - have a vector v so you want to take out
97:50 - the linear combination so these are the
97:51 - group of vector to for for which you
97:54 - want to take out the L linear
97:56 - combination okay so that given group of
97:59 - vectors is called the span is
98:02 - called the span of those vectors the
98:05 - span of those Vector which is a group of
98:06 - vectors span of those vectors so we'll
98:10 - see see one example to help us
98:12 - understand is much better way so let's
98:15 - take one example of that the example
98:17 - which I'm going to take is this is not a
98:19 - very hard example just just going to
98:21 - take a simple example but given a vector
98:23 - U 22 and then you have and you given a
98:26 - vector v which is 1 1 okay so this is
98:30 - the two vectors which is and any any and
98:33 - you can take out the linear combination
98:35 - you can take out the linear combination
98:37 - where when you have 2 3 4 9 10 may you
98:41 - another Vector is also the same stage
98:43 - same
98:44 - size okay so but should be the same
98:47 - Dimension so you can take out the linear
98:48 - combination of this just you need to
98:50 - multiply with a scaler and then add it
98:52 - up and then you'll be getting your L
98:54 - linear combinations so so over here so
98:57 - over here so over here if you so first
99:01 - of all let's take out some set of linear
99:03 - combinations so what I can do if we can
99:05 - multiply this U Vector with some weight
99:08 - I'm not talking I'm not taking the name
99:09 - of scalar just for practice a good
99:11 - practice so we can multiply with some
99:13 - weight
99:14 - two uh 2 two which is and here I can
99:18 - multiply with I think
99:20 - one one one and you can add it up that
99:23 - that will be nothing
99:25 - but 44 going add it up I'm just being
99:29 - transparent so that everyone follows the
99:31 - same is because I think trans being
99:34 - transparent uh just for a sake of it's
99:37 - it's very very important so that the
99:38 - conceptual CCT Clarity uh will be very
99:40 - very easy for you all okay so this is
99:43 - the the first the first for this is the
99:45 - let's take an example this is your l l
99:47 - linear combination so uh your L linear
99:51 - combination is this for for example we
99:54 - written V okay no not V we have already
99:57 - already given so this is your first V
99:59 - your combination let's take out the
100:00 - second or third let's take out a second
100:02 - you can multiply 3 2 2 plus I'm just
100:06 - sticking anything 2 1 1 which will be
100:09 - nothing but 3 6 6 + 2 2 that nothing but
100:15 - 8 8 okay so this is your H which is a
100:18 - second L linear combination let's go on
100:21 - taking the third so this which is the
100:23 - last one for us 4 2 2 3 1 1 which is 8 8
100:32 - + 3 3 which is nothing but 11 11 okay so
100:38 - this is your I which is your another
100:40 - Vector okay not I let's let's let's give
100:43 - it as okay not J as well let's give it
100:45 - as a key okay let's give it as K Vector
100:48 - okay so these three are just you can
100:50 - take out any you can you can take out
100:52 - lot more l combination you can just go
100:54 - ahead just multiplying with some scalar
100:56 - and then multiplying with some vector
100:57 - and then adding it up you'll be getting
100:59 - your linear combination of that two
101:01 - vectors so I'm not I'm not arguing you
101:03 - with that so I've just taken three
101:05 - linear combination so these three the
101:07 - five 58 5 five this is your G just
101:10 - taking z z h and k is the span it's a
101:18 - span a span of vector
101:23 - U and B instead of saying lot of Lear
101:26 - combination so you can just say okay
101:29 - that this these are the span of those
101:30 - two VOR so span if if you see
101:34 - geometrically speaking if you see over
101:36 - the it just your your V combination span
101:40 - hold to the space okay so please see
101:44 - some for some visualizations to to from
101:46 - three three blue one brown they give but
101:48 - I have given a geometric intuition that
101:50 - all the possible linear combination is
101:52 - called the is called the span of those
101:55 - vectors so the notation for writing this
101:56 - is just a span of the vectors V1 V2 V3
101:59 - is written as a span of V1 B2 V3 okay
102:02 - just I've shown you over okay please see
102:05 - the notes the notes are also given to
102:06 - you just for your own good okay so this
102:10 - is this this is what I talked about a
102:12 - bit about span and I hope that you
102:14 - really really understood this let's go
102:17 - on a last topic of this video is Matrix
102:20 - Vector product okay so Matrix Vector
102:23 - product so what I'm going to do is uh I
102:26 - have already talked about Matrix Vector
102:28 - product but just the last thing which
102:29 - I'm going to just talk about so I do
102:31 - have time so let's talk about
102:35 - Matrix Vector product Matrix Vector
102:39 - product so so let's say you're given a
102:41 - vector a you're given a vector a where
102:44 - you have some a b c d e f g h i okay
102:53 - this is your uh Matrix a and then you
102:56 - have a scale uh then you have a vector X
102:59 - then you have Vector X which is X uh for
103:03 - example one X1 X2 and X3 okay so you
103:07 - want to take out the majri vector
103:09 - product so how do you take out so it
103:12 - will be simply what you do so what you
103:14 - do you will simply uh you will simply
103:18 - this is your a x which is simply nothing
103:21 - but uh you you take take this you take
103:24 - this column you take this column you
103:26 - take you take this column which is the
103:27 - column vector and you take this uh sorry
103:31 - row Vector you take this row vector and
103:32 - you take this column Vector you just
103:34 - multiply it or you can say you can take
103:37 - we'll talk about the dot product just
103:38 - don't worry so what we can say we take
103:41 - out the dot product of a row Vector of a
103:44 - row vector and a column vector and a
103:46 - colum Vector okay so what you are what
103:49 - you are specifically doing is taking out
103:51 - the dot product between between two with
103:54 - between a row vector and a column Vector
103:56 - so what does dot product mean means it
103:58 - is just element wise product and sum it
104:00 - all up okay so that will be nothing but
104:03 - uh that that will be a * X1 + B * X2 + C
104:12 - * X3 okay and then when when you add it
104:15 - so let me write a resulting Vector as
104:17 - well okay so that will be a d okay so
104:21 - after you multiply and add it so that's
104:23 - is just a DOT product of of of of two
104:26 - vectors of two of of a row vector and a
104:29 - column Vector which is this we'll talk
104:30 - about the we'll talk in detail about the
104:32 - dot product and a transpose later on
104:35 - okay so and then you do the same D * X1
104:39 - + e * X2 + f * X3 I'm going to do the
104:44 - same G * X3 + H * X2 plus okay this one
104:51 - uh I * X3 okay so you'll be getting e f
104:55 - so whatever the answer is and this is
104:58 - your final uh Matrix Vector product
105:01 - product okay so which you have seen in
105:03 - in our previous video but I'm going to
105:04 - relate as a linear combination so I'm
105:07 - just just going to do what what what I'm
105:09 - going to do just see over here so you
105:11 - have a vector a you have a vector a you
105:13 - have Vector a where I'm going to what
105:15 - I'm going to do a b c d e f g h i j k l
105:24 - okay and I'm I'm going to multiply this
105:27 - this Matrix with a vector with a vector
105:30 - X1 X2 X3 and X4 okay so we have this a
105:34 - vector and X sorry aain Matrix a and a
105:37 - vector X okay so what you going to do
105:39 - you just just going to do the same thing
105:41 - you to multiply ax okay so I'm going to
105:44 - show you how you can do how how I'm
105:47 - going to represent this okay so what I'm
105:49 - going to do is to categorize this Matrix
105:52 - in into column Vector different set of
105:55 - different set of column Vector okay so
105:57 - we can take this we can take this the
105:59 - first First Column the First Column and
106:03 - say this okay okay this is the so you
106:06 - take it a V1 okay take the second column
106:09 - you say this V2 take the third column
106:12 - you say this V3 take the fourth column
106:15 - you say this V4 okay so so so
106:18 - specifically you you you are not taking
106:20 - as a as a as a as as a as a DOT product
106:24 - or or the element wise product of row
106:26 - and column Vector here what you will do
106:29 - here what you do categorize your Matrix
106:31 - a into a different uh into into a set of
106:34 - uh column Vector which is V1 vs2 V3 and
106:38 - V4 okay and then when you multiply when
106:41 - you multiply ax ax which is nothing but
106:44 - what you will do what you will do you
106:47 - will simply do uh
106:50 - X1 * V1 X1 * V1 okay X1 * V1 you
106:56 - multiply all X1 with this a okay X1 * V1
107:02 - okay and this is your scalar this is of
107:04 - course your scalar so let me write in
107:06 - small X1 * V1 okay so V1 V1 + X2 * vs2 +
107:15 - X3 *
107:17 - V3 V3 + X4 X4
107:22 - X4 * V4 okay so that whatever will so
107:27 - this is whatever the result will be for
107:29 - example R will be your linear
107:31 - combination so just listen what I'm try
107:33 - what I'm trying to say you converted
107:35 - this now what you do you take the you
107:37 - take this X1 and multiply X1 * X1 * X1 *
107:42 - C and X2 * X2 * X2 * X3 * X3 * X3 * and
107:48 - X4 time so what you do you given a
107:50 - weight of these column vectors so the
107:53 - resulting Vector will be the linear
107:55 - combination of the column Vector a so
107:58 - this this R will be the
108:00 - linear linear combination linear
108:04 - combination of
108:07 - vector a okay of a vector uh of of of a
108:11 - m of a column Vector a okay of a column
108:15 - Vector a so so so these this result
108:18 - resulting Vector will be the linear
108:20 - combination of the column Vector a of
108:23 - the column Vector I just just just have
108:25 - read column Vector column Vector which
108:26 - is V V1 V2 V3 the V4 so you have seen me
108:30 - how I done this as is to to Showcase you
108:33 - in in the form of linear combination so
108:35 - I hope that you understood a lot from
108:37 - this video and I really really hope that
108:39 - you will uh try to try to do the uh try
108:42 - to mark your tendance as well because it
108:44 - is an LMS so try to mark it your notes
108:47 - are in the description on box below
108:49 - please feel free to assess the notes uh
108:50 - it is very very important to to to work
108:52 - from that uh uh and and also and in the
108:56 - next video we'll be talking about uh the
108:58 - the linear transformation where we'll be
109:01 - introducing the notion of a
109:02 - transformation when when we multiply a
109:04 - two Matrix so that the the that that is
109:07 - th a transforming one Matrix into from
109:10 - one dimensional space to another another
109:12 - dimensional space that is a
109:14 - transformation which we'll talking about
109:15 - in the next video and then and then I
109:18 - hope so that we'll be able to complete
109:20 - the uh chapter number one in some days
109:22 - and then I hope uh it is it is much
109:24 - clear to you as well uh the next the one
109:27 - of the announcement that I want to give
109:29 - is please please please share the video
109:31 - and and mark your attendance in your in
109:33 - your quizzes okay uh uh by by going to
109:36 - the assignment Tab and your LMS if if
109:39 - you are in LMS so please go there and
109:41 - please please please try to try to
109:43 - search for some some some problem set or
109:45 - try to search for some resources
109:47 - although you don't need I talked a lot
109:48 - about more than enough okay for for your
109:52 - journey
109:53 - so so thanks for seeing this video I I I
109:56 - hope that you enjoyed this and you have
109:57 - taken your own notes please feel free to
109:59 - assess the notes on the description I'll
110:01 - be catching up your next video till then
110:02 - bye-bye and have a great
110:07 - [Music]
110:15 - day okay everyone welcome to this
110:18 - lecture on L your transformation so in
110:22 - this video will be is specifically
110:23 - talking about linear transformation in
110:25 - the pre previous video we talked about
110:27 - Matrix Vector product or or or in or a
110:30 - linear combination which which was the
110:32 - very fundamental concept and we also
110:34 - talked about span of a vectors now in
110:36 - this video we'll be talking about linear
110:38 - transformation one the most amazing
110:41 - concept or the beauty of algebra or
110:43 - linear algebra which you will ever see
110:46 - and also this this sets a very
110:48 - Foundation of your of your algebra
110:51 - skills or after study as of now okay and
110:54 - and it is used extensively in the field
110:56 - of deep learning uh when when you read
110:59 - research papers or or when when you
111:01 - staring some algorithm so if you want to
111:03 - understand by the point of view of
111:04 - linear algebra then then I think uh
111:07 - linear transformation is one of the best
111:09 - thing uh to study and and this is a
111:12 - compulsory topic to study as the most
111:14 - most most concept is related to this but
111:17 - you may think hey you just
111:18 - transformation so can you just Define
111:20 - what a transformation is I just want to
111:22 - you to search on Google what a
111:25 - transformation means and then put that
111:27 - in a comment box please give the time
111:30 - stamp so that I could I could know okay
111:32 - you you're here okay so please go on
111:34 - YouTube try to search about
111:36 - transformation and then come back okay
111:38 - so transformation like like just
111:41 - transform something or or do something
111:43 - to that function or or if you know about
111:47 - function so linear transformation can be
111:49 - thought of as a functions okay as the
111:52 - new name given when when we deal with in
111:54 - lanar algebra okay so lanar
111:57 - transformation is nothing or can be just
111:59 - thought of as a functions can be thought
112:01 - of thought of thought of as a functions
112:05 - can be thought of as a functions why I'm
112:07 - telling this in in functions what you do
112:10 - you give some input value you give some
112:12 - in input value and you want your F and
112:14 - you want your F to map this input value
112:18 - to the output value y okay so this is
112:21 - this is this is what the L
112:22 - transformation is doing linear
112:24 - transformation take some some some some
112:27 - some Vector as an input maybe two two
112:29 - dimensional Vector which is two
112:31 - dimensional Vector it wants a function
112:33 - it wants a function T that maps from n
112:37 - dimensional to M dimensional Vector okay
112:40 - this is the definition of linear
112:43 - transformation again let's will see see
112:45 - some of the definitions to help us more
112:48 - clear what actually linear
112:50 - transformation means but in but but in
112:53 - but in big picture uh linear
112:55 - transformation can be thought of as a
112:56 - functions like it takes some some some
112:58 - some sort of vectors or vectors and
113:00 - transforms from one di to n dimensional
113:03 - Vector to another dimensional vector or
113:05 - a different space in that uh plane okay
113:09 - so this is the this is the basic thing
113:11 - which you need to study about so u l
113:13 - transformation can be thought of as a
113:15 - function that takes some some some some
113:18 - vectors or or just transforms that just
113:21 - transforms this Vector from n
113:23 - dimensional space to M dimensional space
113:26 - and that can only be possible with a t
113:28 - which is a function T so the functions
113:31 - are the one which we take some value and
113:33 - Maps input and input values to Output
113:35 - values but in transformation we take we
113:38 - transforms xn means n dimensional Vector
113:41 - to M dimensional Vector okay so that's a
113:45 - linear transformation with you the
113:47 - definition so let's let's let's start
113:49 - with an example so that we could
113:51 - understand it's much much much much more
113:53 - better way okay so just I'm going to
113:55 - write a definition and what in what we
113:57 - do in a case of functions and what we do
114:00 - in a case of uh linear transformation so
114:02 - in functions we take some values we take
114:05 - some values we take some values input
114:09 - values and we map our input values and
114:12 - we map our input values input values to
114:18 - Output values okay so to make a function
114:21 - f that take some Val Val and output the
114:23 - square of X which is Parabola if if you
114:25 - plot it out okay then that in in case of
114:29 - linear transformation we what we do we
114:32 - make a function T we make a function T
114:35 - we make a function T we make a function
114:37 - T that takes the transforms the
114:40 - transforms from from one vector space
114:43 - transform the vector the transform the
114:45 - vector uh from from RN from RN and this
114:49 - is the RN from one from n dimensional
114:53 - into a vector into a vector to RM okay
114:57 - so it just transforms one one one vector
115:00 - from one one vector space to another
115:03 - Vector space okay so this is the basic
115:05 - thing which you need to which which will
115:07 - which will see I just prove prove you
115:09 - out this equation so why it why it seems
115:11 - to be legit so let's start with with an
115:14 - example so that I could just prove you
115:16 - that what whatever I'm telling is
115:17 - correct okay so let's let's take one
115:20 - example let's take one example say you
115:22 - have a you you if for example if you
115:26 - multiply
115:28 - example if we example will be say you
115:31 - multiply your M by n Matrix so you have
115:34 - an M by n Matrix you have your M by n
115:37 - Matrix this is your favorite Matrix you
115:40 - have your m m byn Matrix and what you
115:42 - want to do is simply multiply with a
115:45 - column Vector which is n * 1 which is a
115:49 - column Vector which is n * 1 so we'll
115:51 - just multiply this this Matrix with a
115:54 - column Vector n * 1 so let's let's
115:56 - multiply it out with the column the
115:58 - column Vector n * 1 okay so here it has
116:01 - only one column and the resulting Vector
116:04 - what we are specifically doing we we are
116:06 - taking we are multiplying our Matrix
116:08 - with a vector okay and then that that
116:12 - will be nothing but your n by one column
116:15 - Vector M by m * 1 column Vector okay m m
116:20 - * 1 column Vector which is your which is
116:22 - resulting Vector resulting Vector that
116:25 - that will the resulting column Vector
116:27 - resulting column Vector okay so so what
116:30 - so what it does so what it does it take
116:33 - it it it it took your it took your um
116:37 - just is to took or or or in other words
116:40 - we see over here that an N M * n Matrix
116:44 - that an M and M * n Matrix transforms an
116:49 - n * 1 Vector into an n M * 1 resulting
116:54 - Vector which is another space okay so
116:56 - using this Matrix we transform this
116:59 - Vector into different Vector space like
117:01 - this okay so for example let's see some
117:04 - some example to make sense here we are
117:06 - taking the taking the the the the the
117:08 - Matrix Vector product which we have
117:09 - already seen in our previous videos so
117:11 - let's consider consider this Matrix a
117:14 - let's consider this Matrix a as we as a
117:16 - 3X3 Matrix so 1 2 0 and 2 1 0 which is
117:20 - nothing but 3x three Matrix okay now I
117:24 - want to show you want to show show you
117:27 - want to show that by matrix
117:29 - multiplication by Matrix Matrix Matrix
117:32 - Vector multiplication or that matrix
117:35 - multiplication matrix by matrix
117:37 - multiplication by matrix
117:40 - multiplication you want to show by
117:41 - matrix multiplication a transforms this
117:44 - this Matrix a transforms
117:48 - transforms transforms Vector n r R3
117:54 - vector and R3 mean is which is the which
117:57 - is the column Vector which is which is
117:59 - three threedimensional Vector like this
118:01 - have Vector which is a three dimensional
118:02 - x y z which is from R3 from from in from
118:06 - R3 to
118:08 - R2 or to R2 okay into into a into into a
118:14 - vector into Vector in R2 R2 so you just
118:18 - you using a you want to transform this
118:21 - Vector X X you can transform the vector
118:24 - X and then that will into into an R2
118:27 - which is nothing but your X and Y one
118:30 - two two dimensional rather than being a
118:32 - three dimensional that's that's when we
118:34 - call linear transformation of a matrix
118:37 - okay so let's see so with with an
118:38 - example you have a you have a vector so
118:42 - so R3 so over here R3 R3 are a vector of
118:46 - a size is a vector of a size 3x1 which
118:49 - is a which is the 3x1 threedimensional
118:51 - vector while Vector R2 R2 which you want
118:54 - to transform you you want to show is 2x
118:57 - 3 2 * 3 which is the two sorry uh 2 *
119:00 - it's it's 2 * 1 it's two dimensional
119:03 - Vector so it's trans it's using the a
119:06 - you want to transform your your your
119:08 - your vector means you using a or or a
119:11 - transform you just show that a
119:13 - transforms the vector means from R3 in
119:18 - R3 into a vector R2 okay which you want
119:21 - to show up okay so when you do this when
119:24 - you do this so this this this is of size
119:26 - 2x1 which is which is your uh which is
119:28 - your row Vector I I think it's column
119:30 - Vector column vector and then you if if
119:33 - you multiply a which is your 2x3 Matrix
119:36 - which is a true two okay it's 2x3 okay
119:39 - it's 2x3 matrix by a 3x1 vector by a 3x1
119:43 - vector by a 3x1 vector the resulting
119:46 - Vector will nothing but 2x one so you
119:49 - just showed using the what you do using
119:52 - a you multiply this a with uh which you
119:56 - want to show means R3 and then the
119:58 - resulting Vector which you can see that
120:00 - you showed okay using a you transforms
120:04 - the vector in R3 which is uh three which
120:07 - is this one into a vector R2 which is 2
120:10 - 2 okay or or or a which is a
120:14 - two-dimensional Vector okay so that that
120:16 - is what it is telling so let's see one
120:18 - of one of one of one of the example just
120:19 - to numerically show you so so 1 2 0 2 1
120:25 - 0 and and what you and and you have this
120:28 - metrix
120:29 - a and you want uh um and you and and and
120:33 - you have a threedimensional vector XYZ
120:35 - and you and this is an A and this is a
120:37 - vector X and you want to show you that a
120:42 - transforms of in a trans means uh what
120:45 - do you see the a
120:48 - transforms and and a matrix which is
120:50 - from R3 into an R2 okay using a you want
120:54 - to transform R from R3 R3 into R2 so
120:59 - that will be nothing but x + 2 y and
121:03 - zero of course we don't write it out and
121:05 - and of and and 2x + y okay and that and
121:10 - then it it will be after after you add
121:12 - it up add it up it will be either A and
121:14 - B which is your R2 okay so that is the
121:18 - following that what you done you simply
121:20 - transform your f one one one vector
121:23 - space to another Vector space using a
121:25 - function a or or using a which is your
121:28 - Matrix okay so let's let's let's define
121:31 - it out so what you done what you done
121:33 - you made a function T the transform F
121:36 - and and from from n dimensional to M
121:40 - dimensional means a function T transform
121:43 - the vector M transform the vector RN
121:47 - into a function a function T which
121:49 - transform from RN to RM which another
121:53 - dimensional
121:54 - space got it the linear transformation
121:57 - should satisfy your two constraints the
122:02 - two constraints are the first constraint
122:04 - the transformation t x + y it would be
122:08 - it would be is equals to the TX plus Ty
122:11 - y transformation of X Plus trans
122:13 - transformation of Y and then your trans
122:16 - transformation and then this this is
122:19 - scalar a and this is a vector X and then
122:21 - it's should be and then then it should
122:23 - be a uh a and transformation of X so
122:27 - these are the two conditions which you
122:29 - need to satisfy okay uh these are the
122:32 - two basic conditions which you will ever
122:33 - see okay but but the more form formula
122:36 - which I which I could State over here is
122:39 - linear transformation is the is the the
122:41 - function that transforms your Vector
122:43 - from onedimensional space from RM to RN
122:47 - okay that is your linear transformation
122:49 - of your vectors or Matrix or of of of of
122:52 - of a vector okay so um one one theorem
122:56 - is there one theorem is there the
122:58 - theorem States uh your let T be our let
123:02 - let T be a function or transformation
123:04 - that transform from one vectors RN to RM
123:08 - um which is the transformation of your
123:10 - transformation of X which transforms RN
123:13 - to RM okay so I'm just just just going
123:16 - to write the theorem which I'm not going
123:18 - to prove rigorously but you can prove it
123:20 - you can prove it t equals to R to make a
123:23 - function T you want to make a function T
123:25 - to transform RM to R RM okay uh be a
123:30 - transformation defined by the trans
123:32 - transformation is defined by T of X okay
123:35 - trans you you give uh you give one
123:37 - vector which is of n dimensional and
123:39 - just what it will do using a which is a
123:41 - matrix just transform that Vector means
123:43 - the Matrix Vector product so I just
123:46 - showed you that a matrix Vector product
123:48 - is a linear transformation okay so what
123:51 - you use if even you multiply
123:54 - to
123:56 - u a vector with the Matrix that that
123:59 - that will just give you the linear
124:01 - transformation just it should satisfy
124:03 - the conditions which are listed over
124:05 - here the geometric understanding is also
124:08 - so not it's is very very easy you can
124:10 - you can consider watching three blue one
124:12 - one Brown videos for this I hope that
124:14 - will make more sense there okay so that
124:18 - was a short video On LAN transformation
124:21 - about fth 15 minutes and I really really
124:23 - hope that that you like this video uh in
124:25 - the next video we'll be talking about
124:26 - transpose of AMS and uh and and and and
124:30 - and and and uh uh dot product which
124:33 - which which which is one of the most
124:34 - important concept so let's get on to the
124:36 - next
124:40 - [Music]
124:48 - video okay everyone so today we'll be
124:50 - talking about transpose and a DOT
124:52 - product of a matrix or a vector so we we
124:56 - we'll be talking about that because in
124:58 - the video of linear combination I have
125:01 - taken one example of Matrix Vector
125:03 - product and I showed you how you can
125:05 - represent that Matrix Vector product as
125:08 - a DOT product between the column vector
125:11 - and a row Vector so we'll be talk
125:13 - talking about what does it exactly means
125:16 - uh what is a DOT product and a transpose
125:18 - of a matrix so these are again two most
125:21 - important concept which you will ever
125:23 - see in your journey of linear algebra
125:26 - again it's just one of the basic concept
125:27 - which is very very easy to understand
125:29 - but still it's is very very uh good to
125:31 - know about these things which gives you
125:33 - an extra tools to work efficiently uh
125:36 - and and your deep deep learning problem
125:38 - or machine learning problem or or or
125:40 - other stuffs okay so let's get started
125:43 - so so the what is a transpose of a
125:45 - matrix so I'm going to start with a
125:47 - transpose of a matrix what is a
125:49 - transpose of a matrix so the transpose
125:52 - of a matrix is a kind of operator which
125:55 - flips over the diagonal which flips over
125:58 - the diagonal okay or make all the rows
126:01 - or make all the rows uh for example if I
126:05 - could show you the the the visualization
126:07 - so for example you have uh you want to
126:10 - take you have a column vector or row
126:12 - Vector like this 1 2 so it's if the
126:15 - diagonal is this one the diagonal is
126:18 - this one so it simply flips it okay so
126:20 - it simply flips it
126:22 - then it would be if you take all the
126:23 - transpose of this so it will be 1 2 so
126:26 - what it does it flips over the diagonal
126:28 - for example let's let's take one one
126:30 - more let's take one more let's take one
126:33 - more you have 1 2 3 4 which is a 2X two
126:38 - Matrix now if you take all the transpose
126:41 - of this Matrix so what you do you have
126:43 - this diagonal you have this diagonal you
126:45 - have this diagonal so if you flip over
126:47 - this diagonal if you flip this over the
126:49 - diagonal if you flip this over the
126:51 - diagonal it would be nothing but one if
126:54 - you flip this over a diagonal if you
126:55 - slip flip it there 1 3 and 2 4 okay and
127:01 - two four so what you done you simply
127:03 - flip it and then above going down and
127:06 - down going above so that is what it is
127:08 - trying to tell or or in other words what
127:10 - you can interpret is you make every row
127:13 - or sorry every column as a row and you
127:16 - make every column as a row okay so that
127:20 - you can that you can expect Ed so for
127:22 - example you have another so so if you
127:24 - have another Matrix let's let's take an
127:26 - example that is 6 4 32 and you apply the
127:30 - transpose onto this Matrix so what will
127:33 - be the output so the output will be what
127:35 - you do you take this uh you take this um
127:39 - or you say the row sorry the column or
127:41 - the column vector and make it a row
127:43 - Vector like this make it a row of row
127:45 - Vector six and three and you take this
127:47 - another you make this four and two or
127:49 - other wasse what you can interpret is
127:51 - you flip over the diagonal you flip over
127:53 - the diagonal you make this three above
127:56 - and you make this four down okay so what
127:59 - you specifically doing is to flipping
128:01 - over the diagonal like this okay so you
128:03 - will get the same result as you are
128:05 - doing so it's just not a big deal to
128:07 - understand these things is very very
128:08 - easy to understand okay so I hope that
128:11 - that that you are able to understand
128:12 - what the transpose of a matrix is so
128:14 - let's try a formal definition let's
128:16 - write a formal definition of your
128:18 - transpose of a matrix so the transpose
128:20 - of a matrix the transpose the
128:23 - transpose transpose of a matrix
128:27 - transpose of a matrix is an is a type of
128:30 - operator or operator is a Operator
128:34 - Operator which flips which flips over
128:40 - the diagonal over the
128:42 - diagonal over the diagonal okay so this
128:45 - is the transpose of a matrix so what is
128:48 - does is it the definition states that it
128:50 - is it just flip over the diagonal or to
128:53 - the main diagonal to obtain the a
128:55 - transpose so for example you are so for
128:58 - example you are given a vector a so what
129:01 - you do what you do you reflect a you
129:04 - reflect a you reflect a over its main
129:07 - diagonal to obtain the a transpose or it
129:10 - is just a tech technical verse to flip
129:12 - or reflect but specifically what is
129:14 - doing making the column Vector as a row
129:17 - vector or and and a row Vector as a
129:19 - column Vector that's it okay okay your
129:22 - visce Versa it can be anything so making
129:23 - a colum Vector to a row Vector that's it
129:26 - okay so that is what it is telling and a
129:28 - row Vector to a column Vector whatever
129:30 - whatever seems good to you so a
129:32 - transpose just exactly doing the that
129:34 - okay so for example for for example you
129:38 - want to take you have your Matrix a okay
129:41 - you want to take out a transpose you
129:42 - want to take out a transpose where it
129:44 - either presents your row and J
129:46 - represents your column so now if you
129:48 - after this after applying the transpose
129:50 - at this Matrix now your will be Aji now
129:54 - your column represents your rows and uh
129:56 - rows represents your column again again
129:59 - this is this is very easy to interpret
130:01 - so you have this 2 two 2 two and you
130:04 - have this a matrix what you what what
130:06 - you do you apply the transpose on this
130:08 - Matrix after applying the transpose of a
130:10 - matrix here you are having a transpose
130:14 - okay and you have I which is two or I
130:18 - and G okay you have I and J which when
130:21 - you do that when when you apply the
130:23 - operator this will be nothing but or I
130:26 - could say 0 0 Let's Take This 0 0 okay
130:28 - so when you when you sorry we should not
130:31 - take this 0 0 let's take it as a let's
130:33 - take take it as a one let's take it as a
130:36 - six okay just for understanding okay so
130:40 - what if if you have transpose over here
130:41 - after applying the transpose of a matrix
130:43 - over here your col your your row becomes
130:46 - your column and column and column
130:49 - becomes your row or yeah so so your row
130:52 - becomes your column so your row is the
130:54 - first row becomes your column so after
130:57 - flipping over the diagonal so when when
130:59 - apply so is two two and then your second
131:02 - second column becomes your second row
131:04 - okay one and six okay so what do you
131:07 - specifically done you flipped over or or
131:09 - reflected the main diagonal or flipped
131:11 - the over the diagonal and then you
131:13 - obtain your transpose of a matrix so
131:15 - that is what it is telling so you can do
131:16 - in high dimensional spaces as well for
131:18 - for example you have a m s we have 64 4
131:21 - 9 10 11 12 13 14 15 okay so if you want
131:26 - to apply the transpose on this Matrix
131:28 - that will be nothing you take this take
131:31 - this and put it over there
131:34 - 649 and then you take this 10 11 12 and
131:39 - you take this 13 14 I think 15 okay so
131:44 - this is this this is what you are doing
131:46 - is flipping over the main diagonal
131:48 - flipping over the main diagonal okay to
131:51 - to to or reflect it or reflect it over
131:53 - the main diagonal to obtain your
131:55 - transpose of a matrix so we have we have
131:56 - done a lots of examples so we'll see one
131:58 - or two more and more examples to make
132:00 - you more sense so let's make one or more
132:03 - example so I just hope that you are able
132:05 - to make sense for actually transpose of
132:08 - so that is the transpose of a matrix so
132:10 - it is just what it is doing reflecting a
132:13 - over its main diagonal to obtain the a
132:17 - transpose that set what the transpose
132:19 - doing so if if you take more examples we
132:22 - have a one two okay if you have one two
132:24 - if this is a A or a okay a column uh row
132:29 - Vector if you transpose this a transpose
132:31 - that will be nothing but one two okay uh
132:34 - just flipping over or or or making the
132:36 - row as a column Vector okay the next
132:39 - thing can be for example you have a I
132:42 - want you to solve this I want you to
132:43 - solve this one 2 3 4 apply the transpose
132:48 - on this and see me what the result
132:51 - result would be in the comment box
132:53 - comment
132:54 - box write this answer in the comment box
132:57 - it will very very easy to understand uh
133:00 - so I could say I could see whether
133:02 - you're watching or not okay so that is
133:04 - the that is the transpose of a matrix so
133:07 - just just I'm going going to write out
133:08 - some of the points some of the points
133:12 - please please pause this video and write
133:13 - your answer of a transpose of a matrix
133:17 - so just I I want you to write it out
133:19 - okay so let's see that some some of the
133:21 - properties some of the properties
133:24 - properties of the transpose okay so you
133:27 - want to take out a transpose and then
133:29 - you take out the transpose that so tell
133:31 - me what it will be tell me what it will
133:33 - be tell me it would be nothing but a it
133:36 - would be nothing but equals to A you you
133:39 - first of all transpose it okay and then
133:41 - you take the transpose of that so for
133:43 - example you have a which is 1 by 2 okay
133:45 - you have 1 by two okay so what you do
133:48 - what you do you take the transpose of
133:50 - this which will be 1 2 and then you take
133:54 - the transpose of this which will be 1 2
133:56 - so these are equals to or not these are
133:59 - equals to so that's why we are telling
134:01 - that is the one of the property of a
134:03 - transpose of a matrix another property
134:05 - of a transpose of a matrix another
134:08 - property of a transpose of a matrix is A
134:10 - + B A transpose which will be nothing
134:14 - but a plus b transpose is nothing but a
134:17 - transpose plus b transpose okay so that
134:20 - will be equals so you have a a 1 2 3 4
134:24 - okay and you have a b and and you have a
134:26 - b uh 1 2 3 4 you want to add it by
134:30 - taking all now you want to take all the
134:32 - transpose of these three so what you can
134:34 - do you can take the transpose of you can
134:35 - take take all the transpose of this and
134:37 - that will be your answer okay so let's
134:39 - do this we going to take the transpose
134:40 - of this that will be nothing but 1 3 and
134:43 - 2 4 okay
134:45 - plus uh 1 3 and 2 4 okay which will be
134:51 - nothing but 1 + 1 2 3 + 3 6 2 + 2 4 4 +
134:57 - 4
134:58 - 8 okay that will be your final answer
135:00 - which is 2x2 matrix and this is this is
135:03 - what this property States you can even
135:05 - what you can do you can prove it you can
135:07 - prove it extensively you can prove it
135:09 - you have a you take out the transpose
135:11 - and then you and then what you do first
135:13 - of all let's do the same thing you first
135:15 - of all add it up so when you add it up 1
135:19 - 2 3 4 plus
135:21 - 1 2 3
135:23 - 4 okay when you add it up so 1 + 1 1 2 +
135:28 - 2 4 3 + 3 6 4 + 4
135:33 - 8 it is 2 4 6 8 okay that that will be
135:37 - your answer of this particular and then
135:40 - what you do you take out the transpose
135:41 - of this because you added it up now you
135:43 - take when when you when you take out the
135:44 - transpose it would be nothing but 2 6 48
135:48 - and this is your answer so these both
135:49 - are equal so these both are equal so you
135:53 - can you can you can do ex you can do
135:54 - separately transpose and then add and
135:57 - yeah or yeah or you can or or you can do
136:00 - just first of all add and then take out
136:02 - the transpose both are equ equivalent
136:05 - your answer will be equivalent okay so
136:08 - let's see another uh let's call it
136:10 - another page let's make another page how
136:14 - do we add another page yeah let's add
136:16 - one one one one one one more page so
136:18 - another property is another another
136:21 - property is AB transpose AB transpose
136:24 - which be nothing but equals to B
136:25 - transpose and a transpose okay so you
136:29 - have a let's say let's for the sake of
136:31 - an example let's take a vector 1 2 1 2
136:35 - and then you take a and you take a v v
136:37 - Vector B Vector you take a b Vector
136:39 - which is 22 okay and then what you want
136:42 - to do you want to take out the transpose
136:43 - of this okay so let's first of all do
136:45 - this so one so that would be 1 * 2 1 1 *
136:49 - 2 which is which would be nothing but 2
136:51 - 2 * 2 which is 4 okay and then what you
136:54 - do you take out the transpose of this so
136:55 - when you when you take out the transpose
136:57 - of this that that will be 24 that will
136:59 - be 24 which is a column Vector okay so
137:01 - that will be your first understand this
137:03 - is your first now let's let's check for
137:06 - equivalent for this so you have B so
137:09 - which is b 2 2 if you if you take the
137:12 - transpose of this which will be nothing
137:14 - but 2 two okay and then you and and then
137:17 - what do you do you first of all do this
137:20 - 1 2
137:21 - okay which after taking the transpose of
137:23 - this that that will be 1 2 and then when
137:25 - you multiply it out that will be nothing
137:27 - but 2 4 2 4 which which is both
137:31 - equivalent so so we have proved this we
137:34 - have proved this which we have proved
137:36 - this let's fourth let's go on Fourth
137:39 - property the fourth property is you have
137:42 - scalar C and you have a matrix a when
137:45 - you take the transpose of it that will
137:47 - be nothing but C A transpose okay what
137:49 - you can do you can first of all rather
137:51 - than after multiplying and then taking
137:53 - the transpose you can first take out the
137:55 - transpose and then multiply by the C
137:57 - both will be same okay uh fifth the
138:00 - fifth you can you can actually verify
138:01 - this you can actually verify this is not
138:03 - a big deal okay just the way I'm doing
138:06 - okay so you to take out the determinant
138:09 - of a transpose that will be nothing but
138:12 - determinant of a and if you if if you
138:14 - don't know about this please ignore
138:17 - which we will be studying this
138:19 - extensively in our VAR okay you can
138:22 - ignore this the determinant is is just
138:25 - the area of the parallelogram so you can
138:27 - easily ignore this as of now okay sixth
138:30 - one the sixth one is the last one which
138:33 - I'm talking about a transpose minus one
138:36 - okay uh 12 the^ of-1 a - one transpose
138:40 - so these are equivalent okay so you take
138:43 - the transpose and then and then inverse
138:45 - it first of all you inverse it and then
138:46 - take out a transpose both will be
138:49 - equivalent okay so these are some of the
138:52 - some of the most uh used properties in
138:55 - your transpose and I hope that you that
138:57 - you are able to understand this it's not
138:59 - a big deal to understand okay these are
139:01 - the transpose which we have talked about
139:03 - let's go on to the dot product to
139:06 - understand much better so let's let's
139:08 - let's talk about a bit about dot product
139:10 - so what is dot product dot product what
139:13 - is do what it do what it do it do
139:16 - element
139:18 - element wise
139:21 - product element wise product and sum it
139:27 - all up that's it that's what the dot
139:30 - product is doing is to do element wise
139:32 - product and add a summ that's what the
139:34 - dot product is doing so for an example
139:37 - so for example the dot dot product so
139:39 - the so for example let's let's let's
139:41 - take for the sake of an example you have
139:43 - a vector a where you which is your which
139:46 - is your row Vector so A1 A2
139:50 - all the way around to the a n all the
139:53 - all the way around to the a n and you
139:55 - have a b Vector B row row Vector which
139:57 - is B1 B2 all the way around to the BN
140:01 - okay so this is your two two vectors now
140:04 - what you need to do if if you want to
140:06 - take out the dot product between these
140:08 - two Vector if you want to take out the
140:11 - dot product between these two vectors so
140:14 - how do you take out so what you you
140:16 - simply add a submiss so you take out the
140:18 - a DOT you write dot b is nothing but
140:21 - equals to I = to 1 all the way around to
140:24 - the n a i b i so what it will do first
140:28 - of all a a A1 * B1 + A2 * B2 + A3 * B3
140:39 - plus all the way around to the a n * BN
140:43 - and your final output will be one is
140:44 - scaler C which is your dot product
140:47 - between these two row Vector okay so
140:50 - that is the the particular the the
140:52 - algebraic definition so this is the
140:53 - algebraic definition of your dot product
140:56 - and or or or or in other words what you
140:58 - can tell is the dot dot product between
141:01 - two vectors is nothing but a b transpose
141:04 - the dot product the product between A
141:06 - and B transpose that that is your
141:09 - algebraic definition or this is your
141:11 - formal definition of your dot product
141:13 - product so for example for example you
141:16 - have this and you have another Vector
141:18 - like this your output will be 8 a b c d
141:22 - e f your output when when you take out
141:25 - the dot product between these two A+ d a
141:28 - * D + B * e + C * F and your output will
141:34 - be a scalar okay so if you have seen our
141:36 - Matrix Vector product Matrix Vector so
141:39 - we have we were having we were having so
141:42 - let's let's see which we have seen
141:44 - already in our previous videos so let's
141:47 - say let's say let's say of for for the
141:49 - sake of an example you have a b c d e f
141:53 - g h i okay and you have a vector which
141:56 - is X1 X2 and X3 you want to take out the
141:59 - product between these two so what you do
142:01 - you categorize this into a different
142:03 - categorize this into a different what do
142:05 - you say uh the the the column vector or
142:07 - sorry row Vector yeah column Vector V1
142:10 - V2 V3 then what you do you take out the
142:13 - dot product you take out the dot product
142:16 - between V1 and a vector X so when you
142:19 - take out the dot product between a
142:22 - column Vector with uh with a column
142:24 - Vector so when you take out the dot
142:25 - product between these two that will be
142:27 - your answer which you have already seen
142:30 - in in our previous videos okay so I I
142:33 - don't think that we should uh that we
142:35 - should care about this so I hope that
142:37 - you are able to make sense of these
142:39 - things and please feel free to review
142:41 - the previous video on L linear
142:43 - combination which I talked in detail
142:45 - about these things okay so this is this
142:47 - is what the dot product is simply say
142:49 - simply take off take take out the
142:51 - element wise product and sum it all up
142:54 - okay and that will be your simple
142:57 - scalar okay so that is what the dot
142:59 - product means and if if you have seen
143:02 - our videos on on on hypothesis function
143:05 - which if if you know about hypothesis
143:07 - function of linear regression if we
143:10 - talked about hypothesis function so in
143:13 - hypothesis function what you were doing
143:15 - we were and we were taking out the dot
143:17 - product between our X and W we are
143:19 - taking out the X or a Theta we take
143:22 - Taking of the dot product between X and
143:24 - W that that was resulting in our
143:26 - prediction y hat okay so X was also
143:30 - maybe a matrix or vector so you have a
143:33 - matrix or a vector so a vector of X can
143:36 - be uh uh other met for example 2 4 6 7
143:41 - and your W can be also 2 4 6 7 whatever
143:46 - these are the weights of this feature
143:48 - and what you do you take out you take
143:49 - sorry it's not matrix it's a vector it's
143:51 - a vector do product so you simply
143:54 - multiply it up you simply multiply it
143:56 - up and then what do you do you simply
143:59 - add it up and that will be one scaler
144:01 - which is your answer for for example 2 *
144:03 - 2 + 4 * 4 + 6 * 6 + 7 * 7 and then after
144:09 - doing plus all those stuff then you'll
144:11 - be getting your answer C which is an
144:12 - answer of this particular question okay
144:16 - I hope that you are able to make sense
144:18 - out of it so we have seen our algebra
144:20 - definition of a DOT product now it's
144:22 - time for seeing the geometric definition
144:24 - to help you more make more sense of your
144:28 - dot product of two vectors okay so let's
144:31 - see let's see of that so let's
144:33 - see uh the do
144:36 - product let's see uh let's take let's
144:39 - take an example you want to take out the
144:40 - dot product take out the dot product
144:43 - between two vectors between two vectors
144:45 - which is Vector a and Vector B which is
144:47 - geometrically speaking I'm going to I'm
144:49 - going to talk about geometrically now
144:51 - I'm going to talk about geometrically so
144:54 - for the sake of an example understand
144:56 - let's take an example that you have a
144:58 - vector like this so sorry it's bad you
145:01 - have like this and you have this okay so
145:05 - this is your vector a this is actually
145:08 - Vector a and this is actually vector v
145:11 - and just one thing if you're a calcul
145:12 - student there is a small fun quiz is it
145:15 - continuous is it
145:18 - continuous function we have a function
145:20 - let's take an example that is a function
145:22 - okay is it a continuous function um if
145:25 - is it differentiable if it is continuous
145:26 - function if is it differentiable that is
145:28 - your question okay so please feel free
145:31 - to put in comment just for just for
145:33 - those who are calc student just tell me
145:35 - just ignore this A and B just say uh
145:37 - just say this this is a function and
145:40 - then just tell me it is a continuous and
145:42 - if if it is then if it is differentiable
145:45 - or not okay this is this is your
145:47 - question to ask answer but the one who
145:49 - is studying Le your algebra please
145:51 - filter stick me with this okay kindly
145:53 - ignore the question which I told you
145:55 - cool so the angle between these two
145:58 - Vector the angle between these two
146:00 - Vector is nothing but the angle between
146:02 - is
146:03 - 59.5 De okay so that is the angle
146:06 - between these two Vector okay so this is
146:08 - the angle between these two Vector so
146:11 - you want to take out the dot product
146:12 - between Vector a and Vector B so taking
146:15 - of the dot product between Ang Vector a
146:18 - and Vector B which which will be nothing
146:20 - but it should be nothing but I would say
146:24 - uh it should be
146:28 - nothing but Norm of a norm of a times
146:34 - the norm of B the norm of B time cosine
146:38 - of theta cosine of theta so Theta the
146:42 - angle between them is Theta so the norm
146:44 - of a so for example let's assume the the
146:46 - length of a is uh 10 okay length of a is
146:52 - 10 and your length of B is 13 so here
146:57 - your vector a is nothing but 68 and when
147:00 - you take out the length is 10 using the
147:02 - pythagore theorem Pythagoras Theorem and
147:05 - then uh the vector v is 5 and 12 when
147:08 - you take out using the Pythagoras
147:10 - Theorem that will be or or the or the
147:12 - norm of a vector when you take the norm
147:14 - of a vector that would be 13 okay so the
147:17 - the the length of these I have already
147:19 - told you okay that is 10 and 13 and when
147:21 - you multiply with a cosine of
147:25 - 59.5 okay so then you'll be getting then
147:27 - what you then what you will get 10 * 13
147:31 - *
147:33 - 0.575 and then when you multiply that
147:36 - will
147:37 - 65.9 whatever and then it is
147:40 - approximately 66 if you want to uh what
147:43 - do you say take out now if now we got 66
147:47 - now if you want to take out the
147:48 - algebraic the algebraic definition
147:49 - States 6 8 and 5 12 when you do this 6 +
147:54 - 5 11 8 + 12 20 8 + 12 20 when you add
147:59 - this so sorry uh we are actually taking
148:03 - we are actually taking all the okay
148:05 - that's 6 6 * 5 it should be 6 * 5 why
148:08 - I'm doing 6 6 * - 5 sorry it's 5 yeah 6
148:13 - - 6 * it's - 6 it's not 6 it's - 6 okay
148:17 - so when you do this 6 - 6 * 5 and + 8 *
148:22 - 12 - 6 * 5 + 8 * 12 and you will get -3
148:28 - + 96 which will be 66 and this this is
148:32 - equivalent to this and you can also do
148:34 - with this this this one with a high
148:36 - dimensional spaces okay so I hope that
148:38 - that that you are able to make sense out
148:40 - of it and I also hope that uh you you
148:42 - are able to understand a little bit
148:44 - about this okay so that is the dot
148:46 - product between these two Vector just
148:48 - you need to understand the numeric
148:50 - understanding of dot product that
148:51 - element wise product and add it all up
148:54 - that's it okay so I hope that you are
148:56 - able to make sense out of it and I also
148:58 - hope that that you are able to
148:59 - understand everything this was the dot
149:01 - dot product between these two vectors
149:03 - and a transpose of vectors and a
149:05 - matrices and I also hope that that till
149:08 - now you're you're able to understand
149:10 - most of it out of it and I also hope
149:12 - that uh you will utilize this resource
149:15 - and share this resource to everyone that
149:17 - motivates me to work on this content s
149:20 - and so in the next video we'll be
149:21 - talking about some of the types of mates
149:23 - and then and then we'll talk about rank
149:25 - Trace operators determinant I Val igen
149:28 - vectors and solving the systems of
149:30 - equations and then our L linear algebra
149:33 - will be done so I hope that you like
149:35 - this video I'll be catching up your next
149:37 - video till then bye-bye
149:41 - [Music]
149:52 - hey everyone welcome to this next
149:53 - lecture on linear algebra and M 2 and I
149:57 - really really hope that you are enjoying
149:59 - this course first of all I want to
150:01 - thanks thanks you and congratulate you
150:03 - as well that you had first you had
150:05 - completed your first week and I'm I'm
150:08 - very much happy to see so much of
150:10 - enthusiastic students who are watching
150:13 - these lectures without any kind of
150:14 - problems and they're able to understand
150:16 - and leaving their great feedback in the
150:18 - comment box and I'm seeing the watching
150:20 - hours increased so I would like to thank
150:23 - it's giving me a lot more motivation to
150:25 - make these such videos for free as well
150:28 - as um I would just salute you for your
150:30 - consistency and I would also salute for
150:33 - you m utilizing these kind of materials
150:36 - uh who are who are for free and one
150:38 - thing which I wanted to say that we
150:40 - recently in yesterday we uh we we we
150:44 - released our first homework assignment
150:47 - of the previous week which is the the
150:49 - homework assignment consist of the
150:51 - questions from the five lectures
150:53 - previous five lecture lecture 1 to five
150:56 - that is your first week lectures so
150:59 - basically in that we included the
151:00 - homework homework assignments all the
151:03 - questions from the topics which are
151:04 - already taught okay so if you go and see
151:07 - and I and and I would like to thank
151:10 - vayak Vishnu who has contributed 70% of
151:14 - prepar preparation of these questions
151:16 - who is one of the teaching assistants of
151:19 - our uh of our CS M2 so I would like to
151:23 - thank you thank him and I would also
151:25 - like you to thank him on the Discord
151:27 - server or whenever the comment box
151:29 - thanks vayak Bish so it it would
151:32 - motivate him as well he's doing the
151:34 - community work for free so so let's so
151:37 - here is the pro programming assignment
151:38 - to sorry not programming assignments
151:40 - homework assignment where you're getting
151:42 - around 32 questions and these questions
151:45 - are covering from very Basics to a
151:47 - conceptual understanding of the
151:48 - particular subject having in mind to
151:51 - have a good practice of yourself of the
151:53 - topics which are already taught so you
151:55 - may think hey are you you're doing this
151:57 - stuff the reason why I'm I'm making you
151:59 - practice these stuff is the reason one
152:02 - is mainly when you go in deep learning
152:04 - to have a conceptual understanding of
152:06 - what are vectors and how it is
152:07 - performing computations and what are the
152:09 - resulting Vector size and etc etc etc so
152:13 - it will help you to to to to practice a
152:16 - lot and it would also help you to
152:18 - understand the conceptual understanding
152:20 - or on the very depth understanding of
152:22 - these vectors and and and we we also
152:25 - seen some matrices and then we are
152:27 - performing something and then uh I also
152:30 - taught you about linear combinations xx
152:32 - and 3 contains of linear combination
152:35 - where we are talking about various stops
152:37 - over here and the these These are the
152:39 - questions which are very very very very
152:41 - nice questions which are prepared by
152:42 - vayak Vishnu and as well as I had also
152:45 - added some questions over here which are
152:47 - also related to your deep deep learning
152:49 - context and then you finally have a
152:51 - transformation and this this this is
152:53 - also a great amaz amazing uh uh the
152:58 - questions which are there and it will
152:59 - help you in deep deep Learning Journey
153:01 - that how actually the linear
153:02 - transformation works and behind that and
153:05 - then the we we we talked about
153:07 - transposing the dot product of two
153:09 - matrices or vectors okay so that is the
153:12 - specific thing and then we we ask you to
153:14 - verify this property so I would
153:15 - definitely ask you to visit these things
153:18 - it will it Sol it assignment upload it
153:21 - in your LMS learning management system
153:23 - if you have enrolled into that it's
153:24 - absolutely free for everyone please
153:26 - please see the student handbook in the
153:28 - description down box below and go over
153:31 - there and into the LMS and and Summit
153:34 - your homework assignment and then you'll
153:36 - be getting the detailed feedback on what
153:38 - questions youve done wrong and what
153:39 - question you haven't done wrong okay so
153:41 - let's get started with this video so the
153:43 - title of the video you might have
153:45 - already imagined is the types of a mates
153:48 - types of mates so what are the types of
153:50 - matrices that that that we will study
153:52 - and and and some of the types of
153:53 - matrices are maybe not come in your
153:55 - journey of deep deep learning but I
153:58 - would say ke whenever you're studying
153:59 - the something let's study full of that
154:02 - okay so do not let's let's let's not
154:03 - study the partial stuff so let's study
154:05 - full but most of the thing which I'm
154:07 - going to teach is is being used
154:08 - frequently not too much frequently but
154:10 - it's being used sometimes me when when
154:13 - you talk with some great mathematicians
154:15 - or a deep learning Engineers to take
154:17 - these kind of words so it so it should
154:19 - not might confuse you so that's a VA I
154:21 - asked about so the types of matrices so
154:24 - the T first types of matrices which you
154:26 - already know about in other words like
154:28 - row Vector which is something called as
154:30 - row matrices okay or a row
154:33 - Matrix a row so let me add one thing
154:37 - over here row row Matrix row Matrix okay
154:44 - so what is row Matrix so can you define
154:46 - what is what is a row Vector can you
154:48 - just Define it so the row Vector is the
154:51 - is the is the Matrix which have only one
154:53 - row so the same way row Matrix are the
154:56 - one which has only one row so it you you
154:59 - can say just a row Vector uh yeah so the
155:02 - matrices which have only which have only
155:05 - one one row is called the row Matrix so
155:10 - for example for example the example can
155:13 - be 1 2 3 okay so this is the first this
155:19 - is your row Matrix okay so because it
155:21 - has because it it only have only one row
155:24 - so we can mathematically we can we can
155:26 - write that a is equal this is a matrix
155:30 - where a i g where we have a m * n Matrix
155:35 - where M denotes the number of rows and N
155:37 - then denotes the number of columns in
155:39 - this you have m equals to 1 you have m
155:43 - equal to 1 then it is called as a row
155:45 - Matrix so what is a row Matrix row
155:47 - Matrix is a one which has only one row
155:50 - let's let's talk about second second
155:51 - kind of Matrix which is column Matrix
155:55 - column column Matrix so sorry for in in
155:58 - on the eve of diali many the people are
156:01 - just busting up the crackers in India so
156:04 - yeah don't no no problem in that so what
156:07 - is a column Matrix so column Matrix
156:09 - which have only one column so uh and you
156:13 - have you have heard about column Vector
156:15 - so the same way we have column Matrix
156:17 - which only have only one column and
156:19 - these are these are used very frequently
156:21 - in the in the era of deep learning and
156:24 - and and it is very precise to use these
156:26 - names in deep learning to have to to
156:29 - follow the mathematical conventions ra
156:30 - rather than saying it is a it is a it is
156:34 - a call this the shape either shape you
156:37 - can just say that okay it's is a row
156:38 - Matrix or it is a column Matrix or it is
156:40 - a row vector or it is a column Vector
156:43 - okay so here you have a matrix a where
156:46 - you have a matrix a where you where you
156:48 - have M * n which is a size and where the
156:51 - M can be anything you can be any number
156:52 - of rows but you have only one column
156:55 - okay so here here n n should be equals
156:58 - to one so you should remove this and
156:59 - write one to be considered this as a
157:03 - column Matrix okay the next kind of
157:05 - Matrix we should talk about is zero or
157:07 - null Matrix so as you have already uh
157:11 - imagined about this zero all n Matrix
157:14 - and these are very very easy kind of
157:16 - remembering it's not a I'm not teaching
157:18 - teaching any Rockets science is very
157:20 - very easy to understand so so if in all
157:24 - the matrices or all all the elements
157:26 - into that matrices are zero okay so all
157:29 - the all the elements all the elements in
157:34 - the
157:35 - matrices in the
157:37 - matrices are Zer then that Matrix is
157:41 - called zero Matrix okay so for example
157:45 - you have a where you have 0 0 0 where 0
157:49 - 0 0 0 0 0 where you have a 3X3 Matrix
157:54 - and this is called a zero Matrix or
157:58 - sometimes you call it as a 3X3 null
158:01 - Matrix okay so when someone tell you hey
158:04 - can can you can you just tell what types
158:05 - of Matrix this is okay this is a zero
158:07 - Matrix okay when someone ask you hey
158:10 - you're given a null Matrix what what
158:12 - happened when you multiply this new
158:14 - Matrix with another Matrix where the
158:17 - Matrix satisfy all the Mator
158:18 - multiplication property or Dimension
158:20 - property so you you will say okay uh you
158:23 - will say okay what is a null Matrix so
158:25 - null Matrix are nothing but a zero
158:27 - Matrix okay so that is just a zero
158:30 - Matrix which which is given another name
158:32 - which is called null Matrix then the in
158:35 - in that you have all the elements to be
158:37 - zero okay the next is the next is your
158:41 - favorite single ton Matrix okay so in in
158:44 - in Java one of my friend was uh taking a
158:47 - single turn double turn so in the same
158:49 - way we have single turn matrices okay so
158:53 - single single turn
158:56 - matrices
158:57 - so so in single ter matrices your all
159:00 - the matrices are are or or you say you
159:03 - have only one element into that Matrix
159:06 - okay not all the matri you have total of
159:09 - one element in that Matrix okay so
159:12 - that's why we call it as a only one
159:15 - element only one element in the Matrix
159:19 - in The Matrix and so sorry I'm not too
159:21 - much of creative I don't follow the
159:23 - rules of changing the color and then
159:25 - writing it out I should develop that uh
159:28 - thing so let's so let's use different
159:30 - different color then so the fifth one is
159:32 - the fifth one is horizontal Matrix so
159:35 - let's give some examples of this because
159:37 - I I I I I haven't given example of this
159:39 - which is two then it can be one then it
159:42 - can be three these are the for example a
159:47 - is a matrix where we have this is a
159:48 - single turn m matrices Matrix etc etc
159:51 - etc so these are called the single T
159:54 - Matrix you may have St single ton in in
159:56 - your sets or discret mathematics if you
159:59 - have if you have take taken the course
160:01 - on discret mathematics so I would ask
160:02 - you to do not take the course but yeah
160:04 - remember remember the single T either it
160:07 - is not used too much but yeah you you
160:09 - should know the you should know what a
160:10 - single T because when someone ask you
160:12 - you should be able to answer it
160:15 - horizontal Matrix horizontal Matrix that
160:18 - is a that is a good deal so horizontal
160:20 - measor so C and Define if anyone has
160:22 - taken the linear algebra class please
160:25 - feel free to go down in the description
160:26 - box below please so no description it's
160:29 - comment okay so please feel free to go
160:31 - in comment box please write what is a
160:34 - horizontal Matrix and and it's very very
160:37 - easy it's again very very easy so for
160:39 - example so for example uh for example
160:43 - here you have 1 2 3 4 okay and then you
160:49 - have uh and then you have your favorite
160:53 - and then you have uh 6 9 8 2 so I'm
160:57 - going to consider this Matrix as a
161:00 - horizontal horizontal Matrix you may ask
161:04 - why you should consider this as a
161:06 - horizont horizontal Matrix so just tell
161:08 - me the size size of the Matrix is the
161:11 - size size of the Matrix is what uh 2 by
161:15 - 4 two uh rows and four columns and here
161:19 - the The Columns is greater than rows so
161:21 - that's why we that that is a horizontal
161:23 - Matrix so whoever Matrix where the
161:26 - number of a columns is greater than the
161:29 - number of rows then that's called the
161:31 - horizontal Matrix got it so that is the
161:35 - horizontal Matrix where your number of
161:38 - rows Sor not rows number of number of
161:41 - columns is greater than the number of
161:44 - her rows so that that that is one of the
161:46 - example of a horizontal Matrix now let's
161:49 - see some more examples some more uh some
161:52 - more Matrix types which is something
161:54 - called as vertical Matrix so can you
161:56 - tell me what is
161:58 - vertical Matrix just Define me what is a
162:02 - vertical Matrix so let's make a matrix
162:04 - let's make a matrix let's let's let's
162:06 - make a matrix where you have 1 2 3 4 5 6
162:12 - 7 8 9 10 11 12 okay so this is your this
162:18 - is I'm going to consider this as a
162:20 - vertical Matrix now tell me
162:22 - why this is a vertical Matrix the reason
162:25 - why is a vertical Matrix so let's let's
162:27 - count the size so we have a total of 4x3
162:31 - Matrix and here your number of rows is
162:34 - great greater than number of columns so
162:37 - that's why this is a vertical Matrix in
162:39 - horizontal your number of columns should
162:41 - be greater than uh greater than number
162:43 - of rows to be called as a horizontal
162:45 - Matrix but in vertical Matrix totally
162:47 - opposite of that here your number of
162:49 - rows should be greater than your number
162:51 - of columns okay to be called as to to be
162:54 - said as a vertical Matrix or in formal
162:57 - definition which is Tau in a school is a
163:00 - a matrix is said to be a vertical Matrix
163:03 - if and only if its number of her rows is
163:07 - greater than the number of her
163:09 - columns it's it's it started my school I
163:12 - just I I I just want to thank my school
163:15 - to teach me these definitions no not
163:17 - exactly definitions of vertical Matrix
163:19 - but yeah the definition format or
163:21 - template to tell is that this is said to
163:24 - be this because this okay so I follow
163:26 - the template of my school so thanks to
163:28 - my school H shout out to Sunan cat okay
163:31 - cool the square of Matrix so what is a
163:33 - square Matrix I would say pay attention
163:35 - to this Square Matrix is used
163:37 - extensively whatever we'll study in the
163:39 - like diagonal matrix or or whatever okay
163:42 - like determinant or or igen vectors and
163:45 - IG values we are going to take this
163:47 - Square Matrix so what are square Matrix
163:51 - so let tell me what are square Matrix
163:53 - The Matrix which looks like square is
163:56 - that it yeah so so the Matrix where
164:00 - you're or just wait now here's here's
164:03 - your Matrix here's your Matrix 1 2 2 4
164:07 - and here the here's your a matrix and
164:09 - here's your B Matrix which is 1 2 3 1 2
164:14 - 3
164:17 - okay or let let's consider this as a
164:20 - let's consider let's let's cons consider
164:22 - this as a okay so here you have a total
164:25 - of 2 by two Matrix and this is a total
164:29 - of uh what do you say a 3X3 Matrix so
164:33 - here your number of rows matches with
164:35 - the number of columns so the square
164:37 - Matrix are the one where the number of
164:39 - rows matches with the number of columns
164:43 - okay so the square Matrix is which where
164:46 - your number of rows where your number of
164:48 - rows is equals to the number of columns
164:54 - okay so the the square Matrix is is in
164:58 - which your number of rows is equal to
165:00 - the number of columns then it is said to
165:02 - be a square Matrix so again my school
165:05 - form of definition a matrix is said to
165:08 - be a square Matrix if and only if it's
165:11 - it's it's a real number for rows should
165:14 - be exactly equal to the number of a
165:17 - columns in your Matrix
165:19 - okay so that is your Square mat again
165:22 - shout out to Sun uh just just for your
165:25 - information uh this these are the things
165:28 - which are not taught my school till now
165:30 - I'm in class n so till now they haven't
165:32 - taught this but yeah I know the form I I
165:35 - know I know the definition template
165:36 - because I and in in my school I used to
165:38 - study a lot more definitions so I know
165:41 - the definition of this they the
165:43 - definition is starts with this the same
165:45 - way uh this is said to be this because
165:48 - this and that so that's why we got it
165:50 - this so the same way I frame a square
165:52 - Matrix is said to be a square Matrix a
165:54 - matrix is said to be a square Matrix if
165:57 - and only if your number of rows is
165:59 - matched with the number of columns and
166:01 - so thus if it is follows then it's a
166:03 - square Matrix so so we we know about the
166:06 - square Matrix now so let's go further
166:08 - into understanding the the the diagonal
166:11 - matrix so so what is diagonal
166:14 - matrix what is
166:17 - diagonal what what is diagonal matrix
166:21 - can anyone tell me what a diagonal
166:23 - matrix is anyone try it out so all the
166:28 - elements except the principal diagonal
166:31 - or or let's start with let's start I
166:35 - just want to scho that guy who is
166:37 - bursting the crackers I don't want to
166:39 - see him I'm making my video why is
166:42 - bursting his crackers outside oh my gosh
166:45 - no no problem in that as well Okay cool
166:48 - so let's make a matrix 1 2 3 0 0 0 0 0
166:54 - okay so this is called the diagonal
166:57 - matrix because your your your all the
167:00 - elements in your diagonal matrix except
167:04 - the prpal diagonal this is this
167:06 - principal diagonal and this is the
167:08 - principal diagonal so all the elements
167:11 - into that Matrix except the principal
167:14 - diagonal of a square Matrix so diagonal
167:17 - matrix should be a square Matrix because
167:19 - it is a 3X3 Matrix so it is a square
167:21 - Matrix R Zer so all the elements into
167:25 - that diagonal matrix of a square Matrix
167:28 - are zero then that is called as a
167:31 - diagonal matrix so let's see some more
167:33 - so for example you
167:36 - have uh Z 1 2 oh my gosh zero I think I
167:42 - I I I done wrong so 1 0 0 0 2 0 then 0
167:49 - 03 so this is oh I I I made the same
167:53 - thing again no problem in that oh this
167:56 - is left okay so that is the all the
167:58 - principal diagonal is your is your uh
168:01 - what do you say the the the nonzero and
168:04 - every every elements of except that is
168:06 - your not zero then that's called a
168:09 - diagonal matrix so I hope that you're a
168:11 - able to make sense out of it so the
168:13 - diagonal matrix should be a square
168:16 - Matrix and a square Matrix is nothing a
168:19 - matrix which said to be a square Matrix
168:23 - if and only if your number of rows match
168:25 - with the number of a columns and thus it
168:29 - is called the square Matrix because it
168:31 - looks like square but what is
168:33 - rectangular Matrix so one thing which
168:35 - I'm to mention
168:37 - rectangular rectangular Matrix so what
168:40 - is a rectangular Matrix here where your
168:42 - number of a rows does not matches with
168:45 - number of a columns oh my gosh there's a
168:49 - contradiction so that is nothing but
168:51 - called a regular Matrix sorry oh my gosh
168:54 - it's rectangular Matrix so let let me
168:57 - Define this from my school work so
168:59 - thanks thanks thanks my school sit down
169:00 - sit down yeah so uh what is rectangular
169:03 - Matrix a matrix is said to be a
169:06 - rectangular Matrix if and only if it's
169:09 - it's number of rows that's not match
169:11 - with the number of a columns so that's
169:14 - the rectangular Matrix so so the follow
169:16 - following example is a rectangular
169:18 - Matrix so maybe you may have 2 1 3 4 2 1
169:24 - 3 4 that is your number of rows is 2x 4
169:29 - where 2 is not equals to 4 so that is
169:33 - the rectangular meter that looks like
169:35 - rectangle so that's why we have written
169:36 - the rectangular
169:38 - Matrix cool so let's go further into
169:41 - learning about scalar
169:44 - Matrix so what is scalar so first of all
169:47 - Define a scalar and then try to identify
169:49 - what is scal or Matrix please go ahead
169:51 - and write your answer let's give you
169:53 - give the guess guys U I'm just here to
169:55 - have a fun with you all so give a guess
169:57 - what do you mean by scal or mat because
169:59 - when I started first I given a very good
170:02 - guess and that was totally wrong because
170:04 - this this is like a scaler Matrix so I
170:07 - was little little bit okay scaler is
170:09 - just a number and scaler and these
170:11 - Matrix are are the are the aray of a
170:14 - numbers so how I can consider a scaler
170:16 - as ARS of a numbers that is the best
170:19 - assumption that I made at that point but
170:21 - no problem the scalar Matrix here's the
170:24 - here's your scalar Matrix in in front of
170:26 - you here's the scalar Matrix in front of
170:29 - you so you have - 7 0 0 0 - 7 0 0 0 -
170:40 - 7 0 0 -
170:43 - 7 so listen so this is called scalar
170:46 - Matrix this is called the scalar Matrix
170:49 - this is called the scalar Matrix but now
170:52 - you will say hey I use just now you
170:54 - taught the diagonal matrix in the
170:56 - diagonal matrix you all the elements
170:59 - except the principal diagonal are equal
171:02 - or are zero then it is a diagonal matri
171:04 - so here also your all the elements all
171:07 - the elements except your principal
171:09 - diagonal are zero so why not be calling
171:12 - at as diagonal matrix so I would ask you
171:15 - to have a closer look at this is and
171:19 - tell me what you see over here so if you
171:24 - if you if you if you zoom in further or
171:26 - or if you or if you wear your sunglasses
171:29 - not sunglass if you wear your goggles
171:31 - with minus 2.5 power you will see that
171:35 - your diagonal matrix uh diagonal
171:38 - diagonals principal diagonals scalers
171:41 - are all equal okay so that's what make
171:44 - it as a scalar mates okay so what is
171:48 - scalar Matrix a scalar Matrix is say
171:51 - sorry not a scalar a matrix is said to
171:53 - be an scalar Matrix if and only if if it
171:57 - is principal if all the elements in the
171:59 - principal diagonal are non are zero and
172:03 - principal diagonal elements are should
172:06 - be equals to each other okay so that
172:09 - that is called the scalar matrices so if
172:12 - all the elements in the diagonal matrix
172:15 - okay so all the elements in the diagonal
172:17 - matrix and and what are diagonal matrix
172:20 - diagonal matrices are the matrices where
172:22 - the elements except the main diagonal
172:24 - are zero so if all the elements in the
172:27 - diagonal matrix are uh EX in all the all
172:31 - the matrices in the diagonal matrices uh
172:34 - are of the of of the D is is equal means
172:37 - the principal principal diagonal is
172:40 - equal then that is called the scalar
172:42 - Matrix so that is a scal matrix so
172:44 - please see the notes in description for
172:46 - the F the definition whatever I'm
172:47 - telling so you could not write it out
172:50 - please see the description for the notes
172:52 - of whatever I'm
172:54 - telling okay so so so so one of the so
172:57 - this is your first example so let's say
172:59 - second example second example otk 5 0 0
173:05 - 0 < TK 5 0 0 0un five what is this this
173:11 - is a
173:13 - scalar Matrix now now what what what
173:17 - what do I tell to you is to multiply
173:18 - with some multiply it's with with with
173:21 - the some Matrix okay so multi multiply
173:23 - with the same Matrix multiply with the
173:25 - same Matrix M multiply some The Matrix
173:27 - and then you will getting some other
173:29 - result some other result but I want to
173:32 - tell is to have a matrix like this uh
173:35 - where your all the diagonals are
173:37 - one all the diagonals are one that is
173:41 - your so now multiply with any Matrix
173:44 - just to just to make sure that is a
173:45 - matrix M multiplication is defined
173:48 - multiply with any Matrix or a vector any
173:51 - Matrix or or a vector you will be
173:54 - getting you will be getting your your
173:57 - your Matrix so this is this is this this
173:59 - this is your scaler Matrix so let's say
174:01 - s as as as of now and this is your any
174:04 - Matrix or vector v so when you multiply
174:06 - s * V answer will be V means exact so is
174:10 - it is by multiplying by one you will get
174:12 - exactly so please see please do and see
174:14 - the experiment so when you it this when
174:17 - you multiply this Matrix with some other
174:19 - Matrix or vector you will be getting it
174:21 - is just like multiplying this Vector
174:24 - this is this is one so whenever you
174:25 - multiply s * V means this these types of
174:28 - Matrix where your principal principal
174:30 - diagonal is one all are one then you
174:33 - multiply with some Matrix or vector then
174:35 - that then that will yield or or result
174:38 - to this Vector original Vector to which
174:41 - you multiply that scalar Matrix to okay
174:44 - so that is so so so scientist have seen
174:48 - this y result and named this as a
174:52 - identity Matrix name this as identity
174:55 - Matrix or a unit Matrix or a unit Matrix
175:00 - okay so when you when you multiply this
175:02 - identity Matrix with any Matrix that is
175:05 - simply by multiplying one and you will
175:07 - be getting a result which is V okay so
175:09 - you'll be getting the same result so you
175:11 - have you may consider this Matrix with
175:13 - one and if you multiply any even 10 you
175:15 - will be getting your 10 as output so it
175:17 - is same as that identity Matrix oh my
175:21 - gosh that that the one who's is just
175:24 - flying the of pollution I am really not
175:26 - liking that no problem again so here
175:29 - over here your identity Matrix are just
175:32 - like a m multiplication by one so please
175:35 - pre to this is a wi property which we
175:37 - have given a new name is the Matrix or
175:40 - the scaler Matrix here in your principal
175:42 - diagonals are all one then that's uh
175:45 - nothing but uh identity Matrix
175:49 - okay let's let's go on next page the
175:51 - next page so let's talk about some last
175:54 - matri with something called a triangular
175:56 - Matrix
175:58 - triangular triangular Matrix so the
176:01 - Triangular Matrix are of two types so a
176:04 - square Matrix I'm just try to Matrix so
176:07 - a square Matrix is said to be a
176:10 - triangular Matrix
176:12 - if the elements if the
176:16 - elements if the elements Ave oh my gosh
176:20 - my hand ratting
176:22 - Ave or below the principal diagonal
176:26 - below the principal diagonal
176:30 - principal diagonal are
176:33 - zero okay so for example uh you have the
176:38 - diagonal 3 4 6 1 2 3 and z z0
176:49 - okay okay so this is your oh my gosh
176:51 - what do I made over
176:57 - here 3 1 2 0 4 3 0 0 6 okay so it is
177:05 - telling the Triangular this this is a
177:07 - triangular Matrix because all the
177:09 - elements if the elements above means
177:13 - Above This is this this is your
177:15 - principal diagonal this is your
177:17 - principal diagonal okay so whatever Ave
177:20 - if either above or below either above or
177:24 - below yeah here is your or okay so
177:27 - either above or below either above or
177:30 - below either above or below um to the
177:34 - main principal diagonal are zero then
177:35 - that's called a triangular Matrix so
177:38 - here above is non Zer but below is zero
177:40 - so that's how we call it as a triangular
177:42 - Matrix because it forms a triangle so
177:44 - that's why it's called a triangular
177:45 - Matrix and this is called the upper
177:48 - triangular and here here it forms the
177:50 - triangle so here it forms a triangular
177:54 - part okay so that is the upper triangle
177:56 - so this this where your zeros are below
178:00 - the main principal diagonal so that is
178:02 - called the upper upper triangular Matrix
178:05 - upper triangular Matrix and for example
178:08 - you have 1 0 0 2 3 0 4 5 2 and over here
178:16 - this is your main diagonal okay okay and
178:19 - above you have zero and below so that
178:21 - that is the uh that is the uh lower
178:23 - triangular Matrix that is a lower
178:25 - triangular because it forms a triangle
178:27 - triangle low and below below of the main
178:30 - diagonal okay so that is the lower lower
178:33 - triangle Matrix and upper triangular
178:35 - Matrix cool so I hope that you
178:37 - understood so just just to re
178:39 - recapitulate the Triangular Matrix is
178:41 - said to be a triangular Matrix if the
178:43 - elements above that principal diagonal
178:46 - or below the principal diagonal are zero
178:48 - the the the the the the elements uh the
178:51 - or or the what do you say if the zeros
178:55 - are below the principal diagonal of the
178:57 - Triangular Matrix then that is called
178:59 - the upper triangular Matrix because it
179:01 - forms the triangle upper of the
179:02 - principal diagonal and if the in in the
179:05 - in the Triangular Matrix to the of your
179:07 - principal diagonal of if your zeros are
179:10 - above your principal diagonal then
179:11 - that's nothing called as a triang a
179:14 - lower triangular Matrix that is of two
179:16 - types cool the last thing which I'm to
179:18 - discuss is about symmetric Matrix okay
179:22 - so is about symmetric matrix it's about
179:25 - what do you say repeat me with me
179:27 - symmetric Matrix symmetric who knows so
179:30 - you have this so you have a and when you
179:33 - do this so it should be foldable so that
179:35 - is symmetric so this this paper is
179:36 - symmetric okay so the same way the the
179:39 - the M matri can be symmetric as well the
179:41 - M matrices can be symmetric as well so
179:44 - what is symmetric Matrix so so the
179:46 - definition of a symmetric Matrix
179:48 - definition of a symmetric Matrix if your
179:50 - a transpose is equals to the a a is if
179:56 - your a transpose is equals to the a so
179:59 - that's where we call that as a as a as a
180:04 - triangle or or or a symmetric Matrix
180:07 - okay so it's it's it's it's called a
180:09 - symmetric Matrix if your a transpose is
180:12 - equals to A itself okay so so for
180:16 - example so for example you I'm just
180:19 - going to take you you can think of any
180:21 - example this is your task but I'm just
180:23 - just going to take a small example
180:26 - of uh 2 4
180:31 - 69 okay so 2 469 so when you add the
180:35 - transpose so this is your 2x2 matrix and
180:38 - then if you do the transpose you'll be
180:40 - nothing
180:42 - 2649 2x two 2x two okay so that is the
180:46 - 2x two so over here here you follow the
180:50 - equality of a matrix so the you if you
180:52 - remember the equality of a matrix if you
180:55 - remember the quality of a matrix of of
180:58 - the matrices so if if a matrix a is one
181:02 - equals to Matrix B if its corresponding
181:06 - elements if it's corresponding elements
181:09 - this to this this to this this to this
181:11 - this to this are equal and they are of
181:13 - same order and they are of same size
181:16 - okay so they are of same size but this
181:19 - is equals to this okay four is not
181:21 - equals to 6 so here this is not a
181:24 - symmetric Matrix okay so this is not a
181:27 - symmetric Matrix so your all symmetric
181:29 - Matrix should be should be square Matrix
181:32 - to be symmetric okay but not every
181:34 - Square Matrix can be symmetric but but
181:36 - you for for being symmetric your your
181:39 - Matrix should be squar Matrix for for
181:41 - being symmetric but it is but it's not
181:43 - guaranteed that your every Square Matrix
181:45 - will be symmetric but for being metric
181:48 - it is you have to have a square Matrix
181:51 - for example you have 2 2 2 2 apply the
181:56 - transpose on this what it would be 2 2 2
181:59 - two 2 by two 2 by two this is this is
182:02 - correct this is correct corresponding
182:04 - are also correct the size is also
182:06 - correct that is the symmetric Matrix
182:09 - that is the symmetric Matrix and I hope
182:12 - that you understood about the concept
182:13 - behind symmetric Matrix so this is all
182:15 - so that was we had a talk on these stuff
182:19 - so I hope that you like this video and I
182:21 - also talked a lot and sorry for the
182:23 - crackers please find that guy and beat
182:26 - him as much as you can who's fing up the
182:28 - crackers outside so sorry I I I included
182:32 - Hindi but no problem okay please feel
182:34 - free to sco that guy not beat him
182:36 - because Dali is Festival of having fun
182:39 - but yeah please SC that because they
182:41 - disturb you in studying no problem uh so
182:45 - let's so I I I hope that you understood
182:46 - and please feel free to do your home
182:48 - homework assignments so here is the
182:49 - homework assignment uh the discussion
182:51 - for the solutions of the program
182:53 - programming assignment or sorry homework
182:55 - assignment is being released soon in the
182:57 - form of video so you can assist that but
182:59 - I will wait for two two or 3 days and
183:02 - then I will release one video on uh
183:04 - solving these homework assignments
183:06 - please feel free to do this and please
183:08 - feel and I'm not giving these notes
183:10 - because these notes are already
183:11 - available in the description down box
183:13 - below in the form of some uh good good
183:15 - hand handwriting in the description down
183:17 - box below please please feel free to
183:18 - assess thanks for seeing this video I'll
183:20 - be catching up in the next video till
183:21 - then bye-bye have a great day
183:27 - [Music]
183:35 - bye-bye hey everyone so let's get
183:37 - started with a new lecture on lecture
183:39 - number seven which is on determinant and
183:41 - this is one of the one of the again I
183:43 - would say important concept to study
183:45 - because in principal comat analysis or
183:48 - whether you uh it it it it comes a lot
183:50 - in your machine Learning Journey as well
183:52 - as well as in deep Learning Journey
183:54 - because it tells you how to solve or
183:57 - solving the linear equations or or or or
184:01 - if if I talk about in terms of linear
184:03 - transformation it just tells you how the
184:06 - how the how the change in area or a
184:08 - volume occurs okay and and determinant
184:11 - is nothing when you it's nothing but you
184:14 - just TR you just give some Matrix and
184:16 - then you get one number so we'll be
184:18 - talking about that in detail in this
184:20 - session uh I I think you'll you'll get a
184:22 - lot from this session and and you you
184:24 - can make your own notes or the notes is
184:26 - in description un boox below either it
184:28 - would be updated soon but yeah uh I it
184:31 - is it is already been made it's just
184:33 - sent for processing and that it will be
184:35 - into your description if you like this
184:37 - video please be sure to subscribe this
184:39 - channel as well as like this video and
184:41 - comment because YouTube algorithm knows
184:43 - okay this is a good video to recommend
184:45 - because many many other the people say
184:46 - uh your channel is underrated so I want
184:48 - you I want this channel to be rated
184:50 - Channel because I work a lot on this
184:52 - channel Okay cool so let's get start
184:56 - with solving uh what is determinant so
184:58 - we'll we'll get onto the geometric
185:00 - meaning soon but uh in in determinant
185:04 - what you do if you know about a square
185:06 - Matrix if you know about a square Matrix
185:08 - which which we talked about and and I
185:11 - have told that is very important Square
185:13 - Matrix are very important is used
185:15 - extensively in linear algebra to to use
185:18 - this term terminology so Square Matrix
185:21 - is nothing where your where your number
185:23 - of a rows is equals to the number of a
185:26 - columns for example uh your Matrix a is
185:30 - is maybe it can be 2 2 2 two okay so
185:33 - this is a 2X 2 where n = 2 and M = 2 so
185:38 - n * n Matrix where your Square Matrix is
185:40 - equals to where your number of rows is
185:43 - equals to the number of a columns okay
185:45 - so that is the so this is so what what
185:47 - you do you take your Square Matrix and
185:50 - determinant takes one square Matrix
185:52 - where the number of rows is equals to
185:54 - the number of colums you write
185:56 - determinant of uh an A and A A should be
186:00 - the square Matrix a should be the square
186:03 - Matrix and then you get one scalar or or
186:07 - or a number as an output when you apply
186:09 - the determinant function or or or when
186:11 - you take out the determinant of that
186:14 - Matrix okay so now how this is useful we
186:18 - will see how do we take out the scalar a
186:20 - just in a second numerically but but uh
186:24 - but when you um how how the determinant
186:27 - is useful this is this is one of the
186:29 - most important concept to know so the
186:31 - determinant is useful in in solving and
186:34 - solving linear equation and linear
186:37 - equation is used very very extensively
186:40 - solving linear equation or maybe it can
186:42 - be useful in in in in in in knowing okay
186:46 - in knowing
186:48 - how linear transformation and knowing
186:51 - how linear how linear transformation
186:54 - transformation change their area or the
186:57 - volume okay change their area
187:00 - transformation change their area change
187:03 - their area over volume or volume okay
187:07 - not over it's or volume and it is also
187:10 - and it is also useful uh in other stuffs
187:13 - like uh when solving some some
187:16 - computationally it it it it it does
187:18 - reduces some computer not exactly mean
187:20 - uh doing efficiently not exactly I would
187:22 - say efficiently I would say very
187:24 - precisely so solving the particular
187:27 - linear equation and is used a lot in
187:29 - that so that's why we take out the
187:31 - determinant of a matrix and that when
187:33 - you take out the determinant of a matrix
187:35 - you simply give a squar matrix root to
187:38 - that determinant and then after when you
187:40 - take out the determinant you will get
187:42 - one scaler okay so this is what the this
187:45 - is this is this is what we use and and
187:47 - if you if you talk about um in machine
187:50 - machine learning use case so in machine
187:51 - learning if if you know about machine
187:53 - learning in machine learning you have
187:55 - something called as dimensionality
187:57 - reduction method and in and in that you
188:00 - take out the determinant of that
188:02 - co-variance Matrix so co-variance Matrix
188:05 - okay so when you take out the
188:07 - determinant of that co-variance Matrix
188:09 - and then you and then and and and and
188:11 - then go further into solving the
188:13 - particular problem okay so not exactly
188:16 - covariance yeah so you take out the
188:17 - termin and then you go further into uh
188:20 - into other stuffs like uh uh the igen
188:24 - vectors and igen values and they are
188:26 - extensively used the determinant are
188:28 - extensively used in the igen vectors and
188:30 - igen values in principal component
188:32 - analysis okay so I hope that this is
188:34 - clear why we use determinant and and and
188:37 - what's the determinant is now now we
188:40 - need to care about how do we take out
188:42 - the scalar value because we give a
188:43 - function because we just give a a square
188:46 - Matrix into that determinant and then we
188:49 - will we are going we are we are just
188:50 - getting a scaler as an output so how do
188:53 - we even do that uh so for for doing that
188:57 - assume that you have a matrix a you have
188:59 - a matrix a which is nothing but 2x two
189:02 - so I'm just going to write um a b c and
189:06 - d okay so you have a matrix a b c d
189:09 - which is a 2X 2 Matrix so when you take
189:13 - out the determinant of that Matrix a
189:16 - which is nothing but which is nothing
189:17 - but so a means you take out the product
189:21 - of the diagonals you take out the
189:22 - product of the diagonals a minus BC a D
189:27 - minus BC so for example you have a
189:30 - matrix uh 2 3 4 6 and then you want to
189:35 - take out the Matrix the determinant of
189:37 - that Matrix 2x2 matrix which is nothing
189:39 - but 2 * 6 2 * 6 - 3 3 3 * 4 3 * 4 which
189:45 - is nothing 6 6 2 12 - 3 42 that will be
189:50 - nothing but zero zero is the answer or
189:52 - determinant of this Matrix okay so the
189:55 - terminant of a matrix can be zero we
189:57 - have we don't have any conditions but
189:59 - yeah the determinant of this Matrix is
190:01 - zero okay so this is how you take out
190:03 - the determinant of a matrix
190:04 - geometrically
190:06 - speaking okay so one thing that I want
190:08 - to highlight over here let's say for for
190:11 - example uh what does it mean
190:13 - geometrically what does it mean
190:15 - geometrically so so let's uh let me make
190:19 - one more page so that I could explain
190:21 - you what does it mean geometrically
190:22 - speaking what does it mean geometrically
190:24 - speaking either I could just go on some
190:26 - website to mean to mean what is actually
190:28 - trying to tell so let's go on one
190:31 - website let's go on one website which I
190:33 - want to show you all is this one okay so
190:36 - assume that over here of over here you
190:39 - have let me choose my black color okay
190:42 - here it is so you have um a matrix a
190:45 - matrix a b c d okay you want to take out
190:49 - the determinant of this so this this is
190:52 - this is what you take out so for taking
190:54 - out the determinant you just write
190:55 - either in this A B C D giving a
190:58 - pipelines like this okay or or you write
191:01 - determinant of this uh a a matrix and
191:05 - this a matrix is either uh a b c d like
191:09 - this okay so this is the notation for
191:11 - sooning that you want to take out the
191:13 - determinant of this Matrix okay that
191:15 - pipeline that big big pipeline okay pipe
191:18 - uh line okay now over here your a is 1
191:22 - your B is zero your C is zero and your D
191:25 - is one okay you want to take out the
191:28 - determinant of this you want to take out
191:30 - the determinant of this you want to take
191:34 - out the determinant of this so how do
191:35 - you take out so what does it mean
191:37 - geometrically speaking so geometrically
191:39 - what it's trying to tell is when you
191:41 - plot this Matrix over here first of all
191:43 - you take this and then you go over here
191:45 - so this is nothing but the determinant
191:48 - of a 2X two Matrix is the area of a
191:51 - parallelogram with the column vectors AC
191:54 - and BD okay so this is the the the
191:58 - determinant is nothing but the area of
192:01 - this
192:03 - parallelogram of this parallelogram
192:05 - where the column vectors are AC and BD
192:09 - okay so when you when you plot the 2x2
192:11 - matrix which which looks like this and
192:13 - and and this the the the determinant
192:16 - which means jum Al speaking is nothing
192:19 - but area of that parallelogram which
192:22 - formed by joining everything and then
192:24 - and that area of that parallelogram is
192:26 - nothing but determinant of that Matrix
192:30 - okay this is what does it mean
192:31 - geometrically speaking uh I would ask
192:33 - you to watch one video on three blue one
192:36 - brown to see how how is shown
192:38 - geometrically but yeah uh the the det
192:41 - terminal is nothing but the area of that
192:44 - parallelogram whatever forms so for
192:46 - example you your par so let me reduce
192:49 - the a a bit and then let me do something
192:52 - with
192:53 - this I don't know well how it is
192:58 - working yeah so let me do something like
193:01 - this and let me increase the
193:04 - area okay let me increase the B okay
193:07 - here it is so when you have the column
193:10 - Vector when you have a column Vector at
193:13 - 0.86 and zero okay and then you have
193:16 - another column vector which is 0.52 and
193:19 - the the parallelogram is formed is
193:21 - nothing but your favorite the
193:23 - determinant okay so this is what the
193:25 - determinant means and you can play with
193:27 - it by just going to demonstration
193:28 - wallframe and this with this website so
193:30 - let's go on the 3D view so how does it
193:33 - look 3D so 3D is nothing but area area
193:38 - of that parallel Zoid okay so if you
193:40 - just see over here the area of the
193:43 - paraloid is is nothing but a determinant
193:45 - we'll see how to solve how how to solve
193:48 - this deter this determinant one okay
193:51 - we'll see how to solve um three for the
193:54 - how to take out the determinant of a 3X3
193:56 - Matrix and we will also see how to take
193:58 - out the determinant of a n byn Matrix
194:02 - okay so it's a it's a bit hectic task
194:04 - but we will try to do it so this is this
194:06 - is what the geometrically means and for
194:09 - 2D the area of a parallelogram and for
194:11 - 3D area of a parallel Zoid okay which
194:15 - you can see from the diagrams which are
194:16 - shown over here so if you just if if I
194:19 - could zoom in I can't zoom in but yeah I
194:22 - can just show you this is this this is
194:23 - what you have your uh 3x3 Matrix and
194:27 - then you this is the paraloid which is
194:29 - formed and then when you try to take out
194:31 - the determinant of this is nothing but
194:33 - the area of this parallel Zoid okay so
194:36 - this is what it means and the
194:37 - determinant geometrically is nothing but
194:39 - the area of a parallelogram or parallel
194:41 - joid in 3D dimension okay so this is
194:44 - this is what you need in in a geometric
194:46 - intuition just just just to make sure
194:48 - that what the geometrical it it means
194:51 - okay so now let's see now one of one of
194:54 - the important thing which I want to show
194:55 - you up is is is we have seen we have
194:58 - seen how do we take out the determinant
195:01 - of a 2X two Matrix so the determinant so
195:04 - here is your a here's is your here is
195:06 - your a and you have and then you want to
195:08 - take out the determinant of this A B C D
195:13 - and I'm just writing pipe to denote okay
195:15 - this is a determinant so when you take
195:17 - when you try to take out the determinant
195:19 - of this so it's nothing but equals to uh
195:22 - uh a a minus oh my gosh it's a D minus
195:28 - BC that's uh then when when you take up
195:30 - that's a uh simple scalar which is e not
195:33 - exactly that not 3+ 3.71 1 it's e okay
195:38 - so let's let's give it any scaler which
195:40 - is e okay so this is this is what it
195:43 - means in 2x2 matrix I'm talking
195:44 - specifically 2x2 matrix now
195:48 - now let's talk about how do we take out
195:51 - the
195:52 - determinant of uh 3x3 Matrix so
195:55 - determinant
195:57 - determinant determinant of 3x3 Matrix
196:02 - 3x3 Matrix so how do we even approach we
196:06 - taking out so you have want to take out
196:08 - the determinant of a b c d e f g i okay
196:16 - so G h i is this is your Matrix this is
196:20 - the determinant of this Matrix okay so
196:23 - how do you take out how how do take a
196:25 - determinant of this Matrix and of course
196:27 - your it should be a one scalar okay it
196:30 - should be one scaler or a number or
196:33 - number okay so how do we take out the
196:35 - determinant so can't we do a * a * e * I
196:39 - and then it will not work this is this
196:42 - is not you can you you can just guess
196:44 - how do we do it just try and commment
196:46 - and maybe I can just see and be a bit
196:49 - funny in job so please be sure to write
196:51 - it and I will try to see what you write
196:53 - it okay so so let's start approaching
196:56 - how do we even approach this problem so
196:59 - what we do we simply so so what we do
197:03 - just just make sure that first of all we
197:05 - go to the a11 okay so me first element
197:09 - in that Matrix and then what we do we
197:11 - simply leave this uh column and this row
197:16 - and write a minor Matrix or a submatrix
197:19 - of that of of that uh big Matrix or you
197:22 - can say that we take out the minor of
197:25 - this Matrix how do we take out the minor
197:27 - of this Matrix you simply when for for
197:29 - example you choose this number okay so
197:31 - what you do you you leave this column
197:34 - and you leave this row and then you
197:36 - write uh and then what you do you take
197:38 - out the minor and then you take out the
197:39 - determiner determinant of that by
197:41 - multiplying by a okay so the first
197:44 - element is this and then you have e f h
197:48 - i we left this column and this row and
197:51 - then we write e f hi okay we want to
197:54 - take out the determinant of EF HR okay
197:57 - now what you do now what you do here is
197:59 - your plus sign now it will be a minus
198:01 - sign over here okay you go to the B you
198:03 - leave this column and you leave this row
198:07 - okay which is nothing but b and d f g i
198:12 - DFG because we left this column this row
198:17 - and this column just d f g i okay and
198:22 - then here is your minus then here will
198:24 - be plus plus uh you write C now we left
198:28 - this column this this this column and
198:30 - the first row which is which will be
198:32 - left the determinant of D GH okay and
198:36 - then we have convert now these are
198:38 - called a minor or a
198:40 - submatrix submatrix or the minor of our
198:43 - Matrix a these These are called the min
198:46 - minor these are called nothing but the
198:49 - minor these are nothing but called the
198:51 - minor minor of our Matrix of our Matrix
198:55 - a okay so when you try to now it is very
198:59 - easy a * a * uh EI now you can just
199:06 - apply your 2 x two a EI and FH EI minus
199:09 - FH okay minus b d i FH d i minus FH
199:17 - okay plus C and then you have DH EG okay
199:21 - DH minus EG okay and then you'll be left
199:25 - with some scalar and then you can simply
199:27 - do do this thing and then you simply
199:29 - multiply with this and then you do do
199:30 - some calculation and then you'll be
199:32 - getting your output as maybe some some
199:34 - scalar some scalar value okay so let's
199:37 - see one of one of the one of the one of
199:39 - the problem or or the stuff to to see
199:43 - how how it looks like Okay so let's
199:46 - let's assume that you have a a matrix or
199:48 - 3x3 Matrix so here's a question for you
199:51 - okay maybe you can try try to approach
199:53 - it uh the you want to take over the
199:56 - determinant of I'm writing this pipe
199:58 - that denotes that you want to take out
199:59 - the determinant of that uh for example 0
200:03 - 1 2 uh 1 2
200:06 - 0 uh let's let write 1 1 0 okay just a
200:10 - random random I'll be walking you
200:12 - through it so take out the determinant
200:14 - of this this is a 3X3 Matrix try to take
200:17 - out the determinant of this so how do we
200:19 - take out so first of all we go through
200:21 - the first element and then what we do we
200:23 - take out the minor of this Matrix so the
200:25 - minor so we leave this column and this
200:27 - row so we'll be left with
200:29 - zero and then we and then we write out
200:32 - minor and then we take out the
200:33 - determinant of our sub Matrix okay
200:36 - plus now no no no it will be not plus
200:39 - over here it will be minus because here
200:41 - is our plus minus okay one you leave
200:44 - this column and this row which will 1 0
200:47 - 1 0 okay so 1 1 0 1 0 okay and then you
200:52 - write + 2 and then you have uh you leave
200:56 - one to one one okay you leave this
200:58 - column and this row okay you leave this
201:00 - column and this row you'll be left with
201:02 - one 2 1 1 okay and then you do the sum
201:05 - and then you do the sum so and then you
201:07 - take and then what you do you try to
201:09 - take out zero 2 * 0 which is 0 - 0 okay
201:14 - Min - uh 1 * 1 * 0 of course 0 and 1 * 0
201:21 - 0 okay plus two uh 2 okay 1 * 1 1 2 * 1
201:27 - 2 okay then you'll be left with of
201:31 - course zero then it will be done then
201:33 - done it will be also 1 * Z which is
201:37 - nothing but zero okay it'll be left it 2
201:40 - * - 2 2 * -2 that that will be
201:44 - -4 okay which will which is is your
201:47 - determinant of this Matrix so Min -4 is
201:49 - your determinant of this Matrix which
201:53 - you are seeing over here so sorry here
201:56 - is 1 - 2 it's not it's it's simply min-1
201:59 - so 2 * -2 is the determinant of this
202:03 - Matrix so I'll be so here you got the
202:06 - determinant of this Matrix which is
202:07 - nothing but min-2 okay so this is this
202:10 - is how you take out the determinant of a
202:12 - 3X3 Matrix as well so there are there
202:15 - are some problem for you to work on so
202:16 - I'm just just going to write it out so
202:19 - there's one problem which which which
202:20 - you can approach okay so you want to
202:22 - take out the determinant
202:25 - of uh 371 -4 please answer the HW please
202:30 - answer in the comment box it's just a
202:32 - quiz which you will see in your
202:33 - attendance as well okay so this is what
202:36 - the determinant of that Matrix of the
202:37 - 2x2 matrix or 3x3 Matrix we'll try to
202:40 - solve the determinant of a 3X3 Matrix
202:43 - using lianes formula or the rule of
202:45 - surus
202:47 - okay so which is very good formula to
202:49 - work on so we'll see that but before
202:51 - that I want to highlight some of the
202:52 - properties some of the properties of
202:54 - that
202:55 - properties properties of determinant
202:58 - Matrix of determinant of a matrix
203:01 - determinant of a matrix the first first
203:04 - one is the first one is the first one is
203:07 - for example you want to take out the
203:09 - determinant of this Matrix for example
203:11 - you to take out the determinant of 1 0
203:15 - uh 0 1 okay so try try try to take out
203:17 - the determinant of this 1 * 1 which is 1
203:21 - uh and then and then minus 0 okay what
203:25 - what it will it with one and can you
203:27 - identify this is an identity Matrix even
203:30 - if you have uh three three identity
203:33 - Matrix then that will be nothing but
203:35 - that will be nothing but one so whenever
203:38 - you have identity Matrix whenever you
203:40 - have if if if it is if it is identity
203:45 - Matrix if it is ENT identity Matrix
203:48 - identity Matrix then then then the then
203:53 - the then the determinant of that
203:54 - identity Matrix will be one okay this is
203:57 - this is one of the property second
203:59 - property if the if the rows are the same
204:02 - okay so for example
204:05 - if the rows are the same are the same
204:09 - for for example a a b b okay a a b b
204:14 - then a minus ba will be nothing but zero
204:18 - okay so this is another property third
204:21 - property is you have a scaler multiply
204:24 - it with some uh a and you have another
204:26 - scale c and b d okay so what it will be
204:30 - it it just makes sense r a D minus r CD
204:35 - or r r CB okay you can write write it
204:38 - down like this and then it's it's
204:41 - nothing it's nothing you just you just
204:43 - take that out of out of okay you just uh
204:46 - take take that as a common r a D minus
204:51 - BC okay so we can write this as a we can
204:54 - write this we can write this as a r * a
204:59 - b c
205:00 - d either we can write it now so we
205:03 - proved it so it is just equivalent you
205:06 - can write this so for it will be easily
205:08 - for us to solve okay so it is so it is R
205:11 - times as either it is same as over here
205:14 - so we a D minus BC so it's just
205:17 - equivalent to that so this is another
205:19 - property which you see a lot in while
205:21 - taking of the determinant of a matrix
205:24 - okay so this is these are the some of
205:25 - some of the properties which I want to
205:27 - highlight in front of you so now let's
205:29 - go on to the another stuff is how do we
205:32 - take out the determinant how do we even
205:34 - bother taking out the
205:36 - determinant determinant
205:39 - determinant of 3x3 Matrix so you can
205:43 - just say okay I'm just going to just
205:45 - going to take out the minor of the sub
205:46 - Matrix of the Matrix and then I will do
205:48 - that so here's another another trick
205:51 - which is called the rule of surus I
205:53 - think it's it's it's not a I would say
205:56 - uh okay it's a good technique but okay
205:59 - you can try it out but eventually I like
206:02 - that my Approach but yeah it is very
206:05 - very straightforward approach which I'm
206:07 - going to tell over here okay so assume
206:11 - that you have a matrix M that you have a
206:13 - matrix M Okay so a11 1 a12
206:18 - a13 okay a21 a22
206:23 - a23 a23 a31 a32 a33 okay so you have
206:30 - this 3x3 Matrix now when you wanted to
206:32 - take out the determinant determinant of
206:35 - this Matrix M so how do we even bother
206:38 - doing that so for for for for so you can
206:41 - use the rule of suus rule of suus I
206:45 - think that funny name he has but yeah
206:48 - again I'm no no want to comment on on
206:50 - his name he's a again a great people
206:52 - okay so not I'm not even a one 1% of
206:55 - these people so these are amazing people
206:57 - who give a lot to the world so I think
206:59 - about I'm no one to say about but yeah
207:01 - amazing name so what you do you take out
207:03 - the determinant so you want to take out
207:05 - the determinant of a11 a12 a13 okay so
207:09 - this Matrix I'm just writing this Matrix
207:12 - a21 a22 a23 okay
207:16 - a31 a32 a33 okay and then what you do
207:22 - you take out the first two column and
207:24 - write it in another format like this a11
207:27 - a12 A2 1 this is a trick for solving a
207:30 - 3X3 Matrix a22 and a 31 okay so a31 and
207:35 - a32 so this is
207:37 - a22 okay it's say a21 okay so this is
207:40 - what you now you write this now what you
207:42 - do now what you do so what you do you
207:47 - you simply take out the product you
207:49 - simply take out the product like this
207:52 - the first diagonal okay so this is the
207:54 - diagonal so what you do
207:56 - a11 a 22 a33 okay a11 a22 a33 plus plus
208:05 - a12 a22 a23 okay A2 3 and a31 so what
208:10 - you do you take out the product of these
208:12 - three you take out the product of these
208:13 - three so A1 2 a
208:16 - uh uh 23 okay a 23 uh and a 31 okay now
208:21 - what you do you simply uh do this simply
208:25 - multiply the
208:27 - next next diagonally okay so plus plus
208:32 - a13 a21 okay I'm I'm I'm I'm doing a bit
208:35 - messy so let me do
208:38 - a31 uh if I'm not wrong a13 a a a13 a21
208:44 - and A3 2 okay A a A1 3 A2 1 a32 now you
208:50 - are done with this now what you do now
208:52 - what you do you now go from bottom to
208:56 - top here you are going from top to
208:57 - bottom now you'll go from bottom to top
209:00 - by changing the sign now okay now you go
209:02 - from bottom to top so here's how you do
209:05 - here's how you go further okay so so the
209:09 - way you go is you have a31 so from here
209:14 - so from here a 3 1 okay and then a22 and
209:19 - then you go to a a13 so now you start
209:21 - going at this side like like this okay
209:24 - so
209:26 - a31 I'm just going to write a31 a22 a22
209:32 - and then a13 a13 okay and then what you
209:36 - do and then plus no uh you you minus
209:39 - because you change you go from bottom to
209:41 - top so here you minus it now now you go
209:44 - at this one now you do this
209:47 - and then a32 okay this one this one and
209:51 - this
209:52 - one a32 *
209:55 - a23 yeah if I'm it's a it's a a23 if I'm
210:00 - not wrong yeah a23 and a a11 a11 okay
210:05 - now minus now this is done now you go at
210:08 - last one which is this one
210:11 - a33 a21 a12 okay and here's how you take
210:17 - out the determinant of a matrix using
210:19 - suus rule okay or a rule of suus okay so
210:23 - and then you'll be getting after after
210:24 - doing this all those stuffs you can just
210:26 - cancel it out something if it is so you
210:28 - just you just do do the computation then
210:31 - here's your Matrix and this is just a
210:32 - scalar number or other stuffs so here's
210:34 - the rule of suru so here's how you do
210:36 - you do simply you do you do write the
210:38 - first two column uh at at the side so
210:41 - that it could be easily so uh so what
210:44 - you do you simply take of the product
210:46 - from top to bottom for the first three
210:48 - and then you take out the bottom to top
210:50 - for the second three starting from the
210:52 - last okay so here's here's here's what
210:55 - the full the rule of sarus means here
210:57 - here's how you take out the determinant
210:59 - of that Matrix like this okay so now I'm
211:02 - going to talk about is uh you can see
211:04 - the Wikipedia Pages for a libanese rule
211:07 - because they write a very very kind of
211:09 - libanese stuff so you can just go there
211:11 - and see more see more about this rule
211:13 - okay so the next thing which I'm going
211:14 - to talk about is
211:16 - the next thing which I'm going to talk
211:17 - about is how do we take out the
211:19 - determinant how do we take out even B of
211:22 - taking out the
211:24 - determinant of n by n Matrix the
211:28 - determinant of n by n Matrix so how do
211:31 - we even take out that and how do we even
211:33 - bother taking out that okay so here's so
211:37 - I'm just going to write the N by n
211:39 - Matrix as I'm just going to Rite the N
211:41 - by n Matrix or let's start with a
211:44 - particular example let's start with a
211:46 - particular example so it would to
211:49 - totally make sense okay so let's let's
211:51 - start with a particular example and then
211:53 - at last we'll just write a definition
211:55 - and then we'll end this video okay the
211:57 - example is bit long so I'm just going to
211:59 - maintain my handwriting so the example
212:02 - is you want to take out the determinant
212:06 - you want to take out the determinant of
212:07 - a 4x4 Matrix 1 2 3 oh my gosh three 4
212:14 - okay 6 6 6 9 2 1 and then you have a 4 9
212:20 - 2 1 and then you have a 0 1 1 one okay
212:24 - so here's here's your determinant of
212:26 - this Matrix so to take out the
212:28 - determinant of this Matrix so how do we
212:30 - even approach taking out the determinant
212:32 - of this Matrix so how do we take out the
212:34 - determinant of this so for taking out
212:35 - the determinant of this so for taking
212:37 - out the determinant of this for for
212:40 - taking out the determinant of this you
212:42 - take out you take out you first of all
212:45 - take a the minor or the submatrix of
212:47 - this okay so you you go to the First
212:50 - Column first element and then you leave
212:52 - this column and this row and then write
212:55 - the sub Matrix so you just one and then
212:57 - you take out the determinant of 9 2 1 9
213:02 - 21 1 1 1 okay this is plus sign so it
213:06 - will be minus sign now you go at this uh
213:09 - take take out the sub Matrix leaving
213:10 - this row this column and this row so it
213:13 - would be two first of all you take
213:15 - product it so you multiply with that 2 2
213:18 - * uh the determinant of 6 2 1 6 2 1 42 1
213:25 - and 011 okay and then you uh change the
213:28 - Sign Plus and then go through with three
213:30 - and then you leave this column and this
213:32 - row so it will be nothing but uh three
213:36 - and then you have 621 I also just write
213:38 - okay 66 6 91 I I just leave it so I'm
213:42 - three 691
213:44 - 691 uh 4 4 91 4 91 and 01 1 okay 01
213:51 - 1 now the last one is there four okay so
213:55 - there is four and then you take out the
213:57 - terminant of leaving all the all the uh
214:00 - column one and then row 692
214:03 - 692
214:05 - 492 and uh okay I think it's wrong
214:09 - 492 and 0 1 1 okay so these are the sub
214:13 - Matrix of that Matrix let's name it as
214:15 - an n M okay so this is a matrix M and
214:17 - then you have to take a determinat of
214:19 - that Matrix M so here's the 4 4x4 now
214:22 - you do this now you have this now what
214:24 - you do now here you convert it to 3x3
214:26 - determinant now what you do you convert
214:27 - that to a 2x2 here's how you do so you
214:31 - you don't want to use a suus rule
214:33 - because I I eventually don't like that
214:35 - rule it's very hectic rule sometimes it
214:37 - maybe cause you error but no problem in
214:39 - that so here's how you do it so first of
214:42 - all what you do so first of all what you
214:44 - do you you simply uh multiply uh one
214:49 - okay you simply go ahead and take take
214:52 - your uh one as a so if you can see I
214:55 - just want to take that one as an uh uh
214:58 - this one and then what I and then I go
215:00 - and approaching this so here is your
215:02 - nine so first first of all go at this
215:04 - element take out nine you want to take
215:06 - out the sub Matrix of this Matrix so
215:07 - nine and then you take the termin of 2 1
215:11 - 1 1 so what how this came 2 1 uh you
215:15 - leave this this row and this column 21 1
215:18 - 1 okay now you simply change the sign
215:21 - minus okay you make sure that t you're
215:24 - doing only doing for this you're only
215:26 - doing for this we'll come to this later
215:28 - on but we are only doing for for this
215:30 - a21 okay minus now now we go to this two
215:34 - now we go this two we leave this column
215:36 - and this row which is 91 1 1 okay so
215:39 - which is a what happened yeah so which
215:42 - is nothing but uh what do you say uh two
215:45 - because here is our two leaving this row
215:48 - this this this column and this row
215:50 - 9111 okay uh did the determinant of 91 1
215:55 - 1 okay and then what you do plus now you
215:58 - change the sign and then you go at last
216:01 - 1 leaving this 9 to11 okay so the one
216:05 - and the 921 1 okay so you take out that
216:09 - okay now this was plus now you make it
216:11 - minus okay now here is your two so I
216:14 - will take that outside I will take take
216:16 - that outside okay and then I will just
216:19 - go ahead into solving this so you take
216:22 - this take this as a now you take out the
216:25 - minor of this Matrix or the sub Matrix
216:27 - of this Matrix so here's how you do it
216:29 - so you simply add it minus so here is
216:32 - minus 6 uh 21 1 1 so here here's how you
216:37 - go with this you ignore this column and
216:40 - this row 211 1 now you go to this you
216:42 - ignore this 41 0 41 0 1 and then you go
216:46 - over here ignore this 42 01 okay so this
216:50 - is how I'm I'm I'm going to write minus
216:52 - 2 okay because you go over here 2 over
216:55 - minus 2 CH changing the sign take out
216:57 - the determinant of 4 1 0 1 4 1 01 okay
217:02 - and then you simply plus um now you
217:05 - change the sign you go one go to go to
217:07 - one 42 0 1 42 0 1 uh + 1 uh 4 2 0 1 okay
217:14 - and then what you do and oh my gosh yeah
217:16 - so then what do you do now you now you
217:19 - converted that 3x3 Matrix for this one
217:21 - and for this one now you go to this one
217:24 - okay by changing the sign plus plus and
217:27 - then you go and then you write separate
217:29 - three now you write separate three and
217:31 - then you take out the first one six okay
217:33 - so you leave this SC column and this row
217:36 - which is nothing but uh six and then
217:38 - 9111 which is the sum Matrix of that
217:41 - Matrix Min - 9 because this plus 9 and
217:44 - how how how came you go with this column
217:47 - leaving this column and leaving this row
217:50 - 41 01 which is nothing but
217:53 - 4101 okay plus changing the sign 149 01
217:58 - how this 4901 came is you have this you
218:02 - leave this and this you leave this
218:04 - column and this the row which is 4901
218:06 - okay so this is how you came it and then
218:08 - and then you're done okay now what do
218:10 - you do you you do for the last one you
218:13 - do for the last one this because you you
218:15 - done for this you're done for this
218:16 - you're done for this now you converted
218:17 - that to a 2x2 MRI which is easily deter
218:20 - which which we can easily take out the
218:21 - determinant now you go to this okay so
218:24 - here's how you do it so Min - 4 okay and
218:28 - then what do you do and then what do you
218:29 - do you leave this column and this row
218:32 - taking out the first element so six take
218:35 - the determinant of I was I think it's
218:38 - where it was 9 9211 9211
218:43 - 9211 minus minus
218:46 - 9 okay so over here it was leaving this
218:49 - the SE going to this and leaving this
218:50 - column and this row which is 4201 I I I
218:53 - think about it yeah that's 4201
218:57 - 4201 and then you change the sign plus 2
219:00 - 49 I think it's it's it's more about you
219:04 - leave you go over here 4901 okay that is
219:08 - 4901 okay now you are done now this is
219:11 - what you have written so you converted
219:13 - the first Matrix the first Matrix this
219:16 - one this one and this one as well uh
219:18 - into a minor Matrix which is 2x2
219:20 - determinant so you can easily take out
219:21 - and then do the product and then you
219:23 - take out okay so let's do over here if
219:26 - if I have a chance to do over here but
219:28 - no problem I will do over here okay so
219:31 - here's how you do it here's how you do
219:33 - it so for doing it first of all you have
219:35 - the one available which is over here you
219:37 - have the one available which is over
219:39 - here what do you do you simply 9 - 16 +
219:44 - 7 how how how we came so you have the
219:47 - particularly nine times because of
219:49 - course you want to always want to
219:51 - multiply it out okay so I think about
219:54 - this is you have uh if if you go over
219:57 - here 2 * 1 okay and 1 * 1 so 2 2 2 * 1
220:03 - how much 2 * 1 how much it would be uh 1
220:08 - - 1 I would
220:10 - say uh 2 - 1 which is 1 so it will be 9
220:14 - okay minus minus over here uh 9 9 * 1
220:19 - which is 9 - 1 which is 8 16 - 16 done
220:24 - and then you have 9 * 1 and then minus 2
220:27 - * 1 so 9 - 2 which is 7 okay so here's
220:31 - how it came okay and then you then this
220:33 - the left minus two now you go on second
220:36 - one two over here so when you when you
220:38 - take a two * 1 how much 2 * 1 how much 2
220:41 - * 1 2 - 1 okay that is 6 over here here
220:45 - so we write uh 6 - 8 + 4 okay so here's
220:51 - how you do so it is 6us 4 * 1 of course
220:54 - 4 * 2 it's 0 * 1 of course 0 so 4 * 1 4
220:59 - * - 2 which is - 8 which which you have
221:01 - written over here okay then you go over
221:03 - here 4 * 1 how much 4 * 1 4 and then you
221:07 - four so here here is a plus 4 now now
221:10 - you go to the next + three because you
221:12 - go over here now 9 * 1 how much 9 * 1
221:17 - how much uh 9 * 1 9 of course - 1 uh
221:21 - which is nothing but 8 8 * 6 48 so you
221:24 - have 48 - 36 + 4 okay so here's our 48 -
221:31 - 9 uh so 4 * 1 4 so 4 9 49 36 because
221:36 - this 2 * 0 is 0 then you go over here
221:40 - then you have this 9 9 9 * 0 is of
221:43 - course 0 4 * 1 so 4 2 are 8 eight so
221:45 - over here uh I think I'm wrong over here
221:48 - um you have this 4 * 1 4 91 0 0 so 4 uh
221:53 - it's it's it's it's four okay so it's
221:56 - four it's it it it it it should be eight
222:00 - okay it should be 8 which is nothing but
222:02 - what you do you 4 * 1 4 and then you
222:05 - simply multiply with two which is
222:06 - nothing but 8 okay now you simply - - 4
222:11 - - 4 uh 28 - 30 6 + 88 so how you take
222:17 - out 9 * 1 how much 9 - 2 7 7 7 6 7 76
222:24 - how much uh oh oops it's it's uh it
222:27 - should be 9 * 1 - 2 987 76 42 okay so
222:33 - which will be nothing but 42 what the
222:35 - hell i' written over here so I have to
222:38 - just couple it out so I have to just
222:40 - return it it will be nothing but
222:43 - 42 okay minus minus minus uh minus 4 * 1
222:50 - of course 4 9 are 36 plus 2 4 4 1 are 4
222:54 - 4 2 8 okay so here's how you do it now
222:57 - you simply multiply with this and then
223:00 - first of all do the calculation do the
223:02 - calculation then you'll be getting one a
223:03 - scaler as your output so please feel
223:05 - free to put your answer in the
223:07 - description box below I know the answer
223:09 - but yeah I want to leave it to you to do
223:11 - the rest of the calculation I have done
223:13 - a lot so here's how you take a the
223:15 - determinant of n byn Matrix and how you
223:17 - do it and how you do it it's very very
223:19 - easy you just uh keep converting that to
223:22 - a lower or a sub Matrix and then you and
223:25 - then after that you are done okay so
223:27 - here's how you do it so you you define
223:29 - one you define one sub Matrix AIG J you
223:32 - define one sub Matrix AIG J which is
223:35 - nothing but the Matrix the n n minus 1 *
223:39 - n-1 Matrix n-1 n-1 Matrix if you ignore
223:45 - ignore the I row and I column if you
223:47 - ignore if you ignore the I row which
223:51 - which you are doing I row and J column
223:55 - which which you were doing okay that is
223:57 - your new Matrix which which we were
223:59 - forming that is a recursive this is a
224:01 - recursive you can write a Python program
224:03 - for write a recursive solution for this
224:06 - okay so you were writing a one one and
224:10 - then you were adding the determinant of
224:12 - that subm Matrix and then you are doing
224:14 - so and so on so on that is a recursive
224:16 - solution so you writing the recursive
224:19 - recursive stuff so we we we already
224:21 - written a lot so I hope that you
224:23 - understood for formal definition which
224:24 - you can see yeah we have gone through
224:26 - one of one of the example which is very
224:27 - very much important for us to know okay
224:31 - so I hope that you will uh get a lot
224:33 - from this video and determine it I hope
224:35 - that concept is clear with my examples
224:38 - and I also hope that you enjoyed this
224:39 - video I think I have to wrap up with
224:41 - wrap up with this video I'll be catching
224:43 - up your next video till then bye-bye
224:44 - have a have a great day meet you in the
224:45 - next
224:49 - [Music]
224:58 - lecture okay everyone let's get started
225:00 - with another lecture I know it's bit L
225:03 - late lecture but I apologize for that
225:06 - I'll be I'll I'll be making making sure
225:09 - that I'll be providing you around three
225:10 - to four videos this week so I already
225:13 - provided two videos now I think about
225:15 - two of three videos will provided more
225:17 - this week till your next uh assignment
225:20 - or homework assignment and please make
225:22 - sure that your homework assignment is
225:24 - released if we find any student who are
225:27 - not active we will remove them from our
225:31 - LMS because this is an opportunity which
225:34 - are given for free for others to learn
225:36 - because if the people do not take this
225:38 - opportunity we are going to drop that
225:40 - student so uh we we we highly recommend
225:43 - to to to to be active on LMS please do
225:47 - your assignments please attend your
225:49 - attendance and every stuff okay so
225:52 - please please go there and Mark your
225:53 - attendance and as well as uh complete
225:57 - your assignments even if you complete
225:58 - around out of 25 questions you you have
226:02 - to complete around 20 questions you can
226:04 - complete 20 questions write right in
226:06 - Notebook and then give it to us okay so
226:08 - you your programming assignment sorry
226:10 - the homework assignment will be will be
226:12 - evaluated and then that will be uh till
226:15 - the end end of the course and if we do
226:16 - not find you active in the course we
226:19 - will drop you out okay so this is one of
226:21 - those announcement that our team has
226:23 - told me to give me to give it to give it
226:26 - to you all through Me Okay cool so so
226:29 - what's the mod of this lecture the M of
226:30 - this lecture to talk about the co-actor
226:33 - the minor and educate or the adjoint and
226:36 - invoce of a matrix so these are very
226:39 - very correlated these are very very
226:41 - correlated uh for for taking all the
226:43 - co-actor you need mind and for taking
226:45 - out the minor you need determinant and
226:48 - for taking out the aducate you need
226:49 - co-actor and taking out the inverse you
226:52 - need ugate okay so again I'm explaining
226:56 - for for for taking out the co-actor of a
226:58 - matrix for taking out the co-actor of a
227:00 - matrix you need minor and for taking of
227:03 - the minor you need
227:05 - determinant and and and and after after
227:08 - you take out and for taking out the
227:11 - adjugate for taking out the adjugate for
227:13 - taking out the adjugate for taking out
227:15 - Agate you need co-actor and then for
227:18 - taking out the inverse of a matrix you
227:21 - need aducate okay so these are very very
227:23 - correlated and they are heavily used for
227:25 - many this inverse of a matrix because
227:27 - they are so correlated so I thought okay
227:30 - let's let's start with this video so
227:32 - that everyone knows about co-actor how
227:34 - the N inverse of a matrix is calculated
227:37 - because it is extensively used in the
227:39 - industry okay uh mainly in linear
227:42 - algebra there's not too much use in
227:44 - machine learning some sometimes us
227:45 - machine learning but it's very very good
227:46 - to know about these stuffs okay so first
227:50 - of all how do you be now let's let's
227:53 - let's go ahead and talking about the
227:55 - first two stuff which is which is a
227:57 - minor of a matrix or the co-actor of a
227:59 - matrix so let's start with a minor let's
228:02 - start with a minor minor of a matrix so
228:06 - so so a minor of a matrix a as as if if
228:09 - you remember the minor the a minor of a
228:14 - of of a Matrix a a minor of a matrix a
228:18 - matrix a is the determinant of the small
228:23 - same some some smaller um Square Matrix
228:27 - um a minor of a matrix a is the
228:30 - determinant is the determinant is the
228:35 - determinant of some of some smaller of
228:40 - some
228:41 - smaller squar Matrix as you remember
228:44 - remember that what we do what we do if
228:48 - we were removing the column and the row
228:50 - for that for that point or the element
228:53 - and then we were we were obtaining a sum
228:55 - Matrix in the determinant and that's
228:57 - actually when you when you take out the
228:59 - determinant of of that some Sub sub
229:03 - Matrix that's actually the minor we we
229:06 - will see see one example just just in a
229:08 - second for example for example let's
229:12 - take an example that you have the
229:14 - following that you have the following uh
229:17 - Matrix okay you want to take out the
229:19 - determinant of this Matrix or the minor
229:22 - of this Matrix you want to take out the
229:24 - minor of this Matrix so for example you
229:27 - told okay you want to take the minor of
229:29 - Matrix M for I row and J column okay
229:34 - that so you take out so you for example
229:37 - you choose okay you want to remove the
229:40 - second row uh and the third column okay
229:43 - so you are saying 2 3 so you to you want
229:46 - to take out the minor of a matrix given
229:49 - I = 2 and J = to 3 so what what it will
229:53 - do it will it will leave second row and
229:56 - it will leave the third column okay so
229:59 - the minor of this Matrix a will be left
230:02 - with the the the sub Matrix will be left
230:04 - with 14 - 1 9 as you all are knowing
230:08 - okay so now when you take out the
230:09 - determinant of this Matrix so 1 minus uh
230:13 - this is first of all for taking on the
230:15 - determinant of a 2x2 matrix what we do
230:18 - we simply what we do we simply uh
230:21 - multiply the DI diagonals and then sub
230:24 - subtract it so 1 uh my time uh 9 okay
230:31 - minus uh of course uh sorry this minus
230:35 - and minus 4 okay so that will be left
230:38 - with 9 + 4 which is nothing but 13 so 13
230:43 - is a minor of a Matrix given I = 2 and J
230:47 - = 3 so what does it mean we leave the
230:51 - second row and third column or we delete
230:54 - the second row and third column to
230:56 - obtain the sub Matrix so that is the
230:58 - minor of that Matrix okay so again
231:02 - explaining what we do we simply remove
231:06 - just one row and one column and we take
231:09 - what row you want to remove and what
231:11 - column you want to remove by the user I
231:13 - and J you you remove one row and one
231:16 - column from the square Matrix and make
231:18 - sure that is a square Matrix what is the
231:20 - square Matrix the square Matrix is the
231:22 - one where the number of rows the number
231:25 - of rows matches with the number number
231:29 - of columns okay so the number of a rows
231:32 - matches with the number of a columns and
231:34 - over here when you take out the minor of
231:36 - this Matrix a given you remove one row
231:39 - and one column so you rem the remove the
231:41 - second row and the third column for
231:43 - example if you want to take one one so
231:45 - the minor of this will be you have you
231:47 - want to remove the first row and First
231:49 - Column the minor will be determinant of
231:52 - that uh 3 0 so sorry it will be it will
231:57 - be I think 0 5 91 so take the
232:00 - determinant of this so 0 0 * 11 0 - 9 *
232:04 - 5 45 what it will be it will be 45 or -
232:09 - 45 okay so that will 45 around according
232:12 - to this okay so that's that will minus
232:16 - 45 so that's how what this minor tells
232:19 - you minor tells you okay you want to
232:21 - take out you want to remove that column
232:23 - I you want to remove that I row and J
232:26 - column and then write write down the
232:28 - Matrix and then take out the determinant
232:31 - of that submatrix and whatever the
232:33 - determinant will be that will be your
232:35 - minor of that Matrix okay so this is
232:38 - this is what it's trying to tell you so
232:40 - over here oh my God what is this where
232:42 - is where is my pen so this is what
232:44 - it'sing trying to tell you over here
232:46 - okay so what does minor means minor of a
232:50 - the a minor a is the determinant of some
232:53 - smaller Matrix some a smaller Matrix
232:56 - okay so a minor a matrix a is the
232:58 - determinant of some smaller Matrix for
233:02 - for example which you're seeing over
233:03 - here and then you so how do you take out
233:06 - you remove one row and one column and
233:09 - ride the rest of the Matrix element into
233:11 - a new Matrix and then you take out the
233:13 - determinant of that Matrix and the
233:16 - whatever the determinant a scalar value
233:18 - that scalar value will be uh the minor
233:22 - of that Matrix okay so this is how you
233:25 - calculate the minor of the Matrix so so
233:28 - so so so using minor of a matrix you can
233:32 - calculate the co co-actor of a matrix
233:34 - using minor of a matrix you want to
233:36 - calculate the co-actor of a matrix so
233:38 - why do we you why do we need to
233:40 - calculate the minor for Matrix to
233:42 - calculate the co-actor you we need uh we
233:45 - need to calculate the minor so what does
233:48 - it mean so why do we even care about
233:51 - co-actor why do we need co-actor So Co
233:54 - co-actor is required for co-actor is
233:57 - required for computing determinants
234:01 - Computing Computing high level
234:03 - determinant or larger determinants okay
234:06 - Computing larger determinants or
234:08 - determinants and taking out and in
234:11 - taking out in taking out the inverse of
234:13 - a matrix
234:14 - indirectly inverse of Matrix indirectly
234:18 - as it is not used directly over there
234:20 - you want to take out the adjugate of
234:22 - that for taking out the inverse but
234:23 - adjugate uses adjugate us a CO co-actor
234:27 - and co-actor is being indirectly
234:29 - contributing to taking out the inverse
234:31 - of a matrix we'll see the inverse of
234:34 - Matrix just we will visit after some
234:36 - slides okay so this is this is what it
234:38 - means to be minor so let's talk about
234:40 - the co-actor of a matrix let's talk
234:43 - about the co-actor of a matrix so what
234:46 - is the co-actor co-actor is calculated
234:49 - first of all we need to calculate the
234:51 - the the minor so for example you have
234:54 - the following uh Matrix you have the
234:56 - following Matrix you have the following
234:58 - Matrix 147
235:01 - 305 -1 911 okay so this is this is your
235:06 - this is your um this is this is your 3x3
235:09 - Square Matrix you take out the minor of
235:12 - this Matrix you take out the minor of
235:13 - this Matrix given I to be 2 and J to be
235:17 - 3 okay so you want to take out the
235:19 - determinant of the sub Matrix of the sub
235:23 - Matrix of the sub Matrix so for example
235:26 - I'm just taking one example given I = 2
235:29 - and G = 3 so what you do you leave the
235:32 - second row and third column the second
235:34 - row and third column so left with 1 4 -
235:38 - 1 9 so this is and then when when you
235:40 - take out the determinant determinant of
235:43 - M2 3 that will be what 9 - 9 * 1 9 - um
235:49 - - 4 that will be nothing but 13 and as
235:53 - we shown 9 first of all the product of
235:55 - the diagonals and subtract it minus -4
235:59 - so that will be what 13 okay so that is
236:02 - the minor of that Matrix now when you
236:04 - take out the minor of the Matrix how to
236:06 - calculate the co-actor of a matrix so
236:08 - for calculating the co-actor of a
236:11 - matrix I and j i and J and make sure
236:14 - that I I I matches with the minor minor
236:17 - of that and J matches with the minor of
236:19 - uh or uh whatever the uh J over here so
236:22 - these these two should match equals to
236:25 - -1 the power of I + J * the minor I and
236:32 - J okay so this is how you calculate so
236:35 - for example for this example let's
236:37 - calculate the co-actor so how do how how
236:40 - do we calculate the co-actor so the here
236:42 - I is 2 3 J is 3 is which which is
236:46 - nothing but equals to -1 -1 2 + 3 * 13
236:53 - okay so whatever the value will be over
236:55 - over here we don't care of that it will
236:57 - be minus one because it's five mean
237:00 - min-1 to ^ 5 what it will be -1 of
237:03 - course because if it if it is 6 then it
237:05 - will be 1 so it is -1 * 13 so the output
237:09 - will be the the answer will be Min -3
237:11 - will be the co-actor of this Matrix
237:13 - given I to be two and J to be three okay
237:17 - of that minor okay of that of the two
237:20 - three ENT three okay so what how you
237:23 - write the co-actor the
237:26 - co-actor of 23 entry where two is the
237:30 - row and J is the uh two is the row and
237:33 - three is the column entry is min
237:37 - -3 okay so this is how you C calculate
237:41 - the the co-actor of a particular
237:45 - Matrix okay so let's see one more
237:47 - example to make intuitive sense to you
237:49 - so that it would not left in sense oh my
237:51 - God what is happening with with some
237:53 - example so let's take an example that
237:55 - you have 2 4 6 21 2 1 one1 so this is
238:01 - your Matrix so you select okay you are
238:04 - selecting the third row you're selecting
238:06 - the third row you to take out first of
238:08 - all minor so going to take out the third
238:11 - row and the first column okay so third
238:15 - row and the First Column okay so you to
238:18 - take out the minor of that so first of
238:19 - all you leave the third row and the
238:22 - First Column so the minor will be the
238:24 - minor will be the deter the The subm
238:26 - Matrix will be at what uh 4 61 2 and
238:31 - when you try to take out the determinant
238:33 - of this what it will be 4 * 2 - 6 * 1
238:38 - which is nothing but 8 - 6 which is
238:41 - nothing but 2 okay so the minor of 3A 1
238:46 - entry is nothing but equals to 2 okay so
238:50 - after after you calculate the minor if
238:52 - you calculate the minor you wanted to
238:55 - calculate the the co-actor because you
238:57 - want to calculate the co-actor so for
238:59 - calculating the co-actor you need a
239:01 - minor so you taken out the minor now
239:03 - when you calculate the co-actor of 2x3
239:06 - entry of the two two sorry it's 3x 1 now
239:09 - it's 3x 1 3x1 entry which will be what
239:13 - uh which should what so c i j so formula
239:16 - is c i the co-actor of three I and J
239:19 - entry is nothing but -1 to the power of
239:23 - I + j i + J times the minor Matrix I and
239:28 - G so in this example 2 3 = -1 2 + 3 * 2
239:35 - * 2 so what it will be it will -1 to
239:39 - the^ of 5 * 2 okay so that will -1 time
239:44 - 2 which will nothing but minus 2 is your
239:47 - co-actor of that Matrix okay so the
239:50 - co-actor of that Matrix for that ENT 3 3
239:52 - 3A 1 is -2 so this is how you calculate
239:56 - the co-actor of a matrix so I'm just
239:58 - just going to ride the steps for you to
240:00 - calculate the co-actor of a matrix so
240:03 - first of all you take out you take out
240:05 - you take out the minor you take out the
240:09 - minor take out the minor m i j how you
240:13 - take out the minor so first of all you
240:15 - take out the sub Matrix so you remove
240:17 - the one row I throw I'll just remove I
240:21 - just going to write remove I row remove
240:26 - I row and J column okay and then
240:30 - whatever the sub Matrix would be left
240:32 - and then sub Matrix sub Matrix and
240:35 - whatever the sub Matrix take take out
240:37 - the determinant of that sub Matrix okay
240:39 - that will be a minor after you take out
240:42 - now you can simply calculate the co
240:44 - factor which will be nothing but C J
240:46 - which is nothing but -1 the power of y +
240:49 - J * m j okay so this is how you
240:53 - calculate the co-actor of a matrix c i j
240:57 - for that I
240:59 - entry okay so I hope that you understood
241:02 - what's the co-actor and what's the com
241:03 - Miner of that so some some of the
241:05 - applications so some of the application
241:07 - so here over here what you're trying to
241:10 - actually do is to uh so we can write
241:13 - it's basically this Co co-actors are
241:17 - basically used in prominently in lapl
241:19 - formulas for for the expansion of the
241:22 - larger determinants okay as as we have
241:25 - already seen so the formula of
241:27 - determinant of a which is nothing but I
241:31 - = to 1 all the way around to the n a i j
241:34 - a i j - 1 I plus j m i j okay so this is
241:42 - for taking of the determinant as as I
241:44 - told you one of the most best
241:46 - application of uh of the co-actors it is
241:49 - used in is a laplus formula for taking
241:52 - out the determinant of larger
241:54 - determinant so this is the formula for
241:56 - taking out the determinant of a by using
241:59 - the co-actor so I = to 1 all the d m a i
242:02 - j times uh uh times of course minus one
242:06 - this is the co-actor of your Matrix and
242:09 - this is a minor of that okay for that to
242:11 - I I and J entry
242:14 - now so we had seen the co co-actor we
242:17 - have seen the co-actor and we have also
242:19 - seen the minor and we have talked a lot
242:22 - about determinant and we talked a lot
242:24 - about determinant now let's talk about
242:26 - now let's talk about uh the Agate of a
242:29 - matrix so the adjugate of a matrix or
242:31 - some sometimes call it as a adjoint of a
242:34 - matrix
242:35 - so
242:39 - adjugate Matrix okay so we have to take
242:42 - out the aggate of a matrix so let's
242:44 - write it out so when how we take the
242:47 - adjugate for Matrix so first of all what
242:49 - is the adjugate of for Matrix so the
242:51 - adjugate of a matrix or the or sometimes
242:55 - there are lots of names sometimes we
242:57 - call it as the adjugate sometimes we
242:59 - call it is a classical adjoint classical
243:03 - adjoint and a lot more okay so this is a
243:06 - classical adjoint we also call it as
243:08 - that so how do we take out the adjugate
243:11 - of a matrix is nothing but the transp
243:14 - of its Co
243:16 - transpose transpose of its co-actor
243:20 - Matrix co-actor Matrix so Agate of a
243:23 - matrix nothing but the transpose of its
243:26 - co-actor Matrix okay so here's how you
243:29 - define it so the aggate aggate of a
243:32 - matrix a is nothing but transpose of
243:35 - that co-actor Matrix okay so this is a
243:39 - co-actor
243:42 - co-actor matrix which you taken out for
243:44 - every for every I and J in your Matrix
243:48 - okay so Agate is what is the transpose
243:51 - of that co-actor Matrix so here are some
243:54 - properties so over here the adjugate
243:58 - adjugate of a is nothing but C transpose
244:01 - and it is nothing but uh minus one of
244:04 - course I'm writing the formula I + j i +
244:07 - J time M this is this is the formula for
244:11 - Cal calculating okay and and I and J
244:15 - should go from for all the elements so I
244:18 - and J okay so this is the I and J and
244:21 - this is how you take out the aggate of a
244:24 - matrix this is the definition of the
244:25 - aggate of a matrix okay one of the
244:28 - property is a times the inverse of the
244:33 - Matrix It it means you have a matrix a
244:36 - and you have a matrix inverse of that so
244:38 - the output will be of always the
244:40 - identity Matrix when you multiply the a
244:43 - matrix and the inverse of that Matrix so
244:45 - what is inverse of a matrix so here's
244:47 - how we deal with it so inverse of Matrix
244:50 - of inverse of a matrix is nothing but
244:53 - the AI how you C calculate it if you
244:56 - know about 1 / a this is this this is
244:59 - what we write but here's how we do that
245:01 - so 1 over the determinant of a one over
245:05 - the determinant of a times the Agate of
245:08 - a times the Agate of a and how do we
245:12 - calculate the Agate of a we calculate
245:14 - the aggate of a by taking the transpose
245:16 - of our co-actor Matrix and you all know
245:19 - the for taking out the co-actor Matrix
245:21 - we have this equation -1 the^ I + J * m
245:25 - sub subscript I and J okay so this is
245:28 - how you calculate the uh inverse of a
245:32 - matrix you calculate the 1 over the
245:35 - determinant of a times the adjugate of
245:38 - that a adjugate of that Matrix a this is
245:41 - how you calculate the inverse of a
245:44 - matrix it's not a big deal it's just a
245:45 - small stuff which you are seeing over
245:48 - here okay so so we had talk so we have
245:52 - we have a talk about inverse of Matrix
245:54 - so I'm just going to write it out in one
245:56 - detail about what we had a talk on this
245:59 - now and one one more thing over here if
246:01 - your the determinant of a the
246:03 - determinant of a is equals to zero then
246:06 - your Matrix The Matrix is not invertible
246:09 - if your determinant of that a is zero
246:12 - then your Matrix then your Matrix a is
246:15 - not invertable then it will be
246:17 - non-defined okay so let's uh let me
246:20 - write it out what we had studied we have
246:22 - our co-actor we have our co-actor of a
246:25 - matrix co-actor of a matrix is nothing
246:27 - but uh minus 1 I + J to the power of
246:32 - time m i j okay and then you take out
246:37 - the the the adjugate of a matrix the
246:39 - adjugate of a matrix a is nothing but
246:42 - what the the co-act the transpose of our
246:45 - co-actor Matrix and for taking all the
246:47 - inverse of that for that Matrix for for
246:51 - for for Matrix a we have 1 over the
246:54 - determinant of a times times your uh
246:59 - Agate Agate of a and if your if your in
247:05 - uh the determinant of a is zero if
247:08 - determinant is zero
247:12 - then then
247:14 - your your Matrix is not invertible your
247:18 - Matrix is not invertible is not
247:23 - invertible okay so this is this is this
247:25 - is what the whatever we have studied in
247:27 - this video and I hope that you are able
247:28 - to understand so in the next session we
247:31 - started started talking about uh the
247:33 - systems of equations and and and and and
247:36 - we also talk about uh and then we will
247:38 - talk about igen vectors and igen values
247:40 - we'll also talk about Rank and rank of a
247:42 - matrix and choice of a matrix so I think
247:45 - we should end this video I'll be
247:46 - catching up your next video till then
247:47 - bye-bye have a great
247:51 - [Music]
247:59 - day hey everyone welcome to this next
248:02 - next lecture on trace of a matrix so in
248:05 - this video I'm going to talk about trace
248:08 - of a matrix okay uh Trace will will will
248:11 - will I I I will talk about how do we
248:14 - calculate the trace of a matrix and this
248:16 - video is not only about Trace we'll
248:19 - we'll talk about something called as
248:21 - hardart product which is one of the most
248:23 - important stuff we'll talk about its
248:25 - properties we'll talk about a cyclic
248:28 - property of a trace as well we'll talk
248:30 - about some more properties of a dress
248:32 - trace and then we'll talk about what are
248:35 - some exceptions in Trace and then we'll
248:38 - talk about our uh ner product I don't
248:41 - know how to pronounce it I think think a
248:44 - k is U silent over here so I think Ron
248:47 - roner products we'll talk about that and
248:51 - how with one with one example okay so
248:53 - this is the agenda for this video and
248:56 - the notes for this video is in
248:58 - description box so I hope that you will
249:00 - be able to go in the description box and
249:02 - see there uh the the notes of this video
249:05 - and maybe if and if if you're wondering
249:08 - whom to whom I'm writing on it's the
249:11 - updated Microsoft whiteboard and and I
249:13 - and I simply think that's that this is
249:16 - amazing this is going to beat every
249:18 - whiteboard available in the market uh so
249:21 - this is just an information from my side
249:23 - uh of the stuff so let's get started
249:25 - talking about a trace of a matrix but
249:26 - before that I want to I I want to give
249:29 - you some some class announcement the
249:31 - first announcement is we are removing
249:33 - around five students from the course by
249:36 - personally emailing them first of all
249:39 - understanding their what they're facing
249:41 - and if we think okay we can remove if we
249:43 - don't get a response from that student
249:45 - we are going to just remove them from
249:47 - the LMS okay so new seats are being
249:50 - available so you can go ahead and and
249:52 - draw for that LMS for absolutely free
249:54 - it's absolutely free so you can go ahead
249:57 - and assignments are also being released
249:59 - okay so if you're not able to do the
250:01 - assignments you are most likely uh not
250:03 - going to get uh you are not getting the
250:07 - grades and you'll not most likely not
250:09 - able to pass the course okay so so let's
250:11 - talk about trace of a matrix so so so
250:14 - these are some of the class
250:15 - announcements so let's get get ahead and
250:17 - talking about a trace of a matrix so so
250:21 - for for taking out the trace of a matrix
250:24 - it should be a square Matrix so for
250:26 - example let's say you to calculate the
250:28 - trace want to C calculate the trace
250:32 - trace of a
250:35 - square Matrix so you want to calculate
250:39 - the square of you want to calculate the
250:42 - trace of a square Matrix a okay and
250:46 - usually write that and and usually write
250:49 - that TR R and so this is the this is the
250:53 - formal notation for denoting that we are
250:56 - taking out the trace of the Matrix a so
250:59 - this is read as a uh taking out the
251:02 - trace this this this one is nothing but
251:05 - your Trace there's nothing but your
251:08 - Trace okay so so you're actually taking
251:10 - out the trace of a matrix a cool so I
251:13 - think that this is this is good to go so
251:15 - the trace of a matrix a is is defined by
251:19 - T TR and then in bracket you write a
251:22 - okay so so how do what's so how do we
251:25 - calculate the trace of a matrix so how
251:27 - do we even bother calculating the trace
251:30 - how do we even bother calculating the
251:32 - trace of a matrix for calculating the
251:35 - trace of a matrix we do sum of elements
251:38 - sum of elements sum of elements sum of
251:44 - elements sum of elements on the main
251:46 - diagonal of the Matrix a on the main
251:53 - diagonal
251:54 - of the Matrix a of the Matrix a or you
251:58 - can say of a okay and a is a matrix or a
252:02 - square Matrix so so the the how do we
252:05 - calculate the trace of Matrix the trace
252:06 - of a mat is calculated by the sum of
252:10 - elements on the main diagonal of the A
252:13 - and B say for example say for example
252:16 - you have a matrix a you have a matrix a
252:20 - you have a matrix a so let's assume that
252:22 - the M Matrix is 3x3 Matrix so let's
252:24 - assume the Matrix is 3x3 Matrix so just
252:27 - I'm going to take the maybe this color
252:29 - this shoots okay so you
252:33 - have uh so let's write it out
252:36 - a11
252:38 - a12 a13 so let me just draw it over here
252:41 - as well
252:43 - okay so we have that and then let's draw
252:46 - A2 1 a22 a23 and let's pick this green
252:52 - color a maybe 31
252:56 - a32 and a33 so you have this Matrix so
252:59 - you have this Matrix a so you have this
253:01 - Matrix a and you want to calculate the
253:04 - trace of this Matrix so for calculating
253:06 - for calculating the trace of a matrix a
253:09 - what you will do you will just um sum
253:12 - the elements of the main diagonal so a11
253:15 - + a22 + a33 okay so the main diagonal is
253:21 - this this one and this one then this one
253:24 - okay so and then you'll be getting your
253:26 - scaler as an output it can be C uh
253:29 - whatever the number is okay so this is
253:31 - how you calculate the trace of a
253:33 - particular Matrix and and formally I can
253:36 - Define this I can Define this I I can
253:41 - Define this as
253:44 - uh I can Define this as a oh my God I
253:47 - think sumission is not written correctly
253:49 - I = to 1 all the way down to the three
253:52 - means in this case we are we want for a
253:55 - 3X3 Matrix so I = to 1 all the way to
253:57 - the three all the way to the 3 a i i
254:01 - okay so if we go on first of all a i =
254:05 - to 1 then that will be a11 + a22 +
254:10 - a33 okay so so this is how this is what
254:13 - the formal notation for uh uh for
254:15 - submission notation or the for Loop for
254:18 - particular for taking of the trace of a
254:20 - matrix okay so so let's take one example
254:24 - to perform the to perform the necessary
254:26 - accents accents so let's say let's say
254:29 - for particularly you have a data you
254:30 - have the particular Matrix you have
254:32 - particular Matrix a so just drawing the
254:34 - same Matrix and let's draw uh let's take
254:39 - one3 and let's take this yellow and
254:42 - let's write 11 5 2 let's take U uh green
254:48 - and let's write 62 - 5 and then let's
254:52 - draw a matrix so you have this Matrix
254:54 - now you want to calculate the trace of
254:56 - this Matrix the trace of a matrix is
254:58 - nothing but you want to calculate the
254:59 - trace of the Matrix a the trace of a
255:02 - matrix a is nothing but uh is nothing
255:05 - but it will go from I = to 1 all the way
255:09 - around to 3 a i * a I I think it's a a i
255:13 - * I okay uh this is what you and then
255:17 - sumission will will will this is the
255:19 - submission so it will it will add it up
255:21 - so 1 + 5 + - 5 which will be nothing but
255:27 - 1 and 1 is the trace of a particular
255:31 - Matrix okay so this is the the one is a
255:34 - trace of the particular Matrix and I
255:36 - hope that you are able to make sense out
255:38 - of it and why do we bother studying
255:40 - Trace because because when you when when
255:43 - you go further me solving some some
255:45 - linear equations or or or systems of
255:47 - equations and and this is this is this
255:49 - is mostly used and in other formulas U
255:53 - mainly not in specifically linear
255:55 - algebra mainly some other formulas they
255:57 - are these straight race of magics helps
256:00 - the computation to be easier or help the
256:02 - too too much anotations as well as it
256:05 - has some cool properties okay so this is
256:07 - very very helpful not in only the con
256:09 - not in only the context of deep learning
256:12 - or machine learning it is most in the
256:14 - concept of context of maths and and and
256:17 - Mathematics and and and you you get in
256:19 - linear algebra and you get in other
256:22 - maths field or discrete M applications
256:25 - okay so this is what the trace of a
256:28 - matrix so we have a show you one example
256:30 - so I hope that you able to make sense
256:32 - out of it cool so let's see some some of
256:34 - the properties some of the properties
256:36 - some of the properties of the trace of a
256:38 - matrix so so some some of the properties
256:40 - of a trace of a matrix I'm just going to
256:42 - write in Black back so the
256:45 - properties
256:47 - properties so the first property which
256:49 - want to highlight the first property
256:51 - which I want to highlight is the trace
256:54 - of the Matrix a plus b can be written as
256:58 - the trace of the individual Matrix a
257:00 - plus trace of a matrix B so so I can
257:03 - write in this particular format it now
257:06 - the second so you can you can just think
257:08 - think think about it in a bit means
257:10 - let's let me just show you if if I can
257:12 - and I will just ask you to do experiment
257:14 - with it but I will just take an example
257:16 - to prove it example okay so let's say
257:20 - let's assume that that you're given a
257:22 - matrix a you're given a matrix a which
257:24 - is 2 2 2 2 and you're given a matrix B
257:27 - and and and you're given a matrix B 333
257:31 - 3 okay and you want to calculate the
257:34 - trace trace of a matrix A+ B okay A + B
257:40 - so what you can do first of all you can
257:43 - C calculate the trace of a matrix a you
257:45 - can calculate the trace of Matrix a so
257:47 - the trace of a matrix a is nothing but 2
257:49 - + 2 okay which is four okay and the
257:53 - trace of a matrix B which is nothing but
257:55 - 3 + 3 6 and then and then trace of a
258:00 - plus trace of B which is nothing but 4 +
258:04 - 6 but 10 okay so so this is this is how
258:08 - what you you can take it out or in other
258:11 - words now if you first of all now this
258:13 - is the 10 we have proved we have taken
258:15 - out the uh this side rhs side let's see
258:18 - the LH side so in LHS so let's say let's
258:22 - say for a particular example for a
258:24 - particularly for for for example let's
258:26 - say let's let's take these one Matrix
258:28 - and then try to add it first so first of
258:30 - all We'll add it so let's add this
258:32 - Matrix A and B so 2 2 + 3 which is 5
258:35 - then 2 + 3 which is five five five so we
258:40 - got 2x two Matrix when adding these two
258:43 - Matrix Two element wise addition so you
258:45 - get a matrix a plus B A + B which is
258:49 - this one now if you calculate the the
258:52 - the the the the trace of this particular
258:55 - Matrix a plus b so the trace is the sum
258:58 - of the elements in in the main diagonal
259:00 - so 5 + 5 is what 10 and we had proved
259:05 - therefore LHS is equals to rhs and hence
259:09 - proved okay so this property is proved
259:12 - proved like this okay so this is how you
259:15 - can prove the properties if you want and
259:17 - this is very very useful if you if if
259:19 - you want to get the clarity in your
259:21 - particular Matrix okay so another
259:23 - property which I want to show show
259:24 - showcase to you all is another property
259:27 - which is the trace of a matrix when it
259:31 - when it is Multiplied with some scalar C
259:34 - with some scalar C okay so so what is
259:37 - the trace of this Matrix so a is a
259:40 - matrix and C is some scalar C is some
259:44 - scalar so that is equ equivalent equals
259:47 - to C trace of a okay so first of all you
259:51 - take out the trace of a and then
259:52 - multiply with that c okay and that
259:54 - actually C what it does it just
259:56 - stretches it it simply means it
259:58 - stretches your Matrix okay and if if you
260:01 - talk about a geometric view okay so it's
260:03 - exactly what it's trying to do it's you
260:06 - you can hence prove it as well say for
260:08 - example you want to you have let's say
260:10 - for for for a particular example let's
260:14 - say you have a matrix a which is 2 2 2 2
260:17 - okay and you have the scaler C which is
260:20 - two okay which is two so this is and
260:24 - then you want to take out the trace of
260:26 - CA so first of all you can take out the
260:28 - trace so the trace of a is so first of
260:30 - all let's take out the trace of a the
260:32 - trace of a is four the trace of a is
260:34 - four 2 + 2 4 and then when you when you
260:37 - multiply the trace of the two times the
260:39 - trace of a which is 4 * 2 which is
260:43 - nothing but eight is a particular answer
260:45 - of that question okay so eight is the
260:47 - particular answer for that question and
260:49 - it makes sense you can this is this is
260:51 - the this is the this is the your uh rhs
260:55 - proved and sorry yeah rhs proved you can
260:58 - prove it the same so when you take out
261:00 - the trace of the mrix which is the mrix
261:02 - is eight sorry so sorry it's four plus
261:05 - oh my God 4 4 4 4 and then we take out
261:08 - the trace of this Matrix which is 4 + 4
261:10 - 8 so and hence proved so r and E equals
261:13 - to LHS okay so these are the properties
261:16 - which I'm not going to prove all of the
261:17 - properties I've just showed you how I'm
261:19 - how how I proceed pro pro proving this
261:21 - stuff okay so another another property
261:25 - another property another property is a
261:27 - trace of a matrix a is equals to trace
261:32 - of a transpose of that Matrix it gives
261:34 - you a Clarity whether we transpose it
261:37 - that the main diagonal will be the same
261:39 - and the trace will be also the same okay
261:41 - so I I I need to prove it to Showcase to
261:43 - you all to see the interesting property
261:45 - of this the interesting property of this
261:47 - let's say for for for example you have a
261:50 - 2222 my favorite Matrix so so it is just
261:53 - Square Matrix because for particular
261:55 - Trace you need to have an Square Matrix
261:58 - and yeah it it it would work well okay
262:01 - so you you have a square Matrix 2x two
262:05 - okay so when you do the transpose of
262:06 - this Matrix when you not exactly you
262:08 - don't need a square Matrix to be done
262:10 - transpose you just need to D some some
262:12 - of the elements of the main di diagonal
262:15 - you don't need to do the uh you need you
262:17 - need not to have only the square m I'm
262:19 - just taking an example to make it
262:21 - convenient for you so so first of all
262:23 - you take out the trace of this so let's
262:25 - first of all take out the trace of this
262:26 - Matrix the first uh which is four trace
262:29 - of this will be four because the main
262:31 - diagonal 2+ 2 is four okay when you
262:33 - transpose it when you transpose It 2 2 2
262:36 - two okay 2 two okay so this is this this
262:40 - is how it looks and and when you when
262:41 - you when when you go ahead and then uh
262:44 - add it it is also four okay the
262:48 - transpose does not matter and of course
262:50 - I have taken very very easy easy
262:51 - examples and exactly and Hance proved
262:55 - okay so whether you do the transpose the
262:57 - diagonals will remain same now next next
263:00 - property is uh the trace of a product so
263:04 - so so let's let's let's talk about an
263:06 - another greatest stuff which is a trace
263:08 - of a product
263:10 - trace of a
263:13 - product and the trace of product is the
263:16 - trace of a transpose B okay is equals to
263:21 - a trace of ab transpose which is also
263:25 - equals to trace of B transpose a which
263:29 - is also equals to trace of B A
263:32 - transpose okay so these are equivalent
263:35 - equal whatever we have written these all
263:37 - are equivalent equal whatever I have
263:39 - written over here and is nothing but uh
263:43 - this is we are we are we are multiplying
263:45 - the two mates the trace of the product
263:48 - okay so I hope that you're able to make
263:50 - sense out of it it's not a big deal for
263:52 - you okay so this is this this is just
263:54 - the basic properties which I'm
263:56 - highlighting okay so we had seen some
263:58 - some of the properties and and I want to
264:00 - talk about some more properties which
264:02 - are which are more useful means means we
264:05 - are also going to gather some
264:07 - information about uh a b other kind of
264:10 - products okay that we are going to to
264:12 - deal with Okay so let's let's let's talk
264:15 - about the the the hardart product okay
264:18 - and hardart product is is one of the
264:20 - most important product which you will
264:22 - see so let's talk about hardart product
264:24 - and it's very very important as well
264:26 - because if anybody go on Wikipedia and
264:28 - see some properties what is hardam
264:30 - product and what is um raw Necker
264:33 - product we will see that but before that
264:36 - I want to highlight one more property of
264:38 - a trace one more property of a trace the
264:41 - the trace the cyclic property which I
264:44 - don't which which which I can't forget
264:46 - means I I can't forget it if I forget it
264:49 - no one is going to to leave me so the
264:51 - the the I'm just highlight the cyclic
264:54 - cyclic property cyclic property of Trace
264:58 - okay and then and then we'll talk about
264:59 - two two kind of product which is hardat
265:02 - product and roniker product okay so
265:05 - we'll talk about that two kind of
265:06 - product in detail okay so the cyclic
265:09 - property says that let's say let's say
265:11 - for example going to calculate the trace
265:13 - of a matrix a b c d okay A B C D is just
265:17 - a multi M multiplication of a four
265:19 - Matrix so it is nothing but equals to
265:23 - you can just you can just take the you
265:25 - can just b a c d which nothing but
265:28 - equals a trace of CB uh I think it's c d
265:35 - a c d AB which is nothing but trace of d
265:40 - a b c okay so first of all B Gone to the
265:44 - um B B Gone first then C gone first then
265:47 - D gone first okay that's a cyclic
265:50 - property it's it goes in cycle okay but
265:53 - but but arbitrary permutations are not
265:56 - allowed but arbitary but these are not
266:00 - allowed I I think they done wrong over
266:03 - here it should be something like U first
266:05 - of all it was B and then it should be uh
266:08 - c b a d I think so like this yeah I hope
266:12 - so it should be like this and then when
266:14 - you go ahead um yeah so it should be
266:18 - something like this and then you keep on
266:19 - rotating after to recheck the thing but
266:22 - it but it keeps
266:26 - on so the trace of the cyclic property
266:30 - says over here so b c da a then you have
266:32 - a c d AB so first of all B Gone first
266:36 - and then we have CD and a a going last
266:40 - okay and then this is a cyclic per mut
266:42 - ations that Trace is an invariant under
266:44 - the cyclic permutations and the
266:46 - permutations you all know you can check
266:48 - this out this is the property this this
266:50 - property is known as cyclic permutation
266:53 - and one more thing I want to highlight
266:54 - over here that no arbitrary permutations
266:57 - are allowed you cannot do something like
266:59 - this okay so you have to have the
267:02 - symmetric matrices are considered it
267:05 - means if the product of three matrices
267:08 - symmetric me are considered then any
267:10 - permutation is allowed so for example
267:12 - if it is symmetric Matrix and for being
267:14 - a symmetric Matrix it should be equal to
267:16 - its transpose the the the the matrix
267:19 - product should should should be equals
267:20 - to transpose and the trace of a matrix
267:22 - should be equals to transpose of that
267:24 - matx trace of the transpose of that
267:25 - Matrix then it's called the symmetric
267:27 - Matrix and and then any permutations is
267:30 - allowed okay any permutations is allowed
267:33 - except this this is the only case where
267:35 - the any permutations allowed otherwise
267:37 - we don't allow any permutations are
267:38 - allowed in cyclic property okay so this
267:41 - is this is Clic property which you have
267:43 - to remember okay I may have written it
267:45 - wrong but I hope that you will correct
267:47 - it out by the Wikipedia page so this is
267:49 - this this this this was the cyclic
267:51 - property which I want to highlight and I
267:53 - hope that you are able to uh understand
267:56 - it a much better way so let's talk about
267:59 - um let's let's talk about hardart
268:01 - product so let's talk about hardart
268:03 - product which are B because we haven't
268:05 - we hadn't have a chance to talk on this
268:08 - hard demand hard demand product as as as
268:12 - as it seems to be a bit useful for me
268:14 - because it is also used in various
268:16 - Quantum Computing as well as machine
268:17 - learning as well as deep learning so
268:20 - let's assume let's assume that you want
268:22 - to take out the dot sorry I'm I'm saying
268:25 - dot product it should be hamat product
268:27 - hamat product when it multiplies with
268:29 - the two matrices it just do the element
268:31 - wise product okay and the and the Matrix
268:34 - and just just to the element wise
268:36 - product and gives another Matrix of this
268:39 - of of of the same Dimension Okay so so
268:42 - let's say for for example we given a
268:43 - matrix a multiply The Matrix B okay and
268:47 - this is the sign for the har product is
268:49 - just a small O Okay so so so let's say
268:53 - you have a matrix a we have a matrix a
268:55 - a11 a12 a13 A2 1 a22 a23
269:04 - a31 um a32 a33 so so you have this
269:08 - Matrix a you have this Matrix a and you
269:11 - have Matrix B and you have another
269:14 - Matrix B and another Matrix B is maybe
269:18 - b11 B12 B13 B B21 B22 b23 b31 b32
269:30 - b33 okay you have the two matrices and
269:32 - when you take out the dot sorry hardat
269:35 - hardat product hardat product between
269:38 - these two Matrix which will be nothing
269:40 - but the product wise multiplication so
269:42 - a11 b11 a12 B12 a13 B13 then
269:50 - a21
269:51 - B21 a22 B22 a23 b23 okay a31 B2
270:00 - b31 um um it's it should be a32 b32 and
270:04 - then a33 b33 okay so this is your Matrix
270:10 - uh the heart of our product which is Tim
270:12 - element wise product okay so I hope that
270:14 - you are able to get means element wise
270:16 - product it's not a big deal it's very
270:17 - very easy to understand okay so some s
270:20 - some of the properties of this some of
270:21 - the properties which I want to highlight
270:23 - properties properties which I want to
270:25 - highlight some of the properties of this
270:27 - particular uh product is is you can
270:30 - write it out in this format which
270:32 - is uh which is equals to this sorry B *
270:36 - A B B BB do B Circle A and then a
270:42 - and then b c which is nothing but equals
270:45 - to A
270:48 - B do c I hope that you all remember the
270:51 - properties name okay another is a uh b +
270:57 - C which is nothing but equals to A B + a
271:03 - c okay so these are some some of the
271:05 - properties which are highlighting over
271:07 - here A C nothing but equals to uh
271:12 - okay you can write it out something like
271:14 - this a z which is not nothing but equals
271:16 - to 0 time with 0 a which is nothing but
271:19 - equals to zero okay so this is these are
271:21 - some of the properties of a hardar
271:23 - product which you which I want you to
271:25 - think about it and and solve it on your
271:27 - own and prove it and and then prove
271:30 - these properties if you want okay and
271:33 - just like I've Tak taken some examples
271:35 - and proved it so let's talk about
271:37 - another so one so so let's so I'm just
271:41 - just going to show you what are the
271:42 - property of the trace of a trace of a uh
271:45 - trace of a hardat product the trace of
271:47 - the hardat product where is the where is
271:50 - that trace of a hardart product I I'm
271:53 - not seeing where it at but yeah it's
271:56 - it's very it's one of the important
271:58 - stuff but yeah let's go ahead and
272:00 - talking about um um and droner product I
272:04 - don't know how to pronounce it I'm
272:06 - pronouncing it good or bad way but I
272:08 - want you to think about this means uh
272:11 - you you wanted to the something called
272:13 - as roniker
272:15 - product roniker roniker
272:19 - product so I think we have to talk on
272:22 - this so roniker product let's let's talk
272:25 - let's talk about this Ron roniker
272:27 - product so you have a matrix a you want
272:31 - to calc you want to do the Rona product
272:33 - A and B between A and B okay so let's
272:36 - say and we we this is the roniker
272:38 - product sign for showcasing okay this a
272:40 - r roniker product product and this is
272:43 - how it looks 1 2 3 4 and then uh like
272:47 - this 0 5 6
272:51 - 7 okay and then you and this is how the
272:54 - raw product works so what you do you
272:57 - make a big Matrix means a big Matrix
272:59 - like this and you take this first
273:01 - element and multiply with all as a
273:04 - scaler and multiply The Matrix on Under
273:07 - The Matrix B so something like this 0 5
273:10 - 6 7 okay
273:12 - and then you and then you go on as a two
273:15 - as a scal in the second element you have
273:17 - 0 5 6 7 okay third element you three 0 5
273:23 - 6 7 okay uh 4 0 5 6 7 okay this is how
273:31 - it should work and and and and then what
273:33 - you do and then what you do you simply
273:36 - uh and and then you follow your whole
273:38 - procedure the whole procedure is uh
273:40 - which is nothing but one 1 * 0 then then
273:45 - 1 * 5 1 * 5 then we have 1 * 6 1 * 6 1 *
273:52 - 7 for for for this one I'm I'm doing
273:55 - like this okay and then you keep on
273:57 - doing this 3 * 0 3 * 5 3 * 6 3 * 7 I'm
274:06 - doing doing for this and then let's do
274:08 - for the same 2 * 0 then 2 * * 5 2 * 6 2
274:14 - *
274:15 - 7 4 * 0 4 * 5 4 * 6 4 * 7 okay and then
274:25 - when you when you do this stuff you'll
274:27 - be left with you'll be left with 0 5 0
274:33 - 10 uh 6 7 12 14 0 5 15 actually 0 20 and
274:41 - in 18 21 24 28 and this is what you get
274:45 - as a 4x4 Matrix this is this is nothing
274:49 - but called the roniker product and one
274:51 - of the property and trace this is the
274:53 - the trace of uh the trace of this Ron
274:58 - product is can be written like this can
275:01 - be written like this can be written like
275:03 - this uh trace of trace of a and then
275:09 - times the trace of B okay okay so you
275:12 - can write it something like this but but
275:15 - but one thing which I want to highlight
275:16 - over here you cannot you cannot uh do
275:20 - something like this in a regular case in
275:22 - your in in any your if I could show it
275:25 - to you if if it could have a chance to
275:28 - show show it to you so let me just show
275:30 - it on a Wikipedia page I think that
275:33 - exactly not equals to H so the trace if
275:36 - if if you see the trace trace of a
275:38 - matrix product okay the trace of matrix
275:40 - product it can cannot be written like a
275:43 - * b as a matrix product usual matrix
275:46 - product which is Trace of a which is
275:48 - which is which is completely wrong it it
275:50 - cannot be written but there's a case
275:52 - which is Ronica product which we can
275:54 - write it something like this but in magx
275:56 - product we are not allowed to do that
275:59 - okay so so this was the bit about this
276:02 - uh Ronica product and I hope that you
276:04 - understood the hard demand product and
276:06 - the trace of a matrix so I hope that
276:07 - that we are done with the Core Concepts
276:09 - of linear algebra from the next session
276:11 - we'll be starting talking about systems
276:13 - of equations solving systems of
276:15 - equations lud de composition gion
276:17 - mixtures and then we are done with this
276:20 - uh chapter and I hope and and I and I
276:22 - really appreciate your patience
276:23 - throughout this course and then and and
276:26 - I I guarantee that you will learn
276:27 - calculus and the easiest way that I can
276:30 - and and I also hope that I will
276:32 - providing so much Valu to you all uh
276:34 - even I'm I I think uh it it it motivates
276:37 - me like if if I see the watch hours and
276:40 - it's bit increasing and it motivates me
276:42 - a lot okay uh peoples are watching these
276:45 - videos so please keep watching please
276:48 - share this course it's very very
276:49 - important for me so that it could reach
276:51 - to lots of viewers and I know this
276:54 - course will be boom in upcoming future
276:57 - and many people are going to take
276:59 - benefit from this and some of the
277:00 - comments are say said that you're that
277:03 - please don't stop uploading the videos
277:05 - this course will be boom on future if I
277:07 - just complete the course and then I show
277:09 - it to the viewers hey see the course is
277:12 - completed complete this you are it is
277:14 - more than any premium course you're
277:16 - you're getting for absolutely free
277:17 - you're getting the assignments you're
277:19 - getting everything for free so I think
277:21 - that it is so so much of uh the features
277:24 - which are available for free the people
277:26 - do not give when they when you when you
277:28 - pay for that okay so I hope that's
277:30 - that's it for this video I'll be
277:32 - catching up in next video till then
277:33 - bye-bye have a great
277:34 - day so let's get started talking about
277:37 - systems of equations as this is the last
277:40 - video on linear albra series and I hope
277:43 - that you are able to understand
277:45 - everything in linear algebra as well as
277:48 - some of some of the videos of linear
277:50 - algebra is already released before
277:52 - around nine nine videos and I hope that
277:55 - you are able to make sense out of it uh
277:57 - so we had a talk on everything which is
277:59 - required for deep learning for further
278:01 - deep learning is we have already talked
278:04 - about that and one concept which I
278:06 - haven't talked is igen vectors and igen
278:10 - values I'll I'm going to talk about I
278:12 - have already talked about this in my PCA
278:15 - videos so as I haven't seen much of use
278:18 - in deep learning but yeah surely you can
278:21 - go to my pcab video or principal
278:24 - component analysis video in ml1 and you
278:27 - can see there about igen vectors and
278:29 - igen values if you're interested in
278:31 - learning that so this is the last video
278:33 - on systems of linear equations so I'm
278:36 - just going to write linear as it does
278:38 - not make sense so linear equations so
278:41 - we'll try to solve we'll first of all
278:43 - see what the systems of linear equation
278:47 - means and we will see also how to solve
278:51 - this um this linear equations and every
278:54 - stuff okay so what are linear equation
278:58 - so as the as first of all let's let's
279:01 - start with the what is a linear equation
279:03 - a linear equation is it's an equation
279:05 - for a line so so as you have seen in
279:08 - linear regression so your this is the
279:11 - this is this will this will be your
279:13 - linear equation whatever the hypothesis
279:16 - is maybe the hypothesis h of xal to
279:19 - Theta 0 + Theta 1 * X X1 okay so this
279:25 - this this this equation is your linear
279:28 - is your linear equation the reason why
279:31 - I'm saying is a linear equation because
279:33 - over here it does not have any powers it
279:36 - it has a power of a one and and it is a
279:38 - straight line or or or or a line okay so
279:42 - so some of the examples of linear
279:44 - equations some of the examples of
279:47 - linear linear equation so I'm just going
279:50 - to give some some examples so maybe y =
279:54 - to
279:56 - 3.5 -
280:01 - 0.5x okay another example may be y =
280:07 - 0.57 - x okay but they they both mean
280:10 - the same they both is actually
280:12 - equivalent whatever you write with in
280:14 - that form or you write in that form and
280:17 - you can also write this out in
280:19 - y+
280:21 - 0.5x = 3.5 you have written that into
280:25 - another mode you can also write y + 0.5x
280:29 - - 3.5 = 0 you have written that into a
280:32 - new mode or you have written this this
280:34 - into new mode so they are equivalently
280:37 - the same linear equation equivalently
280:40 - they are same linear equation
280:42 - same linear equation they are not
280:46 - different linear equation as you can see
280:48 - over here uh so let's let me just show
280:51 - you so first of all let's see this one
280:53 - okay y = 3.5 - 0.5 * X and if if you
280:57 - compare y = 0.5 and when you do the
281:01 - manipulation of algebraic manipulation
281:02 - you will get this and you can check that
281:05 - exactly if we will L to that okay uh
281:08 - which you can see 0.5 * 7 - 0.5 * 6
281:13 - exactly what what what what what that
281:15 - second equation is telling and then you
281:17 - just you you you have just converted
281:19 - this 0 Plus 0.5x on the on on on the
281:23 - left on the left hand side okay and
281:27 - making that equ equals 3.5 you just
281:29 - manipulated this is a manipulation and
281:31 - they mean the same okay so this is these
281:34 - are the linear equations which I want to
281:36 - show it to you so so we had a talk on
281:39 - linear equation that is an equation for
281:41 - a line okay so it's an equation it's an
281:47 - equation of a line or a for a line
281:50 - whatever okay so what is systems of
281:53 - linear equation so systems of linear
281:56 - equation is when we have two or more
282:00 - linear regession so so sorry it's linear
282:03 - equation they are working together okay
282:06 - so so as as from from from from history
282:10 - we have seen particular statement is
282:12 - called something and a group of
282:14 - particular statements are called
282:16 - something okay so in the same way for
282:19 - example if if you have seen my linear
282:21 - combination videos so we have we had
282:24 - told the set of all the linear
282:27 - combination I think yeah the linear
282:29 - combination is called span and only one
282:32 - that is is called a span okay so the set
282:35 - of they are working together so in the
282:37 - same way um we have the systems of
282:40 - equations are the set of or when we have
282:44 - two or more linear equations working
282:47 - together okay so system of equation the
282:51 - definition of a systems of equation is
282:53 - when we have two or more 2 plus 2 plus
282:58 - linear equation 2 plus linear equation 2
283:00 - plus linear equation when we have 2 plus
283:04 - L linear equation um when we have two
283:07 - plus linear equation uh they are working
283:10 - together they are they are working
283:12 - together then that's called system of
283:15 - equation working together together they
283:20 - are nothing but called they
283:23 - are
283:24 - called
283:26 - system of
283:30 - linear equation okay so so systems of
283:34 - linear equation means there are two or
283:36 - more linear equations working together
283:38 - that's nothing but the systems of
283:40 - equations so for example so let's say
283:42 - for for for example U 2x + y = 5 - x + y
283:52 - = 2 that is you have the set system of
283:55 - linear equation and you need to find the
283:58 - value the value of X and Y by solving
284:03 - this system of equation we'll see how to
284:05 - solve system of equation using
284:08 - substitution elimination and Al
284:11 - algebraic manipulation we'll try to see
284:14 - how to solve how to find X and Y so they
284:16 - both are working together they they both
284:19 - are they both this is a a system of
284:21 - linear equation these both equations are
284:24 - linear equation and and and and using
284:26 - the system of L or we need to solve this
284:28 - uh system of equation by finding the
284:31 - value of X and Y okay so so so let's so
284:36 - let's see how do we proceed further in
284:38 - solving the systems of linear equation
284:41 - so I'm just going to see the recordings
284:42 - is being recorded yeah correct it is
284:44 - being recorded let's let's go ahead and
284:46 - solving the linear or so sorry it's
284:48 - systems why I'm saying the linear and
284:51 - linear regession uh just want to share
284:53 - one incident I was giving giving a
284:55 - speech and there was a kind of St stuff
284:57 - deep meaning okay so the I was having a
285:01 - quot and then I was telling the the
285:02 - beautiful line is is the title of this
285:05 - and and the deep meaning so I was so I
285:07 - was in a stage and and and and told uh
285:10 - the deep learning of this Cote so I was
285:13 - like so seriously I told the Deep
285:15 - learning but no problem again I
285:17 - corrected it but yeah it was a funny no
285:20 - one was knowing about deep learning in
285:22 - my school that uh at that point okay so
285:26 - solving so so so we need to solve let's
285:29 - let's take one example let's take one
285:30 - example to solve one linear equation to
285:34 - to to to solve a linear equation so
285:37 - let's go ahead let's let's go ahead uh
285:40 - to to to solve the linear equation so
285:42 - let's say solving you need to solve this
285:45 - x + y = 6 3x + Y which is nothing but
285:52 - equals to what uh 2 okay so you need to
285:55 - find the value you need to find the
285:57 - value of X and Y so how you're going to
286:01 - proceed further what we can do is simply
286:04 - merge them together merge them into one
286:06 - question into one uh one one equation by
286:10 - making the LHS to the LHS side and rhs
286:13 - to rhs side okay so x + y - 3x + y okay
286:21 - so what I'm going to do is have over
286:25 - here we subtract it so let's sub
286:26 - subtract it out okay 6 - 2 so we are
286:30 - subtracting the linear the the equations
286:33 - okay so from the the second equation
286:36 - from the first equation sub subtracting
286:38 - the second equation from the first
286:39 - equation and and then we are done so we
286:42 - are sub subtracting with this now we if
286:43 - we x + y + 3x - y = 4 and then you
286:49 - proceed further is 4X and then it is cut
286:52 - okay 4X = 4 and x = 1 okay so when you
286:57 - when you say that you you you got one
287:00 - what you can do so we now know the value
287:02 - of x you now know the value of x so
287:05 - after knowing the value of x you can put
287:07 - them together 1 + y = 6 when you put X x
287:11 - = 1 x = 1 1 + y = 6 and - 3 * 1 + Y = 2
287:19 - and Y = to I think 5 and maybe y = to 5
287:26 - okay so uh so y = 5 of course you you
287:29 - can you you you you check with both the
287:32 - equations and both the equations yields
287:34 - the same y when when when you put X X's
287:38 - okay so so you can solve any of these uh
287:40 - equation and maybe the first equation or
287:43 - the second equation and then take y I've
287:45 - just done to to to show you the the
287:47 - check that okay the second equation
287:49 - means the same what exactly the first
287:51 - equation means okay so the value of of X
287:56 - and Y will be x = 1 and y = 5 is the
288:00 - solution to the system of equation okay
288:04 - so when you when you so you find the
288:06 - value so the question was to find the
288:07 - value of x and y and you are done with
288:09 - finding the X and Y values and and it is
288:12 - nothing but a beauty of algebra and and
288:14 - I seriously like algebra in these cases
288:16 - and and we'll see some of the some of
288:18 - these geometric examples that how it is
288:20 - working and and and and geometry
288:22 - geometrical interface of the systems of
288:24 - equations okay so so so like I think
288:27 - that we are able to make sense out of it
288:28 - and I also hope that you are able to
288:30 - understand it now we had seen the
288:32 - numerical understanding now let's try to
288:34 - Simply understand the geometrical aspect
288:36 - of this okay the geometrical aspect of
288:40 - this let's see let's say you have an X
288:44 - so sorry you have an X and Y AIS so
288:49 - sorry this is X and this is y AIS okay
288:53 - so let let let me to not write this out
288:56 - okay but it make more sense in in in in
288:58 - context of linear algebra so let's write
289:02 - 2 4 6 8 10 12 okay so let's go ahead
289:09 - with solving the same thing 2 4 6 8 10
289:17 - 12 okay now let's plot both of these
289:20 - equation let's plot both of these x + y
289:23 - = 6 and 3x + Y = 2 and graphing these is
289:28 - not a big deal if you to
289:30 - graph if you to graph the equations the
289:34 - equations graphing the equations I would
289:36 - like you we we'll cover that in a
289:37 - pre-calculus but I would like you to
289:40 - have some context understanding of how
289:42 - we graph the equation and the way we
289:44 - graph the way I used to just just just
289:46 - in this example let's assume that you
289:48 - want to grab the function you want to
289:50 - grab the function maybe F ofx = to x²
289:55 - how how how you going to graph it so you
289:58 - can just put the values of different
289:59 - different x's and point and keep keep
290:01 - mark on the point and and then and then
290:04 - and then after certain certain X's value
290:06 - just just join that point or not in the
290:09 - other words let's say for for example
290:12 - you do the same you you try the
290:14 - different different values of X you try
290:15 - the different different values of Y and
290:17 - keep pointing over there so you then you
290:19 - are able to graph the equations and if
290:21 - if if you still not if still confused I
290:24 - would like you to send to one video
290:26 - which will the link in description like
290:27 - graphing the equations which is not a
290:30 - big deal they just form a table and they
290:32 - just try different different X's values
290:34 - and then they get the Y values and then
290:36 - to try the yv value and then they get
290:38 - the x value okay and they keep on doing
290:40 - it until un they found some pattern in
290:42 - it and and it's not a big deal to found
290:44 - a pattern to to to have maybe three to
290:46 - four uh input values of x's and Y then
290:49 - you'll be getting your the the graph of
290:52 - that equation
290:53 - okay so I think uh let's plot this X and
290:56 - Y so the plot of this X and Y table will
290:59 - be nothing but uh will be nothing but
291:02 - what uh six and8 I'm just going to have
291:04 - a good uh so sorry I'm not able to plot
291:08 - clearly
291:11 - let me just touch this it's starting it
291:13 - is touching actually eight okay but it's
291:15 - looking very bad actually so let me try
291:19 - Okay so let's assume that it touches 8
291:21 - okay so this is8 okay and this is six
291:24 - and this is an equation for your x + y =
291:27 - 8 let's try to plot uh maybe this one um
291:31 - this this particular ter stuff 3 minus
291:34 - 3x + Y = 2 okay so how you going to plot
291:38 - it so the so the plot of this would look
291:40 - something like this so looks something
291:42 - like this so when you go ahead and touch
291:45 - this so this is your the the graph of 3x
291:50 - + Y = 2 this this is a graph of that
291:55 - equation and when you see over here so
291:57 - now you have these two equations now if
292:00 - you if you if if you go closer and
292:02 - closer to this uh if if you go a little
292:05 - bit closer to this you'll you'll be
292:06 - noticing that okay I think I done a
292:09 - little bit wrong over here here I've
292:11 - done little bit wrong over here of
292:12 - course and I have done little bit wrong
292:14 - over here I should give two over here
292:18 - rather than giving two over there and
292:20 - one over here okay just just but in if
292:24 - if you if you draw a graph formally on
292:25 - the graph paper you'll be getting exact
292:27 - stuff but I'm just assuming okay some
292:30 - something like this now the the
292:32 - intersection the intersection which
292:34 - you're seeing over here the
292:36 - intersection the intersection which
292:38 - you're seeing over here is actually Your
292:41 - solution of x or or or the values okay
292:45 - or the values or the values of X and Y
292:48 - or the or or you find the solution to
292:51 - both of the you you already solved using
292:54 - graph of the value of X and Y how where
292:58 - where the points intersect so I'm just
292:59 - going to write it out let's remove it
293:02 - first of all so the point intersect so
293:05 - it is nothing but five okay at Y and
293:08 - there nothing but one at X so X = to 1
293:12 - and y = 5 so you get the answer from
293:14 - here as well by geometry purpose the
293:16 - point of intersection the point of
293:19 - intersection is where where the lines
293:22 - intersect is actually the solution to
293:25 - that linear to that system of equation
293:28 - okay so when you when you can plot it uh
293:31 - maybe three linear equation and it will
293:34 - be in in in if if if you three linear
293:37 - equation with with the two variables
293:40 - okay so maybe it may be on uh uh on a a
293:45 - three-dimensional plane and where the
293:47 - plane intersect exactly that's a
293:48 - solution of x y and z if if there are
293:51 - three values X Y and Z that's not a
293:54 - problem of a two two dimensional
293:55 - variable that is for of a problem with
293:57 - three-dimensional variable where you
293:59 - have where you you plot the plane you
294:03 - have a plane something like this of x
294:05 - value y value and Z value and then you
294:07 - have this is this is one of the linear
294:09 - equation maybe you have another L linear
294:11 - equation and maybe you have another
294:12 - linear the point of intersection maybe
294:15 - this one is actually Your solution to
294:17 - that X and Y and Z okay so we'll see one
294:20 - geometric purpose later on uh but this
294:23 - is how the the 2D 2D geometry in 2D
294:26 - plane this is if you want to find the
294:27 - value of X and Y the point of
294:29 - intersection of the the the linear
294:31 - equations is actually the solution to
294:34 - the values of we able to find we we want
294:37 - to find it okay so I I hope that you're
294:39 - able to make sense out of it and it's
294:41 - not a big deal to understand it
294:44 - okay so I would like to go further I
294:47 - would like to go further into we had a
294:49 - talk on this is the two variables which
294:51 - is two 2D ples stuff so now let's go
294:55 - ahead now let's go ahead now now let's
294:57 - go ahead and talking about what that
295:00 - linear equation when we call a
295:01 - particular equation as a linear equation
295:03 - and when we call the particular equation
295:05 - as a nonlinear equation so who can tell
295:08 - me that 2x + y - Z = 4 is it a linear
295:14 - equation yes or no I think this is a
295:18 - linear equation but is this a linear
295:22 - equation but is this a linear
295:25 - equation so sorry I think uh I have done
295:28 - a wrong example but no problem 2x + Y 2
295:33 - - Z = to 4 is this a linear equation yes
295:37 - or
295:38 - no it's no because here the power is
295:41 - through as a quadratic equation okay so
295:43 - this is not a linear equation over here
295:46 - but this is a linear equation okay so
295:50 - I'm just going to get started with with
295:51 - talking about variables which we are
295:53 - dealing up okay so variables which
295:56 - you're dealing it up so if if you see
295:59 - this example if you see this example of
296:01 - this x + y = 6 and 3x + Y = 2 and then
296:07 - you add minus over here so over here
296:10 - here we the the the the system equation
296:12 - is a two Dimension system equation is a
296:15 - twood dimension two Dimension uh
296:19 - variables okay why because you have only
296:21 - two variables okay X and Y but what if
296:24 - if I add x + y = to or plus maybe Z = 6
296:30 - and 3x + y + z = two so you able to find
296:37 - x y and what about Z so this is a
296:39 - threedimensional problem through the
296:42 - system equation is a threedimensional
296:45 - [Music]
296:48 - the system of equation is a
296:50 - threedimensional stuff and
296:55 - uh okay so so this is how we are able to
296:58 - make sense out of it as two Dimension
297:00 - and threedimensional
297:02 - variables okay so let's get started
297:04 - solving up our problem so we have this
297:06 - two Dimension and three dimension Okay
297:09 - so I hope that you're able to understand
297:11 - what I'm trying to convey over here but
297:13 - in let's let me show you some other
297:15 - stuff so here we are seeing only two two
297:18 - system two two linear equation in the
297:20 - system equation your this just
297:23 - identify - 3x + Y = 2 and maybe 2x + 2 y
297:32 - = um 16 okay so just let me know what uh
297:38 - is this a two two two dimension or three
297:41 - dimension it is a two Dimension because
297:43 - there are only two unique variables
297:45 - which you need to find okay but what if
297:47 - I changed 2 Z
297:49 - and maybe 2 Z I just change it and 2 Z +
297:53 - 2 y it's a threedimensional problem then
297:55 - okay so it's a toally based upon the
297:57 - unique variables you have in your
297:58 - problem statement so here we have more
298:00 - variables you you you you in a system of
298:03 - equation you have more variables and
298:05 - it's AR
298:06 - one okay so I hope that you're able to
298:09 - make sense out of it and and I also hope
298:10 - that you are you are able to understand
298:12 - it okay so let's go so so I hope that uh
298:16 - it's just not a big deal for you at
298:18 - least and uh uh please recapture it if
298:22 - if if if you're not even com uh
298:24 - comfortable please rewatch the video
298:26 - again I would ask you okay so we had
298:28 - seen that maybe the the line of
298:31 - intersection is called the solution to
298:34 - the process equation but there are three
298:36 - types of possible solution cases so the
298:38 - first type is no solution second type is
298:40 - one solution third type is infinitely
298:43 - many solutions so for the system of
298:45 - equation you can have either you can
298:48 - have either no solution you can have
298:51 - either no solution you can have either
298:54 - one
298:55 - solution and you can have either
298:59 - infinitely
299:01 - infinitely many
299:03 - solution
299:05 - many solution okay so you are have you
299:08 - can have one solution no solution
299:10 - exactly no solution for that system
299:11 - equation you can have one solution for
299:14 - that equation and then you can have
299:15 - infinitely many equation many solutions
299:17 - for that okay so if there if when there
299:21 - is a no solution for that equations
299:23 - that's called inconsistent or if there
299:25 - is a one or infinitely many solutions
299:28 - that called consistent that's called
299:30 - consistent okay so let's I will just go
299:33 - through the note again but let's see
299:35 - what that geometric mean no solution one
299:37 - solution infinitely many solutions okay
299:40 - so so let's here is our graph here is
299:43 - our
299:44 - graph here is graph so over here let's
299:48 - say for for the second example this is
299:49 - your one so this is a this is maybe two
299:52 - two two variable Stu and two dimensional
299:55 - Stu so you have a two linear linear
299:56 - equation in that system of equation and
299:59 - you have the first equation and you have
300:01 - the second equation and what you can see
300:03 - over here they do not intersect they do
300:05 - not intersect they're parallel to each
300:07 - other so that's why they have the no
300:11 - solution they have the no solution okay
300:14 - so if if there's a point so this is an
300:18 - example of no solution another example
300:21 - is if you have this one and the and you
300:24 - have the another equation something like
300:26 - this the point of intersection the point
300:28 - of intersection which you see over here
300:31 - is this is this this this gra an example
300:34 - of one solution because there is one
300:35 - intersection at that point okay the
300:38 - infinitely Min solution graph will would
300:40 - look like something like this if you
300:41 - have if you have something like X and Y
300:44 - graph X and Y graph and you have the on
300:49 - the you have the first equation and the
300:51 - second equation is is on the same line
300:54 - of that first equation uh then that's
300:57 - called then there where we we will be
300:59 - having the infinitely many solution to
301:01 - that linear equation Okay cool so I hope
301:05 - that you are able to make sense out of
301:07 - it and it's not a big deal to understand
301:08 - it okay so so these are three types of
301:11 - solutions and and and when we call this
301:14 - when there is a no solution we call that
301:16 - as a inconsistent we we call this as a
301:18 - inconsistent and these two are called
301:21 - the consistent these two are called the
301:24 - consistent okay so I hope this is a two
301:27 - these are diagrams of two equations in a
301:29 - variable and in two variables uh not a
301:32 - big deal to understand it cool so you
301:35 - this now now we are done with it and I
301:38 - think I spoke in just just before no
301:41 - problem again so solving system of
301:43 - equation so now let's let's let's start
301:45 - solving system of equation we had solve
301:48 - on system equation using regular rbra
301:51 - but basically real world you not get a
301:52 - system equation in very very easy mode
301:54 - you have to make you have to understand
301:56 - it and I and I will ask you to solve
301:59 - this system equation just to make sure
302:01 - that you're able to understand
302:03 - everything so let's start with solving
302:05 - uh system of equations so uh uh solving
302:09 - system
302:10 - equation so let's go ahead uh 3x + 2 y =
302:14 - 19 so let's take one let's take one
302:16 - example let's solve this algebraically
302:19 - we'll solve this algebraically okay man
302:22 - manipulation we'll do a lot of
302:23 - manipulation in this so let's take one
302:25 - example and the example States 3x + 2 y
302:29 - 3x + 2 y = 19 and x + y = 8 okay so over
302:36 - here you have this and and what we
302:38 - specifically do we make make that to um
302:41 - I I would say make that y or whatever
302:44 - the variable we to find into one side
302:46 - and other other other stuff in the in
302:48 - the left uh opposite side of that which
302:51 - is in rhs so what I'm going to do is 3x
302:55 - + 2 y = 19 y = 8 - x so I so y = 8 - x
303:02 - so now my trick will be I'll replace the
303:05 - Y over here with 8 - x okay replace
303:10 - y with 8 - x with 8 - x so 3x + 2 8 - x
303:18 - + = 19 and you have y = of course 8 - x
303:23 - okay 8 - x now what I will do I will
303:26 - expand this I will expand this okay so
303:29 - after you expand this 3x plus just write
303:33 - expand okay 3x + 16 - 16 - 2x = 19
303:40 - okay so now you have this y = 8 - x okay
303:45 - so you have expanded it now what you
303:46 - will do you will try to solve this okay
303:49 - you'll try to um try to solve this this
303:51 - one so 3x - 2x which is of course X and
303:54 - we have us left with 16 so which is
303:58 - nothing but x + 16 = 19 and Y = 8 - x so
304:03 - x = 19 - 6 and Y = to 8X and this will
304:08 - be yield yielding to your favorite uh uh
304:12 - 16 okay it's 16 no yeah 16 which is with
304:15 - three and then we we got the value of
304:18 - three we got the value of three now you
304:20 - put the value of three over here so so
304:22 - over here 8 - 3 which is nothing but 5
304:25 - so you found x = 3 and Y = 5 as a
304:29 - solution to this linear equation of the
304:32 - system of equation I hope that this is
304:34 - very very easy not very hard to
304:36 - understand this we are sub this is the
304:38 - method called substit tion solving by
304:41 - substitution okay so we sub substitute
304:44 - the values the so what I did my
304:46 - technique was first of all make X on the
304:49 - one side all the all the variables on
304:51 - one side and other all the things on the
304:53 - opposite side of that variable which is
304:55 - the rhs in this case um and then try try
304:58 - to take out and try to do the algebraic
305:00 - manipulation and then you're done you
305:02 - just find one variable and other
305:04 - variable is in front of you okay so now
305:06 - we have seen these two variables now
305:08 - let's try to solve let solving the
305:09 - system ification of three equations in
305:11 - three variables so we'll be having the
305:13 - three equations in three variables
305:16 - okay so now let's see that how how we
305:19 - are going to proceed Sol further solving
305:21 - that okay so let's let me just make that
305:25 - this this this point and let's see let's
305:27 - let's go further so let's say x + z = to
305:32 - 6 okay X+ Z = 6 Z - 3 y okay Z - 3 y y =
305:41 - 7 and 2x + y + 3 Z = 50 I will try to be
305:49 - as much neat as possible to make sure
305:51 - that everything is going very very fine
305:53 - okay so what I will do what in my hand
305:56 - is to do is to I I'm seeing this x I can
305:59 - make this x equal to 6 - Z okay I can I
306:02 - can do something like that okay so what
306:05 - I'm going to do is I can I can just make
306:08 - that X = to X = to 6 - Z so let's go
306:12 - further x = 6 - Z okay x = 6 - Z now we
306:20 - have - 3 y - 3 y + z okay - 3 plus Z
306:26 - because I'm I I or in for so let's let's
306:30 - assume let's try for for or what I'm
306:32 - going to do is to give you give you a
306:34 - good idea about how to be clean so what
306:36 - what I will do I'll I'll I'll I'll make
306:38 - a separate PES for for all of them so
306:41 - this is for X Y plus Z because we we
306:43 - don't have y in this case so we just
306:46 - leave this space for it which is six
306:47 - okay Z which is z okay so what I'm going
306:51 - to write z + 3 y okay minus 3 y = 7 and
306:57 - we have 2X + Y which is for this plus 3
307:02 - Z = 15 this is called the neat and then
307:06 - you what you go further you go further
307:08 - then what you do you you first of all
307:10 - first of all x equals to Alo sorry I
307:13 - don't have to first of all you have to
307:15 - go further okay X then then you leave
307:18 - all the space for Y and Z okay equals to
307:21 - 6 - Z okay 6 - Z okay so that the
307:25 - equation left alone the the the variable
307:27 - left alone and the and our LHS side okay
307:31 - you now you write the same thing
307:33 - again - 3 y + z = 7 and 2x x + y + 3 Z =
307:43 - 15 okay now what I'm going to do is to
307:46 - substitute the values of X over here now
307:50 - I'm going to do is to substitute the
307:52 - values of X over here so after
307:54 - substituting
307:56 - the so let's sub substitute the values
307:59 - of X over here which will be nothing but
308:03 - uh X then we leave this space equals to
308:06 - 6 - Z - 3 y - - 3 y + z = 7 and 2 6 - Z
308:15 - + y + 3 Z which is nothing but 15 okay
308:19 - now you sub substitute the values of X
308:21 - and if we try to solve it if if you try
308:23 - to solve it uh if if you go ahead and
308:26 - try to solve this we be getting this um
308:29 - if you go ahead and try to solve this uh
308:32 - let's go ahead and solving
308:34 - this uh
308:36 - so uh over here if if if I just just
308:39 - solve it which is nothing but y + z = 3
308:44 - okay just I'm solving this the last one
308:47 - which is the last equation I'm I'll be
308:49 - getting y + z which you can check it out
308:51 - okay so if we are solving this 2x - + y
308:54 - + 3 Z = 15 you're getting 6 - Z okay now
308:59 - now we are left with so now we left with
309:01 - x = 6 - Z okay - 3 y + z = to as usual
309:10 - now we write y + z = 3 okay now because
309:14 - this is the which we have solved okay
309:16 - now we repeat the process again now over
309:19 - here I can just make Z = to y y - 3 yes
309:23 - or no yes or no - 3 y + z = 7 and z = 3
309:29 - - y I can do something like this to make
309:32 - the variable alone so that we can sub
309:33 - substitute it now if you go ahead and
309:36 - substitute it so if you go ahead and put
309:38 - put put the values of Y so x = 6 - z u
309:43 - maybe maybe 3 y + sorry Z values so Z
309:47 - what what we were having 3 - y isn't it
309:50 - we were having or not exactly we're
309:52 - having the Y cut I think 3 - y okay = to
309:57 - 7 = to 7 we go ahead which is but Z = to
310:02 - 3 minus uh uh what do you say you go
310:08 - further okay so you are left with - 4 y
310:11 - - 4 y + 3 okay so you'll be uh and and Z
310:16 - = to 3 - Y which which which you are
310:19 - seeing over here okay now we have this
310:21 - now we have now we have substituted the
310:23 - values now we'll try to solve it so when
310:26 - we solve it when we solving when we
310:28 - solving - 3 y + 3 - y = 7 when you
310:34 - simplifies 2 - 4 y = 4 okay or Y = to -1
310:41 - okay when when you try to solve this
310:42 - linear equation and it's not not a big
310:44 - deal to solve what you going do you can
310:45 - just add you just say okay uh this is uh
310:48 - - 4 y + 3 which is - 4 y = 7 - 3 and
310:53 - left with 4 and when you try to and and
310:56 - and and and then you will be left with
310:57 - minus one okay so so this is this is
311:00 - what you get y = to minus1 okay now you
311:04 - you got the value of one value which is
311:06 - x = 6 - z y = to -1 and z = 3 y 3 - y
311:13 - okay 3 - Y which you're having now we go
311:15 - the values of Y we can substitute the
311:18 - values in z x = 6 - z y = -1 and z = 3 -
311:25 - -1 okay nothing but 4 now we got the
311:28 - values of Z so x = 6 - 4 which is 2 okay
311:32 - so we are done with x = 2 y = -1 and Z =
311:38 - 4 which is is our solution solution to
311:41 - our system of liation in a higher
311:44 - Dimension or in three variables okay so
311:46 - this is a process called sub solving
311:49 - that by substitution and it works nicely
311:51 - okay it works very very nice to every
311:53 - most of the cases but what it's a very
311:56 - long process okay it's a very very long
311:58 - process but I would recommend you to
312:00 - have a hands on on this if you're not
312:02 - able to solve uh the system equation
312:04 - using regular algebra okay so we have
312:07 - Sol using this kind of stuff now let's
312:09 - try to solve the system of equation by
312:11 - elimination so I'm just going to write
312:13 - solving by elimination
312:17 - elimination okay so how are we going
312:19 - bother solving
312:21 - it example 3x + 2 y = 19 and x + y = 8
312:30 - okay so this is your this is going we
312:33 - are going to solve it okay so what do
312:35 - elimination means we are going to either
312:38 - multiply
312:39 - add or subtract the the the equations
312:43 - from each other so we'll see how to do
312:44 - that so let's say so over here what you
312:47 - so we we are going to multiply the
312:48 - equation so we are going to multiply so
312:50 - you can see that over 2 Y is over here
312:52 - so here is also y so we are going to
312:54 - multiply the second equation with the
312:55 - two the second second second equation
312:57 - with the two so we are going to multiply
312:59 - the second equation by two 3x + 2 y = 19
313:03 - and 2x + 2 y = 8 okay so when you go
313:07 - further you you you have this and and
313:09 - what you do and what you do you simply
313:12 - uh have the particular now you subtract
313:14 - the second equation from the first
313:16 - equation you subtract this equation from
313:19 - the first equation okay now now you do
313:21 - the sub subtraction over here
313:22 - subtraction over here so actually you're
313:24 - are eliminating the variables from the
313:25 - first one so 3 3x + 2 y - 2x + 2 y which
313:32 - but 3x + 2 y - 2x - 2 y so it get cut
313:37 - down so we will be left with X = to what
313:42 - 3 okay X = because when you when you do
313:44 - this uh 19 minus uh what are you saying
313:48 - uh uh it's I think it's 19 or it's 16
313:52 - yeah so so this this this will be also
313:54 - it I don't know why I haven't multiply
313:56 - the both both side by two now we have to
313:58 - multiply both side by two so 6 19 - 16
314:02 - will be 3 okay so 9 19 - 6 6 16 will be
314:06 - 3 so now we got the values of X so X =
314:10 - to x = 3 and when you do this 2x + 2 y =
314:15 - 16 put the values of 3 2 * 3 + 2 y = 16
314:20 - the Y value would be 5 and you found x =
314:23 - 3 and Y = 5 this is called the solving
314:26 - by elimination you first of all you
314:28 - multiply the number by two for the
314:30 - second equation and then you subtracted
314:31 - the second equation from the first
314:33 - equation and you're done with solving
314:34 - your linear equation or system of linear
314:37 - equation okay
314:39 - so so this elimination is little bit
314:42 - faster but it needs a neatness it needs
314:44 - you to have good logic and it needs to
314:46 - have a good strategy but um you can
314:48 - solve whatever you like uh in the system
314:50 - of equation you can you can make use of
314:52 - any any of them maybe system of equation
314:54 - using elimination Al manipulation May or
314:57 - maybe substitution it totally makes
314:59 - sense to you okay so thanks for seeing
315:01 - this video I hope that you're able to
315:02 - make sense out of it this this this was
315:04 - the last lecture on lar algebra I'm very
315:06 - very excited to to start with uh
315:09 - calculus okay I'm going to teach the
315:11 - first time Cal calculus and whatever I
315:13 - know about calculus I'm just going to
315:15 - put put that in front of you so that it
315:17 - could recapt youate whatever I had
315:18 - studied as well okay so and it makes
315:21 - sense okay I'm able to teach then I'm
315:23 - able to understand that as well cool so
315:25 - I'll be catching up in the next video
315:26 - till then bye-bye have a great day
315:31 - [Music]
315:40 - hey everyone so let's get started with a
315:42 - new lecture on lecture number seven
315:44 - which is on determinant and this is one
315:46 - of the one of the again I would say
315:48 - important concept to study because in
315:50 - principal compon analysis or whether you
315:53 - uh it it it it comes a lot in your
315:55 - machine Learning Journey as well as well
315:57 - as in deep Learning Journey because it
315:59 - tells you how to solve or solving the
316:02 - linear equations or or or or if I if I
316:06 - talk about in terms of linear
316:07 - transformation it just tells tells you
316:09 - how the how the how the change in area
316:12 - or a volume occurs okay and and
316:15 - determinant is nothing when you it's
316:17 - nothing but you just TR you just give
316:19 - some Matrix and then you get one number
316:21 - so we'll be talking about that in detail
316:23 - in this session uh I I think you you'll
316:26 - get a lot from this session and and you
316:28 - you can make your own notes or the notes
316:30 - is in description un box below either it
316:32 - would be updated soon but yeah uh I it
316:35 - is it is already been made it's just
316:37 - sent for processing and that it will be
316:39 - into your description if you like this
316:41 - video please be sure to subscribe this
316:43 - channel as well as like this video and
316:45 - comment because YouTube algorithm knows
316:47 - okay this is a good video to recommend
316:49 - because many many other the people say
316:50 - uh your channel is underrated so I want
316:52 - you I want this channel to be a rated
316:54 - Channel because I work a lot on this
316:56 - channel Okay cool so let's get started
317:00 - with solving uh what is determinant so
317:03 - we'll we'll get onto geometric meaning
317:05 - soon but uh in in in determinant what
317:08 - you do if you know about a square Matrix
317:11 - if you know about a square Matrix which
317:12 - which we talked about and and I have
317:15 - told that is very important Square
317:17 - Matrix are very important is used
317:19 - extensively in linear algebra to use
317:22 - this term terminology so Square Matrix
317:25 - is nothing where your where your number
317:27 - of a rows is equals to the number of a
317:30 - columns for example uh your Matrix a is
317:34 - is maybe it can be 2 2 22 okay so this
317:37 - is a 2X 2 where n = 2 and M = 2 so n * n
317:42 - Matrix where your Square Matrix is
317:44 - equals 2 where your number of rows is
317:47 - equals to the number of columns okay so
317:49 - that is the so this is so what you do
317:52 - you take your Square Matrix and
317:54 - determinant takes one square Matrix
317:56 - where the number of rows is equals to
317:58 - the number of columns you write
318:00 - determinant of uh an A and A A should be
318:04 - the square Matrix a should be the square
318:07 - Matrix and then you get get one scalar
318:10 - or or or a number as an output when you
318:13 - apply the determinant function or or or
318:15 - when you take out the determinant of
318:17 - that Matrix okay so now how this is
318:21 - useful we will see how do we take out
318:23 - the scalar a just in a second
318:25 - numerically but but U but when you um
318:29 - how how the determinant is useful this
318:32 - is this is one of the most important
318:33 - concept to know so the determinant is
318:35 - useful in in solving and solve linear
318:39 - equation in linear equation it's used
318:43 - very very extensively solving linear
318:45 - equation or maybe it can be useful in
318:48 - and in in in in knowing okay in knowing
318:52 - how linear transformation and knowing
318:55 - how linear how linear transformation
318:59 - transformation change their area or the
319:01 - volume okay change their area
319:04 - transformation change their area change
319:07 - their area
319:09 - over volume or volume okay not over it's
319:12 - or volume and it is also and it is also
319:15 - useful uh in other stuffs like uh when
319:18 - solving some some computationally it it
319:21 - it it it does reduces some comput not
319:23 - exactly me doing efficiently not exactly
319:26 - I would say efficiently I would say very
319:28 - precisely so solving the particular
319:31 - linear equation and is used a lot in
319:33 - that so that's why we take out the
319:35 - determinant of a matrix and that when
319:37 - you take out the determinant of a matrix
319:39 - you simply give a squar matrix root to
319:42 - the determinant and then after when you
319:44 - take out the determinant you will get
319:46 - one scaler okay so this is what the this
319:49 - is this is this is what we use and and
319:51 - if if you talk about um in machine
319:54 - machine learning use case so in machine
319:56 - learning if if you know about machine
319:57 - learning in machine learning you have
320:00 - something called as dimensionality
320:01 - reduction method and and in that you
320:04 - take out the determinant of that
320:06 - co-variance Matrix so co-variance Matrix
320:09 - okay so when you take out the determiner
320:11 - of that covariance Matrix and then you
320:14 - and then and and and and then go further
320:16 - into solving the particular problem okay
320:19 - so not exactly covariance yeah so you
320:21 - take out the termin and then you go
320:22 - further into uh into other stuffs like
320:26 - uh uh the igen vectors and igen values
320:29 - and they are extensively used the
320:31 - determinant are extensively used in the
320:33 - igen vectors and igen values in
320:35 - principal component analysis okay so I
320:38 - hope that this is clear why we use
320:39 - determinant and and and what's the
320:41 - determinant is now now we need to care
320:44 - about how do we take out the scalar
320:46 - value because we give a function because
320:48 - we just give a a square Matrix into that
320:51 - determinant and then we will we are
320:53 - going we are we are just getting a
320:55 - scaler as an output so how do we even do
320:58 - that uh so for for doing that assume
321:01 - that you have a matrix a you have a
321:03 - matrix a which is nothing but 2x two so
321:06 - I'm just going to write um a a b c and d
321:11 - okay so you have a matrix a b c d which
321:13 - is a 2X 2 Matrix so when you take out
321:17 - the determinant of that Matrix a which
321:20 - is nothing but which is nothing but so a
321:23 - d means you take out the product of the
321:25 - diagonals you take out the product of
321:27 - the diagonals a D minus BC a D minus BC
321:33 - so for example you have a matrix uh 2 3
321:36 - 4 6 and and then you want to take out
321:39 - the Matrix the determinant of that
321:41 - Matrix 2x2 matrix which is nothing but 2
321:44 - * 6 2 * 6 - 3 3 3 * 4 3 * 4 which is
321:49 - nothing 6 6 2 12 - 3 4 12 that will be
321:54 - nothing but zero zero is the answer or
321:56 - determinant of this Matrix okay so
321:59 - determinant of a matrix can be zero we
322:01 - have we don't have any conditions but
322:03 - yeah the determinant of this Matrix is
322:05 - zero okay so this is how you take out
322:07 - the determinant of how Matrix
322:09 - geometrically
322:10 - speaking okay so one thing that I want
322:12 - to highlight over here let's say for for
322:15 - example uh what does it mean
322:17 - geometrically what does it mean
322:19 - geometrically so so let's uh let me make
322:23 - one more page so that I could explain
322:25 - you what does it mean geometrically
322:26 - speaking what does it mean geometrically
322:28 - speaking either I could just go on some
322:30 - website to mean to mean what is actually
322:32 - trying to tell so let's go on one
322:35 - website let's go on one website which I
322:37 - want to show you all is this one okay so
322:40 - assume that over here of over here you
322:43 - have let me choose my black color okay
322:46 - here it is so you have um a matrix a
322:50 - matrix a b c d okay you want to take a
322:53 - determinant of this so this this is this
322:56 - is what you take out so for taking out
322:58 - the determinant you just write either in
323:00 - this A B C D giving a pipelines like
323:03 - this okay or or you write determinant of
323:06 - this uh a a matrix and this a matrix is
323:10 - either uh a b c d like this okay so this
323:14 - is the notation for swing that you want
323:16 - to take out the determinant of this
323:18 - Matrix okay that pipeline that big big
323:20 - pipeline okay pipe uh line okay now over
323:24 - here your a is 1 your B is zero your C
323:28 - is zero and your D is 1 okay you want to
323:31 - take out the determinant of this you
323:34 - want to take out the ter determinant of
323:36 - this you want to take out the
323:38 - determinant of this so how do you take
323:40 - out so what does it mean geometrically
323:41 - speaking so geometrically what it's
323:43 - trying to tell is when you plot this
323:45 - Matrix over here first of all you take
323:47 - this and then you go over here so this
323:50 - is nothing but the determinant of a 2x2
323:53 - matrix is the area of a parallelogram
323:56 - with the column vectors AC and BD okay
324:00 - so this is the the determinant is
324:03 - nothing but the area of this
324:07 - parallelogram of this parallelogram
324:09 - where the column vectors are AC and BD
324:13 - okay so when you when you plot the 2x2
324:15 - matrix which is which looks like this
324:17 - and and and this the the the determinant
324:20 - which means geometrically speaking is
324:22 - nothing but area of that parallelogram
324:26 - which formed by joining everything and
324:28 - then and that area of that parallelogram
324:30 - is nothing but determinant of that
324:33 - Matrix okay this is what does it mean
324:35 - geometrically speaking uh I would ask
324:37 - you to watch one video on three blue one
324:40 - brown to see how how he shown
324:42 - geometrically but yeah uh the the det
324:45 - terminal is nothing but the area of that
324:48 - parallelogram whatever forms so for
324:50 - example your par so let me reduce the a
324:53 - a bit and then let me do something with
324:57 - this I don't know what how it is
325:02 - working yeah so let me do something like
325:05 - this and let me increase the area
325:09 - okay let me increase the B okay here it
325:12 - is so when you have the column Vector
325:14 - when you have a column Vector as
325:17 - 0.86 and Z okay and then you have
325:20 - another column Vector which is
325:22 - 0.52 and the the parallel gram is formed
325:25 - is nothing but your favorite the
325:27 - determinant okay so this is what the
325:29 - determinant means and you can play with
325:31 - it by just going to demonstration W
325:33 - frame and this with this website so
325:34 - let's go on the 3D view so how does it
325:37 - look 3D so 3D is nothing but area area
325:42 - of that parallel Zoid okay so if you
325:45 - just see over here the area of the
325:47 - paraloid is is nothing but a determinant
325:49 - we'll see how to solve how how to solve
325:52 - this deter this determinant one okay
325:55 - we'll see how to solve um three for the
325:58 - how to take out the determinant of a 3X3
326:00 - Matrix and we'll also see how to take
326:02 - out the determinant of uh n by n Matrix
326:06 - okay so it's a it's a bit hectic task
326:08 - but we will try to do it so this is this
326:10 - is what the geometrically means and for
326:13 - 2D the area of a parallelogram and for
326:15 - 3D area of a parallel Zoid okay which
326:19 - you can see from the diagrams which are
326:21 - shown over here so if you just if if I
326:23 - could zoom like I can't zoom in but yeah
326:26 - I can just show you this is this this is
326:27 - what you have your uh 3x3 Matrix and
326:31 - then you this is the parallel Zoid which
326:33 - is formed and then when you try to take
326:35 - out the determinant of this is nothing
326:37 - but the area of this paraloid okay so
326:40 - this is what it means and the
326:41 - determinant geometrically is nothing but
326:43 - the area of a parallelogram or paraloid
326:46 - in 3D dimension okay so this is this is
326:48 - what you need in in a geometric
326:50 - intuition just just just to make sure
326:52 - that what the geometrical it it means
326:55 - okay so now let's see now one of one of
326:58 - the important thing which I want to show
326:59 - you up is is is we have seen we have
327:02 - seen how do we take out the determinant
327:05 - of a 2x2 matrix so the determinant so
327:08 - here's your a here is your here's is
327:10 - your a and you have and then you want to
327:12 - take out the determinant of this A B C D
327:17 - and I'm just writing pipe to denote okay
327:19 - this is a determinant so when you take
327:21 - when you try to take out the determinant
327:23 - of this so it's nothing but equals to uh
327:26 - uh a a minus oh my gosh it's a minus BC
327:32 - that's uh then when when you take up
327:34 - that's a uh simple scalar which is e not
327:37 - exactly that not 3+ 3.71 1 it's e okay
327:42 - so let's let's give it any scaler which
327:44 - is e okay so this is this is what it
327:47 - means in 2x2 matrix I'm talking a
327:49 - specifically 2x2 matrix now now let's
327:53 - talk about how do we take out the
327:56 - determinant of uh 3x3 Matrix so
328:00 - determinant
328:01 - determinant determinant of 3x3 Matrix
328:06 - 3x3 Matrix matx so how do we even
328:09 - approach we taking out so you have want
328:11 - to take out the determinant of a b c d e
328:16 - f g h i okay so GH I is this is your
328:23 - Matrix this is the determinant of this
328:26 - Matrix okay so how do you take out how
328:29 - how do you take out the determinant of
328:30 - this Matrix and of course your it should
328:32 - be a one scaler okay it should be one
328:35 - scaler or a number or number okay so how
328:39 - do we take out the determinant so can't
328:41 - we do a * a * e * I and then it will not
328:45 - work this is this is not you can you you
328:48 - can just guess how do we do it just try
328:50 - and comment maybe I can just see and be
328:52 - a bit funny in job so please be sure to
328:55 - write it and I will try to see what you
328:57 - write it okay so so let's start
329:00 - approaching how do we even approach this
329:02 - problem so what we do we simply so so
329:07 - what we do just just make sure that
329:09 - first of all we go to the a11 okay so me
329:12 - first element in that Matrix and then
329:14 - what we do we simply leave this uh
329:18 - column and this row and write a minor
329:21 - Matrix or a submatrix of that of of that
329:25 - uh big Matrix or you can say that we
329:27 - take out the minor of this Matrix how do
329:30 - we take out the minor of this Matrix you
329:32 - simply when for for example you choose
329:34 - this number okay so what you do you you
329:37 - leave this call column and you leave
329:39 - this row and then you write and then
329:41 - what you do you take out the minor and
329:43 - then you take out the determinant the
329:44 - determinant of that by multiplying by a
329:47 - okay so the first element is this and
329:49 - then you have e f h i we left this
329:54 - column and this row and then we write EF
329:56 - hii okay we want to take out the
329:58 - determinant of EF hii okay now what you
330:01 - do now what you do here is your plus
330:03 - sign now it will be a minus sign over
330:05 - here okay you go to the B you leave this
330:09 - column and you leave this row okay which
330:11 - is nothing but B and D FG I DF GI
330:17 - because we left this column this row and
330:21 - this column just d f g i okay and then
330:26 - here is your minus then here will be
330:28 - plus plus uh you write C now we left
330:32 - this column this this this column and
330:34 - the first row which is which will be
330:36 - left the determinant of d e g h okay and
330:40 - then we have convert now these are
330:42 - called the minor or a
330:44 - submatrix submatrix or the minor of our
330:47 - Matrix a these These are called the
330:50 - minor these are called nothing but the
330:53 - minor these are nothing but called the
330:55 - minor minor of our Matrix of our Matrix
330:59 - a okay so when you try to now it is very
331:03 - easy a * a * uh
331:08 - EI now you can just apply your 2 x two a
331:11 - EI and FH EI minus FH okay minus b d FH
331:18 - d i minus FH okay plus C and then you
331:24 - have DH EG okay DH minus EG okay and
331:28 - then you'll be left with some scalar and
331:30 - then you can simply do do this thing and
331:32 - then you simply multiply with this and
331:34 - then you do do some calculation and then
331:36 - you'll be getting your output at has
331:38 - maybe some some scalar some scalar value
331:41 - okay so let's see one of one of the one
331:42 - of the one of the problem or or the
331:45 - stuff to to see how how it looks like
331:49 - Okay so let's let's assume that you have
331:51 - a a matrix or 3x3 Matrix so here's a
331:54 - question for you okay maybe you can try
331:56 - try to approach it uh the you want to
331:59 - take the determinant of I'm writing this
332:02 - pipe that denotes that you want to take
332:03 - out the determinant of that uh for
332:06 - example 0 1 1 2 uh 1 2
332:11 - 0 uh let's let's write 1 1 Z okay just a
332:14 - random random I'll be walking you
332:16 - through it so take out the determinant
332:18 - of this this is a 3X3 Matrix try to take
332:21 - out the determinant of this so how do
332:23 - you take out so first of all we go to
332:25 - the first element and then what we do we
332:27 - take out the minor of this Matrix so the
332:29 - minor so we leave this column and this
332:31 - row so we'll be left with
332:33 - zero and then we and then we write out
332:36 - minor and then we take out the
332:37 - determinant of our sub Matrix okay
332:41 - plus now no no no it will be not plus
332:43 - over here it will be minus because here
332:45 - is our plus minus okay one you leave
332:48 - this column and this row which is 1 0 1
332:51 - 0 okay so 1 1 0 1 0 okay and then you
332:56 - write plus 2 and then you have uh you
333:00 - leave one 2 1 1 okay you leave this
333:02 - column and this row okay you leave this
333:04 - column and this row you'll be left with
333:06 - 1 2 1 one
333:08 - okay and then you do the sum and then
333:10 - you do the sum so and then you take and
333:12 - then what you do you try to take out
333:13 - zero 2 * 0 which is 0 - 0 okay - uh 1 *
333:21 - 1 * 0 of course 0 and 1 * 0 0 okay plus
333:27 - 2 uh 2 okay 1 * 1 1 2 * 1 two okay then
333:33 - you'll be left with of course zero then
333:36 - it will be done then it done
333:38 - it will be also 1 * 0 which is nothing
333:41 - but Z okay we'll be leftt with 2 * - 2 2
333:45 - * -2 that that will be
333:48 - -4 okay which will which is your
333:51 - determinant of this Matrix so -4 is your
333:54 - determinant of this Matrix which you are
333:58 - seeing over here so sorry here is 1 - 2
334:01 - it's not it's it's simply -1 so 2 * -2
334:05 - is the determinant of this Matrix so
334:08 - I'll be so here you got the determinant
334:10 - of this Matrix which is nothing but
334:12 - min-2 okay so this is this is how you
334:15 - take out the determinant of a 3X3 Matrix
334:17 - as well so there are there are some
334:19 - problem for you to work on so I'm just
334:21 - just going to write it out so there's
334:23 - one problem which which which you can
334:25 - approach okay so you want to take out
334:27 - the determinant
334:29 - of uh 371 -4 please answer the HW please
334:34 - answer in the comment box it's just a
334:36 - quiz which you will see in your
334:38 - attendance as well okay so this is what
334:40 - the determinant of that Matrix of the
334:41 - 2x2 matrix or 3x3 Matrix we'll try to
334:44 - solve the determinant of a 3X3 Matrix
334:47 - using lianes formula or the rule of
334:50 - surus okay so it's is very good formula
334:53 - to work on so we'll see that but before
334:55 - that I want to highlight some of the
334:56 - properties some of the properties of
334:58 - that
334:59 - properties properties of determinant
335:02 - Matrix of determinant of a matrix
335:05 - determinant of a matrix the first first
335:08 - one is the first one is the first one is
335:12 - for example you want to take over the
335:13 - determinant of this Matrix for example
335:15 - you want to take out the determinant of
335:17 - 1 z uh 0 1 okay so try try try to take
335:21 - out the determinant of this 1 * one
335:24 - which is one uh and then and then minus
335:27 - Z okay what what it will it is one and
335:31 - can you identify this is an identity
335:33 - Matrix even if you have uh 3x3 identity
335:37 - Matrix then that will be nothing but
335:39 - that will be nothing but one so whenever
335:42 - you have identity Matrix whenever you
335:44 - have if if if it is if it is identity
335:49 - Matrix if it is identity Matrix identity
335:53 - Matrix then then then the then the then
335:57 - the determinant of that identity Matrix
335:59 - will be one okay this is this is one of
336:01 - the property second property if the if
336:04 - the rules are the same okay so for for
336:07 - example
336:09 - if the rows are the same are the same
336:13 - for for example a a b b okay a a b b
336:18 - then AB minus ba a will be nothing but
336:22 - zero okay so this is another property
336:25 - third property is you have a scaler
336:27 - multiplied with some uh a and you have
336:30 - another scal c and b d okay so what it
336:33 - will be it it just makes sense r a D
336:37 - minus R CD or r r CB okay you can write
336:42 - write it down like this and then it's
336:45 - it's not it's nothing you just you just
336:47 - take that out of out of okay you just uh
336:51 - take take that as a common r a D minus
336:55 - BC okay so we can write this as a we can
336:58 - write this we can write this as a r * a
337:03 - b c
337:05 - d either we can write it now so we
337:07 - proved it so it is just equivalent you
337:10 - can write this so for it will be easily
337:12 - for us to solve okay so it is so it is R
337:15 - times as either it is same as over here
337:18 - so we a D minus BC so it's just
337:21 - equivalent to that so this is another
337:23 - property which you see a lot in while
337:25 - taking out the determinant of a matrix
337:28 - okay so this is these are the some of
337:30 - some of the properties which I want to
337:31 - highlight in front of you so now let's
337:33 - go on to the another stuff is how do we
337:36 - take out the deter DET minant how do we
337:38 - even bother taking of the
337:40 - determinant determinant
337:43 - determinant of 3x3 Matrix so you can
337:47 - just say okay I'm just going to just
337:49 - going to take out the minor of the sub
337:50 - Matrix of the Matrix and then I will do
337:52 - that so here's another another trick
337:55 - which is called the rule of suus I think
337:57 - it's it's just not a I would say uh okay
338:01 - it's a good technique but okay you can
338:04 - try it out but eventually I like that my
338:07 - approach but yeah it is very very
338:09 - straightforward approach which I'm going
338:11 - to tell over here okay so assume that
338:15 - you have a matrix M and that you have a
338:17 - matrix M okay so a11 a12 a13 okay a21
338:25 - a22 a23 a23 a31 a32 a33 okay so you have
338:34 - this 3x3 Matrix now when you wanted to
338:36 - take out the determinant determinant of
338:39 - this Matrix M so how do we even bother
338:42 - doing that so for for for for so you can
338:45 - use the rule of suus rule of suus I
338:49 - think the funny name he has but yeah
338:52 - again I'm no no one to comment on on his
338:54 - name he's a again a great people okay so
338:57 - not I'm not even a one 1% of these
338:59 - people so these are amazing people who
339:01 - give a lot to the world so I think about
339:03 - I'm no one to say about but yeah amazing
339:05 - name so what you do you take out the
339:08 - determinant so you want to take out the
339:09 - determinant of a11 a12 a13 okay so this
339:13 - Matrix I'm just writing this Matrix a21
339:17 - a22 a23 okay a31 a32 a33 okay and then
339:25 - what you do you take out the first two
339:27 - column and write it in another format
339:29 - like this a11 a12 A2 1 this is a trick
339:33 - for solving a 3X3 Matrix a22 and a 31
339:38 - okay so a31 and a32 so this is
339:41 - a22 okay it say a21 okay so this is what
339:45 - you now you write this now what you do
339:47 - now what you do so what you do you you
339:51 - simply take out the product you simply
339:54 - take out the product like this the first
339:56 - diagonal okay so this is the diagonal so
339:59 - what you do
340:00 - a11 8 22 a33 okay a11 a22 a33 plus plus
340:09 - a12 a22 A2 3 okay A2 3 and a31 so what
340:14 - you do you take out the product of these
340:16 - three you take the product of these
340:17 - three so a12 a uh uh 2 3 okay A 2 3 uh
340:23 - and a 31 okay now what you do you simply
340:27 - uh do this simply multiply the
340:31 - next next diagonally okay so plus plus
340:36 - a13 a a 2 1 okay I'm I'm I'm I'm doing a
340:39 - bit messy so let me do
340:42 - a31 uh if I'm not wrong a13 a a a a13
340:47 - a21 and a32 okay A a a13 a21 a32 now you
340:54 - are done with this now what you do now
340:56 - what you do you now go from bottom to
341:00 - top here you are going from top to
341:02 - bottom now you'll go from bottom to top
341:04 - by changing the sign now okay now you go
341:06 - from bottom to top so here's how you do
341:10 - here's how you go further okay so so the
341:13 - way you go is you have a31 so from here
341:18 - so from here a31 okay and then a22 and
341:23 - then you go to a a13 so now you start
341:25 - going at this side like like this okay
341:28 - so
341:30 - a31 I'm just going to write a31 a22 a22
341:36 - and then a13 3 a13 okay and then what
341:40 - you do and then plus no uh you you minus
341:43 - because you change you go from bottom to
341:45 - top so here you minus it now now you go
341:48 - at this one now you do this and then a32
341:52 - okay this one this one and this
341:56 - one a32 *
341:59 - a23 yeah if I'm it's a it's a a23 if I'm
342:04 - not wrong yeah a23 and a a11 a11 okay
342:09 - now minus now this is done now you go at
342:12 - last one which is this one
342:15 - a33 a21 a12 okay and here's how you take
342:21 - out the determinant of a matrix using
342:23 - suus rule okay or a rule of suus okay so
342:27 - and then you'll be getting after after
342:28 - doing this all those stuffs you can just
342:30 - cancel it out something if it is so you
342:32 - just you just come do do the computation
342:34 - then here's your Matrix and this just a
342:36 - scaler number or other stuffs so here's
342:39 - the rule of suru so here's how you do
342:41 - you do simply you do you do write the
342:42 - first two column uh at at the side so
342:46 - that it could be easily so uh so what
342:48 - you do you simply take of the product
342:50 - from top to bottom for the first three
342:52 - and then you take out the bottom to top
342:54 - for the second three starting from the
342:56 - last okay so here's here's here's what
342:59 - the full the rule of suus means here
343:01 - here's how you take out the determinant
343:03 - of that Matrix like this okay so now I'm
343:06 - going to talk talk about is uh you can
343:08 - see the Wikipedia Pages for a libanese
343:11 - rule because they write a very very kind
343:13 - of libanese stuff so you can just go
343:14 - there and see more see more about this
343:16 - rule okay so the next thing which I'm
343:18 - going to talk about is the next thing
343:21 - which I'm going to talk about is how do
343:23 - we take out the determinant how do we
343:25 - take out mean bother of taking out the
343:28 - determinant of n by n Matrix the
343:32 - determinant of n by n Matrix so how do
343:35 - we even take out that and how do even
343:37 - bother taking out that okay so here's so
343:41 - I'm just going to write the N by n
343:43 - Matrix I I'm just going to write the N
343:45 - byn Matrix or let's start with a
343:48 - particular example let's start with a
343:50 - particular example so it would to
343:53 - totally makes sense okay so let's let's
343:55 - start with a particular example and then
343:57 - at at last we'll just write a definition
343:59 - and then we'll end this video okay the
344:02 - example is bit long so I'm just going to
344:03 - maintain my handwriting so the example
344:06 - is
344:07 - and you want to take out the determinant
344:10 - you want to take out the determinant of
344:12 - a 4x4 Matrix 1 2 3 oh my gosh 3 4 okay 6
344:19 - 6 9 2 1 and then we have a 4 9 2 1 and
344:26 - then you have a 0 1 1 1 okay so here's
344:29 - here's your determinant of this Matrix
344:31 - so to take out the determinant of this
344:33 - Matrix so how do we even approach taking
344:35 - out the determinant of this Matrix so
344:37 - how do we take out the determinant of
344:39 - this so for taking out the determinant
344:40 - of this so for taking out the
344:42 - determinant of this for for taking out
344:44 - the determinant of this you take out you
344:47 - take out you first of all take out the
344:50 - minor or the submatrix of this okay so
344:53 - you you go to the First Column first
344:54 - element and then you leave this column
344:57 - and this row and then write the sub
344:59 - Matrix so you just one and then you take
345:01 - out the determinant of 9 2 1 9 2 1
345:07 - 1 1 1 okay this is plus sign so it will
345:10 - be minus sign now you go at this take
345:13 - take of the sub Matrix leaving this row
345:15 - this column and this row so it would be
345:18 - two first of all you take product it so
345:20 - you multiply with that two two times uh
345:24 - the determinant of 6 2 1 6 2 1 4 2 1 and
345:29 - 01 1 okay and then you uh change the
345:32 - Sign Plus and then go through with three
345:35 - and then you leave this column and this
345:37 - so it will be nothing but uh three and
345:40 - then you have 621 I also just write okay
345:43 - 66691 I I just leave it so I'm three 691
345:48 - 691 uh 491 491 and 01 1 okay 0 1
345:55 - 1 now the last one is there four okay so
345:59 - there is four and then you take out the
346:01 - terminant of leaving all the all the uh
346:04 - column one and then row 692 2 692
346:09 - 492 and uh okay I think it's wrong
346:13 - 492 and 01 1 okay these are the sub
346:17 - Matrix of that Matrix let's name it as a
346:19 - m okay so this is a matrix M and then
346:22 - you have to take a determinat of that
346:23 - Matrix M so here's the 4 4x4 now you do
346:26 - this now you have this now what you do
346:28 - now here you convert it to 3x3
346:30 - determinant now what you do you convert
346:31 - that to a 2X two here's how you do so
346:34 - you you don't want to use a s rule
346:37 - because I I eventually don't like that
346:39 - rule it's very hectic rule sometimes it
346:41 - maybe cause you error but no problem in
346:43 - that so here's how you do it so first of
346:46 - all what you do so first of all what you
346:48 - do you simply uh multiply uh one okay
346:53 - you simply go ahead and take take your
346:57 - uh one as a so if you can see I just
346:59 - want to take that one as an uh uh this
347:02 - one and then what I and then I go and
347:04 - approaching this so here is your nine so
347:07 - first first of all go at this element
347:09 - take out nine we want to take out the
347:10 - sub Matrix of this Matrix so 9 and then
347:13 - you take out the determinant of 2 1 1 1
347:15 - so what how this came 2 1 uh you leave
347:19 - this this row and this column 2 1 1 1
347:23 - okay now you simply change the sign
347:25 - minus okay you make sure that that
347:28 - you're doing only doing for this you're
347:29 - only doing for this we'll come to this
347:32 - later on but we are only doing for for
347:33 - this a21 okay minus now now we go to
347:37 - this two now we go this two we leave
347:39 - this column and this row which is 911 1
347:42 - okay so which is aray what happened yeah
347:45 - so which is nothing but uh what do you
347:48 - say uh two because here is our two
347:51 - leaving this row this this this column
347:53 - and this row
347:54 - 9111 okay uh the determinant of 91 1 1
348:00 - okay and then what you do plus now you
348:03 - change the sign and then you go at last
348:05 - one leaving this 9 21 1 okay so the 1
348:09 - and the
348:10 - 9211 okay so you take out that okay now
348:13 - this was plus now you make it minus okay
348:17 - now here is your two so I will take that
348:19 - outside I will take take that outside
348:22 - okay and then I will just go ahead into
348:24 - solving this so you take this take this
348:27 - as a now you take out the minor of this
348:30 - Matrix or the sub Matrix of this Matrix
348:32 - so here's how you do it so you simply
348:34 - add it minus so here is minus is 6 uh 21
348:39 - 1 1 so here here's how you go with this
348:42 - you ignore this column and this row 211
348:45 - 1 now you go to this you ignore this 41
348:48 - 0 41 01 and then you go over here ignore
348:51 - this 42 01 okay so this is how I'm I'm
348:55 - I'm going to write minus 2 okay because
348:57 - you go over here two over minus 2 CH
349:00 - changing the sign take out the
349:02 - determinant of 41 0 1 4 1 0 1 okay and
349:07 - then you simply plus u now you change
349:09 - the sign you go one go to go to 1 42 0 1
349:12 - 42 0 1 uh + 1 uh 4 2 0 1 okay and then
349:19 - what do you do and oh my gosh yeah so
349:21 - then what do you do now you now you
349:23 - converted that 3x3 Matrix for this one
349:25 - and for this one now you go to this one
349:28 - okay by changing the sign plus plus and
349:31 - then you go and then you write separate
349:33 - three now you write separate three and
349:35 - then you take out the first one six okay
349:37 - so you leave this column and this row
349:40 - which is nothing but six and then 9111
349:43 - which is the sub Matrix of that Matrix -
349:46 - 9 because this plus 9 and how how how 9
349:49 - came you go with this column leaving
349:52 - this column and leaving this row 41 01
349:55 - which is nothing but 41 01 okay plus
349:59 - changing the sign 1
350:02 - 4901 how this 4901 came is you have this
350:06 - you leave this and this you leave this
350:08 - column and this the row which is 4901
350:10 - okay so this is how you came it and then
350:12 - and then you're done okay now what do
350:14 - you do you you do for the last one you
350:17 - do the for the last one this because you
350:19 - you done for this you're done for this
350:20 - you're done for this now you converted
350:21 - that to a 2X 2 m which is easily deter
350:24 - which which we can easily take out the
350:25 - determinant now you go to this okay so
350:29 - here's how you do it so - 4 okay and
350:32 - then what do you do and then what do you
350:33 - do you leave this column and this row
350:36 - taking know the first element so
350:38 - six the determinant of I was I think
350:42 - it's it was 9 9211 9211
350:47 - 9211 minus minus 9 okay so over here it
350:52 - was living this the SE going to this and
350:54 - leaving this column and this row which
350:56 - is 4201 I I I think about it yeah that's
350:59 - 4201
351:01 - 4201 and then you change the sign plus 2
351:04 - 49 I think it's it's it's more about you
351:08 - leave you go over here 4901 okay that is
351:12 - 4901 okay now you're are done now this
351:15 - is what you have written so you
351:17 - converted the first Matrix the first
351:20 - Matrix this one this one and this one as
351:22 - well uh into a minor Matrix which is 2x2
351:24 - determinant so you can easy take out and
351:26 - then do the product and then you take
351:27 - out okay so let's do over here if if if
351:30 - I have a chance to do over here but no
351:32 - work problem I will do over here okay so
351:35 - here's how you do it here's how you do
351:37 - it so for doing it first of all you have
351:39 - the one available which is over here you
351:41 - have the one available which is over
351:43 - here what do you do you simply 9 - 16 +
351:48 - 7 how how how we came so you have the
351:51 - particularly 9 times because of course
351:54 - you want to always want to uh multiply
351:56 - it out okay so I think about this is you
352:00 - have uh if if you go over here 2 * 1
352:04 - okay and 1 * 1 so 2 2 2 * 1 how much 2 *
352:09 - 1 how much it would be uh 1 - 1 I would
352:14 - say uh 2 - 1 which is 1 so it will be 9
352:18 - okay minus minus over here uh 9 9 * 1
352:23 - which is 9 - 1 which is 8 16 - 16 done
352:28 - and then you have 9 * 1 and then minus 2
352:31 - * 1 so 9 - 2 which is 7 okay so here's
352:35 - how it came okay and then you then this
352:37 - the left minus 2 now you go on second
352:40 - one two over here so when you when you
352:42 - take a 2 * 1 how much 2 * 1 how much 2 *
352:45 - 1 2 - 1 okay that is 6 over here so we
352:50 - write uh 6 - 8 + 4 okay so here's how
352:55 - you do so it is 6us 4 * 1 of course 4 *
352:59 - 2 it's 0 * 1 of course zero so 4 * 1 4 *
353:03 - -2 which is - 8 which which you have
353:05 - written over here okay then you go over
353:07 - here 4 * 1 how much 4 * 1 4 and then you
353:11 - four so here here is a plus 4 now now
353:14 - you go the next + three because you go
353:17 - over here now 9 * 1 how much 9 * 1 how
353:21 - much uh 9 * 1 9 of course - 1 uh which
353:25 - is nothing but 8 8 * 6 48 so you have 48
353:30 - - 36 + 4 okay so here's our 48 - 9 uh so
353:36 - 4 * 1 4 so 4 9 4 9 36 because this 2 * 0
353:42 - is 0 then you go over here then you have
353:44 - this 9 9 9 * 0 is of course 0 4 * 1 so 4
353:48 - 2 8 so over here uh I think I'm wrong
353:51 - over here um you have this 4 * 1 4 910 0
353:55 - so four uh it's it's it's it's four okay
354:00 - so it's four it's it it it it it should
354:03 - be eight okay it should be eight which
354:05 - is nothing but what you do you 4 * 1 4
354:08 - and then you simply multiply with two
354:10 - which is nothing but 8 okay now you
354:13 - simply
354:14 - minus- 4 - 4 uh 28 - 36 + 8 so how you
354:21 - take out 9 * 1 how much 9 - 2 7 7 7 6 7
354:27 - 7 6 how much uh oops it's it's uh it
354:31 - should be 9 * 1 - 2 9 8 7 7 6 14 42 okay
354:37 - so which will be nothing but 42 what the
354:39 - hell i' written over here so I have to
354:42 - just couple it out so I have to just
354:44 - return it it will be nothing but
354:47 - 42 okay minus minus minus uh minus 4 * 1
354:54 - of course 4 9 are 36 plus 2 4 4 1's 4 4
354:59 - 2 are 8 okay so here's how you do it now
355:01 - you simply multiply with this and then
355:04 - first of all do the calculation do the
355:06 - the calculation then you'll be getting
355:07 - one a scaler as your output so please
355:09 - feel free to put your answer in the
355:11 - description box below I know the answer
355:13 - but yeah I want to leave it to you to do
355:15 - the rest of the calculation because I
355:17 - have done a lot so here's how you take
355:19 - out the determinant of n byn Matrix and
355:21 - how you do it and how you do it's very
355:23 - very easy you just uh keep converting
355:26 - that to a lower or a sub Matrix and then
355:28 - you and then after that you are done
355:30 - okay so here's how you do it so you you
355:33 - define one you define one submatrix a J
355:36 - you define one sub Matrix a i j which is
355:39 - nothing but the Matrix the n n - 1 * n-
355:44 - one Matrix n minus1 n minus1 Matrix if
355:48 - you ignore the I row and I column if you
355:51 - ignore if you ignore the I row which
355:55 - which you were doing I row and J column
355:59 - which which you were doing okay that is
356:01 - your new Matrix which we which we were
356:03 - forming that is a recursive this is a
356:05 - recursive you can write a Python program
356:07 - for write a recursive solution for this
356:10 - okay so you were writing A1 one and then
356:14 - you were adding the determinant of that
356:16 - submatrix and then you are doing so and
356:18 - so on so on that is a recursive solution
356:20 - so you are writing the recursive
356:23 - recursive stuff so we we we already
356:25 - written a lot so I hope that you
356:27 - understood for formal definition which
356:28 - you can see yeah we have gone through
356:30 - one of one of the example which is very
356:31 - very much important for us to know okay
356:35 - so I hope that you will uh get a lot
356:37 - from this video and determine it I hope
356:39 - that concept is clear with my examples
356:42 - and I also hope that you enjoyed this
356:43 - video I think I have to wrap up with
356:45 - wrap up with this video I'll be catching
356:47 - up your next video till then bye-bye
356:48 - have a great day meet you in the next
356:53 - [Music]
357:01 - lecture hey everyone welcome to this
357:04 - first lecture on single variable
357:06 - calculus from this video I know it's bit
357:09 - 6 days till the video is not being
357:11 - uploaded it's because uh I was I was
357:14 - having some examination that I will that
357:16 - I Was preparing for and I Was preparing
357:18 - for the content for single variable
357:20 - calculus so in this series of lecture
357:23 - we'll be talking about single variable
357:25 - Cal calculus it's U I would say will
357:28 - will come to calculus but uh we had a
357:31 - talk on on linear algebra and some of
357:34 - the videos like I vector and I values to
357:38 - be added okay so that is to be added and
357:41 - that is sent into that is we are making
357:44 - some good examples so that everyone
357:47 - could be could could get some uh fam
357:50 - familiarity with the igen vectors and I
357:52 - values so we always want to deliver you
357:54 - the quality content not the in a fast
357:57 - way or whatever the cont so that
357:59 - everyone could understand igen vectors
358:01 - and igen values that is left and soon it
358:03 - will be added so over here now we'll be
358:06 - starting off with single variable
358:08 - calculus I don't know what happened just
358:10 - in front of my screen but that's a my
358:11 - SQL update no problem just just just for
358:15 - those who wants to know so we'll be
358:17 - starting with the lecture number one uh
358:19 - which is a single variable calculus and
358:21 - this VAR I'll be talking about uh I'll
358:23 - giving you an introduction to calculus
358:27 - and what are the content which you're
358:29 - going to cover okay and what is the next
358:33 - series of videos in this ml2 okay and
358:36 - from now on the video lectures will be
358:39 - around 3 to four videos per week now my
358:41 - everything has got over I'll be focusing
358:44 - on the completing this series as soon as
358:46 - possible cool so uh so in this video
358:50 - we'll talking about that uh friendly
358:52 - introduction to calculus I have prepared
358:53 - my own lecture notes so that I could not
358:55 - miss any topics the first thing which I
358:58 - talk about is giving you what is
359:00 - calculus many people think there is some
359:02 - misconceptions about that I will I will
359:04 - say you uh I will I will just I will
359:07 - just give you some examples a geometric
359:09 - examples of what calculus is and what
359:12 - calculus is not okay uh what are the
359:15 - problems that can be solved using
359:17 - calculus what are the problems that
359:19 - cannot be solved using Cal that that
359:21 - does not require calculus okay we'll
359:24 - talk about two big ideas in calculus
359:26 - such as differentiation and integration
359:29 - now you may think here you you're going
359:30 - to start with differentiation
359:32 - integration don't worry my friend it's
359:34 - very very very I will say I will just
359:36 - give you an I I would say a ta a trailer
359:41 - means 0.1% what is going to come in
359:44 - differentiation and what is going to
359:45 - come in integration okay the geometric
359:48 - view only okay so then then we'll end
359:51 - this lecture talking talking about this
359:53 - lecture now what will be the content
359:55 - which will be in this lect uh single
359:57 - variable calculus course so first of all
360:00 - I don't want anyone to be missed for
360:02 - example many uh high school students
360:05 - watch watch this video so we are going
360:07 - to start with the pre-calculus we are
360:09 - going to start with a pre-calculus so I
360:11 - hope that everyone will be comfortable
360:13 - with the pre-calculus I'll be not
360:15 - covering logarithms I'll be not covering
360:18 - basic algebra because I require you to
360:21 - have a knowledge till class 10th math
360:23 - and uh I hope so you'll be having that
360:26 - like logarithms like uh I'll be talking
360:29 - about trigonometry as well and don't
360:31 - worry about that so I want logarithms
360:33 - and basic algebra like factoring GC F
360:36 - and a lot more so these are the algebra
360:38 - 1 and Algebra 2 which are required like
360:40 - exponential functions as well so uh in
360:44 - pre-calculus we'll be talking about the
360:46 - two two main Concepts which will be not
360:49 - two main Concepts I'm talking about
360:51 - functions talking about functions mainly
360:54 - the
360:55 - functions functions and we're talking
360:58 - about uh the second one is uh we'll be
361:00 - after after talking about functions
361:02 - we'll be talking about trigonometry
361:05 - Trigon
361:06 - metry okay so just review these two
361:09 - topics from a scratch so that everyone
361:12 - could be familiar it with what's going
361:14 - on and the and and in the further
361:17 - lecture so we'll be covering
361:18 - pre-calculus in two videos okay so we'll
361:21 - be talking about calculus then we'll
361:22 - going then then we'll talk about limits
361:25 - okay and after talking about limits
361:26 - we'll talk about differentiation solve
361:30 - taking out the derivative some of the
361:33 - power rule power rule then the there are
361:36 - lot lot lot of the rules which which we
361:38 - which are going to learn for taking out
361:40 - the derivative we'll talk about
361:41 - integration
361:43 - solving taking taking on integral for a
361:46 - function and then in infinite series
361:50 - okay so these are the things which will
361:51 - be taught uh in this single variable
361:53 - calculus Series so let's get started
361:56 - with this video so basically in this
361:58 - video we'll Define what is calculus
362:00 - anyone could Define me what is calculus
362:02 - just go in Des description box and tell
362:05 - me
362:06 - what what
362:09 - is
362:11 - calculus what is calculus so anyone
362:14 - could give it a try so calculus is is is
362:18 - is is is you you may think it's a
362:20 - different subject it's not a different
362:23 - subject it's not a something oh my God
362:26 - what is this a rocket science it's a
362:29 - it's a very Advanced version for your
362:32 - algebra and geometric problem okay so
362:36 - calculus I think is a Zach Newton and
362:38 - other the great great physicist
362:40 - mathematicians invented it for solving
362:43 - the some of the great problems so
362:45 - calculus is nothing but Advanced version
362:49 - of your regular algebra Advanced version
362:54 - Advanced version of your regular algebra
362:56 - which you see
362:58 - regular regular algebra and geometry and
363:03 - geometry which you see in real world
363:06 - okay so we'll we'll see some examples so
363:08 - what do you mean with this we'll I will
363:10 - give you a formal definition of calculus
363:12 - when we go further in the course the
363:14 - reason why I'm don't want to give you
363:16 - the formal definition to so that you
363:19 - canot confuse you could not confuse with
363:21 - a definition because I want to be very
363:23 - very clear in the first
363:25 - lecture so I'm going to I want to take
363:28 - some example of it okay so let's take
363:32 - let's take an example let's take one one
363:34 - example let's take one example so
363:37 - example can be example can be let's say
363:41 - that you want to find that you want to
363:43 - find the slope of this line slope of
363:48 - this line so slope of this line can be
363:51 - fin
363:52 - over
363:54 - uh like like this so you run so this is
363:58 - called run and this called rise okay
364:08 - rise okay the slope can be found using a
364:11 - reg regular math of the straight line by
364:14 - the slope can be can be regarded as a
364:17 - rise over run rise over run rise over
364:21 - run okay or Y2 - y1 / X2 - X1 okay so it
364:29 - tells uh it it just tells how much the Y
364:32 - changes when X changes okay that's how
364:35 - we this uh this is this is telling which
364:37 - will come to that we'll talk about slope
364:39 - briefly in our future videos but over
364:42 - here you can calculate the slope of this
364:45 - line by simply your regular math problem
364:48 - your reg regular math problem isn't it
364:51 - you're using a regular regular
364:55 - mathematics okay mathematics but you
364:58 - told but you're told to find maybe you
365:02 - you select this
365:03 - point and want to find the slope at this
365:06 - point and at every point in this for
365:09 - example over here the slope will be same
365:11 - for this line so over here if you to
365:13 - find the slope at this point you can
365:15 - take another point and then take out the
365:17 - slope for example it goes one uh one at
365:19 - the rise and goes up at the top so that
365:22 - will be slope for every point the slope
365:24 - will be of same okay so so this is a
365:27 - this is this is you can calculate so for
365:29 - every point in that line the slope will
365:31 - be the same now coming to the another
365:34 - this is this is your first first figure
365:36 - this is your first figure number one
365:38 - okay so this you can calculate the slope
365:40 - of this line using your regular math
365:42 - which you can see over here now I if I
365:45 - if I just tell you if I just give you a
365:49 - a line so let me just uh oh my God where
365:52 - is this okay so this is
365:57 - your this is your thing and and you have
366:01 - me just draw a little bit big
366:04 - line uh
366:06 - something like this you want to
366:09 - calculate you want to calculate the
366:12 - slope of this curve line I'm to
366:15 - calculate the slope of this curve line
366:18 - you cannot use you cannot use slope over
366:21 - here slope over here will be not used
366:24 - your regular math your regular math is
366:27 - can cannot be used because every time
366:29 - the slope is changing every time the
366:31 - slope is changing every time the slope
366:34 - will is changing so so you cannot use
366:37 - your regular mathematics over here okay
366:40 - you may think I will just take out the
366:43 - slope and then take out the average but
366:45 - what if I tell you take out the exact
366:47 - slope okay so that it could be same at
366:50 - every
366:51 - Point how do we do that how do we even
366:53 - do that so for example if I try do it
366:56 - over here okay I try to I try to take
366:59 - take out the slope at this point you may
367:01 - think okay I'm going that side but it's
367:03 - but isn't this changing if I go over
367:05 - here here this the the the the slope is
367:08 - changing the slope is changing so what
367:10 - if I to say you to calculate the slope
367:14 - of this line you'll be not able to using
367:16 - reg you'll be not able to calculate
367:18 - using regular
367:19 - algebra okay so it is solved so
367:24 - mathematicians has comes up comes C up
367:26 - with
367:27 - Calculus they have came with Calculus so
367:30 - calculus using calculus you can take out
367:33 - that slope on on on on any particular
367:37 - point in that curve okay so using
367:41 - calculus you can calculate the slope of
367:46 - uh what slope of a maybe curve okay so
367:52 - using calculus there's topical
367:53 - differentiation which will come to that
367:55 - so using you can calculate the slope of
367:58 - a curve okay so that's that's what the
368:01 - one of the calculus problem is okay so
368:04 - uh you can see that I differentiated the
368:06 - regular math problem with a calculus
368:09 - problem this is a calculus problem this
368:11 - is a regular math problem now let's see
368:13 - you have a one you have a one Tower okay
368:18 - so assume I'm taking another example I'm
368:21 - taking another example okay I'm taking
368:23 - another example over here let's let's
368:26 - say let's for for for the sake of an
368:28 - example let's say that you have a tower
368:32 - over here so assume this is a tower of
368:34 - yours okay so this is a tower I'm adding
368:37 - T1 which Tower number one and this is
368:39 - your Tower number two okay Tower number
368:43 - two now you need to calculate now this
368:46 - is your straight line okay this is your
368:50 - straight line cool so you can calculate
368:54 - the length of this using your regular
368:56 - math okay using your regular mathematics
369:01 - problem the length of this can be
369:03 - calculated using a regular math problem
369:06 - but what if I give you there's one Tower
369:10 - there is one Tower and there's another
369:12 - Tower and the wire is something like
369:15 - this the wire is something like this and
369:18 - then and you're given given this stuff C
369:21 - calculate the length you're given the
369:23 - specific stuff and you calculate the
369:24 - length how are you going to calculate it
369:27 - you cannot calculate using regular
369:31 - mathematics whatever you have in your
369:33 - toolbox so there the calculus comes
369:37 - there the calculus comes for calculating
369:41 - the the the slope of a curvy line okay
369:45 - so this is there's the calculus plays an
369:47 - important role so let's example you want
369:50 - to calculate the slope of of of course
369:52 - over here the slope will be zero but you
369:54 - told to calculate the slope over of this
369:57 - it will be you'll be not able to
369:59 - calculate using a regular problem here
370:01 - you need calculus okay so these are some
370:03 - of the instances where the calculus are
370:06 - being used extensively in real world
370:09 - examples okay so there are two big ideas
370:12 - in calculus which I want to highlight
370:14 - there are two big ideas in calculus
370:16 - which I want to highlight the first the
370:19 - ideas in calculus let let me write what
370:21 - are the ideas in calculus the ideas in
370:24 - calculus the first
370:25 - is
370:27 - differentiation different
370:32 - differentiation second is integration
370:37 - integration okay so there are two two
370:40 - big problem which are differentiation
370:42 - and integration so so I will just go go
370:45 - over it very quick way to make you
370:48 - understand about this what does these
370:49 - term means so let's talk about
370:52 - differentiation let's talk about what
370:54 - does it mean
370:55 - differentiation and of course there will
370:57 - be long long Le lectures coming on this
370:59 - briefly talking about this a lot with a
371:02 - lot more examples Okay so uh
371:05 - differentiation is a process for finding
371:09 - the derivative of a curve derivative of
371:12 - a curve so it's the
371:16 - process it's the
371:19 - process for finding for finding the
371:24 - derivative
371:26 - the the
371:30 - derivative of a curve of a curve okay so
371:36 - differentiation is the process of
371:37 - finding the derivative of curve but what
371:40 - is a derivative what is a derivative
371:43 - derivative in calculus derivative in
371:46 - calculus is just a fancier version for
371:50 - saying slope derivative is nothing but
371:53 - is just a fancier version is just a
371:56 - fancier version
371:58 - fancier
372:00 - version for slope okay so in calculus we
372:05 - we we call D we call slope as a
372:09 - derivative okay derivative for the curve
372:13 - okay so you find the derivative of a
372:15 - curve and derivative is nothing but a
372:18 - slope of a curve okay so derivative in
372:22 - in a nutshell a derivative is nothing
372:25 - but
372:26 - derivative is nothing but I is equals to
372:29 - slope and is as well as it is a rate it
372:33 - is a rate of instantial
372:36 - instanes change okay so we talk about
372:39 - that but you can think of it as a
372:40 - derivative as a rate so what is rate
372:42 - over here so let's take an example you
372:45 - want to calculate miles per hour miles
372:48 - per hour okay how much this miles
372:52 - changes when hour means it means what it
372:55 - if you go 1 hour then the miles is
372:57 - bigger like that miles per hour taken or
373:00 - a profit per item these are the rates
373:04 - okay profit per item so these are the
373:07 - derivative is that okay so derivative is
373:09 - just a fancier version for a slope
373:12 - because in slope we take out we can we
373:14 - can't take out the slope of a curve line
373:17 - but using the derivative you can
373:19 - calculate the slope of a curve
373:22 - line okay so uh this is this this is
373:25 - what I want to tell is over here you
373:27 - have uh this problem and you have this
373:30 - problem this problem so this is the ra
373:32 - this is a run you run X
373:35 - x uh X on in X direction and you go uh
373:40 - and then then you rise okay so the slope
373:43 - is nothing but rise over run so it just
373:46 - tells how much the how much the Y
373:48 - changes when X changes okay so this is
373:51 - what the slope is and the derivative is
373:54 - nothing but the slope of a curve okay so
373:58 - what's the slope of a curve what's the
374:00 - slope of a curve
374:03 - what's the slope
374:06 - of a curve what is the slope of that the
374:09 - slope of a curve is the uh let's let's
374:12 - let's take one example let's take one
374:14 - example I'm just I just believe in
374:16 - taking an example okay so we have an
374:19 - example something like this you have a X
374:21 - and Y plane and you have a X and Y plane
374:24 - something like
374:27 - this okay so you have something like
374:31 - this oh my God what is this
374:35 - okay so this is this is your curvy line
374:37 - you take out the slope okay so you
374:39 - select any two point A and B okay you
374:43 - select any two point A and B and join as
374:47 - made a secant line over here so we'll
374:48 - talk about this later on so you joined
374:50 - it okay now what you do now over here
374:54 - the slope or the steepness of this curve
374:57 - the slope the
375:00 - slope or the steepness steepness of the
375:05 - particular
375:06 - curve is changing is
375:10 - changing every is constantly changing
375:13 - between your point A and B okay so if
375:17 - you go over here the slope and steepness
375:19 - is constantly is very steep over here
375:22 - but it is so over here is the slope or
375:24 - the steepness is constantly changing for
375:26 - this curve between your two points which
375:29 - is a and b okay so let's take an example
375:33 - that you want to calculate the SL slope
375:35 - at the at at the at the exact point
375:38 - exact slope at the point C so how you're
375:41 - going to calculate so the geometric view
375:44 - which which I want to tell what you do
375:46 - in basic differentiation you zoom in as
375:49 - infinitely you zoom in infinitely far
375:52 - zoom in infinitely far in that at at
375:54 - that point so for for example you you
375:57 - zoom in infinitely far I'm telling you
375:59 - the geometric view telling you the
376:01 - geometric view so what what you do you
376:03 - zoom in so when you zoom zoom in
376:05 - infinitely far this curvy line becomes
376:08 - your formal straight line okay so when
376:12 - you zoom at this reason you zoom in
376:15 - extremely far infinitely far although
376:18 - you cannot get too much infinite but
376:20 - just assume as a geometric you zoom in a
376:22 - lot there is something called limit
376:23 - property which which we'll talk about
376:25 - but when you zoom in too much what it so
376:28 - it would this curve line will become a
376:31 - straight line okay so when when you zoom
376:34 - it so this curve line will become a
376:36 - straight line so let's take an example
376:38 - that be zoomed in too much too much and
376:42 - and now over here this curvy line
376:44 - becomes a straight line now here's a
376:46 - point C now this is a regular calculus
376:49 - problem so sorry regular math problem
376:51 - you can just uh go over rise okay rise
376:56 - over run okay now you'll be calculating
376:59 - the slope at that exact point okay so
377:02 - what you exactly doing is zooming
377:06 - zooming infinitely far infinitely
377:10 - far okay Zoom infinitely far that that
377:14 - that curve okay uh at for at that point
377:18 - at that point C so the curve becomes
377:22 - curve becomes straight curve becomes
377:26 - straight at that point okay and then you
377:28 - can solve using the regular math problem
377:30 - which is rise over run okay it's not a
377:32 - big deal to understand it just geometric
377:34 - View but we will again revisit
377:36 - differentiation these topics again in in
377:38 - our later videos okay so that is
377:41 - differentiation which is finding the
377:43 - derivative or the slope of a curve at
377:46 - any point C okay so now so we have seen
377:50 - differentiation now now let's see
377:53 - integration the definition of
377:56 - integration so we'll talk about
377:58 - integration means it is not if you're
378:01 - getting confused bear with me these
378:03 - concepts are too too much uh I'll
378:06 - explain in too much easy way so you
378:08 - don't need to uh to to worry about these
378:11 - but if you're being worried don't worry
378:13 - I'm just giving you a geometric view
378:14 - we'll again revisit these concept times
378:16 - and times so let's say for an example
378:20 - that your it's is the area under the
378:23 - curve it's the area under the curve
378:25 - actually okay so I'll just give you what
378:28 - is happening
378:30 - integration uh so this is your favorite
378:33 - uh X and plane and this is your oh my
378:38 - God so this is your thing and then you
378:41 - have a line something this and you're
378:45 - told to calculate the area of the Shaded
378:48 - region area of the Shaded so this is a
378:50 - rectangle this is this is a rectangle
378:52 - you can just calculate the area of this
378:54 - sh line so sorry what you can do
378:56 - actually do so say say for example this
378:58 - is your y5 and you want to calculate the
379:01 - area of this C
379:02 - line okay length in the breadth okay not
379:05 - a big deal to to talk about because
379:07 - maybe this is this very easy to
379:09 - understand it isn't it so so over here
379:12 - we are we can this is reg regular math
379:15 - problem this is a regular math problem
379:18 - regular math problem regular math
379:22 - problem but what if if if you get
379:25 - something like this what if if if if you
379:27 - get something like this so you have this
379:31 - and you have this and let's say for
379:33 - example that you have the curve like
379:36 - this and you are told to calculate the
379:39 - area of the Shaded portion over here
379:43 - it's it does not make any sense that how
379:45 - we going to calculate the area of this C
379:47 - line because not anything we cannot use
379:49 - our regular math over here so here's
379:51 - what we Cal integration is nothing but
379:55 - area
379:56 - under the curve area under the curve
380:01 - okay area under the curve and
380:03 - differentiation nothing but the SL
380:05 - derivative or the slope of the curve
380:09 - slope of the curve okay so different in
380:12 - integration helps you to find the area
380:15 - under the curve okay by adding the we we
380:19 - do is it is nothing but addition okay
380:21 - it's not a big deal we understand it
380:23 - much better way so you write a
380:25 - differentiation for a function we we'll
380:27 - talk talk about I'm not going to
380:29 - introduce it as of now so this is your
380:31 - integration which is the area under the
380:33 - curve and that is your difference
380:34 - differentiation the first topic will be
380:36 - differentiation we're talking about and
380:38 - then we'll go on integration okay so I
380:41 - hope that's not a big deal and I also
380:42 - hope that you understood this now this
380:44 - is this was the first lecture if you
380:46 - have understood a little bit about
380:47 - calculus and I hope that you understood
380:49 - a little bit about this so please give a
380:52 - thumbs up to this video and I hope that
380:54 - you liked this video I'll be catching up
380:56 - your next video in talking about
380:59 - pre-calculus about functions and then
381:01 - trigonometry and then talking about
381:03 - differentiation and I hope that you
381:04 - enjoyed this tutorial I'll be catching
381:06 - up in the next video till then byebye
381:08 - have a great
381:12 - [Music]
381:21 - day hey everyone welcome to this second
381:23 - lecture on single variable calculus uh
381:26 - so in this video we'll be talking about
381:29 - pre-calculus basically we'll be
381:30 - completing our pre
381:32 - pre-calculus uh and the in two videos as
381:35 - I think that I should review functions
381:38 - and I should review trigonometry so that
381:40 - everyone who has lost touch in this uh
381:43 - they can review it and the one who is
381:46 - who has at least seen these functions in
381:49 - trigonometry before they can uh just
381:52 - refresh up it is not a full full videos
381:55 - on functions and tri trigonometry I'll
381:57 - recommend watching some of lectures on
381:59 - trigonometry and functions if you want
382:02 - to learn actually trigonometry and
382:04 - functions and solving it but basically
382:06 - it will Pro provide you everything you
382:08 - need for calculus mainly the what you
382:10 - need for functions and you will whatever
382:12 - the trigonometry you need for for for
382:14 - taking of the trig values about uh we
382:17 - we'll talk about sakova we'll talk about
382:19 - one one more thing which is uh uh the
382:23 - special right triangles we'll see some
382:25 - of the tricks for solving the for for
382:28 - for solving it uh we'll also see the
382:30 - unit circle where we'll be seeing the
382:32 - radiance degrees and every stuff will
382:35 - give you the graph of cosine and uh uh
382:39 - cosine and S we'll try to give you the
382:41 - graph of that so I hope that you will
382:43 - enjoy the these two set of videos on
382:46 - pre-calculus if you're completely new to
382:48 - math and your class I think class 10
382:51 - then I think that you're good to go just
382:53 - review the lecture of this if you are a
382:56 - little bit go ahead but means you can if
382:58 - if you're in college or or other set of
383:01 - classes I would say uh these these will
383:04 - be good refresher for you cool so we'll
383:07 - revie we'll be reviewing our function so
383:10 - what is the function if if if you can go
383:12 - ahead and write in the description box
383:15 - uh basically the functions is tells you
383:17 - the relationship it's a relationship
383:19 - between your two things in which one
383:22 - depends on other for the for for for for
383:25 - doing some processing okay so basically
383:27 - I think about the functions it's it's a
383:30 - relationship it's a relationship Rel
383:35 - relation it's a relationship just let me
383:38 - write
383:39 - relation
383:41 - relationship between two things between
383:45 - two things in which and between two
383:48 - things in
383:51 - which one depends on other in which one
383:54 - depends on other in which one depends on
383:58 - other it may it it may happen that
384:00 - you're that you're not able to get this
384:02 - what the definition is so let me write a
384:06 - function which takes any value X and
384:11 - Returns the square of that X if you are
384:14 - into programming and I hope that you are
384:16 - in so uh it's it's it's okay that you
384:20 - are not in so you may have seen
384:22 - functions before and and I really hope
384:26 - that uh in in in your programming you
384:28 - just Define the function so what the
384:30 - function does it takes some arguments
384:33 - okay so over here it takes some input
384:35 - values okay and the function do
384:38 - something and Returns the output okay so
384:41 - what is trying to do what is doing is
384:42 - taking the value in input value
384:46 - input value and returning the and doing
384:50 - the square of this so what is doing in
384:53 - the box so we are giving the giving x to
384:56 - the function which is f it is doing the
384:58 - square of this it's doing the square of
385:00 - this and giving your output y Okay so
385:04 - for example the function f let's say we
385:07 - give two to this function it will return
385:09 - it will it will take out the four and
385:12 - then it will give y as a four okay so
385:15 - basically it is the function f is doing
385:17 - some processing and giving some output
385:20 - okay the same way you think about the
385:22 - functions it's it's um it's it's it's
385:24 - kind of a processing what it is doing
385:27 - for example in this F ofx = to x² so it
385:30 - is taking the input value and then
385:32 - squaring them up the function can be
385:35 - let's take an example it takes uh
385:38 - maybe a set okay a set I would say giant
385:42 - X and Joint X is nothing but x equals to
385:45 - the X1 X2 and X3 okay and and and it
385:50 - doing what what it does this function
385:53 - tries to multiply with some Theta okay
385:56 - for example it multiply with some
385:58 - constant to 2 * X1 + 2 * X2 + 2 * X3 so
386:03 - it take the set of input values okay so
386:07 - I hope that that function is making
386:09 - sense what the function actually is so
386:11 - it takes one input value and do do the
386:14 - square of it and give the output value
386:16 - so Y is consider over your output value
386:19 - and X is considered as input value Okay
386:24 - cool so so so let's so let's let's go
386:27 - ahead um basically uh input value is the
386:31 - one which we input anything for example
386:33 - in this case we input two into that and
386:36 - then it does some processing as I said
386:38 - and Returns the square of that or gives
386:40 - the output
386:42 - value cool so I hope that that you that
386:45 - you're making that you made sense out of
386:47 - it so now let's talk about some of the
386:49 - terminology which I want to introduce to
386:53 - you first of all domain of a
386:56 - function the domain of a function so set
386:59 - of all the inputs the set of all the
387:02 - inputs of a function
387:05 - set of all the
387:08 - inputs of a function f let's take F
387:12 - function f is called the domain and what
387:16 - is the
387:17 - range what is the range range is set of
387:21 - all the outputs set
387:23 - of all the outputs set of all the
387:28 - outputs of a function is the range of a
387:32 - function okay so so say for say for an
387:36 - example that that you have a function
387:39 - that you have a function f that takes
387:41 - the value first of all that do the
387:43 - square of that do square of that so you
387:46 - give one and you give min-2 over here
387:50 - you return 1 and four okay the domain is
387:54 - 1A -2 and uh range is 1A 4 okay it may
388:01 - happen that you may enter the third two
388:03 - and it will give four so it may it it it
388:06 - cannot be it it can be the same thing
388:08 - four four it can be the same thing okay
388:10 - so these are the certain terminology
388:12 - which you will see a lot in the uh up
388:15 - upcoming videos okay so I want to
388:18 - introduce you two things which is
388:20 - independent independent variables so all
388:24 - the input values are independent
388:26 - variable or values okay so basically so
388:30 - basically all your input values input
388:33 - Val
388:34 - values input
388:37 - values are the
388:40 - independent
388:44 - independent
388:47 - variables independent variables okay all
388:51 - the input
388:52 - values are the independent variables and
388:56 - all the and and the output values are
388:59 - called the dependent variable output
389:02 - values
389:04 - output values it it makes sense are the
389:08 - I I just want to include this one this
389:10 - one this one this one you can just write
389:12 - it out okay are the dependent variables
389:15 - so so it makes sense that for example we
389:18 - give the function f maybe X okay and X
389:22 - is not dependent on anything to be
389:24 - relied because X we have to give it is
389:27 - independent it is not dependent on
389:28 - anyone but this Y is dependent on these
389:33 - X on the input value so that's why we
389:36 - say output value is dependent variable
389:39 - and input values are independent
389:42 - variables okay these are two things so
389:45 - let's take an example I'll take I'll
389:48 - take one small example let's say let's
389:51 - say you have a function you have a
389:53 - function uh F ofx okay
389:57 - x² and you have another function G of X
390:01 - okay which is 5x - 8 okay so you have
390:06 - two functions f ofx g of x - 8 now now
390:11 - there is a chain of function
390:13 - so let's say F of G of3 if someone ask
390:17 - you to evaluate this function so how
390:20 - will you do so this is these functions
390:23 - are called as composite function
390:27 - composite functions these type of
390:29 - functions are called the composite
390:32 - functions Okay so what we do first of
390:35 - all as as you already might have been
390:37 - guessed okay I'm going to just evaluate
390:39 - G of3 and whatever the output G of3 will
390:42 - give I will give the input to the f of x
390:46 - okay so what how this work workflow will
390:48 - do first of all we'll evaluate the G of
390:51 - 3 okay so 5x - 8 we give 3 to the input
390:55 - and we get output as a 7 now we give
390:58 - this seven as a input to to our F x² now
391:02 - that is uh F and then we get the output
391:05 - 49 49 okay and 49 is our uh output value
391:12 - okay so 49 is the answer or or or the
391:18 - result when we apply the G of3 with the
391:21 - output uh uh f f on the output of the G
391:24 - of3 okay so these the these functions
391:27 - are called composite function okay and I
391:30 - hope that you you will see a lot when
391:32 - when when we started change rule of
391:34 - differentiation and it's very very used
391:36 - there you will you'll make use of
391:38 - composite function you make use of the
391:40 - terminology of composite
391:41 - function cool so uh so maybe if you just
391:47 - you can you can just go ahead we have a
391:48 - line so let's let's let's talk about uh
391:51 - slope okay it's always better to talk
391:54 - talk about slope okay so uh let's go
391:58 - ahead talk about the slope
392:04 - say for an example you have the line you
392:07 - have the line okay so I I just draw a
392:09 - very bad line so let me just pick up
392:12 - this and let me
392:14 - just I don't know how to do it leave it
392:18 - so you let's let's take an example that
392:20 - you have a
392:21 - line okay that you have something like
392:24 - this and you have a line and you're told
392:29 - to find the slope of this straight line
392:32 - and you can do with regular math it is
392:34 - nothing but the slope of this line will
392:37 - be rise over run okay so so you can take
392:40 - any two values any to values you Y2 - y1
392:46 - / X2 - X1 so how much y changes when X
392:51 - changes okay so it is just telling how
392:55 - much y
392:57 - changes when X changes okay so this is
393:01 - nothing but the slope okay slope is not
393:04 - a big deal to understand it so I hope
393:06 - that it's that it makes sense out of it
393:08 - now let's try to do one thing let's try
393:10 - to do one thing is Let's uh let's let's
393:14 - plot one line so graphing of a function
393:17 - how we do the graphing of a function
393:19 - let's let's let's talk about that a
393:21 - little bit so graph of a function graph
393:25 - of a function so let's take a let's
393:28 - let's say you to you have a function y
393:30 - and you write a function yal 3x + 5 you
393:34 - can also write the function something
393:35 - like this let's take an example you want
393:38 - to make a graph of this function so how
393:41 - are how are we going to do that so
393:43 - basically what I'm going to
393:45 - do let me just remove these straight
393:48 - line I'm just going to draw every
393:49 - everything over here so let's let's take
393:51 - let's take an example 2 4 6 8 10 12
394:01 - 14 uh let's let's do 14
394:05 - 14 uh
394:07 - 16
394:09 - 18 and 18 and 20 okay you have you have
394:15 - 2 4
394:18 - 6 6 oh my God this is 8 10 12
394:26 - 14
394:28 - 16
394:30 - 18 18 and 20 Okay so so how do we even
394:34 - plot our function y = 3x + 5 so we make
394:37 - a table we make a table so let's make a
394:40 - table let's make a table some some
394:43 - something like this let's make a table
394:47 - okay some something like this and you
394:49 - give X and Y okay so Yi you put the
394:54 - values of X so for example you choose x
394:57 - = 0 so when you evalue 3 * 0 means the
395:00 - output will be five okay so the so the
395:03 - first point you got 0a 5 so you put the
395:05 - point on that okay so on y value we have
395:08 - some over
395:10 - here okay now you put 1 and when you 3 *
395:15 - 1 3 + 5 which is 8 okay so 1A 8 is
395:19 - another another point so one and 8 okay
395:25 - one and 8 so maybe you go ahead my
395:28 - diagram is not good so you you will you
395:31 - will keep doing that and and just is uh
395:34 - just to draw a s straight line over
395:36 - there okay just just just draw a
395:38 - straight line over there uh just joining
395:41 - the points and you'll be getting your
395:43 - graph of a function I know it's I think
395:45 - because of my X and Y plane it is
395:47 - failing to draw but you can just take a
395:50 - graph paper and work on that okay so
395:53 - this so we had a talk on slope and we'll
395:55 - revisit the slope in times and times
395:56 - when we talk about differentiation so
395:58 - don't worry if if if you don't
396:00 - understand it but I but I can just hope
396:02 - that you understand a little bit so a
396:05 - straight line is if if you know about
396:07 - slope intercept form slope intercept for
396:12 - okay so over here you have y = mx + b is
396:17 - an equation for a straight line okay uh
396:21 - straight straight line okay where m is
396:24 - the slope m is the slope m is the slope
396:28 - if you all have already been seen and B
396:32 - is the y intercept
396:34 - B is the Y intercept okay so what is the
396:37 - inter Y intercept and x intercept if
396:40 - your line crosses some from here so the
396:44 - Y intercept of this will be four because
396:46 - it intercepts the the the Y AIS at the
396:50 - point 4 okay so that's the that's that's
396:54 - called Y intercept and slope is nothing
396:56 - but rise over run okay which is Y2 - y1
397:00 - / X2 - X1 cool
397:04 - so these these are some of the talk on
397:06 - functions and I hope that that you are
397:08 - able to get the sense out of it now
397:10 - let's do one thing let's try let's let's
397:13 - let's try to talk about these the two
397:15 - two kind of functions which you will see
397:16 - a lot in your different in your Calculus
397:19 - journey is parabolic function and
397:22 - absolute value function okay so let's
397:25 - let's talk about that main the graph so
397:29 - U so let's take F ofx = x² this is is an
397:33 - equation for your Parabola okay so maybe
397:37 - let me let me take my black pen I think
397:39 - that that
397:41 - works
397:46 - okay
397:49 - okay okay so now when you when you try
397:52 - to plot it when just just just make a
397:54 - table X and y1 and try to put put the
397:58 - values and get the values so you put the
398:00 - values of X you'll get the value of y
398:02 - put the random Val of X put the Rand get
398:05 - the get the value of y and keep keep
398:07 - putting on the over here so you will get
398:11 - the parabola something like this let me
398:14 - touch
398:14 - it I think it's just not a it's it's not
398:17 - a good good Parabola but yeah okay so
398:20 - this this this is the diagram of your f
398:22 - ofx equals to or a graph of f ofx = to
398:26 - x² okay so you you may you'll think that
398:29 - this function is a continuous function
398:33 - we talk talk about that I I I just don't
398:35 - want to introduce you over here okay so
398:38 - uh this function is a continuous
398:40 - function we we see we'll see how to uh
398:43 - this it is also differentiable so we
398:46 - will talk about this so so that's why
398:47 - I'm in introducing to the graph of these
398:50 - plots so that it would be easy to
398:52 - understand that at at that point so this
398:54 - is your don't worry if you don't
398:56 - understand what is continuous and what
398:57 - is differentiable just I've told you
399:00 - we'll talk about in detail in our later
399:02 - videos okay so this is a function for
399:04 - your F ofx this this is a graph of your
399:07 - F ofx = to
399:09 - x² okay so I just want to draw another
399:12 - make a graph of another function f ofx
399:14 - equals to absolute value of x so
399:18 - absolute value of x so the the the so
399:23 - the the graph of the function will look
399:25 - like straight line let me join it over
399:28 - here join it over
399:31 - here okay so this is your graph of the
399:35 - function okay this is your graph of the
399:38 - function f ofx this is this this is a
399:41 - graph of function a g of X let's name a
399:43 - g of X because it will be confusing G of
399:46 - X Okay g of X where this this is your
399:50 - graph of function okay you can put the
399:51 - values you will get the exact graph of
399:53 - this okay but one thing which I want to
399:56 - tell you that this is a continuous
399:59 - function this is but this is
400:02 - nondifferentiable you may think hey why
400:04 - you telling this now you can tell me
400:06 - later on so just for your information
400:08 - I'm just telling so that why we are
400:10 - introducing these kind of graphs and in
400:12 - front of you so first of all I told you
400:14 - one property that the which is
400:17 - continuous and which is differentiable
400:19 - if you don't understand ignore it the
400:21 - second the second conclusion which you
400:23 - can make out of it the second Insight
400:26 - which you can get of this is both are
400:30 - symmetric both both function
400:33 - fun are symmetric both functions are
400:38 - symmetric with respect to two with
400:41 - respect to Y AIS okay so if you know
400:45 - symmetrical okay this makes them even
400:48 - function these are being symmetric this
400:52 - these are the these these functions are
400:54 - called the even
400:56 - functions okay another thing is the the
401:00 - polinomial function like this or let
401:02 - function like this F of f of f ofx okay
401:06 - = to 9
401:08 - x^ 4 - 4x² + 3 so this function is also
401:15 - the even function can you guess why
401:18 - because the power the powers all the
401:21 - powers all
401:23 - powers are even okay all the powers are
401:29 - even okay so so that's why it is a even
401:32 - function so so so so that's why it is an
401:34 - even function if the powers are odd okay
401:38 - then that will be a odd function okay so
401:41 - I hope that you that that you understood
401:43 - either it's not important to know what
401:45 - is even function what is odd function
401:47 - but just for a general purpose I I told
401:49 - you cool so we had a talk on functions
401:52 - now let me see how much the time is
401:54 - being there I think 20 minutes we can go
401:56 - ahead talk about trigonometry now so we
401:58 - had a we had a brief talk on functions
402:00 - and I hope that you had got a very good
402:02 - sense on that so now what I will do I
402:05 - will just start off with uh with the
402:07 - basics of
402:09 - trometry uh either we can get started in
402:11 - the next video I think yeah so let's so
402:14 - I think it the I will just pre prefer in
402:17 - the next video for trigonometry because
402:18 - because there there's a lot to talk I
402:20 - can't continue with this video so I'll
402:22 - be catching up your next video in
402:24 - trigonometry because this is a two pages
402:26 - so I need to and I need to make around
402:28 - your 40 minutes to talk about
402:30 - trigonometry because trigonometry is a
402:31 - big topic to understand so we after
402:34 - after trigonometry we'll talk about
402:35 - limits we will talk about there are a
402:37 - two three videos coming on limits first
402:39 - of all what is limits the evaluating
402:42 - limits squeeze theorem so we'll talk
402:44 - about that mean the sandwich theorem so
402:47 - we'll talk about that so I hope that you
402:49 - had understood this very properly I'll
402:51 - be catching up your next video till then
402:53 - bye-bye have a great day
402:58 - [Music]
403:07 - so now we'll start with the trigonometry
403:10 - I think we'll just review trigonometry
403:13 - in such a way so that you could be from
403:15 - comfortable with upcoming calculus
403:17 - videos and this is not a math Channel I
403:21 - don't know why I'm feeling like I'm
403:22 - teaching mathematics it's it is a part
403:25 - of our machine learning course sorry
403:27 - deep learning course which I'm teaching
403:29 - so that's why I want everyone to go from
403:31 - very scratch to such an advanced level
403:33 - Because deep learning is a b speed so so
403:36 - that's why you have to first of all make
403:39 - your fundamental strong okay so we'll
403:42 - we'll be getting it started with the
403:43 - trigonometry okay we'll just review uh
403:47 - some of the trick functions we'll also
403:49 - review the unit circle maybe some of you
403:52 - have far good in it or I don't know
403:53 - about that I will also review how to
403:56 - graph sign and cosine okay so basically
404:00 - what is trigonometry first of all maybe
404:02 - you can just uh it's it's it's a study
404:04 - of triangles or maybe the right
404:07 - triangles which you see so mainly the
404:09 - trigonometry is the study of right
404:11 - triangle and there are total of three
404:14 - main trigonometric functions so there
404:17 - are total of three main Tri Tri trick
404:20 - functions I'm just going to write trig
404:23 - functions so they are total of three
404:25 - main trick functions the first one is
404:28 - the F the first one is sign okay s
404:33 - second one is cosine the third one is
404:38 - tangent okay so these are the three uh
404:42 - main trigonometri functions and there
404:45 - there there there are some more which
404:47 - which are called cosecant secant and
404:50 - cotangent so these three and the
404:54 - reciprocals they they reciprocals they
404:57 - reciprocals are also there some some
405:00 - important trick functions so they
405:03 - reciprocals the reciprocals are also the
405:06 - very important trick function such as uh
405:10 - cosecant Co
405:14 - coant secant second one is
405:18 - secant and third one is cotangent okay
405:23 - so we will talk about these in detail in
405:26 - in our whole session we'll talk about
405:28 - these six Str trick functions as these
405:31 - three are the reciprocal of this so
405:33 - we'll will study everything today uh
405:35 - these three six functions our our whole
405:38 - lecture will go around these St
405:40 - functions okay so I want to introduce to
405:43 - you something called a some some I would
405:46 - say um uh u i it's I I would say a good
405:50 - weapon for you okay a weapon that helps
405:53 - you helps you to solve uh trick trick
405:56 - trick functions very easily and very
405:58 - very very very very important uh the the
406:02 - sign to remember the what is a sign of
406:05 - the particular angle so that is nothing
406:09 - but cira
406:12 - so TOA so maybe some of you some of you
406:15 - have seen this and some of you have not
406:17 - so this so SOA is very very important in
406:21 - your trigonometry Journey it's very
406:24 - again I'm saying very very important
406:26 - many many people has different different
406:27 - stuff but very very easy to understand
406:30 - and very very easy to uh it it it helps
406:33 - you to remember the definition of your s
406:36 - cosine and tangent so using CA you can
406:40 - remember the the the definition of what
406:43 - that sign means what the cosine means
406:45 - and what the tangent means and it is
406:48 - very very useful and very very important
406:50 - about this okay many people say
406:52 - different different Rhymes to to to
406:54 - identify the definition of s cosine and
406:56 - tangent but I think this is the best and
406:59 - one thing which is easily remember
407:02 - rememberable to anyone okay so what I'm
407:05 - going to do is to I'm going to make a a
407:08 - right triangle okay so but before that I
407:11 - I I just introduce you in that right
407:13 - triangle what does it mean so first of
407:15 - all let me make the pen a little bit
407:17 - lower yeah so so let's take let's let's
407:21 - say let's say for an example that you
407:24 - have this one and you have this one okay
407:29 - I I just don't know how to draw a good
407:32 - line and and this one is and this is
407:35 - your right triangle let me okay so
407:38 - basically this this is called the
407:40 - hypotenuse so let me just give a name so
407:43 - this this is called the
407:45 - hypotenuse hypotenuse which is given
407:47 - Edge okay and hypotenuse is the largest
407:51 - side in your triangle okay this is uh
407:54 - this this this is your angle let's say x
407:57 - and uh and this is this is called the
408:00 - adjacent adjacent side
408:03 - adjacent side to that angle X okay
408:07 - adjacent side to that angle X as it's
408:09 - the right triangle and this is called
408:12 - opposite side to that angle opposite
408:15 - side opposite side to that angle okay so
408:20 - basically what I'm saying that this is a
408:22 - hypotenuse H okay um and we have an
408:25 - angle X where this uh let's let's give
408:29 - some name ABC okay and BC is adj
408:33 - side to this angle X and AC is an
408:36 - opposite to the angle X okay so which is
408:40 - opposite of the angle which which is a
408:42 - side which is opposite to angle X so now
408:46 - what I'm going to do is to take out is
408:49 - just to give you the definition of s of
408:52 - x s of x cosine of x and tangent of X
408:59 - okay so what I'm going to do is to using
409:01 - the definition of Circ K so what I'm
409:04 - going to do
409:06 - so to TOA so so s of theta and Theta is
409:13 - an angle Theta is an angle over here it
409:16 - will be X we we'll solve it but before
409:18 - that over here which you can see s of
409:21 - theta okay cosine of theta and tangent
409:26 - of theta tangent of theta so let's go
409:29 - with the definition of so so so let's
409:31 - try to Define s of theta so when you
409:35 - define the S of theta so so opposite
409:38 - over hypotenuse so s is s of theta is
409:42 - equals to the opposite side over the
409:45 - hypotenuse so opposite
409:48 - opposite opposite over hypotenuse so
409:53 - that's a definition for your sign of an
409:55 - angle the sign of an angle is nothing
409:57 - but the opposite over hypotenuse what's
410:00 - what is a cosine what is the cosine of
410:02 - an angle cosine of an angle is adjacent
410:05 - over hypotenuse adjacent adjacent over
410:10 - hypot tenous okay or in other words a
410:14 - over H as we given the name A and H okay
410:19 - so uh this is a sign this is a cosine
410:21 - and the the the the for for taking of
410:23 - the sign of an angle the definition for
410:25 - sign of an angle is nothing but the
410:27 - adjacent over hypotenuse if you want to
410:30 - calculate the tangent tangent of an
410:34 - angle which is nothing but uh which is
410:37 - which is nothing but opposite over
410:39 - adjacent opposite opposite over
410:44 - adjacent opposite over adjacent which is
410:47 - O and H okay so with the with the
410:50 - definition opposite over adjacent so
410:52 - using this rhyme SOA you able to uh to
410:57 - to to Define these three trick functions
411:00 - okay so now let's use this CA to take
411:03 - out the trig values okay so but before
411:06 - that let's give let's say the adjacent
411:10 - side over here the adjacent side uh
411:13 - let's let's assume adjacent side is four
411:15 - whatever the centimeters or meters let's
411:17 - let's assume centimeters the hyp the the
411:20 - the AC which is which is opposite side
411:23 - to that angle or the height so it will
411:26 - be three and may you can take out the
411:29 - hypotenuse how you can take out the
411:31 - hypotenuse this is very very easy using
411:33 - Pythagorean theorem Pythagorean theorem
411:36 - tells a² + b² = c² a sare is your BC b
411:42 - square will be your AC equals to c² so
411:46 - when when you go ahead when you go ahead
411:48 - 3² so sorry 4² + 3² = to what c² okay 4²
411:57 - + 4² 16 + 9 = to what c²
412:07 - 616 25 = c² and when you when you when
412:11 - you just remove this so it will be <
412:14 - tk25 = C which will be C = 5 so the
412:18 - hypotenuse will be five okay so you
412:21 - given the sides of an of of a right
412:24 - triangle now what you need to do you
412:26 - need to take out the S of the x s of x s
412:31 - of X so how are you going to do take out
412:34 - the S of X but taking out the S of x the
412:37 - sign the sign definition is so K so sign
412:40 - is opposite over hypotenuse so opposite
412:43 - over here is three and hypotenuse over
412:46 - here is five is your s of that s of s of
412:51 - the x s of X is nothing but the 3 over 5
412:55 - so what is cosine of x so what is cosine
412:59 - of x so what is cosine of x cosine of x
413:02 - is C so so C A so adjacent over
413:07 - hypotenuse so adjacent side over here is
413:10 - a thing BC and this is 4 ided H which is
413:16 - 5 so cosine of x is 4X 5 what about
413:20 - tangent of X so tangent of X is nothing
413:24 - but opposite over adjacent and opposite
413:26 - to that angle X is three and adjacent to
413:29 - that angle is four okay so which is
413:32 - which is nothing but 3 over 4 is the
413:35 - tangent of X okay so these three s
413:40 - cosine and tangent we are taking out and
413:42 - this giv you definition what is a sign
413:45 - what is a cosine and what is a tangent s
413:49 - is nothing but the opposite over
413:50 - hypotenuse cosine is nothing but
413:53 - adjacent over hypotenuse tangent is
413:56 - nothing but opposite over adjacent how
413:58 - you remember it using the SOA definition
414:02 - and I think this is this is pretty much
414:03 - easy okay so we had used this s we have
414:07 - used
414:09 - thisa to identify this s of the
414:13 - particular x cosine of the X and tangent
414:16 - of X okay so we have seen this uh s
414:20 - cosine and tangent so we have seen the
414:23 - definition of these three now what about
414:25 - the reciprocals of it now what about the
414:28 - reciprocals of this so the same way the
414:30 - other three functions are the reciprocal
414:33 - of s cosine and tangent so let's let's
414:38 - let's let's talk about that so other
414:41 - trig functions
414:43 - other trig
414:45 - functions other trig
414:48 - functions are the
414:51 - reciprocals are the
414:54 - reciprocals of these existing these s of
414:59 - x cosine of theta and tan tangent of
415:02 - theta so let's give the Theta name Theta
415:04 - angle okay so so uh let's let's let's
415:08 - talk about the first one which is the
415:11 - cosecant okay cosecant
415:15 - cosecant
415:16 - cosecant so what is this cosecant is a
415:20 - reciprocal cosecant is the reciprocal
415:23 - cosecant is the
415:25 - reciprocal reciprocal of s cosecant is a
415:31 - reciprocal of sign we'll see what the
415:33 - reciprocal means just in a second second
415:36 - one we have which is secant and we
415:39 - denote this as SC SEC okay set so secant
415:45 - is nothing but is
415:47 - the reciprocal
415:50 - reciprocal reciprocal of your cosine of
415:55 - your cosine and the the the the last one
415:58 - which is the over here is cotangent
416:02 - cotangent which which we didn't denote
416:04 - is a cot okay is the reciprocal of your
416:08 - tangent is the
416:10 - reciprocal of tangent and how do you
416:13 - define cosecant cosecant is SC CC CS C
416:18 - okay secant SEC
416:20 - C and what about cotangent Co T okay so
416:25 - in sign we write s n cosine cos tangent
416:29 - t Okay the these are sh form given to
416:32 - them so now what you mean by reciprocal
416:35 - so let's take out the let's let's take
416:37 - the definition
416:38 - of cant cosecant of theta for for for
416:44 - for for any angle okay which is nothing
416:47 - but the reciprocal of s sin Theta so 1 /
416:51 - sin Theta so reciprocal of the S of that
416:54 - angle sin Theta okay so it will be one
416:58 - over s let's let's remember our favorite
417:01 - definition from one and only so so CA
417:07 - SOA so sign is opposite over hypotenuse
417:11 - s is opposite over hypotenuse s is
417:14 - nothing but opposite over hypotenuse it
417:17 - goes up of it will be H over o h over o
417:22 - which is nothing but your cosecant so
417:25 - definition of a cosecant of a Theta uh
417:29 - is nothing but a hypotenuse over
417:32 - uh over opposite of that angle okay uh
417:35 - the next one is the next one is secant
417:39 - of theta secant of theta is the
417:42 - reciprocal of cosine which will be 1 /
417:45 - cine of theta so it will be nothing but
417:48 - so what is the cosine adjacent over
417:50 - hypotenuse 1 over adjacent over
417:53 - hypotenuse it goes above so it nothing
417:56 - but h of a h over a which is hypotenuse
418:00 - over adjacent okay so the the definition
418:03 - of secant of the Theta is nothing but
418:05 - the hypotenuse over adjacent the last
418:09 - one over here is cotangent so what's the
418:11 - time so the last over the last one is
418:14 - over here is cotangent cotangent which
418:17 - is cot of theta is is the reciprocal of
418:20 - a tangent 1/ tan of theta So Tan of
418:24 - theta is nothing but opposite over
418:25 - adjacent so one over uh opposite is over
418:29 - over here o by a okay so that is so it
418:33 - goes above 1 over so sorry a over o
418:37 - which adjacent over opposite angle okay
418:40 - so opposite to that angle Theta so
418:44 - cotangent the definition of a cotangent
418:47 - is adjacent over opposite of that Theta
418:50 - okay so we have a g given you definition
418:53 - of CA s cosine and tangent and we have
418:57 - also given you the the definition of
419:00 - cosecant secant and cotangent of theta
419:06 - okay so we have also we have given you
419:07 - everything about this now I hope that
419:09 - you had got a good idea about what is
419:12 - these angles tells you but before that
419:15 - what I'm going to do what I'm going to
419:16 - do is to solve for the triangle take out
419:20 - we have to have taken out the sign of
419:22 - that triangle which we have built up S
419:25 - of this was 3 over 5 cine 4 over 5
419:28 - tangent 3 over 4 so what will be the
419:32 - cosecant of these uh of of of this uh
419:38 - triangle okay so let's let's take out
419:40 - the cosecant the cosecant of X of the
419:44 - angle X which will be 1 over the S of
419:48 - theta okay so which is nothing but again
419:52 - when you when you go ahead so so let's
419:54 - let's not write it again let's let's use
419:56 - our formal definition so cosine cosecant
419:58 - of theta is hypotenuse over adjacent
420:02 - hypotenuse was F five and adjacent was
420:05 - four so F 5 over 4 is the answer of this
420:10 - so sorry it's 5 over3 okay because
420:12 - adjacent because uh over over here your
420:16 - uh adjacent opposite angle so this is
420:19 - this this is uh opposite okay I don't
420:22 - know where it
420:24 - is yeah so it is opposite so 5 over 3
420:28 - okay which is the reciprocal of your
420:30 - sign if if you told to take out the
420:32 - cotangent or the secant first of all
420:34 - let's L secant of X which will be
420:37 - nothing but secant of X which will be
420:40 - nothing but let's let's let's say for
420:42 - for for take example secant of X is
420:45 - hypotenuse over adjacent hypotenuse was
420:48 - five adjacent was four okay so that is
420:52 - just the reciprocal of your cosine what
420:55 - is the tangent of X what is the tangent
420:58 - of X so tangent of X is adjacent over
421:02 - opposite okay so
421:04 - adjacent so adjacent will be uh four and
421:09 - uh opposite is three four and three okay
421:13 - so we have taken out for for for for
421:16 - this triangle we taken out the cosecant
421:18 - secant and who told tangent it's
421:22 - cotangent okay so mainly we would add C
421:25 - okay so um so these are the things which
421:28 - I which I want to remember and I hope
421:30 - that you remember it as well
421:32 - okay now I hope that that you have got a
421:35 - better idea about what is this SOA means
421:38 - and the form form formal definition of
421:40 - this uh CA and these six definitions I
421:45 - want to introduce you to to to two
421:48 - special right triangles it is very very
421:50 - important to know okay so I'm I'm just
421:53 - going to introduce to you about two
421:55 - important uh two special that helps you
421:58 - to remember these stct values because
422:01 - these will very handy in your uh
422:04 - calculus Journey so I'm just going to
422:06 - introduce to that so there are I'm just
422:08 - going to write two special right
422:12 - triangles
422:13 - two
422:16 - special right triangles right triangles
422:21 - so the first one is the first one is
422:25 - 45°
422:26 - 45° 90°
422:29 - triangle 90° triangle this is a cute
422:32 - angle okay so over here when you when
422:36 - you when you try to make a make a that
422:39 - so let's make a right there is one angle
422:42 - which is of 90° and other two
422:46 - angle are 45° okay so I'm just going to
422:49 - write
422:49 - 45° and
422:52 - 45° okay so this is the following and
422:56 - you have let's say for you you you'll be
422:59 - getting these base so base will be one
423:02 - it will be also one and the hypotenuse
423:04 - will be 1.41 which is < tk2 okay and the
423:08 - hypotenuse will be < tk2 because when
423:09 - when you take the hypotenuse how you'll
423:11 - do 1 + 1 1 s + 1 s = c² when you when
423:16 - you do this 2 = to c² when your < tk2
423:20 - okay that is your hypotenuse now now
423:23 - let's apply Saka and their
423:26 - reciprocals to take out the sign of the
423:30 - 45° okay so let's apply the ca over here
423:34 - to take out the sign of 45 Dee okay of
423:38 - this
423:38 - angle let's let's apply that uh let's
423:42 - let's let's go ahead uh yeah so s of
423:47 - 45° will be first of all SOA let's let's
423:52 - remember our definition
423:54 - so TOA so opposite over hypotenuse so
423:58 - opposite 1 over hypotenuse
424:02 - < tk2 okay so which will which will be
424:05 - nothing but when you rationalize it when
424:08 - you rationalize it it will be nothing
424:10 - but < tk2 by 2 I hope that you how you
424:14 - you know how to rationalize you simply
424:16 - multiply something this I think that's
424:18 - some of you do not know about it so <
424:20 - tk2 * < tk2 < tk2 will beunk 2 by two
424:27 - that's what you get okay so that so you
424:30 - you get this and then when you when you
424:31 - go ahead that it is approximately equals
424:34 - to 0.71 you can calculate using your
424:37 - calculator okay so that is your s of
424:40 - your
424:41 - 45° what is the cosine of 45°
424:45 - cosine cosine of
424:48 - 45° cosine of 45° so adjacent over
424:52 - hypotenuse so adjacent is one and
424:55 - hypoten puts two okay so adjacent is 1
424:59 - and < tk2 1 / < tk2 which will be again
425:03 - < tk2 / 2 by rationalizing approximately
425:07 - equals to 0.71 that is a cosine of
425:10 - 45° now what is the in in in this in the
425:13 - part for this triangle what is the
425:15 - tangent of
425:18 - 45° 1 / 1 how do you say opposite over
425:22 - adjacent which will be around 1 so 1 is
425:25 - the tangent of your 45° for this
425:28 - triangle so what about the reciprocals
425:30 - of it so what about the reciprocals of
425:33 - it so the reciprocals of it will
425:36 - be first of all secant first of all
425:39 - let's let's let's talk about uh co co
425:43 - cosecant cosecant of 45° which will be
425:47 - uh the the the reciprocal of it so
425:51 - hypotenuse over uh opposite angle so
425:55 - hypotenuse is < tk2 < tk2 by 1 which is
425:59 - nothing but approximately equals to 1. 4
426:02 - 1 okay what is the secant of 45° so
426:06 - secant will be h/ a h/ a hypotenuse
426:12 - hypotenuse is < tk2 over 1 which will be
426:16 - also approximately
426:18 - 1.41 what is a cotangent what is
426:22 - cotangent so what is a cotangent is will
426:25 - be 1/ 1 because adjacent over hypotenuse
426:28 - which will be one okay so these are the
426:31 - reciprocal the trig values so so we got
426:34 - the trig values of you can see over here
426:36 - that using this special these these
426:38 - special right triangle will be so so
426:40 - much in your journey so that's why I'm
426:42 - introducing to you this one okay now
426:45 - let's go ahead let's let's talk about
426:48 - another now you got to know these stct
426:49 - values let's talk about another uh
426:52 - special Tri triangle which which is
426:54 - nothing but um
426:57 - 30
426:58 - 60 90° triangle
427:02 - 90° diory angle and of course this is a
427:06 - acute angle okay so when
427:10 - you when you have
427:13 - this uh let's let's let's print it out
427:17 - yeah so over here you have this and
427:19 - let's say let's say for for sake of an
427:21 - example your
427:24 - uh uh your uh
427:28 - 30° 60°
427:32 - 90°
427:35 - okay adjacent side of 30° is < tk3 which
427:38 - is approximately equals to
427:40 - 1.73 and it is one the the the the the
427:44 - the height or AC is sorry the height
427:48 - will be one and the hypotenuse is two
427:51 - because this is all because mainly
427:52 - people what they do they put that into
427:54 - the base and that is wrong because the
427:56 - hypotenuse is also always the longest
427:59 - side okay now let's do one thing let's
428:02 - apply the ca so K TOA on
428:09 - 30° okay for the 30° angle for the 30°
428:14 - angle okay so go ahead apply the ca for
428:18 - the 30° angle then go ahead and apply
428:21 - the Circa and the reciprocals as well
428:25 - and the reciprocals are well for the 60°
428:28 - angle and you are good to go and you are
428:30 - and you'll be getting your trick values
428:33 - and that that that will be yielding to
428:35 - around so much of trig values in no time
428:38 - which you got it okay so I want you to
428:41 - do these two things taking out and write
428:43 - the answer in description box uh sorry
428:47 - I'm saying in comment Box about the the
428:50 - the the the all the all the trig value
428:52 - and the reciprocals for the 30° and all
428:54 - the all the all the tri function with
428:57 - the reciprocals for 660 de okay
429:02 - cool so we had done lots of practice
429:04 - okay but you may think yeah you can't we
429:07 - do for greater angle because all these
429:09 - are cute angle can't we go ahead talk
429:11 - about the uh the the the large angles
429:15 - like maybe
429:16 - 270° okay so maybe maybe bit more than
429:20 - that something like cosine of pi/ 3
429:23 - cosine of maybe 3 Pi or sine of 3 pi/ 3
429:29 - sine of 7 High over six uh maybe
429:34 - anything anything can be happened okay
429:37 - so how you go ahead and do that how you
429:41 - go ahead and do that so you can just uh
429:43 - maybe what is the sign of 2
429:46 - 210° what is this cosine of 120° there a
429:50 - lot more things so for that for that we
429:54 - have something called as unit circle you
429:57 - have we have something called as unit
430:00 - circle which which is very very very
430:02 - very important and it may happen some of
430:05 - you may think he you I cannot remember
430:07 - unit circle because many many of you has
430:10 - an enemy with a unit circle in your I
430:12 - think class 9th and and class 10th I
430:14 - think most mostly in class 10th uh so so
430:17 - don't worry about it very very trick is
430:19 - available online about remembering it
430:21 - and it's very it's you don't need to
430:23 - remember it it makes sense as well
430:25 - sometimes okay if if you think about
430:27 - geometrically don't think about
430:29 - remembering everything you can just make
430:30 - sense of it because it's it's it's it's
430:33 - it's kind of U very very important in
430:36 - your Calculus Journey because these are
430:39 - the very very frequent radians which you
430:41 - get or degrees which you get in your
430:43 - whole calculus
430:45 - Journey okay so what I'm going to do is
430:48 - to introduce to you a unit circle I'm
430:50 - not going to give you a trick for
430:52 - remembering the unit circle but I will
430:54 - leave a link in the video description
430:56 - about the trick for remembering this but
430:59 - as always Google is up for you you don't
431:01 - have to worry about it okay cool so um
431:06 - let's go ahead let's let me show you uh
431:09 - let me show you the unit circle let me
431:12 - show you the unit circle where is my
431:14 - unit circle here here we go so just
431:17 - going to take in between make it a big
431:21 - okay so this is your unit circle this is
431:23 - your unit circle I'm just going to make
431:25 - it a little bit big yeah so this is this
431:28 - this is your unit circle and very very
431:30 - uh uh scary or disastrous unit circle
431:34 - and and it's very very important as well
431:36 - so uh I'll just introduce you so that
431:38 - every everyone is familiar here you have
431:41 - three things the first thing is
431:44 - coordinates second thing is you have the
431:47 - uh the radians okay of the coordinates
431:51 - and then you have the degrees angles
431:54 - angles in radians and angles in degrees
431:58 - okay so you have the coordinate so we
432:01 - talk talk talk about that
432:05 - coordinates you have angles in radians
432:10 - in
432:11 - radians and you have angles in
432:15 - degrees in degrees okay so first of all
432:19 - let's let's let let's talk about this
432:21 - one uh let's let's talk about this one
432:24 - so this these go first of all over here
432:26 - you have y AIS and x-axis so uh the the
432:30 - the radi with this origin to this point
432:34 - the the radius of the circle is r = 1 R
432:38 - = to 1 so the coordinate at this point
432:41 - will be 1 comma 0 will be 1A 0 why 1
432:44 - comma 0 1 on xais and it's 0 XIs 0 okay
432:48 - so 1 comma 0 it is the same 0 0a 1 on x
432:52 - axis is z so radius is areal to 1
432:55 - everywhere so it will be 0a 1 so this is
432:59 - y axis this is- one this is xais this is
433:02 - 0 comma minus1 so these are coordinates
433:05 - the same way these are also coordinates
433:07 - this for x-axis and this is for y axis
433:10 - this is for x-axis this is for y AIS
433:13 - okay and you may think how you have come
433:15 - up with these kind of uh coordinates how
433:19 - you come up with this it's it's
433:20 - basically uh more about geometric
433:23 - understanding you can go ahead see if if
433:26 - you go in much more detail as a bit out
433:29 - of the bound of this video because we
433:31 - just reviewing it so over here which
433:33 - you're seeing over here X and Y
433:34 - coordinates so these these are all the
433:36 - things which you're seeing outside the
433:37 - circle are the
433:40 - coordinates inside which you are seeing
433:43 - this Pi what whatever the fraction which
433:45 - you're seeing uh is the radians is the
433:48 - radian okay so over here you have 2 pi
433:52 - you have 2 pi and then pi over 6 pi over
433:56 - 4 pi over 3 for for that coordinate so
433:58 - there are three over here and Pi / 2
434:00 - then go ahead we have these all the
434:02 - things are the are the for the for for
434:05 - where the radian is 4 Pi or 3 the
434:07 - coordinates are this 5 5 pi over 4 the
434:10 - coordinates are this okay so these are
434:12 - the things the next thing is you have
434:15 - the degrees you have the degrees you
434:18 - have the radian you have the degrees
434:20 - 120° 135° 90° 6 60° maybe you all
434:25 - remember using a pro protector okay it
434:28 - is very very uh familiar to you so I
434:31 - hope that's not a big deal now but
434:33 - there's one thing which I want to tell
434:34 - you is how do you remember these things
434:37 - so first of all what you do you have the
434:39 - three three points over here you give
434:41 - two two
434:43 - two two all the denominator will be
434:47 - given two okay and now uh 1 2 3 1 2
434:55 - three okay you you do the square root
434:58 - every numerator if you put it does not
435:00 - require it < tk3 < tk2 < tk2 < tk2 and
435:04 - of course it does not require so you get
435:05 - this so these these some kind of tricks
435:07 - for remembering it it's very very useful
435:10 - in this okay so it's it's very very
435:13 - useful as well okay uh so you you
435:16 - remember these coordinates these Pi is
435:18 - the 1/3 Pi / 6 is 1/3 of Pi / 2 so you
435:23 - need to only remember some of the
435:24 - coordinates and some of the radians only
435:27 - that and using that you are getting uh
435:29 - using that you can get more radi or
435:31 - whatever okay so these are there are
435:33 - some tricks which you which you need to
435:35 - remember uh maybe you can search online
435:37 - otherwise the link is in the description
435:39 - Box about the trick for remembering this
435:42 - these things okay so you have the
435:43 - radians you have the degrees okay now
435:47 - what so every coordinate X and so every
435:50 - coordinate is a pair of X and Y okay x
435:54 - coordinate and y coordinate but it is
435:56 - indicating this is this cosine of theta
436:00 - and S of theta so X indicates the cosine
436:04 - of that maybe the radian or or or an
436:07 - angle and S maybe this Theta can be the
436:10 - radiant or that uh or that angle okay so
436:14 - let's let's say for for the S sake of an
436:16 - example let's take an example you told
436:18 - you want to find out the cosine of Pi /
436:22 - 3 of Pi / 3 how are you going to find it
436:25 - so you search 5 over3 over here what is
436:29 - the x coordinate is what is the x
436:31 - coordinate because x coordinate
436:32 - indicates the cosine of theta which is 1
436:34 - / 2 what is the S of < / 3 so y
436:39 - coordinate is root < tk3 / 2 okay so
436:43 - these are thing these are the trig
436:45 - values okay which is which is very very
436:47 - useful maybe in your examination or in
436:50 - calculus you can take out anything with
436:52 - this say for an example you're told to
436:55 - find cosine of pi/ 6be cosine of pi/ 6
437:01 - so over Pi / 6 is will will be the the x
437:05 - value is < tk3 < tk3 / 2 let's take an
437:09 - example you told s of 3 piun / 6 uh 3 3
437:14 - piun over 4 so what it will be it will
437:16 - be nothing but minus sorry < tk2 /2
437:20 - because that indicates the y coordinate
437:22 - so coordinates indicates the pair of the
437:25 - cosine and the sign of that angle or the
437:29 - radian okay so some of the interesting
437:31 - thing which I'm to show you to you is
437:33 - what is the tangent of 3 piun / 4 what
437:38 - is the tangent of this 3i 4 so tangent
437:41 - you all know tangent is equals to S of
437:44 - that 3 piun 4 / the cosine of 3i 4 okay
437:49 - so s of theta divid by the cosine of
437:51 - theta so it will be the S of theta 4
437:54 - will be < tk2 / 2 okay- < tk2 / 2
437:59 - because that's Co sign is which which is
438:02 - min-2 which which you can see over here
438:04 - so the the the the denominator cancel
438:07 - which will be minus 1 the tangent of the
438:10 - 3 Pi 4 will be minus one so this all
438:12 - about manipulations more all all about
438:15 - other things which you need to which you
438:16 - need to just uh apply some logic and you
438:19 - will getting getting your trig value so
438:20 - this gives you lots of trig values okay
438:25 - now what I'm going to do now what I'm
438:27 - going to do now what I'm going to do is
438:31 - um is to do one thing uh first of all
438:36 - uh what is what is the
438:41 - cosine what is the cosine what is the
438:46 - cosine of 3 pi over here we we only
438:49 - still have 2 pi okay but what is 3 Pi so
438:53 - it it here the radian is 2 2 2 pi so
438:57 - what is the cosine of 3 Pi so how how is
439:00 - construct at 1 Pi okay then 2 pi then 3
439:05 - pi over here it will come over here the
439:07 - 3 Pi the cosine is always referred to
439:10 - the minus one so the answer will be
439:11 - minus one how first of all here is your
439:13 - Pi okay you rotate there is two first of
439:17 - all you go over here you rotate half
439:19 - circle Pi then 2 pi then 3 pi over here
439:24 - so 3 Pi will becoming minus so what is
439:27 - the S of 3 Pi which is zero because the
439:31 - y coordinate is zero so it is not matter
439:35 - whatever you you can have it does not
439:37 - matter okay now there is one last thing
439:40 - which I want to
439:41 - tell which I want to tell is there is
439:45 - one last thing which I want to tell is
439:47 - what is the S of - piun / 4 what is the
439:52 - S of- piun / 4 which will be nothing so
439:57 - over Pi but this is pi but there is
439:59 - minus so you here it is in second one so
440:02 - you what you do you start going you
440:04 - start going into the negative Direction
440:07 - okay so leaving one you go to two Okay
440:11 - because here is here the second
440:12 - coordinate you go over here 2 one and
440:14 - that is that is so sign of my - piun 5
440:18 - will be sine of 7 piun over 5 4 okay so
440:23 - that is nothing but two okay so uh root
440:28 - so sorry it's minus 2 2 over 2 okay so
440:32 - this is how you go in the negative
440:34 - direction if it is over here then you go
440:36 - from here so this is how everything is
440:38 - working you using the unit circle and I
440:41 - hope that you understood very very
440:42 - intuitively and I hope that's very not a
440:45 - not a big deal to understand these
440:47 - things so this is a whole thing and and
440:49 - and and basically you will see whole
440:52 - calculus and revolving around this okay
440:55 - most of the calculus problems TR TR
440:58 - calculus problems
441:00 - cool so I just want to talk about the
441:03 - last thing which is very very useful
441:05 - over here is how do you graph s cosine
441:09 - and tangent maybe you can search online
441:12 - because but I I just show you the graph
441:15 - of s of yal s of X looks like so here is
441:22 - your thing so it it it will look
441:25 - something like this it will look
441:27 - something like
441:29 - this it will look
441:31 - in cyclic form okay in cyclic form it it
441:34 - will look in cyclic Forum something like
441:36 - this okay so it looks in cyclic form but
441:41 - the same way your your your cosine will
441:43 - be will be different will be different
441:46 - because over here it it it is some some
441:49 - something y s of X something like this
441:51 - but what is the cosine cosine will be
441:53 - let let let let little bit different a
441:55 - little bit different it will be
442:02 - okay it will
442:10 - be okay so it will is cosine y = cosine
442:14 - of x will be something like this and and
442:17 - and this these are the graphs which you
442:18 - will see later a lot when when when when
442:21 - I will teach I will take these kind of
442:22 - graphs about I'll go in different
442:25 - differentiations very very important
442:27 - okay so I hope that you understood the
442:29 - trigonometry whatever I taught and um
442:32 - it's just a review there there a lot to
442:34 - learn in trigonometry there a lot to
442:36 - practice in trigonometry like inverse
442:38 - strict functions and ASM tootes we have
442:42 - lot more things which you can explore
442:44 - please please please go ahead see some
442:47 - tutorials if you're going to go ahead
442:49 - deeply in trigonometry these are things
442:51 - which I need to revise before going to
442:53 - the calculus now we are ready now we are
442:56 - ready to go in calculus from the next
443:00 - very we'll be talking about limits and
443:02 - we completing I think limits in three to
443:04 - four videos I think only three videos
443:05 - will required for limits short short
443:08 - videos we completing limits and then we
443:10 - are good to go and then we start with
443:12 - differentiation okay so and then we will
443:15 - go ahead talking about solving different
443:17 - take take Taking out the derivative like
443:19 - quotient rule chain Rule and a lot more
443:21 - and then we'll go ahead uh there's also
443:24 - power rule which I can't forget about
443:27 - that so there's um so about different op
443:30 - after differentiation I think we'll go
443:31 - ahead with integeration okay so I hope
443:34 - that you understood I'll be catching up
443:35 - the next video till then bye-bye have a
443:37 - great
443:41 - [Music]
443:49 - day hey everyone welcome to this lecture
443:52 - from this lecture we'll be actually
443:53 - starting off with uh limits and and
443:56 - limits is one of the most important
443:58 - topic which I cannot leave we will not
444:01 - too much talk about limits we'll not do
444:02 - that much kind of examples but yeah I
444:05 - will give you a a brief about limits and
444:08 - what the limits are and there there are
444:10 - lot more lot of the things which we'll
444:11 - talk about as well as we'll talk about
444:14 - uh uh how to evaluate the limits using
444:18 - various various techniques which we'll
444:20 - study in the next video basically in
444:22 - this video I'll just give you what
444:23 - exactly the limits is okay so we'll we
444:26 - we we see lot lot of examples to
444:29 - understand uh why this is called the
444:31 - microscope of maths so it's this very
444:34 - very nice uh nice things which have
444:37 - invented so far uh in the history of uh
444:41 - math and I think that differentiation
444:45 - the definition the formal definition of
444:47 - differentiation will'll Define using the
444:49 - limits property okay so now let's get
444:52 - started with one of one of the example
444:55 - which is in front of you which is that
444:57 - maybe you are familiar with this plot it
445:00 - is the the parabola plot if you know
445:02 - because we had a talk on this and how
445:04 - you plot you just plug in the x value
445:06 - just put the point on the X x coordinate
445:08 - and the y-coordinate and then you join
445:10 - that the points so this is what you get
445:12 - and this is not a best Parabola diagram
445:14 - in the history of math uh but yeah this
445:17 - is the basic uh basic stuff which you
445:20 - seeing over here now what I'm going to
445:23 - what I'm going to do is uh uh make a pie
445:27 - wise function but before that what I'm
445:28 - going to do is to just just uh talk
445:31 - about the this function and then we'll
445:32 - make a PE wise function but before that
445:34 - what I want what I want is to find the
445:38 - value I I I I want to find F of two F of
445:42 - two F of two so what it will be so we
445:47 - give to the X and this will be nothing
445:50 - but four assume that this is a four okay
445:52 - so just want to make it some over here
445:55 - so it is a four okay so this is nothing
445:57 - but four but what do you I say you have
446:01 - a function G of X you have a function G
446:04 - of X you have a function G of X you have
446:07 - a function G of X where you uh first of
446:10 - all uh what it X squ for every values
446:15 - except xal to 2 except x = 2 there is x²
446:21 - okay and another is when when when uh
446:24 - the input value is two then the output
446:27 - will be one when X = to 2 then the Y
446:29 - value will be be one uh in the first is
446:32 - telling x² in every input values except
446:36 - two and one for for the input value
446:39 - equals to two so that is these functions
446:42 - are nothing but called the pie wise
446:44 - function pie wise function uh which you
446:47 - which you might have observed already
446:49 - and I hope that you had already so now
446:52 - if I want to plot it over here this plot
446:55 - G of x if I try to plot it the G of X so
446:58 - what it the what the oh my God yeah so
447:02 - uh what if I if I want to plot it so for
447:05 - plotting it let me just remove this one
447:08 - because I oh my God I don't want to do
447:11 - this uh I don't know how to select the
447:13 - good uh the Eraser let's use very simple
447:16 - eraser let's first of all remove this
447:19 - and there is a discontinuity okay so if
447:22 - fals to two if fals to 2 then if input
447:27 - value equal to two we just saying that
447:29 - it is it should it should be one okay so
447:33 - this is your function of your G of X
447:36 - this is your function of your G of X
447:38 - where we are saying it this is a
447:40 - function this is a graph of x² this is a
447:43 - graph for X squ but we had told there is
447:46 - one discontinuity which we'll talk about
447:48 - today about discontinuity but there is
447:50 - one discontinuity in this function and
447:52 - you're saying if at xal to 2 make that
447:55 - to two and this x² does not work for x x
447:58 - is equals to 2 okay okay so this is a
448:01 - function this is a graph of your
448:02 - function G of X okay uh this this pie
448:06 - wise function this is how it looks like
448:08 - now coming to the next part is what is
448:12 - I'm just asking one thing what is the
448:14 - limit what is the limit what is the
448:18 - limit of your function G of X of your
448:21 - function G of X when when X approaches
448:25 - to when X approaches two not exactly two
448:30 - when X approaches two from the right
448:32 - hand side and the left hand side okay so
448:35 - it it may be confusing to you what
448:37 - exactly I'm saying what exactly I'm
448:39 - saying what is the limit of G of X when
448:44 - X approaches 2 from the right hand side
448:47 - and from the left hand side so let's
448:48 - start with the from the from the left
448:50 - hand side so let's say the let's say the
448:53 - input values the input so let's start
448:55 - from the from this value so it gets
448:58 - closer and closer to two okay so it
449:00 - approaches Clos and Clos it so we simply
449:02 - write limit of G of X when X approaches
449:06 - to so this is this is your limit uh
449:09 - notation to write it out so basically we
449:12 - are telling uh here we are telling what
449:15 - is the limit of when X approaches two
449:18 - from the left side okay so when X
449:21 - approaches two from the left side it
449:24 - seems to be approaching four it seems to
449:26 - be approaching four we are not taking
449:28 - exactly F of two because that is
449:30 - undefined at that point the function x²
449:32 - is undefined but basically there is
449:35 - discontinuity and we are we are getting
449:37 - closer and closer to two from the left
449:40 - side for for for example we are taking
449:42 - the value 1.9 1.999 1.999 1.999 N9 we
449:48 - are not getting exactly to two but we
449:50 - taking these values and taking the
449:52 - square of it okay so it seems to be
449:54 - approaching fold how I'm perceiving that
449:57 - so let's do one thing let's try to give
450:00 - the let's try to do from the left hand
450:02 - side first of all so we have an x value
450:04 - and and we have a g of X okay we have a
450:07 - g of X so you give 1.9 to XX when you
450:11 - square it up you get
450:12 - 3.61 okay and because it is defin else
450:17 - everywhere except X exactly equals to
450:19 - two so at 1.99 it will be
450:24 - 39601 that is a square of it okay so we
450:27 - give
450:28 - 1.999 so that is
450:31 - 3.99 601 okay we are not taking exactly
450:35 - it's technically do not can get exactly
450:37 - two four but it if you you use the
450:40 - calculator 1 1999999 it will just take
450:43 - out the square it will just round of
450:45 - that but actually this technical not
450:47 - true okay so over here you you're are
450:50 - getting you when when you're are
450:51 - approaching two when you're are
450:53 - approaching two when you are approaching
450:55 - xal 2 from the left hand side it seems
450:58 - to be approaching four your function G
451:01 - of X seems to be approaching four let's
451:03 - take an example of this so when you're
451:06 - approaching two from this side when
451:08 - you're approaching two from the left
451:09 - side it seems to be approaching four
451:12 - okay it's not exactly taking as a two as
451:14 - an input we saying if the the the the
451:18 - limit of a g of X tells you when X
451:20 - approaches 2 from the left hand side and
451:23 - when from from the left hand side so
451:26 - over here when the limit uh of G of X
451:29 - when X approaches 2 is approaching okay
451:32 - limit of of G of X when x x approaches 2
451:35 - is approaching four the fun is
451:37 - approaching four so it is approaching
451:40 - four it is approaching four it is
451:45 - approaching four so when you when you
451:46 - get Clos and closer to two from the from
451:49 - from from the left hand side it seems
451:51 - the function G of X is approaching Clos
451:53 - and Clos it's it's it's approaching four
451:56 - it's not exactly four but it's
451:58 - approaching four okay now let's see the
452:00 - same way going from the right hand side
452:03 - so let's go with the right hand side so
452:05 - when we try X and G of X we give 2.1
452:12 - 4.41
452:15 - 2.01 uh
452:17 - 4.41 so from the right hand side as well
452:20 - from this side as well from this side as
452:23 - well from this side as well it seems
452:27 - from this side as well we are
452:28 - approaching to from this side this seems
452:31 - like it is approaching so it seems like
452:33 - it is also approaching four it is also
452:36 - approaching four so the limit from the
452:39 - right hand side limit of f ofx f ofx =
452:43 - to 2 of G of x from the right hand side
452:46 - is equals to 4 and the limit limit of x
452:50 - approaches 2 from the left hand side is
452:53 - also four so the limit of G of X when X
452:56 - approaches 4 2 is exactly equals to 4
452:59 - and this is how you take out the limits
453:00 - of a function okay and this is damn easy
453:03 - we have we had to Made made use of table
453:05 - or Forum using calculator and so sorry
453:09 - and then we are done okay this is what
453:12 - limits are limits are nothing uh it's
453:16 - it's it's it's a function limit of a
453:18 - function limit of a function if it exist
453:22 - because limit cannot exist as well I
453:24 - will tell you the reason lat later on so
453:27 - limit cannot exist as well for some x
453:30 - value C and we denote so we're just
453:32 - going to write the formal let's write
453:34 - let's make a new page yeah so I'm just
453:36 - going to write the for formal definition
453:38 - the formal definition says that limit
453:41 - not no it is not a formal definition but
453:45 - um this is an informal definition but
453:47 - just now for the
453:48 - definition okay the limit of a function
453:53 - limit of a function limit of a function
453:58 - if it exists a of course if it
454:02 - exists if it exists limit of function if
454:05 - it exists for some x value for some x
454:08 - value C for some x value C okay for some
454:13 - x value C to the height of a function to
454:15 - the height of a function to the height
454:18 - of a function the height of a function
454:21 - gets closer and closer gets closer and
454:24 - closer as we saw that is gets closer and
454:26 - closer closer and closer
454:31 - to uh as X gets Clos and closer gets
454:36 - Clos and
454:37 - closer to to the to see gets closer and
454:41 - closer to see from the left side from
454:45 - the left and the right side from the
454:49 - left and the right side okay so for
454:52 - example this is your function this is
454:54 - this is your function assume that this
454:56 - is your function and this is there is a
454:59 - discontin it over here and you're
455:01 - telling that okay what is the what is
455:03 - the value of x so for example it's
455:06 - something like this okay and you're
455:08 - telling that's two so what is g of X
455:11 - when the what is the limit of what is
455:13 - the limit what is the limit of x
455:15 - approaches to of your function G of X
455:18 - okay so when X approaches to from the
455:19 - negative side sorry the left side and
455:21 - the positive side from the from the from
455:23 - the right side it seems to be
455:25 - approaching from the first of all from
455:27 - here and then it seems to be approaching
455:29 - for exactly it is four okay so it's
455:32 - check the gets the x value gets Clos and
455:34 - closer from the left and the right side
455:36 - and then you decide and then if the both
455:38 - are equal then you say okay that's the
455:40 - limit which exactly equals to
455:43 - four okay so now we had seen the one
455:46 - example and I hope you had got a very
455:48 - good idea about limits and we'll I will
455:51 - just give you the formal definition of a
455:53 - limits because it's very important for
455:55 - you as well to understand what exactly a
455:57 - limit is okay so so what I'm going to do
456:00 - now is to take another example so that
456:02 - you could get a more uh more more good
456:05 - definition for you at least uh we we
456:08 - we'll see that okay so we'll just give
456:10 - you the good good definition of the
456:13 - limits and it's very very important as
456:15 - well uh mainly if you uh for for for for
456:18 - for forgetting more about what exactly
456:20 - the limits is
456:24 - okay cool so uh what I'm going to do is
456:28 - take another another function take
456:30 - another function and the function is uh
456:32 - the function is the function is uh F ofx
456:36 - = 3x + 1 so what I'm going to do is to
456:39 - take a y i just to take a function three
456:43 - y = 3x + 1 y = to 3x + 1 so this is your
456:49 - function uh F ofx this is your function
456:52 - f ofx okay and you will identify the
456:55 - need of the limits from this function
456:59 - and you you will get to know the need
457:00 - what's exactly the need of limits from
457:03 - this function so let's get stress so
457:05 - let's what what do I ask you to do take
457:08 - your graph paper try to plot this
457:10 - function by putting X values and try to
457:12 - put the coordinates on the your graph
457:13 - paper and then you are done okay so what
457:17 - I'm going to do is to make the make a
457:19 - quick quick uh diagram of this function
457:23 - I don't know it it is it is a bit hard
457:25 - to make but no problem I'll try to make
457:27 - it as much as I can so just want to
457:30 - is it is uh just wait for a second I'm
457:34 - just checking the diagram yeah so what
457:37 - I'm going to do is to make an X and Y
457:38 - plane so I'll just put this make let's
457:42 - make in blue because it works okay so
457:45 - this is your X sorry y cord y plane y
457:49 - AIS and this is your
457:51 - x-axis okay what I'm going to do now is
457:54 - to just plot it now is two is approach a
457:59 - 7even so one two oh my God what is this
458:04 - oh I I I I got to know about this now
458:07 - one
458:08 - two uh let's let's let's make it a
458:11 - little bit more we two 3 4 5
458:18 - 1 2
458:21 - 3
458:23 - 4
458:25 - 5
458:27 - 6 7 okay so this is your uh let's let's
458:32 - make it inal way now when I'm going to
458:36 - try to put the value of one so let's put
458:38 - the value of one over here that is four
458:40 - and it is so that is four okay put the
458:43 - value of two that is that is nothing but
458:46 - uh seven I think so yeah that is seven
458:50 - okay so when I when I try to just join
458:52 - the points I'm going I try to just join
458:54 - the points I'm just trying to fully join
458:56 - the points okay so that is
459:00 - this is what I get when joining it okay
459:03 - so it is not a best function in the
459:05 - history of math but yeah that's what I
459:07 - can draw in just now with quick graph so
459:10 - this this this is the function y = 3x +
459:13 - 1 so this is this this is a function y =
459:16 - 3x + 1 coming to that now this is a
459:20 - function now what I want to know is
459:23 - limit limit limit of your function let's
459:28 - say f ofx let's give the name of this
459:30 - function f ofx limit of function limit
459:33 - of a function f ofx when X approaches
459:36 - when X approaches
459:37 - 2 what what what it will be you can
459:40 - simply it it is not it is continuous it
459:43 - is we we have this this is not a pewi
459:45 - function it is continuous and over here
459:49 - we don't have any discontinuity we don't
459:51 - have any whole over here okay it is just
459:54 - a point just just for Dra drawing the
459:56 - diagrams but but over here your uh this
459:59 - function is continuous and if you go
460:02 - when when when when when when we
460:04 - approach from the negative side as of
460:06 - the positive side and even when we get
460:08 - when we get x = 2 even when we get x = 2
460:14 - so you'll be getting uh y = to 7 and
460:17 - that is the fully valid according to
460:19 - your function that is defined that is
460:22 - defined so the limit of your F ofx when
460:25 - X approaches to from the both the side
460:27 - is seven when X approaches to from the
460:29 - both the side from the from from this
460:30 - side and from that side even it gets two
460:33 - it will be defined because your function
460:35 - is not set it that okay your Y is 3x + 1
460:40 - is not equal to 2 so it is not something
460:42 - like this it's not something like this
460:44 - it is defined at at X = to 2 so the
460:47 - limit of your function you can simply
460:49 - put the value of x and you'll be getting
460:50 - your limit value over here so you can
460:52 - simply put the value over here and then
460:54 - you'll getting your uh the limits even
460:56 - when when you say it it goes from the
460:58 - left side and the right side and it
461:00 - seems to be approaching seven okay
461:03 - coming to that now we have seen this now
461:05 - what I'm going to do is a little bit
461:06 - twist it so that so that you could get
461:09 - uh at least the idea of why limits are
461:12 - too much useful in this era so what I'm
461:15 - going to do what I'm going to
461:17 - do is um is uh say say for say for an
461:21 - example say for say for an example uh uh
461:26 - I'm just going to take a take a take
461:28 - this and then I'm just make a hole over
461:30 - here okay so just make a hole over here
461:34 - oh my God this is what I made the hole
461:37 - okay so now now the function is defined
461:40 - this function y is defined everywhere
461:43 - except X = to 2 and when it's xal to 2
461:47 - it is one it is one okay so so so so
461:52 - what is so what is the limit of f of x
461:55 - when X approaches 2 it's seven It's s of
461:58 - course it's seven but you you will see
461:59 - the but you'll see the the main idea
462:01 - that if your function has discontinuity
462:03 - it will tell you that if we get closer
462:06 - and closer to two from both the side
462:08 - from the left side and the right side
462:10 - say say say for say for an example you
462:13 - have X so you put 1.5 so you put it you
462:18 - go from left side that is you have uh y
462:22 - function so that is one when you put one
462:25 - you'll get four when you put 1.5 you get
462:27 - 5 5.5 when when you put 1.9 you get 6.7
462:31 - you get 1.99 you get 6.97 you get 1.999
462:36 - you get 6.99 97 so this is this is
462:39 - getting closer and closer to seven if if
462:42 - you get closer and closer to from the
462:43 - from the left hand side it is getting
462:45 - Clos and closer to seven coming to that
462:47 - from the negative side coming to that
462:49 - from the sorry from the right right
462:51 - right hand side so that is that is that
462:54 - is nothing but
462:56 - 2.01 that is 7.3 2.01 that is 7.03 2.1
463:03 - 7.3 so this is also seems to be
463:05 - approaching 7 from the from from the
463:07 - right hand side so the limit is seven
463:10 - okay so this is how you you you you take
463:13 - out the limits and I hope that it's
463:15 - completely making sense at least to you
463:18 - okay coming to that later on now now
463:22 - what I'm going to do what I'm going to
463:24 - do is uh is to just go ahead
463:29 - give you the the the the topic which is
463:32 - one-sided limits okay so you all have
463:35 - seen the one-sided limits but I this
463:38 - it's my job to make you familiar with
463:41 - these stuff okay so I'm just going to
463:43 - take this one this diagram I'm just
463:46 - going to take only this diagram just
463:48 - going to take only this diagram okay now
463:51 - uh just let me just make it in the new
463:54 - page so that I could make you understand
463:56 - a better way okay so when X approaches
463:59 - to from this side and then that side
464:02 - okay let's make a new function let's
464:04 - make a new function let's make a new
464:06 - function don't ask me what's the name of
464:08 - the function just just let's draw a
464:09 - random function so that just just we can
464:11 - understood visually let's call that
464:13 - function P of X okay let's call that
464:15 - function P of X of course this is a peie
464:18 - wise function this is a pie wise
464:19 - function um it will be like this x² for
464:24 - and I will just tell you later on okay
464:27 - coming coming to that so let's make uh
464:30 - let's let's make a
464:32 - function this one uh let's make a
464:35 - function let me draw a DOT line at least
464:37 - a line uh something like this and then
464:42 - something like this I want it a little
464:44 - bit big so that everyone could be able
464:45 - to see it now uh just going to take this
464:49 - now what I'm going to do
464:51 - one uh it's it will be something like
464:55 - this
464:59 - okay so I just let's do it and this is
465:04 - from this side
465:12 - [Music]
465:37 - okay so this assume that this is your
465:39 - function uh this is your function it's
465:42 - just not a big big big function which
465:43 - you have ever seen and your eror
465:48 - okay let me make a little bit so that
465:50 - everything
465:52 - is yeah okay so you have the okay so you
465:58 - want to you have a piecewise function
466:00 - have peie wise function so assume that
466:02 - this is a p of X this is a p of X which
466:04 - is a which is a piecewise function which
466:07 - you ever seen in your life uh again just
466:10 - just just assume that that is a function
466:12 - and and over here and and over
466:17 - here uh over here uh you
466:21 - have
466:23 - limit what is the limmit of X
466:26 - approaches uh 2 okay so what is the
466:29 - limit of x approaches 2 from the
466:32 - negative side of your function f of e of
466:35 - X so when you when you get closer and
466:37 - closer to two from the negative side
466:40 - means from the left side it gets Clos
466:43 - and closer to five that gets Clos and
466:44 - closer to five it gets Clos and closer
466:46 - to five that is nothing but five okay so
466:49 - when you get Clos and closer to two from
466:51 - the negative side that's got Clos and
466:52 - closer to
466:56 - five and when you get closer closer to
466:59 - seven then when you get closer limit of
467:02 - P of x from the right hand side from the
467:05 - right hand side you get it is getting
467:07 - closer and closer to two that is nothing
467:09 - but two so over here you you you're
467:13 - taking the limit from the from the left
467:15 - hand side and taking the limit from the
467:17 - right hand side and these both uh right
467:20 - right hand limit and left-handed limit
467:22 - are not equal are not equal are not
467:27 - equal okay are not
467:31 - equal and so so that's why uh this limit
467:34 - does not exist this limit does
467:39 - not exist why because the limit because
467:44 - the limit from X approaches 2 from the
467:47 - negative side of a function P of X is
467:49 - not equals to the limit of your P of X
467:53 - when X approaches 2 from the positive
467:55 - side it's not equals to which which you
467:58 - can clearly over here which you can
467:59 - clearly see over here that this the X
468:02 - approaches to from the left hand side
468:04 - and X approaches to from the right hand
468:05 - side is not equal but in previous cases
468:08 - which you have seen they both were equal
468:10 - so there the limit exists but here the
468:12 - limit does not
468:14 - exist okay so this is what the whole
468:17 - idea of the one-sided limit when someone
468:20 - ask you to evaluate something like this
468:21 - get to get the limit from the negative
468:23 - side you're just going to go go uh get
468:24 - Clos and closer make a table format make
468:26 - a TBL format and then go ahead but we'll
468:29 - see how to evaluate the limits in our
468:31 - next section of of of of a function so
468:33 - don't worry about that
468:35 - okay I hope that everything is clear and
468:39 - now what I'm going to do is uh have have
468:42 - a small understanding of uh of of of a
468:45 - function again a pie wise function this
468:48 - is a peie wise function so let's let's
468:51 - let's take another pie VI punch so what
468:54 - I'm going to do uh it's the it will tell
468:57 - you the idea of infinity which will talk
468:58 - about we will talk about this in our
469:00 - next video a little bit more way I just
469:02 - going to go to give you the intuition
469:04 - about this okay so this is your this is
469:08 - your something like this okay assume
469:12 - that this is 1 2 3 4 5 6 okay um this is
469:19 - also 1 2 3 4 5 and this is also 1 1 2 3
469:27 - 4 5 5 1 2 3 4 okay now what I'm going to
469:32 - do is to just make uh is to just make us
469:36 - or
469:37 - here okay to make something like this
469:41 - make something like this and when we say
469:44 - when we say that when X approaches when
469:47 - the limit when the limit of when the
469:51 - When when when when the limit of f ofx
469:53 - let's say the function f ofx when X
469:56 - approaches negative uh from the from
469:59 - from the right right hand side from the
470:01 - right hand side even even from the left
470:03 - let's say from the left hand side from
470:05 - the left hand side when the limit get
470:07 - close and close to left hand side so it
470:09 - says that it's it's getting to Infinity
470:12 - it's getting to Infinity okay because
470:14 - you have a vertical asymptotes over here
470:17 - vertical asymptotes okay so it's getting
470:19 - to infinite so we say that that is a
470:22 - Infinity that is the infinity okay so
470:25 - this is a general idea when a limit can
470:28 - be in
470:29 - okay so don't worry about this will talk
470:31 - about vertical ASM tootes because this
470:33 - is a vertical ASM toote so we say that
470:35 - when when when when when the X approach
470:37 - is three it gets the limit of function
470:39 - gets gets to the infinity Okay cool so
470:44 - what I'm going to do now is talk about
470:46 - the last things of the topic which which
470:48 - which which we are left on is about
470:50 - continuity I think so yeah we'll talk
470:52 - about continuity but before that I want
470:53 - to give you a formal definition of a
470:55 - limit so that you could be under you
470:57 - could just write some somewhere so that
470:59 - everyone is familiar with it so the
471:02 - formal definition of a limit the formal
471:04 - definition of a limit the formal
471:05 - definition of a
471:07 - limit let F be the function let F be the
471:11 - function okay so the let let F be a
471:17 - function let F be a function let F be a
471:20 - function and C be any real number and C
471:24 - be any real number any real number
471:29 - okay limit limit of x approaches C of
471:34 - your function C is then C is the arrow
471:37 - number mainly we call it an arrow number
471:39 - which we which we have to approach exist
471:42 - this limit exists exists if and only if
471:47 - if and only if three conditions are made
471:50 - first condition limit of f ofx when X
471:55 - approaches C from the negative side
471:57 - exists
471:59 - exists second limit from X approaches
472:03 - see from the positive side
472:05 - exists and and third one is limit of x
472:10 - approaches C from the negative side of f
472:12 - ofx equals the limit of x approaches C
472:15 - of function f from the positive side so
472:18 - they should be both equal okay so this
472:20 - is so these are the three conditions
472:22 - where you all have seen in a real life
472:24 - examples which I just showed you about
472:26 - this example or maybe or this example
472:28 - Maybe Okay cool so now let's talk about
472:31 - the last thing which is nothing but
472:33 - continuity which is nothing but Contin
472:36 - nity so what I'm going to do is just is
472:39 - just spend some minutes talking on this
472:42 - uh talking on this uh so we'll talk
472:44 - about continuity so let me just give a
472:46 - definition
472:48 - continuity okay continuity a a
472:51 - continuous function basically the
472:53 - definition States a continuous
472:55 - function uh cont
472:59 - newest function is simply is
473:04 - simply a function with no gaps with no
473:09 - gaps or holes in
473:11 - between holes okay so a function which
473:14 - you can draw without without tick for
473:16 - example I can draw something like this
473:18 - so with it has no gaps okay or end holes
473:22 - in between okay so this is a this this
473:25 - is a this is a continuous function okay
473:28 - so we'll see some of the examples where
473:30 - where the functions are continuous and
473:31 - where the functions are not continuous
473:33 - so let's see some of the examples where
473:34 - the functions are
473:36 - continuous where the functions are
473:38 - continuous so just going to make it
473:40 - something this oh my God it's not
473:42 - correct so let's assume oh my God what
473:44 - is this what is this I think that I'm
473:47 - losing my mind I think so yeah so over
473:50 - here this this this this is a function
473:52 - this is function what I let's let's say
473:54 - p of X let's say p of X and this is
473:57 - continuous because you are drawing
473:59 - without taking out your hand and since
474:01 - one of one of the main thing about
474:03 - continuous function if you are making it
474:05 - making it draw without taking your
474:07 - pencil off the paper then it's
474:10 - continuous that's that's a that's
474:12 - another cool definition which is
474:13 - provided in some of the book okay so
474:15 - this this is some of that this is what
474:17 - you this is a continuous function let's
474:19 - draw let's draw uh something like this
474:21 - something like this it is also the
474:23 - continuous function so this is also the
474:25 - continuous function of the absolute
474:27 - absolute value of x
474:29 - okay so this is also continuous function
474:31 - but what are not continuous functions so
474:33 - let's take an example let's take an
474:35 - example of this function of this
474:41 - function the function states you have uh
474:44 - maybe something like this you have
474:46 - something like oh my
474:48 - God you have something you have
474:51 - something like this and let's do one
474:54 - thing let's just make a hole in between
474:57 - okay let's just make a hole
474:59 - and this is and this is not a continuous
475:02 - function okay another stuff is I will
475:05 - tell you the the good definition another
475:08 - another can be another can
475:10 - be uh let's take example of
475:19 - this okay this is uh I'm going to make a
475:22 - piecewise function I'm going to just
475:24 - make a pie wise
475:26 - function and this function is also not
475:29 - continuous function okay this this this
475:32 - function is also not a continuous
475:34 - function let draw vertical ASM tootes so
475:36 - this is also not a uh the the the the
475:39 - function which is continuous okay
475:42 - because we are you're taking off your
475:43 - hand and another stuff which I'm going
475:45 - to talk about is another pie wise
475:47 - function another pie wise function which
475:49 - is this one okay make whole there and
475:53 - let's
475:55 - make there's a hole over here so this is
475:57 - also not a continuous function okay so
476:01 - uh so when Whenever there there is some
476:03 - kind of piece the constraints it's it's
476:06 - it's difficult so uh basically you
476:09 - you're seeing the two functions so let
476:11 - me draw another function another same
476:13 - function which you all had already seen
476:16 - so that I could take one example of this
476:19 - uh something like this one and something
476:21 - like this one okay so these are the two
476:23 - functions which are not continuous so
476:25 - these two functions with the gaps are
476:28 - not not continuous everywhere these
476:30 - functions are not not I not say but the
476:32 - more precisely I would say the these two
476:35 - functions with the gaps are not
476:37 - continuous everywhere these two
476:38 - functions are continuous everywhere but
476:40 - these two functions are not continuous
476:42 - everywhere but sometimes a function is
476:44 - continuous everywhere it's
476:46 - defined okay such a function is
476:48 - described as a being continuous over its
476:50 - entire domain so if the function is
476:53 - continuous that that means that the
476:55 - function is continuous over its entire
476:57 - domain okay which means that it it's
477:00 - it's it's it's gaps or gaps occur at X
477:03 - values where the function is undefined
477:06 - okay so which you're seeing the function
477:07 - P of X is continuous over the entire
477:11 - domain is continuous over the entire
477:13 - domain which you're seeing over here
477:14 - it's continuous over the entire domain
477:17 - it has it don't have Gap in between but
477:20 - over here G of X on the other hand it's
477:23 - not continuous over its entire domain
477:26 - over its entire domain okay so G of X is
477:29 - not continuous over its entire domain
477:32 - maybe let's say for say for example you
477:34 - have this diagram so this this diagram
477:37 - so this function y is continuous over it
477:39 - in in entire domain except at Value
477:43 - except at xal to 2 because at xal 2 it
477:45 - has a whole so the function y is not
477:48 - continuous everywhere except x = 2 okay
477:51 - so this is how you say if if the
477:53 - function is a continuous or not maybe
477:55 - you say the it's you you say say that
477:58 - the function is continuous over its
477:59 - entire domain but basically as a more
478:02 - precise would say that okay the function
478:04 - is continuous everywhere except at this
478:06 - point okay so this was all about this uh
478:11 - continuity and I hope that you got to
478:13 - know what limits it's it's it's it's a
478:15 - good definition to to to to just take in
478:18 - your mind and and I hope that you will
478:20 - consider uh this uh in your own and I
478:23 - also hope that this limits got in your
478:25 - mind in the next section we'll talk
478:27 - about the evaluating limits and then
478:29 - we'll get on differentiation my favorite
478:32 - topic and then we'll talk about
478:33 - integration calcus over okay and then
478:36 - we'll go to probability Theory because
478:38 - it's probability theory is also very
478:39 - very important uh in this era and let me
478:42 - know that I'm that I was reading a book
478:45 - uh not what you want gift in a Christmas
478:48 - because I'm thinking that I should make
478:51 - a very very very comprehensive data
478:55 - science onee plan master plan or road
478:58 - map to complete and in in depth to get
479:00 - in Fang what do you say I'm not in Fang
479:03 - but I think many many of my students got
479:05 - into Fang uh so I think I'll be I'll be
479:08 - eligible I'll be help taking help from a
479:10 - lot of people to make that road map
479:13 - available okay so I think that if if you
479:16 - want that please comment it down below
479:18 - I'll be very happy to give you as a
479:19 - Christmas gift to make a full data
479:23 - science plan data scientist plan which
479:25 - goes from very scratch mass and this go
479:28 - go step by step with book
479:30 - recommendations with every stuff let me
479:32 - know in the comment box below I'll be
479:33 - catching up your next video till then
479:35 - byebye have a great day hey everyone
479:37 - welcome to this video um basically in
479:39 - this video we'll talk about how do we
479:41 - evaluate the limits because in the
479:43 - previous video I've just given you an
479:44 - introduction to limits we haven't solved
479:47 - as I've just shown you the numerically
479:48 - how uh sorry
479:51 - geograph maybe graphically how do we
479:54 - even uh go ahead and take out the limits
479:56 - with only few examp examples in this
479:59 - video what I what I will do is I will
480:01 - try to show you some of the methods for
480:04 - how we evaluate the limits we'll talk
480:06 - about substitution main the plugging one
480:09 - and then we'll talk about how do we
480:11 - evaluate limits using calculator and
480:14 - even if does not work there is algebra
480:16 - which is always there for you in maybe
480:19 - factoring evalu EV evaluation of a
480:22 - limits using factoring and rationalizing
480:25 - and conjugate and Sarah will talk about
480:27 - that today
480:28 - okay and I will just give you a a brief
480:31 - about squeeze theorem so that you could
480:33 - be you could be familiar with what is
480:36 - squeeze theorem is okay just a sandwich
480:38 - theorem cool and uh the frequency of the
480:42 - video will be bit low these days the
480:44 - reason why um uh cs001 is preparation is
480:48 - going on and there a lot of works on is
480:51 - on my head so it's bit slow but yeah I
480:55 - it's just like three videos per week is
480:56 - coming as men mention as promised but
480:59 - previously there was five videos Six
481:01 - videos were coming but I think it's my
481:03 - apology that the I'm not going to upload
481:07 - uh more than three videos per week okay
481:09 - and and the homework set says
481:12 - updated sorry the homework set will be
481:15 - updated on your LMS uh please go there
481:17 - and try to solve it I'll just uh have
481:20 - our T to to to to to grade your uh
481:23 - assignments Okay cool so uh plugging and
481:28 - using the calculator so let's talk about
481:30 - plugging let's let's just talk about
481:32 - plugging so limit of x approaches three
481:37 - okay and of a function of of a function
481:39 - x² - 10 okay so what is telling this is
481:44 - your function f ofx so function f ofx is
481:47 - nothing but x² - 10 okay so what is the
481:51 - limit when X approaches 3 okay so maybe
481:55 - just when when when you plot this fun
481:57 - function and when when you when you go
482:01 - when you approach X from the left hand
482:03 - side and when you approach X from the
482:04 - right hand side it seems to be
482:06 - approaching minus one okay but but it's
482:09 - not possible for every time for you to
482:11 - draw a graph and then go ahead so what
482:14 - you do you first of all try the
482:16 - substitution you first of all try the
482:18 - substitution so over here it seems to be
482:21 - approaching three and it is a it seems
482:22 - like a continuous function okay and it
482:25 - is a continuous function so over here
482:28 - when you what you do you put three into
482:30 - that X you put the three input to the F
482:33 - ofx so what do you do you simply put 3
482:36 - over here okay so 3 S - 10 which is
482:40 - equal to -1 so the limit of x approaches
482:43 - 3 of of this function is equals to
482:46 - minus1 it says that it's the the the
482:49 - function when X approaches 3 from the
482:51 - left hand side and the right hand side
482:53 - seems to be approaching minus1 okay or
482:56 - is approaching minus1 so what you do
482:58 - first of all is to try direct
483:00 - substitution you just give give the
483:02 - value of x to the function which is over
483:05 - three over here and then you take out
483:07 - the answer of it okay so this is called
483:10 - substitution method or plugging and it
483:13 - only works with a function which is
483:15 - continuous it only works with a function
483:18 - only works when your function is
483:20 - continuous otherwise it it works very
483:23 - well in most of the cases okay but but
483:26 - do don't you think that these this is
483:28 - just like a function no this is this is
483:30 - not like a function it is just a method
483:33 - which we use everywhere I will tell you
483:35 - how it is being used everywhere but
483:37 - every time it is not possible that this
483:39 - case happen let's take another example
483:42 - let's say an example that the you want
483:44 - to take the limit of the function which
483:46 - is 10 / x - 5 when X approaches 5 when
483:51 - you try this plugging method over here
483:53 - when you try this plugging method over
483:55 - here what what will happen what will
483:57 - happen happen it it will simply this is
484:01 - this this is a function to 10 - 5 - 5 so
484:04 - you put x 5 that will be 10 /0 that is
484:08 - indeterminate form we say that is
484:10 - indeterminate form and this is undefined
484:14 - and the limit does not exist but the
484:16 - limit do exist in this case
484:20 - okay may maybe in some time we'll we see
484:23 - the tools that will say the limit that
484:26 - will that that will help help us to
484:28 - evaluate these kind of limits but over
484:30 - here this is undefined so that's why the
484:33 - substitution does not work everywhere
484:36 - does not work everywhere because it is
484:38 - giving you the indeterminant form if it
484:40 - is even a continuous Okay cool so now
484:45 - what we now this is the first method
484:47 - which you have seen let's see the second
484:48 - method which is of using the calculator
484:51 - so using calculator let's let's say for
484:53 - for the sake of example I'll take
484:55 - another example limit of X approaches
484:59 - the limit of x approaches 5 of the
485:03 - function X x² - 25 / x - 5 equals to
485:08 - what
485:10 - okay so this is this this is the thing
485:13 - so what I will do I'll make use of
485:15 - calculator I'll make use of calculator
485:17 - what you can do first of all is to put
485:19 - the values of five over here put the
485:20 - values of five so maybe 5 S - 25 5 - 5
485:25 - so 5 2 25 - 25 over 5 - 5 that will be
485:32 - 0/ 0 and when you divide something by 0
485:34 - that is undefined that is undefined so
485:38 - the plugging method is not working over
485:40 - here so what method would work work over
485:42 - here so what you do you you take your
485:44 - calculator okay you start approaching
485:46 - you start putting the values of x from
485:48 - the left hand side and you start putting
485:51 - the values on the right hand side and
485:53 - see and make a table of it and see to
485:55 - whom it is approaching okay so say for
485:58 - an example this this is this this is
486:00 - your X this is your
486:02 - X this is your X and this is your y this
486:06 - is your X and this is your y so X maybe
486:09 - let's say
486:10 - 4999 uh 8 okay so 4998 8 that the when
486:15 - when you put this 4.9 and 8 when you
486:17 - when you go from the left hand side when
486:19 - you put this in function 4.9 and 8 that
486:21 - will be
486:23 - 99.998 okay now you put 4.99
486:27 - 9 okay that will be
486:29 - 99.999 okay and when you when you put
486:32 - five it gives it is undefined okay now
486:36 - you have seen from the left hand side it
486:38 - seems to be approaching what why is
486:40 - approaching what it seems to be
486:42 - approaching 10 so the limit of x
486:45 - approaches 5 from the neg from the left
486:48 - hand side of this
486:50 - function x- 5 is equals to what 10 it
486:54 - seems to be approaching 10 from the left
486:57 - hand side okay but the but the
486:59 - definition of the limit what we have
487:00 - seen the limit I'll just draw it the
487:02 - limit the limit of x approaches a from
487:09 - the negative side of the function f of x
487:11 - should should be equals to limit of x
487:13 - approaches a from the positive side to
487:16 - be the limit called as so that the limit
487:18 - X approaches a is equals to whatever
487:21 - okay so if this is 10 this should be
487:23 - also 10 10 so that it could be called as
487:25 - a so that the limit from the both both
487:27 - both sided limit should be same okay so
487:30 - over here when we put five when when we
487:32 - put 5.01
487:34 - 5.1 okay so when when we go from the
487:37 - right hand side when we go from the
487:39 - right hand side it it is seems to
487:41 - approaching also 10 to 10.1 so when we
487:44 - put 5.2 from the right hand side I'm
487:46 - telling 10.2 so 5.2 it seems to be
487:49 - approaching
487:50 - 10.2 okay so basically the
487:54 - limit of x approaches five from the
487:56 - positive side of this function f ofx
487:58 - let's let's assume this function f ofx
488:01 - is also 10 is also 10 so as as for the
488:04 - definition of a limit the limit of a
488:07 - function f ofx when X approaches 5 from
488:11 - both the sides it seems to be
488:13 - approaching of the function f ofx it
488:15 - seems to be approaching 10 so the using
488:17 - the pluging our limit was undefined but
488:20 - using the calculator the limit we Define
488:23 - the limit okay the limit do
488:25 - exist okay so this this is this this is
488:28 - the second method using calculator but
488:31 - every time calculator does not works
488:33 - okay maybe uh the the advance calculator
488:36 - may may work but basically your computer
488:38 - calculator mobile calculator is not work
488:40 - May maybe some of the cases so that is
488:42 - using the calculator now let's see
488:44 - another thing another uh method for
488:47 - solving of it is using algebra but I I
488:50 - do want to see the timing of it so
488:51 - what's the timing is 9 minutes okay I
488:54 - just want to complete it very fast way
488:56 - so that I could not take mod of your
488:58 - time cool so what I'm going to do is to
489:01 - solve limit I just to solve limit solve
489:06 - limit using your basic algebra okay uh
489:10 - first of all what we'll do we'll try
489:12 - factoring we'll try factoring okay and
489:14 - then we'll do the rationalizing or
489:16 - conjugation okay so what's the limit
489:19 - what's the limit when X approaches five
489:22 - when X approaches 5 of the function x²
489:26 - 25
489:27 - of x - 5 that's the same function you
489:30 - know that the it should be 10 10 it
489:32 - should be 10 x - 5 we have we have seen
489:35 - that now we'll we'll make use of
489:38 - factoring to uh evaluate this limit so
489:42 - so what so basically the limit what it
489:44 - tells when X approaches five when when
489:48 - in this function when X approaches five
489:50 - from the both side what it is
489:51 - approaching okay okay so that's what it
489:54 - is the Lim the the limit tells what
489:57 - happens when X approaches five from the
489:59 - right hand side and the left hand side
490:01 - and what will be the Y value of it okay
490:04 - so the first thing which you'll try is
490:06 - plugging the first thing always you have
490:08 - to try is plugging okay if now the
490:11 - plugging will be the indeterminate form
490:13 - or undefined okay so plugging is
490:16 - undefined plugging is undefined over
490:18 - here so first of all every time you you
490:20 - have to try the pluging first second
490:22 - what you have to do is to factor x² - 25
490:26 - x² - 25 so it is basically x² - 5² so a²
490:33 - - B squ so what it will be so a squ - b
490:36 - square will be uh A- b + a a minus B and
490:41 - then a + b okay so that's the that that
490:44 - you have already studied so what I'm
490:46 - what I'm going to do the limit of a
490:48 - function X approaches 5 x² - 25 / x - 5
490:53 - will be what the limit of x approaches 5
490:57 - and that we have factored it out okay
491:00 - that is x - 5 and x + 5 and in the
491:05 - denominator we have x - 5 isn't it isn't
491:09 - it so what so what we will do we'll
491:11 - cancel out these two so what will what
491:14 - we have left with X limit of x + 5 now
491:20 - what you do now you put your x value
491:22 - over here now you use sub substitution
491:25 - limit of x approaches 5 of your function
491:28 - 5 + 5 so that will be 10 so limit of x
491:31 - approaches 5 is nothing but 10 so what
491:34 - you have done you factored it you do
491:36 - some algebra man manipulation and then
491:39 - you at last you plug in the now you left
491:41 - with something now you plug in and then
491:42 - you now you plug in or use the
491:44 - substitution method and then you are
491:46 - done with the the limit of this function
491:48 - will be 10 that exactly what you have
491:49 - seen
491:50 - before okay I hope that this is this is
491:53 - pretty much clear to you what I'm going
491:55 - to do just recap recapture Rec capsulate
491:59 - once you again is basically what we have
492:01 - done is to first try the plugging method
492:03 - first try the tried the plugging method
492:06 - second what we have done is to factor x²
492:10 - - 25 and 25 is perfect square of 5 okay
492:15 - so x² - 5 S okay and then using the a a
492:20 - - b and a + b okay we can write
492:23 - something like this so that it could be
492:25 - cut in like this
492:27 - okay so the just and then we'll be left
492:29 - with this and you can plug in the values
492:31 - of X and then you are done with this
492:34 - okay that's that's that's that's pretty
492:37 - much easy this way for evaluation of the
492:39 - limits is what I'm going to do now is to
492:42 - take another example to Showcase you the
492:45 - rationalization stuff and uh so that it
492:47 - would be very easier for you at least so
492:50 - another example is what is the limit
492:54 - when X approaches 4 of this function fun
492:57 - otk x - 2 by x - 4 okay so this is your
493:03 - basically the the the the you have to EV
493:06 - evaluate the limit it's just simply
493:08 - telling when X approaches 4 from the
493:09 - both side what it is approaching on the
493:12 - Y AIS okay so first of all what what I
493:15 - have told to you what I have told to you
493:17 - is to try out is to try out uh a
493:23 - plugging method plugging so let's try
493:26 - out so so it will be nothing but 4 < TK
493:31 - -2 okay 4 - 4 whatever above comes 4 - 2
493:36 - that is undefined that is undefined okay
493:41 - so plugging does not working at a first
493:43 - second thing what I've told to do the
493:45 - second thing what I've told to do is to
493:48 - multi uh now we have to think of
493:49 - something like this so limit of x
493:52 - approaches 4 x -
493:55 - 2 X - 4 what we can do so that uh this x
494:00 - - 4 cuts off okay because this is what
494:03 - is causing the problem at the
494:04 - denominator so if if you know about the
494:06 - conjugate the please see the definition
494:09 - of conjugate simply changes the sign of
494:11 - it okay please see the conjugate over
494:13 - the Internet it's very one one minute
494:15 - definition you multiply with the
494:17 - conjugate of x x -2 you multiply the
494:20 - conjugate of x - 2 so times x + 2 the
494:24 - conjugate of x - 2 x + 2 x + + 2 okay
494:28 - now what now what you will do so you are
494:31 - left with x - otk x - 2 otk x + 2 so a -
494:38 - b and a + b that will be what a s - b s
494:44 - so otk x² - b² okay and this will be
494:49 - left with x - 4 and this otk x + 2 otk x
494:55 - + 2 now when you when you do this x - 4
494:59 - above will be x - 4 x - 4 < TK x + 2 otk
495:06 - x + 2 okay I think I'm correct so this
495:09 - is cutting out so the the left will be
495:12 - root x + 2 root x + 2 now this is your
495:17 - now what you do you put the value of uh
495:19 - four over here okay so 1 / < TK4 + 2 so
495:24 - that will be nothing but 1 / 2 + 2
495:26 - that's 1X 4 is the limit the limit when
495:30 - X approaches 4 of this function of this
495:33 - function x - 2 x - 4 is nothing but
495:36 - equals to 1 1X 4 that is your
495:40 - answer okay so you have tried plugging
495:44 - mainly the substitution the second thing
495:46 - which you have done is to um this the
495:49 - second thing which you have done is
495:51 - simply uh uh factoring main not not
495:55 - exactly factoring multiply the conjugate
495:57 - so that the so that what is causing the
495:58 - problem will will be eliminated and then
496:01 - you put put in the value then again you
496:04 - substitution to take out 1x4 is the
496:07 - answer of this EV limit okay I hope it's
496:11 - pretty much clear to you at
496:13 - least now what I'm going to do is to
496:16 - talk about nothing but a squeeze theorem
496:19 - I think that I'm very bad at this means
496:21 - a drawing squeeze theorems but I'll
496:24 - fully fully try to just help you
496:26 - understand what this exuse theorem tell
496:27 - you I'm just going to not spend so much
496:30 - time on it but yeah it's it's good
496:31 - theorem so basically this this is also
496:34 - called a sand theorem sand Wich theorem
496:39 - okay
496:41 - so say you have a function f g and H so
496:47 - let's draw a function FG and H so let's
496:50 - draw a function f g oh my God this is
496:54 - this is not f and g Okay so this is your
496:57 - function so let's draw some something
497:00 - like this oh what the hell this is just
497:04 - want to take simply this this
497:08 - one okay so this is your thing and what
497:12 - I'm going to do is to
497:15 - add wait for a
497:18 - second okay and this is your another
497:23 - function okay so this is your I think I
497:27 - think that this is your F and this is
497:30 - your G and this is your
497:33 - H okay so I hope that this is a squeeze
497:37 - theorem something like this so I I can
497:39 - just explain you this so you sech search
497:41 - online for the exact picture so that the
497:44 - so that it would be better for you at
497:45 - least so you have a function you have
497:48 - three functions f g and H where G is
497:51 - sandwiched below your function f and H
497:55 - okay so basically this G is stand
497:57 - pitched below between your function f
497:59 - and H and you can see and you can see
498:03 - the when this let's let's let's assume
498:05 - this is one this is two okay and this is
498:08 - also one 2 and this is three okay so
498:13 - what is the limit of x x approaches 2 of
498:18 - a function f of x of a function f ofx
498:22 - okay what is the when when when it goes
498:24 - from this side and this side it seems to
498:26 - be approaching three it seems to be
498:28 - approaching three okay okay so this
498:32 - seems to be approaching three but it
498:34 - seems to be approaching three what is
498:35 - the limit of H of X when x x x
498:38 - approaches 2 it it is also three because
498:40 - when you because H is lower than the
498:43 - your F okay but when when we see see
498:45 - when when when when we go from right
498:47 - hand side and left left hand side it
498:48 - seems to be also approaching three so
498:50 - the limit of x approaches 2 of a
498:54 - function f ofx is equals to the Limit of
498:57 - X approaches to of your h of X is equal
499:00 - to the Limit and if if they both are
499:02 - equal then the that is also the limit
499:04 - when X approaches 2 of your G of X is
499:07 - also equals to 3 so these three will be
499:10 - equals to three okay so this is what the
499:12 - scuse theorem tells you the rigorous
499:15 - proof of this theorem can be seen on the
499:17 - internet if if you want to dive in okay
499:21 - so the basically what it's telling the
499:23 - limits of these three theorems will be
499:25 - the same okay don't don't worry about
499:29 - this it's it's it's it's kind of uh not
499:31 - a big deal just a basic example to help
499:34 - you understand when X approaches to from
499:35 - the left hand side and both right right
499:38 - hand side in all the three
499:40 - functions
499:41 - cool so we have if you haven't
499:44 - understood uh understood squeeze theorem
499:47 - don't try to understand it's not even on
499:49 - your deep deep learning
499:51 - syllabus okay so we have seen this uh
499:55 - very precisely and I also hope that you
499:57 - understood it as well uh in the next
499:59 - video we'll start off with the
500:00 - differentiation I will try to complete
500:02 - differentiation as much as I can I'll
500:05 - will try to there is three to four
500:07 - videos on differentiation I just want to
500:08 - talk in in a very cool way so that
500:11 - everyone could understand what the
500:12 - differentiation is and it's it's nothing
500:15 - but the change mathematics of change
500:17 - when one changes just a fancy slope
500:20 - we'll talk about that in detail I hope
500:22 - you will understand it I'll be catching
500:24 - up your next video till then bye-bye
500:26 - have a good day hey everyone welcome to
500:28 - this lecture uh in this lecture we'll
500:30 - talk about differentiation we I think
500:32 - we'll just uh get get get some idea
500:34 - we'll reach till the formal definition
500:37 - of derivative we we'll try to Define
500:40 - derivative with which is uh using the
500:43 - different coecient uh definition so
500:45 - we'll try to Define derivative we'll try
500:48 - to take out the derivative of a function
500:50 - which you which you can think of
500:51 - something like 1/ 4x2 we we will derive
500:56 - these kind of function f ofx = to or
500:59 - we'll derive the function like f ofx =
501:02 - x² you will get the tools after this
501:05 - video where you will be able to derive
501:08 - these kind of uh the the functions which
501:11 - are not too much uh funky functions
501:14 - something like this okay so so it is
501:17 - important as per the AI and ml Ai and ml
501:20 - uh if you are just want to get an idea
501:23 - about how differentiation works and how
501:26 - do we take out the derivative or you to
501:28 - do your back propagation because in deep
501:31 - learning we are going to do everything
501:33 - from a scratch maybe deriving the back
501:36 - propagation U um using uh derivatives
501:39 - and we we'll try to derive the we'll try
501:42 - to take out the derivative of the
501:43 - function when doing the back propagation
501:46 - and then we'll do the lot more stuffs
501:47 - from very scratch okay so so that's why
501:51 - I want you all of you to just take a
501:53 - very good attention at this in the next
501:55 - couple of uh very videos as well as
501:57 - because in this video I'll give you a
501:59 - geometric intuition as well as the
502:00 - intuition of the definition of
502:02 - differentiation the next video we'll see
502:04 - the differentiation rules like quotient
502:06 - rule power rule chain Rule and a lot
502:08 - more we'll also see the vector Cal
502:10 - calculus after we complete
502:12 - differentiation and integration because
502:14 - uh I'm just going to cover up
502:16 - integration as well it's the only reason
502:18 - because in probability and Theory you
502:21 - probability Theory and statistics you in
502:23 - probability density function CDF thetic
502:26 - require you to have a knowledge of
502:27 - integration and as well as some some of
502:30 - the deep learning research papers it is
502:32 - not too much found integration in deep
502:34 - learning but yeah it is uh in behind of
502:38 - the the con concepts of PDF CDF which
502:41 - you usually do okay so we had a lot more
502:45 - talk now let's get started with uh with
502:47 - with the defining with journey of
502:49 - defining differentiation and I hope that
502:51 - you will be able to understand but
502:53 - before that what I want you to do is
502:55 - just recall the slope and then using the
502:57 - slope I just if you if if you have seen
502:59 - my previous videos I told you
503:02 - differentiation uh is is is the process
503:04 - of taking on the derivatives and
503:06 - derivatives are nothing but the slope of
503:08 - a curve okay or slope of a function so
503:12 - let me just take one example let me take
503:15 - one example example States oh my God
503:19 - yeah example States example states that
503:23 - you have uh for example example you have
503:27 - this line okay so let me just draw the
503:29 - line you have this straight linear um
503:34 - straight line okay uh this is this this
503:37 - is on your graph paper where it where it
503:40 - goes on x-axis one unit and it goes
503:43 - above 1 and half unit okay and it does
503:47 - it is the linear so it is same at
503:49 - everywhere so one 1 / two etc etc so you
503:53 - pick any two point you pick any two
503:54 - point and you see okay it is running
503:57 - towards uh it it is running one unit and
504:00 - is going above Rising one 1 and A2 units
504:04 - okay so it is uh running one unit and
504:08 - Rising 1 and a half unit so slope
504:10 - formula is nothing but the rise overrun
504:14 - rise over run okay or Y2 - y1 and X2 -
504:19 - X1 we are seeing that is running one
504:21 - unit and is going Rising it is running
504:25 - one unit and it's rising 1 and/ half
504:26 - unit okay so it is Rising 1 and/ half
504:29 - unit over 1 okay so the final slope the
504:34 - final slope which you will get which
504:36 - will be 1/ half okay that is the slope
504:40 - of this function let's give this
504:41 - function name as G of
504:43 - X okay so the the slope of this function
504:46 - is g 1/ 2 and simply
504:49 - means how much y changes how much y
504:55 - changes when X changes you can think
504:58 - something like this as X is changing one
505:00 - unit X is going one unit and Y is move U
505:05 - rising up 1 and half unit okay when X
505:08 - changes okay so that's that t you can
505:11 - think of slope or the steepness of your
505:13 - line that's also the that's that's also
505:15 - you can understand by the slope okay the
505:17 - steepness of your line okay so uh this
505:21 - is your slope now coming to the calculus
505:25 - term what is the derivative what is the
505:27 - derivative of this function G ofx what
505:31 - is the derivative of this function G of
505:34 - X with respect to x with respect to X
505:39 - that will be nothing but 1 / 2 that that
505:42 - will be nothing but 1 / two why it is
505:44 - one 1 / two is over you because the
505:48 - derivative as as as I told
505:52 - derivative is nothing but the slope
505:55 - nothing but the slope that how much how
505:58 - much this G of X or Y changes how much
506:02 - this y changes when X changes how much
506:05 - this y changes when X changes okay so
506:09 - that is nothing but a slope but the
506:11 - fancy term in calculus we we we give
506:14 - which is derivative of course there's a
506:16 - lot more difference between the your
506:17 - regular slope and the derivative which
506:20 - we'll see later on but as of now what
506:22 - you can see that the derivative of this
506:23 - function G of X is nothing but 1/ 2 2 is
506:26 - nothing but 1/ two and how this 1/ two
506:29 - came is we are seeing that okay it is uh
506:32 - it is a linear line so derivative is
506:34 - nothing but a slope but the slope is
506:36 - rise over run and we are going one one
506:39 - at the this at running towards one and
506:43 - Rising 1 and half okay so the derivative
506:47 - of this function G of X with respect to
506:49 - X how much X Change or how much y
506:51 - changes or it is just equivalent to y 2
506:56 - - y1 over X2 - X1 you can think of like
506:59 - that because this is a slow formula okay
507:02 - so that is uh that is the derivative of
507:04 - this function is nothing but 1/ two and
507:07 - this derivative is nothing but telling
507:09 - exactly the same how much this function
507:11 - or G of Y changes how much this y
507:14 - changes when X changes a little bit okay
507:17 - so you can think of like that okay so
507:20 - now I I'll take another example so you
507:23 - are more comfortable with okay so you
507:25 - have a l line you have a line let let me
507:28 - just draw a straight line something like
507:31 - this okay you you're moving towards you
507:34 - are
507:35 - running one one unit let's assume you
507:40 - running one unit okay and you're running
507:43 - one unit and you're Rising three units
507:46 - you're Rising three units okay so you're
507:50 - running to one one unit you're running
507:52 - one unit and you're rising and you're
507:55 - Rising Three three units Rising three
507:57 - units so what is the slope of this what
508:00 - is the slope of this um function so the
508:03 - slope of this graph or the function is
508:06 - uh maybe 3 over 1 3 over 1 which will be
508:09 - nothing but three okay what is the slope
508:12 - slope is three okay what is the
508:14 - derivative what is the derivative and
508:16 - derivative of this linear we we don't
508:18 - require derivative over here but what is
508:20 - the derivative so derivative Dy by DX Dy
508:24 - by DX which is which is just tells how
508:27 - much y changing when X is changing X is
508:30 - changing one it is going rise it is
508:32 - going one and it it it it is Rising one
508:36 - then Y is rising three okay so that is
508:39 - how much y changes when X changes okay
508:43 - so that that will be nothing but uh uh 3
508:47 - over 1 okay 3 over 1 y changes 3 when
508:52 - when X goes 1 then y goes up three when
508:56 - X goes x x run one then y run Rises
509:00 - three okay so that the derivative will
509:02 - be also three so we write this is this
509:05 - is a formal this is a formal way we
509:08 - write Dy by DX Dy by DX okay so
509:12 - derivative of this function y with
509:14 - respect to X
509:16 - derivative of y with respect to that
509:20 - point x okay with respect to X so for
509:24 - example if you want want to take out
509:26 - derivative of this point derivative of
509:29 - this point so you'll get the same thing
509:30 - you're going one at this side and you're
509:33 - moving up okay so I'm I'm not taking the
509:37 - exact stuff but the but in the the
509:39 - function you'll be going so this is uh
509:43 - this is a point okay so that will be
509:46 - three okay so the slope of the slope of
509:49 - the linear lines will be always constant
509:52 - so any point you go on X for example if
509:55 - I go any point on this the slope will be
509:57 - same it goes the slope will be same of
509:59 - course the slope will be same the
510:01 - derivative at this point for say X1 the
510:04 - derivative of y with respect to X1 will
510:07 - be also maybe it goes one unit and 1 one
510:09 - that will be one okay so it is it is
510:13 - writing Dy by DX which is the derivative
510:16 - of this function y we we we we can we
510:19 - can name the function but you know the
510:21 - function for example we can remove this
510:23 - F ofx we can just write y okay
510:26 - so the derivative of this function with
510:28 - respect to X okay and X can be Point
510:32 - okay over here okay so this is so this
510:35 - is a formal way of representing Dy by DX
510:38 - another way to represent U derivative uh
510:41 - it it is read as Dy by Dy DX okay Dy DX
510:47 - which is nothing but uh uh uh derivative
510:51 - of the function with respect to that X
510:54 - okay another way we we can write in
510:57 - various ways we can write the derivative
510:58 - of function in various ways let me make
511:00 - you familiar with the ways as well okay
511:04 - so uh so we can write let's let's assume
511:06 - we have a sum function f ofx okay which
511:09 - is let's assume uh any function okay any
511:14 - function let's let's assume F ofx so
511:15 - when you want to take out the derivative
511:18 - derivative of the function f ofx with
511:20 - respect to X you you can just write
511:23 - derivative Dy DX d y DX so sorry Dy DX
511:28 - okay the derivative of function ex it is
511:31 - equivalently equals to this it is we we
511:34 - can also write F Prime X or f-x that
511:39 - exactly that tells you the derivative of
511:41 - this function derivative of the function
511:43 - f with respect to X it is ALS we can
511:46 - also write this as a Y dash because Y is
511:49 - a function and we say what is the
511:50 - derivative of the function y with
511:52 - respect to X or we can write f F like
511:56 - like this or we can write y this there
512:00 - are lot more techniques we can write d x
512:04 - f something like this so they all are
512:06 - equivalent but we'll follow the formal
512:08 - notation which is this one okay Dy by DX
512:12 - which is nothing but how much y changes
512:16 - when X changes and that is nothing but
512:18 - the slope that is nothing but a slope
512:20 - but don't worry I'll give you the formal
512:22 - definition of a limit in in just a
512:25 - second when be try to Define
512:27 - differentiation with the help of a rate
512:30 - okay so we will try we'll come on that
512:32 - don't worry okay so I'll just give you
512:34 - the formal definition of a limit so that
512:36 - you could uh you could just Define limit
512:38 - what it is but as of now how much y
512:40 - changes when X changes okay so there so
512:44 - there are tons of not exactly tons of
512:47 - ways there are so many of ways for
512:48 - writing the derivative notation okay as
512:52 - you know the peoples are very smart on
512:54 - the earth they
512:56 - they just invent these kind of stuff I
512:58 - don't know why they haven't stick with
513:00 - that Dy by DX because I like that
513:03 - notation I don't know what the
513:04 - researcher have thought of maybe ISAC
513:07 - Newton has thought of this one and then
513:09 - a lot of researchers that thought this
513:11 - one and they all fight it together to
513:13 - get the work done but I don't know what
513:15 - they have done I have to go at the past
513:18 - and then ask them okay cool so we have
513:22 - seen the derivative of a line and I hope
513:25 - that you understood it as well so uh one
513:28 - thing which I Let's uh do we want to go
513:30 - ahead yeah let's go ahead let's let's
513:32 - let's talk about one similar example so
513:34 - that everything u i I don't think we
513:37 - should go ahead and uh see one more
513:39 - simple example because we have already
513:41 - seen it now we have seen the derivative
513:43 - of a line now we have seen the
513:45 - derivative of a line which is nothing
513:47 - but a slope of the line okay so you may
513:50 - think hey I Ed Dera is that simple no
513:52 - it's not that simple we'll we'll see uh
513:53 - we'll see but yeah it it it it it is not
513:56 - it is doable if you uh keep your good
513:59 - mind concentrated over here okay cool so
514:02 - one of the example which I want to
514:04 - highlight is from
514:07 - calculus for dummy's book okay example
514:11 - is from calculus for dumy book you can
514:13 - check out this book it's amazing book
514:15 - which I ever seen on calculus for
514:16 - beginners is is what I use for teaching
514:19 - as a reference uh there there there's an
514:22 - example where there there are two
514:24 - students there are two students let's
514:27 - Assume My Name aush let's assume uh
514:31 - let's assume there are two students
514:34 - aush and let's assume there second
514:36 - student which is
514:37 - Risha okay which is Risha uh maybe R is
514:42 - my best friend over the school but these
514:44 - are the two uh students these are two
514:48 - friends okay whiches around I think they
514:51 - are they are they are um I I don't know
514:55 - the the name of this seesaw yeah that is
514:58 - seesaw when when one goes down then up
515:00 - for example this one okay when um one is
515:04 - sitting above on the bench one goes down
515:06 - and when uh this goes down this goes up
515:10 - okay let see so I I don't know how what
515:12 - what what the name of the game is but
515:14 - that is the game okay so here it is
515:16 - telling that the IOU this aush aush is
515:23 - the the weight of aush is twice as as
515:27 - much as Laurel okay as sorry as much as
515:30 - Risha okay so iush weight is a twice as
515:35 - much as rishab okay so the rishab age is
515:39 - 20 sorry weight is 20 then aush weight
515:42 - is 14 okay so if the rer age is X then
515:46 - iush weight is 2x okay that is twice of
515:51 - Russia now they are they are on they are
515:54 - on seesaw they are on seesaw and this is
515:57 - something like like this this is a
516:00 - ground this is a ground and uh uh rishab
516:04 - is sitting here and aush is sitting here
516:08 - okay and a a is sitting here and rishab
516:11 - this is rishab and this is aush and of
516:15 - course I just changed the name so that
516:17 - uh uh it it it is not uh too much so so
516:22 - so that it would be understandable uh to
516:25 - to you all because I find those names
516:27 - very hard to pronounce and you can more
516:29 - connect with the Indian names if you're
516:30 - an Indian okay so there are two students
516:33 - rishab and Laurel where aush weits is
516:35 - twice as much as rishab now rishab has
516:39 - to has to sit twice as much closer so
516:44 - they have to sit as much closer to uh
516:47 - rishia okay for them to balance up okay
516:51 - because it is to much heavy is very hard
516:53 - to balance okay uh and for every inch
516:57 - for every inch aush goes down for every
517:00 - inch aush goes down rishab goes up by
517:05 - rishab goes up by 2 in okay so when uh
517:09 - when the aush goes down by 1 in okay
517:14 - then rishab goes up by 2 in okay so if
517:19 - the aush goes down by 10 in the Risha
517:22 - will go over by 20 in okay so aush
517:27 - weights of course higher so that's so so
517:29 - just listen my thing is when aush goes
517:32 - down by 10 Ines then rishab goes up by
517:35 - 20 in when a goes down by 1 in then
517:39 - rishab goes up by 2 in down up down up
517:42 - something like this okay so when aush
517:45 - when aush goes down by down by 1 in then
517:50 - the rishab goes of above by 2 in so it
517:53 - is so rishab mov moves twice as much as
517:57 - a so
518:00 - Ria
518:03 - moves twice as much as a us okay so then
518:12 - you got this is this that's a derivative
518:15 - you got the derivative so so we'll see
518:17 - it you got the derivative and you may
518:19 - recall this is nothing but your favorite
518:21 - rate okay we'll see it so
518:25 - uh rishab moves
518:27 - twice as much as hardi uh twice as much
518:32 - as aush I'm just taking the book books
518:33 - example because I have written my notes
518:35 - over there uh that the the name of the
518:37 - hard over there but uh basically uh
518:41 - rishia moves twice as much as a reserve
518:45 - moves twice as much as a
518:49 - so uh Dr okay so I didn't denote Ria
518:53 - with r is equals to the D 2 D A okay so
518:58 - the r goes R moves twice up as um uh
519:04 - aush okay so with the calculus symol you
519:07 - can write it something like this and
519:09 - when you uh when when you divide both
519:12 - side by da or you move it over here Dr
519:16 - by da is equal to 2 and that's nothing
519:20 - but your favorite derivative which you
519:23 - got that's nothing but the favorite
519:26 - derivative which you got and over here
519:30 - this derivative this this your your your
519:32 - your your basically this what what what
519:34 - you're seeing is the derivative of R
519:38 - okay so the derivative of your R okay
519:42 - derivative of rer r with respect to ause
519:46 - derivative of rer r with respect to ause
519:49 - so Dr can be thought of as a change in
519:54 - uh r position as dhda can be thought of
519:58 - as a change in a position okay so if
520:02 - hardi goes down is if aush goes down by
520:05 - 1 in then rishab goes up by 2 in so Dr
520:09 - can be thought as change in rishop
520:11 - position and D can be thought of as a
520:13 - change in a position okay so that this
520:17 - is this is what you got when you the
520:18 - calculation this is what you got as uh
520:21 - this is what you call it as a derivative
520:23 - and this that D DN by DH it's just
520:26 - telling derivative of r with respect to
520:31 - a okay derivative of rishab with respect
520:34 - to aush so what does it mean that rishab
520:38 - is moving two times as much as aush
520:44 - rishab rishab moves two
520:49 - times as much as aush
520:55 - okay that's the that's exactly what
520:57 - you're telling that this is uh this is
521:00 - dy by DX which is derivative of r with
521:03 - respect to a is nothing but telling the
521:05 - rishab moves two times as much as
521:08 - Hardy aush okay so that's that's how you
521:11 - got the derivative you can take a look
521:14 - this is this is the derivative which we
521:16 - taken out from the from the uh Ria point
521:19 - of view okay because R was moving up but
521:21 - over here but over here uh you you can
521:25 - take out the derivative from the aush
521:28 - point of view from the aush point of
521:30 - view okay so so the so the da a the
521:35 - derivative da can be thought of as a
521:37 - change in a i position is equals to 1 /
521:41 - to DH when aush it is just telling it is
521:46 - just telling uh aush moves half as much
521:51 - as uh this guy Rish Okay so aush moves
521:56 - half as much as uh risham okay so da
522:00 - equal to Dr R Dr so when when do the
522:04 - calculation da by Dr is equal to2 and
522:07 - that's your derivative which you got
522:10 - which which you got and that is nothing
522:13 - but the derivative of aush with respect
522:16 - to rishab and it simply means that a
522:20 - moves 1 and2 inch for every inch Laurel
522:25 - uh that rishia moves okay so so so
522:29 - basically what is doing is uh it is the
522:33 - derivative saying that uh aush is moving
522:37 - 1 and half when I aush is moving moves 1
522:42 - and2 in for every inch Ria okay so how
522:47 - much as as I told how the change in
522:50 - position so how much a changes when R
522:54 - changes okay okay so a um when a a a is
522:59 - changing 1 and half when R is changing
523:03 - so when aayush moves 1 and/ 12 in when
523:06 - uh rishab moves 1 in okay that's that's
523:11 - pretty much clear I I I hope so uh it is
523:14 - if if you're getting confused please
523:16 - recall the video again so that you could
523:18 - be easily understandable it is very very
523:20 - easy to understand again I'm recalling
523:23 - that uh this is d a by Dr da by Dr is is
523:30 - the derivative from the aush point of
523:32 - view so it is just sing how much iuse
523:34 - changes when R changes when rup changes
523:37 - and it's how much rup changes when iuse
523:39 - changes okay so this these are two from
523:42 - two point of views so it is just in this
523:44 - one how much how much I use changes how
523:48 - much I use changes how much I use
523:50 - changes how much I use changes when how
523:53 - much I use changes oh my God how much
523:55 - Rish up changes when iuse changes and
523:59 - how much iuse changes how much iuse
524:01 - changes when Rish up
524:03 - changes okay so this is from different
524:05 - point of view and we taken out the
524:06 - derivative from the same okay so I hope
524:10 - that everything is clear I'm not going
524:12 - to talk about too much on that because
524:14 - we have already taken a lot of time on
524:15 - it and I hope that is very very
524:17 - understandable to you as
524:18 - well coming to different thing I I I
524:22 - think I'll just I'll just give you a
524:24 - proper definition
524:25 - of uh not I'll not able to give you the
524:28 - proper definition because it is already
524:29 - let me the timing that this one HS up so
524:33 - just see the timing it's 24 okay I can
524:35 - continue it I think I can continue it
524:38 - yeah so uh let's talk about let me give
524:42 - you the formal definition let me give
524:45 - you the formal definition of uh just
524:48 - just just just a English definition the
524:51 - English definition which you which the
524:53 - example you saw this example you saw is
524:57 - the derivative so as I already told you
525:00 - thousand times a derivative a
525:05 - derivative is simply is
525:09 - simply a measure a
525:12 - measure of how much how much one
525:19 - thing
525:21 - changes compared to another
525:25 - compared to
525:27 - another okay so d a by Dr saying how
525:32 - much a use changes compared to Russia
525:35 - okay that's exactly called the
525:37 - derivative cool so another we can
525:40 - another we we can think of Let's uh
525:44 - uh uh another another example can be uh
525:48 - when when a person driving at a constant
525:50 - speed of 60 M hour for for example there
525:54 - there is a car I don't know how to draw
525:56 - a car oh I I got to know about the car
526:00 - okay so this is a car this this is a car
526:03 - and this is going with a constant speed
526:05 - of 60 M hour 60 m per hour okay 6 6 60 M
526:14 - hour and it's driving at a 60 M hour so
526:18 - uh what is the derivative derivative of
526:21 - p with the D DP by DT so is telling how
526:25 - much how much how much position changes
526:29 - when time changes if there's 2 hour then
526:33 - where's the car is okay so that we we
526:35 - can think of it as a derivative as well
526:38 - okay so we had said it we had talked a
526:41 - lot about linear only and I hope that
526:43 - you understood that understood it as
526:45 - well okay so we have only talked about
526:48 - linear over here it is going to the
526:50 - constant speed it is just going at a
526:52 - constant speed and we have we have
526:55 - talked about only the straight lines we
526:57 - we are not talk about the the the the
527:00 - the derivative of a curve okay so the
527:04 - derivative of a curve where the slopes
527:07 - are constantly changing at every point
527:11 - where the slopes are constantly where
527:14 - the slopes are constantly changes so
527:16 - this is a parabola where the slope are
527:18 - constantly changing at each and every
527:21 - point so let's say you want to take out
527:23 - the slope at this point C how you going
527:26 - to take out let's say you want to take
527:27 - out a point F how how you going to take
527:30 - out say you H how you going to take out
527:33 - so there are so we so we'll see today
527:36 - about this
527:38 - okay so uh what I'm going to do what I'm
527:42 - going to do is just just tell you what's
527:44 - the derivative okay just going to tell
527:47 - you what's the derivative but how I take
527:50 - out how I take out I will talk about in
527:52 - the next videoi or in the in in the next
527:55 - video where we'll learn about the
527:57 - difference coecient or I'll try to cover
527:59 - the different coecient in this video
528:01 - only okay let's let's let's let's try to
528:03 - cover the different coecient in the next
528:05 - in this video only say you have a
528:07 - function f ofx = to 14x squ
528:12 - so the diagram for this will be
528:16 - like uh will be like uh something like
528:19 - this I I'm just going to make it
528:21 - something like this so that's a um a
528:24 - graph of this function that's a graph of
528:26 - this function that's a graph of the
528:27 - function I hope so that's a graph of
528:29 - function so you have an X you have an Y
528:33 - and the derivative of
528:36 - d by DX so derivative so say for say say
528:40 - for an example say for an example you
528:43 - want to take out the derivative of this
528:44 - point and this x is 2 and Y is what Y is
528:49 - what uh let's assume uh or X is one so
528:54 - let's assume X is 1 and y is0
528:58 - 0.25 okay so that is derivative 0.25 D
529:04 - D1 okay so what it will be what it will
529:08 - be it will be I'll just take uh what is
529:11 - what is the slope at this point uh that
529:13 - is we we can use the derivative to take
529:16 - out the slope at at at at that point uh
529:19 - you you I already explained you how we
529:21 - will we'll see how it how how how it
529:23 - does but later on but over here the
529:26 - derivative of this function is nothing
529:27 - but 1 / 2x you can plug in the values of
529:30 - X you can plug in the values of X say
529:34 - for an example that I plugged in uh I
529:38 - plugged in
529:39 - 1/ 2 1 / 2 * or x/ 2
529:45 - * uh I plug in two okay so it will be
529:48 - one and the derivative and the
529:51 - derivative at this point one at this
529:53 - point one and it will be one it will be
529:56 - what it will be one the slope at this
529:59 - point the slope at this point the slope
530:01 - at this point D the slope at this point
530:03 - will be one okay you have the derivative
530:07 - and you can put any point you can put in
530:09 - any x value you will get the slope at
530:11 - that point okay because we make use of
530:14 - derivative to take over the slope at at
530:16 - exactly that point okay so you can make
530:19 - use of you can make use of 1x 2x we'll
530:22 - come to that how we evaluated 1X 2X and
530:24 - the later videos but in differentiation
530:26 - rules thought I will cover in this video
530:28 - only uh so you can this is how you take
530:30 - out the Dera we have we have saw using
530:33 - the derivative you can put in any value
530:35 - of x after taking out the derivative of
530:38 - function which is how much y changes
530:41 - when X changes okay so which is one 1 1/
530:45 - 2X and X which the value which you have
530:47 - to input in to get the slope at that
530:50 - point or to get the derivative at that
530:51 - point or to get the rate at that point
530:55 - okay or you you all know okay so we had
530:58 - this we we we'll see how do we how do we
531:00 - got 1/ 2x but before that let's talk
531:03 - about the difference coent let's let's
531:06 - talk about the difference coecient
531:08 - because it is the most important concept
531:10 - which one need to know okay because most
531:13 - of the most of the things in calculus is
531:16 - based on it's just because it is it is
531:19 - needed because uh I have think I I I
531:23 - think uh if it it just tells you how it
531:26 - gets infinitely closer to make the line
531:29 - from curve to a straight line okay so we
531:33 - so we'll see today one okay let me find
531:37 - uh the copy which I want so that I can
531:39 - at least go ahead and I'll have help you
531:42 - out okay because I usually make notes
531:46 - before teaching in video because I can
531:48 - just hope that I don't make any uh
531:51 - mistakes in the
531:53 - videos see say for say for an example
531:56 - you have a graph something like this
531:59 - okay you have a graph something like
532:02 - this okay let me just uh draw this
532:07 - one graph at this point yeah that's good
532:11 - okay and you pick two points and you
532:13 - pick two points let's say this
532:16 - one
532:18 - X and this
532:21 - one okay so you pick the two points and
532:24 - you assume that this is a point x there
532:27 - on the x coordinate is X and there's a h
532:30 - distance between this point so this is x
532:32 - + H so this is H so for example this was
532:36 - 2 and the distance from this to this is
532:39 - uh h = 2 then over here it will be four
532:43 - okay so you can assume like this so H so
532:45 - H depends on H okay so uh the distance
532:49 - is H okay the distance is H so this is a
532:53 - point this the x coordinate is is x and
532:55 - y coordinate so and another point so
532:58 - this is X1 one coordinate and this is
533:01 - the second coordinate x coordinate okay
533:03 - so what is the coordinates of this point
533:06 - it will be X which is on X and this is
533:09 - your f of x function okay so this is f
533:13 - ofx f of x okay and what is the
533:16 - coordinate of this point the coordinate
533:18 - of this point will be X+ H is your X is
533:21 - the point is xais and that will be
533:25 - nothing but f of x + h f of x + H
533:29 - because we want this is also the
533:31 - distance of H okay so f of x plus h uh
533:36 - which you're seeing over here okay now
533:39 - we we got the we got the coordinate of
533:41 - this we got the coordinate of this so
533:43 - the coordinate of XA f of x and there H
533:46 - distance where this there on X X Plus H
533:50 - okay and this one is X the the
533:52 - coordinate of the x-axis and this is of
533:54 - Y AIS okay so I hope so it is making
533:58 - sense try if if it is if it is getting
534:00 - confused try to think on yourself is if
534:03 - you if you move this X Plus H and if you
534:06 - if you try to put put in that function f
534:08 - ofx and we can add some value H so
534:11 - that's it that's that's that's pretty
534:13 - much the common sense uh this is a very
534:16 - common sense to understand X+ H okay now
534:21 - coming to this now coming to this what
534:23 - we can do we can draw a secant line we
534:26 - can draw the secant line so let me draw
534:28 - a secant line something like this let me
534:30 - draw a secant line something like this a
534:32 - secant line the definition of a secant
534:34 - line is a line which intersects two
534:37 - points two points on a which which
534:41 - intersect two points on a graph which
534:44 - intersects two points on a graph that's
534:47 - a secant line or a more formal
534:50 - definition of a secant line let me just
534:52 - have you the more formal definition of a
534:54 - secant line a secant line is a line with
534:57 - that intersects a curve at two points
535:00 - okay to intersect a curve at two points
535:04 - uh this is where we draw the seant line
535:07 - and the secant line is something this
535:09 - now now what you can do if you take if
535:13 - you take this point if you take this
535:15 - point if if you take this point oh my
535:18 - God I think I have to choose the pen
535:21 - yeah if you take this point okay you
535:25 - make it closer to this point okay so
535:27 - when you slide this point with your
535:29 - secant line you slide this point to over
535:32 - here with your second line okay so let's
535:35 - do do do the thing now you you'll be
535:37 - getting just make sure that you're
535:39 - following me just make sure you will be
535:42 - having something like oh my God the SEC
535:44 - L do does not work correctly over here
535:47 - let me make something touching over
535:49 - there okay now what you do you you now
535:53 - this this point has come to over here
535:55 - now what you do you you take this point
535:57 - you slide a little bit with the SEC line
535:59 - you slide a little bit om
536:02 - mg uh you slide this a little bit okay
536:06 - now let me just draw it okay you you
536:09 - slide with the secant Point as well you
536:12 - slide with the secant line as well okay
536:15 - you you now what you do you take this
536:18 - point again you slide
536:19 - it again you slide it again you slide it
536:24 - okay now let's slide it again uh it will
536:28 - be make sure that it intersect the both
536:31 - the point okay now again take this red
536:34 - one again take this red one slide a
536:36 - little little bit over
536:38 - here and let's continue doing that okay
536:43 - that will be intersecting at the two
536:46 - points okay at some point at some point
536:51 - it will be so let's let's draw the graph
536:54 - again again so what you're actually
536:55 - doing you are making this you are making
537:00 - this H you're making this H to be to to
537:05 - approach to to approach zero okay you're
537:09 - making this H we can make use of limit
537:12 - we can make as as we want to go uh
537:15 - closer and closer to not exactly X but
537:18 - we but what we can do we can make this
537:21 - H approach is zero when X when h
537:24 - approaches zero when H approaches zero
537:26 - it will be F ofx so oh my God uh when f
537:29 - of x okay f of x and this f of x plus h
537:34 - when this H becomes H approaches zero
537:37 - then then that will be so let me just
537:40 - make you familiar let me complete that
537:42 - thing okay so we will come to this will
537:45 - come to this okay so this was yours and
537:48 - this was your two points this was your
537:50 - two points now when you do this when you
537:53 - do this you will be left with something
537:56 - like this a secant line from us from
537:59 - sorry a tangent line a tangent or over
538:03 - here from the secant when you when when
538:05 - when when you take your H approaches
538:08 - zero when you when you just make your H
538:11 - when you may just make your P um make
538:14 - use of limit make use of limit when H
538:17 - approaches zero when it approaches zero
538:20 - okay so this point will go over here
538:22 - here here here here so at some point
538:24 - will become a tangent it will become a
538:26 - tangent and the derivative of any
538:30 - function of a function is the is is is
538:34 - the slope okay of a tangent the slope of
538:37 - the secant would be if if I draw a
538:40 - secant line the slope would be we go at
538:42 - this point so we go something this so
538:44 - rise of run so it is very easy so secant
538:48 - line becames the tangent line becames
538:50 - the tangent line and the tangent the
538:52 - slope of the tangent line is nothing but
538:55 - the slope uh the derivative the
538:57 - derivative the slope of this tangent
538:58 - line is nothing but the derivative so
539:01 - what you do so the so the slope of the
539:03 - secant line so let's let's start writing
539:05 - the formal definition let's start
539:07 - writing the for formal definition I
539:09 - think uh it would be very easy for me at
539:11 - least to draw it again okay so everyone
539:14 - is on same
539:16 - Pace OMG let me mix little bit up yeah
539:20 - so this is your one point and this is
539:23 - your second point Point okay let's draw
539:26 - a secant line over here okay and uh so
539:30 - what is what what will be the slope what
539:32 - will be the slope of this of this of the
539:35 - secant line so we have we were having
539:38 - our x coordinate okay and then we were
539:41 - having x + H and this was a distance of
539:45 - H okay and this was uh X comma f of x
539:52 - and this was nothing but X XA f of x + H
539:57 - okay so this is what you have so what so
540:00 - what would be the slope the slope of
540:01 - this would be Y2 - y1 X2 - X1 okay so f
540:08 - of x + H - f of x ided by x + H - x okay
540:20 - so this plus minus this cuts down okay
540:23 - so you'll be left with f of x + H - f of
540:28 - x / H okay so uh now uh now you got the
540:34 - you got the slope of the secant line
540:36 - that is nothing but called the
540:38 - difference
540:39 - coecient
540:41 - difference coent okay quotient okay why
540:44 - I'm pronouncing it wrong so that is the
540:46 - slope this is the slope of your secant
540:48 - line so when you when you make your H
540:52 - approaches zero when you you make your
540:54 - hedge approaches zero so this will
540:57 - become a tangent line so it will go
540:59 - slide over here you slide it over here
541:01 - you slide over here so it will be
541:03 - something like this and then it will
541:04 - something like this and it some it it
541:07 - will somewhat become like this it will
541:10 - it will only touch uh okay it will
541:14 - become something like this this is the
541:16 - tangent line This is a tangent line so
541:20 - what how you can do this you can make
541:22 - use of limits
541:24 - to make your H approach zero because if
541:27 - the H gets if the f of x so it make in
541:31 - in other words you can you're sliding
541:33 - this point to this point and that will
541:35 - be equivalent the coordinate will be
541:37 - equivalent so you just want H H to be
541:39 - zero so H approaches zero not not
541:42 - exactly zero technically you can you
541:43 - can't get exactly zero f of x plus h f
541:47 - of x plus s- f of x / H okay and that's
541:53 - the the derivative of your function with
541:56 - respect to
541:57 - [Music]
541:59 - X that's the derivative that's the
542:02 - definition of a derivative with respect
542:05 - to X okay so the seant line becomes the
542:08 - tangent line when you approach H to the
542:10 - zero so I hope that this this this this
542:13 - makes a complete complete sense okay so
542:16 - it just tells how much the function is
542:18 - changing when X is changing it it what
542:21 - it does so what it does
542:24 - again I'm again I'm telling it slides
542:27 - you can see the slides over here it
542:29 - started to become the tangent line so it
542:30 - started making it h h approaching zero
542:34 - okay you made use of limit to Define uh
542:38 - this uh this this this a derivative and
542:41 - the formal definition of a derivative
542:43 - let me just Define it for you let me
542:45 - just Define it for you let me just
542:47 - Define it for you
542:50 - um okay uh I think I have to find that
542:53 - definition of it
542:59 - yeah so the definition so the definition
543:02 - the definition
543:06 - is derivative of f ofx with respect to X
543:11 - is equals to limit when H approaches
543:15 - zero f of x + hus f ofx ided h okay
543:22 - that's the formal definition and what it
543:24 - does what it does this slope or this
543:27 - simply the shrinking we are shrinking
543:29 - rise over run we are shrinking the rise
543:32 - over run okay we we are making shrinking
543:35 - we are making of limit property we
543:37 - shrinking over here sliding this uh this
543:40 - this this coordinate this coordinate
543:42 - okay so we had a talk a lot and I hope
543:45 - that you are uh enjoying it as well even
543:48 - I am enjoying it okay but one thing the
543:51 - just one question which I want to solve
543:53 - is take out the derivative of the
543:55 - function f ofx which is 1 /
543:58 - 4x² which is 1 over 4X s we are going to
544:01 - take out the derivative now okay which
544:03 - you have seen we have taken the
544:04 - derivative okay one one 1/ 2X I will
544:07 - just show you how I taken out so this
544:10 - this is your function now let's go ahead
544:13 - now let's put in let's we have a
544:15 - definition now we can limit of H
544:18 - approaches zero limit of H approaches
544:20 - zero we can put this uh X this x is the
544:25 - coordinate which which he wants so we
544:28 - just going to give X and we can take
544:29 - this 1 over 4 as a coent okay because
544:33 - that that only wants 1 / 4 okay x + H
544:37 - and we don't have to leave a square
544:39 - because this is the one X we want okay
544:42 - minus your whole function 14
544:46 - x² okay 14
544:48 - x² cool divided by H divided by h
544:54 - okay so this is your now we are going to
544:57 - evaluate this limit so the limit of H
545:01 - appro H approaches zero 1 /
545:05 - 4x² + 2 x h / 4 + h² A + B whole Square
545:14 - you know 1/
545:17 - 4² 1 / 4 x² so this one cuts out this
545:22 - one this one cuts out out this one so
545:24 - you are left with you are left with
545:26 - limit of edge approaches Z 2x + H over
545:33 - 4 plus uh maybe uh I I yeah I can write
545:39 - something H Square okay you can write
545:41 - the four over here as well you can write
545:43 - this four over here as well okay because
545:46 - when you when you when you multiply this
545:48 - oh my God why have left this when when
545:49 - you multiply 1 over4 with every member
545:52 - of this so that will be nothing but uh
545:55 - let me be little bit transparent let me
545:57 - be a little bit transparent let me be a
545:59 - little bit transparent then I think that
546:01 - I've done wrong a little bit I don't
546:03 - have to do wrong over
546:05 - here yeah let me be a little bit
546:08 - transparent so oh my God sorry for
546:11 - delays but 1 / 4 just expand in x² + 2x
546:17 - H + h² - 1 / 4x² now you simply limit of
546:24 - H approaches Z 1 / 4x² + 2 x h by 4 + h²
546:31 - by 4 - 1 / 4x² this cuts out with this
546:36 - limit of H appro H approaches 0 1 and
546:40 - now it cuts down so 2x H 4 + h² 4 okay
546:46 - let's uh let's let's keep keep keep on
546:48 - working on it so that let's write the
546:51 - same thing again H approaches zero 2x H
546:55 - by 4 + h² by 4 by H I think I forgot in
547:00 - everything H we don't have to forget
547:04 - H okay by H so H is there don't forget
547:08 - okay now what I will do I will try to
547:10 - add it when H approaches 0 4 okay so
547:15 - that will be nothing but 2 x h + h² okay
547:21 - the limit of h h approach is zero we
547:24 - take common H 2x + H uh by
547:30 - 4 by H and this cuts out this and this
547:34 - cuts out okay so limit now it's very
547:37 - easy 2x + H by 4 now you can do the
547:42 - substitution you can do the substitution
547:45 - limit uh now we don't have to write
547:47 - limit 2x + 0 by 4 and that will be
547:52 - nothing but 2X by 4 and that will
547:55 - nothing but X by 2 or 1 / 2 x okay this
548:01 - was that much easy to calculate the
548:04 - derivative of a function okay using our
548:08 - formal definition of a limit and we
548:10 - showed you the geometric interpretation
548:12 - of the differentiation I and I hope you
548:14 - really enjoyed it try to take out the
548:16 - derivative of x² so just just just for
548:20 - your convenience the limit it it will be
548:24 - x + h² - x² / H okay divid by H so let
548:33 - me see if it is exactly equals the same
548:35 - yeah it is divided by H okay so this now
548:39 - try to take out now this take out the
548:41 - derivative of a function f ofx which is
548:43 - x² and using the limit formal definition
548:46 - okay so I hope that this is pretty much
548:48 - clear to you at least uh I hope so it it
548:52 - is very much clear although you get a
548:55 - very very complex function complex
548:57 - function it is not a we we we we can't
549:00 - use our uh we have a different
549:02 - differentiation rules which will help us
549:04 - to do take out the derivative very
549:06 - quickly which we'll talk about in the
549:07 - next video till then byebye have a great
549:08 - day you have done a great job today meet
549:10 - you in the next video to this uh video
549:12 - uh in this lecture basically we'll talk
549:14 - about differentiation rules in our
549:16 - previous video we talked about uh
549:19 - differentiation which is approximately
549:20 - 40 minutes of video so sorry for that
549:23 - much long the only reason is uh I just
549:26 - completed the whole differentiation
549:28 - definition because in the previous video
549:30 - we given you the geometric intuition of
549:33 - differentiation and in this video we'll
549:35 - try we'll we'll see some of the rules
549:37 - for taking out the derivative of a
549:39 - function of even a complex functions
549:41 - we'll try to take out in the in the in
549:44 - this video and the next couple of videos
549:46 - so try to see that and then we'll see a
549:49 - local Minima and Maxima we'll try to see
549:53 - that uh and then future varibles and
549:55 - then we'll go on integration we'll
549:57 - complete the integration the same way as
549:59 - we completed differentiation however
550:01 - we'll not focus on that much on
550:03 - integration because it is not being used
550:06 - too much but but we'll surely take a
550:08 - look because sometimes it comes
550:10 - integration comes over the research
550:12 - paper then we'll go to the probabil
550:15 - probability Theory and a statistics and
550:18 - then we'll start with deep deep learning
550:20 - and you may be thinking hey why we are
550:21 - learning these much things
550:23 - and just I let let me tell you what
550:26 - happens if the beginners get very
550:28 - confused oh my God what is happening
550:30 - behind them behind the algorithms when
550:33 - they study deep learning so when we are
550:35 - going to start with deep learning then
550:36 - we are going to go very fast way okay um
550:39 - it's because you'll be understanding
550:41 - each and everything each and everything
550:42 - whatever I'm going to write it out and
550:44 - if you don't see these videos if you
550:46 - don't know what is calculus if you don't
550:48 - know what is uh uh what is
550:50 - differentiation integration you been not
550:53 - getting the actual point of view of deep
550:55 - learning and you'll be not understanding
550:56 - deep learning so that'sa we are doing
550:58 - all those things and it's very very
551:00 - useful to learn these as well for your
551:01 - professional career because it will
551:04 - maybe if if you want to go in research
551:05 - it will very uh you'll be able to
551:07 - understand hardest research papers in
551:10 - this era okay so let's get started with
551:12 - the uh differentiation rules I'm not
551:15 - going to recap the previous video but I
551:18 - want you to see the previous video uh
551:21 - which which which you have already had a
551:23 - talk on that but we have we had
551:25 - formulated a definition we have for
551:27 - formulated a definition of
551:30 - differentiation so let me just give you
551:32 - what what definition we have formulated
551:36 - if you want to take out the derivative
551:37 - if you want to take out the derivative
551:39 - Dy by DX and Y is a function okay so we
551:44 - we are solving for y so we if you to
551:46 - take out the derivative Dy by DX we were
551:50 - have we were having a limit definition
551:52 - okay this a using the difference coent
551:54 - when H approaches zero limit sorry f of
551:58 - x + H minus F ofx by H okay and this is
552:06 - this is the this is and You by solving
552:09 - this by using the this we are we are
552:11 - able to get it but when you have a too
552:13 - much complex functions it would very
552:15 - tedious and almost impossible for you to
552:19 - achieve using this okay so that's how we
552:22 - have differentiation rules which will
552:24 - help us to uh take out the derivative of
552:27 - the functions okay so let's get started
552:31 - so let's get at least get started with
552:33 - the today's video uh the the first rule
552:36 - which I'm going to talk about is the
552:39 - first rule which I'm going to talk about
552:41 - is uh the the the constant rule okay so
552:45 - every everyone should should have to
552:47 - should should be familiar with it it's a
552:49 - constant rule the first rule which I'm
552:52 - going to talk about about is the
552:56 - constant the constant rule so what this
553:00 - rule tells if you have a function let's
553:02 - say F ofx equal to 5 okay f ofx equals
553:08 - to 5 and and this and the derivative
553:12 - will be zero the derivative will be zero
553:14 - and the slope will be also zero because
553:16 - derivative is nothing but a slope and
553:18 - over here the it the the derivative the
553:21 - change the it is nothing is changing so
553:25 - derivative will be zero so the
553:27 - derivative the derivative of this
553:30 - function so basically we'll write y = 5
553:32 - so derivative of f ofx with respect to X
553:36 - so we write f
553:38 - ofx is equals to Z okay for this
553:41 - function the derivative for this
553:43 - function f ofx is with respect to X is
553:46 - equals to zero nothing is changing okay
553:49 - nothing is changing uh just I'm to
553:51 - introduce to you the note notations just
553:54 - I want to introduce to to the notations
553:56 - we can write this in a short form as
553:58 - well F Prime x equals to Z so this is
554:03 - this this not notation this notation d
554:07 - uh F
554:08 - ofx uh by DX D of Dy by DX is
554:14 - equivalently equals to this equivalently
554:17 - equals to this okay so it is just
554:19 - telling fime X just a short form which
554:21 - will use interchangeably we we we'll use
554:24 - this uh consistently both the formats so
554:27 - that everything is crystal clear so
554:29 - we'll use both the things uh very
554:31 - frequently at the same time okay so that
554:33 - it saves the time and I like this
554:36 - notation much better but for the
554:38 - definitions of the con of the rules
554:40 - we'll use this notation so that
554:42 - everything is clear okay so we'll use
554:44 - that these two notations very frequently
554:46 - over this
554:47 - video so let's take some more examples
554:50 - so this was the one of one of the
554:51 - example the another example which I want
554:54 - to talk about is G of x = to 7 G of x =
554:59 - to 7 so what is the derivative what is
555:02 - the derivative of this function G of X
555:05 - of this function G of X with respect to
555:07 - X which is also equals to Z or F G Prime
555:12 - X is equals to Z okay so this is the
555:15 - constant rule which we had a talk on
555:18 - okay now let's go ahead now let's go
555:21 - ahead now let's go ahead let me just uh
555:24 - finish this uh this stylist now let's go
555:27 - ahead let's talk about the functions
555:30 - like let's take an example uh you want
555:33 - to you want to take out the derivative F
555:36 - ofx = to X the power 5 x to the power 5
555:41 - you can use this limit definition to to
555:43 - do this but but there is a very quick
555:46 - way to take out the derivative of this
555:48 - function okay how how much X is changing
555:51 - when uh Y is changing changing okay so
555:54 - uh you see the definition in in my
555:56 - previous video because you'll be getting
555:58 - more bro broader view what this
556:00 - derivative is trying to do okay so we we
556:04 - we can use a rule called power rule okay
556:08 - there is this is a very very extensively
556:10 - used in other rules as well so please
556:13 - make sure that that you understand each
556:14 - and everything so f of x = to x to the
556:17 - power 5 okay so so let's take an example
556:21 - you take out the derivative of this
556:23 - function f ofx of this function f ofx
556:27 - with respect to X so how how will you do
556:30 - it so derivative of this function f ofx
556:33 - with respect to X which will be nothing
556:35 - but what you do you take this power you
556:38 - take this power and bring in front of
556:41 - the bring as a coefficient of x so you
556:43 - will take you'll take this power okay
556:46 - bring that in the front of the X okay
556:50 - and reduce that power
556:53 - uh by 1 okay so 5 - 1 so it will be
556:57 - nothing but 5 x ^ 4 okay 5 x X to the^ 4
557:03 - so there are two things which you have
557:04 - done there are two things which you have
557:06 - done the first thing which you have done
557:08 - is to brought the power brought the
557:12 - power brought the power to the to the
557:18 - just as a coefficient of that X okay and
557:22 - reduce
557:23 - used the the the the power at that x
557:27 - value at that x value by one by one okay
557:33 - let's see some some more examples of it
557:35 - so that it would more make sense at
557:37 - least to you okay so let's take an
557:40 - example that you have a function again
557:43 - let's use the function name of f ofx
557:45 - which is equals to x² which is equals to
557:49 - x² okay so how you're going to take out
557:52 - the derivative of this function the
557:54 - derivative of the
557:57 - function derivative of the function so
557:59 - how you will take out so what we do we
558:02 - take this two bring in front of x 2 x 2
558:07 - - 1 so it will be left with 2X and this
558:11 - is one so we we we can neglect it but
558:13 - this is your actual derivative this is
558:16 - your derivative of your uh x² okay so uh
558:21 - this is the D derivative of this
558:22 - function f ofx okay if if you use this
558:25 - limit definition it would take around 2
558:27 - to 3 minutes or maybe 5 minutes for you
558:30 - at least to take out two to reach till
558:32 - 2x but in two steps or two seconds or
558:35 - even one seconds we we completed it okay
558:38 - so so so that's why these rules are very
558:41 - very important as well let's talk about
558:44 - one more example you have another
558:46 - example x^ -2 x the^ - 2 so what will be
558:52 - the derivative ative of this function so
558:54 - the derivative of the function the
558:56 - derivative of this function which is D
558:58 - Dy by DX which is nothing but equals to
559:02 - you bring this minus 2 in front of it
559:05 - you bring this minus two in front of it
559:07 - okay uh now what do you do uh x the^ 2 -
559:12 - 2 - 1 so it will be nothing but - 2 x -3
559:17 - and this is the derivative of this
559:20 - function this is the derivative of this
559:22 - function okay bringing the power in the
559:25 - front of the X and then uh reducing That
559:29 - Power by one okay so that is called the
559:32 - the the the power rule okay the another
559:37 - uh rule which which is which is related
559:40 - to power rule which is nothing but the
559:43 - constant multiple rule the
559:46 - constant the
559:49 - constant multiple multiple
559:53 - rule okay the constant multiple rule so
559:56 - let's take an example that you have f of
559:59 - x okay you have you have a function you
560:01 - have a function you have a function
560:04 - let's name it a y because I I have to
560:06 - write f of x times and times so let's
560:08 - use y equal to 4X to the^ CU okay the X
560:14 - sorry 4X CU and you want to take out the
560:18 - derivative you want to take out the
560:19 - derivative of y Prime or Dy by DX the
560:25 - derivative of the function with respect
560:27 - to X okay so what you do what you
560:31 - do you simply take this scalar okay that
560:36 - multiple four
560:38 - times the d uh uh maybe uh the
560:43 - derivative of x CU okay derivative of x
560:46 - Cube okay so what you will do Dy by DX
560:51 - of uh this x Cube or wait for a second
560:55 - let me just make it more transparent at
560:57 - least so that it will be very very
560:58 - useful for you at least Dy DX xq okay so
561:03 - four * the derivative of x CU will be
561:06 - the derivative of x CU will be what you
561:08 - can use the power rule bring the three
561:11 - in front of it and reduce the power by
561:13 - two
561:15 - 3x² 3x2 it will be nothing but 12 X2 is
561:21 - the derivative is the derivative of the
561:23 - function y okay so Dy by
561:26 - DX which would be nothing but 12 x²
561:30 - let's take another let's take another
561:32 - example let's take another example Y is
561:36 - equals to uh
561:39 - 2x² okay what you what you do you take
561:42 - out the derivative of this and after you
561:44 - taken our derivative and then multiply
561:46 - with two okay so the derivative is
561:50 - equals to uh two * the derivative X squ
561:54 - will be what bring two in the front sub
561:57 - subtract which is 2x which is 4X is the
562:01 - derivative of this function okay so
562:04 - that's called the constant multiple rule
562:07 - are are you getting what I'm trying to
562:09 - say yeah so uh this is the two example
562:14 - let's see one last example so that it's
562:16 - it make completely sense to you uh 4X ^
562:20 - 3 okay 4X to the power oh my my god I've
562:24 - already taken it uh 5 x to the power
562:27 - maybe 4 okay so so how do you take out
562:31 - the derivative of this so what you do
562:33 - five * the derivative of X4 will be
562:37 - bring three in the front sorry bring
562:39 - four in the front then subtract X
562:43 - subtract 4 - 1 which is 3 which is 20x
562:46 - cu okay so this is the derivative of
562:50 - this function the derivative of the
562:52 - function will be 20 x Cub okay so this
562:55 - is this this is called the constant
562:57 - multiple rule another rule which which
563:00 - are which which we are going to see
563:02 - which is called the sum rule okay which
563:05 - is which is again the useful rule uh is
563:08 - very fairly simple rule okay so let's
563:11 - see the sum rule so the sum rule states
563:14 - the sum rule states um let's say you
563:17 - have a function you have a function f
563:19 - ofx which is equals to x ^ 6 + x ^ 5 + x
563:28 - ^ 3 + x ^ 2 + x + 10 what is the
563:34 - derivative what is the derivative of
563:36 - this function
563:38 - or what is the derivative of this
563:41 - function what is the derivative of
563:43 - function so the derivative of the
563:45 - function will be so what is the
563:47 - derivative function so what you do you
563:49 - take out the derivative of every terms
563:52 - okay take out the derivative of every
563:55 - terms okay so what is the derivative of
563:58 - x CU sorry x^ 6 so we we can use the
564:02 - power rule so what is the derivative
564:04 - which will be bring 6 in the front Okay
564:07 - 6X to the power 5 plus bring five in the
564:11 - front 5 x 4 plus beinging three in the
564:14 - front
564:15 - 3x² plus bring two in the front 2x 1
564:19 - plus it will be what it will be 1 okay
564:23 - so because um this is 1 - 1 which will
564:26 - be what uh zero okay so x x to the power
564:30 - Z which will be equals to one so one and
564:34 - this is called this this was the the the
564:38 - constant rule okay so so this the this
564:41 - what is the derivative of 10 which will
564:43 - be zero because the the the there is
564:46 - nothing is changing over there in that
564:48 - for that function okay now this is your
564:51 - actual derivative so we can simplify it
564:54 - 5x ^ 4 + 3x 2 + 2x and this is the
565:00 - derivative of this function okay so this
565:03 - is called the sum rule okay uh the same
565:06 - with diff uh the difference rule works
565:09 - okay so instead of plus there will be
565:12 - minuses okay instead of plus there there
565:14 - will be minuses the one example which I
565:17 - want you to work on is uh G of X which
565:20 - is equals to 2X x ^ 5 + 6 x ^ 8 + 10 x
565:27 - the power - 1 okay try to work in it and
565:31 - take out the derivative if the function
565:33 - is differentiable take out the
565:34 - derivative of it
565:36 - okay maybe it it is differentiable maybe
565:40 - make it plus okay so you want to take
565:43 - the derivative of this function G of X
565:46 - Okay cool so uh try try to do this just
565:50 - pause this video and try to do this if
565:51 - you can go ahead otherwise let me try to
565:54 - do this for you so so um so let's do
565:59 - this so 2 * uh it is 4 uh X so sorry I
566:05 - just m it's it's my habit I don't know
566:08 - why 5 x ^ 4 + 6 * uh 8 x ^ 7 plus uh May
566:18 - over here it would be 10 * 1 okay okay
566:22 - 10 * 1 and over here the left will be
566:26 - the left will be uh 10 X4 + 68 48
566:33 - X7 + 10 okay so this is the derivative
566:37 - of this function okay so try to work on
566:40 - several examples the problem set which
566:42 - will be released through your LMS okay
566:45 - you you can see from there uh the soon
566:49 - the the LMS will be updated with the
566:52 - prior information the certificates of
566:54 - the the chapter number one and the
566:57 - chapter number two will be given to you
566:59 - okay please see the elements for the
567:01 - same and the email if you have enroll
567:03 - elements because there there are only
567:05 - around 100 students which have enrolled
567:07 - I want more of the people to enroll at
567:09 - least
567:10 - 400 okay another uh now we have seen
567:15 - this the constant rule power rule
567:16 - multiple sum Rule and difference Rule
567:19 - now what I want you to do is to remember
567:23 - some the how do we differentiate trig
567:25 - values and it would very tedious start
567:27 - for me to at least showcase so what what
567:29 - I want I have I have seen a lot of
567:31 - people to me memorize this because this
567:34 - is very very handy okay so we are going
567:37 - to talk about differentiating
567:39 - differentiating just just don't memorize
567:41 - it you don't have to see how it came and
567:44 - how we derived it okay this is um most
567:47 - of the people just remember this means
567:50 - who who whoever is is uh good at this
567:54 - okay uh just you can uh what is the
567:56 - derivative of sinine of X with respect
568:00 - to X which is nothing but cosine of x
568:03 - okay the derivative of s of X with
568:05 - respect to X is cosine of x
568:08 - derivative of tangent of X with respect
568:11 - to X which is nothing but
568:15 - secant SEC SEC squ X okay uh another is
568:21 - uh
568:22 - derivative of SEC x with respect to X
568:27 - which is nothing but SEC X Tan x okay
568:33 - another is
568:34 - derivative of cosine of x with respect
568:38 - to x minus sin of x minus sin of X and
568:43 - another is derivative of cotangent of X
568:48 - with respect to X which is nothing but
568:50 - equals to minus
568:53 - cose Square X okay and the last one is
568:58 - derivative of cosecant of X with respect
569:02 - to derivative with respect to X which is
569:05 - nothing but minus the S cosecant x times
569:09 - the cent of X okay so the these are the
569:14 - six trig values which I want you to
569:16 - remember okay because it will be very
569:19 - very useful when we talk when when we
569:21 - talk for further rules and very very
569:23 - handy as well okay you can always view
569:26 - means if if you do the lot of practices
569:28 - you will it will be automatically
569:29 - memorized in your mind because I haven't
569:31 - seen the lecture notes for writing these
569:33 - even what whatever I'm teaching I'm not
569:35 - seeing my lecture notes because I
569:37 - practice a lot of it I remember
569:39 - everything okay so uh try to just
569:42 - practice it out it will very easy for
569:43 - you to at least understand it these are
569:45 - six streak values which is very very
569:47 - Handful in your Calculus Journey till
569:49 - now okay and this is taken from one of
569:51 - the most most famous book on calculus
569:53 - are calculus for dummies okay so now
569:58 - what I'm going to now what I'm going to
570:00 - do is we have seen the some of the diff
570:03 - how do some of the different some of the
570:05 - derivatives of the some of the trick
570:06 - values which is six Tri values uh there
570:10 - you can search for online if you want to
570:12 - see the steps how they have come up with
570:15 - and if you want me to do just feel free
570:17 - to comment it below maybe I can make a
570:19 - next video on it but again this is a
570:21 - toally optional one okay okay so now
570:26 - what I'm going to do is uh talk about
570:28 - the last thing differentiating
570:31 - exponential and logarithmic functions
570:34 - okay at least this is very very
570:35 - important as well how do we
570:37 - differentiate a logarithmic function and
570:40 - how to differentiate uh how to
570:43 - differentiate exponential functions and
570:45 - logarithmic functions and then we'll see
570:48 - uh in the next video we'll try to see uh
570:51 - the product rule and the quotient Rule
570:52 - and in the next and in and then in the
570:55 - next video we'll try to see in the next
570:57 - to next video we'll try to see the chain
570:59 - rule of differentiation we'll try to see
571:01 - how to use different different like
571:03 - products rule in chain rule how it is
571:05 - very very useful in that then we'll see
571:08 - the diff implicit differentiation we'll
571:10 - see the logarithmic differentiation
571:12 - which is very which is very very useful
571:14 - in in your Calculus Journey at least
571:17 - okay let's get started with uh
571:19 - exponential functions okay exponential
571:23 - functions let's get started with that
571:26 - uh exp differentiating
571:30 - exponential
571:31 - exponential uh functions exponential
571:35 - functions let's start with it uh so what
571:39 - is the derivative what is the
571:42 - derivative of e ^ x by sorry with
571:47 - respect to with respect to X which is
571:51 - itself
571:52 - okay so that the derivative of this e to
571:54 - the^ X is itself the derivative is is
571:57 - its own function okay so e to a to the
572:00 - power x okay if you want to see how it
572:04 - came I would just link some video some
572:07 - resources which I really really like to
572:09 - because if I again go ahead and teach
572:10 - you it will very very uh boring um and
572:14 - also it is out out of the bound so what
572:16 - I suggest you to see the link in the
572:18 - video description okay so the the
572:21 - derivative of any exponential function
572:23 - means the e e to the power x is equals
572:26 - to the itself okay but what if we have a
572:30 - function f ofx which is equals to 5 E
572:33 - power x 5 e the power x so what is the
572:37 - derivative what is the derivative of
572:39 - function what is the derivative of this
572:42 - function what is the derivative of this
572:45 - function what is the derivative of the
572:47 - function so the derivative of this
572:50 - function will be what you can do what
572:52 - you can do you can simply you can use
572:55 - the constant multiple rule over here
572:57 - okay you can use the constant multi
572:59 - multiple rule so here's how here's how
573:01 - I'm going to do it so Dy d by DX of 5 e
573:08 - x so what I'm going to do is to take
573:10 - that five outside to it 5 * d by DX e to
573:15 - the power x okay so which will be
573:18 - nothing but 5 *
573:22 - uh which would be uh which will be
573:25 - nothing but five times and it the the
573:28 - derivative of the E to ^ x is itself e
573:30 - to the^ X so that will be nothing but 5
573:32 - e x and which yields the same thing okay
573:36 - so the derivative of any exponential
573:38 - function most of them which have ever
573:40 - seen it's its Alpha
573:42 - function okay so I hope that pretty much
573:45 - everything is clear and here we use the
573:47 - constant uh constant multiple rule
573:51 - multiple root okay uh to to to to take
573:55 - that five outside the derivative and
573:58 - then multiply with the derivative of e^
574:02 - x cool so we have seen some exponential
574:05 - functions you can try out
574:07 - 6X 10 x whatever you you want to try try
574:11 - it man okay so we have seen exponential
574:14 - function now let's see uh now let's see
574:18 - another thing which is a logarithmic
574:20 - function which is Game Changer over here
574:23 - and I if you don't know about logarithms
574:26 - I'll link a video I link a I link a
574:29 - simple uh uh Nancy p video which I
574:32 - really like her videos because uh she's
574:34 - she she is an MIT graduate and she
574:37 - teaches very well these things she
574:39 - teaches very well and uh now she stopped
574:42 - making the videos but uh his her videos
574:46 - on calculus I know the it is it is bit a
574:50 - half incomplete but she she taught these
574:53 - things very well like logarithms some
574:56 - the rules like chain rule I even watched
574:59 - her video amazing videos by her so you
575:01 - can check out the channel I'll just give
575:03 - a link in the description okay so uh
575:07 - another thing which I'm going to talk
575:08 - about is lo
575:11 - logarithmic functions how do you
575:13 - differentiate the logarithmic
575:15 - functions okay so let's take an example
575:17 - you to differentiate the function uh
575:20 - which is f of X which is nothing but
575:23 - natural logarithm of X okay so how do
575:26 - you take out the
575:27 - derivative of this function which is
575:30 - natural logarithm of X okay so which is
575:33 - nothing but log base ex isn't it log
575:36 - base e so which will be nothing
575:40 - but um which which which which will be
575:42 - nothing but 1 /x so the derivative of
575:46 - the natural logarithm of X which is
575:47 - nothing but 1 /x okay uh we will see
575:51 - when the when your base is not when when
575:54 - when your base is something because what
575:56 - we do we tend to me memorize some of the
575:59 - differ derivatives okay and then we'll
576:02 - solve further things using that whatever
576:04 - we had memorized and it's not a big deal
576:06 - to remember four to five derivatives
576:09 - okay so this is your derivative of
576:11 - natural logarithm of divided with
576:13 - respect to X which will be nothing but
576:15 - One /x
576:17 - one/ X so if your if if the log base if
576:23 - the
576:24 - log base B is a number is a number other
576:30 - than e
576:32 - other than e if it is other than e you
576:36 - can tweak this derivative a little bit
576:39 - okay so let's take let's take let's for
576:41 - for for the sake of an example you have
576:44 - you have a function f of
576:46 - x make it f ofx equals to uh oh my God
576:50 - my hand is painting now so F ofx which
576:53 - is nothing but equals to uh what say uh
576:57 - what say uh log base 2x okay uh log base
577:02 - 2X and uh how we do it so what we do
577:07 - what we do we simply if you take the
577:09 - derivative of this of this function
577:12 - which is nothing but 1 /x okay which is
577:15 - your natural logarithm by okay ln2
577:20 - natural logarithm of two okay so natural
577:23 - logarithm of two okay so when you do
577:26 - this 1 / X natural logarithm of 2 this
577:29 - is how you do it for if the log Bas is
577:31 - different let's take let's take another
577:34 - example if you want to take out the
577:36 - derivative of a function which is log
577:40 - base 10 x you you you also write as log
577:43 - X which is by default the the base is 10
577:47 - okay so I'm going to take out the
577:48 - derivative of this Dy by DX X of this
577:52 - function log base 10 x which will be
577:55 - nothing but equals to first of all 1 /x
577:59 - you all agree 1 1 /x divided by natural
578:04 - logarithm of 10 and uh when you do this
578:07 - 1 /x Ln 10 okay and that is the
578:11 - derivative of this function so this is
578:13 - how you take out the the the the
578:15 - derivative of the logarithmic functions
578:18 - and I hope this is pretty pretty pretty
578:19 - much clear to you okay
578:22 - okay what if we try to cover this in
578:24 - this video our product Rule and quent
578:26 - Rule just wait for a second I have to
578:27 - see uh the timing which we have covered
578:31 - we can do yeah um yeah let's cover the
578:34 - product rule let's cover the quo rule
578:37 - okay so uh no no no let's let's get on
578:40 - to the next video to cover these two the
578:43 - product rule and the question rule uh it
578:46 - it would be better for us at least so
578:47 - that the video duration should be short
578:49 - okay so we have seen a lot in this video
578:51 - I'll be catching up in next uh so now
578:54 - let's get started with uh product rule
578:57 - because in the previous video we had a
578:59 - talk on this uh various basic
579:01 - differentiation rules Which is far more
579:04 - very very easy to understand and uh from
579:06 - this video the things will get started a
579:08 - little bit difficult but as a sum rule
579:13 - as the as the difference rule the same
579:15 - will be the product rule but the product
579:17 - rule is bit important and quent Rule is
579:19 - also bit important for solving the
579:22 - derivatives questions and as well as uh
579:24 - in a when and the the reason why I'm
579:26 - teaching these because this is this
579:28 - product rule and question rule is
579:29 - important in your chain Rule and
579:31 - implicit differentiation and chain rule
579:33 - is the the far most important thing
579:34 - which you have to cover uh for deep
579:36 - learning and then so so so so that's why
579:38 - I don't want to leave this product rule
579:41 - and the quti rule otherwise I would have
579:42 - definitely skipped but these two
579:44 - important rules which I can't forget the
579:46 - reason why you all know is uh these are
579:50 - my favorites uh because it is e easy to
579:53 - teach okay so assume that's what I can
579:58 - say to you that's what I can say to you
580:01 - assume that you have two functions oh my
580:04 - God my hand is painting again I don't
580:07 - know what happened to my hand and what
580:09 - do you think I should uh take some break
580:11 - or what no let's not take break let's
580:14 - continue working on otherwise tomorrow I
580:17 - will die and uh no one knows what will
580:20 - happen uh so you have a two function f
580:22 - of x and G of X which are
580:25 - differentiable okay which are
580:28 - differentiable means it is we can take
580:29 - out the derivative of this we'll see
580:31 - when the function is differentiable and
580:33 - when the function is not in our later
580:35 - videos or you can search online for the
580:36 - same but say you have function which is
580:39 - differentiable and and now if you take
580:43 - now if the function are in this format
580:46 - so you have H ofx which is f f of x time
580:53 - G of X okay so you
580:56 - have like a product of kind of thing for
580:59 - example you have a function y x Cub time
581:05 - sin of x s of X so how how how you going
581:09 - to take out so this is your f of x this
581:12 - is your f of x and this is your G of X
581:16 - okay so how do you take out the
581:17 - derivative of this so
581:19 - derivative the derivative of this kind
581:22 - of function Dy by DX of a function f
581:28 - ofx g of X which will be nothing which
581:32 - will be equals to F
581:35 - ofx times okay the derivative the
581:40 - derivative of G of
581:42 - X of G of X okay of G of X Plus G of X
581:50 - time the derivative of f
581:53 - ofx of f ofx okay so so this this is how
581:59 - you do it first of all um you you you
582:02 - leave the first thing and then you
582:03 - multiply with the derivative of the
582:05 - second function then you plus it then
582:06 - the you leave the second thing you
582:08 - multiply the first one okay it get you
582:11 - can write in this way because just to
582:14 - make sure that you will be not be
582:16 - confused I can write in different
582:18 - different ways I can write in different
582:20 - different ways is first of all I can
582:22 - take this derivative and then this one
582:25 - okay so for example d d by DX f of x
582:31 - okay uh times G of X leaving the second
582:35 - function alone plus then leaving the
582:37 - first function alone times taking of the
582:40 - second function changing the position
582:42 - which does not matter at all okay
582:44 - because you will see it that I will do
582:46 - it because that's my way of doing this
582:49 - everything
582:50 - interchangeably coming to the next
582:52 - notation we can write this out let's
582:54 - assume let's assume if let assume let
582:57 - assume u b f
583:00 - ofx and uh V I think V yeah let's assume
583:06 - V be G ofx let's let's assume V be G ofx
583:12 - so you can also do this but you you'll
583:14 - just replace u u f f of x with u and G
583:18 - of X with v okay another notation which
583:22 - which which I'm going to write it out
583:23 - which is very very uh important F
583:27 - ofx times G Prime X the so this we can
583:32 - write the derivative in this way so this
583:34 - this can be written this way plus G of x
583:37 - * F Prime X okay so this is the quotient
583:42 - rule U which we which is the definition
583:45 - of a quotient Rule now let's see some of
583:47 - the examples so that this is pretty
583:49 - pretty pretty much clear to you every
583:51 - every every one of you so y = x Cub *
583:56 - sin of X sin of X okay so what is dy by
584:02 - DX it will be nothing but so you take
584:05 - this x Cube so so X CU * d by
584:11 - DX sin of X okay
584:15 - plus uh you leave the you now now what
584:18 - do you do you leave the second one as it
584:22 - is times you take out the derivative of
584:24 - the first function first okay first f of
584:27 - x which is X the derivative
584:31 - of x CU which will be something like
584:35 - this coming to now now uh X CU times the
584:39 - derivative of s of X remember remember
584:43 - the derivative of s of X which is what
584:45 - equals to cosine of x cosine of x plus
584:50 - now you leave the S of X as it is times
584:53 - the derivative of x Cub so you bring
584:55 - three in the front of
584:56 - it x² okay now it will pretty much
585:00 - everything is clear everything will be
585:02 - clear which will be nothing but X Cub
585:06 - cosine X Plus sin of x uh okay so you
585:12 - can do something like this maybe bit
585:14 - more cleaner 3x² sin of X okay so this
585:19 - is the derivative this this is the
585:21 - derivative of your function okay so let
585:24 - me just make it a little bit more
585:26 - transparent so that everyone is AAL to
585:28 - so this is a derivative and this is the
585:30 - product rule from which we have achieved
585:33 - let's take another simple example so
585:35 - that uh it would very very very much
585:37 - clear to everyone uh 6 x² + 7 x CU so
585:45 - what is the derivative of this function
585:48 - Dy by DX so first of all you take out
585:52 - the first one and leave the second one
585:54 - so 6x^2 Prime okay so this Prime I'm
585:58 - saying this the derivative of this
585:59 - function 6 S Plus okay let's let's
586:03 - multiply we have to multiply 7 x Cub
586:06 - plus now you take the second second one
586:09 - prime uh times you leave out the first
586:12 - one and then when you when you do this
586:15 - uh you just simp 6 * 2x² which is
586:19 - nothing but uh 2x which is
586:22 - 12x uh * 7 X3 7 X3 okay * 7 X3 uh now
586:30 - you simply you can simplify further it
586:32 - and then you can do it and now you take
586:34 - out the Dera derivative of this and then
586:36 - you can simplify it further on which is
586:38 - very very easy for you at
586:40 - least okay uh try to do with and answer
586:43 - answer me in the comment box coming to
586:46 - the this this was a product rule now
586:48 - let's talk about another rule which is
586:50 - the I rule which is the quotient rule
586:53 - where you may you may have already been
586:56 - guessed your functions are being divided
586:58 - okay so let's do this let's let's let's
587:02 - let's do this so let's say you have uh
587:05 - you want to take out the derivative of a
587:08 - function like y = to sin of x / X to
587:14 - power 4 so how you how you're going to
587:16 - take up so we have something called as
587:19 - the uh uh quotient rule the quotient uh
587:24 - rule which which you can use to take out
587:27 - the derivative of these kind of function
587:29 - uh d by
587:32 - DX uh let me write the formal definition
587:35 - first because I do believe in formal
587:37 - definition so let's write a formal
587:39 - definition and then we'll come up with
587:40 - then then we'll Sol solve this example
587:43 - okay so what you will do uh so the
587:46 - derivative of this the quent rule state
587:48 - some something like this G of x time F
587:52 - Prime x f Prime x minus you leave the
587:56 - now you leave the f of x now you simply
587:58 - multiply with uh G Prime X okay and then
588:03 - divide it with GX s GX s okay leave G of
588:10 - X multiply with the derivative of f ofx
588:12 - minus f of x leave f of x and then
588:14 - multiply the derivative of G of X and
588:16 - then multiply with G of x² okay sorry
588:19 - divide by G ofx Square okay and this is
588:22 - the your quotient rule we can write it
588:25 - in this way format as well G of x * the
588:29 - derivative of um f ofx okay minus F ofx
588:36 - time Dy by d uh
588:41 - GX okay so you and then divided by G
588:48 - x² okay now you now you now using this
588:52 - you can take out the the the the the the
588:56 - the the the the derivative okay uh of
589:00 - these kind of functions so now now let's
589:02 - do one of one of the example now let's
589:04 - do one of one of the
589:06 - example let's do one of one of the
589:08 - example uh which is d by
589:13 - DX okay you going to take out
589:16 - of sin of X
589:19 - over X x the^ 4 okay so how you will do
589:23 - it how will you do it so uh first of all
589:27 - this is your F ofx and this is your G of
589:29 - X which is first of all you take out F
589:31 - ofx sin of X derivative of this
589:36 - times leave x ^ 4
589:40 - minus uh s of X leave that times take
589:45 - out the derivative of this okay divided
589:48 - by x^ 4² great okay uh now what do you
589:53 - do the co the that the derivative that
589:55 - you all remember cosine of x time uh x ^
590:00 - 4 - sin of X so times the it is nothing
590:07 - but uh 4X ^ 3 okay and divided by x ^ 8
590:15 - you keep doing this now let's simplify
590:17 - it bit a little little bit x to the
590:20 - power
590:21 - 4 cine of
590:24 - xus uh 4 x CU sin of x / x ^ 8 okay now
590:32 - you just keep on doing this H keep take
590:35 - as a factor x^ 3 okay and then
590:40 - x cosine of x for this minus uh 4 sin of
590:46 - X okay and take that as X to the^ 8 now
590:51 - you can simply use the EXP exponents you
590:53 - can simply uh use uh you can simply
590:56 - minus it out okay you can simply minus
590:58 - it out so it will be left with X cine of
591:03 - x - 4 sin of X okay uh when you X which
591:09 - is will be which will be x^ 5 okay this
591:13 - is X and this is your final derivative
591:17 - after after you simplify everything this
591:18 - is your derivative either you can go
591:20 - with this but I suggest some simplify as
591:23 - much as you can so at least it is doable
591:25 - just just you can put in the values of X
591:27 - and then you are done okay so we are we
591:30 - are done with this quent rule as well as
591:32 - we are we are done with this period rule
591:35 - okay I hope that this is pretty much
591:37 - clear to everyone if you have any
591:39 - problem any doubt you can ask in our
591:41 - Discord server uh or you can see the
591:45 - student manual if if you don't know how
591:46 - to enroll in LMS just to update uh let
591:50 - me write in enroll in LMS so just
591:54 - enroll enroll
591:57 - in
592:01 - LMS everything is Free Your da support
592:05 - is free everything is free so you can go
592:08 - there enroll there and I hope that you
592:10 - will take this opportunity to uh learn
592:13 - deep learning from a scratch and learn
592:16 - as as compr as as comprehensive as you
592:20 - can okay so thanks for seeing this video
592:22 - I'll be catching up your next video
592:23 - talking about the chain Rule and
592:25 - implicit
592:26 - differentiation till then bye-bye have a
592:28 - great thing hey everyone in this lecture
592:30 - we are going to talk about chain rule of
592:34 - differentiation we'll try to know about
592:36 - more about it and it is very very very
592:39 - very very used in deep learning or
592:41 - whether we learn back propagation we'll
592:45 - we'll be we'll be using this this rule
592:47 - which is a chain rule to take out the
592:49 - derivative when we back propagate okay
592:53 - so I ask you to pay a special attention
592:56 - at this rule so
592:58 - basically so this rule tells if you have
593:03 - if you want to take the derivative of a
593:05 - function okay so let's assume Dy by DX
593:10 - it's nothing but the derivative the
593:13 - derivative
593:15 - derivative of inner function leaving the
593:19 - outside function and the
593:21 - derivative of the inside function okay
593:24 - so derivative of the outside function
593:26 - leaving the inside function time times
593:30 - the derivative of inside function but
593:33 - but it I know it does not make any sense
593:35 - to you so let's talk about uh uh wait
593:39 - wait for a second let me just delete it
593:41 - out so let's let's let's let's take one
593:44 - example and then we'll formulate this
593:46 - definition then we'll formulate this
593:49 - definition of chain R so the example
593:51 - States the example States let's say you
593:55 - have uh y =
594:00 - 3x + 1 to the^ 7 okay so this is your
594:07 - function this is your function which you
594:09 - want to take out the derivative so you
594:11 - can actually use a power rule and then
594:13 - you can use a lot of rules available but
594:16 - um it it will be very uh hectic for you
594:19 - to do it so we have a chain rule of
594:22 - differentiation which will help you to
594:24 - do this so what is the derivative of
594:27 - this function so Dy by DX or Dy by DX so
594:33 - using the chain rule what what we do
594:36 - first of all let's use the power
594:38 - rule okay so let's use the uh let's use
594:42 - the power rule to just make that seven
594:45 - into the front of 7 so what we are doing
594:49 - is taking out the derivative of the
594:51 - outside of function so the derivative of
594:55 - outside of a function okay we we are not
594:58 - touching inner one this is our inner one
595:00 - so we are taking the derivative of the
595:02 - outer one okay so outer one is some
595:05 - value okay seven so what we will do
595:08 - derivative of this will be we bring
595:10 - seven into the front of it whatever will
595:13 - be and this will be 6 7 - 1 using the
595:16 - power rule okay so leaving the inside
595:19 - function as it is leaving the inside
595:21 - function as it is and then using the
595:23 - power rule we have bring the seven over
595:25 - here and subtract one from there and
595:27 - this is the derivative of the outside
595:30 - function
595:32 - derivative derivative of outside
595:36 - function okay derivative of outside
595:39 - function okay and out outside function
595:42 - was uh we have some value and just to
595:45 - the power 7 okay that's the derivative
595:49 - now the time
595:50 - the derivative of inside function times
595:53 - the derivative of inside functions so
595:55 - what is the derivative of over here 3x
595:58 - the derivative of 3x will be what will
596:01 - be 3 plus we'll use a sum Rule and
596:05 - derivative of constant is simply zero so
596:07 - that will be 3 okay so the derivative
596:11 - will be U 7 * 3 21 3x + 1 to the^ 6 and
596:19 - this is the D derivative of your
596:21 - function which is this 3x + 1 the^ 7
596:25 - again I'm recapitulating so that it
596:27 - could make more sense to you okay uh
596:30 - let's take an example another
596:33 - example let's say you have G of X which
596:36 - is nothing but um let's say
596:39 - 2x plus maybe 4 okay and we are squaring
596:46 - this up we are squaring this up so um so
596:50 - so now let's try to solve it so first of
596:52 - all what we what we will do we'll take
596:54 - out the derivative of the outside of
596:55 - function the derivative outside of
596:57 - function which will be you bring two in
596:59 - the front of it two leave this as it is
597:02 - subtract one from there add one that's
597:05 - fair enough times we have taken our
597:07 - derivative now we times 2x + 4 constant
597:12 - this is this is a constant so we this is
597:14 - a zero the derivative of this will be
597:16 - zero and this will be
597:18 - two okay time 2 so it will be 4 2x + 4
597:24 - is the derivative of this function G of
597:27 - X okay so this is what the chain rule
597:30 - says now let's take another now let's
597:33 - take another example so that it could
597:35 - make more and more sense to you
597:38 - otherwise it would be very U hectic for
597:40 - you at least so let me take let me just
597:41 - remove it out let me take another
597:44 - example um so another another example is
597:48 - uh let's say you have f ofx X okay which
597:51 - is equals to sin 3 okay uh 5 x² - 4x so
598:00 - what is the derivative of this function
598:02 - so the derivative of this
598:04 - function we can write in this way so the
598:07 - derivative of the function will be
598:10 - sin 5x^2 - 4x we can write in this way
598:15 - as well okay so we can write in this way
598:18 - okay because you all know that
598:20 - trigonometry is not just on this uh so
598:22 - it will be three over here now we can go
598:25 - ahead and taking out the outer
598:26 - derivative we can take out we can go
598:29 - ahead and take out the U out outside of
598:32 - this function so outside of the function
598:35 - will be so outside of a function will be
598:38 - uh this will
598:41 - be three we bring three in the front of
598:44 - it we bring three in the front of it
598:47 - s f of x² - 4X the same inside we leave
598:51 - inside one okay subtract 3 from two as a
598:54 - power rule states times the derivative
598:57 - of inside of a function derivative of
599:00 - insert inside of a function so the
599:03 - derivative of of inside of function sin
599:08 - 5x² - 4x okay take out the derivative of
599:13 - inside of a function okay so I hope that
599:17 - this is pretty much clear as of now now
599:19 - you can go ahead and take out the
599:21 - derivative of this inside of a function
599:23 - and you will be good to go okay and you
599:26 - will be good to go you just take out the
599:27 - derivative and this is your homework you
599:29 - all know how to take out so just try to
599:32 - take out and please give me in the
599:34 - comment box okay now another example you
599:37 - can just take out the derivative as s um
599:41 - is cosine you can actually use the
599:42 - product rule which you all know so go
599:45 - ahead and try it
599:47 - out coming to coming to the next example
599:50 - is let's say you have uh let's say you
599:54 - have um let's let me take another
599:57 - example you have a k of X or G of X
600:00 - let's say you have a function G of X
600:02 - where you have a function
600:04 - sine x² - 3x so what you will do first
600:09 - for first thing is take out the
600:11 - derivative of outside of a function so
600:13 - the outside of function so the that is
600:15 - nothing but there is a sign of something
600:18 - okay so you then when when we take out
600:20 - the derivative outside of function so
600:22 - what is the derivative of a sign we have
600:25 - studied the cosine okay and leaving the
600:28 - inside as it is times the derivative of
600:32 - inside of a function okay so the x² the
600:35 - derivative x² is 2x - 3 okay so the
600:41 - derivative so the derivative will be
600:44 - what the derivative will be cosine of x²
600:49 - - 3x x * 2x - 3 is the derivative uh G
600:56 - derivative of a function G of X okay so
600:59 - I hope that chain rule is giving you
601:01 - some of the definition or some of the uh
601:04 - some of the way to think about it how
601:06 - this and this works as you as you see
601:08 - these are composite functions okay so so
601:11 - that's why CH this chain rule works you
601:13 - can see online why this chain chain rule
601:16 - works because it is important to know
601:18 - about that another example which I want
601:20 - to put in front of you is here I I want
601:24 - you to work in this in this example you
601:26 - have H of X which is equal to X2 + 5x 5x
601:32 - - 6 to the^ 9 so what you do you bring 9
601:37 - into so first of all you take out the
601:39 - derivative outer of a function that will
601:40 - be 8 x^2 + 5x - 6 to the power uh sorry
601:47 - this is 9 this is 8 okay uh now times
601:51 - the derivative of inside of a function
601:53 - so the derivative of inside of a
601:55 - function be 2x + 5 okay that is 2X and
602:00 - five and this constant is zero okay so 9
602:04 - X2 + 5x - 6 ^ 8 and 2x + 5 is the
602:12 - derivative of H
602:15 - ofx okay so now I hope that is pretty
602:18 - much clear so let's write write the
602:20 - formal definition let's write the formal
602:23 - definition let's write the formal
602:25 - definition of chain R so let's say you
602:28 - have a function if you have a function
602:31 - okay which is f g of X which is just
602:34 - like composite function okay and it is a
602:38 - comp composite function then then the
602:42 - derivative of y of of a function will
602:47 - be uh the derivative of will will be
602:51 - first of all we take out the derivative
602:53 - out Outer side okay okay I'm noting this
602:56 - is f Prime Times the derivative times
603:00 - the derivative of the inside of a
603:02 - function G Prime X okay so we are taking
603:06 - the derivative of outside of a function
603:08 - leaving the inside as it is multiplied
603:11 - by the derivative of inside of a
603:13 - function okay so over here this is of
603:16 - derivative derivative
603:20 - of
603:22 - outside leaving the inside and this is
603:25 - the derivative of inside of function
603:30 - okay inside of a function so we can we
603:32 - can we can write on different notation
603:34 - so let's WR let's write it out in
603:37 - different notation so the different
603:39 - notation will be let's say if if uh Y is
603:45 - equals to the F of U okay and U is equal
603:50 - to G of X so basically it is just saying
603:53 - F of of G of X okay so U is equals to
603:56 - this now we can write that out into the
603:59 - other formal notation which you mostly
604:01 - see which is not nothing but just wait
604:04 - wait wait for a second let me just drag
604:06 - it over here the derivative of function
604:09 - this Dy by DX will be Dy by du okay so
604:16 - so first of all take taking out the Dera
604:20 - of a function okay times du by DX okay
604:25 - so first of all taking out the outsider
604:28 - then taking out The Insider okay so this
604:32 - is what the formal definition States so
604:34 - I hope that that this is pretty pretty
604:37 - much clear with a lot of examples which
604:39 - you did now coming to the uh chain rule
604:42 - can be used with different different
604:44 - rules okay chain rule with product rule
604:47 - quent rule so that's why chain rule
604:49 - makes it easy to for for for for you to
604:50 - work on so let's let's go let's go ahead
604:54 - okay let's go ahead and and do one uh
604:59 - example which is example uh
605:03 - 4x² okay s of xq so in this example
605:10 - we'll make use a product rule product
605:13 - rule and chain rule to take out the
605:16 - derivative to chain rule take out the D
605:19 - derivative so just have to remind about
605:21 - the product rule if you to take out the
605:23 - derivative of the two functions which is
605:26 - being multiplied this one and this one
605:30 - okay when when when two functions being
605:32 - multiplied okay so which is f ofx f ofx
605:37 - time G of X so that will be the
605:41 - derivative will be first of all we take
605:44 - we we leave the first function as it is
605:46 - times the derivative of the second
605:48 - function which which is G of X plus the
605:52 - derivative of the first function the
605:53 - derivative of the first
605:55 - function times the leaving the second
605:59 - function as it is so this is the product
606:01 - rule which we have started now coming to
606:04 - this what if we let's let's name this
606:06 - function as F ofx now we want to take
606:09 - out the derivative of it so the
606:11 - derivative of it will be 5 Prime X which
606:14 - will be nothing but U first of all we'll
606:19 - by by going we can we can have this as a
606:21 - first and we can have this as a second
606:22 - so it does not it matter order does not
606:25 - matter first of all let's take out the
606:26 - derivative of the F ofx so let's assume
606:29 - that this is the G of X and this is your
606:31 - f of x okay so let's let's let's take
606:34 - out the 4X s okay so the derivative uh
606:38 - 4X s okay take out times leaving the
606:43 - leaving the other function as it is
606:44 - leaving the other function as it is okay
606:47 - plus taking all the derivative taking
606:50 - all the derivative so multiplying F ofx
606:52 - first of all
606:54 - 4x2
606:55 - times uh the derivative s of X Cub okay
607:01 - derivative of the SEC second function
607:04 - okay uh second function now now let's
607:07 - let's take out the derivative so after
607:10 - we take out the derivative which will be
607:12 - nothing but 8 x sin xq so when we
607:17 - multiply so that derivative will be 8X
607:20 - and when we multiply with this so 8X sin
607:22 - x Cub plus now over here which which
607:26 - will get as uh the derivative of s of
607:30 - something is cosine of something but
607:32 - before that over here which you're
607:34 - seeing there is some chance of chain
607:37 - rule there is a some chance of chain
607:39 - rule which in this case which are seeing
607:41 - we'll use the chain rule in this case
607:43 - because we have outer function and we
607:45 - have an inner function and we're a bit
607:47 - confused because we are not seeing any
607:49 - power we we can't do easily with the
607:51 - power rule on anything okay so we'll
607:53 - leave 4X squ times what is the
607:58 - derivative of outside of function so we
607:59 - have a sign of something we have a sign
608:01 - of something so this is outside of
608:03 - function so what is the D derivative of
608:05 - outside of function which would be
608:07 - cosine leaving the inside function
608:11 - times the in the the derivative of
608:13 - inside function the X cube is 3x okay
608:17 - and we and using the chain rule we got
608:19 - the derivative now you're good to go now
608:22 - the derivative is which of this function
608:24 - 8X let me just use a different color so
608:27 - which will be 8X sin x Cub okay + 12 x
608:36 - to the^
608:37 - 4 cosine of x when you simplify it you
608:41 - will be getting this x to the power 3
608:44 - okay and this is your derivative of a
608:47 - function so use product rule and then in
608:50 - one of one of the functions or one of
608:52 - the part we use the chain rule to
608:54 - differentiate nice so we are we we are
608:57 - good to go with u chain rule of
608:59 - different differentiation and I hope
609:01 - that you uh that you got got to know
609:04 - about it much better and I hope you will
609:07 - be uh remembering these stuffs
609:11 - now let's talk about logarithmic
609:14 - differentiation the last thing which I'm
609:16 - talk about is logarithmic
609:18 - differentiation using logarithms you
609:21 - will be easily able to take out the uh
609:24 - to differentiate okay and and these are
609:26 - used to quickly take out the derivatives
609:29 - okay so the last thing which will talk
609:31 - about is logarithmic
609:34 - logarithmic
609:37 - logarithmic
609:42 - differentation okay so we want to take
609:44 - out the logarithmic differentiation now
609:46 - coming to this you have a function f ofx
609:50 - which will be equals to uh let's this
609:53 - this is this is the question we have to
609:55 - take out so X Cub - 5
610:00 - 3x^ 4 + 10 4 x 2 - 1 2x ^ 5 - 5 x 2 - 10
610:12 - we want to take out the
610:15 - derivative of this function and you may
610:18 - be thinking here you oh my God what the
610:20 - hell this is and you'll be also in oh
610:23 - like oh my God how how I'm going to
610:25 - approach this problem so let me tell you
610:28 - this is very very easy problem but uh we
610:31 - will use something called logarithms to
610:33 - solve this so what we are going to do
610:35 - now is to take out the logarithm we just
610:39 - take out the logarithm on both the side
610:41 - okay so take out the log logarithm
610:44 - logarithm of f ofx equals to logarithm
610:49 - just the same thing okay so let me just
610:52 - um snap it out how I'm going to snap it
610:58 - out no no no wait wait wait let me just
611:02 - do it for me at least so that I could
611:04 - not add to the current
611:07 - page over here okay so you take out the
611:10 - logarithm so you take out the
611:13 - logarithm both the sides so oh my God I
611:17 - think it's I have done wrong yeah so now
611:20 - you take out the logarithm of above the
611:23 - side now coming to this now coming to
611:25 - this now what you do you know the
611:28 - property of a logarithm if if you have a
611:30 - lot of fun um in this you you you know
611:32 - all the property of logarithm uh which
611:35 - will be logarithm of X Cube to the
611:40 - individual U terms inside it and to
611:44 - individual uh these things okay so times
611:48 - uh * logarithm 3x ^ 4 + 10 * logarithm
611:56 - 4X 2 - 1 * logarithm 2x 5 - 5x - 10 okay
612:04 - now now when we uh when you you all know
612:07 - the when how when we take out the uh the
612:12 - the when we take out the derivative of a
612:16 - logarithm and you all have studied
612:18 - because I've already talked about this
612:20 - it it is nothing but when when you take
612:22 - out it it will be nothing but uh 1 by F
612:28 - ofx times F Prime X okay so we we can
612:33 - write it out F Prime X over f of x at
612:37 - this side and all and in this side what
612:40 - you do you simply one over 1 / x ^ 3 - 5
612:47 - * *
612:50 - the derivative X x3 - 5 derivative so it
612:55 - will be nothing but 3x 2 3x
613:00 - 2 by x3 - 5 x3 - 5 okay now times do the
613:09 - same thing with this with when then
613:11 - we'll get 12 x CU 12 x Cub by 3x 4 3x^ 4
613:19 - + 10 * uh 8X so it will be 8X by
613:26 - 4x^2 - 1 so you can recheck how how we
613:29 - got it it's simply 1 / 4X 2 - 1 * 4X 2 -
613:35 - 1 derivative so 8 there will be 8X and
613:40 - this is zero so that is 8x by so times
613:43 - uh the last one which will be 10 x ^ 4 -
613:47 - 10 x and we can write it out below
613:50 - things 2x the power 5 - 5 + uh sorry -
613:55 - 10 okay so this is what you got when you
613:59 - take out the when you just take out the
614:02 - derivative of the natural logarithm
614:06 - okay basically uh you when you when you
614:09 - when when when you take out the
614:10 - derivative of a natural logarithm you
614:12 - will do something like this okay and it
614:16 - should be plus when you when you go
614:18 - ahead uh when you take out the natural
614:20 - logorithm of uh of the this this is a
614:23 - property you it will be becoming plus
614:26 - okay will becoming plus now then you
614:29 - differentiate it and then you get it now
614:32 - at last now at last let me just go ahead
614:35 - or let me just keep working on it now
614:38 - you will get F Prime X which is equals
614:41 - to the step number four which is equals
614:44 - to what you will do what you will do uh
614:48 - you will use something called as uh we
614:51 - we'll simply multiply with the logarithm
614:55 - and the actual thing this one okay and
614:59 - this one Whatever the differentiate we
615:01 - got it okay so that will be nothing but
615:04 - first of all we write whatever we got
615:06 - after taking the derivative of our
615:09 - logarithm six this one + 12 x Cub / 3x^
615:17 - 4 + 10 + 8 x by 14 x 2 - 1 + 10 x^ 4 -
615:28 - 10 x / 2x^ 5 - 5 x^ 2 - 10 times big
615:37 - times the question x x ^ 3 - 5 3x^ 4 +
615:44 - 10 4 x^ 2 - 1 2X to^ 5 - fx^ 2 - 10 and
615:54 - this is the your derivative of your
615:56 - large functions function which you saw
615:59 - over here and you may think yeah use
616:02 - this is this this is also a long process
616:04 - I agree this is a long process but if
616:07 - you go and solve with other process I
616:10 - assure you you will be getting you can
616:11 - use product rule to solve it but it will
616:13 - be very this is the very easy to solve
616:15 - this particular Problem by logarithmic
616:18 - difference iation so we talked about
616:20 - chain Rule and the most important thing
616:22 - was chain rule if if you haven't
616:23 - understood the logarithmic you can
616:25 - safely ignore this okay so I hope that
616:28 - you got the problem and I'll be catching
616:29 - up the next video with the introduction
616:32 - to integration and then we'll start off
616:34 - with probability and statistics thanks
616:35 - for watching this video I'll be catching
616:37 - up in next video till then bye-bye hey
616:39 - everyone from this lecture we are going
616:41 - to start off with deep learning we'll
616:43 - talk about we'll start off our journey
616:45 - with actual deep learning which you all
616:47 - are uh w from lots of time and I just
616:51 - want to make a disclaimer is I assume
616:53 - you that you have already covered the
616:54 - previous videos and some videos of
616:57 - probability uh which is not yet uploaded
617:00 - which will be uploaded in coming days
617:02 - but I assure that whatever the videos is
617:04 - uploaded like mathematical prerequisite
617:07 - please be sure that you complete those
617:08 - lectures first you can was in watch that
617:12 - in 2x lectures because all the things
617:14 - which you will study will heavily depend
617:16 - on the previous one okay so so please
617:19 - make sure that you have a better
617:20 - understanding of mathematical intuitions
617:22 - which I already taught if you faced any
617:25 - problem just join our Discord server
617:27 - we'll get or maybe we can get on a live
617:29 - with some Tas you can just discuss that
617:32 - and make your doubt clear and you can
617:33 - simply ask at M our Discord server which
617:36 - will be in the description box uh the
617:38 - link will be in the description box okay
617:41 - so uh what is deep learning so we'll try
617:43 - to answer in this lecture um so first of
617:46 - all I'll try to answer what is machine
617:48 - learning machine learning is an
617:50 - artificial intelligence domain where we
617:52 - extract patterns from the data and
617:55 - analyze the data and make intelligent
617:57 - predictions on the new data according to
618:00 - the pattern which we has already learned
618:02 - or our machine has already learned okay
618:04 - so we have a function f that that takes
618:07 - some input values and that map that
618:09 - input values to an output variable y
618:12 - okay so I'm talking about in the context
618:14 - of supervised learning so basically so
618:16 - basically you have a function f
618:19 - that that takes the input value X and
618:22 - that Maps the input value to Y so you
618:24 - want to construct a function so what so
618:26 - you learn a function f that that ex that
618:30 - extract parents from the data that
618:32 - extract parents from the data or loan
618:35 - learn from the data and analyze the data
618:38 - and make intelligent predictions so we
618:40 - can give X to this input value it will
618:42 - give us y variable for for for for for
618:45 - getting the desired y variable we do
618:48 - need a a very good function which
618:50 - actually does that specific job so
618:52 - exactly what machine learning tries to
618:54 - do is to make a function f that Maps the
618:57 - input value x to the output value y if
618:59 - you haven't already familiar with
619:01 - machine learning I'll ask you to take an
619:03 - introduction video or introdu int int
619:05 - introductory course of machine learning
619:08 - maybe you can go to ml1 or you can go to
619:11 - some other machine learning course but I
619:13 - highly S Suggest mll1 is the best course
619:17 - for you to consider at this point
619:19 - cool so um so you you got to know about
619:22 - the definition but there are different
619:24 - machine learning algorithms so you may
619:26 - ask that how you how we are going to
619:28 - extract patterns from the data and how
619:30 - we analyze the data so that is the first
619:32 - question which may come in your mind how
619:35 - we extract the patterns from the data
619:37 - and how we make intelligent predictions
619:39 - or how we do how do we analyze the data
619:42 - okay so so for making intelligent
619:44 - predictions first of all you need to
619:46 - extract patterns from the data so how
619:49 - are you going going to extract patterns
619:50 - from the data which we'll see uh in a
619:53 - review of machine learning in the course
619:55 - but there are different set of
619:56 - algorithms which will help you to
619:58 - extract patterns from the data or
620:00 - analyze the data some of the examples
620:03 - are logistic regression logistic
620:05 - regression logistic regression we have
620:08 - linear regression we have linear
620:10 - regression and I'm preparing I think a
620:14 - four to five hours of lecture four to
620:17 - five hours of lecture on linear
620:19 - regression is the mainly video titled as
620:22 - multivariate analysis you can go on New
620:24 - Era YouTube channel you can go on newa
620:27 - YouTube channel to maybe it will be
620:29 - uploaded till the end so maybe you can
620:31 - just go there you'll be seeing a title
620:33 - multivariate and regression analysis I
620:35 - have taught linear version in great
620:37 - detail there cool so uh so back to our
620:41 - topic so there are different set of
620:42 - algorithms which will help you to
620:44 - extract patterns from the data say for
620:46 - example legis ression and logistic
620:49 - regression is a classification algorithm
620:51 - classification algorithm we can use for
620:53 - classification tasks classification
620:56 - tasks and linear regression is a
620:59 - regression is a regression algorithm
621:01 - which will help you to perform
621:02 - regression tasks and many people just
621:05 - think about linear version is very
621:07 - simple algorithm uh it does not perform
621:09 - well but but but you but just go in
621:12 - statistics uh term and then you will see
621:15 - the power of linear regression mainly
621:18 - people forget to write to use Lear
621:21 - regression in a right way so you can
621:23 - watch the multivariate regression
621:25 - analysis which will be uploaded soon so
621:28 - um n base which is another yet powerful
621:31 - classification learning algorithm which
621:33 - can be used to predict whether an email
621:35 - is a spam or ham so using nbase you can
621:39 - construct a function f that takes an
621:42 - email that takes a email and classify
621:45 - that as a Spam or not non spam or non
621:51 - spam and let's denote spam with zero and
621:54 - let's denote non spam with one okay so
621:57 - you want to learn a function f that
621:59 - takes the input value X and that Maps
622:01 - either zero and one or or in other words
622:06 - we can say y be the member of zero and
622:09 - one okay so there is n base cool so the
622:13 - performance of these simple algorithms
622:16 - heavily depends on the representation of
622:18 - the data which you have okay you should
622:21 - have a good data to for your machine
622:24 - learning algorithm to form to perform
622:26 - well if you have a bad data your machine
622:28 - learning algorithm will not perform well
622:31 - but this ex this extract or this this
622:34 - this paragraph is taken from book
622:37 - written by Deep
622:38 - learning uh by Ian Goodfellow um I I
622:42 - only know the one one one auor name and
622:44 - others are I think U um I I don't know
622:47 - exact name but but y yosua Benjo so so
622:50 - you can just go there and see the book
622:53 - by Yan Goodfellow so this is an extract
622:55 - from that book and what is it indicates
622:58 - the performance of these simple
622:59 - algorithms depends heavily on the
623:02 - representation of the data which you
623:03 - have the representation of the data
623:05 - which you have or which you're given say
623:08 - for an example you're using linear
623:10 - regression to predict the house prices
623:13 - okay so so basically what exactly is
623:15 - trying to tell uh you you're using using
623:18 - Lear ression um to predict the house of
623:21 - the prizes so on time of prediction say
623:24 - uh so you train an algorithm you train
623:26 - an algorithm which takes size and as an
623:30 - input okay and uh and number of a fans
623:33 - of a house number of a fans of a house
623:36 - and then using these two features it
623:38 - predict your y variable y variable okay
623:41 - so this is how you train a function f
623:43 - that takes size and number of fans and
623:45 - then give you the pricee of the house so
623:48 - basically basically it is saying that
623:50 - you are using linear R to predict the
623:52 - house prices so now you trained it using
623:55 - these two features now when the time of
623:58 - prediction came user have to provide one
624:01 - size and the number of a fans okay so in
624:04 - let's take a bedroom ra rather than fans
624:07 - so number of bedrooms to make
624:09 - predictions okay but when you give
624:13 - number of a fans and then ask your model
624:15 - to predict the price your model will not
624:17 - be able to predict do you do you do you
624:20 - know the reason why it's very simple
624:22 - because you have trained your model on
624:24 - the size on two feature on based on two
624:27 - features and the number of a bedrooms
624:29 - and then you getting price what if if
624:32 - you give only the fans you'll be not
624:35 - able to predict it isn't it so that's
624:38 - the major problem of representation that
624:42 - that that that these performance of
624:44 - these machine learning algorithms
624:45 - depends heavily on the representation of
624:47 - the data which you have given and these
624:50 - are features so size is also a feature
624:53 - number of a fans is also a feature
624:55 - number of a bedrooms is also a feature
624:57 - so I hope that this is clear we'll we'll
624:58 - come to that we'll we are coming to
625:00 - definition of uh uh what machine
625:02 - learning sorry deep learning cool so the
625:06 - problem of feature representation so as
625:08 - I've taken one example to start the to
625:11 - start with a problem of feature
625:12 - representation or feature uh uh uh
625:15 - learning so we will talk about that AI
625:17 - task can be solved if we have right set
625:21 - of features and make a mapping from
625:24 - feature to desired output so all the AI
625:27 - tasks which is available today okay so
625:31 - say say say for an example that you
625:33 - wanted to predict the price of the house
625:36 - okay want to predict the price of the
625:42 - house you want to predict the price of
625:44 - the house so so so for for predict in
625:48 - this you should have the right set of
625:50 - features for this to get to particular
625:53 - uh answer so you want to learn a
625:55 - function f okay that correctly maps from
625:59 - some feature X1 X2 all the way around to
626:03 - the xn given these n features it
626:08 - predicts your y variable and these n
626:10 - features should be right set of features
626:13 - should be right set of feat if you have
626:15 - bad set of features where our AI model
626:18 - will not not able to perform very well
626:20 - okay so we do want the right sort of
626:22 - features so that uh we can train our
626:25 - model on these rights of features and
626:27 - then get the desired output one of
626:29 - example which I want to represent to the
626:30 - real world example say for example that
626:33 - you're learning some science concept
626:35 - where you have feature of the particular
626:37 - thing so uh say for an example I'll take
626:40 - a simple example so that uh it would be
626:42 - very easy um let's take a example in in
626:46 - real world concept um
626:48 - car okay so a car and what are the
626:52 - features of it what what what are the
626:54 - features of it so the features of it can
626:56 - be it should have four wheels it should
626:59 - have four
627:01 - wheels um it should be not too much big
627:05 - it it we have we have steering we have a
627:10 - steering we have seat
627:13 - belts you have seat belts so these are
627:15 - some of the features we used to identify
627:18 - cars okay and they they they come in
627:21 - different different color so there are a
627:23 - set of features we need a right set of
627:25 - features to uh to understand that is a
627:28 - car and how do we learn it uh how do we
627:31 - learn it say for an example that uh
627:33 - these seats and then four-wheel steering
627:37 - these are the right set of features that
627:39 - we that we used to identify whether
627:42 - that's a car or not okay so that's how a
627:45 - baby learns it identifies some set of Fe
627:48 - features okay uh my my my my father is
627:51 - telling that okay it is the steering So
627:54 - based on that I will identify okay
627:55 - that's a car because I've identified
627:58 - some of the features of that because
628:00 - I've identified some of the features of
628:01 - that uh one of one of the main main
628:03 - thing which I want to highlight is
628:05 - identifying the the brand of a car okay
628:08 - brand of a car identifying the brand of
628:13 - car that's that's a very very that's
628:16 - that that I face in real world
628:18 - is for every car we have identified
628:22 - feature okay say when I when I when when
628:24 - I see a car I'll take a look at logo of
628:27 - that in in front of the car so in front
628:30 - of that there is a logo and if it is
628:32 - logo in something like this then it's
628:34 - Audi okay then it's AA then it's AA
628:38 - because of this logo I'm able to
628:41 - identify what's the because of this
628:44 - feature mainly the logo we let's this
628:47 - because of this feat feature I'm able to
628:49 - identify okay that's a car so this is
628:52 - the right or unique set of feature that
628:55 - this car have so so for example so when
628:59 - you can train your model okay so you
629:01 - need right set of features okay um so so
629:05 - your model is able to identify that I
629:07 - hope that this this give you pretty much
629:09 - lot of sense to you at least okay say
629:13 - for example you want to detect the cat
629:16 - in in an image so you want to build a
629:18 - model that given a image eye that will
629:22 - detect whether that is a cat or
629:25 - non-cat okay so a cat can have can have
629:31 - different set of features so you can
629:32 - hand design right set of features by
629:34 - yourself like you can design you can
629:37 - just the the values are in pixel format
629:39 - the values are are in pixel format okay
629:42 - the the picture so you can take the only
629:44 - the pixel which are of ears you can take
629:47 - only the pixel which are of ears so you
629:49 - can design the right sort of features
629:51 - like ears nose and which are very
629:55 - consuming time consuming task because
629:58 - for because you need to be very good at
630:00 - the the the at the background of that
630:02 - domain or or or or or you should have a
630:05 - good domain background for identifying
630:07 - the right sort of features because in
630:08 - this example it's very easy car image
630:11 - but what if uh in real world you don't
630:13 - have a problem like this you do have a
630:15 - problem like this but but like weave
630:17 - detection
630:18 - um like in weather you you should have a
630:21 - great background inside that so you
630:23 - should write you should design right set
630:26 - of features and that will be very timec
630:28 - consuming task and getting the right set
630:31 - of features for your model is very very
630:33 - challenging in today's era for many
630:35 - tasks around us okay say for an example
630:40 - um like uh wave detection or or whatever
630:43 - um that we should we we don't know uh
630:46 - what are the right sort of features we
630:48 - we think okay every every feature is the
630:50 - right right set of feature but getting
630:53 - the right set of features in today's era
630:54 - for many task is also very challenging
630:58 - okay so so what so so to in using
631:02 - machine learning using machine learning
631:04 - what we are able to do what we are able
631:06 - to do we are hand designing the rights
631:08 - or features we are hand designing the
631:10 - rights or features so we get an input we
631:14 - get an input we get an input and then we
631:17 - hand designed right s of features so
631:20 - this is a car so we we taken out the
631:22 - pixel of maybe wheels for that image and
631:26 - then we we taken our right s of features
631:28 - like Wheels steering
631:32 - logo okay so there are some sort of
631:34 - features we hand designed by ourself by
631:37 - programming and whatever and then we
631:39 - train a machine learning algorithm on
631:41 - these pixels or whatever on this
631:43 - whatever our features which we' have
631:45 - extracted let X1 X2 all the way around
631:47 - to the xn okay we train a Model F let's
631:51 - let's put in a giant X okay and then we
631:54 - give this set X and then we train our F
631:57 - maybe it can be uh classification
632:00 - algorithms like legis regression it
632:02 - takes the value of x and then classify
632:04 - that either a car or a not car okay so
632:09 - in machine learning what you're doing is
632:12 - you're hand designing the right set of
632:14 - features by yourself because when you
632:16 - see your machine learning data data
632:18 - which you have you're given the in
632:20 - several input values several input
632:22 - values 1 2 3 4 5 okay and you are given
632:28 - the the target label and these these Fe
632:31 - features are hand designed these
632:33 - features are hand designed okay so this
632:36 - is how uh this is what today's uh
632:38 - today's machine learning is able to do
632:40 - is it also require feature extraction it
632:42 - also required to extract features from
632:44 - the data by human human human human will
632:46 - do it human will Design right of
632:48 - features so this is what the problem
632:51 - which we have is the is hand designing
632:53 - the rights s of features which are very
632:55 - challenging and is very time consuming
632:58 - task for Designing the right right set
633:00 - of
633:02 - features so so what is the solution to
633:04 - the pro problem the one solution to this
633:07 - problem is making machine learning not
633:10 - only learn the mapping from features to
633:13 - Target labels also learn the
633:16 - representation to y
633:18 - okay so basically we are asking our
633:22 - computers to not only learn this
633:24 - classification stuff not only make a
633:27 - model given the set of
633:29 - features given the set of features also
633:32 - learn features by itself okay so given a
633:37 - car given a car learn feature EXT
633:41 - extract feature automatically and make a
633:44 - model on the feature which you extracted
633:46 - Okay so say for example uh in machine
633:49 - learning you are hand designing set of
633:51 - features and then you're giving giving
633:52 - to the model whoever the right set of
633:53 - features is but in deep learning what
633:57 - what what exactly you were doing is
633:59 - given a car an image to your to to your
634:02 - system what it does first of all it
634:04 - extract features that's called feature
634:06 - extraction or feature learning or
634:08 - representation learning it extract right
634:10 - set of features and extract right set of
634:13 - features let's say Wheels let's say
634:16 - anything okay and based on extracted
634:19 - features it builds a classification
634:21 - model or whatever learning algorithm and
634:23 - then is able to classify so so there's a
634:27 - huge amount of time which is preserved
634:30 - and this actually perform very well in
634:32 - today's ERA with state-of-the-art models
634:35 - which are doing very perfectly so what I
634:38 - again I'm going to repeat that one
634:40 - solution is to not only learn mappings
634:43 - from X to Y but also learn X's as well
634:47 - also learn the features of it as well
634:50 - okay of course you're going to going to
634:52 - give you give the input which is the car
634:54 - but but but in but for if we have to
634:57 - identify that problem that can be solved
634:59 - using deep learning or that problem that
635:01 - that can be solved using machine
635:02 - learning you have to identify that okay
635:05 - the model takes the raw data as input
635:07 - which is the image and perform feature
635:10 - extraction by learning the feature
635:11 - representation so it ex automatically
635:13 - extract the features and learn the
635:15 - parameters to perform the necessary task
635:18 - or to build a model f that maps from
635:20 - features the Learned features to Output
635:23 - but one thing which you will ask
635:25 - question that how do we go on performing
635:27 - feature extraction so how it performs
635:30 - and what is what these uh three three
635:32 - things means don't worry about it just
635:35 - just I will will briefly talk about in
635:38 - this future detail don't worry about
635:40 - anything just understand what exactly
635:41 - deep learning trying to
635:43 - solve okay so I think it's very very
635:46 - very very much clear here I just want to
635:49 - move
635:50 - forward cool so what is the definition
635:53 - of machine learning so this is a
635:55 - definition by the book again I'm saying
635:58 - this is by Ian goodfella yosua benju
636:01 - book and deep learning methods aim at
636:04 - learning feature hierarchies with
636:07 - features from higher level of features
636:09 - form by composition of lower level
636:12 - features and automatically learning
636:15 - features at multiple levels of
636:17 - distraction allowing a system to learn
636:20 - complex function mappings from the input
636:23 - to the output directly from the data
636:26 - without depending completely on human
636:28 - crafted features and learning deep
636:31 - architectures for AI and that and and I
636:35 - personally know that you are very
636:37 - confused with this statement so let's
636:39 - get ahead and make you understand that
636:42 - what the exactly definition trying to
636:43 - tell maybe you know about hierarchies
636:46 - okay the hierarch means like we have
636:49 - this hierarchy of a feature so you have
636:51 - different different feature different
636:52 - different levels so basically this is
636:55 - the this is in this level you are
636:57 - extracting some level of features some
637:00 - lbel of features you are extracting so
637:02 - that is visible layer that is the input
637:04 - pixel so you give this pixel this pixel
637:08 - this pixel this pixel this pixel so
637:11 - these are input level fixes which is
637:13 - given to your model which is the raw in
637:16 - raw raw input raw raw input and then the
637:20 - then it is passed don't worry about what
637:22 - is this what is this what this Arrow
637:24 - just assume whatever just one you
637:26 - understand what exactly I'm telling
637:28 - first the input which we give our which
637:30 - we give to our model or whatever
637:32 - algorithm is the pixel value is the
637:34 - pixel value is the pixel value okay um
637:39 - is the pixel value which which we given
637:41 - in which is visible to us now now the we
637:45 - we give to the second level we second
637:48 - layer which you usually call as a hidden
637:50 - layer but don't worry about it just just
637:52 - don't just just don't worry worry about
637:54 - it then we give to the next level which
637:57 - extract some patterns some feature like
638:00 - this some feature like this okay then we
638:02 - go then then we give to the next feature
638:05 - whatever the information contained
638:06 - whatever the information is G given to
638:09 - the next feature and then we learn some
638:11 - sort of features again then we extract
638:13 - extract from this image some set of
638:15 - features then again next next l level we
638:18 - extract some set of features we extract
638:20 - some set of features and then based on
638:22 - these learned features you're getting
638:24 - whether that person is a car with person
638:26 - or animal okay uh so basically what
638:31 - exactly you are doing is extracting
638:33 - features till this level you're
638:36 - extracting features and at last you're
638:38 - making a model here and then making
638:41 - prediction whether whether that is a car
638:43 - person or animal don't worry about it
638:45 - what exactly um what what exactly is
638:48 - trying to what what exactly these Arrow
638:50 - represent and how how are we making
638:52 - classification model just don't worry
638:54 - worry about that the only thing to worry
638:56 - about that you are understanding the
638:58 - right thing here the right thing is that
639:01 - you are learning features at different
639:04 - different levels and you are learning
639:06 - feature hierarchies with features from
639:08 - higher levels of the hierarchy so here
639:12 - you have you have some set of features
639:13 - learned and then the second and the
639:15 - third so have multiple levels where
639:17 - you're are learning different different
639:19 - features and because of this you're able
639:22 - to make complex functions we'll talk
639:25 - complex mapping from the pixels to the
639:28 - car person or animal okay you're here no
639:31 - human Loop is needed it is automatically
639:34 - creating it is automatically extracting
639:36 - his her uh glasses or whatever it is
639:41 - auto automatically no involvement no
639:43 - human crafted features at level one it
639:45 - extracted some sort of features at at
639:47 - level two extracted some some set of
639:49 - features at level one it extract edges
639:52 - okay mainly this edges edges level two
639:55 - extracted Corners mainly this one and
639:58 - Contour level three it extracted object
640:01 - Parts like this okay level level three
640:05 - so it is learning complex features now
640:08 - your model is able to make complex
640:10 - function of comp complex function f that
640:13 - takes the input image and then then
640:16 - powerfully making or making a very good
640:18 - prediction comparable to human U score
640:22 - or accuracy so I hope that that this is
640:25 - very clear on the definition of deep
640:26 - learning and and general idea what the
640:29 - what exactly deep learning trying to
640:30 - solve is learning feature hierarchies
640:34 - okay with features feature hierarchy
640:36 - from higher levels of the hierarchy
640:38 - formed by the composition of lower level
640:40 - of features so you're not involving any
640:42 - any human Loop just you're learning
640:44 - feature learning exactly what the Deep
640:46 - learning is trying to solve
640:48 - cool so um we we we we'll again
640:51 - precisely Define uh you will learn when
640:54 - when you will learn neural networks then
640:56 - you will understand what exactly de
640:57 - printing is but I hope so that that that
641:00 - that give you a better sense over here
641:02 - coming to the different applications
641:04 - there are thousands of applications
641:08 - which can be solved which are which are
641:10 - in production today which are using deep
641:12 - learning to solve the real world problem
641:14 - the first one is um op classification
641:18 - problem given an image classify that
641:21 - whether that image is of cat or a
641:23 - non-cat so how does it helpful and maybe
641:27 - and when you go to the company you just
641:29 - show your face and that allows gate
641:31 - opens and then you go inside that's face
641:33 - recognition where it identifies whether
641:36 - you are that person or not okay uh
641:39 - classification plus localization to
641:41 - localize that where that object is for
641:43 - if for example it can be used in cell
641:45 - driving car which you're seeing in front
641:47 - front of you it it is able to localize
641:49 - the stuff uh so that it would make their
641:52 - own decision to where to go further and
641:54 - then we have art generation using from
641:57 - text so given a a beautiful woman it
642:00 - will generate the image of beautiful
642:02 - woman it can be possible I'll show you
642:04 - one demo today and then we have chat
642:07 - Bots uh chat Bots like AI coder uh AI
642:12 - Q&A system using gpt3 and then uh lots
642:15 - of things uh search recommendation
642:18 - system like Google is using San
642:20 - Francisco whatever is there um so I
642:23 - think these are some of the examples
642:24 - another examples are speech recognition
642:26 - task you can just make use of Alexa how
642:29 - are you and then it will give you the
642:31 - answer music generation using deep
642:33 - learning you can actually generate the
642:35 - music using deep learning and then brain
642:38 - tumor detection it is used also used in
642:40 - medical industry so these are some sort
642:42 - of applications which I highlighted for
642:44 - you at least to get to know much uh in
642:47 - in in much more detail so I hope that
642:49 - this is very clear now and now what I'm
642:51 - going to do now is to show you some of
642:53 - the cool application which I built as a
642:55 - project by myself so I hope that that
642:58 - will give you more sense to you so I'm
643:00 - just going to show it to you
643:03 - um the first one which I want to show to
643:05 - you is the chatbot which we had
643:08 - developed at Anton so let me just uh let
643:12 - me just make it little bit oh wait for a
643:15 - second I'll just make it
643:18 - 1080p so that it would be more precise
643:21 - for you to at least see
643:23 - that so you can see over here uh and
643:26 - chat which is which is able a Discord
643:28 - chatbot which is able to chat with me uh
643:32 - like uh which you can see no problem
643:34 - let's talk about something else and then
643:36 - I seriously haven't heard about this
643:37 - song we did it and this is an chat which
643:40 - you can of course opt for it you can
643:42 - just make you make that in your own
643:44 - Discord server if you have just email us
643:46 - we will uh my company has launched this
643:49 - product which is anat which uses gpt3 so
643:52 - this is one of one of the example of
643:54 - deep learning the another example is I
643:56 - just show you another example we have
643:59 - ancer which actually given program for
644:02 - taking the factorial it gives a Python
644:04 - program for that which you're seeing in
644:06 - front of you that it it it is able to
644:08 - solve any kind of programming question
644:11 - which you presenting in front of your
644:12 - screen and I I hope this is anodo which
644:16 - is again you can email us to get the
644:18 - access of
644:25 - it now coming to the next part is n Q&A
644:28 - q& is a system which you can see over
644:31 - here that we have n q you can just ask
644:34 - what is a circle and whatever it is able
644:36 - to answer your question and that is we
644:40 - even answering uh in machine learning
644:42 - tasks and I hope that this this is what
644:45 - the C assist says system we have
644:47 - developed and it is available for uh
644:50 - which you can email at Anon will be
644:52 - happily will'll be happily giving you in
644:54 - your own Discord server to make your uh
644:57 - server produ productive coming to the
645:00 - next project is Art generation using
645:03 - deep learning which is generated
645:04 - adversarial networks which we will do in
645:06 - this course uh using Gans and which you
645:09 - can see that how this is generating the
645:12 - image and this is the image which is
645:15 - generated by an this is on my own
645:17 - YouTube channel this is a image which is
645:19 - generated by uh peoples
645:23 - okay coming to this we have another
645:26 - another art which is generated which you
645:28 - can see over here uh using text to
645:32 - image and I really hope that it made a
645:35 - lot of sense to you you can just go at
645:37 - my YouTube channel and uh just sub
645:40 - subscribe this YouTube channel if you
645:42 - want and then uh go to playlist go to M2
645:46 - and also also what I what I want to
645:47 - highlight is uh you can just uh have you
645:51 - can just enroll in LMS you can just go
645:53 - there and enroll in LMS by going to this
645:55 - Doc and uh then going to uh reading all
646:00 - the course and then assignments and a
646:03 - lot of lots of stuffs you can go there
646:06 - and uh enroll in your LMS which which I
646:09 - written over there you can enroll in
646:11 - your Neo LMS okay so you have so I hope
646:15 - that this is pretty much clear and I
646:18 - also hope that you have enjoyed this
646:21 - video If you haven't uh please leave a
646:24 - comment at how I can improve this uh
646:26 - these lectures uh otherwise uh I hope
646:29 - that this video give g g given a lot of
646:32 - intuition of behind deep learning I'll
646:34 - be catching up in next video till then
646:36 - bye-bye have a great day everyone
646:37 - welcome to this lecture we'll be finally
646:39 - starting off with introduction to neural
646:41 - networks uh so let's get started with
646:43 - this lecture um so uh first first of all
646:47 - we we will talk about biological neural
646:49 - network as it is very important um that
646:53 - how our neurons in our brain work so
646:55 - I'll try to relate the biological neural
646:58 - network with our neural networks
647:00 - specifically we'll talk about perceptron
647:03 - so in this particular lecture here's the
647:05 - outline of this lecture we'll start off
647:07 - with the biological neural network we
647:10 - will will get into the the the that how
647:13 - we can relate this to uh something
647:15 - called as perceptron on and then we'll
647:17 - go ahead talk about different things
647:19 - kind um activation functions and whole
647:21 - outline can be found on a course website
647:24 - as we had detailed uh sections and
647:27 - subsections provided there cool so uh
647:31 - let's start with biological neural
647:33 - network so here in front of you you're
647:34 - seeing is a neuron in a human brain so
647:38 - basically human nervous system contains
647:40 - which is something called as cells and
647:43 - you know cells are the basic unit of a
647:45 - life you have already studied in class
647:47 - so so so human nervous system contains
647:50 - cells called neurons and the neurons are
647:54 - connected to one another with the use of
647:57 - exons and the dendrites and the
647:59 - connecting regions between exons and
648:02 - dendrites are referred as synapsis so
648:04 - what exactly it is telling that you have
648:06 - a several neurons let's assume this as a
648:09 - specific neuron so you have a neuron and
648:12 - this is connected with another neuron
648:14 - another neuron okay with the help of
648:18 - Exon and dendrites and dendrites and
648:22 - these are the the reasons which connects
648:25 - the exons and the dentrites are called
648:27 - synapses but you may be thinking hey uh
648:30 - you are not in Biology class then why
648:32 - why you all are teaching this so I'll
648:34 - try to relate this with the particular
648:37 - stuff like this the the the keywords the
648:40 - dentes are the input terminal which
648:43 - takes the some input from other new
648:46 - neuron say say for an example you have a
648:49 - neuron over here so this is your
648:51 - particular neuron and this gives some
648:55 - input to the another neuron so they are
648:57 - connected to each other so this is a
649:00 - neuron which gives which which which
649:03 - gives an output which is connected to
649:05 - another neuron and this the output is
649:08 - taken the output of another neuron for
649:11 - for for for this particular neuron it it
649:13 - act as a input input terminal where
649:16 - takes the output of the previous neuron
649:19 - okay and so that's why we have a
649:21 - dendrite which is nothing called the
649:24 - input terminal okay the next thing is
649:29 - the Exxon which is the output wire which
649:32 - is the output wire so Exon terminal is
649:34 - an output wire so this so whatever it
649:38 - outputs the neurons whatever it outputs
649:41 - so what dite received from the neuron
649:44 - denr is called the input terminal and
649:47 - whatever it is outputed is from the Exon
649:50 - terminal because it's the it's the Exxon
649:53 - which is the output wire and which
649:55 - output something and Exon terminal are
649:58 - the output terminals which output
650:00 - something let's say let's say output
650:03 - something let's say this and this is
650:05 - again transferred and taken input from
650:07 - using the dendrite so again I'll Recaps
650:10 - youate this stuff as Dent rites are the
650:12 - input terminals the Exon is the output V
650:15 - and the Exon terminals are the output
650:18 - Terminals and this is a particular
650:20 - structure of a neuron this is a
650:23 - particular structure of a neuron so I
650:25 - hope that this this this gives you a
650:27 - better sense of um moment cool so we
650:30 - will not um want talk about this
650:33 - biological neuron we'll go ahead and
650:35 - talk about we'll try to relate this with
650:38 - artificial neural networks okay so so
650:41 - let's take an example uh you have a
650:44 - neuron which gets an input say for an
650:47 - example you're getting X1 you are
650:50 - getting X2 X3 X4 which is an input
650:54 - received in dendrites so you have a
650:56 - dendrite which is a dendrite terminal
650:58 - which is the input terminal so let's say
651:01 - you get four um inputs don't see this
651:04 - diagram as of now just only see this
651:06 - diagram and listen to what I'm saying so
651:08 - the information which then right
651:11 - receives is called X1 X2 X3 X4 okay so
651:15 - uh for for for for for this example you
651:18 - have X1 X2 X3 X4 which are the input
651:22 - which is received in D rights and the
651:26 - information the information X1 X2 X3 and
651:29 - X4 are weighted by some weights and W1
651:35 - W2 W W3 and W4 and it determines the
651:39 - effect of your input so uh say for
651:41 - example X1 how much this effect of an
651:45 - input in this neuron on okay so we'll
651:48 - take a specific example say say for an
651:50 - example you saw something okay so you
651:53 - you have a human eye so just and you saw
651:56 - something like um like a dog like a dog
652:00 - okay so you saw different different
652:02 - thing like its eyes it's eyes let's
652:05 - let's assume this eyes is X1 its ears X2
652:10 - it's tail X3 and its legs X4 so all
652:15 - these
652:17 - in your mind all these contain some
652:19 - weights let's say example that you saw
652:22 - that you are quite familiar with the
652:25 - mouth or the eyes of the dog to be
652:28 - similar so let's take W1 X1 okay so this
652:32 - input how much weight it contains how
652:35 - much effect to so because you want to
652:38 - identify that dog so that's why you want
652:40 - to identify how your your your maybe
652:43 - mind knows how much effect your eyes
652:47 - there okay so eyes of a dog how much
652:50 - effect does that ear of a dog effects uh
652:53 - on for for for for you so that you are a
652:55 - successfully able to classify that as a
652:57 - dog okay so so most important thing say
653:01 - for example it's its tail may be having
653:05 - the the biggest weight because by seeing
653:08 - the tail you're able to identify whether
653:10 - it is a Dober man or anything so tail
653:13 - would be the containing the highest
653:15 - weight so the whole whole whole phen or
653:17 - whole thing of weight is that that the
653:20 - W1 W2 W3 it contains the information is
653:24 - weighted we give some weights to every
653:26 - information whatever we get in the D
653:28 - right so that we'll be able to identify
653:31 - the effect of each input in your
653:34 - dendrite okay so basically in in an in
653:37 - an overview it determines the effects of
653:40 - the input so W1 X1 so what we do we
653:44 - simply multiply with the input so all X
653:48 - X1 has has has its respective weight say
653:52 - for example
653:54 - X1 X2 and X4 so we have four features we
653:58 - have four features and let's say that X1
654:01 - is more important in I in doing that
654:04 - task so X1 is weighted more W1 W3 W4 and
654:08 - W5 let's say X1 is important so its
654:11 - weight will be high because it is
654:13 - weighted and this X1 has the the most
654:17 - important effect of the input okay so
654:20 - we'll come to that weight or uh a ton of
654:22 - time the whole deep deep learning is
654:24 - based on that getting good weight so
654:26 - we'll talk about that okay just assume
654:30 - that you have any weights we don't we
654:32 - still don't know how do we get the
654:34 - actual weights we still don't know about
654:36 - the formal definition I'm just defining
654:38 - it informally so the weighted
654:40 - information now what you do you multiply
654:43 - the weights with the inputs so you can
654:45 - determine the the effect of the input uh
654:48 - using that
654:49 - weights okay so the weighted information
654:52 - are aggregated in the nucleus so
654:55 - basically this is a nucleus which is
654:57 - nothing but the PowerHouse okay so it's
655:00 - aggregated in the nucleus as a weighted
655:03 - sum as a weighted sum as a weighted
655:08 - weighted sum as a weighted sum so what
655:11 - you specifically do you simply um
655:14 - multiply the weights with the respective
655:17 - inputs and add it up okay um so W1 X1
655:22 - plus W2 X2 + W3
655:26 - X3 + W3 * X3 + W4 X4 okay simply add it
655:33 - that's it that's what what what what you
655:35 - do you simply aggregate in it in a sum
655:38 - variable or a nucleus let's call it as a
655:41 - z okay that's a nucleus that's a nucleus
655:44 - where you aggregate all the information
655:46 - at one point plus you add some biom okay
655:50 - let's let's let's add some bium let's
655:53 - add some bium and don't do uh that is a
655:56 - bias which we will have a detailed talk
655:59 - uh in this uh section please don't worry
656:02 - about that we'll have a detailed talk on
656:04 - that just you can ignore this or assume
656:06 - some constant like 0.1 okay don't worry
656:10 - about that what W1 how how how we'll
656:13 - choose W1 we'll see in the whole section
656:17 - ction then what then what we do so here
656:21 - we came to a nucleus and nucleus perform
656:23 - one action to it so this now nucleus now
656:28 - this is down here you apply the the the
656:33 - what do you say the nonlinear function
656:35 - the nonlinear function so on Z so you so
656:39 - you on Z you applied some kind of uh
656:42 - some kind of function don't worry what
656:44 - that function do okay just just just
656:46 - assume that we apply some kind of
656:48 - function may maybe you have already seen
656:51 - something called as sigmoid
656:55 - function I'm just taking an example of a
656:57 - Sig word function there's a lot of
656:58 - functions which which we apply which
657:00 - we'll formally Define in this section
657:02 - don't worry about that the only thing
657:04 - which you need to worry is to understand
657:06 - the whole process of uh neural networks
657:09 - and I'm just trying to for informally
657:11 - Define the neural networks with the help
657:13 - of your neuron so so where we are we
657:17 - have set of inputs which we got from a d
657:20 - right then carrying these input carry
657:23 - carries the the the weights or the or
657:26 - the or the information is carried by the
657:29 - weight okay weighted by the weights
657:31 - which is W1 all the way to W4 why all
657:35 - the W4 because we have four inputs which
657:37 - we got okay it determines the effect of
657:41 - each input by taking or or the product
657:44 - between their respective um inputs and
657:47 - then what we do and then we reach to the
657:50 - nucleus where all the information is
657:53 - aggregated by taking a sum where we add
657:56 - one bias term we'll talk about it l
657:58 - later on what exactly that bias term do
658:02 - okay and then and then after after a
658:05 - aggregation now some processing applies
658:08 - like nonlinear function which is a sigma
658:11 - function in this case maybe if you know
658:13 - about it if if you know about legis
658:15 - ression then you might be knowing that
658:17 - and then it is further sent and then it
658:20 - is further sent for further processing
658:23 - to Exon y okay so it is further sent to
658:27 - another now we got our output which is
658:31 - some non non on some function on Z and
658:34 - then we get our output y okay and then
658:36 - it is sent to as a input to another
658:39 - neuron as an input to another neuron or
658:42 - we get a final output or we reach to the
658:45 - destination what we wanted to reach so
658:48 - what does it mean it reaches to
658:49 - destination so let's take an example you
658:52 - saw something you saw a dog okay you saw
658:55 - a dog
658:57 - and here the the information uh let's
659:01 - say you want to identify whether whether
659:03 - that dog is a is a German shepher or a
659:07 - doberman okay Doberman pure so
659:10 - specifically what you will do you all
659:12 - know that a German shefer has a long
659:15 - long long hair and kind of wolfy um
659:19 - nature kind of stuff and Dober man is
659:21 - very uh thin and have a and don't don't
659:26 - don't have a long tail as well as they
659:29 - are very um they very strength and kind
659:31 - of stuff you have a specific mind in
659:33 - your uh your specific picture in your
659:35 - mind so when you see Doberman or when
659:39 - when you see a dog you need to and that
659:42 - dog is either German shepher or dman pin
659:45 - okay so here is a dog that dog is either
659:48 - German Shepherd so how how your mind
659:50 - will work and I'm just taking an example
659:53 - of it so what happens is you see his
659:57 - hair you see his tail so all the
660:01 - information get into your into your eyes
660:03 - and through eyes you get into into your
660:06 - mind through eyes it gets into your mind
660:09 - and all the things like hair Dober Manel
660:12 - these are your input these are your
660:14 - input hair tail ear eyes these are your
660:17 - input and over here the that's a hair is
660:22 - carried by some weights carried by some
660:24 - weights all the weights so that we can
660:27 - understand the effect of our inputs so
660:30 - say for an example that uh the your hair
660:34 - has the the the weight of a hair because
660:37 - in your mind in in at least in my mind I
660:40 - I'll I'll I'll be having more influence
660:42 - of hairs so w one here will be very
660:47 - large okay so I'll be able to identify
660:50 - that and then what and then what happens
660:53 - after after we aggregate the information
660:55 - we simply aggregate this information in
660:58 - a nucleus so every feature is very
661:00 - important so you aggate the information
661:02 - as a nucleus and then you apply a
661:05 - nonlinearity or some some kind of
661:07 - function don't worry about what is non
661:09 - nonlinear Etc what what this function
661:11 - does just simply we apply some kind of
661:13 - function or some kind of processing and
661:15 - then we reach to the T destination
661:18 - whether that dog is a doberman PTI or
661:20 - German Shepherd okay so or or maybe that
661:24 - output whatever whatever output we get
661:27 - after applying that function it it will
661:29 - be sent to another neuron so that we so
661:32 - maybe if required I don't know about
661:34 - much more about
661:35 - brain cool but is this exactly happens
661:39 - in our maybe um um a specific neuron
661:44 - it's it's kind of no absolutely no is
661:47 - just an inspiration from a neuron it's
661:50 - not like that how these kind of exact
661:53 - maths is being calculated it's not that
661:56 - it's just an inspiration so that like
662:00 - his scientists previously just take an
662:03 - inspiration of it and made a
662:04 - mathematical Morel out of it okay so do
662:07 - not relate it to exactly how neuron
662:09 - works you can see other videos on how
662:12 - neuron and how brain works but but
662:14 - specifically this is taken for
662:16 - inspiration for this kind of uh
662:19 - statements that I've described up Okay
662:22 - cool so I hope that this gives you a
662:25 - better sense about how everything works
662:27 - let's go ahead um over here the
662:30 - pictorial representation of a workflow
662:32 - so exactly this is the artificial neural
662:36 - networks so this is in mathematical
662:39 - model this is whatever you're seeing is
662:41 - not a real neural network or or or a
662:45 - brain is just an artificial neural
662:48 - networks where we have we will talk
662:49 - about what is neural what is networks
662:52 - later on but this is a pictorial
662:54 - representation of what our workflow
662:56 - States so just recapitulate you you have
662:59 - a set of inputs you have a set of inputs
663:03 - X1 X2 X3 all the way around to the X4
663:07 - okay and all these inputs are carried by
663:11 - or all all the information is carried by
663:13 - the weights which we denote with the W1
663:16 - W2 W3 W4 and a biased term okay and um
663:21 - and these weights determine the effect
663:24 - of your input and this is a nucleus this
663:28 - is a nucleus where all the information
663:32 - is aggregated by taking out the sum of
663:36 - and this is this is the summation
663:37 - notation uh W1 X1 + W2 X2 + W3 X3 + w4x
663:45 - 4 and then we had the bias term and then
663:49 - in that and then let's assume that this
663:51 - is the nucleus and then we apply and in
663:53 - nucleus we apply some kind of a function
663:55 - on it so let's say Sigma on Z okay and
663:59 - then we get some output Y and then it it
664:02 - it is either your destination it is
664:04 - either your destination which you want
664:05 - to
664:07 - achieve or or it is or it is sent to
664:11 - another another neuron the same neuron
664:13 - like this where it have where it again
664:16 - multiply without which which will'll see
664:17 - later on
664:19 - okay cool so I hope that this gives you
664:22 - a very specific sense about artificial
664:25 - neural networks and I hope that uh
664:28 - you'll you'll you are able to understand
664:30 - how how exactly it is
664:33 - working so now what I will do I'll just
664:37 - just make a remark that neural networks
664:40 - whatever artificial neural network which
664:42 - you see is called exactly this is what
664:44 - artificial neural network is okay so
664:48 - neural network basic units is inspired
664:52 - from machine learning and machine
664:53 - learning is obviously spy from a brain
664:56 - and stuff so so here which you're seeing
665:00 - if if I know this the course
665:02 - prerequisite is machine learning
665:03 - fundamentals I either recommend in my
665:06 - mll1 course or anything but you're
665:08 - exactly seeing that this logistic
665:11 - regression or or or whatever we have
665:14 - studied is exactly logistic ression if
665:17 - you see okay so basically uh this is the
665:21 - one unit so this is what you what you're
665:24 - seeing this is a logistic regression
665:26 - what do you do you simply multiply with
665:28 - the given weights and the inputs and
665:30 - then we agre create it and then we and
665:34 - then we apply a nonlinearity or the
665:36 - sigmo function which you usually know 1
665:38 - / 1 + e to the^ minus g z and then you
665:42 - will get some output let's say a and
665:44 - then if
665:46 - a is greater than 0.5 you identify okay
665:50 - that that
665:52 - value um uh let's uh one or if it is or
665:57 - else if it is smaller than or equals to
666:00 - 0.5 then zero okay then zero so this is
666:04 - this is usually used for classification
666:06 - problems and why do we apply this sigmod
666:09 - function we'll talk about in detail in
666:11 - this deep learning we'll talk about this
666:12 - Sigma function in very detail but what
666:15 - this sigment function tells you um so
666:17 - what it does it you you get your output
666:20 - you get your output in a range in a
666:23 - continuous manner okay so it simply
666:25 - squeezes that output into bit between 0
666:29 - to one between 0 to one now this was
666:33 - your basic unit which you already seen
666:35 - in machine learning what do you do you
666:38 - put take that basic points and you put
666:40 - several units and stack them up okay and
666:43 - take that stacked layer and make several
666:46 - layers okay so so you this is a
666:49 - particular unit which is exactly what
666:51 - exactly computation is doing is simply
666:54 - multiplying with different different
666:55 - weights and then applying processing Etc
666:58 - and putting up different different basic
666:59 - units and then at last with that kind of
667:03 - stack or basic units are able to
667:05 - generate predictions so this is the Deep
667:08 - neural so we will talk about that later
667:10 - on but the basic idea that this slides
667:13 - want to give you that neuron networks
667:16 - are inspired from basic units of machine
667:19 - learning or puts up puts up lots of
667:21 - basic unit together and then get your
667:22 - output but don't worry what this exactly
667:25 - diagram States forget about
667:28 - it about it if you don't understand okay
667:32 - please rewatch the video if you don't
667:34 - understand otherwise uh you can simply
667:36 - ignore what what exactly each neuron is
667:38 - doing will will cover that in detail in
667:40 - a multi neural network uh or perceptron
667:44 - section cool
667:46 - so the perceptron now what we'll do is
667:49 - formally Define a perceptron so that it
667:52 - would very helpful for you at least so
667:55 - let's take an
667:57 - [Music]
667:59 - example so now we will formally Define
668:03 - the perceptron so um here you can see
668:07 - the simplest neural network is referred
668:11 - as the perceptron as the perceptron
668:14 - where uh where we have the basic which
668:17 - is the simplest neural network which we
668:19 - which we refer as a perceptron so what
668:22 - we are given we are given the
668:24 - information from X1 to X dimensional
668:28 - okay so I'll take an example I'll take a
668:30 - simple example because I just want to
668:32 - may have a conversation with you so that
668:34 - at least you can understand everything
668:35 - and please make sure that you watch in
668:37 - kind of a 2X manner it's okay for you I
668:39 - do I I don't care about that so uh let's
668:43 - take an example of a diabet or maybe huh
668:46 - yeah so diabetes predictions system
668:49 - diabetes predictions I'll just take a
668:52 - different pen so that at least it gives
668:54 - me good a feeling so uh what's your
668:57 - favorite color I don't know it's blue so
669:01 - diabetes
669:03 - prediction
669:04 - system diabetes prediction system so in
669:09 - machine learning you're given set of
669:10 - features and you're told to take that
669:13 - set of features and map to or make a fun
669:15 - F that takes a particular set of
669:17 - features and maps to an output variable
669:19 - y that's it that's that's exactly what
669:22 - in machine learning you're trying to do
669:23 - so these so let's say your X1 denote U
669:27 - maybe some kind of a um let's say the BP
669:32 - okay and X2 denotes
669:35 - BMI okay X3 denotes your
669:39 - age X4 denotes your maybe um uh symptoms
669:44 - which have a certain SE or the height
669:47 - okay X4 Den notes your Heights so these
669:49 - are your information these are your
669:51 - information okay and these information
669:54 - are carried by the weights are are these
669:56 - information are weighted by weights that
669:59 - how much this how much this have effect
670:02 - or or or or how much um uh how much how
670:06 - how how much our input effects okay so
670:09 - or how much our information carries
670:11 - information okay W3 and W4 so w 1 W3 W4
670:17 - okay so you have a respective weights
670:19 - which tells you how much it affects okay
670:22 - how much how much your information have
670:24 - weights or how much how much weightage
670:27 - or how much information it has
670:30 - Okay cool so what I will do I'll just
670:33 - have my X and uh this is
670:37 - X1 X2 all the way around to the XD okay
670:41 - and that is transpose so that it's
670:44 - become it's a column Vector I think yeah
670:45 - it's a column Vector it should when when
670:47 - you do the transfers it becomes the row
670:49 - Vector that is a x which is the input
670:52 - value and output value output value Y is
670:55 - either zero or one I'm taking an example
670:59 - of a classification case of a
671:01 - classification case so you are your
671:03 - output is either zero or one cool so
671:08 - specifically over here you have a
671:10 - weights you have a weights um which is
671:13 - carried by the weights your information
671:14 - where
671:16 - what is learning so where learning
671:18 - occurs how you learn the particular
671:20 - system how you make a diabetes
671:22 - prediction system so how you learn it so
671:24 - learning occurs by changing the weights
671:28 - and the goal of changing the weights is
671:30 - to modify the computer function to make
671:33 - the predictions more correct in future
671:36 - iterations so what is the learning tells
671:38 - you how do we how do we learn it okay so
671:41 - the whole learning stuff is getting the
671:45 - right set of weights or identifying how
671:48 - much X1 contains or or identifying the
671:51 - right set of Weights so that we'll be
671:54 - able to identify which input have more
671:57 - effect okay which input have have more
672:00 - effect if you don't understand from this
672:02 - point that how learning occurs don't
672:04 - worry about it we'll talk about in
672:06 - detail what exactly we call as a the the
672:10 - learning
672:11 - problem okay sure so um what exactly how
672:16 - what what is learning and in high level
672:18 - overview what we do the learning occurs
672:21 - by changing the weights W1 W2 W3 and W4
672:27 - so we change the weights until and
672:29 - unless our prediction uh be more correct
672:33 - in our future iterations okay and then
672:38 - uh this and then you you strive to find
672:40 - the the best weights the best weights by
672:44 - choosing algorith like gradient descent
672:46 - algorithm or stochastic gradient descent
672:48 - and there are lots of optimization
672:49 - algorithm which are out there cool so
672:52 - now after now what you do you you simply
672:56 - multiply or or or or multiply the or
672:59 - take out the product of the inputs and
673:01 - the weights and then you aggregate the
673:04 - weighted information okay the whatever
673:07 - the information which you contain
673:08 - whatever the information which you
673:10 - contain you simply multiply with the
673:12 - inputs just order to X so um simply
673:15 - aggregate the information XI and wi so
673:19 - um W1 X1 so we identify so we are able
673:24 - to uh say okay in a nucleus you'll be
673:27 - having the weighted information or the
673:29 - aggregated information so that we are
673:31 - just adding it up and then we are adding
673:33 - the bias STM and then we apply some kind
673:36 - of activation function which is some
673:38 - nonlinearity okay which is some kind of
673:40 - function here here it is a sigma
673:42 - function before sending it to the
673:44 - destination
673:45 - y okay so now we are we are ignoring too
673:49 - much about bias STM we are ignoring bias
673:52 - term and then we are ignoring
673:54 - nonlinearity or activation function okay
673:57 - let's talk about that and then we'll end
673:59 - this
674:01 - [Music]
674:03 - video cool so over here the perceptron
674:07 - with the bias term so what exactly do
674:11 - what exactly bias term help you to do
674:13 - say for example take a simp simple
674:15 - example y = mx + b y = to mx + b so you
674:20 - all know about it very very very
674:22 - carefully so you all know know know
674:24 - about this specific term called y = mx +
674:28 - b where you have
674:32 - this where let me just erase it out so
674:34 - that it would be much better because I
674:36 - don't know why I'm not able to draw very
674:38 - good kind of stuff oh my God no problem
674:42 - so you have this and then you you have
674:45 - this Okay cool so this is the the
674:48 - equation for this straight line Y = to
674:51 - mx + b okay and M here is the
674:56 - coefficient or yeah coefficient of x
674:59 - coefficient of x and B here is your Y
675:03 - intercept you B here is Y intercept or
675:06 - we can say m is slope of this particular
675:09 - line okay so what if just I'll I'll I'll
675:13 - not go into y MX plus b because you
675:16 - already know because it scores expects
675:18 - you to go go at Algebra 1 and Al Algebra
675:20 - 2 so um what is B tells you if we B if
675:24 - we make that b to be equals to zero what
675:27 - will happen if we make that b equals to
675:29 - Z what will happen so over here your y
675:34 - your y will be zero okay your y will be
675:37 - zero and always pass through the origin
675:39 - which is 0 and Z where X is z and y is z
675:43 - and Depends and your straight line or
675:45 - your or your or your or your line will
675:47 - depend only on one parameter so y = to
675:51 - MX where if you have b equals to Z then
675:54 - then we don't need to write it so over
675:56 - here it only depends your slope okay it
675:58 - only depend you can either make this way
676:00 - or either make this way but the only
676:03 - parameter which it which which it it
676:05 - will depend is M okay is M now over here
676:11 - you're only able to make a function
676:13 - you're only able to make a function
676:16 - you're only able to make a
676:18 - function which just Maps which which is
676:22 - which is non complex function which
676:23 - passes only through origin and just only
676:25 - depends on one parameter but when we add
676:28 - B there when we add B there y = mx plus
676:32 - let's say 2 so over here let's assume
676:35 - two so you you able to make more complex
676:38 - you're able to make more complex
676:40 - function and shift the graph okay say
676:43 - for example
676:45 - it it's if we add B if we have zero then
676:48 - it does not shift the graph when we had
676:50 - the bias stor it shift from here to here
676:53 - so what it does it shifts the graph and
676:56 - hence it is able to represent more
676:59 - complex situations which it was not able
677:02 - to um make it before so from this y = mx
677:05 - plus C example it is simply saying it is
677:09 - able to represent the more complex
677:11 - example or shift the graph a little bit
677:14 - up the same way it will work in your
677:16 - neural network bias allows you to shift
677:20 - the activation function by adding a
677:23 - constant to it that is the given bias to
677:25 - the input you can think of it as a
677:28 - linear constant which we learn as a
677:30 - weight okay which we need to find a good
677:33 - bias okay we we we of course we don't
677:35 - just just just we do do not only need a
677:38 - simply add a bias we do need to find a
677:40 - very good bias which we'll talk about L
677:43 - later on that how do we get the W 1 W2
677:45 - or how do we get B1 and Etc so and make
677:49 - sure that W1 W2 W3 uh W1 W2 these are
677:54 - the weights these are the weights or we
677:57 - sometimes call the parameters and you I
677:59 - I I have already discussed these weights
678:03 - are are the weights for your inputs for
678:05 - your inputs or every input has it
678:07 - certain weights okay so specifically in
678:10 - this case your bias storm help you to
678:13 - shift the graph of your activation
678:15 - function so if you have seen your
678:17 - logistic regression if youve seen your
678:20 - logist logistic regression the graph of
678:23 - this the graph of logistic regression is
678:27 - s shaped s shaped okay s shape like like
678:31 - this so when bias is zero it the the
678:34 - origin is over here and bias is zero now
678:38 - when we say bias one then it is able to
678:41 - shift the graph and hence it is able to
678:44 - represent more complex situation and
678:46 - when we do plus one then it is able to
678:48 - represent much more complex situation so
678:51 - basically bias term shifts the graph a
678:55 - little bit or a linear as a linearly and
678:58 - then it is it is able to represent more
679:00 - complex situation so that's why bior is
679:04 - absolutely necessary in neural networks
679:06 - it's not kind of it it will not work
679:09 - without biom but it's very very
679:11 - necessary part of neuron
679:12 - networks cool
679:16 - cool so uh over here which you're seeing
679:19 - is neural network which is let's say you
679:22 - want to build a house price predictor
679:24 - because we have all we we we are only
679:26 - seeing out the examples of maybe kind of
679:30 - stuff like um we are only seeing
679:32 - examples of maybe kind of a
679:35 - classification problem but what if you
679:37 - if you get a regression problem where we
679:39 - want to predict the house prices so
679:41 - let's say we are given the information
679:42 - X1 X2 so these information is carried by
679:45 - the W1 W2 W3 and for a bi and okay so so
679:50 - then we apply a linear activation
679:52 - function we don't apply any any kind of
679:54 - nonlinearity we don't apply any
679:57 - nonlinearity we don't apply any
679:59 - nonlinearity you don't apply that you
680:02 - simply apply the linear activation
680:04 - function which is just the identity
680:06 - function okay we just it's it's a
680:09 - function when you give your Z which Z is
680:12 - aggregated information of nucleus which
680:15 - just gives Z the input okay which is a
680:18 - linear activation function okay and then
680:21 - you and then you reach the output and
680:23 - then we have your
680:25 - familiar m e meanus square error okay
680:29 - and then you take the partial derivative
680:32 - of it but don't worry about it we'll
680:35 - we'll talk about talk
680:38 - about gradient descent in Greater detail
680:42 - okay don't worry ignore it just I I have
680:45 - just written it out so that how training
680:47 - occurs you'll get to know but just
680:49 - ignore
680:50 - this cool so the last thing which I'll
680:53 - discuss in this video is intuition
680:56 - behind activation function why do we
680:59 - need this n nonlinearity which you're
681:02 - seeing that why do we need a sigma
681:04 - function and why do we need activation
681:06 - function after after we aggregate the
681:08 - information why do we even need that
681:10 - next step which is nonlinearity why do
681:13 - we even need to think about that so
681:15 - don't worry about what the Picture Tells
681:18 - don't worry about what the Picture Tells
681:20 - worry about some uh a story okay so if
681:24 - you smell something delicious delicious
681:27 - okay I'm pronouncing it correct so if
681:29 - you smell something delicious think of
681:31 - your favorite food my favorite is let's
681:34 - say an example of pizza okay take any
681:39 - example your favorite food so if we
681:41 - smell something delicious which is your
681:43 - favorite food in your neurons your sum
681:47 - neurons which which learn to identify
681:50 - that favorite food will get activated
681:52 - and will help you to get sense or tast
681:55 - it so your so you have several neurons
681:58 - you have millions of neurons and some
682:00 - neurons will get activated when you
682:02 - smell it okay to uh when when when when
682:05 - you smell some when you smell your
682:06 - delicious food your some neurons will
682:09 - get get activated and then it will give
682:12 - the signals to your mind to to just get
682:15 - a sense of something or taste something
682:18 - okay and if you s smell something which
682:20 - is not good the same neurons do not get
682:23 - activated okay now this is kind of a
682:26 - critical in this case it's very very
682:28 - kind of inspiration from brain like say
682:31 - for example you have a neuron 1 you have
682:33 - a neuron 2 you have a neuron three you
682:35 - have a neuron 4 you have a neuron 5
682:37 - neuron 6 neuron 7 neurons eight okay so
682:39 - several neurons you smell something
682:42 - delicious you smell something delicious
682:44 - so what will happen so what will happen
682:47 - maybe the first neuron get activated the
682:50 - first neuron get activated this neuron
682:53 - get activated and this neuron get got
682:56 - activated these three neurons got
682:58 - activated and then it helps and then it
683:01 - it it it tells your hand or tells your
683:03 - mind to just uh taste something or kind
683:05 - of that but but but now now what will
683:10 - happen I'll just have this now the same
683:13 - neurons which we are having the same
683:15 - neurons which which we are having now
683:17 - when you Sim something which is not
683:18 - delicious the same neurons will not get
683:20 - activated other neurons which will get
683:22 - activated which is not the same which
683:24 - which is used for kind of so it will not
683:28 - get activated the same will will not get
683:30 - activated but other neurons will get
683:32 - activated that will help you to leave
683:34 - that food so in general same happens
683:38 - with your neural network that can happen
683:42 - as one activated and zero
683:45 - nonactivated okay so we can say if the
683:47 - value closer to the value to zero the
683:50 - Lesser it is activated so in this
683:52 - example in this in this example in this
683:55 - example um say for an example you have
683:58 - X1 X2 X3 and X4 now these are your
684:03 - inputs now over
684:05 - here this this neuron got activated this
684:08 - this neuron got most and most activated
684:11 - because it for for for this task these
684:14 - two got more activated these two got
684:16 - more activated as compared to this these
684:18 - two are completely zero these two are
684:20 - completely zero and these two are
684:22 - completely one these two are completely
684:24 - one and maybe this is kind of maybe
684:27 - 0 um 75 okay more one okay so um this is
684:33 - how the interation of activation
684:34 - function which activates you which
684:37 - activates a particular neuron so that it
684:40 - is able to do one
684:41 - task this is a this this is kind of a
684:44 - relation to your uh brain kind of stuff
684:47 - uh so uh using linear activation
684:50 - function so why our why our um
684:53 - activation function should be linear why
684:56 - do we need nonlinear so here your data
684:59 - is linearly separable so we do not need
685:02 - any nonlinear activation function we are
685:05 - happy with because here your data is
685:08 - easily separable by a straight line so
685:10 - it is linearly separable but in real
685:12 - world your data is not nonlinearly
685:15 - separable so that's why you have your
685:17 - favorite nonlinear activation functions
685:20 - one of the example of a nonlinear
685:21 - activation function is sigmoid function
685:25 - okay which is 1 / 1 + e^ minus Z and all
685:29 - null linear activation functions are
685:32 - differentiable when you take out the
685:35 - derivative of this you take out the
685:38 - derivative with respect to Z is nothing
685:41 - but time 1 -
685:45 - C okay but don't worry about it we'll
685:47 - see the derivations later on so over
685:50 - here your data you cannot fit a straight
685:52 - line to this or fit a straight line like
685:54 - like this to to separate it out so so so
685:57 - for that you need a this you need a or
686:00 - you need a line or a circle which
686:03 - separates it out so so it separate the
686:05 - green and this blue one which is a non
686:07 - linear which which exactly nonlinear
686:09 - activation function achieves is is using
686:12 - to find a nonlinear decision boundary
686:15 - okay so here your data is nonlinear it
686:17 - helps you to achieve the nonlinear
686:18 - activation function to help you to build
686:20 - a nonlinear decision boundary okay so I
686:22 - hope that this gives you a better sense
686:24 - of it so I hope
686:26 - um um so I'll just start off with the
686:30 - last of uh I'm saying from a lot of time
686:32 - so so basically using nonlinear
686:34 - activation function so you have a set of
686:37 - uh inputs X1 and carried by the weights
686:40 - and then you aggregate the nucleus and
686:43 - whatever get net J and then what you do
686:46 - you apply to a non a nonlinear
686:48 - activation function because your data is
686:50 - non non non L linear okay so provide
686:55 - that into a nonlinearity okay and then
686:57 - and then it gives your output and then
686:59 - using a threshold like Z if it is
687:01 - greater than 0.5 you'll just say Okay
687:04 - this the person has a what's say the
687:07 - person has U diabetes so if if it a
687:10 - classification problem but what if it if
687:13 - it is a regression problem we use a
687:15 - linear activation function okay just uh
687:18 - pass that uh the output or or or or the
687:21 - net J rather than applying activation
687:23 - function okay and then and then we get
687:26 - an output but you may be thinking you
687:27 - may be having some questions around it
687:29 - hey hey hey I use you told in can we
687:32 - have a nonlinear data in regression
687:35 - problem yes absolutely we can have but
687:39 - but you will soon realize in neural
687:41 - networks you don't have only one neuron
687:42 - you have a several neuron and and at
687:44 - last you have one output layer and in
687:46 - that output layer uh where there you
687:49 - don't apply any activation function but
687:51 - other neurons you do apply activation
687:53 - function but in rest but in output
687:55 - neuron you don't apply neuron Network
687:57 - sorry activation function and then you
687:58 - simply apply a linear and then you get
688:00 - your output for regression problem but
688:02 - don't worry about it we'll discuss that
688:04 - in very detail so I hope that this is
688:06 - very clear on activation functions and I
688:09 - hope that you understood it very well
688:11 - now we'll talk about the training how do
688:13 - we how do we make it learn or how do we
688:16 - make it learn a real world neural
688:17 - network or how how do we make it how do
688:20 - we train this neural network so that it
688:22 - would be able to cor correctly classify
688:24 - uh the particular neuron and what
688:25 - exactly learning means and what is
688:27 - learning problem so let's talk about
688:29 - that in the next lecture hey everyone
688:32 - welcome to this lecture so in the
688:33 - previous lecture we had a talk on
688:35 - introduction to neural networks we had
688:38 - defined artificial biological neural
688:40 - network we also talk artificial um
688:44 - neural network in inspired by biological
688:47 - neural network we had also told you
688:49 - about um what are weights and Etc we had
688:52 - also given an intuition behind what are
688:54 - activation functions and why why do we
688:57 - need it we talked about linear and
688:59 - nonlinear activation function so now we
689:01 - had seen that how we we we we go ahead
689:05 - and make prediction like with the
689:07 - information carried so what eventually
689:10 - we do so we'll just just do the quick
689:12 - recap of it so basically basically what
689:14 - exactly we do is this is what the
689:17 - perceptron looks like so you have the
689:19 - input feature X1 X2 X3 okay and X4 so
689:25 - these are the four input features and
689:27 - each feature is carried by the weights
689:31 - and these weights like W1 W2 W3 W4 and
689:36 - we have an bias term please see the
689:38 - previous video to know why do we require
689:40 - bi so these are some of the weights so
689:44 - these all the and we have x0 which is
689:47 - always set to one okay so basically an
689:50 - x0 is being multiplied with this so X1
689:53 - be multiplied with its weight and it
689:57 - identifies the effect of our input so X1
690:00 - + W1 + X2 + W2 all the way down to the
690:04 - X4 + W4 and these information are
690:07 - carried into a nucleus if you talk about
690:10 - in biological way or is we we do the
690:14 - some of it and then we apply some kind
690:16 - of processing on it which usually called
690:18 - as activation function whether to
690:21 - activate the activate this particular
690:23 - neuron or not okay and if if it is gives
690:27 - zero then then we say okay or or maybe
690:30 - if let's say you are working on some
690:32 - classification problem it g 0.4 so it if
690:35 - it is smaller than our threshold if it's
690:37 - smaller than our threshold H let's say
690:40 - epsilon then we say Okay um say that
690:44 - returns zero and maybe zero can be the
690:46 - person has a diabetes and if it is
690:48 - greater than or equals to maybe greater
690:51 - than or equals to Epsilon then you can
690:53 - say return one and that one can be the
690:55 - person does not have ades so it it is
690:58 - totally dependent on your case so that's
691:01 - just that was a particular neuron where
691:03 - we talked about that how the forward
691:05 - thing work okay so how we do the forward
691:08 - propagation how we do the this is called
691:10 - a forward propagation this is called the
691:13 - for forward propagation where we are
691:15 - prop propagating forward to make
691:18 - prediction okay so we we where we are
691:21 - forward where we are doing for
691:24 - propagating forward to make prediction
691:26 - or get our particular variable Y what
691:28 - exactly we are doing is multiply with
691:30 - these input feature with the weights and
691:33 - do the weighted sum of it and then apply
691:35 - some n activation function and then we
691:38 - get our
691:39 - output okay and now who will tell us
691:43 - that we we are getting the good output
691:45 - we are getting the good output and how
691:47 - do we how do we get these weights how do
691:50 - we get this W1 W2 W3 W4 and B because
691:55 - these are the input feature which we'll
691:56 - get from the user these are the input
691:58 - feature but how we will identify these
692:00 - weights how we identify these weights
692:03 - because it is playing a crucial role
692:05 - over
692:06 - here when when being multiplied so we
692:08 - need to identify what is the effect of
692:11 - our input okay because uh we we need to
692:15 - know how do we get these weights so
692:17 - let's get started talking about this so
692:19 - now let's get started what exactly do we
692:22 - mean by learning this perceptron so so
692:24 - how we can make this perceptron learn to
692:26 - make good prediction or can be using in
692:29 - in in or or okay so what we'll do now is
692:34 - to formalize our learning problem so
692:38 - what exactly does it mean is you have
692:40 - your input feature and you have your
692:43 - input and this is a giant X so we'll say
692:46 - x is a set or giant set which contains
692:50 - the input features and please note I'm
692:53 - not adding the design Matrix if you
692:55 - wanted to know what exactly we will what
692:59 - what is design Matrix I would recommend
693:01 - you to watch my machine learning
693:04 - fundamentals or ml1 videos like first
693:07 - video which was multivariable or
693:09 - multivar uh like multivariable um linear
693:13 - regression it would be very nice for you
693:15 - to know about how we structure our
693:17 - inputs uh so that you could get to know
693:19 - that exactly how the input is structured
693:22 - cool so you have X1 X2 X3 and X4 okay so
693:28 - these are the input features an input
693:30 - feature can be n features X1 all the way
693:34 - around to the xn okay so we have n
693:36 - features in our input set X cool now
693:40 - given these input feature given by the
693:42 - user so this this this will be given by
693:46 - the user this will be given by the user
693:49 - and we need to and we need to and then
693:52 - we'll be having our output if this is a
693:54 - case of supervised learning problem if
693:57 - this is this this is a case of
693:58 - supervised learning problem then we'll
694:00 - be having our label or we can say then
694:03 - we'll be having our supervisor which is
694:05 - Target variable Y which will tell us is
694:09 - which will tell us that how we which
694:10 - will help us our to our model to learn
694:13 - to okay so I hope that you know
694:15 - supervised learning so if it is a case
694:17 - of supervised learning problem then
694:18 - you're giving the given the input set
694:20 - feature X and then you given output
694:22 - variable Y and there's a ground Ru that
694:25 - that is a ground Ruth which we know
694:29 - because these are the these x and x X1
694:34 - y1 X2 Y2 these are your training
694:37 - examples which is given to you now we
694:39 - need to learn a function f which takes
694:43 - the input value X and we want our
694:46 - function to map to Target variable y
694:50 - okay so to map a Target variable y so
694:54 - you want to learn a function f that
694:56 - takes these input feature X and Maps
694:59 - this Maps this to a Target variable y
695:02 - hat and this y hat this y hat should be
695:06 - equals to why don't worry about other
695:09 - cases like overfitting Etc worry about
695:12 - that what exactly we need to learn we
695:14 - need to learn a function X we need to
695:16 - learn a function f that Maps your input
695:19 - variable to some Target variable okay
695:22 - and then um so and this target variable
695:26 - should be equals to your actual Target
695:27 - variable while training and then you can
695:29 - use this learn function to predict on
695:32 - new new examples where you don't have
695:34 - these your ground Thro okay so I hope
695:37 - that you're getting this this this is
695:38 - what machine learning tries to do it
695:40 - tries to find a function f that Maps
695:42 - your input variable to some output
695:44 - variable and and now so here's some
695:46 - diagram of it so you have unknown
695:48 - function you have not learned any kind
695:51 - of function which is f which Maps your
695:53 - input variable to Y and now what you do
695:56 - you have your training example with that
695:59 - training example you give that training
696:01 - example to the learning algorithm let's
696:03 - say in this case neural network or we
696:06 - can say linear regression or we can say
696:09 - logistic regression or we can say
696:12 - support Vector machine so so you give to
696:14 - the support Vector machine or whatever
696:16 - and then it will output and then it will
696:18 - output a learn function
696:21 - G learn function G which Maps these
696:24 - input variables to our output variable
696:26 - and this course comes with the machine
696:28 - learning fundamentals prerequisite so I
696:30 - hope that you know about these
696:31 - algorithms previously okay cool very
696:35 - nice now let's go further so this was
696:38 - the formal formalization of our learning
696:40 - problem so what exactly is learning so
696:43 - so how our perceptron or any algorithm
696:46 - learns so learning happens by changing
696:49 - the values of our weights so what
696:51 - exactly we do what I'm going to do is X1
696:54 - and X2 okay and I'm neglecting the bias
696:58 - term as of now okay so neglecting the
697:01 - bias term as of now you have X2 and X1
697:04 - we give to a neural network which is a
697:08 - perceptron what it does and each
697:10 - information is carried by the weights
697:13 - carried by the weights and we say X1 W1
697:16 - + X2 W2 and then you apply the procing
697:21 - or the nonlinearity on this Z and Z is
697:24 - this one okay so so so you may be
697:27 - thinking okay you you will you are
697:29 - getting X1 and X2 but who will give you
697:32 - W1 and W2 whole learning depends on
697:36 - getting these good W1 and W2 please
697:40 - listen me carefully whole learning means
697:43 - in all the learning what exactly you
697:45 - need to do is find
697:48 - good
697:50 - find fine good
697:54 - weights F good weights and some or
697:59 - parameters
698:02 - parameters is something if someone if
698:05 - someone is telling you that that this
698:07 - neural network is learning eventually
698:09 - what is doing is getting good weights or
698:12 - getting the good set of Weights that's
698:15 - getting the good set of Weights that
698:17 - that for the particular problem so let's
698:19 - say in starting when we start in
698:22 - starting what your perceptron will do
698:24 - the perceptron will set your W1 to be
698:27 - zero and W2 be0 okay in starting we'll
698:30 - set all the all our weights to zero okay
698:34 - now it will output some y okay that that
698:38 - will be also zero because when you
698:39 - simply multiply with 0 it will be
698:41 - eventually zero um so what what exactly
698:44 - it is doing is it change the value of
698:47 - Weights in every iteration so now you
698:49 - when you initialized your weights let's
698:51 - say W1 to be Z and W to be Zer and we'll
698:54 - see initialization techniques later on
698:56 - but let's say for this example we had
698:58 - initialized our weights now now we now
699:02 - we'll start training it eventually what
699:03 - this training means is getting the good
699:06 - WR rights or the getting the good set of
699:09 - weights or right set of Weights like W1
699:11 - and W2 so in first iteration an
699:15 - iteration number we have to check we
699:17 - have to give as a hyper hyper parameter
699:19 - let's say that we want to do for th000
699:22 - iterations okay so so what essentially
699:24 - it will do it will change the value of
699:27 - weights from 0 to maybe
699:29 - 0.2 and W2 to be
699:32 - 0.3 okay that will change the value of
699:34 - weights and check okay because you will
699:37 - get to know that we have something
699:38 - called as loss function and this loss
699:41 - function in this case it's if we apply
699:43 - the nonlinearity which is a logistic
699:46 - loss then then it is a log logistic
699:48 - regression or log loss which we'll use
699:50 - over here so basically we want uh so now
699:54 - after updating the weights we'll check
699:57 - if the loss is decreasing or the error
699:58 - is decreasing by changing those weights
700:01 - if it is decreasing then we update these
700:04 - weights from 0 to 0.2 and 0.3 that's
700:08 - first iteration is done we will do the
700:10 - second iteration and in second iteration
700:12 - what we will we'll change those weights
700:14 - like
700:15 - 0.4 and W2 by 0.5 and I'm doing this
700:20 - completely because some people will ask
700:22 - how are you writing these These are
700:24 - completely random okay you will get to
700:26 - know in late later on that eventually
700:28 - when we do the back propagation I'll do
700:30 - a step by step one example to show you
700:33 - how to show you every cases but over
700:35 - here I'm just sticking as a random okay
700:37 - now over here you up in the second this
700:39 - is a second iteration this is a second
700:41 - iteration this is a second second ration
700:43 - where W1 is this and W2 is this where it
700:46 - changed the weights or tweaked the
700:47 - weights from 0.2 to 0.4 and from for W2
700:51 - to from 0.3 to
700:54 - 0.5 now we check if the loss function
700:57 - loss function now we check the L loss
700:59 - function is decreasing if it is
701:01 - decreasing then we update from 0.2 to
701:04 - 0.4 and
701:07 - 0.5 okay So eventually what we are doing
701:10 - is changing the values of weights until
701:13 - and unless we get our good or or or or
701:17 - our loss function is equals to zero or
701:19 - our error is zero so in brief we'll use
701:22 - our batch gradient descent algorithm
701:24 - maybe you already know about batch
701:26 - gradient descent algorithm to minimize
701:28 - our loss functions to minimize our loss
701:31 - function So eventually our goal our goal
701:34 - what is our goal to minimize to minimize
701:38 - our error our error with respect to for
701:42 - with respect to um yeah so we need to
701:45 - minimize the uh error um we need to
701:48 - minimize our error that's our end goal
701:50 - so we'll use gr gradient desent
701:52 - algorithm to find good weights don't
701:54 - worry you don't have to do by your hand
701:56 - will you make use of the algorithm which
701:58 - is gradient descent algorithm to find
702:00 - the global Minima of our function J of
702:03 - theta and in the case of regression
702:05 - problem your J of theta is 1 / 2 m i = 1
702:11 - all the way down to the m s h of x - y
702:15 - n² so that is a mean square error and
702:19 - the and what you do you have a function
702:21 - and you need to find the global Minima
702:23 - in this function okay and when you when
702:26 - you find the global Minima in this
702:28 - function that will be here okay and we
702:32 - will we'll cover this so we'll we need
702:34 - to find the until and unless our loss is
702:37 - equals to zero or it it reach to Global
702:40 - Minima in our function J of theta
702:43 - cool so I hope that this is pretty much
702:45 - clear about what exactly learning means
702:47 - learning simply means is changing the
702:49 - values of our weights in every iteration
702:52 - and checking if the all f is decreasing
702:54 - if it is then update those values to the
702:57 - new weights from the old weights to the
702:59 - new weights cool and uh in brief what we
703:02 - do is use bash gradi algorithm in this
703:06 - example but there are lots of
703:08 - optimization algorithm that that that
703:10 - covers the limitation of these gradient
703:12 - descent which which will help us to find
703:14 - the global Minima of our function or to
703:16 - converge or to get the good values okay
703:20 - so in anation of gradient descent what
703:22 - exactly is doing don't worry we'll cover
703:25 - gradient descent we have already covered
703:27 - actually gradient descent in our machine
703:28 - learning course if you want to go there
703:30 - you can go there and watch that what
703:31 - exactly does it mean but don't worry if
703:33 - you don't want to watch that we'll cover
703:34 - gradient descent again to to to let you
703:37 - know how exactly the working uh means
703:39 - how exactly the geometric purpose of
703:41 - gradient desent works
703:43 - cool so let's go ahead let's go ahead
703:46 - talking about the gradient descent so
703:48 - that you could understand if you haven't
703:50 - understand don't worry about it you will
703:51 - you will understand it later on cool so
703:55 - we'll use our standard gradient descent
703:56 - algorithm to train our perceptron means
703:59 - what do we mean by training we mean by
704:01 - training is getting those good weights
704:04 - W1 and W2 okay and and of course we do
704:08 - need a bi as well we do need a bias as
704:10 - well so we need a bi so we need to get
704:13 - W1 good we need to get a good W1 W2 and
704:16 - the B we need to get these three
704:18 - parameters or weights uh which are good
704:20 - for your problem so here how it works so
704:23 - in every iteration okay in every
704:26 - iteration it changes the value of
704:28 - Weights okay of her parameters and check
704:32 - how well your model is performing with
704:34 - the current weight if it is performing
704:36 - well it simply update the weights with
704:38 - the current value it keeps on doing this
704:41 - until unless you convert first to your
704:43 - Global Minima okay so over here over
704:47 - here let's say over here let's say you
704:50 - have your loss which is very high loss
704:53 - very high error very high error very
704:56 - high error which is J of theta let's say
704:58 - um 500 okay that is your error which is
705:02 - pretty big now over here when the when
705:05 - when your loss was 500 your weights were
705:08 - Theta 0 oh my God W1 to be Z and w 2 to
705:13 - be zero and bias to be also zero so
705:16 - these are all our zeros when you're for
705:19 - when your all thetas were or sorry when
705:21 - your all weights are zero then you your
705:24 - loss was very high okay now when you
705:28 - update these laws when you update these
705:31 - laws now what do you do using gradient
705:33 - sent what exactly it's doing it's it's
705:37 - it's changing the weights of this it's
705:39 - changing the weights of this of all of
705:41 - these like now it changes the weight to
705:45 - W1 to be 1.2 and W to be 0.4 and B to be
705:50 - 0.1 so it changed the weights now
705:53 - checked if the loss function is
705:54 - decreasing so you have a less loss and
705:56 - now when we updated it loss function
705:59 - seems to decreased loss function seems
706:02 - to decreased it CES to 400 now we update
706:06 - our weights to be not this to be this
706:09 - okay now again what we do in next
706:11 - iteration it updates these weights to be
706:14 - the new weights and checks if the loss
706:16 - one decreasing and over here when
706:18 - updating those weights again tweaking
706:21 - this means uh let's say 4.1 so in this
706:23 - over here it by tweaking these weights
706:26 - we are getting the loss
706:28 - function loss function to be 200 which
706:31 - is eventually very low okay so keep on
706:33 - doing this until unless you reach to be
706:36 - here because here your loss function is
706:38 - zero and you get good set of parameters
706:40 - W1 W2 W3 all the W1 or in this for
706:44 - example W1 W2 and B so when you reach at
706:47 - this where your loss is zero you have a
706:50 - good parameters and this is come some
706:52 - this this is a parabola and something
706:54 - like that because it should look like
706:56 - Parabola um just for showcasing you all
707:00 - so I hope that what what does it mean if
707:02 - you want to know this in depth I have a
707:04 - video on my ml1 ml1 course please go
707:09 - there and watch that and on my YouTube
707:11 - channel um New Era so this was a
707:14 - geometric purpose of it this this was a
707:17 - geometric purpose of it now let's get
707:18 - ahead now let's get ahead to talk about
707:22 - um like the the the formulization um
707:26 - like the mathematical part of it like
707:28 - what what is strategy do we use to
707:30 - update the weights or change tweak the
707:32 - weights and Etc so we have already
707:35 - studied the dead derivatives it is just
707:38 - a fancy slope so what do the slope means
707:40 - how much y changes when X changes so
707:43 - what what do we need to know in this
707:45 - case what do we need to know in the case
707:47 - of here we need to know how much how
707:51 - much loss is changing how much loss is
707:57 - changing when we
708:01 - change
708:02 - W1 okay the same question again how much
708:06 - loss is changing when we change W2 how
708:09 - much loss is changing when we change to
708:12 - W or B okay for this example so so so it
708:16 - will tell us how much loss is changing
708:18 - cool so I hope that this gives you
708:20 - pretty much sense about now this is
708:23 - exactly slope and that is a slope and in
708:26 - calculus term if you want to take out
708:28 - the slope of a of a nonlinear or a curve
708:32 - if you want to take a slope of a curve
708:33 - we call that as a derivatives which you
708:35 - have already studied if you don't know
708:37 - about derivatives something will pop up
708:39 - or you can just go on New Era YouTube
708:41 - channel New Era YouTube channel there is
708:44 - a playlist called single variable
708:46 - calculus single
708:49 - variable
708:50 - calculus okay go there and complete the
708:53 - all set of videos then you'll be knowing
708:55 - the geometric purpose of the derivatives
708:57 - is just a fancy slope is just tells how
709:00 - much that loss changes when that weight
709:03 - changes okay how much that loss changes
709:06 - when that weight changes so how much
709:08 - cost function is changing when feature
709:10 - weights changes so we have a lot over
709:13 - here and this example we have taken the
709:15 - log loss we have taken the log loss
709:17 - because we are dealing with the
709:18 - classification problem in this case
709:20 - let's be honest we are dealing with the
709:21 - classification problem in this
709:23 - case cool so over here you have 1 / M
709:28 - where m is the number of examples it is
709:30 - looping through I = to 1 all the way the
709:33 - m y I log of H of x i + 1 - y I log of 1
709:38 - - s h of x i if you don't know about
709:41 - this log loss I again go recommend to
709:43 - complete mll1 course which is machine
709:45 - learning fundamental course now now what
709:49 - Z means Theta transpose X and H of X
709:52 - means um sigmoid of X and sigmoid is
709:55 - nothing but 1 / 1 + e^ minus Z and E has
709:59 - some value which you all know now we
710:02 - have a loss what our goal is we need to
710:05 - optimize our our we need to minimize our
710:09 - loss function and find the the good
710:12 - weights we need to find the good weights
710:14 - it should not be here it should be here
710:17 - we need to find the good weights good
710:19 - set of weight and that minimizes this
710:22 - loss function we need to find good set
710:24 - of Weights W1 W2 all the w n that
710:28 - minimizes the loss function J of Z okay
710:31 - so how do we solve this op optimization
710:33 - problem because you all know about this
710:35 - that um you maybe if you have already
710:38 - not studied calculus I'd again recommend
710:40 - you to do my calculus course because in
710:42 - this this course you will get to means
710:45 - it's to because I don't I don't ignore
710:48 - calculus in this course please please
710:50 - please go there and watch my single
710:51 - variable calculus videos and then it
710:54 - will it will make pretty much every
710:55 - sense to you now so now you need to
710:58 - solve this optimization problem so what
711:01 - exactly we do is the first step is to
711:03 - initialize all weights to be zero
711:05 - initialize W1 to be zero W2 to be zero
711:09 - all the way around to the w i to be zero
711:12 - with the bias term also equals to zero
711:14 - okay now you keep on doing this you keep
711:17 - on and now you keep on doing this step
711:21 - until and unless it converged or until
711:23 - unless it is great it is greater than or
711:25 - equals to that iteration the number of
711:27 - iterations you want to do or until
711:29 - unless it finds the global Minima and
711:31 - your loss function or until and unless
711:33 - your your JF s minimized okay so you
711:37 - have your gradient so this is your
711:40 - gradient which is which is which you
711:42 - already studied in that course of single
711:44 - variable calculus and this is maybe you
711:47 - will see in Vector calculus this is just
711:49 - a notational argument to write the
711:51 - derivatives so so what exactly we'll do
711:54 - for vectorization we'll put that all in
711:57 - in a column Vector so this is a column
711:59 - Vector it'll make that as a row Vector
712:01 - now so we so that is we need to that is
712:05 - the partial derivative of your loss
712:08 - function J with respect to W so that is
712:11 - the that is the vector
712:12 - of gradients uh gradient Vector now this
712:16 - Vector contains the partial derivative
712:18 - of J with respect to W1 why are we
712:21 - taking the partial derivative of J with
712:23 - respect to W1 if you don't know about
712:25 - the part difference between the partial
712:27 - derivative and the derivative it 90%
712:30 - means the same u means in it's it's it's
712:33 - kind of a don't worry if you don't know
712:35 - about the difference between partial
712:37 - derivative and derivative for this if
712:39 - you don't know about the difference
712:41 - please understand as a say okay so it's
712:43 - just saying how much the loss function
712:44 - is changing when we are changing this W1
712:47 - okay when we are changing this W1 So
712:50 - eventually you're taking out the
712:52 - derivative of a loss function with
712:54 - respect to W1 we taking out the
712:55 - derivative of a loss function with
712:57 - respect to W1 okay uh now now when we
713:00 - take the derivative so let's say W1
713:03 - equals to W1 which was old so I'll just
713:06 - write W1 old the old which in this case
713:09 - will be zero if it is the first
713:10 - iteration minus the alha Alpha which
713:13 - which which which I'll come to that
713:14 - let's assume that is 0 0.1 I will come
713:17 - to this Alpha in just a second and then
713:20 - you write partial der um partial der of
713:23 - with respect to W1 you said doj with by
713:26 - do by do W1 okay so this is what it is
713:30 - doing it simply it's simply simply what
713:35 - taking of the partial how much the loss
713:36 - function changes when W1 changes and
713:38 - multiply with the alpha okay and then
713:41 - subtract this particular whole
713:42 - operations with this old one okay and
713:45 - then update that W one to be the new
713:47 - whatever you get by doing this operation
713:50 - and you do this for every example and
713:52 - this keep changing the W new and W old
713:55 - will keep changing okay uh for example
713:58 - this is W1 is over now it will do for W2
714:01 - all the way around to the W J that is a
714:04 - first sttion okay that is a frustration
714:06 - in second iteration it will do W1 W2 all
714:09 - the way under WJ okay for the second
714:11 - iteration for third iteration it will do
714:13 - the same okay so uh so this is what does
714:16 - it mean what do I mean with this so you
714:18 - take out the partial derivative of J
714:20 - with the respect to L one if you don't
714:22 - understand till now don't worry about it
714:24 - I'll again talk about keep on talking
714:26 - about this but just I just I I I can
714:29 - just hope that you know about this
714:31 - particular thing what exactly we are
714:32 - doing because the you already have
714:34 - studied in your machine learning course
714:36 - so I just don't want to focus more on
714:38 - this so the learning rate GS and general
714:42 - know some additional control over how
714:45 - large the step should be make so if the
714:47 - learning rate is too large you can't you
714:50 - can overstep the minimum and even
714:52 - diverge so so you you all know that we
714:57 - are that we are doing this and we are
715:00 - here so if we have a now we need to
715:03 - identify which rate we need to go to the
715:05 - minimum which rate we need to go to go
715:07 - to this local minimum but this rate is
715:11 - set by the Learning rate Alpha learn
715:13 - learn learning rate alpha or we or you
715:16 - will see sometimes this notation okay so
715:19 - you will uh you want to uh know the
715:21 - learning rate or the rate by which you
715:23 - can control how large your iteration or
715:26 - steps should be um for your change so if
715:29 - the learning rate is too large you you
715:32 - can either diverse so let's say you have
715:35 - this uh you have this loss function and
715:37 - this you you need to go here now learn
715:40 - learning rate is around one what if what
715:42 - what you will keep on doing is go there
715:45 - go there go there it will just do like
715:47 - this it will never convert in local
715:49 - minimum it will just keep on diverging
715:51 - in The Jig jaag manner okay and if you
715:53 - have a small you will I I think uh
715:56 - you'll never converge okay so you can go
715:59 - there um https developers.google.com
716:01 - machine learning classc filtergraph this
716:05 - gives you a geometri or grab view of the
716:08 - learning
716:09 - rate so so over here we are we have on
716:12 - solving this optimization problem so
716:14 - again I'll repeat what exactly we are
716:16 - doing or where we are we are
716:17 - initializing the weights to a zero and
716:19 - we are repeating this step until an
716:21 - convergence so what exactly is doing it
716:23 - is simply M it is what is exactly doing
716:26 - it's first of all taking the partial
716:27 - derivative of of of what do you say of J
716:31 - with respect to W and that is W is old
716:34 - okay not the current on means what w is
716:37 - obviously whatever the W you have so you
716:39 - take out the partial Dera of clause
716:41 - function with respect to
716:43 - W1 and then what you do is what exactly
716:46 - is telling how much the loss changes
716:48 - when W1 changes and then we get to know
716:51 - about that and then we multiply with the
716:53 - learn with the alpha which is a learning
716:55 - rate which gives you additional control
716:57 - over how large the step should be and
716:59 - then you're subtracting from the old
717:00 - weight whatever you get that whatever
717:03 - you get from this operation which is
717:05 - this one and what which is this one and
717:08 - then you get your new weight cool I hope
717:11 - that this this gives you a pretty much
717:13 - Clear sense about it but the the next
717:16 - step is update rule so we are updating
717:19 - our old one with a new one and you may
717:21 - think hey a why are we subtracting it so
717:25 - one way to understand it is the way I
717:28 - understand it is we are subtracting why
717:31 - are we sub subtracting this operation
717:33 - from the W1 is because if you need to
717:36 - reach Global
717:37 - Minima when you if you need to reach the
717:40 - global Minima so let's take an example
717:42 - of
717:43 - this so you want to move down not up you
717:47 - want to not go above you want to go go
717:50 - so sorry you want to go down right so
717:53 - you are subtracting it to go down okay
717:55 - not you want if you add it it will go
717:57 - above it will it will go above okay so I
718:00 - hope that this this gives you pretty
718:02 - much sense sense sense about it so why
718:04 - we are Sub Sub sub subtracting it is the
718:06 - one reason to go down it's kind of a one
718:09 - way to understand there are several ways
718:11 - when you when you would do the partial
718:14 - derivative by your own hand you'll
718:16 - understand in more detail uh you'll get
718:17 - in problem sets so we are updating our
718:20 - old
718:22 - old weights with the new weight and
718:24 - instead of addition we are subtracting
718:26 - is the only reason because we need to
718:28 - reach our Global Minima over here you
718:30 - want to minimize your cost function so
718:32 - you need to reach to Global Minima so we
718:34 - need to subtract to go
718:36 - down so why gradi isent use derivatives
718:40 - so you may be thinking why are needing
718:42 - this kind of a partial derivative or
718:44 - derivative why do we even need it so
718:47 - what exactly is doing let's take an
718:49 - example that you are at this you are at
718:52 - this point and this is this this is this
718:54 - is your function of your loss or let's
718:56 - say this of course this is a parabola
718:58 - and let's assume that this is a function
719:00 - of J of Thea where your loss is very
719:02 - high at this point your loss is very
719:04 - high at this point but you need to reach
719:06 - over here but you need to reach over
719:08 - here but you need need to reach over
719:10 - here so when you take out the so what
719:12 - what it does it draws a tangent line on
719:16 - this it draws a tangent line on this
719:18 - okay T tangent line on this point when
719:20 - you take out the partial derivative of J
719:23 - with respect to W1 it takes out the in
719:26 - geometric view I'm saying it is a
719:28 - tangent line at this so what is this
719:29 - telling what it is telling if we draw a
719:32 - tangent line at that so let me just say
719:35 - if we draw a tangent line at the green
719:38 - point we know that if you're moving
719:40 - forwards okay okay so if we draw a
719:43 - tangent line or how does tangent line is
719:46 - drawn at that point it's taking out the
719:48 - derivative of your loss function or
719:50 - whatever that function is so if you take
719:52 - out if you draw a tangent line at that
719:55 - green point if we say if we moving
719:58 - upwards we are we are seeing that we are
720:01 - that we are going away from our local
720:03 - Minima sorry Global Minima or the Minima
720:06 - of the function where we need to reach
720:08 - where our loss function is zero that
720:10 - that we exactly want and when your loss
720:12 - function is zero you know that you have
720:13 - a good weight so if you draw a tangent
720:16 - line at the green point we know that if
720:18 - we are moving upwards we moving away
720:21 - from the Minima and if we if we are if
720:24 - we go down we are going close to our
720:26 - Minima okay so a tangent gives us a
720:29 - sense of the steepness of our slope of a
720:32 - function okay another thing it tells
720:36 - another thing which is a very crucial
720:38 - this derivatives tells another thing
720:40 - which this which is very CR crucial this
720:42 - derivative STS is the first is the
720:45 - direction to move Theta in or the or the
720:48 - parameters in so over here over here if
720:51 - you see that we are moving upwards we
720:53 - are going away from the Minima but if we
720:54 - go downwards we are moving or we are
720:57 - getting closer to Minima so it will tell
720:59 - you okay we need to go down ra rather
721:01 - than going upwards we need to go
721:02 - downwards okay the first thing it tells
721:05 - is to move s in the second is how big a
721:09 - step to take okay that how big a step
721:12 - how big step to take if the slope is
721:15 - large we need to take a large step
721:17 - because we are far away from the minimum
721:19 - and if the slope is small we need to
721:21 - take a smaller step because there the
721:24 - slope is smaller okay and for this
721:27 - illustration What do I recommend you to
721:29 - do maybe you if if you go to this
721:32 - website or the link when you put it it
721:34 - first of all it takes a larger steps but
721:37 - when when it start reaching to the
721:39 - Minima it start taking a small small
721:41 - steps because there your slope is small
721:44 - okay so it it tells you how big the step
721:47 - to take so these are the two things
721:49 - which this derivatives tells and what
721:51 - exactly we do need okay so I hope that
721:54 - this the calculus is again the very good
721:56 - thing which you need to know are again
721:57 - Toom to know about the geometry purposes
721:59 - by going on my YouTube channel Numa it
722:01 - would very helpful for you cool so now
722:05 - over here you have below is a perceptron
722:08 - which you all see which you which we
722:10 - haven't se which we have already seen it
722:13 - now what we will do is see how we will
722:16 - try train this perceptron to perform
722:18 - well and what do learning means or what
722:20 - does training means is getting good set
722:23 - of Weights W1 W2 all the way around to
722:25 - the W4 with the bias term so this is a
722:29 - perceptron which you're seeing over here
722:31 - and now I hope that uh you know I hope
722:33 - that you got to know about uh the
722:35 - perceptron and everything out here in
722:37 - the next video what I'll try to do is to
722:40 - make you train everything give you
722:42 - equations that how we will do and do
722:44 - we'll do one work example as well as
722:46 - we'll work on some activation functions
722:48 - for taking of the Deb we'll we'll do the
722:50 - lots of derivative steps in our next
722:52 - video till then bye-bye have a great day
722:55 - so in the previous lecture we had a talk
722:57 - on um the introduction to neural
723:00 - networks and I hope that you really uh
723:02 - like that lecture and understood every
723:05 - concept because most of the concept
723:07 - later on will be dependent on the
723:09 - previous one so basically um now we'll
723:12 - start talking about the learning problem
723:15 - and you don't need to worry about if you
723:17 - don't understand it it's just for giving
723:20 - you an intro if you know about machine
723:22 - learning you just know you know about
723:23 - grade in descent how gr grade in descent
723:25 - works then it will be pretty much clear
723:28 - to to understand this section if you
723:31 - don't know about it I will ask you to
723:32 - watch my calculus videos to understand
723:35 - it better and if you don't understand
723:37 - even if you know calculus I'll ask you
723:39 - to wait for the next lecture because
723:41 - we'll be having we'll be doing a
723:43 - step-by-step example uh using numeric
723:46 - example to help you out cool so so what
723:49 - is so over here what I'm going to do is
723:51 - to formulate the learning problem so
723:55 - basically in this we are formulating the
723:57 - learning problems so here you're given
723:59 - the input variable X and then you have
724:02 - input variable Y and then you want to
724:04 - make a function f which takes this big
724:07 - or or the big X or set X and Maps it to
724:11 - Output VAR varable and we want our
724:13 - function to map our input to Output
724:15 - variable so you you all know in machine
724:18 - learning what we do we take a set of
724:20 - inputs let's say you want to you you
724:22 - wanted to build a diabetes prediction
724:24 - system so you take uh let's say the age
724:28 - of a person the height of a person the
724:30 - BMI and Etc whatever Foods they eat and
724:34 - you take the inputs and then you make a
724:36 - function f and then using that and then
724:39 - it learns to predict a given those input
724:42 - features or those input values or those
724:44 - information to map it to Output variable
724:47 - whether the person has a diabetes or not
724:49 - okay so you want to learn a function f
724:52 - and your data if you if you I I assume
724:55 - that you already know machine learning
724:57 - and and a bit of machine learning the
724:59 - machine learning fundamentals so
725:01 - basically in Mach what what what what
725:03 - you do you you you have a you have data
725:07 - so X1 and y1 which is the one training
725:10 - example X2 and Y2 which is second
725:13 - training example X X3 and Y3 which is
725:16 - third training example and x and YN but
725:19 - over here you have for example input and
725:22 - the target variable input and the target
725:23 - variable okay so this this is your data
725:26 - now you want to learn uh F which takes
725:29 - that input variable X and then maps to
725:31 - Output variable y so so you so when you
725:35 - pass it through this algorithm which is
725:36 - NN and you pass through this algorithm
725:39 - you pass F to this algorithm you will
725:41 - get G which is trained function or which
725:45 - is learned function which is uh which is
725:48 - what what do you say which is very good
725:50 - function to Maps your input variable x
725:53 - to the output variable
725:55 - y so that's a case about learning
725:58 - formulation so I hope that you already
726:00 - know about nonlinearity I won I don't
726:02 - want to jump into this so what is
726:05 - learning
726:06 - so learning happens by changing the
726:10 - values of our Ates and in every
726:12 - iteration of it we should decrease in
726:15 - brief we'll use our bat grent descent
726:17 - algorithm to minimize our cost function
726:20 - so we had already had a talk on this we
726:21 - all we we also talked on our gradient
726:24 - descent algorithm we had also also seen
726:26 - our this and now we also also seen the
726:29 - learning rate we have also seen the
726:30 - update rule we we have also seen the Y
726:33 - gcent users
726:35 - derivative now what we will do is um
726:39 - jump on the training of perception so
726:42 - you have a perceptron where you have a
726:44 - set of inputs you all know what what
726:46 - exactly perceptron does it takes set of
726:49 - inputs with a biased term with a biased
726:52 - term and then pass it to nucleus or a
726:55 - neuron which we say in neuron it do two
726:58 - things first of all aggregate the
727:00 - information and take the weighted sum
727:02 - exactly what it is doing there and the
727:04 - second half it applies the processing in
727:06 - it which is 1 / 1 + e^ minus Z which is
727:10 - an active ation function whether to
727:13 - activate this neuron or not and then we
727:17 - get our output by hat and then we check
727:20 - now we got a y hat which is a model
727:22 - prediction now we check whether this y
727:24 - hat is equals to that Y which is a
727:27 - ground through and if it is uh we we
727:29 - take a loss uh using the log loss which
727:32 - is the negative log log likelihood which
727:35 - you're seeing over
727:36 - here okay so now what we have planned to
727:39 - do is to use grid algorithms so we have
727:43 - already seen what what exactly learning
727:45 - means learning means changing the values
727:47 - of weights until and unless we get a
727:51 - good model until and unless um our loss
727:54 - is minimized so basically W1 W3 all the
727:57 - around of the B are the parameters or
728:00 - the weights which we need to find good
728:02 - which we need to find perfect weights
728:04 - which works very well so basically in
728:06 - whole deep learning era what do we need
728:08 - to do is to find the set of weights
728:11 - which is W1 W2 W3 W4 and all the way
728:15 - around to the B um and B to get if we
728:19 - have good weights we'll be we'll be able
728:20 - to get good output and if you have bad
728:22 - weights we'll be able to get the bad
728:24 - output so you may be thinking hey you
728:27 - how we are going to get this weights if
728:30 - uh so so basically we initialize the
728:32 - weights so before training our neuron or
728:36 - before before training our neuron or
728:38 - before learning these weights what
728:40 - exactly we do what what exactly is we do
728:44 - we initialize all our weights so
728:46 - basically we initialize W1 to be zero W2
728:49 - to be0 W3 to be0 W4 to be0 and B to be0
728:53 - okay and then we start training it
728:55 - because now we have the weights now you
728:57 - now your model will performing very bad
728:58 - because everything is zero so you'll get
729:00 - your final output to be zero okay and
729:02 - that is very bad right so so what you do
729:06 - you take out the derivative because you
729:08 - need to find the local sorry Global
729:10 - Minima but in this case it will be able
729:12 - to find the local Minima so you need to
729:14 - find the Minima of that function uh of
729:17 - that function loss okay of of of of the
729:20 - loss so what you need to do you need to
729:22 - take out the derivative of L with
729:25 - respect to W1 so basically what is
729:27 - saying how much the loss changes or J of
729:31 - theta changes when W1 changes when you
729:35 - weight changes okay so basically it's
729:38 - just it's just a pretty much easy
729:39 - version like how much how much your loss
729:42 - or how much your error changes when you
729:44 - change W1 a little bit if it decreases
729:47 - if the if the loss changes when one when
729:50 - W1 is 0.2 then you update that way W1
729:54 - okay so that's why we take out the
729:56 - derivative because it tells lot of
729:57 - things to us which direction to move
729:59 - Theta in and uh yeah so and and and it
730:03 - all tells us the slope andc etc etc okay
730:06 - so basically so that's why we take a
730:08 - look at we in the previous video we have
730:10 - seen that why grad desent use derivative
730:13 - please see that so over here we get our
730:15 - output now what we need to do is to take
730:18 - out the derivative of L with respect to
730:21 - W1 okay that tells how much l changes
730:25 - when W1 changes a little bit so let's
730:28 - let's go ahead and so over here you need
730:31 - to take a losses there and then you and
730:33 - there is a path a full path okay so
730:36 - basically this is uh here we'll use the
730:38 - multivariable chain rule of calculus so
730:40 - for taking out the D do L by do W1 which
730:44 - is partial derivative of L with respect
730:47 - to W1 what do you do first of all
730:51 - partial derivative of L with respect to
730:55 - sigo of Z okay the sigo of Z because we
730:58 - can't directly go to we we can't
731:00 - directly take out the derivative of L
731:02 - with respect to W1 we have to go through
731:04 - the path so over here we'll use the
731:07 - chain rule of differentiation
731:08 - multivariable chain rule of
731:09 - differentiation what exactly we are
731:11 - doing this computational graph is
731:14 - derivative of L with respect to that
731:17 - Sigmar function which is um Sigma of Z
731:20 - okay and the sigment of Z is 1/ 1 + e e
731:23 - the^ minus Z okay so you take out the
731:26 - derivative of L with respect to Y hat
731:28 - because that's a y hat not that not the
731:32 - it is basically this basically this um
731:35 - this particular sige of Z is nothing but
731:38 - your prediction what you're getting as
731:40 - an output right so you're taking the
731:42 - derivative of L with respect to Y hat
731:45 - equivalent it does not matter and then
731:47 - you take the derivative of this um Sigma
731:50 - function with respect to this one or or
731:55 - or you can say with respect to this one
731:56 - and with respect to this one but it does
731:58 - not need need to be because this is your
732:00 - one neuron this is your one neuron which
732:02 - is okay you take out the derivative and
732:04 - then you can simply go to w one okay
732:06 - without even going here just just have
732:08 - marked a line to make you understand but
732:10 - this whole process takes in one one line
732:13 - Okay cool so uh so here we have you so
732:16 - we'll simply Dera of L with respect to
732:20 - this neuron and then using this the
732:22 - derivative of this neuron with respect
732:24 - to W1 okay
732:27 - cool and then for taking out the
732:29 - derivative of um now over here you the
732:32 - derivative of sigar function with
732:34 - respect to W1 which which are over here
732:36 - so now let's go ahead and take out the
732:39 - the mathematically about let's solve
732:42 - this particular problem so we'll be
732:43 - getting our D derivative okay so let's
732:45 - solve
732:48 - it so let's get ahead so we want to take
732:52 - out a derivative of L with respect to W1
732:55 - the first parameter so we need to choose
732:57 - because we need to find W1 W2 W3 and W4
733:01 - okay these four parameters to be good
733:03 - parameter okay and then we'll getting
733:05 - our good output so basically what you do
733:08 - derivative L using the chain rule of
733:10 - calculus okay so so let's take out the
733:13 - first of all let's take out the
733:14 - derivative the derivative of loss with
733:18 - respect to Y hat okay with respect to Y
733:21 - hat or prediction okay so basically your
733:23 - log loss is nothing but minus I'm
733:26 - writing the negative log likelihood y
733:30 - log of H of X which is your prediction
733:34 - or we can say the sigma of X which is a
733:36 - prediction plus 1 - y log of 1 minus
733:43 - sigmoid of Z okay so basically you have
733:47 - this and then when you take out the
733:49 - derivative of it when take out the
733:51 - derivative of L with respect to uh this
733:54 - Z so Sig of Z so basically what you'll
733:58 - do so just for your information the
734:00 - derivative of of log X with respect to x
734:06 - with respect to X is nothing but 1 / X
734:09 - so that's your derivative
734:11 - 1 /x so you'll you you'll be left with -
734:14 - y Sigma of Z + 1 - y 1 - Sig of Z when
734:19 - you take out the derivative of this if
734:21 - you don't know calculus don't worry
734:23 - about it um you can just think about
734:26 - okay when we take out the D derivative
734:28 - you'll get this calculation or you can
734:29 - learn learn calculus or see on some
734:32 - YouTube videos to understand how it came
734:34 - but basically is very very easy to
734:36 - understand first of all the derivative
734:38 - this is a constant this is uh this is a
734:40 - very varable uh this is uh your your um
734:43 - so when you take out the what do you say
734:46 - 1 over uh this that you'll be left with
734:50 - the derivative of log X so that will be
734:53 - this and plus 1 + 1 - Y and then you
734:58 - simply add 1 by 1 - Z okay so that's it
735:02 - that that exactly how the how we take
735:05 - all the derivatives now let's go further
735:07 - and uh and now now we now we have had
735:11 - found we had found this one which is
735:13 - this one we have we we done with this
735:14 - one now we need to find the this one so
735:16 - basically we're taking out the
735:18 - derivative of Sigma of Z with respect to
735:20 - W1 so Sigma of Z Sigma of Z which is the
735:23 - function 1 / 1 + e^ minus Z so that's
735:27 - your function which is a sigo function
735:29 - so you need to take out the derivative
735:31 - of that with respect to this Z with
735:33 - respect to Z because we can't directly
735:35 - go to W1 but with respect to Z and then
735:38 - and then and then we'll be having this
735:40 - this one so from here to here and using
735:44 - this Z we'll get our W one okay so
735:47 - basically how you take out so 1 + 1 1 +
735:50 - e^ minus
735:52 - Z to the to the^ minus 1 because we had
735:55 - converted to this fractional form to
735:58 - aine form so you can t take take take a
736:00 - look like this and then you use the
736:02 - power rule of differentiation so what
736:04 - you did is to transfer this over here
736:07 - transfer this over here so let me just
736:10 - how I come up come come up with this you
736:13 - can simply feel free to um ask me over
736:17 - comments or see any online video because
736:19 - I assume that you already know these uh
736:22 - what do you say Sigma function or or or
736:25 - the derivatives uh
736:27 - calculus don't worry we'll derive this
736:30 - in our next lecture now you have taken
736:33 - out all the now we have now we have
736:36 - taken out all the derivative of L with
736:38 - respect to we can simply multiply with
736:40 - this derivative of Z is this and
736:42 - derivative of Sigma Z with respect to is
736:44 - this one and how this X1 came how how
736:46 - this x X1 came so because the derivative
736:49 - of Z with respect to W1 and Z was
736:52 - nothing but W1 * X1 * X1 okay uh we just
736:57 - assume that W1 * X1 over here over when
737:01 - you take out so over here we need so
737:03 - you'll be left
737:05 - with X
737:07 - +0 okay so you you're left with W1
737:11 - uh sorry X1 X now let's go further we so
737:15 - that's why we got X1 and then you got
737:16 - the derivatives of respect to one but if
737:20 - you have in a rest don't worry about it
737:22 - pretty much easy we'll again re re
737:24 - revisit the same thing but we'll do the
737:26 - all the calculations by ourself so that
737:29 - you get a very good understanding of it
737:31 - but don't worry if if you haven't
737:32 - understood is just for those who knows
737:35 - calculus for
737:37 - those for those who knows calculus
737:41 - who knows calculus okay to to take out
737:44 - an Al who who are who are good in
737:52 - mathematics so we have t so now we have
737:55 - seen our so now we'll see some of the
737:57 - activation functions so now we'll see
738:00 - some of the activation functions and
738:02 - then take out the derivative of those
738:03 - activation function okay so this first
738:06 - activation function which you have
738:07 - already seen is the logistic function
738:10 - the main main reason why we use Sigma
738:12 - function is because it exist it it it
738:14 - makes that the output of Z into a range
738:18 - of 0 to 1 okay so when you take out the
738:22 - derivatives which you're seeing in front
738:24 - of you we had already taken out by
738:27 - derivative of Z with respect to uh Sigma
738:29 - of Z with respect to Z okay cool uh
738:33 - let's go further so we have another
738:35 - activation function which is the tan
738:38 - activation function the hyperbolic
738:40 - tangent activation function is often
738:43 - referred as a Tage activation function
738:45 - it is very similar to activation
738:48 - function it's just similar to activation
738:49 - function but rather than making that Z
738:52 - to in the range of 0 to one it makes in
738:55 - the range of minus 1 to 1 it range in
738:58 - the range it it makes in the range of
739:00 - minus one to 1 rather than making it to
739:02 - the 0 to one so that's why we Define as
739:04 - a tan activation function what is the
739:07 - function formula the formula for that e^
739:09 - - x e ^ - x / e^ x + e^ - x so you just
739:16 - instead of Z over here it's X so you can
739:19 - just put the x value or input value to
739:22 - convert into range of Z sorry minus one
739:24 - to
739:25 - 1 when you take up the Dera it's a bit
739:28 - hard and I don't want you to get into
739:31 - this because it's kind of you just need
739:32 - need need to know about this particular
739:35 - stuff you don't need to you you need to
739:37 - know about the derivative but if you are
739:39 - seeing over here the Der ative of that
739:41 - e^ minus Z is nothing but 1 - t h squ
739:46 - okay so all the derivatives which be
739:48 - here we are using the chain rule
739:49 - differentiation exponential uh
739:52 - derivative Etc to get it to 1 minus t z
739:56 - ² which is the derivative so why why I'm
739:58 - telling you the derivative because we'll
740:00 - use it in our later CL later lectures so
740:02 - that's why if if you're not not getting
740:04 - it why we are why we are using it please
740:07 - be sure to uh please be sure to complete
740:09 - the whole class class and then you'll be
740:11 - understanding why do we need to use
740:13 - these so then the next activation
740:15 - function is widely used activation
740:17 - function Rel activation function which
740:19 - is a rectified linear unit the main
740:22 - advantage of this reu activation
740:25 - function over another activation
740:26 - function it does not activate all the
740:28 - neurons at at the same time so basically
740:32 - if if what it does it takes the max of
740:35 - zero and Z okay assume that your Z your
740:39 - input value Z is 1 so when you take the
740:43 - max of 0 and 1 what it will be answer
740:46 - will be zero so so that particular
740:48 - neuron is not activated because in
740:49 - neuron we have a particular Z and then
740:51 - you have some kind of activation
740:52 - function so Z is let's say one and when
740:55 - you apply this Max then the neuron is
740:57 - not activated okay at the same time
740:59 - because it help it really helps uh and
741:02 - and and and the special property of all
741:04 - the activation function reu activation
741:06 - function whether whether it be Tage
741:08 - activation function or whether it be
741:10 - what do you say Sig mode activation
741:13 - function all these um functions are
741:16 - nonlinear in nature and is
741:19 - differentiable so all are differentiable
741:21 - so that's why we are able to take out
741:22 - the derivative but what do you think
741:24 - about the derivative of this so that so
741:26 - we have the equation Max of 0 comma Z so
741:29 - the derivative of activation function is
741:33 - one if x is greater than zero and zero
741:36 - if x is smaller than zero okay so so so
741:38 - the the derivative is this uh which we
741:41 - can represent through this diagram cool
741:44 - so we'll discuss about it advantages and
741:46 - disadvantages later in the course and we
741:49 - continuously revisiting these activation
741:51 - functions a lot to help us better
741:53 - understand and we'll be introducing
741:55 - later activation sorry extensions of
741:58 - some activation functions to overcome
742:00 - their advantages so if you don't know
742:01 - about any of these things and if you
742:03 - didn't got it really don't worry
742:06 - directly move to the next lecture you'll
742:07 - be understanding everything because
742:09 - there I have gone very slow in
742:11 - understanding step bystep gradient an
742:14 - example so here's an recap so exactly
742:17 - perception learns is by changing the
742:19 - ways and here the path you follow from
742:21 - going to L to the W1 by first of all the
742:25 - Der partial derivative of L with respect
742:27 - to that Sigma of Z and then partial
742:30 - derivative of Sigma is Sigma of Z going
742:32 - to Sigma of Z and then partial Dera of s
742:36 - of going to W1 okay and this is the same
742:39 - for every particular what do you say um
742:44 - the every particular feature over here
742:47 - okay so I hope that's that's that's
742:49 - pretty much clear to you and I hope that
742:51 - you are able to understand everything
742:52 - out here in the next lecture we'll start
742:54 - off with multier perceptor and I hope
742:56 - you'll enjoy that let's go on next video
742:58 - hey everyone welcome to this lecture on
743:00 - multi-layer neural network in our course
743:03 - mlo2 and I hope that you are really
743:05 - liking this course so let's get started
743:08 - with uh multi-layer uh neural Network so
743:11 - what is multi-layer neural network so in
743:14 - in perceptron which we have seen we have
743:17 - a particular unit I would say we have
743:21 - particular unit like this and and and in
743:25 - multi-layer percepton we include multi
743:29 - uh lots of perceptron and stack them
743:32 - together okay so let's take one
743:35 - example so taking one example which
743:38 - you're seeing that we have a particular
743:40 - so so which which we have seen in
743:42 - previous videos we have several inputs
743:45 - several inputs and we give to a
743:47 - particular Neutron and then we get our
743:49 - output y okay so what we will do what we
743:53 - will do is have mulp on perceptron like
743:58 - this say for example we have information
744:01 - X1 X2 and X3 we give to the to this
744:05 - particular neuron and then the output
744:08 - the output from this neuron
744:11 - out output from this neuron is given to
744:13 - another neuron and then we get our
744:15 - output right so this is a multi-layer
744:18 - perceptron this is a multi-layer
744:20 - perceptron where we have a multiple
744:22 - perceptron or or more than one
744:24 - computational lab okay in perceptron you
744:28 - have you were having an input layer and
744:30 - an output layer right so you have a
744:32 - input layer which is X1 all the way down
744:35 - to the X4 and then you were you you were
744:39 - having the out output like this right so
744:42 - yeah so you're having the neuron and
744:44 - then you're getting output so from this
744:46 - particular neuron you are getting the
744:47 - output so this was called as an output
744:49 - layer right output layer and because we
744:52 - here we are having the because from this
744:54 - neuron we are getting the output so
744:55 - that's why we call this as a output
744:56 - layer where all computation was
744:58 - happening in the only in output layer
745:01 - right so all the computations like the
745:03 - aggregation of the information like this
745:05 - I = to 1 all the way around to the J XJ
745:08 - and WJ and then we have apply the
745:10 - nonlinear activation function over here
745:12 - we are applying the sigmod activation
745:14 - function like this and then we are and
745:16 - then we were getting our output right
745:18 - that is that all the all the all the
745:20 - computation was happening in this neuron
745:22 - now over here in in multier perceptron
745:26 - we our input is passed through several
745:28 - layers so we have our input so we have
745:31 - our input X1 X2 and X3 it is passed to
745:36 - one neuron one neuron and then the
745:38 - output from this neuron the out put
745:40 - after doing the computation and
745:42 - computation what it does first of all
745:44 - the aggregation of I = to 1 all the
745:46 - around J XJ and WJ and then the
745:49 - pre-processing okay or the activation
745:51 - function so this processing is same okay
745:53 - so we app do the all the computation and
745:55 - then the output from this is passed to
745:58 - another neuron right and then it
746:00 - performs aggregation and the processing
746:02 - then passed to another neuron that we
746:04 - have to specify now you may ask me ask
746:07 - me a question here
746:09 - use and how many number of layers which
746:11 - we have so over here in this example and
746:13 - then we get our output so here in this
746:15 - example we have total of three layers
746:18 - you have total of three layers um which
746:20 - is called the uh these two are called
746:22 - The Hidden layer and then we have them
746:25 - then then we have an output lay so let's
746:27 - discuss about the terminologies which we
746:30 - will eventually use a lot in this say
746:32 - for an
746:33 - example you you have this example and
746:36 - then you have an input layer you have
746:38 - input layer and then you have a weight
746:40 - for for with respect to every input and
746:42 - then we have one bias right and then we
746:43 - give to the and then we give to the one
746:46 - neuron and then the output from this
746:48 - neuron is carried by some weights with
746:51 - the with the information from the
746:52 - previous neuron so this this weights is
746:54 - need to be learned because because over
746:57 - here we need to learn these weights as
746:59 - taught in the previous video we need to
747:00 - learn these weights so basically the
747:03 - information from the from this neuron is
747:06 - carried by some weights which we denote
747:09 - by w we we talk about what this 21 means
747:11 - later on but over here the E information
747:14 - is carried by the weights right and then
747:16 - and then the it is passed through in
747:18 - over here now what what will the
747:20 - computation the computation in this this
747:23 - neuron will be w21 times times uh the
747:28 - h11 okay the information which we got
747:31 - from here right and plus the bias term
747:34 - plus the bias term okay and then we get
747:36 - our output then then we get our output
747:38 - which is the which is our y so over here
747:41 - we have a total of two uh layer neural
747:44 - two two layer so we in this particular
747:47 - example we have two layer Network and
747:51 - here we have one hidden layer we have
747:54 - one hidden layer the reason why we call
747:56 - this because we are not see the user is
747:59 - not seeing this right what the the
748:01 - hidden layers that's why KOB we call
748:03 - this as a hidden layer but we are seeing
748:05 - the output so that's why this output
748:07 - layer this is called output layer we do
748:09 - not count our input layer because
748:11 - eventually we are not doing any
748:12 - computation in input layer but in in in
748:15 - Hidden layer and output layer we are
748:16 - doing computation right so that is our
748:19 - favorite output layer and hidden layer
748:21 - so again I'm again I'm repeating the
748:24 - through several layers which is not
748:26 - visible to the user that's what we call
748:29 - that call it as a hidden layer right so
748:32 - that was pretty much easy to understand
748:34 - it in much better way now let's talk
748:37 - about uh we will talk about these termin
748:40 - ologies to or notational arguments what
748:43 - exactly this 14 and 15 means and what
748:46 - this just relates to what this relates
748:48 - to and what this relates to and all the
748:50 - stuff but over here this was an
748:52 - introductory kind of a slide for making
748:55 - you understand about multi-layer
748:56 - perceptron like how we just extend the
748:59 - idea of perceptron to multi-layer
749:01 - perceptron right so let's go on the next
749:03 - slide to to to understand much better
749:06 - better way let's go over here so let's
749:09 - take a let's let's take one example over
749:11 - here but before that I want to make you
749:13 - familiar with ter logies so let me just
749:17 - show it to you so here it is so over
749:19 - here you have an input layer then you
749:22 - have one hidden layer the layers which
749:24 - is not visible of course the and this is
749:27 - called the output layer the L which
749:30 - which is the output layer right so
749:32 - information is carried by some weights
749:34 - so information is carried by the weights
749:36 - W W1 W2 W1 W3 and all the way
749:40 - did bias and then we have four input
749:42 - layer and then we have an layer and then
749:44 - we have an output layer right so so the
749:47 - the notation which you're following to
749:49 - be very much consistent is we have a w
749:52 - to denote a weight and then we seeing
749:55 - lay number lay number and weight number
749:59 - okay so this this this this weight this
750:03 - weight is of which lay it's for layer
750:07 - number one so we write layer number one
750:10 - and what is the what is the number what
750:12 - is the weight number is the first weight
750:15 - right so we write as a land number and
750:17 - the uh weight number so that we have lay
750:21 - number one because it's going in layer
750:23 - number one and then we have a second
750:25 - second weight first layer third weight
750:28 - first layer fifth weight right so over
750:31 - here we'll change the argument but for
750:33 - being precise I'm doing like five we
750:36 - have fifth uh weight which we need to
750:38 - learn but a but as we go further we'll
750:41 - slightly change our notations to help
750:44 - you much more better on more concrete or
750:47 - industri stand wise right so just for
750:49 - information you can assume that a 15
750:52 - okay and this is layer number this is
750:55 - layer number which is going for layer
750:57 - number two and this is the weight number
751:00 - there is only one weight since since
751:01 - there is only one weight so we have to
751:04 - one right and over here you have an
751:07 - hidden layer so we denote by at which
751:10 - denotes the hidden layer and then this
751:12 - is your unit number this is your unit
751:15 - number and this is your layer number
751:17 - okay so this is a first hidden layer so
751:19 - we denote by one and this is a one there
751:22 - is one perceptron layer we will see that
751:25 - we will having a several perceptron like
751:28 - this and each each denotes a unit right
751:31 - so over here we have the first unit
751:34 - right so we have the first unit okay and
751:38 - then over here we have an output layer
751:40 - which is denote by o and then we have
751:43 - and then this is this is this is nothing
751:45 - but your layer number or output number
751:47 - you can understand in this way so sorry
751:49 - that yeah that is an output number or we
751:51 - can think of in different way that that
751:53 - is an output number and this is your
751:56 - layer number okay but we can slightly
751:59 - change it to our
752:00 - formation is like
752:03 - this I think uh we can slightly do the
752:05 - changes like this where this denotes the
752:08 - unit number where this denotes I think
752:11 - there's a typo it should be one over
752:13 - here and there it should be two over
752:15 - here okay that denotes the unit number
752:18 - and that denotes the layer number to be
752:21 - consistent over here okay I hope that
752:24 - that's pretty much clear about the
752:25 - notations so let's go on our example
752:28 - which we have seen previously so let's
752:30 - go on examples so here we have a very
752:34 - good example to understand a much better
752:35 - way so diabetes prediction system given
752:39 - age height and BMI okay so you have
752:44 - three information or three input value
752:46 - and there is one bias term where we
752:48 - always set x0 = to 1 okay and then here
752:52 - we take an example then let's take an
752:54 - example AG is 4.5 understand this way H
752:59 - can be 4.5 years old and then height is
753:03 - 5 cm or and BMI is six and then x0 is
753:08 - obviously one by the convention so we
753:10 - taken a default value for this example
753:12 - though no one need to understand how
753:14 - this example came it's just a random
753:16 - example which I've taken now I have
753:18 - given the random weights don't worry how
753:20 - these weights come we'll learn how to
753:21 - get good weights just for just a random
753:24 - stuff right just to make sure that is
753:26 - you're clear about the process which is
753:27 - going on so over here over here you have
753:31 - weight 1 one which is 0 0.1 we had just
753:34 - defaulted this okay so we have w11
753:37 - weight 1 weight 2 weight 3 weight four
753:39 - and then we have one bias term like this
753:41 - okay which is randomly initialized now
753:43 - we go to the first hidden layer in the
753:45 - first hidden layer we have one Z and
753:48 - then we have an H so what do Z does Z
753:51 - Aggregates the information right so it
753:53 - just multiplies the in the weights with
753:55 - the input values so over here if you
753:58 - multiply 0.1 times your uh your input
754:02 - feature 4.5 plus the 0.2 * 5 0.3 * 6 0.4
754:09 - * 1 okay so that is 0.4 so sorry 0.4
754:13 - times uh I think I've done did wrong
754:16 - over here I think this should be removed
754:18 - oh my God this should be removed uh
754:21 - please ignore this and then we have an
754:23 - bium please ignore this
754:25 - okay uh and then we have an h11 which is
754:28 - over here 1 1 by 1 1 + e^ minus Z
754:32 - because here we are applying the sigma
754:33 - activation function which squeezes your
754:35 - output into the given range right so
754:38 - over here we have 0.9
754:40 - 9 okay so you get your uh the output
754:43 - from the first hidden layer and this is
754:45 - your output from the first hidden layer
754:47 - okay and then we give to the let me just
754:50 - uh do this little bit concrete it should
754:52 - be 0 2 and 1 the output layer now you
754:57 - simply have initialize a random bait 0.3
755:00 - times the information from the previous
755:03 - layer and then we add the bias term and
755:05 - the bias term which you have taken over
755:06 - here is 0.2 as a randomly as a randomly
755:09 - the bias term is 0.2 and then we get our
755:12 - output and then we apply the activation
755:14 - function which is 0.62 and then we get
755:17 - our output now if this is a diabetic
755:19 - prediction system we can have a rule
755:22 - which is we can the our prediction is
755:25 - zero the person is not having a diabetes
755:27 - if the predi if the probability is
755:29 - smaller than 0.5 okay and over here it
755:32 - is greater than 0.5 so it is nothing but
755:35 - one the person is having a diabetes with
755:38 - these information
755:40 - okay however don't just think this is
755:43 - wrong right uh means this is just I've
755:46 - taken one example a very random example
755:49 - a very abstract example a very imaginary
755:52 - example okay don't worry this is for
755:54 - actual case this is just to show you how
755:57 - we are going forward in our network with
756:00 - multier perception so how we are going
756:03 - how we are doing how how we are going
756:05 - forward in our Network so we call this
756:07 - as a forward
756:10 - forward propagation we call this as a
756:13 - forward propagation which we call
756:16 - because we are we are propagating
756:18 - forward in our multi-layer perceptron
756:22 - right so that is our uh that that is our
756:24 - just an example to help you can to help
756:27 - you better understand these
756:31 - things cool so we have talked about
756:33 - rotations so now we can do for
756:37 - multiclass classific prediction as well
756:40 - with with lots of hidden layers in
756:42 - between like this okay so over here so
756:45 - I'll just make you understand much more
756:47 - detail so over here you have a example
756:50 - of multiclass classification prediction
756:53 - you're given the color size and the and
756:57 - the size of the bird so you're given the
756:59 - color of a bird and then you're given
757:01 - the size of the bird you need to predict
757:05 - you need to
757:06 - predict you and and you're given the
757:09 - sound of a bird right so so so you're
757:11 - given the color of a bird size of a bird
757:15 - and sound of a bird you need to predict
757:18 - uh whether that bird is a sparrow
757:22 - whether that bird Sparrow or parrot okay
757:25 - given these three information you need
757:26 - to predict whether that bird is a
757:29 - sparrow or parrot okay so you're given
757:33 - these three information every
757:34 - information is carried by those
757:36 - respective weights with with one bias St
757:39 - right so so over here the number of
757:42 - Weights which you will get and and so
757:44 - you have a one you have of of over here
757:48 - over here you have one hidden layer you
757:51 - have second hidden layer you have third
757:54 - hidden layer okay and then we can
757:56 - increase and then we can increase lots
757:57 - of hidden layers that's that's that's
757:59 - what we have to choose now you may ask
758:01 - one question H is there any way we can
758:04 - have we can def we can have the number
758:07 - of layers defined like how are how how
758:10 - are we going to choose the number of
758:11 - layers which we want right the number of
758:13 - hidden layers which we want so this is a
758:15 - total very active area of research that
758:18 - there is there is no fixed number of
758:20 - hidden layers which we can use to get
758:21 - optimal results but it is just activate
758:24 - your research we just tune our we just
758:27 - check with two layers it works or not we
758:29 - just tune our hyperparameters like this
758:31 - to get our output right to do to analyze
758:34 - which number of head layers are working
758:37 - well right so here you have the first
758:41 - here and there you simply do the
758:43 - information aggregation and the pro
758:45 - processing step right so act activation
758:47 - step so over here you have a total of so
758:51 - the the the so the the size of an input
758:54 - layer the size of the the size of a
758:57 - input layer is n n n * 3 the the the
759:01 - number of weights at uh at this hidden
759:04 - layer is 4 * 1 because there are four
759:08 - weights times the the the there is only
759:11 - one uh unit because further we'll see
759:13 - we'll be having several units so that's
759:15 - why that is your four * 1 which is the
759:18 - number of Weights okay and the
759:19 - information is carried by the the the
759:21 - output from the edge11 is uh at
759:24 - subscript one to the power superscript
759:27 - one is carried by the weight w subscript
759:31 - 2 one and over here you have un biased
759:35 - term as well so you have a 2 * 1
759:43 - so over here you have w31 and then it is
759:46 - carried by and then the output from this
759:48 - Le is carried by some weights with one
759:50 - bias term like this now over here we
759:53 - need two outputs first of all for
759:56 - Sparrow and second for parrot okay so
760:00 - every so every so we we we every every
760:04 - uh o output is carried by some weights
760:06 - or the information is carried by
760:08 - different different weights for
760:08 - different different units right and then
760:11 - we get our particular output the maximum
760:13 - so we say let's assume that that this is
760:15 - for a sparrow and this is for parot and
760:18 - this is for 3 * 2 over here we have
760:21 - three weights for the for the sparrow
760:24 - for the for the parot and the bias okay
760:27 - and the information scried by the the
760:29 - the information the weights for the the
760:32 - the information got or the output got
760:34 - from this hidden layer is carried by
760:37 - three three weights the first weight is
760:39 - for the sparrow second is for the parrot
760:42 - third is the bias term okay and then we
760:45 - have a to Total two units over here
760:47 - right so there are a total of six
760:48 - weights there total of six weights over
760:52 - here okay so I think there there there
760:55 - there's a typo have total of three
760:58 - weights over here but but we eventually
761:00 - don't count it right so over here so
761:03 - over here you take out the maximum of Pi
761:06 - right so you take out the maximum of of
761:09 - the probability of 012 so so let's let's
761:13 - let's assume we get 0.7 and this is 0.4
761:17 - so what is the maximum 0.7 so we
761:19 - classify given these three information
761:21 - our uh uh the the bird is a sparrow okay
761:26 - so that is your with multiple uh hidden
761:28 - layers as well as with multiple
761:32 - output so now we'll see one hidden layer
761:35 - neural network now what we will do is to
761:39 - to stack different perceptron in a
761:41 - particular layer for getting our output
761:44 - right so over here assume that you have
761:47 - two inputs
761:49 - so and just make you familiar with it so
761:53 - you have two inputs X1 and X2 okay and
761:56 - then you have a the one unit second unit
762:00 - third unit and fourth unit okay so that
762:02 - is your first neuron second neuron third
762:05 - neuron and fourth neuron and then we are
762:08 - having the output layer so the this is a
762:09 - one her and layer neuron l so over here
762:12 - let's go with the so so now your
762:15 - information
762:16 - X2 is scattered by weights like w21 for
762:20 - the first neuron for the first neuron in
762:23 - layer number one it's scattered by the
762:24 - weights right and these weights are need
762:26 - to be learned right weights are the
762:28 - thing which we need to learn right which
762:30 - which we use optimization algorithms to
762:32 - learn these which we which we'll do in
762:34 - our couple of next videos to understand
762:36 - how we learn these weits okay so X2
762:39 - information X2 is carried by the W2
762:42 - carried by different different weights
762:43 - for different different neuron say for
762:45 - example in this the W2 is the the the
762:48 - weight for H1 by this X2 is carried by
762:51 - this the for the X2 the information is
762:54 - carried by this weight for this
762:55 - particular neuron and the information of
762:58 - this weight the information of this is
763:01 - carried by this weight for this neuron
763:03 - and for this
763:05 - okay and then we get our output like
763:07 - this and then uh the out outut from this
763:10 - is carried by this the output from this
763:12 - is carried by this way the output from
763:13 - this is carried by this way the output
763:14 - from this is carried by this way and
763:16 - then we do our formal uh in in every
763:19 - step we do our formal uh what say Pro
763:24 - multiplication or aggregation and
763:26 - activation stuff right so in every
763:28 - neuron this is this is what is happening
763:30 - okay these two these two steps which is
763:32 - happening in a particular neuron and
763:35 - assuming currently I'm neglecting so I'm
763:38 - I'm and over here for Simplicity for
763:43 - Simplicity
763:44 - neglecting
763:46 - neglecting a biasm neglecting bias okay
763:50 - so I'm currently neglecting biasm over
763:53 - here so that is for the second now let's
763:56 - let's go further to understand this this
763:59 - this way so for this for the first we
764:03 - have the first information scattered by
764:05 - this way for this particular neuron and
764:07 - then for f for for this we have this
764:08 - particular particular neuron and for
764:10 - this this information scattered by this
764:11 - weight for this particular neuron and
764:13 - for this information scattered by this
764:14 - weight for this particular
764:17 - neuron okay so you may have and then we
764:20 - do the formal process to get our
764:26 - output so over here if you combine those
764:29 - things this is how it looks like this
764:31 - this is how it look like so let me just
764:33 - show it to you so you have W1 1 and 1 w
764:39 - 1 2 superscript 1 W2 so sorry so sorry
764:45 - W13 right 1 and w14 1 okay so this is
764:53 - this indicates the layer number this
764:56 - indicates the L number this indicates
765:00 - from where it is coming right so from
765:02 - which from where it is coming the unit
765:04 - one the input input unit one or input
765:07 - unit 2 so it is coming from input unit
765:09 - one that that indicates the input unit
765:12 - okay input unit input unit right and uh
765:17 - uh this is the weight number the weight
765:20 - index so this is the first weight right
765:23 - so over here weight index so the same
765:25 - goes with the the the the the input
765:27 - number so let me just change the color
765:30 - so that it makes sense and W2 is is your
765:32 - weight index and this is your land
765:35 - number same goes with this same goes
765:38 - with this this and then goes with this
765:41 - okay so this is how the notation is
765:43 - working for this particular example we
765:45 - have
765:46 - w21 and then we have
765:50 - w22
765:52 - w23 and
765:54 - w24 right so you have a w21 so over here
765:58 - this is the input unit 2 this is the
766:01 - because X2 and this is the the the the
766:04 - the the weight index and this is your
766:06 - land number right so this is how we
766:08 - denote the weights over here and if you
766:11 - see this example this has changed the
766:13 - the this one this one over here
766:16 - w11 to this indicates your uh the the
766:21 - the hidden unit right hidden unit and
766:24 - this indicates your weight index and
766:26 - this indicates your layer number so that
766:30 - is your notation arguments now in every
766:34 - particular um uh what is happening in
766:37 - every in every uh this neuron what is
766:40 - happening so I've done a very good uh
766:42 - very good analysis over here so I'll
766:44 - just tell you that what exactly is
766:46 - happening in every neuron or the unit
766:49 - unit hidden units okay so first of all I
766:53 - have given different different names so
766:54 - for so for aggregation steps aggregation
766:58 - steps aggregation steps I'm using Z okay
767:03 - and for post active post the the post
767:05 - activation I'm using H as our final
767:07 - output right so so over here so let's
767:10 - assume for this let's assume for this
767:13 - what is what is happening over here so
767:15 - basically this is this this indicates
767:18 - the the hidden unit the hidden unit and
767:21 - this indicates the layer number the
767:23 - layer number okay so for the first unit
767:26 - and in the first hidden layer we are
767:29 - multiplying the weights of the the first
767:33 - w11 this weight okay and then we
767:36 - multiplying for the first information
767:38 - and the second information right so we
767:40 - are not just uh because uh the first
767:43 - information and the second information
767:44 - is carried by different different
767:46 - weights and then given to this neuron
767:48 - okay because we have several neurons so
767:49 - for the the the two neurons is given to
767:52 - this the two input values is given to
767:53 - this neuron right okay and then we add
767:56 - the bias over here and then this is what
767:59 - the aggregation step means and then we
768:01 - apply the re activation function value
768:03 - activation function on the on on on the
768:05 - output which we
768:06 - get and then over here what what is
768:09 - happening in this it we are simply
768:12 - multiplying the weight from the for the
768:15 - the information X1 is carried by the
768:17 - weights with to the H the hidden unit
768:20 - number two same goes with the
768:22 - information number two is carried by
768:24 - some weights to the to the second second
768:27 - hidden unit okay and then we add the
768:29 - bias and then we get our
768:31 - output same goes with j3 j3 is our this
768:35 - this unit right so over here this is
768:38 - your pre preactivation preactivation
768:41 - preactivation in preactivation the
768:43 - information is aggregated okay with the
768:45 - respective weights and then applying the
768:47 - r for Z4 we are for this one we are
768:52 - doing the pre aggregation step and then
768:54 - applying
768:56 - value okay and then we have and then we
768:58 - get our output which is simply which
769:01 - simply multiplying the multiplying the
769:04 - weight with the output which you get
769:06 - from the particular unit
769:10 - this is also the out this is the weight
769:11 - which is carried by the information by
769:13 - the edge2 this is the weight the this
769:16 - one is scattered by the weight the
769:17 - inform the the the the information is
769:19 - scattered by this weight the information
769:21 - of this scattered by this weight and
769:23 - this is the B and then we get our output
769:25 - and then we apply some kind of
769:27 - activation function over here like reu
769:29 - or sigmoid or tan whatever we apply over
769:32 - here and then we get our
769:35 - output right so I hope that makes sense
769:38 - let's go further so if we can write in
769:41 - vectorial format to have it much more
769:43 - better way so I'll just see so we'll
769:46 - cover till uh different Laya
769:49 - so let's cover over here and then we'll
769:52 - uh go from the next video Let's cover
769:54 - the vectorial format and then we'll uh
769:56 - cover the rest in the next videos so we
769:59 - can write in this way we can write in
770:02 - vectorial format so we can have our big
770:05 - Z we can have a we can have a uh a
770:09 - matrix or or a vector which is z okay
770:13 - and we can store all our weights in a
770:15 - matrix and in
770:18 - know for this particular example we can
770:20 - store this uh our our weights in a
770:23 - particular Matrix so we store this we
770:26 - store for
770:28 - the first uh we store for the uh first
770:31 - information we store this weights and
770:33 - for the second for this for for the
770:35 - weights for this information and the
770:37 - weights for this information as stored
770:38 - like this okay where we have for this
770:41 - for this which are for first first
770:43 - information weights these are second
770:45 - information weights or the second put
770:47 - weights okay I store them into a
770:50 - particular weight which is weight Matrix
770:54 - one okay for this indicates the L number
770:57 - now we have now we make for the second
770:59 - which is which is St as a as as as a
771:02 - column Vector is 112 which indicates
771:05 - which which you can see over here which
771:06 - is weight number two and this weight
771:09 - number two contains the weights for the
771:11 - particular for for every um or or or you
771:15 - would say like this okay and we can
771:18 - write in this way we can write in this
771:20 - way uh in the vectorial format and then
771:22 - we can simply uh take out Z okay so what
771:26 - you can do you can uh simply multiply
771:28 - the weights with the with with the with
771:31 - the column Vector which is X1 and X2 we
771:34 - can put that into a vector as well and
771:37 - when you apply a matrix vector product
771:39 - with with a bias term so we'll we we'll
771:41 - add the bias term in every
771:44 - neuron the bias term is like this so
771:47 - when you add this when you add this so
771:49 - you'll you'll be getting you'll be
771:51 - getting your when you do the Matrix
771:53 - Vector product and then add the BM then
771:57 - add the B term you'll get a uh you'll
771:59 - get a vector is 1 2 3 4 for this
772:02 - particular example and then you apply
772:04 - the Sig mode function and then you have
772:06 - a big H1 and then you apply the
772:08 - activation function and then you apply
772:10 - the activation function like this uh
772:12 - like this and then you and then you'll
772:14 - be getting your output Vector Big H one
772:18 - where you'll be having these things so
772:20 - this is more efficient or more
772:21 - computationally efficient over here and
772:24 - then what you do and then you do the
772:26 - same for12 where you apply the Sig word
772:28 - function every element Sig word function
772:31 - on these on on on the particular vector
772:33 - vector which you'll get from here okay
772:36 - so that's what we have written in a
772:37 - vectorial format
772:38 - so so there's no any kind of a
772:41 - theoretical concept which needs to be
772:42 - explained over here everything is very
772:45 - very clear in terms of writing these
772:47 - things out here main part will be your
772:50 - back propagation stuff which needs a
772:52 - little little bit of more explanation
772:54 - but these examples are pretty pretty
772:56 - much easy to understand I hope that this
772:58 - makes to sense to you I'll be catching
773:00 - up in next video till then bye have a
773:01 - great day so uh welcome to this lecture
773:05 - on uh on on neural networks so in this
773:09 - lecture what we will do is maybe we'll
773:11 - talk start talking about um chain rule
773:14 - of calculus um and try to try to start
773:17 - off with that propagation but before
773:19 - that I'll just take some examples or
773:21 - worked examples of of multier perceptron
773:23 - just to be very clear in uh in
773:26 - calculating all those things and maybe
773:28 - we'll talk about different applying
773:30 - different application uh sorry act
773:32 - activation functions on on on your in in
773:36 - neural networks and how things work and
773:38 - F prop so maybe we'll revise it and then
773:41 - we'll go on computational graph which is
773:43 - of chain rule of calculus and then we'll
773:45 - try to see through the through the
773:47 - computational graphs like how do we
773:48 - calculate the how do we about great
773:52 - great info about chain Rule and then we
773:54 - will go ahead and and and end this video
773:55 - on starting introduction to to back
773:58 - propagation cool so let's get
774:02 - started so till till previous videos we
774:05 - talked till uh nine slides so I'll just
774:08 - want to make you familiar with
774:22 - it right so we talk till n9th class uh
774:25 - sorry ninth slides so what we will do
774:27 - we'll just Recaps youate it so that it
774:30 - would be better for you so we have a z
774:32 - variable we have a variable Z and then
774:34 - we have a variable X and then we have a
774:36 - biased term B right and and then if
774:39 - these these are our weights these are
774:41 - our weights this is the input feature
774:44 - this is Matrix Vector multiplication
774:46 - plus the biased term and then uh and
774:48 - then you get your and the biased term is
774:50 - the vector then you get your uh pred uh
774:53 - a vector which is where we apply the the
774:56 - the the post post activation sorry sorry
775:00 - the activation functions on each Z right
775:03 - so when you when you take out the dot
775:04 - product between uh your weights and and
775:07 - input value use X or the information
775:10 - then and then you apply the
775:11 - pre-processing stuff like this sigmoid
775:14 - in this case and then you get your
775:15 - output which is a vector uh a vector
775:19 - which you will get uh in over here and
775:21 - then this Vector is Multiplied with uh
775:23 - another uh weights Vector weights Vector
775:26 - this Vector H1 is Multiplied with
775:28 - another weights Vector plus the bium so
775:30 - basically we're taking the dot product
775:32 - between these two uh vectors maybe yeah
775:35 - so we are taking uh we are taking the
775:36 - dot product between these two vectors
775:38 - and then uh we we were getting this 012
775:41 - okay uh so this was our basic uh basic
775:44 - revision of you of our previous lecture
775:46 - and I urge you to to basically have a
775:49 - good understanding of these things by
775:51 - watching our previous lecture because we
775:53 - had discussed a lot in that cool so now
775:57 - let's start seeing the worked example of
775:58 - one layer neural network so basically
776:01 - the the current neural network which we
776:02 - have is one lay one layer neural network
776:07 - one layer neural network Network so
776:09 - basically we have 1 two um three and
776:13 - four neurons four neurons right we have
776:17 - four neurons and then we have and then
776:20 - we have uh one particular one particular
776:23 - output neuron right so we have output
776:26 - neuron and over here we have two input
776:29 - features and x0 is always equals to one
776:32 - so so we don't need to worry about that
776:35 - all right and and and for assumption for
776:38 - Simplicity we have taken our bias term
776:41 - bias term all equals to zero for all uh
776:44 - neurons in this case so basically we
776:47 - have a weights we have weights like 0
776:50 - 0.1 so we have um so we have the first
776:54 - weight in the F from so W uh from the
776:57 - first to the first neuron so we have 1
777:00 - one to be 0.1 for the first layer for
777:02 - sure so 0.1 then for 0.2 for the first
777:06 - information so the first for the first
777:08 - information X1 is carried by these
777:10 - weights into the four neurons for the
777:12 - first neuron for the second for the
777:13 - third for the fourth and then X2 for the
777:16 - first for the second for the third for
777:17 - the fourth so you have a weight Vector
777:20 - we we call this as a weight Vector we
777:22 - call this as a we call this as a weight
777:26 - Vector right and this is your input
777:29 - Vector which you're seeing which is 21
777:32 - right so so the in we have taken a very
777:34 - dummy example to Showcase this is two
777:36 - and this is one and the and the weights
777:39 - for the second layer is
777:40 - 0.91 1.1 and one2 that is that are just
777:44 - random and just for the sake of example
777:47 - of a forward propagation like how do we
777:49 - multiply and do all those stuff right so
777:52 - first of all we we when we reach when we
777:55 - do calculation till here we get uh the
777:57 - vector uh for this we have 0.7 for this
778:01 - we have 1 uh after doing all the
778:04 - processing and the processing which is
778:06 - over here in every neuron is the first
778:09 - one is your I uh the the aggregation wi
778:12 - I * XI and then you apply the the the
778:15 - processing currently we're not applying
778:16 - the processing over here but the the
778:19 - first one is the is the weighted average
778:22 - or or the or what do would say the dot
778:25 - perk between your input features and the
778:27 - and the weight values right and then you
778:30 - have done the first part of it the
778:31 - second part is applying applying the the
778:34 - activation function so in this case we
778:37 - for this for for particular getting
778:38 - edge1 one then we apply the activation
778:42 - function on Z which we get like Z1 Z2 Z3
778:46 - and Z4 you apply that and then you get a
778:49 - final Vector which is the output from
778:52 - this particular hidden layer so this
778:54 - hidden layer is this H H1 okay the first
778:58 - hidden layer outputs this and then this
779:00 - is Multiplied with with weights over
779:03 - here um we have another Vector weight
779:06 - and of H1 * times the the the weight for
779:09 - the second layer we take out the dot per
779:11 - between both of both both of them and
779:14 - then and then uh we get our final output
779:17 - okay so this was your applying the
779:19 - sigmoid activation function where it
779:20 - squeezes the output in the range of 0 to
779:22 - 1 so this one is squeezed into 0.73 1.6
779:28 - into 0.832 1.3 into 0.78 5 Okay so this
779:34 - this was your basic example of applying
779:36 - sigmo function let's go ahead and see uh
779:39 - the the the next neuron so for the for
779:41 - the we have we we are done with the H1
779:43 - so we are done with H1 now we do for 01
779:48 - um for for for this so basically what I
779:49 - can do uh that indicates it's it's the
779:52 - second layer and this is the first unit
779:54 - right and uh this is what it does and
779:58 - now if if you see this is this is your
780:01 - uh output this is your uh the weights
780:04 - this is your weights W2 so this is your
780:07 - weights for the for the second layer and
780:09 - this is your your H1 which you got from
780:12 - the previous layer so information from
780:14 - the previous layer so this information
780:16 - for the previous layer is Multiplied
780:17 - with the given weights and then you get
780:19 - the particular output and then over here
780:21 - first of all you did the uh what do you
780:24 - say weighted average Now by taking the
780:26 - dot product between those those two
780:28 - vectors now what you do you apply the
780:30 - sigmoid function you apply the sigma
780:32 - function and on 2.5 it gives you a range
780:35 - of
780:35 - 0.93 right and then you get your 0 93 as
780:38 - an output so that's a one head and layer
780:41 - neural network right so where we are
780:43 - using the sigmoid activation function
780:45 - now let's go ahead and talk about Ru
780:47 - activation function the same the same
780:50 - example carries out so we have a 4x2
780:52 - Matrix Matrix where it contains I think
780:55 - um a 4x2 Matrix of weight Matrix we have
780:59 - 2x1 input Matrix and we have four 4x1
781:02 - bias bias uh sorry not Matrix 2x1 vector
781:06 - and 4 4x1 uh uh 4 * 1 uh bias vector and
781:10 - then we are getting the output and then
781:12 - we have an H1 which is what it does it
781:15 - over here we instead of uh applying uh
781:18 - the sigmo activation function we are
781:20 - applying reu activation function so
781:22 - basically what you mean and relu what it
781:24 - does it squeezes your output between one
781:28 - and zero right so so B basically not one
781:31 - and Zer it just takes out the max it
781:34 - just take out the max of zero and uh and
781:38 - zero I think yeah so it takes the max of
781:41 - zero and Z so whoever is bigger it just
781:45 - gives that as an output okay so 0.7 is
781:48 - bigger than zero 1 is bigger than Z so
781:50 - if there is minus one and your and and
781:53 - and let's say uh let's say 0 and minus
781:57 - one so so your Z is minus one then the
782:00 - output will be uh Z so there will be
782:02 - zero right so reu just what it does is
782:04 - the equation is Max of 0 comma Z and and
782:07 - whoever is bigger it just uh information
782:11 - for the next headden lay it is widely
782:12 - used Rel activation function and we'll
782:14 - see the advantages and disadvantages
782:16 - later on uh here we apply the tan
782:19 - activation function the tan activation
782:21 - function what it does over here which
782:22 - you can see um it's uh oh my God I think
782:26 - there's a typo over here we are applying
782:27 - the tan activation function uh tan
782:30 - activation function over here and this
782:32 - is the formula for it so we we urge you
782:35 - to watch the previous lectures if you
782:36 - haven't seen about these these
782:38 - activation functions in detail so we had
782:41 - a talk on on on a one layer hidden
782:44 - hidden neural network now here we see
782:47 - see an example of two hidden neural
782:49 - network two layer hidden neural network
782:52 - where we have for the for the first
782:54 - hidden layer for the first hidden layer
782:56 - our weight Matrix look like this our
782:58 - weight Matrix look like this we can also
783:01 - write in this format W1 right and then
783:04 - we can write in this format transpose we
783:06 - can write uh in this format so for the
783:09 - first uh weights right so we can simply
783:11 - write this as a this this row Vector
783:14 - into our column Vector right so sorry
783:16 - row Vector this column Vector into a row
783:18 - vector and then we can either transpose
783:20 - over here but but but for Simplicity
783:22 - let's let's stick with our example so
783:24 - basically we have W1 and then we have
783:26 - another this is a weight Matrix right
783:29 - and over here this this is a weight
783:32 - Matrix for for the first layer the
783:33 - weight Matrix for the second layer looks
783:35 - like this where for this you have the
783:38 - particular uh column Vector for the
783:41 - second uh hiden unit you have and you
783:44 - have this red Vector for this which you
783:46 - can see in detail so we have this uh
783:48 - weight Matrix for the second layer and
783:51 - then we have a third uh
783:53 - third weight Matrix or the or the column
783:57 - Matrix for the third layer right so
783:59 - basically for the output neuron okay so
784:02 - you can see it out just pause this video
784:04 - if you want to go ahead and see it out
784:05 - how it just works so we have different
784:08 - layer of neural networks so basically
784:09 - here we have a two layer neural network
784:11 - we have I think uh in in previous
784:13 - previous example this is three- layered
784:14 - neural network uh but in if we include
784:18 - this particular output neuron so this is
784:20 - three layer neural network this is three
784:22 - layer neural network
784:24 - Okay cool so over here we have several
784:26 - kind of different layers neural network
784:29 - and and every uh and these are all
784:31 - carried by the weights and our primary
784:33 - goal is to optimize these weights to get
784:35 - good Theta okay or the primary or the
784:38 - weights over here so we can have L layer
784:41 - neural networks like this and we will
784:44 - formally introduce you to the Larn
784:46 - neural network maybe at the end of this
784:48 - video U maybe at the end of the back
784:50 - propagation when we'll formally give you
784:51 - the tools or the definition of neural
784:54 - networks both forward propagation and
784:57 - backward propagation for L layer neural
784:59 - network and when we'll Implement our own
785:01 - neural networks right so that's why I
785:02 - think we can script this L neur neural
785:05 - network right now so let's get ahead so
785:07 - now we'll talk about the chain rule of
785:09 - calculus so the chain rule of calculus
785:11 - so first of all we will'll start off
785:13 - with the derivatives derivatives of the
785:15 - computation graphs and over here let us
785:18 - assume that you want to take out the
785:20 - derivative of P the partial derivative
785:23 - the partial derivative of p with respect
785:26 - to X1 right so the partial derivative of
785:29 - p with respect to
785:32 - X1 okay the partial Dera of p with
785:35 - respect to X1 so what it will be over
785:37 - here we can't directly take out the
785:39 - partial derivative of p with respect to
785:40 - X1 we need to go with the path and this
785:42 - is where the chain rule comes in right
785:44 - so the chain rule comes in first of all
785:46 - you take the partial derivative of p
785:49 - with respect to Z with respect to Z1
785:52 - times the partial derivative of of Z1
785:56 - with respect to X1 then you will get to
785:58 - this so basically first of all you you
786:00 - you make a relation with this by taking
786:02 - the partial derivative of p with respect
786:04 - to Z1 and then times right so that is
786:06 - the CH of calcul multi multiv ch ch R of
786:10 - calculus times U uh the partial
786:13 - derivative of Z1 with respect to this
786:16 - particular X1 all right so for for
786:19 - reaching from here to here you take you
786:21 - you you need to follow the path but
786:24 - there is one more way we can reach to
786:26 - this X1 okay so that's why we add a plus
786:29 - sign here the partial derivative of p
786:32 - with respect to Z2 times the partial
786:35 - derivative of Z2 with respect to X one
786:37 - so there's one more way so there's one
786:40 - One path to go here and there is another
786:42 - path to go here right so basically and
786:45 - then we'll and then we'll be having our
786:47 - partial derivative of p with respect to
786:49 - partial derivative of X1 the first path
786:52 - is this path and the second path is
786:54 - going by this path right so basically
786:56 - you you over here this is the come this
787:00 - is the parti the derivative of a chain
787:02 - rule of Al so basically again I'm
787:04 - repeating the partial derivative of this
787:06 - particular p don't worry about these
787:08 - things what they are just assume that
787:10 - all are differentiable and and we are
787:11 - able to calculate the partial derivative
787:14 - or the derivative the partial derivative
787:16 - is quite similar to derivative because
787:17 - the difference between the partial and
787:19 - partial derivative and and derivative is
787:21 - just uh you you can see my Matrix
787:23 - calculus course for this but basically
787:25 - the difference between them is um the
787:28 - difference uh it's it's it's it's more
787:30 - uh partial derivative is involved in
787:32 - vectors right so that's why we we add
787:34 - the partial derivative over the partial
787:36 - derivative of this p P right with
787:39 - respect to X1 so it is it is asking how
787:41 - much this P changes when X1 changes so
787:44 - this this is basically change saying how
787:47 - much how much P changes how much P
787:52 - changes when X1 changes so that's what
787:55 - the derivative is D derivative is a
787:58 - slope right so change in y over change
788:00 - in X right so for for for understanding
788:03 - the change in y change in P over change
788:06 - in X you need to follow the root to to
788:09 - understand the change in X when you
788:10 - change P right so when you change X1 how
788:13 - much P changes okay or how much P
788:16 - changes when X1 changes so basically for
788:18 - taking the for taking out the change you
788:21 - for uh for for for for the for for this
788:24 - with respect to uh for for this with
788:26 - respect to this then you need to follow
788:28 - two parts the first part is the partial
788:30 - der of p with respect to this Z1 and
788:32 - then multiply the partial D of Z1 with
788:35 - respect to X1 this tells how how much
788:38 - for in this tells the how much P changes
788:40 - when Z1 changes and how much Z1 changes
788:43 - when X1 changes so inly it is telling
788:46 - how much P changes when X1 changes right
788:49 - but there is one more path which is the
788:51 - partial of p with respect to Z2 and then
788:54 - multiply it by the partial derivative of
788:56 - Z2 with respect to X1 this is telling
788:58 - how much P changes when Z2 changes and
789:00 - how much Z2 changes when X1 changes okay
789:03 - and then we get our final partial
789:05 - derivative of p with respect to X1 now
789:08 - coming to the next the coming to the
789:10 - next part the next part State the
789:12 - partial derivative of p with respect to
789:15 - X2 the partial der of p with respect to
789:17 - Z2 right uh and so basically if if you
789:20 - want to calculate the partial der of P
789:22 - me how much P changes when X2 changes so
789:25 - there are a total of two paths the first
789:27 - path which we can go is by this and the
789:30 - second part which we can go is by this
789:33 - right the partial der of p with respect
789:35 - to Z2 and then by following this path
789:38 - the partial der of Z2 with respect to X2
789:41 - right uh with respect to X2 and then you
789:44 - get your how much P changes when X2
789:46 - changes for the first path for the
789:48 - second path the partial derivative of p
789:50 - with respect to Z1 and partial
789:51 - derivative of Z1 with respect to X2
789:54 - right so how much P changes when Z1
789:55 - changes and how much Z1 changes when X2
789:58 - changes and then you'll be getting a
789:59 - partial der of p with respect to X1 and
790:02 - partial derivative of p with respect to
790:04 - X2 so it's basically if you have seen a
790:06 - great recent thing it we we are what we
790:10 - doing what we were doing we're simply
790:11 - taking the partial D of g z with respect
790:14 - to with respect to Theta or W1 so it
790:17 - just tells how much J changes or loss
790:19 - changes when W1 changes so I'm just
790:21 - teaching you the concept so that you
790:23 - could understand the back propagation in
790:25 - much easier way when when we study about
790:26 - back propagation later on right so
790:29 - that's that's the first thing about the
790:31 - derivatives of the computation graph uh
790:33 - when when we talk about the chain Rule
790:35 - now let's get ahead I hope that this
790:36 - this this makes sense to you and this is
790:39 - now let's let's keep talking about the
790:41 - uh the Deep computational graph if you
790:44 - have a deep computational graphs how
790:46 - things work right so we had a talk on
790:49 - forward propagation all these things
790:50 - which which we talk on forward
790:52 - propagation but now we'll start talking
790:53 - about the backward propagation how we do
790:55 - the how we back propagate okay so the
790:58 - first thing we want to talk about is
790:59 - chain rule in deep computational graphs
791:01 - so over here you have this this H1 this
791:05 - this H1 a takes information it's h h H1
791:10 - is is is is is is the function of X1 and
791:13 - X2 and this h12 is a function of X1 and
791:18 - X2 right and this h21 is a function of
791:23 - h11 and h12 and this H22 is a function
791:27 - of h11 and h12 and P is a function of
791:31 - h21 and H22 so it takes these two values
791:34 - to compute their output right so that's
791:36 - what the that's what these these
791:37 - indicates over here so now so now I want
791:41 - to talk about is what is the the partial
791:45 - derivative of p with respect to the
791:48 - partial derivative of p with respect to
791:52 - X1 so first of all the partial
791:54 - derivative of p with respect to h21 so
791:57 - first of all we go this way times the
791:59 - partial derivative of h21 with respect
792:02 - to H1 U sorry it's X1 yeah so the so
792:05 - that's X1 so basically we we do this way
792:08 - right so we this all for sure the path
792:12 - but using this we are able to reach X1
792:14 - right and the partial der of p with
792:17 - respect to H22 right and the partial der
792:20 - of H22 with respect to X2 right but over
792:24 - here if you see we can calculate but
792:26 - there's when calculating h21 h21 to we
792:30 - have one one object over here there
792:33 - there there's one blockage so first of
792:35 - all what you do partial Dera of h21 with
792:39 - respect to with respect to partial
792:41 - derivative of of what do you say h11 h11
792:46 - times partial der of h11 with respect to
792:50 - partial with respect to X1 right this is
792:53 - how you calculate it this is how you
792:54 - will calculate it okay just for
792:57 - Simplicity let's let me showas you to
792:59 - you how we Cal how we calculate the
793:01 - partial der of p with respect to X1 is
793:05 - partial der h21 with respect to with the
793:08 - partial D of p with respect to h21 first
793:09 - of all we get two here and then with
793:12 - this and then with this we calculate the
793:14 - partial der of h21 with respect to X1
793:17 - for the we have another path partial der
793:20 - of p with respect to H22 times the
793:22 - partial derivative of H22 with respect
793:24 - to X1 right so there's two path but in
793:27 - in in in in the second path h21 with
793:30 - respect the partial derivative of S21
793:32 - with respect to X1 and partial D s22
793:35 - with respect to X1 has a block in
793:37 - between right so so basically for
793:40 - calculating the partial derivative of
793:41 - S21 with respect to X1 what you will do
793:44 - you will calculate the partial
793:45 - derivative of h21 the partial derivative
793:48 - of h21 times the partial D with respect
793:52 - to uh part with respect to h11 times the
793:56 - partial derivative of h11 with respect
793:59 - to X1 so the the blockage clears out so
794:02 - the blockage so the blockage clears out
794:05 - right plus there is one more way we can
794:08 - get to this X1 so basically the partial
794:10 - derivative of h21 with respect to h12
794:13 - and partial with and and and partial
794:16 - derivative of h12 with respect to X1 and
794:19 - then we calculate this particular
794:20 - partial derivative of h21 with respect
794:22 - to X1 so there's two path over here
794:25 - which we can get to uh that how much h21
794:28 - changes when X1 changes over change in
794:31 - h21 over change in X1 right so this is
794:34 - how we calculate for X1 right and then
794:37 - we'll do the same for this particular
794:40 - because there's some blockage in between
794:41 - right so let's let's do for that as well
794:44 - so basically when we do the partial
794:46 - derivative of H22 the partial derivative
794:48 - of H22 with respect to X I think X1 okay
794:52 - so we are calculating for X1 so for X1
794:55 - it is nothing but the partial der H22
794:57 - with respect to h11 so there's a
794:59 - blockage in between right so there's the
795:01 - blockage in
795:02 - between and then the partial derivative
795:05 - of of what do you say okay so over here
795:08 - I think uh I I did a little bit wrong
795:10 - over here I think so yeah so basically
795:13 - over here is B basically telling you
795:15 - have another path which we can go in
795:17 - this way so basically if if you see over
795:19 - here it starts Cal calculating but over
795:22 - here if if you see the partial
795:24 - derivative of H22 with respect to h11
795:27 - and the partial and times the partial
795:29 - derivative of h11 with respect to X1 it
795:32 - should be X1 over here right and then it
795:35 - goes with the same with with this for
795:37 - s22 I think the the the diagram is bit
795:40 - wrong I'll just put the picture right so
795:42 - it understand much much well you can
795:45 - simply ignore this you can simply ignore
795:47 - this this is kind of um misleading
795:49 - diagram so for calculating the partial
795:51 - der s22 with respect to X1 so first of
795:55 - all the partial derivative of s22 with
795:57 - respect to h11 right um H h11 h11 right
796:03 - and then times the partial D h11 with
796:06 - respect to X1 right plus the partial
796:10 - derivative of H22 with respect to H H22
796:15 - with respect to h12 right so we we have
796:17 - another another path which is like this
796:19 - times the partial der of h12 with
796:22 - respect to X1 so we calculated this as
796:25 - well and then we were having two two
796:27 - parts this path and this path and then
796:29 - we eventually will get our output as our
796:31 - final output right so this this was
796:33 - little bit misleading this was also
796:35 - little bit misleading but I will change
796:36 - change it um so basically this is the
796:39 - two paths which you calculate right so
796:42 - that's uh that's a particular thing
796:43 - which I talk about uh and and then same
796:45 - go and then we can write in this format
796:47 - and then like which which you're seeing
796:49 - over here and then we can calculate our
796:51 - partial of p with respect to how much P
796:53 - changes when X1
796:55 - changes so that's a that's a that's a
796:57 - lot of things which is over here I hope
797:00 - that you underst to this uh deep
797:02 - computational graphs so in the next
797:04 - video what I will do I I'll I'll go
797:05 - through I I'll make you go through the
797:07 - the iteration of back propagation so we
797:09 - we'll solve an example of an iteration
797:12 - of back propagation out here to
797:14 - understand much very well like how
797:16 - everything works and how everything does
797:17 - not works all right so we'll so we'll do
797:20 - in our next video of an iteration of
797:22 - back propagation and we'll try to
797:24 - formally Define the back propagation in
797:26 - deta so thanks for watching this video
797:28 - I'll be catching you
797:29 - next so everyone in our previous lecture
797:32 - uh we had a talk on propagation we had
797:35 - talked about comparation graphs we had
797:37 - talked about how do we back propagate in
797:40 - computational graph and I and I hope
797:42 - that given you a so much sense about
797:44 - back propagation and back propagation is
797:47 - not an algorithm it's uh it's it's a
797:50 - part of gradient descent which we do
797:53 - which you'll come to that later on but
797:54 - basically today what I'm going to do is
797:57 - to do an duration of back propagation uh
797:59 - mathematically by solving a a neural
798:02 - network where I where I'll be doing one
798:05 - iteration of back propagation and show
798:07 - you the updated weights uh which which
798:10 - optimizes the loss a little bit okay so
798:13 - I'll be doing only one iteration but in
798:15 - but in but in but in practice we usually
798:18 - have the parameters of how many
798:20 - iteration which you're which you're
798:21 - going to do which will talk about later
798:23 - on in our hyperparameter optimization
798:26 - sessions so basically the back
798:29 - propagation which I'm going to talk
798:30 - about is just uh is taken from a Blog
798:32 - which I link in description box below so
798:35 - the explanation of this written is also
798:37 - available in the form of blog which I
798:39 - link to the description box below I
798:41 - would like to give give a big shout out
798:44 - to that guy and I'll try to explain it
798:47 - much more detailed way so that you could
798:49 - also understand by watching this video
798:52 - so so basically this this is the basic
798:54 - uh stuff which is in front of you which
798:56 - is a basic a twed neural network where
798:59 - where we are having uh one hidden layer
799:02 - and one output layer with two outputs or
799:04 - two outputs from your neural network so
799:07 - basically let me take my pen so
799:09 - basically you have several weights uh
799:12 - which is carry your information is CED
799:15 - by several weights so basically this uh
799:17 - X X1 is CED by this W uh 1 one or w11
799:23 - okay so basically what I what I'm going
799:25 - to do is to vectorize this neural
799:27 - network and write this into the form of
799:29 - linear uh algebraic format or I would
799:32 - say majores format or vectors so
799:35 - basically I'm going to what what I'm
799:36 - what what I'm going to do is to store
799:38 - our input so basically the input which
799:40 - which it which it will go is just 0.05
799:44 - and 0.10 and they're just I I've just
799:46 - taken a sake sake sake sake of an
799:48 - example this is this is nothing much
799:50 - more to worry about so basically this is
799:53 - your inputs X1 and X2 0 0.05 and 0.10
799:57 - that is your input to your model and uh
800:00 - that's your that's that's our column
800:02 - Vector which is the X and I should
800:04 - denote with the small X rather than big
800:07 - X so basically small X and you have a
800:09 - weight Matrix for the first hidden layer
800:12 - the weight Matrix for the first hidden
800:13 - layer so for for for the layer this is a
800:16 - layer so so basically you have 0.15 and
800:21 - 0.20 right so 0 0.15 is a weight uh is
800:25 - is a weight for X1 and 0.20 is a weight
800:30 - for x uh of course uh X1 going to uh if
800:34 - you see X2 okay okay so X 0.15 is X1
800:39 - 0.20 is for X2 so basically your
800:41 - information is scattered X2 X1 is
800:44 - scattered by
800:45 - 0.15 uh from the first information to
800:48 - the first hidden layer or the neuron and
800:50 - your second uh way and second
800:52 - information is carried by some weight to
800:55 - the first hidden lay so basically that
800:57 - is
800:57 - 0.20 okay now we have the another neuron
801:01 - as as as I said we stack the neurons or
801:04 - the basic units right so we stack the so
801:07 - so basically this h21 basically this is
801:09 - a hidden layer and we have two neutrons
801:11 - in it and this H this X1 is CED by
801:14 - weights and being aggregated at h21
801:18 - which is the second hidden layer second
801:20 - hidden neuron in the first hidden layer
801:22 - and then this uh and then X the S2 then
801:25 - X2 information is also C by some ways
801:28 - for particularly this neuron so 0.25 and
801:31 - 0.30 which is weights for this for the
801:34 - first layer weights for the second layer
801:38 - weights for the second layer is W2 is
801:40 - equals to uh 0 0.40 which is going uh
801:45 - just is carried this h11 H1 uh H
801:49 - superscript one sub subscript one is
801:51 - carried by some weight 0.40 and for and
801:55 - then this this this this this
801:57 - information is also carried by 0 050 for
802:01 - the second neuron and this the second H2
802:03 - the second second neuron also gives out
802:05 - some information
802:07 - which is carried by some weights which
802:08 - goes in the which which which goes in
802:10 - the second out the first output neuron
802:13 - and this and the same information is
802:15 - Cared by another another weights for the
802:18 - second neuron okay so this is this is
802:20 - the basic neural network which you have
802:22 - and you have a bias term and you have a
802:25 - bias term which is first of all for B
802:27 - over here where for this you have 0.35
802:30 - and for this you have 0 0.35 for this
802:32 - you have 0.60 for this you have also
802:35 - 0.60
802:40 - pretty much it okay so now what I'm
802:42 - going to do is to do a forward
802:44 - propagation so basically that's that's
802:46 - that's what I'm what I'm going to do is
802:48 - to do a simple forward propagation and
802:51 - basically uh uh what what I'm going to
802:53 - do for propagation is get some predicted
802:56 - value o o sub superscript 2 subscript
802:59 - one and o superscript 2 and O sub
803:01 - subscript 2 so that's for what I'm going
803:04 - to to get so let's go so here let's
803:07 - let's go in our first uh hidden lay and
803:10 - in the first unit or neuron so when when
803:12 - you go there as as as I already say to
803:15 - you a neuron is Con a neuron in a hidden
803:19 - lay consists of two things first it gets
803:23 - aggregated in as uh in a nucleus which
803:26 - we call this as a z and then we have
803:29 - something called is post post uh sorry
803:32 - AC activation function which activates
803:35 - this neuron which decides whether to
803:37 - activate this neuron for the particular
803:39 - task or not okay so so basically if this
803:43 - is zero then the neuron is not activated
803:45 - but if this is something then a neuron
803:47 - is activated and that contributes some
803:48 - information that can go into the second
803:51 - layer of network so basically if you're
803:53 - seeing that z z superscript one sub
803:56 - subscript one is can be calculated by
803:59 - using something like w transpose X plus
804:02 - b and basically your W if you if you
804:05 - think for for but for this W was 0.15
804:09 - and 0.2 right so this this this was your
804:13 - uh for the first the first information
804:15 - carried by two ways into two two
804:17 - different neuron right but but basically
804:20 - uh but basically over here uh which
804:23 - which you're seeing over here which
804:24 - which which we have this this over here
804:27 - this is carried by two weight 0.15 and
804:29 - 0.2 right uh 0 0 0.15 0.2 from two two
804:34 - different inputs and then we add a bu
804:36 - term which is 0.35 and multiply by 1
804:39 - because x0 is always equals to one and
804:41 - then you get your output so basically
804:42 - this is your weights this this is your
804:44 - input X1 this is your W 21 okay for the
804:49 - first and this is w11 okay and this is
804:52 - your bias term right so this is this is
804:55 - basically your W transpose x + B which
804:58 - is your hypothesis and then this this
805:00 - this is aggregated and then what you do
805:03 - you simply apply an activation function
805:05 - here we are applying a sigmoid
805:07 - activation function sigmoid activation
805:09 - function but we'll see that this sigmoid
805:11 - activation function has a really big
805:13 - fall of Vanishing gradient isent problem
805:15 - so which will talk about it that later
805:17 - on but for Simplicity let's go with
805:20 - sigmoid so when you apply the Sig
805:21 - sigmoid to this output of Z1 the it it
805:25 - it is it say 0.59 it means this neuron
805:28 - is activated uh right and then you get
805:30 - your final output for this is nothing
805:32 - but 0.59 blah blah blah okay so that's
805:35 - your out that's your output from this
805:37 - for for for this you have again the Z1
805:40 - and A1 and and this is this this in this
805:44 - newon X1 is carried by some weights and
805:46 - X2 is carried by some weights and then
805:49 - we aggregate that and then we add the
805:51 - bias term which is 0.35 and then we get
805:54 - our output and then we apply an
805:55 - activation function which is Sigma
805:57 - activation function and then you get
805:58 - your
805:59 - output right so that that that that is
806:02 - for the first H h11 and h12 now let's
806:06 - let's go ahead and talk about the the
806:08 - the output neurons right so that's
806:10 - that's a production so basically uh if
806:13 - you're seeing over here Z2 if so
806:16 - basically if you're going over here so
806:18 - this is a second layer of course and the
806:20 - first neuron right so basically when you
806:22 - when you calculate that this this output
806:24 - output neuron is carried by two
806:26 - information from the previous layer
806:28 - which is uh this information and this
806:31 - information so this information is
806:32 - carried by some bits and this
806:34 - information is carried by some bits and
806:36 - please note that these informations or
806:38 - the weight value are random as of now
806:40 - okay your your algorithm main goal of
806:43 - your algorithm to learn these weights
806:45 - okay uh that's that's that's that's what
806:48 - which is super important and this a12 is
806:51 - then you apply an activation function on
806:53 - top of it and then we get our o o o21 as
806:56 - an output okay same we do with o22 for
807:00 - which which we get an output as like
807:02 - this so now so now we have we have got
807:06 - our output now what we want to do we
807:08 - want to calculate the error as well like
807:10 - with these weights with these particular
807:12 - weights how well our models performing
807:14 - or how is the loss function or how is
807:16 - the cost function or or what's the
807:19 - what's the error is or how well are
807:20 - models performing right so basically uh
807:23 - let us assume that a ground Thro is for
807:26 - that a ground Thro is 0.01 and o22 for
807:29 - o22 we have 0 0.99 but our model given
807:33 - 0.75 for the for the first one and and
807:36 - 0.1 uh 0.77 for the second output neuron
807:40 - which is your model model output so what
807:42 - you what your model will do uh so
807:44 - basically we can calculate the error by
807:47 - by Sub Sub sub subtracting uh 0.75 then
807:51 - this one and then 0.77 with this one
807:54 - okay so basically you know how to take
807:55 - out the loss which is a mean square
807:57 - error what what whatever you can use
807:59 - over here and then you can uh and then
808:01 - we and then we make use of mean squ
808:03 - error to C calculate the error which is
808:06 - .20 29837 blah blah okay so that's what
808:10 - we are done with for propagation where
808:12 - where we got the error J with all of
808:15 - these parameters now we need to optimize
808:17 - these uh this this J so basically what I
808:20 - told you in our previous lectures or our
808:23 - previous grent lectures is we take the
808:26 - the the partial derivative of of of our
808:29 - C function J with respect to our
808:32 - parameters whoever over there and what
808:35 - we do we simply adjust these weights and
808:38 - check if the loss function is decreasing
808:41 - if the loss function is decreasing we
808:43 - update those weights right so that's
808:45 - that's what we do uh when when when we
808:48 - talk about uh derivative so derivative
808:51 - tells you two
808:52 - things how much weight to change and in
808:55 - which direction your your your your your
808:58 - weight will go in so please note that
808:59 - you was the lecture of gradi and descent
809:02 - so that you have understood in much more
809:04 - uh good way so what I'm going to do is
809:07 - to take out the partial first of all we
809:09 - need to update this weight W1 w211 so
809:12 - what I'm going to do is take of the
809:14 - partial derivative of J cost function
809:17 - with respect to w21 which which tells us
809:20 - how much loss changes how much loss
809:23 - changes or cost changes when that w21
809:27 - that that W changes or that parameters
809:29 - Chang or that weight changes okay so so
809:32 - how we can calculate that so that's
809:34 - quite easy to calculate so if you want
809:36 - to take out the partial derivative of
809:38 - cost function with respect to your W12
809:41 - so you have some blockage in that so
809:43 - first of all you need to calculate the
809:45 - calculate the the the the the the
809:48 - partial derivative of J with respect to
809:51 - a12 so the activation function of this
809:53 - so if you're seeing that you to
809:54 - calculate the first of all you have to
809:55 - go over here multiplied by as your as we
809:59 - invoke the chain rules so basically this
810:01 - a12 so first of all you take out the
810:03 - partial de derivative of J because over
810:05 - here over here if you if you want to go
810:08 - back then over here you have two things
810:11 - Z and a okay and Z this this is nothing
810:14 - but A2 like
810:15 - this okay so that's what we need to do
810:18 - so that's what we need to do is is is
810:21 - just take out the partial derivative of
810:23 - J with respect to a12 after that now we
810:27 - need to C because we still haven't
810:29 - reached there we still haven't reached
810:31 - there to our W12 so basically we
810:33 - calculated the partial derivative of A1
810:35 - this one
810:36 - this uh this this one with respect to
810:39 - this okay because we need to calculate
810:41 - the partial der of that so we calculate
810:43 - that and then using Z1 and then using Z1
810:46 - we can go to this parameter to know how
810:48 - this is changing okay so basically let's
810:52 - let's so basically here's the
810:53 - calculation which is involved so here's
810:55 - our cost function where where I have for
810:58 - Simplicity I have written this of course
811:00 - and then you take out the partial
811:01 - derivative of J with respect to a12 so
811:04 - basically what I what I'm doing over
811:06 - here so basically what what I'm doing
811:07 - over here is to using the the chain rule
811:10 - of calculus and the power rule ofal
811:12 - calculus to perform this calculation so
811:15 - basically what what I'm doing is is
811:18 - performing uh performing the first of
811:20 - all the the partial derivative of the
811:23 - inner function of the inner function
811:25 - inner function and the partial
811:27 - derivative or The Taking of the
811:29 - derivative not partial derivative the
811:31 - derivative of inner function leaving the
811:33 - outside function as it is sorry so sorry
811:36 - partial derivative outer function
811:37 - leaving the inner function as it is and
811:40 - and then taking out the times the
811:42 - partial the derivative of inner function
811:44 - so first of all when we take the the
811:46 - derivative of outer function so
811:48 - basically here is two so using the power
811:50 - rule we take it over here right and
811:53 - subtract Min -1 from this right so when
811:56 - when we 2 2 * 1 by 2 that is cuts down
811:59 - so we are left with uh y - h of X now
812:05 - that that is our the derivative of outer
812:08 - variable now outer the outer function
812:10 - somehow because over here we have
812:12 - something but there is something which
812:13 - is one / two something something and
812:15 - this is two so the derivative of outer
812:17 - function which where we take two over
812:19 - here multiply with it as I said as you
812:21 - have listened me that I told in gradient
812:23 - when when when when do in gradient send
812:25 - it canceled out so that's why we do that
812:28 - and then we leave this as it is times
812:31 - your your the derivative of inner in
812:34 - inner function okay so when when you
812:36 - take out the derivative of inner
812:38 - function when when you take out the
812:39 - derivative of inner function this why
812:42 - this this this this y have uh IID say uh
812:47 - i' say if you to take out with a12 so
812:49 - this so This eventually becomes -1 -1 +
812:54 - 0 and 0 is nothing but I would say uh
812:58 - the the the reason why we got zero
813:00 - because the the derivative of any uh
813:03 - inner in inner uh constant is zero and
813:07 - here we are using the sum rule because
813:08 - we are applying the derivative of n on
813:10 - every function and then the reason why
813:13 - we have minus one because uh when you
813:15 - multiply this this this minus one with
813:17 - this so you so it'll be left with this
813:19 - final formula which is a partial
813:21 - derivative of J with respect to a12 so
813:23 - that's that's your that's your the the
813:25 - partial derivative which you got okay
813:27 - now this just tells us how much cost
813:30 - changes when that activation function or
813:33 - a to1 changes okay that's what what you
813:35 - got over here now you can plug in the
813:37 - values to get that value right so when
813:40 - you plug in the values so basically put
813:42 - put the minus sign as it is this this is
813:44 - a ground Ruth and this is a model
813:46 - predicted which you get which is 0.74
813:49 - right so that's you you got the value of
813:51 - this one now you go the same for the
813:53 - derivative of A1 2 with respect to Z1 so
813:56 - basically this is your as usual sigmo
813:58 - function and we have already derived
814:00 - this in our intro to NN chapter so when
814:02 - you take out the derivative of your
814:03 - activation function this something looks
814:05 - like like this and then when you take
814:07 - out the and then you put in the values
814:09 - of sigmoid of Z1 so basically your Z1
814:11 - was what what what what was your
814:14 - Z1 we have just calculated priorly when
814:17 - doing for
814:18 - propagation so it it was this times
814:22 - minus 1 1 minus this and then you get
814:25 - your final output which is 0.186 which
814:27 - you got the value of this as well now
814:30 - the last one which you're left with is
814:32 - z12 where we are simply the the the the
814:35 - the the z12 is this so we going to take
814:37 - out the partial Dera of z12 with respect
814:40 - to this variable sorry this parameter so
814:42 - basically when you calculate that when
814:44 - you calculate that you are one left with
814:46 - you're one left with uh this this
814:49 - particular value which is uh H1 one okay
814:54 - that's that's that's that's that's what
814:56 - you're left with let let me just make
814:58 - you sure that what exactly your left
815:00 - with here so when you when you go back
815:03 - here so the when then you take out the
815:06 - partial derivative of z12 with respect
815:09 - to w11 then you left is f 5 93 because
815:14 - uh that that uh because that's that's a
815:16 - constant right and and you have XI which
815:19 - is left so when when you do the
815:21 - calculation you'll be getting this
815:23 - 0.59 which is quite well to understand
815:26 - okay so that's a that's the first that's
815:29 - the first parameter
815:30 - updation now what I what I'm going to do
815:33 - is to just change the little bit of
815:34 - calculation so that that you are you up
815:36 - to the standards of the calculation
815:38 - which are going on so Delta rule the
815:40 - Delta rule is when you take out the
815:42 - partial derivative of J with respect to
815:43 - W1 as how much cost changes when that
815:47 - changes right so that is which we have
815:49 - just taken out which we have just taken
815:51 - out as you're seeing over here we are
815:53 - just just taken out we have this so we
815:55 - can fill in the formulas W12 that is a
815:58 - ground Thro minus your actual value
816:01 - sorry sorry yeah mod predicted value and
816:04 - we have minus over just we add little
816:06 - bit of notations times this one which is
816:10 - uh which is partial der Z1 with two and
816:13 - then Etc which is the which is the
816:14 - partial derivative of your segment
816:16 - function which you got okay this this
816:18 - formula and this formula and then in
816:20 - this when you take out the partial
816:21 - derivative of Z1 with respect to
816:23 - whatever with respect to your parameter
816:26 - you have the output as a uh a12 so
816:29 - basically when you when go there uh uh
816:32 - this a12 which is a11 so basically that
816:37 - uh that particular value so yeah so
816:41 - basically you're you're left with a11
816:43 - which is over here okay so basically you
816:45 - left with a11 and then you using that
816:47 - a11 you you get that W12 so basically
816:51 - using that A1 because this h11 have the
816:54 - a11 of course you you h11 is an output
816:57 - of your z11 and and a11 right so your
817:01 - a11 is the final output of course so
817:03 - basically you have this a11 over there
817:06 - if you're confused a little little bit
817:07 - on that so let me just make sure that
817:09 - you're that that that you're that you're
817:11 - understanding what I'm saying so over
817:13 - here I think this there is a slightly
817:14 - change it should be 7 six whatever but
817:17 - but basically we are we are going here
817:19 - and this is where we are we we are using
817:21 - uh this is from this is where we have
817:24 - this uh h11 basically where we're going
817:27 - backwards right so with here we have Z
817:30 - as
817:31 - w11 uh W2 * whatever uh the the output
817:36 - from a11 right so this is your constant
817:39 - of this so that is your a11 is which is
817:42 - an output when if you studied about
817:44 - calculus then then you might be familiar
817:45 - with this so that's your partial
817:49 - derivative of Z1 so sorry U yeah so so
817:53 - basically uh that that that that was a
817:56 - part derivative of z11 with Z with
817:59 - respect to your parameter W and please
818:01 - be sure that I'm making lots of Errors
818:03 - over here because I think that it's it's
818:05 - very tough to at least pronounce their
818:07 - name but no problem in that but now we
818:10 - can write this into Delta notation and
818:12 - Delta notation here we can do we can
818:14 - take this uh and put that into a Delta
818:17 - 012 which is the which for that variable
818:22 - right and then multiply it by a11 and
818:24 - that's your partial deriva of J with
818:26 - respect to
818:27 - w211 okay so that's that's a Delta rule
818:31 - which you also see in real world so now
818:33 - we can just go ahead and and and and do
818:36 - all the calculations so we had taken out
818:38 - all all all of these in over here now
818:41 - what we can do we can we can multiply
818:43 - with that and then we get a final output
818:44 - and now what we can do we can update our
818:47 - uh weight because that because now we
818:49 - need to update our previous weight right
818:51 - so that's the that's the first update
818:53 - which we going to do let let us assume
818:55 - we have taken a learning rate which is
818:56 - 0.5 which is fairly a large learning
818:59 - rate but previously our weight was 0.4
819:02 - but minus uh which 0 0.5 times the
819:06 - derivative okay and then you get the new
819:08 - weight okay previously 0.4 now it was
819:11 - now it is 0.35 which optimizes the
819:14 - error now what I'm going to do now what
819:17 - I'm going to do is um is just have
819:21 - something which is partial derivative of
819:24 - J with respect to w21 and basically over
819:27 - here what we are doing is take out first
819:29 - of all go back and then first of all
819:32 - take out a21 because over here you have
819:34 - something like this yes right so so
819:36 - basically you need to go back and this
819:38 - this is for a21 right so basically this
819:40 - is uh this is for uh second uh second
819:44 - weight going into first neuron as far as
819:46 - I know right so basically you have this
819:48 - a12 which is the second uh new second
819:51 - activation function so basically you
819:53 - take out a partial deriva of J with
819:54 - respect to Second activation functions
819:56 - you can do the calculation Now by your
819:57 - own and then using that you go to over
820:00 - here and using Z you take out the
820:02 - partial derivative of Z with respect to
820:05 - w21 which which tells how much G changes
820:08 - when this parameter changes and when you
820:10 - do that you'll get this 0.822 and when
820:13 - you when you when you update your
820:14 - parameter by the old one by minus 0.5
820:18 - which is the learning rate you get the
820:19 - 0.40 which is uh pre previous one was
820:22 - 0.45 now was 0.40 right you do the same
820:26 - for the other two for the second for the
820:28 - second layer neural network for the
820:30 - second layer uh out for for for the
820:32 - output layer weights so you can do in
820:34 - this way the same way the same will
820:36 - follow up right and and there is
820:38 - something called a memorization there is
820:40 - something called memorization which I
820:42 - just just want to tell is when you take
820:44 - out say say for an example you take out
820:47 - the partial derivative of J A1 to and
820:50 - and your previous iteration you will
820:52 - will come to that the instance where
820:54 - you'll see the partial derivative
820:55 - already taken which which can be used in
820:57 - next iteration so we just make use of
820:59 - that rather than read read read redoing
821:03 - the partial uh the partial derivative of
821:04 - that so so that's what I we just make
821:06 - use of that of the of the one res all
821:08 - cre but but there will be a separate
821:10 - video coming up coming up on this so we
821:12 - had now updated our all uh parameters
821:16 - which is all the parameters of this till
821:18 - now now we need to update the parameters
821:20 - of this because we are back propagating
821:22 - back propagating over here so now we're
821:25 - updating the values of the parameters of
821:26 - the first hidden layer so basically we
821:29 - have to calculate the partial derivative
821:31 - of J with respect to w11
821:35 - uh with for the first uh parameter so
821:37 - basically what I'm going to do is
821:39 - partial derivative of J with respect to
821:42 - a11 right first of all we need to go
821:44 - there times the partial derivative of J
821:47 - with respect a11 with respect to Z1
821:50 - because we thebr we also go to Z1 and
821:53 - then using Z1 we go to W1 okay so as you
821:57 - know we first go here we first go here
822:00 - we first go here the partial derivative
822:02 - of J with respect to the G with respect
822:05 - to uh A1 a11 and then and then using
822:11 - this we go over here uh you take out the
822:13 - partial derivative of this with respect
822:15 - to Z uh 1 one right and then using this
822:19 - we take out the partial derivative of Z
822:21 - with respect to this and this this this
822:23 - this tells us if and using the chain
822:25 - rule as I told this is a chain Rule and
822:27 - there is a some notion called
822:28 - intermediate variables and and all those
822:31 - stuff which I which I'll come to that
822:32 - later on but basically this is the basic
822:34 - idea if something is causing blockage
822:36 - that's that's the chain rule which if
822:38 - you have already known about that so you
822:40 - take up when when when you do the
822:41 - calculation you'll get your partial Del
822:44 - of J like how much cost changes when w11
822:47 - changes one superscript one and Sub sub
822:50 - subscript one one
822:52 - changes so now if you if you go over
822:56 - here if if you go over here uh for
823:00 - calculating the partial derivative of J
823:02 - with respect to a11 a11 so there a one
823:05 - blockage which is causing over there so
823:07 - we need to calculate the partial
823:09 - derivative of e eo1 because that's
823:12 - that's our output with respect to a11
823:15 - right plus the the the partial
823:17 - derivative of e O2 with respect to A1
823:21 - okay because as we know as we go over
823:23 - here and from this side as well as I
823:26 - told in your previous section uh in in
823:28 - in our previous lecture so that's that's
823:30 - the path because there are two paths
823:31 - which which we can go further so
823:34 - basically the part should derivative of
823:35 - Z with a11 so here's here's your two
823:39 - paths the first path for for going to
823:41 - a11 and this is the second path for for
823:43 - for for going to the Z1 like this and
823:46 - like this right as I told so first let's
823:48 - take out this partial derivative E1 with
823:51 - respect to a11 so when you when you take
823:53 - out the partial dtive E1 that is your a
823:56 - that that is nothing but uh a one 2
824:00 - that's that's that's what it is right
824:02 - that's that's the output of it right uh
824:05 - as far as I know that's that's a soorry
824:07 - so sorry uh uh just just wait for a
824:11 - second let me confirm it if I'm correct
824:13 - over here maybe I
824:16 - should uh yeah I guess that I'm correct
824:21 - 0.75 yeah I'm correct I
824:24 - think so basically this is this is not
824:27 - this is not an error I think you can
824:28 - ignore this this is an output output of
824:31 - the of the of the out this is basically
824:33 - an output not an error please ignore
824:35 - this as of now this is an output not an
824:37 - error okay so basically this is an
824:39 - output not an error between the ground
824:41 - throw this just an output which is a21
824:44 - a12 okay so and this is
824:47 - a22 okay so that's that's activation for
824:50 - which you get from output now basically
824:52 - when so basically you have to go from
824:55 - here to here so basically you are here
824:57 - so you you take the partial derivative
824:59 - of this with respect to this and then
825:02 - using this you go over here that's what
825:05 - we need to do right so you do the same
825:08 - and then for taking of the partial
825:10 - derivative of eo1 with respect to z12
825:13 - right so z z z12 we need to first of all
825:17 - take out the partial derivative of a11
825:20 - with respect to a12 times the partial
825:22 - derivative of A1 to divided by so sorry
825:26 - with with respect to z12 so basically
825:29 - this is quite confusing as of now I know
825:31 - but basically uh what we are doing we
825:33 - have two different paths to go above
825:36 - okay we have two different paths to go
825:37 - above sit down and see where it is going
825:41 - okay sit down and see where it is going
825:44 - and then you'll understand that we have
825:46 - a two paths where it going from this
825:48 - side uh from from from from from this
825:51 - side and from this side and you'll be
825:53 - seeing that we have taken from the this
825:56 - side this is partial derivative E1 with
825:59 - respect to a11 and this is the and and
826:03 - and when you take out the partial
826:05 - derivative for from this side as well
826:07 - okay from this side as well right so so
826:10 - so basically what what you can do is uh
826:14 - have this particular which you're seeing
826:16 - which is your uh which you can calculate
826:18 - that and then you got your final so
826:20 - basically you can plug in the values of
826:23 - that you can plug in the values of that
826:25 - because we need two these two for
826:27 - calculating the Z how much a changes
826:29 - when this changes plug in the values and
826:31 - and then we get our values and then we
826:33 - simply plus that because because that's
826:35 - the two path to go at that a11 okay
826:40 - so after that we done with this now we
826:43 - Cal so basically we we done with
826:45 - calculating the partial J with respect
826:47 - to a11 partial Dera of J a11 because it
826:50 - involves lots lots of calculations not
826:52 - only from this side it will go from this
826:55 - side as well if there's a two paths for
826:57 - that and that's why back back
827:00 - propagation is one of the hardest stuff
827:02 - in deep learning but it but but but it
827:03 - comes over uh learning all the stuff I
827:06 - suggest you to maybe read a Blog with it
827:09 - as well as watch the video that would
827:11 - make more sense uh so basically now we
827:14 - done with that so we take out the
827:15 - partial derivative of a11 with respect
827:17 - to z11 because without that we can't go
827:21 - so basically you all you the partial
827:23 - derivative of your Sigma function is
827:24 - always you know and then you have a z
827:26 - when you take out that which gives X1
827:29 - which is output now you can simply plug
827:31 - in the values which is
827:33 - 0.036 time 0.24 which is another
827:37 - derivative then times 0.05 and then you
827:40 - get your output as a final output okay
827:42 - so that's the derivative of how much J
827:44 - changes when w11 changes and now what
827:46 - you going to do now what you going to do
827:48 - is just is just update your values 1 one
827:51 - is equals to W is new one this is the
827:54 - old one right this is the old one minus
827:57 - you're learning at Alpha and the D
827:59 - derivative of J with respect to 1 one
828:03 - okay and this is your out put which is
828:05 - 0.04 and then you can calculate for 1
828:08 - one same you can do with W uh W uh W one
828:13 - two for the first layer but you need to
828:16 - identify the paths right the number of
828:18 - paths and then do it do it in that way
828:21 - okay so that's what the the iteration of
828:23 - back propagation means so we did a one
828:25 - one iteration of back propagation and I
828:27 - hope that this this this was a really
828:29 - nice session uh basically I just uh
828:33 - basically uh this was a a really really
828:35 - nice session on back back propagation uh
828:37 - if you like this lecture let me know and
828:39 - if if you have any questions or any
828:41 - doubt

Cleaned transcript:

this deep learning course is designed to take you from beginner to proficient in deep learning IU sing created this course he's an experienced data scientist and popular course creator aush will teach you the fundamental concepts architectures and applications of deep learning in a clear and practical way so get ready to build train and deploy models that can tackle real world problems across various Industries what if I tell you that there's a deep learning course that teaches you deep learning from very scratch to a core level so what I really mean by scratch is teaching you the core and the Crux of the mathematics which is required for deep learning like linear algebra single variable calculus and much more and not only this we give you detailed lecture notes along with the Practical assignments on data Wars for absolutely free so that you can follow through this course and become the master hi this is aayush I'm cofounder of second brain laps and in past worked as a lead data scientist at triplate a UK based esteemed organizations working on large scale career economy product as well as I've worked as an emops engineer in a core ziml team in order to streamline mlops Frameworks and furthermore I worked in several usbased companies as a contractual roles as a data scientist and not only this I love teaching my courses has got millions and millions of views throughout the internet and I've helped several thousands of students in order to get their first paycheck or their first job and but why you should consider learning deep learning and what is the core problem which is coming into the deep learning content throughout the internet now it is all of the companies are asking for a deep learning skill set into a particular candidate and to be honest people think deep learning extremely hard and pretty hard to understand and it's my personal opinion I feel that because of the instructors on YouTube it's becoming a little bit hard to understand it's not because it is very complex I agree that the whatever things are little bit difficult or hard to interpret it but if taught in the right way deep learning is the most I think easiest subject as compared to even core machine learning I will teach you deep learning in a very core way from very scratch we will mathematically do every iteration by our mathematics so that you understand okay this is how the flow is going and this is how each steps is helping your model to learn or become better let's get started with our course hey everyone welcome to this first lecture on linear algebra so today we are going to talk about uh vectors and we'll be exploring vectors A bit okay so uh first we'll start off with what is linear algebra why do we even bother to study this as some of you all already are familiar with algebra you you just want to uh just re refresh your memory of your algebra or you wanted to uh you're from very scratch and then s so that's why let's let's start with the definition of linear algebra so I've written one definition of linear algebra is it's the mathematics of the data yeah you heard me correct I saw this definition online and I found this a very basic definition to tell you uh rather than taking too much of uh mathematical terms is algebra L linear algebra is the mathematics of data and why I'm saying it's mathematics for data because uh L linear algebra contains of matrices and vectors so so these uh these these two are the language of the data so whatever we are going to study you which you will see in your machine learning or deep Learning Journey whatever you are going to study so that's why we we just say that is it's the mathematics of the data because in machine learning data is so much of uh basic component or or a mandatory component the same way the we use algebra or L lar algebra to work with the data mathematically okay so this is a simple definition of linear algebra over here so let's get started with the first uh first uh the component which first thing which we'll study in this video is vectors okay so so we'll start with the vectors so we starting with vectors so if you if if you have any definition of a vectors please please please stop stop this video pause this video and then go down in the comment and please tell me what are vectors so the the we will start with very scratch uh you can assume a vectors as an arrows as an arrows we we have an arrow so these These are the geometric inition of a vectors so the definition of a vector can be it's it's an arrays of a numbers okay so vectors are arrays of a number or a tuple of a numbers okay or or or you can you can take it as an arrays okay so you can consider a vectors vectors can be arrays of a numbers arrays of number or you can consider this vectors uh as an as an arrow you can consider vectors as an arrows as an arrows or you can consider a vector as a tuple of a numbers you can consider ve Vector as a tuple of numbers okay so these are you you can just imagine this an an array is maybe an array can be 1 2 2 3 this is called this is this is called the draw Vector which you'll study you can safely ignore this so this is this is also a vector which is special type of vector which is called the row Vector this can be tle of a numbers or it can be arrows okay so so so the way I like to represent uh vectors or to make you very very much comfortable with it is to to make you familiar with an arrows so this is the geometric division of a twodimensional Vector so let's see how the vector looks numerically okay uh in in terms of mathematical so let's see let's let's see how the vector look so let's name the vector as U okay so let's let's name the vector as U equals so let's store 2 and four so U is a vector where you have the elements so the the numbers inside the vector you you enclose into a a square bracket so here here is two and here is four okay so why I'm saying we have this is this this is like a an arrow so the first element is called X component and the first element is called X component and uh second element is called a y component it's called a y component so let's let's see how this looks on on a graph paper or or to or an X and Y plane so let me plot let me plot that so 1 2 3 4 5 1 2 3 4 and five okay so this is my y plane and this is my xaxis okay so let's plot this Vector onto this uh X and Y plane so so X component is 2 and Y component is four okay so X component is two and Y component is four so here's the point and it passes through the origin so so this is your vector U okay so this is your vector U where you have 2x 4 where two indicates the X component and four indicates the Y component okay so this makes sense I hope so okay so and vectors are arrays of a numbers or you can say the the Tuple of a numbers which which which consist of numbers where it is it has only it is here here our Vector is two dimensional okay but but uh here you can have M number of a rows here you can have in vectors you can have M number of rows you can have M number of rows but and you can and in vectors you have only one number you have only one column okay so so so you can have any number of you can have M number of rows for for for example for example let let me show you a vector U Can Be A B all the way around to the N okay so it can be n dimensional Vector so here our this Vector is two dimensional Vector this Vector is 2D Vector okay and geometrically I showed you by plotting on this XY plane that this is the the two two dimensional plot means first of all X we we we we we go through x x is the four the two units on the xaxis and the four units on the Y AIS and then we and then we uh taken the from the origin and that point okay so so this is this a graph for you but let's take an example eight so this is our Vector a where you going store n dimensional Vector uh it's not key that you should only have two two dimensional Vector you can have n dimensional Vector okay so so but but showing you geometrically you can show three three dimensional Vector geometrically so you can just draw a straight line over here and this is your Z okay so you can plot a threedimensional Vector so so so you can plot it for example you taken K as a vector and you can plot 2 32 on this threedimensional plane or the three threedimensional graph but you can but you can kind plot your you cannot plot your fourdimensional or or five dimens Vector over here so for geometrically understanding I have just just showed you how this Vector looks like but it's not a manner that you can only have a twodimensional vector or or only three dimensional Vector you can have nend dimensional Vector because scientist or researchers most care about your uh or nend dimensional uh your numerically rather than uh most of of course they care about geometrically as well but uh but I just showed you it's not possible for me to draw a FL four dimensional and show you how this how how how we are going to plot but but geometric contribution of a vectors are are we can plot it like this and for example for example you you you have a 2 4 3 so here your K here your K is 3x 1 so first is what are the number of columns which usually denote as three number of columns and you have only one row of course in vectors you can only have one row okay uh okay so so so here you have X component here you have X component here you have y component and here three is your Z component which is in threedimensional and so on okay so this is how you represent vectors so the whole so the whole intuition about vectors so I hope that you understood what I'm trying to convey you over here okay so so so let's so let's see so let's see uh so let's let's go further into understanding uh some more intuitively one last examples of a vectors to to get us what is trying to convey and and and it's it's it's it's much better for for for us to understand okay so I'm going to just just just draw an X and Y plane over here so I'm just just going to draw an X and Y uh X and Y plane like this X and Y and I'm going to take one I'm going to take 2 3 4 5 and six 1 2 3 4 5 okay so this is our X and Y plane now now what what I'm going to do is make it for for example you can you want to plot the vector one 2 okay so how so here you go one unit or X's unit or one unit on the X because this this is your X component this is your X component this is your X component so you go on X unit over here so we'll we'll go till here okay and then two units above so this is your this is your final vector v okay is a final vector v and and to for denoting the Y always the name of the vector should be in lower case with one Arrow Above So this this indicates that it's it's a vector okay so this is this is how you can you have to practice so just try to plot a vector U where X component to be 4x4 okay and a vector can be nend dimensional it can be 6 7 8 9 it can be in dimensional so this is this is how you this is what the vectors are okay so vectors are a two polar array of numbers which we which we just shown shown you today okay so now let's see how we can take out the length of a vector so for example you just draw this Vector so the v v Vector so how do you how can you take out the length of this Vector it's it's a good good good good good way to think about this okay so what I'm going to do now what I'm going to do now is to just remove this and show you uh so take another another example so I'm I'm talking about how do you take out so let's take one example that you have a vector U you have a vector U you have Vector U we have a vector U where your X component is four and 4 by 4 and four okay so here you have here you have a twodimensional Vector twodimensional Vector so you can you can also write w with the member of R2 okay so uh so this is the this is the w u is the member of twodimensional real numbers okay so this is this is this this is your example so what what you want to do is plot the or just let's plot the vector on this okay so 4X 4 I should see over here so X unit four over here and four units of Earth okay and then let's touch this point I think it's wrong bit but no problem this is so how how are you going to take out the length of this Vector this is a good good question to ask to you so for taking out the length of this Vector okay so what you can do here you can see here you can see here you can see that this is also four units this this is also four units and this is also four units so I'm just just going to change the this this is also four units this is also four units okay and you can see this forms a right triangle right triangle at a 90° okay so this forms a right triangle so you know this this is so you know this so which is four units you know this which is four units and you know the this x which is which is here the base four units so here height is 4 units and your base is four units can't you take out the hypotenuse okay or the vector length by using Pythagorean theorem of course you can take out so you can use the Pythagorean theorem you can use the pytha Pythagorean theorem Pythagorean theorem to take out the hypotenuse so for taking the hypoten so the the u² equal to b² + h² okay so you don't know U Square you know b square which is 4 you know the H Square which is 4 okay so U ^2 = 16 + 16 which = 32 and then U ² = 32 now want to U equals to square < TK of 32 that is actually 5.6 5 and and and nearest 100 okay so this is your length of the vector this is the length 5.65 is the length of the vector so so the norm you you usually say the norm okay so in in linear algebra terms so the norm of the vector which is equivalent to length of vector is equals to 5.65 which is your an which is your the length of the vector okay so I hope that you understood what I'm trying to convey with geometric intuition over here okay so what if if we have n dimensional Vector okay so what if we have n dimensional Vector so we will we will see what if we have n dimensional Vector but but let's see let's let's go further let's understand a bit more intuition about um the the how many number of elements what what are the terminologies and then we'll see how do we take out the length of an N dimensional Vector okay so uh just as a notation or terminology or to to remember so that everything is clear everything is clear uh the elements in the vectors are the dimension of the vectors I'm not saying that uh elements it's the the number of elements in the vector is equivalent to the dimensions of your vector okay so the number so the number of elements elements here is numbers okay so elements are usually the numbers like four is an element four is an element okay the dimension are the dimension dimension of the vector of the vector okay so this is the first terminology so here here you can see that you have Vector U which has a and b so here it has two two elements so this so so here is two elements so here it the U the vector U is a 2d Vector is so I I am forgetting is a 2d Vector is a 2d Vector two dimensional Vector so you can plot it if it is three threedimensional you you'll be having a bit difficulty in plotting in a threedimensional plane but you can plot it if it is fourdimensional you cannot plot it on on over here okay so this is a number of elements are the the are the dimensions of your vector okay the next terminology vectors can be n dimensional as I stated the vectors can be vectors can be can be n dimensional Vector n Dimension and dimensional so you can have the U Vector as a b or all the way around to the N okay so here it can be n dimensional Vector so as I stated that it should not be only 2D Vector it can have a n dimensional Vector okay so so as as I left you hey hey how you're going to take out so you take out the length of this Vector U by just uh by just using the Pythagorean theorem but how you how you are going to take out the length of vector U which is an N dimensional so how do you take out the length so the norm of U so I'm talking about this U the norm of U is a square root of U1 2 + U2 2 + u3 squar all the way around to the U N squ okay so it's it's just equals that not nothing much deeper which which we have talked or i u i s okay so this is the this this this is what I want to convey over here this is what I'm going to convey okay so you can actually actually think think about it and in in that way that you want to take for taking the length you just Square U square plus u Square all the way around to the whatever the number of elements in your okay so over here what we were doing we can simply do like this we can simply do over here if if you want to take out you can simply use this of course you are it's just related to some Pythagorean theorem okay so over here you can just add a square root of your b² + h² which is b square is U1 H square is U2 okay means this one four and four okay and then you take out so it's just equivalent whatever whatever we had seen over here okay so so the same way we take out the the dimens or the length of our Vector uh U okay which is an N dimensional Vector I hope that understood till now whatever I taught Okay so so so so let's so let's see so let's see uh bit more so we have studied the how what are the vectors how do we take the length how do we represent the vector v vectors can be n dimensional so now let's see some of the let's do some of the operations on our Vector okay so so we'll start doing the operations on our Vector so I'm just just going to give a headline uh doing operations doing operations on Vector okay on Vector so it's a it's a very great it's it just not too much hard it's very very easy so so the first operation which you want to do is addition of a vector so how are how are we going to do the addition of a vector the first component is addition so you are given you are given so let me State the problem you given the vector the vector a which is 2x 2 and you're given the vector B which is 4X 4 got it you want to what you want to do you want to calculate you want to calculate you want to calculate Vector C by adding a + b means you want to calculate the vector C by taking out the the the uh adding by by summing this to Vector okay so how are we going to sum some this 2x two and 4x 4 means so what you can do you can do the element wise you can do the element so so you can what you can do c c will will be equals to 2 + 4 and 2 + 4 2 + 4 2 + 4 and then also 2 + 4 okay so two element wise addition so it will be nothing but equals to 6 by 6 okay where you add the one you when when you add the vector 2 by 2 by 1 + 2 by 1 2 * 1 which is the the dimension of the resulting Vector will be also 2x 1 okay so what what do you do you you 2x 1 uh which is the dimension of your a vector and then 2 2x 2 * one where the there is two elements and the one one one column so that is also B and then the resulting Vector is 2 * 1 okay so it is the 6X 6 so you get you calculate C to be 66 where your X component is 6 and Y component is 6 so the 66 is your resulting Vector so your resulting Vector C is a twodimensional Vector okay some of the things which I want to highlight is your dimensions of your both the vector which is a and b should match okay if for example for example if your a vector a vector a vector is 2x two and your V vector is 446 then you try to add it the resulting Vector will be undefined will be undefined okay so your dimensions of the addition of vector should match okay otherwise it is undefined operation okay so so this this is this is what I to convey over here so let's see how you add the vectors how you add the vectors uh how you add the vectors geometrically so we seen the numerically how we add the vectors so let's see how you how you add the vectors geometrically okay so let's let's do something let's do something is uh we will do now some geometric because people tend to understand more geometrically rather than numerically okay so we'll understand geometrically so here I'm drawing I'm going to draw One X and Y plane like this okay X and Y plane okay so so let's see so let's see that you that your vector a that this is this this this is this is your vector a so let's let's let's keep uh let's make it total okay so this is so this this this is your vector a and this is your vector B this is your vector v okay this this is your vector v and this this is your vector a you want to add this two Vector geometrically speaking you want to add add this Vector a plus Vector B okay you want to add this so when you do this geometrically okay so what what you going to do you're going to take this Vector a so I'm just just just going to say you take this Vector a take this Vector a and put the tail of this Vector the tail of this Vector onto the onto the top of the vector B okay you put the tail of this Vector onto the top of the vector B okay and then you put this this this one onto the head of this okay what what you want to do is uh take this VOR whatever whatever the length okay whatever may be the length you just put it like this I'm I'm not drawing correct but no problem in that so the length of the length of this should should be same as whatever you are doing you're just taking this a vector and putting it over here okay you just um you're just taking this Vector a and putting this tail onto the head and and then you are and then you are putting it over here and then what what you do you take this vector v you take this Vector B and put the ta onto the top of a and put the and and just match this uh with this okay so take a vector B and then you just put it like this I think it's not correct too much but this is how you are going to do you first to take the vector a put it on the the the tail of that on the head of B and then you whatever the length you just uh you just put put that over here and then you take this and that and then put put put this state onto the head of this and then you match the heads okay and then and then and then the head the the the resulting Vector so the resulting Vector so the resulting Vector I'm just just just going to take this so so you just take you just this is this will be this will be this will be your resulting Vector we are blue one in the case A+ b a vector plus b Vector okay so this is this this this is how you add the vector geometrically and then it makes sense as well okay if Ison don't worry let's see one more example to make more clarity okay so over here which you let's plot so this is how you just take this and then you draw and then you are done with this and then you attach the the here's the tail on the origin okay and then you attach and then you draw a straight line on like that okay so this this is called the parallelogram method okay so this is called the parallelogram this this is called the parallelogram method for showing the geometrically for showing the geometrically let's see triangle method that that will make make more even sense okay so that will make more even sense so here is your vector a here is your vector a and here is your vector v here is your vector v okay here is your vector v you want to add this up so what do you do you simply you simply what you what you don't do anything extra you simply attach this to make a triangle to make a triangle like this and then this resulting Vector is your addition of the two vectors okay so this the resulting Vector which is which I'm going to highlight with this is your resulting vector and this is this is the this this this is how you do the addition geometrically speaking okay so this this is called the triangle method this is called the triangle method for addition of two vectors okay try to play with it a much much more better way so that it could make sense to you as well okay so try try try to play with it try to draw some diagrams of it and then show and then try try try to little bit juggle with it and then then and then you will better better understanding rather than uh just seeing okay so you can you will be getting some assignments on this as well so you can approach the assignments uh in geometrically speaking okay so so let's see one more operations which is the vector subtraction Vector subtraction okay so I'm just just going to show you Vector subtraction subtraction so what this Vector sub subtraction will do so let's say you want to subtract Vector U minus minus vctor V okay so that will be simply we can frame it as an addition of a vector U plus minus B okay so then it will be much easier to show it geometrically speaking okay got it so what I'm going to say over here is you have a vector so you can simply do like this for showing it geometrically now it will very easy to show it geometrically like this by adding of the vector okay so what what what what you do you just make this and then just for for showing it geometrically speaking okay so for example you have a vector U which is 2x3 and we have a vector and and you have vector v which is 1x 1 subtracted so resulting Vector the resulting Vector will be 2 1 and 3 1 that will be nothing but equals to 2 okay it's 1 2 okay so X component y component so this is your resulting Vector okay so let's let's see how it looks like uh so it it would make more sense to you as well okay so let's assume 1 2 3 okay 1 1 okay so let's plot the U Vector so I'm just just going to plot the U Vector which is 23 okay okay so okay it's two now so it's 2 three so okay I think I done wrong it's 2 three so just going to okay so this is your vector U so this is your vector U this this this is your vector U and you want to plot the vector v v so this is your vector v this this is your vector v okay this is your vector v and the resulting Vector is 1 by 2 okay so this is your resulting Vector which is your after subtracting okay so after you subtract uh uh this from this the vector v from U okay so that will be your resulting Vector that makes sense geometrically speaking as well okay so this is how you do the vector subtraction geometrically if you don't understand geometrically it's no worries but it's not more than tough which which which is is very very easy not more than tough okay so this is your vector addition and Vector subtraction so let's see the last concept which you which I to make you familiar with is is is Vector scalar multiplication Vector scalar multiplication Vector scalar multiplication so I'm just but but first of all what is in scalar what is an scalar so scalers are give scalar such as constant or a numbers for for for example four is in scalar two is in scalar one is in scaler or or or or anything okay so this is scalers are just constant okay so it is it's a number but in algebra or linear algebra term we call it as a scalar okay so it plays an important role when we stud about L linear combinations or linear Transformations it plays a very important role so so so this is so what if if you multiply a scalar for example you have you want to multiply scalar a times the vector U you times the vector U so let's let's take the you you take the scalar a to be two and the Vector U as a 2x2 okay so multiply 2 * 2x 2 so it 2x2 so it do the element wise Product 2 * 2 4 by 4 that will be equals to 2 * 1 okay so Dimensions will be 2 * 1 that that is simply the vector scalar multiplication so what it actually does in geometrically speaking it stretches the vector so it doubles the vector so for example you have this Vector 2x two okay and then you multiply with two then it will be doubled then then it will be stretched okay then then it will be stretched means transformed the vector okay or stretched the vector by by following the linear structure it stretched the vector like like this so this this this was your initial Vector so after applying the a times the vector U that is stretch which is your final Vector after applying your uh the scalar and Vector multiplication okay so this is your stretch stretched Vector so please please please please ensure that it's stretched so before applying it was U and after applying is doubled okay so this is this is this is the vector scalar multiplication and the reason why we are studying these because this helps to build a very good foundation the stretching the geometrical speaking this helps a very good foundation when when we talk about Transformations con combination combinations uh igen values I vectors these these plays an important role in that so so this was uh so so so we have seen a bit about vector and scalar multiplication the last concept the last two concept which I want to introduce to you is the unit vectors is the unit vectors is the unit vectors and the zero Vector okay so the unit Vector so unit Vector is any Vector with a length one so so the definition States the unit Vector the unit Vector is any Vector is any Vector for for example any Vector okay any Vector with length one whose length is whose length is one whose length is one that's a unit Vector okay what is a Zero's Vector Z's Vector is whose length is zero whose length is whose length is whose length is zero whose length is zero okay so that is the zero Vector you usually denote the z z Vector in very bold way okay that is the zero vector and that is the unit vector vector and vector okay so these are the two basic basic very very basic ter terminology which you need to know about okay so so we have seen a lot about our vectors in this video so I hope that you understood every Everything whatever we had have have have a talk on this so just just just to make sure that everyone understood this so what what what you actually do so let me show you one one more example of that so so for example you have this okay so you have this Vector a and you have this vector and you have this Vector B okay so what do you do for addition of these two vectors you can just you can what you can do you can just uh make this make this a triangle okay but let's approach within using a parallelogram method okay so what do you do you put you take this you take this vector and put on the this tail out of the head and then you just draw a parallel to this and you take take this and then you take this and then you join the and this this A+ B is a result in Vector okay so this is how you go further into approaching these stuffs and I hope and maybe you can solve it using a triangle method to show you how it works geometrically speaking okay so I hope that you understood whatever I'm trying to convey you over here okay so I hope that you understood geometric intuition the triangle method what the unit what are zeros what are scalers what are what are what are vectors and etc etc etc okay so I think that we are done with this lecture um on it's 30 minutes so I hope that you understood vectors what are vectors uh so you you can find the notes of this the lecture notes maybe all of these in the description round box below uh the the or in LMS and the assignments will will be also related to this will will be released at the end of this week and I hope and I really really really hope that you understood this and if if not please feel free to ask the question in description down box below or in our Discord server we are we will be very really happy to answer your questions the next announcement is you can simply uh the next the next lecture will will be based on matrices okay so we'll talk about a bit about matrices okay and then and then we'll talk about after after completing a bit of mat matrices we'll talk about linear combinations then trans linear Transformations okay so we will be studying these things so don't worry we'll gohe very slow pie and very easy way okay so thanks for seeing this video I'll be catching up you in the next video till then byebye and have a good day hey everyone in this lecture we'll be talking about mates in our pre previous lecture we talked about vectors and I really really really hope that tat to understood about vectors I know this these are very very easy concept for you but but uh let me tell you these sets a foundation when we study about combinations or Transformations and other other stuffs so that's how we are prely focusing on this so from this video we'll be starting stepping up a bit difficult note from matrices and then we'll talking about some some matrices operations and then we'll talk about some properties of matrices multiplication and then I will just end up with this video with Matrix Vector products and then I will show you the wide results at the end of the video that the linear combination of the column Vector okay so we'll be talking about I'll just introducing a notion of a linear combination so that in the next video is totally based upon your linear combination so that's why I will just give you a taste of linear combination at the end of the video so let's get started with matrices so today we'll be talking about matrices so let's let's recall a bit about vectors so so vectors are uh n dimensional where where it can have n and where where it can have n number of rows but only one column okay so we were having this it can have one two all the way around to the n and here this is n * 1 so the shape of this vector v the shape of this vector v is n * 1 and it can have n number of a rows and one number of a column and this is a vector specifically we can call this as a column Vector okay so so so we specifically call this as a column Vector okay so so it is given a new name and when we when I will introduce you a notion of a matrices then then we will use extensively in the in the later videos but but this is also called the column vector and and for example your vector can be in this 1 2 3 all the way around to the end okay so this is called the row Vector this is called the row Vector okay okay so this is called the row Vector so this for this these are the two two things which I want to in introduce to you so we'll be covering this again just after we complete the matrices okay so let's start with what are matrices so matrices are a are a set of numbers or or a multidimensional l okay where it can have n number of rows or n number of rows and M number of columns it can have a multiple columns and multiple rows okay so in vectors we we in vectors in vectors we were have having only we were having only n n rows and one column which is the example of the example will be 1 2 3 okay so this this this is an example of a vector v okay so the matrices so the matrices can have can have n number of rows as usual but n number of columns but can have M number for column so as an example we can make that Matrix a equals to 1 2 three so one column 2 3 4 second column 3 4 6 third column so it can have M number of M number of a row M number of columns and and and N number of a rows and N number of a rows okay so here the shape of this a is 3x 3 uh where three is number of a rows number of a rows and this one is number of a columns number of columns okay so so over here this this first first indate the 3X3 Matrix so this is the example is 3x3 Matrix so the formal definition of a matrix is is matrices are a set of numbers okay mates are a set of numbers arranged in a rows and a columns which is n rows and N columns so to form a rectangular array okay so here here it on the rectangular array in other words matrices can have n number of columns and M number of rows okay so let let me write a formal definition of matrices over here so and the definition which is just let me write the a good definition of this so that everyone can can Define what a matrix is so matri so here's the definition of a matrices where it it it is arranged in a rows and a column so I'm going to give the name of a rows going to give the name of a rows to n and columns to M so to form a rectangular l so for for for example we can have here it can have a b c d e f g h i okay so it's can have any here we have this is this this is an example of 3x3 Matrix where we already have one two three rows and one two three columns okay or we can say in other words it's going to have n number of rows and N number of columns M number of columns okay so I'm again I'm saying n is for number of rows number of rows and M is for number of columns number of columns all those are in small small letters okay so this is your form formal definition of a matris and the lecture notes is in description box below please go there and assess your lecture notes for you to better to to just revise in the meantime Okay so so so just let's write a formal notation of how the maor matrices are so that so that it it is uh easily inter interpretable so I'm going to make a matrix a going to make a matrix a where I'm going to make a matrix a where it I'm going to make this a11 so the how do we assess the the first element so here it is in first row and First Column so this a is in first row and First Column so that's why I written I and Z okay so a i and Z indicates I what is the row number and g z what is the column number so for example for assessing the elements so over here you have this you have this Matrix we have this Matrix a and what you going to do is assess three so how do you want to assess so the you the formal the formal assessing things is a i j or or yeah so a i j where I indicates the row number and J indicates the the column numb okay so over here you can see the three is on is on we start with 1 2 3 okay not from zero so 2 2 A 2 means a A2 means the row number is two and the column number is also two column number is two that is nothing but equals to three okay so here's how you assist so first of all you write the row number then you write the column number so I indicates the row number and J indicates the column number okay so I hope that you understood what I'm trying to say so it is telling go and this is the first element where is first row and First Column then it is second first row and second column then uh all the way down to the first row and N column okay or M column okay and over here it can have 8 2 1 okay so second row First Column second row second column all the way down to the A2 m m second row M column okay uh so uh this can be a31 a32 a3m here all the way around to the a N1 so n is number of rows or a N2 all the way down to the all the way down to the a n m okay so here it is the formal notation or or a definition which we can write over here which is the formal notation for writing uh so so this is a matrix okay so here's how I developed this so it can have M number M number of a column A rows n number of rows so 1 2 3 4 all the way down to the N which is n number of rows and it can have only it can have only M number of columns okay it can have only M number of columns so this is your formal formal formal definition which you have which I have given to you for uh matrices okay so I hope that you understood now what I'm going to talk about is I introduced a notion of a row vector or a column Vector is it so I introduce you so can you just go and just type me what is a row vector and what is a column Vector so let's take one example so let's take one example is you have a you have a matrix but just just just one thing that the vectors are subset of matrices okay so the the the vectors are a n * 1 Matrix or n * 1 matrices okay so the vectors are a subset of matrices okay so if you if you extract this extract this extract this row that this is just a vector okay so what I'm going to do now is uh is going to just make a vector make a matrix a make a matrix a which contains just just don't relate I'm just taking examples I'm just taking examples you have a vector a I'm going to store 1 2 3 4 5 6 okay so this is my this is my 2x3 Matrix okay so so what you do you take the first row okay and then stored in another so C okay that is 1 2 3 okay so what is C over here C is called the the the the the the the Matrix with one row okay the Matrix with one row is called the row Vector this is called the row Vector this is called called the row Vector The Matrix which has only one row is called row vector and the Matrix with only one colum which is nothing but called a column Vector okay so so so if we if you take this 1x4 okay in D that is 1x 4 it has only one column that is nothing but equals to column Vector which is nothing but is column Vector okay I I hope that you are understanding whatever I'm trying trying to tell here so so these are the size of a matrix over of size of a matrix where we have row vector and what is row Vector row Vector are nothing but the Matrix with one row with one row is called the row vector and the matrices with one column is called the column Vector okay so if you take one example so just just just just assume that this is your this is so this is your first of all row Vector sorry column column Vector because you have uh you have so so you have sorry column vector so this this this this one is a column Vector which is V1 so V V1 over here is a column Vector okay so if we take this one if we take this one so here you have only one row so that is B2 which is your row Vector okay so this is this is what the notation of the notion of a row row vector and column Vector means and I really really hope that you understood about this okay so if not please please feel free to ask ask a question below where you're stu please please please use Discord server or whatever that's the doubt support which is provided to you so that you can get most out of out of this course and if you need any guidance for absolutely free please feel free to reach out to me via email Discord comment we can get on a meet to help you solve the doubts it's it's for Okay cool so let's so we have we have seen what some matrices are so just just want to Recaps youate everything so matrices are a set of numbers arranged in a number of rows and number of column s to form a rectangular array where it can have an m n number for rows and M for mango number for columns okay so so the notation for for the definition of geometrically over here is the a so I I have made this as an example the show showcase shoe and we have we have seen some of the Matrix size where you where the the the terminology which is where the Matrix has only one row that's the row vector and with the Matrix has only one column that is a column Vector okay so I hope that you understood what I'm trying to convey over here cool so now let's talk about so now let's talk about so now let's talk about some of the operations because in pre previous video we talked about vectors and then operations so the same day I'm going to talk about operations on a matrices okay so operations on matrices just just just just just going to show you show it to you operations operations I think my handwriting is not good too much no problem operations operations uh on matrices okay so you want to do the operations on matrices so the first operation which you're going to do is Matrix and a scalar multiplication okay so what I'm going to do is Matrix Matrix scalar multiplication okay not a not a big deal it's very very easy to understand okay so assume that you have a matrix a you have a matrix a you have a matrix a 10 6 4 3 which is your nothing but a 2x2 matrix okay 2 rows and two columns okay and you have a scalar you have a scalar a which is nothing but two okay so you're given these two now what you want to do is uh you want to multiply uh a a okay so we multiply a scaler with a matrix which nothing but 2 * 10 6 43 okay that is so what will be the result please anyone please please please feel free to to pause this video so what you're going to do is multiplying a scaler with a 2x2 matrix so what will be the result anyone please please please in the comment okay how how do you tell okay no problem but please please free to ask say in the comment box I will be very happy to see the if you're still here so the answer of this is first of all what do you do you you simply do the element wise product with this scalar okay so 2 * 10 which is nothing but 20 which is nothing but 20 2 * 6 which is nothing but 12 2 * 4 what it is 9 no it's 8 okay 2 * 3 which is nothing but six okay so the resulting the resulting Matrix is 2 * 2 Matrix so what you do you transform or or or not a transform I would say you just multiply a scalar with a a matrix and and then you get a 2x2 matrix which is the same size of your a okay so 2 * 2 = to 2 * 2 which result when you multiply with any scaler okay so what it does is simply do the element wise produ product okay so the so the formul notation for this is you you multiply C with with a okay and a have I and J row okay so what it will do it will simply M do the element wise a c * a i j that is nothing but what it will do first of all it will uh so in more more notational terms it will just for for for example you have a vector you have a a scaled a and you multiply with c d e f okay so what do you do you simply uh a * C A * d a * E and A * F okay so this this will be the form this is this this will thing and then you'll be getting some values 2 by two okay which with your values okay the same this is this is what it is doing so this is your formal definition formal definition of your Matrix Vector multi multiplication sorry uh scalar matrix multiplication and I really really hope that you understood this let's go on to the next operation which want to see let's go on the next operation which want to see is addition of our matrices okay so the next operation which we are going to cover is addition addition of matrices okay additions of a matrices so let's Zoom so we are you are you are given a matrix a you give a matrix a which is 1 3 1 uh 1 0 0 okay and you have and you are given Matrix B okay and Matrix are always written in a capital letters make sure and the vectors are always in small letters okay with one over here uh and the scalers are also in like this yeah so that's B is 05 75 okay so what you want to do you want to add these two matrices A + B add this two matrices so this is the operations which you want to do this is the operation which you want to perform so how how are you going to perform this operation how are you going to perform this operation so for performing this operation so for performing this operation you will just what what you will do you wanted to just have this 13 1 1 0 0 plus 05 750 what you what you will do you will nothing but uh 1 + 0 at element y some okay in scaler you doing so you will do the same 1 + 0 3 + 0 1 + 5 okay element wise sum okay 1 + 7 0 + 5 0 + 0 okay so this is this this is what you do and then you'll be getting your answer which is 1 3 6 6 and then you getting 8 5 0 that is a resulting U Matrix okay which is also 3x3 Matrix okay so what you do you simply do do this and you're getting a 3X3 Matrix okay so so when you what you do you you just added a 3X3 Matrix 3x3 Matrix and then and then you and the resulting Matrix is also 3x3 okay so so this is how you do the addition of a matrix and the formal definition for this I which which which which I can state so because definition is very very important for fundamentals for for making your fundamental strong okay so the definition is your given a matrix a which can have a uh b c d e f okay that is nothing and your B is also some some kind of k g i h o p okay so add these two so you what what you will do a + k b + G C + I D+ D + h e + o e + O and F + P that will be nothing but equals to 3x3 Matrix okay so I have just taken example your your Matrix can have n dimensional the can have any number of rows and any number of columns but there are some constraints which you you need to there are some properties like Dimension property which you have to take care while adding the matrices okay so it can have it can have uh ABC you can it can have a 10 x 10 matrix the size of the Matrix and then you are adding the 10 x 10 matrix with another 10 by 10 matrix and that would resulting in another 10 by 10 metri okay so that is the that is the thing uh so some of the property which I want to highlight which you all have to focus on so some of the properties of a very very very very uh simp simp simple property that commutative property commutative property so addition of a matrix is commutative commutative and all of these is written in your notes please please please feel free to write see from there if you if you wanted to just just just revise it up okay uh in in future but I would highly highly recommend to complete this video V plus so you so you have a matx say you have a matrix you can do B+ a okay uh the next thing is associated property your your your your addition of a matrices are associ associative as well okay so associative associative property associated property I'm not writing a lot of properties over here but the one who are important I'm writing over here A + B+ C which is nothing but equals to A + B plus C and all of these can be proved very very easy the proof is very very easy not a hard please feel free to search on internet about the proof it is very very easy associative commutative it's it's the proof are available on internet okay so so the last thing what I'm which the first property is commutative the second is associative the last one is dimension property the dimensions are be the dimensions the dimensions the dimensions of the dimensions of your of your uh uh the two two two Matrix would be same The Matrix the matrices would be same the matri should be same okay the dimensions okay if it is not then then it will be undefined your operations will be undefined Okay cool so so now we have we have we have talked about one one of the operation which is addition of a matrix and I really really hope that you understood addition you you understood a scaler but one thing which I'm going to spend some some two minutes talking about is you may be thinking yeah you do we really really need to know about these stuffs uh so I would say yeah you need to know about although you don't need to just worry about how I'm going to code it you can actually develop it from very very scratch not a big deal but there are some libraries like numai which would be in this course we'll be using in this course or or pytorch that'll be doing using uh the LI library for S computation for addition of a matrix for multiplication of a matrix which they handle which they are very efficient okay because in real world your Matrix are not 3x3 Matrix they are they are they are billions size size is millions okay Millions by millions so so they are they are very very large so so so Maj mag multiplication with your own for Loops are very very time taken okay so that's that's that that's a big deal okay so your your time complexity will increase as your input size increases okay so that's a big deal so you we are learning this to understand the inner workings of our of our function so that we can we can know how how our algorithm is doing and how everything is working behind so that it it becomes very easy to debug something or you get some error or or or to to have very good or decent knowledge of what your code is performing Okay cool so addition of a matrix is also done now let's you can do you can do the same with sub subtraction of a matrices please try it out by your own the next thing which which I'm going to spend some time talking about is matrix multiplication okay bit bit I'll spend some time talking on this okay so the first the next next thing which I'm going to to talk about is Matrix uh Matrix Matrix multiplication Matrix Matrix multiplication okay so this is your uh next operation which is which is one of the most important important uh important what do you say uh the operations which you which you need to learn okay so you given a matrix a you're given a matrix a I'm going to take very easy example 1 7 2 4 okay and you're given a matrix B given a matrix B which is 3 5 3 2 okay now you need to now what you need to do you want to multiply Matrix a matrix B okay so how do you do you may be thinking here you here is 2x two here is 2x two is 2 by 2 1 * 3 7 * 3 2 * 5 that's that's that's that's that's not how you do okay so the matrix multiplication the way you do is like this you take the first row of that a matrix you take the first row I'm going to change my pen you take the first row of that a matrix and multiply with the multiply with the First Column of the b b Matrix multiply with the First Column with the B Matrix okay so so here's here's how you do so resulting resulting M Matrix c will 1 * 3 okay plus taking the dot product of your of your row Vector times the column Vector so what you what you are actually doing is taking out the dot product dot product which you'll see in our later videos don't don't worry about that dot product of this this is your because if if you see this is your it this it it has only one row okay so of a vector of a vector of a of a vector of a row Vector of a row row vector and column Vector column Vector so this this is your column Vector where it has only one column so specifically you're taking out a DOT product of a row vector and a column Vector what is dot product so dot product is element one so what what you do you simply multiply and add it up but element wise adding and add it up okay so over here what what what you want to do you one * 3 and 7 * 5 1 * 3 7 * 5 and then you will add it okay now take this again this row again this row with this row row vector and multiply with this column Vector multiply with this column Vector so that that will be nothing but equals to 1 * 3 + 7 * 2 okay now what do you do now what do you do you you now you have this The Taking of the dot product of row row vector and the second column VOR which is V2 okay now what do you do you go go go further into this the second row Vector which is 2x4 and then you do the same 2 * 3 dot product of the 2 4 with row Vector with the column Vector okay 4 * 5 okay and then 2 * 3 4 * 5 2 * 3 2 * 2 okay so 2 * 3 + 4 * 2 okay that will be nothing but equals to C which is nothing equals to uh 1 * 3 which which will be how much 1 1 * 3 which will be 3 + 5 35s okay 1 * 3 3 + 14 2 * 3 6 + uh 20 which is 26 2 * 3 6 + 8 which is nothing but 14 okay cool so this is your resulting and then what you do you simply make your 38 17 26 14 will be your resulting Vector which is 2x2 matrix which is the 2x2 matrix okay so when you multiply with this you will be getting uh your favorite the after after multiplication of the Matrix this this is your answer of this of your particular question got it and I really really hope that you are that you understanding what I'm what what whatever I'm trying to say but you it may you you can use you it can have any dimensional but some properties are there okay so let's let's visit the property I'm just going to constraint that Dimension property is very important in this so what dimensions should match to be so that the matrix multiplication is not undefined okay so so the properties of for MRI multiplication so the first property which I'm going to talk about property the first property is for for example you're given a matrix a b and c so these are three Matrix which are n byn Matrix which are n by n Matrix where you have n n byn Matrix okay so where you have n rows and N columns okay so the first thing which holds is commutative property of multiplication of this multiplication does does not hold okay so M multiplication is not commutative is not commutative is not is not okay so when you multiply a b ba a which is which will be totally wrong it can be proved it can prove it it can prove very easily then the next property associative property of matrix multiplication is is is there okay so associative property associative associative property is there okay so a b + C which is nothing but uh a okay so I think it's I have written for this distributive I written for a c which is nothing but equals to A B C okay it can prove rigorously and of course you're multiplying it over here okay it can it can prove very very easily it is distributive property distributive property this this it is also distributive property so what you want to do a B+ C that that will be A+ a c and you can prove these you can prove this very very easily which which can be found on internet the next thing is the most important Dimension property Dimension property Dimension property so the dimension property is you can have M number of a rows you can have M number of rows you can have any number of rows but an N number of columns okay this is for the dimension of a matrix a your dimension of a matrix B should be your your your number of a rows should be same as uh number of columns in that Matrix okay times K any okay so your resulting will be M * K okay so so it makes sense as well if you have you can have M number of rows at the a a matrix but you can and N number of columns okay but with here you can have only n number of columns n number of a n number of rows sorry n number of rows okay so here you have two 2 by two so here it has two and the resulting will be the resulting will be 2 by 2 Vector sorry Matrix okay so that the the the resulting size will be this okay so this is your dimension property of your matrix multiplication cool so we have talked a lot about Matrix and and and and you seeing that you're are going and then and you are seeing that that you are going bit up bit bit little bit little bit up so the last thing which I will end this video which I promised you is is talking about Matrix Vector product is a matrix Vector multiplication okay so what I'm going to talk about is Matrix Vector multiplication yeah so let's let's let's let's do that then so let's do that so you have a matrix a you have a matrix a which is nothing but which is nothing but so I'm just just going to define rigorous very very definition of Matrix Vector product so I'm just just just going to write it very very fast a11 a12 all the way around to the A1 n and a12 a22 A2 n uh a M1 which is is N1 okay N2 okay that be A and M okay so this is your Matrix this this is your Matrix so it is having M number of columns and N number of a rows okay this is your Matrix a and you want to multiply with multiply with uh a vector a column a row L row a column Vector okay X2 XM XM okay so to do the multiplication of it so how do we do it how you how you how you will will will you do it so you want to multiply a matrix which is a which is M * n Matrix times the vector a scalar uh sorry Vector col color Vector X which will be the definition will be so so what what it be the answer of this so how will you perform the so for performing the operations is nothing but equals to a11 X1 so what you are actually doing you can take this a11 okay and you multiply this with this multiply this with this okay so what you actually doing what you what you are actually doing you're are multiplying you're multiplying the the column Vector the column sorry row Vector sorry row Vector to the column Vector okay element wise and adding it up okay so the dot product between your row vector and a column vector and adding it up okay so a11 plus you're adding plus over here see a12 X2 plus A1 3 X3 all the way around to the A1 M A1 M * x m XM okay then you do the same A1 2 X1 + a uh plus A2 all the way around to the A2 uh n okay so you're multiplying with X whatever the X so what what you're actually doing you're multiplying you're taking this column Vector taking this column Vector so row vector and multiplying with this so you are taking on the dot product between taking all the dot product taking all the dot Dr taking out the dot product and giving your answer that's it okay taking taking out the dot product of between the row vector and the column Vector so let's see with one of one of the example so it it it would make more sense so here you have 3 0 3 2 1 7 1 9 okay and you have a vector column Vector 2 3 4 1 okay so what what will be the output so the answer will be uh minus 3 * so so we taking this you're taking this and multiplying with this okay 3 * 2 + 0 * 3 okay 3 it's minus + 3 * 4 + 2 * 1 it's yeah it's 1 okay now you go for the 1 * 2 1 * 2 + 7 * 3 + 1 * * 4 + 9 * 1 that will be nothing but equals to after after you add it everything after calculating okay after calculating that will be a and b which is 2x 2 2x 2 sorry 2x one vector so which is a column Vector which is 2x one so after after after doing the Matrix Vector you transform you what what you do you have this R4 R4 we is four dimensional vector you transform it to D after doing the after using this V using this Matrix say you transform it into a a and b which is the 2x1 okay which is from R4 to R2 okay so here you transformed using this Matrix a so so so we'll see in our four videos that matrix multiplication are a linear transformation okay so we'll see in our later videos but but now as of now I I hope that you understood the mutrix Vector product okay so in the next video which what I'll be showing you a wired thing over here or not a vired thing a very useful thing over here is the linear combination using the help of Matrix Vector product so I will take an example of matric vector product as a linear combination of uh of so that it would make more sense and then we'll complete the linear combination now in in our next video and then and then we'll end up uh this uh uh so we'll be completing and then then we talking about the linear transformation I really really hope that that you understood this I'll be catching up you in the next video till then byebye have a great day and and please please and one more thing attendance is you have to mark your attendance so please please feel you to do so byebye have have a great day okay so welcome to this lecture in this lecture we we'll be discussing about linear combination of a vector so and this is this is one of the most important concept as you will go further and you will understand okay so it sets up a very strong fundamentals to mathematically understand or to see the Deep learning or machine learning into a linear algebra point of view so this is one of the most important concept which we'll focus on so in this video we'll talk about that specifically so uh as so I'll in this video I'll just give you I'll be giving you some definition of linear combination and then I will giving you some some examples and then we're talking about a matrix Vector product as a l linear combination because in previous video at last we talked about Matrix Vector product so that's why we are in this video we'll be talking about the as an example taking that as an example for linear combination of our Matrix Vector problem okay so so the definition States so the definition of a linear combination States uh so let's let's do something let's start with an example let's start with an example and example States and an example States key that you have so first example that I want take is you have a scalar you have a scalar and you multiply with the sum Vector U okay and then you have and then plus two with some vector v okay and that will be some resulting Vector that will be some resulting vector which is just the scale version of u and v which is nothing but uh for example D okay so that is the resulting Vector of our uh after applying off first of all we we added it up sorry mult M multiplied and added the vector okay so that that will be the resulting Vector so the resulting Vector is a linear combination of vector U and a v okay so again listen me that what you we we have taken this example and and this in this example we have one scalar three and then we multiply with the vector U so for example we can have a vector U to to be 2x two so when when you multiply three times okay that would be nothing but 3 six six that would be the six okay 6 six okay plus you have some some Vector B it is nothing but 4 4 okay so 2 * 4 so when when you multiply or do the scalar scalar Vector multiplication that will be simply element wise product so 2 * 4 which is 8 and 2 * 4 which is 8 okay so when you uh now you add 8 by 8 okay so that will be 8 by 8 that will be nothing but 8 by 8 okay so the resulting Vector will be 14 by 14 okay so that the the resulting Vector will be 14 by 14 which is your D which is your vector D so this the vector D is a linear combination of your vector U of your vector U and of your vector v okay so the the the resulting Vector is a linear combination of these two Vector which is u and v so I hope that you that that you are understanding what what whatever I'm trying to tell and this 14 is just the the 6 + 8 which is okay so this is the D is a result resulting Vector uh after after you do so what you specifically done is multiply there is some scalar so there is some scaler so you are given any number of a vector so the I'm writing the definition so what you specifically done you're given you're given any number any number of any number of vectors we given any number of vectors and the linear combination linear combination linear combination of the vector of the vectors so you are given any number of a vector but the linear combination of this Vector means these vectors are simply the result of are simply the result when we multiply when we when we multiply when we multiply each Vector each vector by a scalar scalar and add the vector vectors so so how you get the linear combination is you are given any number of vectors okay so you are given any numbers of vectors and the linear combination of those vectors the given vectors is simply the result is this in this case the result of when the the result when you multiply when you multiply each Vector which is you are you are given u and v over here so when you multiply each each vector by a given scalar which in in this case is three and two okay and add them up that the resulting Vector is your linear combination of those vectors okay the formal notation which I can write is you are given a vector a you are given you are you are given a vector B you are given a vector C you you given a vector D okay so you want to find out the linear combination of these vectors so so the linear combination of these vectors will be simply uh when you multiply with some scalar okay with some scalar so I'm just going to write it uh uh a okay so this so let me write a different name of this so I will just so your given given vectors given vectors are are I think uh v u g okay so this these these three are your uh given Vector okay uh now what you do uh you simply multiply with some scalar a okay so so what what will be the so we want to ask what will be the linear combination linear combination linear combination of these vectors so what is linear combinations so the definition states that a linear combination is the result when you multiply the given vectors by some scalar and add the vector okay so that is the resulting vector so what you do simply multiply a scalar with a vector plus b with the U Vector plus C with a z Vector the resulting Vector D which will be nothing but your linear combination of v u and G okay so that will be the resulting that will be the linear combination so let's see more of the example to get comfortable with this so that you could get uh you could get a good feeling okay this is the linear combination Okay so so another example can be you can have a vector U you can have a vector U and you can have a vector v you can have a vector v so so what you do you it can be like this any number minus one * U Vector which is a scaler plus z b the answer whatever the resulting Vector will be will be the linear combination of these two vectors which are the given vectors okay so it can be fraction as well your it for the the the scalars can be fraction and fraction as well 51 7 by 11 it can multiply with some some some some some Vector U and 1 195 by 2 with some vector v the resulting Vector the resulting Vector the resulting Vector will be your linear combination of these two vectors which is U and which is V okay so whatever the resulting Vector is will be the linear combination of u and v Vector okay so let's take one formal example so that we we could understand this much much better okay so one formal example of linear combination say you given a u uh say you given a vector U say you given a vector u u which is which is uh which is two two dimensional Vector which is a 2x1 vector or we can say it's a column Vector minus 5 by 0 because column Vector is what what the the vector the the matjes is with only one column and over here this is we have only one column so so so so that's why it's called a column Vector so that's why we are telling it to the column Vector please see the previous video to help us understand to help you understand much more better way okay uh and you have a vector and you have a vector v which is 02 okay so you so you given a vector u and v okay this is also a column Vector this this is also a column Vector so what you want to do is to take out the linear combination of these two vector and and and the linear combination can be uh we can multiply this this this is this uh U Vector so what we can do we can multiply any scalar with this U Vector plus any scalar with this V Vector we will'll be getting the linear combination of these two vectors so let's take an example so let's let's take one example you multiply 1 with 5 0 okay 5 0 plus uh you multiply you you you have a scalar one which is place of v and you have 0 by 02 okay that is your VV and the resulting Vector which will be nothing but 5x2 okay and this is the linear combination of these two vectors and you can you can you can go ahead you can try it at different different scalers and and the same will be so you triy first you can try different different scalar with two with the a 5 0 + 2 it it can be any number 02 that will be nothing but when you do do this to which is 10 and then 10 0 + 0 4 that will be nothing what equals to 10 4 so this is 104 is a linear combination of these two Vector yeah you heard me correct this is the lar combination of these two Vector as well as this is the lar combination of these two vectors you heard me correct it can be you can multiply with some scalar 4 okay with 50 0 Plus plus I think okay you can do with one uh 02 the resulting Vector whatever the resulting Vector after doing this a will be the linear combination of these two vectors you heard me correct yeah exactly so your linear combination will be is it it it can be it it can be anything okay after the whatever means you take any scaler multiply with a given Vector uh you will and add it up you will be getting a linear combination so linear combination cannot be over uh some some finite over here okay except some exceptions which are there okay so over here linear combinations are said to be a vector if there exist a scalar a b okay and then whatever the resulting Vector will be will be the linear combination of those two vectors okay do not think that linear combinations can be one one is line combination of any two Vector can be only one no it can be anything it it can be any number of a linear combination of that two Vector what you need to do simply multiply the vector with some scalar A or B whatever and add it up the resulting Vector will be your linear combination of those two vectors so again I'm writing one formal formal definition so we have already written one formal definition but let's write one definition which will give you a more idea about what we have seen so far so the so the definition States so the definition States a vector R okay so a vector r a vector a vector r a vector R is said to be the linear combination is said to be a linear combination a linear combination a linear combination a linear combination of a b C okay uh a a vector is said to be a linear combination of a vector a b and c Etc okay so a a vector R is said to be the linear combination of these given vectors A B C which in this case this was u and v in this case this this was u and v these are the given these are the vectors okay these are the given vectors so the same way a b c d these are the the given vectors okay if there exists if if there exists if there exists scalers if there exist scalers x y z Etc whatever the means whatever how whatever the number of your vector is such that such that your resulting Vector is equals to x a plus YB zc all the way around to the n so so a vector R is said to be the linear combination of these vectors of these vectors if you multiply the scalar with a given vectors respectively respectively and the then then the r is said to be the linear combination of these two vectors of these all the vectors so again I'm repeating what's the linear combination means it simply means that it simply means that the the vector R is is is is we can call it as a linear combination okay how we can call we we are given a vector we are given these vectors we are given these vectors and if there exists exists some scalar and such that in such a way that your that the resulting Vector is equals to the multi the the the r is the is the multi when you multiply a scalar with a vector and add add the add the vector the resulting Vector will be your linear combination of that uh given vectors okay so let's take one example one simple simple example is uh is let's you want to you want to say is 1x 4 so you're you're you're given a vector U so I'm just going to take take one example so that everything is um make you understandable so your your your example you you're given a vector U you're given a vector U which is 5 which is a two two dimensional vector and you given a vector v you're given a vector v you're G given a vector v which is 02 which is 02 so you want to show you want to show is is your 1x4 which is your R the vector R is the is also a linear combination is also the linear combination combination of vector U and vector v so you want to show is this Vector is the linear combination of these two Vector okay you want to show this you want to show this this is a problem so you want to show this is this the resulting which is r 1 by4 is the linear combination of these two u and v Vector is you want to show it so how you going to show it so for showing it we we will see the systems of equations solving the systems of equations later on but we can I will just give you a tool so so 1X 4 so this will be the resulting Vector so if there exists some scalar which which is 1x 5 so this is this this is my scalar times your vector 5 0 plus there exists the second scalar times the 02 so your resulting Vector is 1x4 and hence and hence this 1x4 is the linear combination of this U and of this V Vector okay so I hope that you that that you are able to understand what's I'm talking about the linear combination of these vectors I hope so okay for a linear combination of a vector U and a vector v and I have given you also the formal definition of a linear combination so so let's let's see one terminology the terminology States terminology States terminology states that terminology states that the constants or the scalers which is here 1.5 this is two so these These are called the weights of the given Vector so in we we don't call it as a scalar we we call it as a weights rather than calling the scaler so if you have seen your if if you have seen your uh your hypothesis function so you have a Theta so that's so so so that is so that is the weights weights of your vector so the same way over here we don't call it as a scalar we call it as a weight of that u and v Vector okay so I hope so that you are getting a point uh to towards linear algebra and viewing machine learning or deep learning into the view of linear algebra okay so I hope that you are understanding so let's take one example to understand the weights so for example for example for example 1 by 5 is your linear combination of vector 1 4+ 1 one when we multiply with the some weight this is called the weight so three that will be the this is the linear combination of these two vectors these two vectors okay and over here over here that 1 4 1 14 which is a vector and vector and 1 1 with weights with weights which is 2 and 3 okay so 1 by 5 is the linear combination of a vector 1 4 and 1 one with weights Min 2 and 3 we don't don't we we don't call it as a scaler we even call it as a weights of that thing okay so I hope so that you are understanding whatever I'm trying to say you over here so I'm just going to talk about one last thing if I have a time I do have a 10 minutes time so what I what what I can talk about today is is the next concept which is the span of a vector okay so I'm just just just going to just make you familiar this is this this is just very easy so the we be talking about a span of a Vector I'll just giving you a small introduction to span and in the next video if possible I can show you some some some geometric intuition of a span okay so what is a span so let me let me write it span of okay so what is span span is a set of all the possible linear combination of a given group of vectors so for example we showed you over here we showed you you can have a multiple linear combination of a given Vector u and v you can have a multiple L your combination are you getting me so you can you can you can have multiple linear L linear combination of that given vectory U and B or u and v so the same so the group of all the or the set of all the linear combination of a given group of Vector in this case u and v is the span of those vectors I hope so that you're getting me okay so let me show you within help of example or or before that I'm going to give you a formal formal definition of this span because I think I I I TR some definitions also so SP definitions gives you a clear way of thinking this so I'm going to just give you a definition this this definition of a span the definition of a span the definition of a span is the set of all the possible the set of the set of all the possible all the possible possible linear combinations I'm just just going to write linear combinations linear combinations L linear combinations of given of given group of vectors okay so group of vectors what do I mean with this that uh how the the like in in this for example you have a vector U and you have a vector v so you want to take out the linear combination so these are the group of vector to for for which you want to take out the L linear combination okay so that given group of vectors is called the span is called the span of those vectors the span of those Vector which is a group of vectors span of those vectors so we'll see see one example to help us understand is much better way so let's take one example of that the example which I'm going to take is this is not a very hard example just just going to take a simple example but given a vector U 22 and then you have and you given a vector v which is 1 1 okay so this is the two vectors which is and any any and you can take out the linear combination you can take out the linear combination where when you have 2 3 4 9 10 may you another Vector is also the same stage same size okay so but should be the same Dimension so you can take out the linear combination of this just you need to multiply with a scaler and then add it up and then you'll be getting your L linear combinations so so over here so over here so over here if you so first of all let's take out some set of linear combinations so what I can do if we can multiply this U Vector with some weight I'm not talking I'm not taking the name of scalar just for practice a good practice so we can multiply with some weight two uh 2 two which is and here I can multiply with I think one one one and you can add it up that that will be nothing but 44 going add it up I'm just being transparent so that everyone follows the same is because I think trans being transparent uh just for a sake of it's it's very very important so that the conceptual CCT Clarity uh will be very very easy for you all okay so this is the the first the first for this is the let's take an example this is your l l linear combination so uh your L linear combination is this for for example we written V okay no not V we have already already given so this is your first V your combination let's take out the second or third let's take out a second you can multiply 3 2 2 plus I'm just sticking anything 2 1 1 which will be nothing but 3 6 6 + 2 2 that nothing but 8 8 okay so this is your H which is a second L linear combination let's go on taking the third so this which is the last one for us 4 2 2 3 1 1 which is 8 8 + 3 3 which is nothing but 11 11 okay so this is your I which is your another Vector okay not I let's let's let's give it as okay not J as well let's give it as a key okay let's give it as K Vector okay so these three are just you can take out any you can you can take out lot more l combination you can just go ahead just multiplying with some scalar and then multiplying with some vector and then adding it up you'll be getting your linear combination of that two vectors so I'm not I'm not arguing you with that so I've just taken three linear combination so these three the five 58 5 five this is your G just taking z z h and k is the span it's a span a span of vector U and B instead of saying lot of Lear combination so you can just say okay that this these are the span of those two VOR so span if if you see geometrically speaking if you see over the it just your your V combination span hold to the space okay so please see some for some visualizations to to from three three blue one brown they give but I have given a geometric intuition that all the possible linear combination is called the is called the span of those vectors so the notation for writing this is just a span of the vectors V1 V2 V3 is written as a span of V1 B2 V3 okay just I've shown you over okay please see the notes the notes are also given to you just for your own good okay so this is this this is what I talked about a bit about span and I hope that you really really understood this let's go on a last topic of this video is Matrix Vector product okay so Matrix Vector product so what I'm going to do is uh I have already talked about Matrix Vector product but just the last thing which I'm going to just talk about so I do have time so let's talk about Matrix Vector product Matrix Vector product so so let's say you're given a vector a you're given a vector a where you have some a b c d e f g h i okay this is your uh Matrix a and then you have a scale uh then you have a vector X then you have Vector X which is X uh for example one X1 X2 and X3 okay so you want to take out the majri vector product so how do you take out so it will be simply what you do so what you do you will simply uh you will simply this is your a x which is simply nothing but uh you you take take this you take this column you take this column you take you take this column which is the column vector and you take this uh sorry row Vector you take this row vector and you take this column Vector you just multiply it or you can say you can take we'll talk about the dot product just don't worry so what we can say we take out the dot product of a row Vector of a row vector and a column vector and a colum Vector okay so what you are what you are specifically doing is taking out the dot product between between two with between a row vector and a column Vector so what does dot product mean means it is just element wise product and sum it all up okay so that will be nothing but uh that that will be a * X1 + B * X2 + C * X3 okay and then when when you add it so let me write a resulting Vector as well okay so that will be a d okay so after you multiply and add it so that's is just a DOT product of of of of two vectors of two of of a row vector and a column Vector which is this we'll talk about the we'll talk in detail about the dot product and a transpose later on okay so and then you do the same D * X1 + e * X2 + f * X3 I'm going to do the same G * X3 + H * X2 plus okay this one uh I * X3 okay so you'll be getting e f so whatever the answer is and this is your final uh Matrix Vector product product okay so which you have seen in in our previous video but I'm going to relate as a linear combination so I'm just just going to do what what what I'm going to do just see over here so you have a vector a you have a vector a you have Vector a where I'm going to what I'm going to do a b c d e f g h i j k l okay and I'm I'm going to multiply this this Matrix with a vector with a vector X1 X2 X3 and X4 okay so we have this a vector and X sorry aain Matrix a and a vector X okay so what you going to do you just just going to do the same thing you to multiply ax okay so I'm going to show you how you can do how how I'm going to represent this okay so what I'm going to do is to categorize this Matrix in into column Vector different set of different set of column Vector okay so we can take this we can take this the first First Column the First Column and say this okay okay this is the so you take it a V1 okay take the second column you say this V2 take the third column you say this V3 take the fourth column you say this V4 okay so so so specifically you you you are not taking as a as a as a as as a as a DOT product or or the element wise product of row and column Vector here what you will do here what you do categorize your Matrix a into a different uh into into a set of uh column Vector which is V1 vs2 V3 and V4 okay and then when you multiply when you multiply ax ax which is nothing but what you will do what you will do you will simply do uh X1 * V1 X1 * V1 okay X1 * V1 you multiply all X1 with this a okay X1 * V1 okay and this is your scalar this is of course your scalar so let me write in small X1 * V1 okay so V1 V1 + X2 * vs2 + X3 * V3 V3 + X4 X4 X4 * V4 okay so that whatever will so this is whatever the result will be for example R will be your linear combination so just listen what I'm try what I'm trying to say you converted this now what you do you take the you take this X1 and multiply X1 * X1 * X1 * C and X2 * X2 * X2 * X3 * X3 * X3 * and X4 time so what you do you given a weight of these column vectors so the resulting Vector will be the linear combination of the column Vector a so this this R will be the linear linear combination linear combination of vector a okay of a vector uh of of of a m of a column Vector a okay of a column Vector a so so so these this result resulting Vector will be the linear combination of the column Vector a of the column Vector I just just just have read column Vector column Vector which is V V1 V2 V3 the V4 so you have seen me how I done this as is to to Showcase you in in the form of linear combination so I hope that you understood a lot from this video and I really really hope that you will uh try to try to do the uh try to mark your tendance as well because it is an LMS so try to mark it your notes are in the description on box below please feel free to assess the notes uh it is very very important to to to work from that uh uh and and also and in the next video we'll be talking about uh the the linear transformation where we'll be introducing the notion of a transformation when when we multiply a two Matrix so that the the that that is th a transforming one Matrix into from one dimensional space to another another dimensional space that is a transformation which we'll talking about in the next video and then and then I hope so that we'll be able to complete the uh chapter number one in some days and then I hope uh it is it is much clear to you as well uh the next the one of the announcement that I want to give is please please please share the video and and mark your attendance in your in your quizzes okay uh uh by by going to the assignment Tab and your LMS if if you are in LMS so please go there and please please please try to try to search for some some some problem set or try to search for some resources although you don't need I talked a lot about more than enough okay for for your journey so so thanks for seeing this video I I I hope that you enjoyed this and you have taken your own notes please feel free to assess the notes on the description I'll be catching up your next video till then byebye and have a great day okay everyone welcome to this lecture on L your transformation so in this video will be is specifically talking about linear transformation in the pre previous video we talked about Matrix Vector product or or or in or a linear combination which which was the very fundamental concept and we also talked about span of a vectors now in this video we'll be talking about linear transformation one the most amazing concept or the beauty of algebra or linear algebra which you will ever see and also this this sets a very Foundation of your of your algebra skills or after study as of now okay and and it is used extensively in the field of deep learning uh when when you read research papers or or when when you staring some algorithm so if you want to understand by the point of view of linear algebra then then I think uh linear transformation is one of the best thing uh to study and and this is a compulsory topic to study as the most most most concept is related to this but you may think hey you just transformation so can you just Define what a transformation is I just want to you to search on Google what a transformation means and then put that in a comment box please give the time stamp so that I could I could know okay you you're here okay so please go on YouTube try to search about transformation and then come back okay so transformation like like just transform something or or do something to that function or or if you know about function so linear transformation can be thought of as a functions okay as the new name given when when we deal with in lanar algebra okay so lanar transformation is nothing or can be just thought of as a functions can be thought of thought of thought of as a functions can be thought of as a functions why I'm telling this in in functions what you do you give some input value you give some in input value and you want your F and you want your F to map this input value to the output value y okay so this is this is this is what the L transformation is doing linear transformation take some some some some some Vector as an input maybe two two dimensional Vector which is two dimensional Vector it wants a function it wants a function T that maps from n dimensional to M dimensional Vector okay this is the definition of linear transformation again let's will see see some of the definitions to help us more clear what actually linear transformation means but in but but in but in big picture uh linear transformation can be thought of as a functions like it takes some some some some sort of vectors or vectors and transforms from one di to n dimensional Vector to another dimensional vector or a different space in that uh plane okay so this is the this is the basic thing which you need to study about so u l transformation can be thought of as a function that takes some some some some vectors or or just transforms that just transforms this Vector from n dimensional space to M dimensional space and that can only be possible with a t which is a function T so the functions are the one which we take some value and Maps input and input values to Output values but in transformation we take we transforms xn means n dimensional Vector to M dimensional Vector okay so that's a linear transformation with you the definition so let's let's let's start with an example so that we could understand it's much much much much more better way okay so just I'm going to write a definition and what in what we do in a case of functions and what we do in a case of uh linear transformation so in functions we take some values we take some values we take some values input values and we map our input values and we map our input values input values to Output values okay so to make a function f that take some Val Val and output the square of X which is Parabola if if you plot it out okay then that in in case of linear transformation we what we do we make a function T we make a function T we make a function T we make a function T that takes the transforms the transforms from from one vector space transform the vector the transform the vector uh from from RN from RN and this is the RN from one from n dimensional into a vector into a vector to RM okay so it just transforms one one one vector from one one vector space to another Vector space okay so this is the basic thing which you need to which which will which will see I just prove prove you out this equation so why it why it seems to be legit so let's start with with an example so that I could just prove you that what whatever I'm telling is correct okay so let's let's take one example let's take one example say you have a you you if for example if you multiply example if we example will be say you multiply your M by n Matrix so you have an M by n Matrix you have your M by n Matrix this is your favorite Matrix you have your m m byn Matrix and what you want to do is simply multiply with a column Vector which is n * 1 which is a column Vector which is n * 1 so we'll just multiply this this Matrix with a column Vector n * 1 so let's let's multiply it out with the column the column Vector n * 1 okay so here it has only one column and the resulting Vector what we are specifically doing we we are taking we are multiplying our Matrix with a vector okay and then that that will be nothing but your n by one column Vector M by m * 1 column Vector okay m m * 1 column Vector which is your which is resulting Vector resulting Vector that that will the resulting column Vector resulting column Vector okay so so what so what it does so what it does it take it it it it took your it took your um just is to took or or or in other words we see over here that an N M * n Matrix that an M and M * n Matrix transforms an n * 1 Vector into an n M * 1 resulting Vector which is another space okay so using this Matrix we transform this Vector into different Vector space like this okay so for example let's see some some example to make sense here we are taking the taking the the the the the Matrix Vector product which we have already seen in our previous videos so let's consider consider this Matrix a let's consider this Matrix a as we as a 3X3 Matrix so 1 2 0 and 2 1 0 which is nothing but 3x three Matrix okay now I want to show you want to show show you want to show that by matrix multiplication by Matrix Matrix Matrix Vector multiplication or that matrix multiplication matrix by matrix multiplication by matrix multiplication you want to show by matrix multiplication a transforms this this Matrix a transforms transforms transforms Vector n r R3 vector and R3 mean is which is the which is the column Vector which is which is three threedimensional Vector like this have Vector which is a three dimensional x y z which is from R3 from from in from R3 to R2 or to R2 okay into into a into into a vector into Vector in R2 R2 so you just you using a you want to transform this Vector X X you can transform the vector X and then that will into into an R2 which is nothing but your X and Y one two two dimensional rather than being a three dimensional that's that's when we call linear transformation of a matrix okay so let's see so with with an example you have a you have a vector so so R3 so over here R3 R3 are a vector of a size is a vector of a size 3x1 which is a which is the 3x1 threedimensional vector while Vector R2 R2 which you want to transform you you want to show is 2x 3 2 * 3 which is the two sorry uh 2 * it's it's 2 * 1 it's two dimensional Vector so it's trans it's using the a you want to transform your your your your vector means you using a or or a transform you just show that a transforms the vector means from R3 in R3 into a vector R2 okay which you want to show up okay so when you do this when you do this so this this this is of size 2x1 which is which is your uh which is your row Vector I I think it's column Vector column vector and then you if if you multiply a which is your 2x3 Matrix which is a true two okay it's 2x3 okay it's 2x3 matrix by a 3x1 vector by a 3x1 vector by a 3x1 vector the resulting Vector will nothing but 2x one so you just showed using the what you do using a you multiply this a with uh which you want to show means R3 and then the resulting Vector which you can see that you showed okay using a you transforms the vector in R3 which is uh three which is this one into a vector R2 which is 2 2 okay or or or a which is a twodimensional Vector okay so that that is what it is telling so let's see one of one of one of one of the example just to numerically show you so so 1 2 0 2 1 0 and and what you and and you have this metrix a and you want uh um and you and and and you have a threedimensional vector XYZ and you and this is an A and this is a vector X and you want to show you that a transforms of in a trans means uh what do you see the a transforms and and a matrix which is from R3 into an R2 okay using a you want to transform R from R3 R3 into R2 so that will be nothing but x + 2 y and zero of course we don't write it out and and of and and 2x + y okay and that and then it it will be after after you add it up add it up it will be either A and B which is your R2 okay so that is the following that what you done you simply transform your f one one one vector space to another Vector space using a function a or or using a which is your Matrix okay so let's let's let's define it out so what you done what you done you made a function T the transform F and and from from n dimensional to M dimensional means a function T transform the vector M transform the vector RN into a function a function T which transform from RN to RM which another dimensional space got it the linear transformation should satisfy your two constraints the two constraints are the first constraint the transformation t x + y it would be it would be is equals to the TX plus Ty y transformation of X Plus trans transformation of Y and then your trans transformation and then this this is scalar a and this is a vector X and then it's should be and then then it should be a uh a and transformation of X so these are the two conditions which you need to satisfy okay uh these are the two basic conditions which you will ever see okay but but the more form formula which I which I could State over here is linear transformation is the is the the function that transforms your Vector from onedimensional space from RM to RN okay that is your linear transformation of your vectors or Matrix or of of of of of a vector okay so um one one theorem is there one theorem is there the theorem States uh your let T be our let let T be a function or transformation that transform from one vectors RN to RM um which is the transformation of your transformation of X which transforms RN to RM okay so I'm just just just going to write the theorem which I'm not going to prove rigorously but you can prove it you can prove it t equals to R to make a function T you want to make a function T to transform RM to R RM okay uh be a transformation defined by the trans transformation is defined by T of X okay trans you you give uh you give one vector which is of n dimensional and just what it will do using a which is a matrix just transform that Vector means the Matrix Vector product so I just showed you that a matrix Vector product is a linear transformation okay so what you use if even you multiply to u a vector with the Matrix that that that will just give you the linear transformation just it should satisfy the conditions which are listed over here the geometric understanding is also so not it's is very very easy you can you can consider watching three blue one one Brown videos for this I hope that will make more sense there okay so that was a short video On LAN transformation about fth 15 minutes and I really really hope that that you like this video uh in the next video we'll be talking about transpose of AMS and uh and and and and and and and uh uh dot product which which which which is one of the most important concept so let's get on to the next video okay everyone so today we'll be talking about transpose and a DOT product of a matrix or a vector so we we we'll be talking about that because in the video of linear combination I have taken one example of Matrix Vector product and I showed you how you can represent that Matrix Vector product as a DOT product between the column vector and a row Vector so we'll be talk talking about what does it exactly means uh what is a DOT product and a transpose of a matrix so these are again two most important concept which you will ever see in your journey of linear algebra again it's just one of the basic concept which is very very easy to understand but still it's is very very uh good to know about these things which gives you an extra tools to work efficiently uh and and your deep deep learning problem or machine learning problem or or or other stuffs okay so let's get started so so the what is a transpose of a matrix so I'm going to start with a transpose of a matrix what is a transpose of a matrix so the transpose of a matrix is a kind of operator which flips over the diagonal which flips over the diagonal okay or make all the rows or make all the rows uh for example if I could show you the the the visualization so for example you have uh you want to take you have a column vector or row Vector like this 1 2 so it's if the diagonal is this one the diagonal is this one so it simply flips it okay so it simply flips it then it would be if you take all the transpose of this so it will be 1 2 so what it does it flips over the diagonal for example let's let's take one one more let's take one more let's take one more you have 1 2 3 4 which is a 2X two Matrix now if you take all the transpose of this Matrix so what you do you have this diagonal you have this diagonal you have this diagonal so if you flip over this diagonal if you flip this over the diagonal if you flip this over the diagonal it would be nothing but one if you flip this over a diagonal if you slip flip it there 1 3 and 2 4 okay and two four so what you done you simply flip it and then above going down and down going above so that is what it is trying to tell or or in other words what you can interpret is you make every row or sorry every column as a row and you make every column as a row okay so that you can that you can expect Ed so for example you have another so so if you have another Matrix let's let's take an example that is 6 4 32 and you apply the transpose onto this Matrix so what will be the output so the output will be what you do you take this uh you take this um or you say the row sorry the column or the column vector and make it a row Vector like this make it a row of row Vector six and three and you take this another you make this four and two or other wasse what you can interpret is you flip over the diagonal you flip over the diagonal you make this three above and you make this four down okay so what you specifically doing is to flipping over the diagonal like this okay so you will get the same result as you are doing so it's just not a big deal to understand these things is very very easy to understand okay so I hope that that that you are able to understand what the transpose of a matrix is so let's try a formal definition let's write a formal definition of your transpose of a matrix so the transpose of a matrix the transpose the transpose transpose of a matrix transpose of a matrix is an is a type of operator or operator is a Operator Operator which flips which flips over the diagonal over the diagonal over the diagonal okay so this is the transpose of a matrix so what is does is it the definition states that it is it just flip over the diagonal or to the main diagonal to obtain the a transpose so for example you are so for example you are given a vector a so what you do what you do you reflect a you reflect a you reflect a over its main diagonal to obtain the a transpose or it is just a tech technical verse to flip or reflect but specifically what is doing making the column Vector as a row vector or and and a row Vector as a column Vector that's it okay okay your visce Versa it can be anything so making a colum Vector to a row Vector that's it okay so that is what it is telling and a row Vector to a column Vector whatever whatever seems good to you so a transpose just exactly doing the that okay so for example for for example you want to take you have your Matrix a okay you want to take out a transpose you want to take out a transpose where it either presents your row and J represents your column so now if you after this after applying the transpose at this Matrix now your will be Aji now your column represents your rows and uh rows represents your column again again this is this is very easy to interpret so you have this 2 two 2 two and you have this a matrix what you what what you do you apply the transpose on this Matrix after applying the transpose of a matrix here you are having a transpose okay and you have I which is two or I and G okay you have I and J which when you do that when when you apply the operator this will be nothing but or I could say 0 0 Let's Take This 0 0 okay so when you when you sorry we should not take this 0 0 let's take it as a let's take take it as a one let's take it as a six okay just for understanding okay so what if if you have transpose over here after applying the transpose of a matrix over here your col your your row becomes your column and column and column becomes your row or yeah so so your row becomes your column so your row is the first row becomes your column so after flipping over the diagonal so when when apply so is two two and then your second second column becomes your second row okay one and six okay so what do you specifically done you flipped over or or reflected the main diagonal or flipped the over the diagonal and then you obtain your transpose of a matrix so that is what it is telling so you can do in high dimensional spaces as well for for example you have a m s we have 64 4 9 10 11 12 13 14 15 okay so if you want to apply the transpose on this Matrix that will be nothing you take this take this and put it over there 649 and then you take this 10 11 12 and you take this 13 14 I think 15 okay so this is this this is what you are doing is flipping over the main diagonal flipping over the main diagonal okay to to to or reflect it or reflect it over the main diagonal to obtain your transpose of a matrix so we have we have done a lots of examples so we'll see one or two more and more examples to make you more sense so let's make one or more example so I just hope that you are able to make sense for actually transpose of so that is the transpose of a matrix so it is just what it is doing reflecting a over its main diagonal to obtain the a transpose that set what the transpose doing so if if you take more examples we have a one two okay if you have one two if this is a A or a okay a column uh row Vector if you transpose this a transpose that will be nothing but one two okay uh just flipping over or or or making the row as a column Vector okay the next thing can be for example you have a I want you to solve this I want you to solve this one 2 3 4 apply the transpose on this and see me what the result result would be in the comment box comment box write this answer in the comment box it will very very easy to understand uh so I could say I could see whether you're watching or not okay so that is the that is the transpose of a matrix so just just I'm going going to write out some of the points some of the points please please pause this video and write your answer of a transpose of a matrix so just I I want you to write it out okay so let's see that some some of the properties some of the properties properties of the transpose okay so you want to take out a transpose and then you take out the transpose that so tell me what it will be tell me what it will be tell me it would be nothing but a it would be nothing but equals to A you you first of all transpose it okay and then you take the transpose of that so for example you have a which is 1 by 2 okay you have 1 by two okay so what you do what you do you take the transpose of this which will be 1 2 and then you take the transpose of this which will be 1 2 so these are equals to or not these are equals to so that's why we are telling that is the one of the property of a transpose of a matrix another property of a transpose of a matrix another property of a transpose of a matrix is A + B A transpose which will be nothing but a plus b transpose is nothing but a transpose plus b transpose okay so that will be equals so you have a a 1 2 3 4 okay and you have a b and and you have a b uh 1 2 3 4 you want to add it by taking all now you want to take all the transpose of these three so what you can do you can take the transpose of you can take take all the transpose of this and that will be your answer okay so let's do this we going to take the transpose of this that will be nothing but 1 3 and 2 4 okay plus uh 1 3 and 2 4 okay which will be nothing but 1 + 1 2 3 + 3 6 2 + 2 4 4 + 4 8 okay that will be your final answer which is 2x2 matrix and this is this is what this property States you can even what you can do you can prove it you can prove it extensively you can prove it you have a you take out the transpose and then you and then what you do first of all let's do the same thing you first of all add it up so when you add it up 1 2 3 4 plus 1 2 3 4 okay when you add it up so 1 + 1 1 2 + 2 4 3 + 3 6 4 + 4 8 it is 2 4 6 8 okay that that will be your answer of this particular and then what you do you take out the transpose of this because you added it up now you take when when you when you take out the transpose it would be nothing but 2 6 48 and this is your answer so these both are equal so these both are equal so you can you can you can do ex you can do separately transpose and then add and yeah or yeah or you can or or you can do just first of all add and then take out the transpose both are equ equivalent your answer will be equivalent okay so let's see another uh let's call it another page let's make another page how do we add another page yeah let's add one one one one one one more page so another property is another another property is AB transpose AB transpose which be nothing but equals to B transpose and a transpose okay so you have a let's say let's for the sake of an example let's take a vector 1 2 1 2 and then you take a and you take a v v Vector B Vector you take a b Vector which is 22 okay and then what you want to do you want to take out the transpose of this okay so let's first of all do this so one so that would be 1 * 2 1 1 * 2 which is which would be nothing but 2 2 * 2 which is 4 okay and then what you do you take out the transpose of this so when you when you take out the transpose of this that that will be 24 that will be 24 which is a column Vector okay so that will be your first understand this is your first now let's let's check for equivalent for this so you have B so which is b 2 2 if you if you take the transpose of this which will be nothing but 2 two okay and then you and and then what do you do you first of all do this 1 2 okay which after taking the transpose of this that that will be 1 2 and then when you multiply it out that will be nothing but 2 4 2 4 which which is both equivalent so so we have proved this we have proved this which we have proved this let's fourth let's go on Fourth property the fourth property is you have scalar C and you have a matrix a when you take the transpose of it that will be nothing but C A transpose okay what you can do you can first of all rather than after multiplying and then taking the transpose you can first take out the transpose and then multiply by the C both will be same okay uh fifth the fifth you can you can actually verify this you can actually verify this is not a big deal okay just the way I'm doing okay so you to take out the determinant of a transpose that will be nothing but determinant of a and if you if if you don't know about this please ignore which we will be studying this extensively in our VAR okay you can ignore this the determinant is is just the area of the parallelogram so you can easily ignore this as of now okay sixth one the sixth one is the last one which I'm talking about a transpose minus one okay uh 12 the^ of1 a one transpose so these are equivalent okay so you take the transpose and then and then inverse it first of all you inverse it and then take out a transpose both will be equivalent okay so these are some of the some of the most uh used properties in your transpose and I hope that you that you are able to understand this it's not a big deal to understand okay these are the transpose which we have talked about let's go on to the dot product to understand much better so let's let's let's talk about a bit about dot product so what is dot product dot product what is do what it do what it do it do element element wise product element wise product and sum it all up that's it that's what the dot product is doing is to do element wise product and add a summ that's what the dot product is doing so for an example so for example the dot dot product so the so for example let's let's let's take for the sake of an example you have a vector a where you which is your which is your row Vector so A1 A2 all the way around to the a n all the all the way around to the a n and you have a b Vector B row row Vector which is B1 B2 all the way around to the BN okay so this is your two two vectors now what you need to do if if you want to take out the dot product between these two Vector if you want to take out the dot product between these two vectors so how do you take out so what you you simply add a submiss so you take out the a DOT you write dot b is nothing but equals to I = to 1 all the way around to the n a i b i so what it will do first of all a a A1 * B1 + A2 * B2 + A3 * B3 plus all the way around to the a n * BN and your final output will be one is scaler C which is your dot product between these two row Vector okay so that is the the particular the the algebraic definition so this is the algebraic definition of your dot product and or or or or in other words what you can tell is the dot dot product between two vectors is nothing but a b transpose the dot product the product between A and B transpose that that is your algebraic definition or this is your formal definition of your dot product product so for example for example you have this and you have another Vector like this your output will be 8 a b c d e f your output when when you take out the dot product between these two A+ d a * D + B * e + C * F and your output will be a scalar okay so if you have seen our Matrix Vector product Matrix Vector so we have we were having we were having so let's let's see which we have seen already in our previous videos so let's say let's say let's say of for for the sake of an example you have a b c d e f g h i okay and you have a vector which is X1 X2 and X3 you want to take out the product between these two so what you do you categorize this into a different categorize this into a different what do you say uh the the the column vector or sorry row Vector yeah column Vector V1 V2 V3 then what you do you take out the dot product you take out the dot product between V1 and a vector X so when you take out the dot product between a column Vector with uh with a column Vector so when you take out the dot product between these two that will be your answer which you have already seen in in our previous videos okay so I I don't think that we should uh that we should care about this so I hope that you are able to make sense of these things and please feel free to review the previous video on L linear combination which I talked in detail about these things okay so this is this is what the dot product is simply say simply take off take take out the element wise product and sum it all up okay and that will be your simple scalar okay so that is what the dot product means and if if you have seen our videos on on on hypothesis function which if if you know about hypothesis function of linear regression if we talked about hypothesis function so in hypothesis function what you were doing we were and we were taking out the dot product between our X and W we are taking out the X or a Theta we take Taking of the dot product between X and W that that was resulting in our prediction y hat okay so X was also maybe a matrix or vector so you have a matrix or a vector so a vector of X can be uh uh other met for example 2 4 6 7 and your W can be also 2 4 6 7 whatever these are the weights of this feature and what you do you take out you take sorry it's not matrix it's a vector it's a vector do product so you simply multiply it up you simply multiply it up and then what do you do you simply add it up and that will be one scaler which is your answer for for example 2 * 2 + 4 * 4 + 6 * 6 + 7 * 7 and then after doing plus all those stuff then you'll be getting your answer C which is an answer of this particular question okay I hope that you are able to make sense out of it so we have seen our algebra definition of a DOT product now it's time for seeing the geometric definition to help you more make more sense of your dot product of two vectors okay so let's see let's see of that so let's see uh the do product let's see uh let's take let's take an example you want to take out the dot product take out the dot product between two vectors between two vectors which is Vector a and Vector B which is geometrically speaking I'm going to I'm going to talk about geometrically now I'm going to talk about geometrically so for the sake of an example understand let's take an example that you have a vector like this so sorry it's bad you have like this and you have this okay so this is your vector a this is actually Vector a and this is actually vector v and just one thing if you're a calcul student there is a small fun quiz is it continuous is it continuous function we have a function let's take an example that is a function okay is it a continuous function um if is it differentiable if it is continuous function if is it differentiable that is your question okay so please feel free to put in comment just for just for those who are calc student just tell me just ignore this A and B just say uh just say this this is a function and then just tell me it is a continuous and if if it is then if it is differentiable or not okay this is this is your question to ask answer but the one who is studying Le your algebra please filter stick me with this okay kindly ignore the question which I told you cool so the angle between these two Vector the angle between these two Vector is nothing but the angle between is 59.5 De okay so that is the angle between these two Vector okay so this is the angle between these two Vector so you want to take out the dot product between Vector a and Vector B so taking of the dot product between Ang Vector a and Vector B which which will be nothing but it should be nothing but I would say uh it should be nothing but Norm of a norm of a times the norm of B the norm of B time cosine of theta cosine of theta so Theta the angle between them is Theta so the norm of a so for example let's assume the the length of a is uh 10 okay length of a is 10 and your length of B is 13 so here your vector a is nothing but 68 and when you take out the length is 10 using the pythagore theorem Pythagoras Theorem and then uh the vector v is 5 and 12 when you take out using the Pythagoras Theorem that will be or or the or the norm of a vector when you take the norm of a vector that would be 13 okay so the the the length of these I have already told you okay that is 10 and 13 and when you multiply with a cosine of 59.5 okay so then you'll be getting then what you then what you will get 10 * 13 * 0.575 and then when you multiply that will 65.9 whatever and then it is approximately 66 if you want to uh what do you say take out now if now we got 66 now if you want to take out the algebraic the algebraic definition States 6 8 and 5 12 when you do this 6 + 5 11 8 + 12 20 8 + 12 20 when you add this so sorry uh we are actually taking we are actually taking all the okay that's 6 6 * 5 it should be 6 * 5 why I'm doing 6 6 * 5 sorry it's 5 yeah 6 6 * it's 6 it's not 6 it's 6 okay so when you do this 6 6 * 5 and + 8 * 12 6 * 5 + 8 * 12 and you will get 3 + 96 which will be 66 and this this is equivalent to this and you can also do with this this this one with a high dimensional spaces okay so I hope that that that you are able to make sense out of it and I also hope that uh you you are able to understand a little bit about this okay so that is the dot product between these two Vector just you need to understand the numeric understanding of dot product that element wise product and add it all up that's it okay so I hope that you are able to make sense out of it and I also hope that that you are able to understand everything this was the dot dot product between these two vectors and a transpose of vectors and a matrices and I also hope that that till now you're you're able to understand most of it out of it and I also hope that uh you will utilize this resource and share this resource to everyone that motivates me to work on this content s and so in the next video we'll be talking about some of the types of mates and then and then we'll talk about rank Trace operators determinant I Val igen vectors and solving the systems of equations and then our L linear algebra will be done so I hope that you like this video I'll be catching up your next video till then byebye hey everyone welcome to this next lecture on linear algebra and M 2 and I really really hope that you are enjoying this course first of all I want to thanks thanks you and congratulate you as well that you had first you had completed your first week and I'm I'm very much happy to see so much of enthusiastic students who are watching these lectures without any kind of problems and they're able to understand and leaving their great feedback in the comment box and I'm seeing the watching hours increased so I would like to thank it's giving me a lot more motivation to make these such videos for free as well as um I would just salute you for your consistency and I would also salute for you m utilizing these kind of materials uh who are who are for free and one thing which I wanted to say that we recently in yesterday we uh we we we released our first homework assignment of the previous week which is the the homework assignment consist of the questions from the five lectures previous five lecture lecture 1 to five that is your first week lectures so basically in that we included the homework homework assignments all the questions from the topics which are already taught okay so if you go and see and I and and I would like to thank vayak Vishnu who has contributed 70% of prepar preparation of these questions who is one of the teaching assistants of our uh of our CS M2 so I would like to thank you thank him and I would also like you to thank him on the Discord server or whenever the comment box thanks vayak Bish so it it would motivate him as well he's doing the community work for free so so let's so here is the pro programming assignment to sorry not programming assignments homework assignment where you're getting around 32 questions and these questions are covering from very Basics to a conceptual understanding of the particular subject having in mind to have a good practice of yourself of the topics which are already taught so you may think hey are you you're doing this stuff the reason why I'm I'm making you practice these stuff is the reason one is mainly when you go in deep learning to have a conceptual understanding of what are vectors and how it is performing computations and what are the resulting Vector size and etc etc etc so it will help you to to to to practice a lot and it would also help you to understand the conceptual understanding or on the very depth understanding of these vectors and and and we we also seen some matrices and then we are performing something and then uh I also taught you about linear combinations xx and 3 contains of linear combination where we are talking about various stops over here and the these These are the questions which are very very very very nice questions which are prepared by vayak Vishnu and as well as I had also added some questions over here which are also related to your deep deep learning context and then you finally have a transformation and this this this is also a great amaz amazing uh uh the questions which are there and it will help you in deep deep Learning Journey that how actually the linear transformation works and behind that and then the we we we talked about transposing the dot product of two matrices or vectors okay so that is the specific thing and then we we ask you to verify this property so I would definitely ask you to visit these things it will it Sol it assignment upload it in your LMS learning management system if you have enrolled into that it's absolutely free for everyone please please see the student handbook in the description down box below and go over there and into the LMS and and Summit your homework assignment and then you'll be getting the detailed feedback on what questions youve done wrong and what question you haven't done wrong okay so let's get started with this video so the title of the video you might have already imagined is the types of a mates types of mates so what are the types of matrices that that that we will study and and and some of the types of matrices are maybe not come in your journey of deep deep learning but I would say ke whenever you're studying the something let's study full of that okay so do not let's let's let's not study the partial stuff so let's study full but most of the thing which I'm going to teach is is being used frequently not too much frequently but it's being used sometimes me when when you talk with some great mathematicians or a deep learning Engineers to take these kind of words so it so it should not might confuse you so that's a VA I asked about so the types of matrices so the T first types of matrices which you already know about in other words like row Vector which is something called as row matrices okay or a row Matrix a row so let me add one thing over here row row Matrix row Matrix okay so what is row Matrix so can you define what is what is a row Vector can you just Define it so the row Vector is the is the is the Matrix which have only one row so the same way row Matrix are the one which has only one row so it you you can say just a row Vector uh yeah so the matrices which have only which have only one one row is called the row Matrix so for example for example the example can be 1 2 3 okay so this is the first this is your row Matrix okay so because it has because it it only have only one row so we can mathematically we can we can write that a is equal this is a matrix where a i g where we have a m * n Matrix where M denotes the number of rows and N then denotes the number of columns in this you have m equals to 1 you have m equal to 1 then it is called as a row Matrix so what is a row Matrix row Matrix is a one which has only one row let's let's talk about second second kind of Matrix which is column Matrix column column Matrix so sorry for in in on the eve of diali many the people are just busting up the crackers in India so yeah don't no no problem in that so what is a column Matrix so column Matrix which have only one column so uh and you have you have heard about column Vector so the same way we have column Matrix which only have only one column and these are these are used very frequently in the in the era of deep learning and and and it is very precise to use these names in deep learning to have to to follow the mathematical conventions ra rather than saying it is a it is a it is a call this the shape either shape you can just say that okay it's is a row Matrix or it is a column Matrix or it is a row vector or it is a column Vector okay so here you have a matrix a where you have a matrix a where you where you have M * n which is a size and where the M can be anything you can be any number of rows but you have only one column okay so here here n n should be equals to one so you should remove this and write one to be considered this as a column Matrix okay the next kind of Matrix we should talk about is zero or null Matrix so as you have already uh imagined about this zero all n Matrix and these are very very easy kind of remembering it's not a I'm not teaching teaching any Rockets science is very very easy to understand so so if in all the matrices or all all the elements into that matrices are zero okay so all the all the elements all the elements in the matrices in the matrices are Zer then that Matrix is called zero Matrix okay so for example you have a where you have 0 0 0 where 0 0 0 0 0 0 where you have a 3X3 Matrix and this is called a zero Matrix or sometimes you call it as a 3X3 null Matrix okay so when someone tell you hey can can you can you just tell what types of Matrix this is okay this is a zero Matrix okay when someone ask you hey you're given a null Matrix what what happened when you multiply this new Matrix with another Matrix where the Matrix satisfy all the Mator multiplication property or Dimension property so you you will say okay uh you will say okay what is a null Matrix so null Matrix are nothing but a zero Matrix okay so that is just a zero Matrix which which is given another name which is called null Matrix then the in in that you have all the elements to be zero okay the next is the next is your favorite single ton Matrix okay so in in in Java one of my friend was uh taking a single turn double turn so in the same way we have single turn matrices okay so single single turn matrices so so in single ter matrices your all the matrices are are or or you say you have only one element into that Matrix okay not all the matri you have total of one element in that Matrix okay so that's why we call it as a only one element only one element in the Matrix in The Matrix and so sorry I'm not too much of creative I don't follow the rules of changing the color and then writing it out I should develop that uh thing so let's so let's use different different color then so the fifth one is the fifth one is horizontal Matrix so let's give some examples of this because I I I I I haven't given example of this which is two then it can be one then it can be three these are the for example a is a matrix where we have this is a single turn m matrices Matrix etc etc etc so these are called the single T Matrix you may have St single ton in in your sets or discret mathematics if you have if you have take taken the course on discret mathematics so I would ask you to do not take the course but yeah remember remember the single T either it is not used too much but yeah you you should know the you should know what a single T because when someone ask you you should be able to answer it horizontal Matrix horizontal Matrix that is a that is a good deal so horizontal measor so C and Define if anyone has taken the linear algebra class please feel free to go down in the description box below please so no description it's comment okay so please feel free to go in comment box please write what is a horizontal Matrix and and it's very very easy it's again very very easy so for example so for example uh for example here you have 1 2 3 4 okay and then you have uh and then you have your favorite and then you have uh 6 9 8 2 so I'm going to consider this Matrix as a horizontal horizontal Matrix you may ask why you should consider this as a horizont horizontal Matrix so just tell me the size size of the Matrix is the size size of the Matrix is what uh 2 by 4 two uh rows and four columns and here the The Columns is greater than rows so that's why we that that is a horizontal Matrix so whoever Matrix where the number of a columns is greater than the number of rows then that's called the horizontal Matrix got it so that is the horizontal Matrix where your number of rows Sor not rows number of number of columns is greater than the number of her rows so that that that is one of the example of a horizontal Matrix now let's see some more examples some more uh some more Matrix types which is something called as vertical Matrix so can you tell me what is vertical Matrix just Define me what is a vertical Matrix so let's make a matrix let's make a matrix let's let's let's make a matrix where you have 1 2 3 4 5 6 7 8 9 10 11 12 okay so this is your this is I'm going to consider this as a vertical Matrix now tell me why this is a vertical Matrix the reason why is a vertical Matrix so let's let's count the size so we have a total of 4x3 Matrix and here your number of rows is great greater than number of columns so that's why this is a vertical Matrix in horizontal your number of columns should be greater than uh greater than number of rows to be called as a horizontal Matrix but in vertical Matrix totally opposite of that here your number of rows should be greater than your number of columns okay to be called as to to be said as a vertical Matrix or in formal definition which is Tau in a school is a a matrix is said to be a vertical Matrix if and only if its number of her rows is greater than the number of her columns it's it's it started my school I just I I I just want to thank my school to teach me these definitions no not exactly definitions of vertical Matrix but yeah the definition format or template to tell is that this is said to be this because this okay so I follow the template of my school so thanks to my school H shout out to Sunan cat okay cool the square of Matrix so what is a square Matrix I would say pay attention to this Square Matrix is used extensively whatever we'll study in the like diagonal matrix or or whatever okay like determinant or or igen vectors and IG values we are going to take this Square Matrix so what are square Matrix so let tell me what are square Matrix The Matrix which looks like square is that it yeah so so the Matrix where you're or just wait now here's here's your Matrix here's your Matrix 1 2 2 4 and here the here's your a matrix and here's your B Matrix which is 1 2 3 1 2 3 okay or let let's consider this as a let's consider let's let's cons consider this as a okay so here you have a total of 2 by two Matrix and this is a total of uh what do you say a 3X3 Matrix so here your number of rows matches with the number of columns so the square Matrix are the one where the number of rows matches with the number of columns okay so the square Matrix is which where your number of rows where your number of rows is equals to the number of columns okay so the the square Matrix is is in which your number of rows is equal to the number of columns then it is said to be a square Matrix so again my school form of definition a matrix is said to be a square Matrix if and only if it's it's it's a real number for rows should be exactly equal to the number of a columns in your Matrix okay so that is your Square mat again shout out to Sun uh just just for your information uh this these are the things which are not taught my school till now I'm in class n so till now they haven't taught this but yeah I know the form I I know I know the definition template because I and in in my school I used to study a lot more definitions so I know the definition of this they the definition is starts with this the same way uh this is said to be this because this and that so that's why we got it this so the same way I frame a square Matrix is said to be a square Matrix a matrix is said to be a square Matrix if and only if your number of rows is matched with the number of columns and so thus if it is follows then it's a square Matrix so so we we know about the square Matrix now so let's go further into understanding the the the diagonal matrix so so what is diagonal matrix what is diagonal what what is diagonal matrix can anyone tell me what a diagonal matrix is anyone try it out so all the elements except the principal diagonal or or let's start with let's start I just want to scho that guy who is bursting the crackers I don't want to see him I'm making my video why is bursting his crackers outside oh my gosh no no problem in that as well Okay cool so let's make a matrix 1 2 3 0 0 0 0 0 okay so this is called the diagonal matrix because your your your all the elements in your diagonal matrix except the prpal diagonal this is this principal diagonal and this is the principal diagonal so all the elements into that Matrix except the principal diagonal of a square Matrix so diagonal matrix should be a square Matrix because it is a 3X3 Matrix so it is a square Matrix R Zer so all the elements into that diagonal matrix of a square Matrix are zero then that is called as a diagonal matrix so let's see some more so for example you have uh Z 1 2 oh my gosh zero I think I I I I done wrong so 1 0 0 0 2 0 then 0 03 so this is oh I I I made the same thing again no problem in that oh this is left okay so that is the all the principal diagonal is your is your uh what do you say the the the nonzero and every every elements of except that is your not zero then that's called a diagonal matrix so I hope that you're a able to make sense out of it so the diagonal matrix should be a square Matrix and a square Matrix is nothing a matrix which said to be a square Matrix if and only if your number of rows match with the number of a columns and thus it is called the square Matrix because it looks like square but what is rectangular Matrix so one thing which I'm to mention rectangular rectangular Matrix so what is a rectangular Matrix here where your number of a rows does not matches with number of a columns oh my gosh there's a contradiction so that is nothing but called a regular Matrix sorry oh my gosh it's rectangular Matrix so let let me Define this from my school work so thanks thanks thanks my school sit down sit down yeah so uh what is rectangular Matrix a matrix is said to be a rectangular Matrix if and only if it's it's number of rows that's not match with the number of a columns so that's the rectangular Matrix so so the follow following example is a rectangular Matrix so maybe you may have 2 1 3 4 2 1 3 4 that is your number of rows is 2x 4 where 2 is not equals to 4 so that is the rectangular meter that looks like rectangle so that's why we have written the rectangular Matrix cool so let's go further into learning about scalar Matrix so what is scalar so first of all Define a scalar and then try to identify what is scal or Matrix please go ahead and write your answer let's give you give the guess guys U I'm just here to have a fun with you all so give a guess what do you mean by scal or mat because when I started first I given a very good guess and that was totally wrong because this this is like a scaler Matrix so I was little little bit okay scaler is just a number and scaler and these Matrix are are the are the aray of a numbers so how I can consider a scaler as ARS of a numbers that is the best assumption that I made at that point but no problem the scalar Matrix here's the here's your scalar Matrix in in front of you here's the scalar Matrix in front of you so you have 7 0 0 0 7 0 0 0 7 0 0 7 so listen so this is called scalar Matrix this is called the scalar Matrix this is called the scalar Matrix but now you will say hey I use just now you taught the diagonal matrix in the diagonal matrix you all the elements except the principal diagonal are equal or are zero then it is a diagonal matri so here also your all the elements all the elements except your principal diagonal are zero so why not be calling at as diagonal matrix so I would ask you to have a closer look at this is and tell me what you see over here so if you if you if you if you zoom in further or or if you or if you wear your sunglasses not sunglass if you wear your goggles with minus 2.5 power you will see that your diagonal matrix uh diagonal diagonals principal diagonals scalers are all equal okay so that's what make it as a scalar mates okay so what is scalar Matrix a scalar Matrix is say sorry not a scalar a matrix is said to be an scalar Matrix if and only if if it is principal if all the elements in the principal diagonal are non are zero and principal diagonal elements are should be equals to each other okay so that that is called the scalar matrices so if all the elements in the diagonal matrix okay so all the elements in the diagonal matrix and and what are diagonal matrix diagonal matrices are the matrices where the elements except the main diagonal are zero so if all the elements in the diagonal matrix are uh EX in all the all the matrices in the diagonal matrices uh are of the of of the D is is equal means the principal principal diagonal is equal then that is called the scalar Matrix so that is a scal matrix so please see the notes in description for the F the definition whatever I'm telling so you could not write it out please see the description for the notes of whatever I'm telling okay so so so so one of the so this is your first example so let's say second example second example otk 5 0 0 0 < TK 5 0 0 0un five what is this this is a scalar Matrix now now what what what what do I tell to you is to multiply with some multiply it's with with with the some Matrix okay so multi multiply with the same Matrix multiply with the same Matrix M multiply some The Matrix and then you will getting some other result some other result but I want to tell is to have a matrix like this uh where your all the diagonals are one all the diagonals are one that is your so now multiply with any Matrix just to just to make sure that is a matrix M multiplication is defined multiply with any Matrix or a vector any Matrix or or a vector you will be getting you will be getting your your your Matrix so this is this is this this this is your scaler Matrix so let's say s as as as of now and this is your any Matrix or vector v so when you multiply s * V answer will be V means exact so is it is by multiplying by one you will get exactly so please see please do and see the experiment so when you it this when you multiply this Matrix with some other Matrix or vector you will be getting it is just like multiplying this Vector this is this is one so whenever you multiply s * V means this these types of Matrix where your principal principal diagonal is one all are one then you multiply with some Matrix or vector then that then that will yield or or result to this Vector original Vector to which you multiply that scalar Matrix to okay so that is so so so scientist have seen this y result and named this as a identity Matrix name this as identity Matrix or a unit Matrix or a unit Matrix okay so when you when you multiply this identity Matrix with any Matrix that is simply by multiplying one and you will be getting a result which is V okay so you'll be getting the same result so you have you may consider this Matrix with one and if you multiply any even 10 you will be getting your 10 as output so it is same as that identity Matrix oh my gosh that that the one who's is just flying the of pollution I am really not liking that no problem again so here over here your identity Matrix are just like a m multiplication by one so please pre to this is a wi property which we have given a new name is the Matrix or the scaler Matrix here in your principal diagonals are all one then that's uh nothing but uh identity Matrix okay let's let's go on next page the next page so let's talk about some last matri with something called a triangular Matrix triangular triangular Matrix so the Triangular Matrix are of two types so a square Matrix I'm just try to Matrix so a square Matrix is said to be a triangular Matrix if the elements if the elements if the elements Ave oh my gosh my hand ratting Ave or below the principal diagonal below the principal diagonal principal diagonal are zero okay so for example uh you have the diagonal 3 4 6 1 2 3 and z z0 okay okay so this is your oh my gosh what do I made over here 3 1 2 0 4 3 0 0 6 okay so it is telling the Triangular this this is a triangular Matrix because all the elements if the elements above means Above This is this this is your principal diagonal this is your principal diagonal okay so whatever Ave if either above or below either above or below yeah here is your or okay so either above or below either above or below either above or below um to the main principal diagonal are zero then that's called a triangular Matrix so here above is non Zer but below is zero so that's how we call it as a triangular Matrix because it forms a triangle so that's why it's called a triangular Matrix and this is called the upper triangular and here here it forms the triangle so here it forms a triangular part okay so that is the upper triangle so this this where your zeros are below the main principal diagonal so that is called the upper upper triangular Matrix upper triangular Matrix and for example you have 1 0 0 2 3 0 4 5 2 and over here this is your main diagonal okay okay and above you have zero and below so that that is the uh that is the uh lower triangular Matrix that is a lower triangular because it forms a triangle triangle low and below below of the main diagonal okay so that is the lower lower triangle Matrix and upper triangular Matrix cool so I hope that you understood so just just to re recapitulate the Triangular Matrix is said to be a triangular Matrix if the elements above that principal diagonal or below the principal diagonal are zero the the the the the the elements uh the or or the what do you say if the zeros are below the principal diagonal of the Triangular Matrix then that is called the upper triangular Matrix because it forms the triangle upper of the principal diagonal and if the in in the in the Triangular Matrix to the of your principal diagonal of if your zeros are above your principal diagonal then that's nothing called as a triang a lower triangular Matrix that is of two types cool the last thing which I'm to discuss is about symmetric Matrix okay so is about symmetric matrix it's about what do you say repeat me with me symmetric Matrix symmetric who knows so you have this so you have a and when you do this so it should be foldable so that is symmetric so this this paper is symmetric okay so the same way the the the M matri can be symmetric as well the M matrices can be symmetric as well so what is symmetric Matrix so so the definition of a symmetric Matrix definition of a symmetric Matrix if your a transpose is equals to the a a is if your a transpose is equals to the a so that's where we call that as a as a as a triangle or or or a symmetric Matrix okay so it's it's it's it's called a symmetric Matrix if your a transpose is equals to A itself okay so so for example so for example you I'm just going to take you you can think of any example this is your task but I'm just just going to take a small example of uh 2 4 69 okay so 2 469 so when you add the transpose so this is your 2x2 matrix and then if you do the transpose you'll be nothing 2649 2x two 2x two okay so that is the 2x two so over here here you follow the equality of a matrix so the you if you remember the equality of a matrix if you remember the quality of a matrix of of the matrices so if if a matrix a is one equals to Matrix B if its corresponding elements if it's corresponding elements this to this this to this this to this this to this are equal and they are of same order and they are of same size okay so they are of same size but this is equals to this okay four is not equals to 6 so here this is not a symmetric Matrix okay so this is not a symmetric Matrix so your all symmetric Matrix should be should be square Matrix to be symmetric okay but not every Square Matrix can be symmetric but but you for for being symmetric your your Matrix should be squar Matrix for for being symmetric but it is but it's not guaranteed that your every Square Matrix will be symmetric but for being metric it is you have to have a square Matrix for example you have 2 2 2 2 apply the transpose on this what it would be 2 2 2 two 2 by two 2 by two this is this is correct this is correct corresponding are also correct the size is also correct that is the symmetric Matrix that is the symmetric Matrix and I hope that you understood about the concept behind symmetric Matrix so this is all so that was we had a talk on these stuff so I hope that you like this video and I also talked a lot and sorry for the crackers please find that guy and beat him as much as you can who's fing up the crackers outside so sorry I I I included Hindi but no problem okay please feel free to sco that guy not beat him because Dali is Festival of having fun but yeah please SC that because they disturb you in studying no problem uh so let's so I I I hope that you understood and please feel free to do your home homework assignments so here is the homework assignment uh the discussion for the solutions of the program programming assignment or sorry homework assignment is being released soon in the form of video so you can assist that but I will wait for two two or 3 days and then I will release one video on uh solving these homework assignments please feel free to do this and please feel and I'm not giving these notes because these notes are already available in the description down box below in the form of some uh good good hand handwriting in the description down box below please please feel free to assess thanks for seeing this video I'll be catching up in the next video till then byebye have a great day byebye hey everyone so let's get started with a new lecture on lecture number seven which is on determinant and this is one of the one of the again I would say important concept to study because in principal comat analysis or whether you uh it it it it comes a lot in your machine Learning Journey as well as well as in deep Learning Journey because it tells you how to solve or solving the linear equations or or or or if if I talk about in terms of linear transformation it just tells you how the how the how the change in area or a volume occurs okay and and determinant is nothing when you it's nothing but you just TR you just give some Matrix and then you get one number so we'll be talking about that in detail in this session uh I I think you'll you'll get a lot from this session and and you you can make your own notes or the notes is in description un boox below either it would be updated soon but yeah uh I it is it is already been made it's just sent for processing and that it will be into your description if you like this video please be sure to subscribe this channel as well as like this video and comment because YouTube algorithm knows okay this is a good video to recommend because many many other the people say uh your channel is underrated so I want you I want this channel to be rated Channel because I work a lot on this channel Okay cool so let's get start with solving uh what is determinant so we'll we'll get onto the geometric meaning soon but uh in in determinant what you do if you know about a square Matrix if you know about a square Matrix which which we talked about and and I have told that is very important Square Matrix are very important is used extensively in linear algebra to to use this term terminology so Square Matrix is nothing where your where your number of a rows is equals to the number of a columns for example uh your Matrix a is is maybe it can be 2 2 2 two okay so this is a 2X 2 where n = 2 and M = 2 so n * n Matrix where your Square Matrix is equals to where your number of rows is equals to the number of a columns okay so that is the so this is so what what you do you take your Square Matrix and determinant takes one square Matrix where the number of rows is equals to the number of colums you write determinant of uh an A and A A should be the square Matrix a should be the square Matrix and then you get one scalar or or or a number as an output when you apply the determinant function or or or when you take out the determinant of that Matrix okay so now how this is useful we will see how do we take out the scalar a just in a second numerically but but uh but when you um how how the determinant is useful this is this is one of the most important concept to know so the determinant is useful in in solving and solving linear equation and linear equation is used very very extensively solving linear equation or maybe it can be useful in in in in in in knowing okay in knowing how linear transformation and knowing how linear how linear transformation transformation change their area or the volume okay change their area transformation change their area change their area over volume or volume okay not over it's or volume and it is also and it is also useful uh in other stuffs like uh when solving some some computationally it it it it it does reduces some computer not exactly mean uh doing efficiently not exactly I would say efficiently I would say very precisely so solving the particular linear equation and is used a lot in that so that's why we take out the determinant of a matrix and that when you take out the determinant of a matrix you simply give a squar matrix root to that determinant and then after when you take out the determinant you will get one scaler okay so this is what the this is this is this is what we use and and if you if you talk about um in machine machine learning use case so in machine learning if if you know about machine learning in machine learning you have something called as dimensionality reduction method and in and in that you take out the determinant of that covariance Matrix so covariance Matrix okay so when you take out the determinant of that covariance Matrix and then you and then and and and and then go further into solving the particular problem okay so not exactly covariance yeah so you take out the termin and then you go further into uh into other stuffs like uh uh the igen vectors and igen values and they are extensively used the determinant are extensively used in the igen vectors and igen values in principal component analysis okay so I hope that this is clear why we use determinant and and and what's the determinant is now now we need to care about how do we take out the scalar value because we give a function because we just give a a square Matrix into that determinant and then we will we are going we are we are just getting a scaler as an output so how do we even do that uh so for for doing that assume that you have a matrix a you have a matrix a which is nothing but 2x two so I'm just going to write um a b c and d okay so you have a matrix a b c d which is a 2X 2 Matrix so when you take out the determinant of that Matrix a which is nothing but which is nothing but so a means you take out the product of the diagonals you take out the product of the diagonals a minus BC a D minus BC so for example you have a matrix uh 2 3 4 6 and then you want to take out the Matrix the determinant of that Matrix 2x2 matrix which is nothing but 2 * 6 2 * 6 3 3 3 * 4 3 * 4 which is nothing 6 6 2 12 3 42 that will be nothing but zero zero is the answer or determinant of this Matrix okay so the terminant of a matrix can be zero we have we don't have any conditions but yeah the determinant of this Matrix is zero okay so this is how you take out the determinant of a matrix geometrically speaking okay so one thing that I want to highlight over here let's say for for example uh what does it mean geometrically what does it mean geometrically so so let's uh let me make one more page so that I could explain you what does it mean geometrically speaking what does it mean geometrically speaking either I could just go on some website to mean to mean what is actually trying to tell so let's go on one website let's go on one website which I want to show you all is this one okay so assume that over here of over here you have let me choose my black color okay here it is so you have um a matrix a matrix a b c d okay you want to take out the determinant of this so this this is this is what you take out so for taking out the determinant you just write either in this A B C D giving a pipelines like this okay or or you write determinant of this uh a a matrix and this a matrix is either uh a b c d like this okay so this is the notation for sooning that you want to take out the determinant of this Matrix okay that pipeline that big big pipeline okay pipe uh line okay now over here your a is 1 your B is zero your C is zero and your D is one okay you want to take out the determinant of this you want to take out the determinant of this you want to take out the determinant of this so how do you take out so what does it mean geometrically speaking so geometrically what it's trying to tell is when you plot this Matrix over here first of all you take this and then you go over here so this is nothing but the determinant of a 2X two Matrix is the area of a parallelogram with the column vectors AC and BD okay so this is the the the determinant is nothing but the area of this parallelogram of this parallelogram where the column vectors are AC and BD okay so when you when you plot the 2x2 matrix which which looks like this and and and this the the the determinant which means jum Al speaking is nothing but area of that parallelogram which formed by joining everything and then and that area of that parallelogram is nothing but determinant of that Matrix okay this is what does it mean geometrically speaking uh I would ask you to watch one video on three blue one brown to see how how is shown geometrically but yeah uh the the det terminal is nothing but the area of that parallelogram whatever forms so for example you your par so let me reduce the a a bit and then let me do something with this I don't know well how it is working yeah so let me do something like this and let me increase the area okay let me increase the B okay here it is so when you have the column Vector when you have a column Vector at 0.86 and zero okay and then you have another column vector which is 0.52 and the the parallelogram is formed is nothing but your favorite the determinant okay so this is what the determinant means and you can play with it by just going to demonstration wallframe and this with this website so let's go on the 3D view so how does it look 3D so 3D is nothing but area area of that parallel Zoid okay so if you just see over here the area of the paraloid is is nothing but a determinant we'll see how to solve how how to solve this deter this determinant one okay we'll see how to solve um three for the how to take out the determinant of a 3X3 Matrix and we will also see how to take out the determinant of a n byn Matrix okay so it's a it's a bit hectic task but we will try to do it so this is this is what the geometrically means and for 2D the area of a parallelogram and for 3D area of a parallel Zoid okay which you can see from the diagrams which are shown over here so if you just if if I could zoom in I can't zoom in but yeah I can just show you this is this this is what you have your uh 3x3 Matrix and then you this is the paraloid which is formed and then when you try to take out the determinant of this is nothing but the area of this parallel Zoid okay so this is what it means and the determinant geometrically is nothing but the area of a parallelogram or parallel joid in 3D dimension okay so this is this is what you need in in a geometric intuition just just just to make sure that what the geometrical it it means okay so now let's see now one of one of the important thing which I want to show you up is is is we have seen we have seen how do we take out the determinant of a 2X two Matrix so the determinant so here is your a here's is your here is your a and you have and then you want to take out the determinant of this A B C D and I'm just writing pipe to denote okay this is a determinant so when you take when you try to take out the determinant of this so it's nothing but equals to uh uh a a minus oh my gosh it's a D minus BC that's uh then when when you take up that's a uh simple scalar which is e not exactly that not 3+ 3.71 1 it's e okay so let's let's give it any scaler which is e okay so this is this is what it means in 2x2 matrix I'm talking specifically 2x2 matrix now now let's talk about how do we take out the determinant of uh 3x3 Matrix so determinant determinant determinant of 3x3 Matrix 3x3 Matrix so how do we even approach we taking out so you have want to take out the determinant of a b c d e f g i okay so G h i is this is your Matrix this is the determinant of this Matrix okay so how do you take out how how do take a determinant of this Matrix and of course your it should be a one scalar okay it should be one scaler or a number or number okay so how do we take out the determinant so can't we do a * a * e * I and then it will not work this is this is not you can you you can just guess how do we do it just try and commment and maybe I can just see and be a bit funny in job so please be sure to write it and I will try to see what you write it okay so so let's start approaching how do we even approach this problem so what we do we simply so so what we do just just make sure that first of all we go to the a11 okay so me first element in that Matrix and then what we do we simply leave this uh column and this row and write a minor Matrix or a submatrix of that of of that uh big Matrix or you can say that we take out the minor of this Matrix how do we take out the minor of this Matrix you simply when for for example you choose this number okay so what you do you you leave this column and you leave this row and then you write uh and then what you do you take out the minor and then you take out the determiner determinant of that by multiplying by a okay so the first element is this and then you have e f h i we left this column and this row and then we write e f hi okay we want to take out the determinant of EF HR okay now what you do now what you do here is your plus sign now it will be a minus sign over here okay you go to the B you leave this column and you leave this row okay which is nothing but b and d f g i DFG because we left this column this row and this column just d f g i okay and then here is your minus then here will be plus plus uh you write C now we left this column this this this column and the first row which is which will be left the determinant of D GH okay and then we have convert now these are called a minor or a submatrix submatrix or the minor of our Matrix a these These are called the min minor these are called nothing but the minor these are nothing but called the minor minor of our Matrix of our Matrix a okay so when you try to now it is very easy a * a * uh EI now you can just apply your 2 x two a EI and FH EI minus FH okay minus b d i FH d i minus FH okay plus C and then you have DH EG okay DH minus EG okay and then you'll be left with some scalar and then you can simply do do this thing and then you simply multiply with this and then you do do some calculation and then you'll be getting your output as maybe some some scalar some scalar value okay so let's see one of one of the one of the one of the problem or or the stuff to to see how how it looks like Okay so let's let's assume that you have a a matrix or 3x3 Matrix so here's a question for you okay maybe you can try try to approach it uh the you want to take over the determinant of I'm writing this pipe that denotes that you want to take out the determinant of that uh for example 0 1 2 uh 1 2 0 uh let's let write 1 1 0 okay just a random random I'll be walking you through it so take out the determinant of this this is a 3X3 Matrix try to take out the determinant of this so how do we take out so first of all we go through the first element and then what we do we take out the minor of this Matrix so the minor so we leave this column and this row so we'll be left with zero and then we and then we write out minor and then we take out the determinant of our sub Matrix okay plus now no no no it will be not plus over here it will be minus because here is our plus minus okay one you leave this column and this row which will 1 0 1 0 okay so 1 1 0 1 0 okay and then you write + 2 and then you have uh you leave one to one one okay you leave this column and this row okay you leave this column and this row you'll be left with one 2 1 1 okay and then you do the sum and then you do the sum so and then you take and then what you do you try to take out zero 2 * 0 which is 0 0 okay Min uh 1 * 1 * 0 of course 0 and 1 * 0 0 okay plus two uh 2 okay 1 * 1 1 2 * 1 2 okay then you'll be left with of course zero then it will be done then done it will be also 1 * Z which is nothing but zero okay it'll be left it 2 * 2 2 * 2 that that will be 4 okay which will which is is your determinant of this Matrix so Min 4 is your determinant of this Matrix which you are seeing over here so sorry here is 1 2 it's not it's it's simply min1 so 2 * 2 is the determinant of this Matrix so I'll be so here you got the determinant of this Matrix which is nothing but min2 okay so this is this is how you take out the determinant of a 3X3 Matrix as well so there are there are some problem for you to work on so I'm just just going to write it out so there's one problem which which which you can approach okay so you want to take out the determinant of uh 371 4 please answer the HW please answer in the comment box it's just a quiz which you will see in your attendance as well okay so this is what the determinant of that Matrix of the 2x2 matrix or 3x3 Matrix we'll try to solve the determinant of a 3X3 Matrix using lianes formula or the rule of surus okay so which is very good formula to work on so we'll see that but before that I want to highlight some of the properties some of the properties of that properties properties of determinant Matrix of determinant of a matrix determinant of a matrix the first first one is the first one is the first one is for example you want to take out the determinant of this Matrix for example you to take out the determinant of 1 0 uh 0 1 okay so try try try to take out the determinant of this 1 * 1 which is 1 uh and then and then minus 0 okay what what it will it with one and can you identify this is an identity Matrix even if you have uh three three identity Matrix then that will be nothing but that will be nothing but one so whenever you have identity Matrix whenever you have if if if it is if it is identity Matrix if it is ENT identity Matrix identity Matrix then then then the then the then the determinant of that identity Matrix will be one okay this is this is one of the property second property if the if the rows are the same okay so for example if the rows are the same are the same for for example a a b b okay a a b b then a minus ba will be nothing but zero okay so this is another property third property is you have a scaler multiply it with some uh a and you have another scale c and b d okay so what it will be it it just makes sense r a D minus r CD or r r CB okay you can write write it down like this and then it's it's nothing it's nothing you just you just take that out of out of okay you just uh take take that as a common r a D minus BC okay so we can write this as a we can write this we can write this as a r * a b c d either we can write it now so we proved it so it is just equivalent you can write this so for it will be easily for us to solve okay so it is so it is R times as either it is same as over here so we a D minus BC so it's just equivalent to that so this is another property which you see a lot in while taking of the determinant of a matrix okay so this is these are the some of some of the properties which I want to highlight in front of you so now let's go on to the another stuff is how do we take out the determinant how do we even bother taking out the determinant determinant determinant of 3x3 Matrix so you can just say okay I'm just going to just going to take out the minor of the sub Matrix of the Matrix and then I will do that so here's another another trick which is called the rule of surus I think it's it's it's not a I would say uh okay it's a good technique but okay you can try it out but eventually I like that my Approach but yeah it is very very straightforward approach which I'm going to tell over here okay so assume that you have a matrix M that you have a matrix M Okay so a11 1 a12 a13 okay a21 a22 a23 a23 a31 a32 a33 okay so you have this 3x3 Matrix now when you wanted to take out the determinant determinant of this Matrix M so how do we even bother doing that so for for for for so you can use the rule of suus rule of suus I think that funny name he has but yeah again I'm no no want to comment on on his name he's a again a great people okay so not I'm not even a one 1% of these people so these are amazing people who give a lot to the world so I think about I'm no one to say about but yeah amazing name so what you do you take out the determinant so you want to take out the determinant of a11 a12 a13 okay so this Matrix I'm just writing this Matrix a21 a22 a23 okay a31 a32 a33 okay and then what you do you take out the first two column and write it in another format like this a11 a12 A2 1 this is a trick for solving a 3X3 Matrix a22 and a 31 okay so a31 and a32 so this is a22 okay it's say a21 okay so this is what you now you write this now what you do now what you do so what you do you you simply take out the product you simply take out the product like this the first diagonal okay so this is the diagonal so what you do a11 a 22 a33 okay a11 a22 a33 plus plus a12 a22 a23 okay A2 3 and a31 so what you do you take out the product of these three you take out the product of these three so A1 2 a uh uh 23 okay a 23 uh and a 31 okay now what you do you simply uh do this simply multiply the next next diagonally okay so plus plus a13 a21 okay I'm I'm I'm I'm doing a bit messy so let me do a31 uh if I'm not wrong a13 a a a13 a21 and A3 2 okay A a A1 3 A2 1 a32 now you are done with this now what you do now what you do you now go from bottom to top here you are going from top to bottom now you'll go from bottom to top by changing the sign now okay now you go from bottom to top so here's how you do here's how you go further okay so so the way you go is you have a31 so from here so from here a 3 1 okay and then a22 and then you go to a a13 so now you start going at this side like like this okay so a31 I'm just going to write a31 a22 a22 and then a13 a13 okay and then what you do and then plus no uh you you minus because you change you go from bottom to top so here you minus it now now you go at this one now you do this and then a32 okay this one this one and this one a32 * a23 yeah if I'm it's a it's a a23 if I'm not wrong yeah a23 and a a11 a11 okay now minus now this is done now you go at last one which is this one a33 a21 a12 okay and here's how you take out the determinant of a matrix using suus rule okay or a rule of suus okay so and then you'll be getting after after doing this all those stuffs you can just cancel it out something if it is so you just you just do do the computation then here's your Matrix and this is just a scalar number or other stuffs so here's the rule of suru so here's how you do you do simply you do you do write the first two column uh at at the side so that it could be easily so uh so what you do you simply take of the product from top to bottom for the first three and then you take out the bottom to top for the second three starting from the last okay so here's here's here's what the full the rule of sarus means here here's how you take out the determinant of that Matrix like this okay so now I'm going to talk about is uh you can see the Wikipedia Pages for a libanese rule because they write a very very kind of libanese stuff so you can just go there and see more see more about this rule okay so the next thing which I'm going to talk about is the next thing which I'm going to talk about is how do we take out the determinant how do we take out even B of taking out the determinant of n by n Matrix the determinant of n by n Matrix so how do we even take out that and how do we even bother taking out that okay so here's so I'm just going to write the N by n Matrix as I'm just going to Rite the N by n Matrix or let's start with a particular example let's start with a particular example so it would to totally make sense okay so let's let's start with a particular example and then at last we'll just write a definition and then we'll end this video okay the example is bit long so I'm just going to maintain my handwriting so the example is you want to take out the determinant you want to take out the determinant of a 4x4 Matrix 1 2 3 oh my gosh three 4 okay 6 6 6 9 2 1 and then you have a 4 9 2 1 and then you have a 0 1 1 one okay so here's here's your determinant of this Matrix so to take out the determinant of this Matrix so how do we even approach taking out the determinant of this Matrix so how do we take out the determinant of this so for taking out the determinant of this so for taking out the determinant of this for for taking out the determinant of this you take out you take out you first of all take a the minor or the submatrix of this okay so you you go to the First Column first element and then you leave this column and this row and then write the sub Matrix so you just one and then you take out the determinant of 9 2 1 9 21 1 1 1 okay this is plus sign so it will be minus sign now you go at this uh take take out the sub Matrix leaving this row this column and this row so it would be two first of all you take product it so you multiply with that 2 2 * uh the determinant of 6 2 1 6 2 1 42 1 and 011 okay and then you uh change the Sign Plus and then go through with three and then you leave this column and this row so it will be nothing but uh three and then you have 621 I also just write okay 66 6 91 I I just leave it so I'm three 691 691 uh 4 4 91 4 91 and 01 1 okay 01 1 now the last one is there four okay so there is four and then you take out the terminant of leaving all the all the uh column one and then row 692 692 492 and uh okay I think it's wrong 492 and 0 1 1 okay so these are the sub Matrix of that Matrix let's name it as an n M okay so this is a matrix M and then you have to take a determinat of that Matrix M so here's the 4 4x4 now you do this now you have this now what you do now here you convert it to 3x3 determinant now what you do you convert that to a 2x2 here's how you do so you you don't want to use a suus rule because I I eventually don't like that rule it's very hectic rule sometimes it maybe cause you error but no problem in that so here's how you do it so first of all what you do so first of all what you do you you simply uh multiply uh one okay you simply go ahead and take take your uh one as a so if you can see I just want to take that one as an uh uh this one and then what I and then I go and approaching this so here is your nine so first first of all go at this element take out nine you want to take out the sub Matrix of this Matrix so nine and then you take the termin of 2 1 1 1 so what how this came 2 1 uh you leave this this row and this column 21 1 1 okay now you simply change the sign minus okay you make sure that t you're doing only doing for this you're only doing for this we'll come to this later on but we are only doing for for this a21 okay minus now now we go to this two now we go this two we leave this column and this row which is 91 1 1 okay so which is a what happened yeah so which is nothing but uh what do you say uh two because here is our two leaving this row this this this column and this row 9111 okay uh did the determinant of 91 1 1 okay and then what you do plus now you change the sign and then you go at last 1 leaving this 9 to11 okay so the one and the 921 1 okay so you take out that okay now this was plus now you make it minus okay now here is your two so I will take that outside I will take take that outside okay and then I will just go ahead into solving this so you take this take this as a now you take out the minor of this Matrix or the sub Matrix of this Matrix so here's how you do it so you simply add it minus so here is minus 6 uh 21 1 1 so here here's how you go with this you ignore this column and this row 211 1 now you go to this you ignore this 41 0 41 0 1 and then you go over here ignore this 42 01 okay so this is how I'm I'm I'm going to write minus 2 okay because you go over here 2 over minus 2 CH changing the sign take out the determinant of 4 1 0 1 4 1 01 okay and then you simply plus um now you change the sign you go one go to go to one 42 0 1 42 0 1 uh + 1 uh 4 2 0 1 okay and then what you do and oh my gosh yeah so then what do you do now you now you converted that 3x3 Matrix for this one and for this one now you go to this one okay by changing the sign plus plus and then you go and then you write separate three now you write separate three and then you take out the first one six okay so you leave this SC column and this row which is nothing but uh six and then 9111 which is the sum Matrix of that Matrix Min 9 because this plus 9 and how how how came you go with this column leaving this column and leaving this row 41 01 which is nothing but 4101 okay plus changing the sign 149 01 how this 4901 came is you have this you leave this and this you leave this column and this the row which is 4901 okay so this is how you came it and then and then you're done okay now what do you do you you do for the last one you do for the last one this because you you done for this you're done for this you're done for this now you converted that to a 2x2 MRI which is easily deter which which we can easily take out the determinant now you go to this okay so here's how you do it so Min 4 okay and then what do you do and then what do you do you leave this column and this row taking out the first element so six take the determinant of I was I think it's where it was 9 9211 9211 9211 minus minus 9 okay so over here it was leaving this the SE going to this and leaving this column and this row which is 4201 I I I think about it yeah that's 4201 4201 and then you change the sign plus 2 49 I think it's it's it's more about you leave you go over here 4901 okay that is 4901 okay now you are done now this is what you have written so you converted the first Matrix the first Matrix this one this one and this one as well uh into a minor Matrix which is 2x2 determinant so you can easily take out and then do the product and then you take out okay so let's do over here if if I have a chance to do over here but no problem I will do over here okay so here's how you do it here's how you do it so for doing it first of all you have the one available which is over here you have the one available which is over here what do you do you simply 9 16 + 7 how how how we came so you have the particularly nine times because of course you want to always want to multiply it out okay so I think about this is you have uh if if you go over here 2 * 1 okay and 1 * 1 so 2 2 2 * 1 how much 2 * 1 how much it would be uh 1 1 I would say uh 2 1 which is 1 so it will be 9 okay minus minus over here uh 9 9 * 1 which is 9 1 which is 8 16 16 done and then you have 9 * 1 and then minus 2 * 1 so 9 2 which is 7 okay so here's how it came okay and then you then this the left minus two now you go on second one two over here so when you when you take a two * 1 how much 2 * 1 how much 2 * 1 2 1 okay that is 6 over here here so we write uh 6 8 + 4 okay so here's how you do so it is 6us 4 * 1 of course 4 * 2 it's 0 * 1 of course 0 so 4 * 1 4 * 2 which is 8 which which you have written over here okay then you go over here 4 * 1 how much 4 * 1 4 and then you four so here here is a plus 4 now now you go to the next + three because you go over here now 9 * 1 how much 9 * 1 how much uh 9 * 1 9 of course 1 uh which is nothing but 8 8 * 6 48 so you have 48 36 + 4 okay so here's our 48 9 uh so 4 * 1 4 so 4 9 49 36 because this 2 * 0 is 0 then you go over here then you have this 9 9 9 * 0 is of course 0 4 * 1 so 4 2 are 8 eight so over here uh I think I'm wrong over here um you have this 4 * 1 4 91 0 0 so 4 uh it's it's it's it's four okay so it's four it's it it it it it should be eight okay it should be 8 which is nothing but what you do you 4 * 1 4 and then you simply multiply with two which is nothing but 8 okay now you simply 4 4 uh 28 30 6 + 88 so how you take out 9 * 1 how much 9 2 7 7 7 6 7 76 how much uh oh oops it's it's uh it should be 9 * 1 2 987 76 42 okay so which will be nothing but 42 what the hell i' written over here so I have to just couple it out so I have to just return it it will be nothing but 42 okay minus minus minus uh minus 4 * 1 of course 4 9 are 36 plus 2 4 4 1 are 4 4 2 8 okay so here's how you do it now you simply multiply with this and then first of all do the calculation do the calculation then you'll be getting one a scaler as your output so please feel free to put your answer in the description box below I know the answer but yeah I want to leave it to you to do the rest of the calculation I have done a lot so here's how you take a the determinant of n byn Matrix and how you do it and how you do it it's very very easy you just uh keep converting that to a lower or a sub Matrix and then you and then after that you are done okay so here's how you do it so you you define one you define one sub Matrix AIG J you define one sub Matrix AIG J which is nothing but the Matrix the n n minus 1 * n1 Matrix n1 n1 Matrix if you ignore ignore the I row and I column if you ignore if you ignore the I row which which you are doing I row and J column which which you were doing okay that is your new Matrix which which we were forming that is a recursive this is a recursive you can write a Python program for write a recursive solution for this okay so you were writing a one one and then you were adding the determinant of that subm Matrix and then you are doing so and so on so on that is a recursive solution so you writing the recursive recursive stuff so we we we already written a lot so I hope that you understood for formal definition which you can see yeah we have gone through one of one of the example which is very very much important for us to know okay so I hope that you will uh get a lot from this video and determine it I hope that concept is clear with my examples and I also hope that you enjoyed this video I think I have to wrap up with wrap up with this video I'll be catching up your next video till then byebye have a have a great day meet you in the next lecture okay everyone let's get started with another lecture I know it's bit L late lecture but I apologize for that I'll be I'll I'll be making making sure that I'll be providing you around three to four videos this week so I already provided two videos now I think about two of three videos will provided more this week till your next uh assignment or homework assignment and please make sure that your homework assignment is released if we find any student who are not active we will remove them from our LMS because this is an opportunity which are given for free for others to learn because if the people do not take this opportunity we are going to drop that student so uh we we we highly recommend to to to to be active on LMS please do your assignments please attend your attendance and every stuff okay so please please go there and Mark your attendance and as well as uh complete your assignments even if you complete around out of 25 questions you you have to complete around 20 questions you can complete 20 questions write right in Notebook and then give it to us okay so you your programming assignment sorry the homework assignment will be will be evaluated and then that will be uh till the end end of the course and if we do not find you active in the course we will drop you out okay so this is one of those announcement that our team has told me to give me to give it to give it to you all through Me Okay cool so so what's the mod of this lecture the M of this lecture to talk about the coactor the minor and educate or the adjoint and invoce of a matrix so these are very very correlated these are very very correlated uh for for taking all the coactor you need mind and for taking out the minor you need determinant and for taking out the aducate you need coactor and taking out the inverse you need ugate okay so again I'm explaining for for for taking out the coactor of a matrix for taking out the coactor of a matrix you need minor and for taking of the minor you need determinant and and and and after after you take out and for taking out the adjugate for taking out the adjugate for taking out the adjugate for taking out Agate you need coactor and then for taking out the inverse of a matrix you need aducate okay so these are very very correlated and they are heavily used for many this inverse of a matrix because they are so correlated so I thought okay let's let's start with this video so that everyone knows about coactor how the N inverse of a matrix is calculated because it is extensively used in the industry okay uh mainly in linear algebra there's not too much use in machine learning some sometimes us machine learning but it's very very good to know about these stuffs okay so first of all how do you be now let's let's let's go ahead and talking about the first two stuff which is which is a minor of a matrix or the coactor of a matrix so let's start with a minor let's start with a minor minor of a matrix so so so a minor of a matrix a as as if if you remember the minor the a minor of a of of a Matrix a a minor of a matrix a matrix a is the determinant of the small same some some smaller um Square Matrix um a minor of a matrix a is the determinant is the determinant is the determinant of some of some smaller of some smaller squar Matrix as you remember remember that what we do what we do if we were removing the column and the row for that for that point or the element and then we were we were obtaining a sum Matrix in the determinant and that's actually when you when you take out the determinant of of that some Sub sub Matrix that's actually the minor we we will see see one example just just in a second for example for example let's take an example that you have the following that you have the following uh Matrix okay you want to take out the determinant of this Matrix or the minor of this Matrix you want to take out the minor of this Matrix so for example you told okay you want to take the minor of Matrix M for I row and J column okay that so you take out so you for example you choose okay you want to remove the second row uh and the third column okay so you are saying 2 3 so you to you want to take out the minor of a matrix given I = 2 and J = to 3 so what what it will do it will it will leave second row and it will leave the third column okay so the minor of this Matrix a will be left with the the the sub Matrix will be left with 14 1 9 as you all are knowing okay so now when you take out the determinant of this Matrix so 1 minus uh this is first of all for taking on the determinant of a 2x2 matrix what we do we simply what we do we simply uh multiply the DI diagonals and then sub subtract it so 1 uh my time uh 9 okay minus uh of course uh sorry this minus and minus 4 okay so that will be left with 9 + 4 which is nothing but 13 so 13 is a minor of a Matrix given I = 2 and J = 3 so what does it mean we leave the second row and third column or we delete the second row and third column to obtain the sub Matrix so that is the minor of that Matrix okay so again explaining what we do we simply remove just one row and one column and we take what row you want to remove and what column you want to remove by the user I and J you you remove one row and one column from the square Matrix and make sure that is a square Matrix what is the square Matrix the square Matrix is the one where the number of rows the number of rows matches with the number number of columns okay so the number of a rows matches with the number of a columns and over here when you take out the minor of this Matrix a given you remove one row and one column so you rem the remove the second row and the third column for example if you want to take one one so the minor of this will be you have you want to remove the first row and First Column the minor will be determinant of that uh 3 0 so sorry it will be it will be I think 0 5 91 so take the determinant of this so 0 0 * 11 0 9 * 5 45 what it will be it will be 45 or 45 okay so that will 45 around according to this okay so that's that will minus 45 so that's how what this minor tells you minor tells you okay you want to take out you want to remove that column I you want to remove that I row and J column and then write write down the Matrix and then take out the determinant of that submatrix and whatever the determinant will be that will be your minor of that Matrix okay so this is this is what it's trying to tell you so over here oh my God what is this where is where is my pen so this is what it'sing trying to tell you over here okay so what does minor means minor of a the a minor a is the determinant of some smaller Matrix some a smaller Matrix okay so a minor a matrix a is the determinant of some smaller Matrix for for example which you're seeing over here and then you so how do you take out you remove one row and one column and ride the rest of the Matrix element into a new Matrix and then you take out the determinant of that Matrix and the whatever the determinant a scalar value that scalar value will be uh the minor of that Matrix okay so this is how you calculate the minor of the Matrix so so so so so using minor of a matrix you can calculate the co coactor of a matrix using minor of a matrix you want to calculate the coactor of a matrix so why do we you why do we need to calculate the minor for Matrix to calculate the coactor you we need uh we need to calculate the minor so what does it mean so why do we even care about coactor why do we need coactor So Co coactor is required for coactor is required for computing determinants Computing Computing high level determinant or larger determinants okay Computing larger determinants or determinants and taking out and in taking out in taking out the inverse of a matrix indirectly inverse of Matrix indirectly as it is not used directly over there you want to take out the adjugate of that for taking out the inverse but adjugate uses adjugate us a CO coactor and coactor is being indirectly contributing to taking out the inverse of a matrix we'll see the inverse of Matrix just we will visit after some slides okay so this is this is what it means to be minor so let's talk about the coactor of a matrix let's talk about the coactor of a matrix so what is the coactor coactor is calculated first of all we need to calculate the the the minor so for example you have the following uh Matrix you have the following Matrix you have the following Matrix 147 305 1 911 okay so this is this is your this is your um this is this is your 3x3 Square Matrix you take out the minor of this Matrix you take out the minor of this Matrix given I to be 2 and J to be 3 okay so you want to take out the determinant of the sub Matrix of the sub Matrix of the sub Matrix so for example I'm just taking one example given I = 2 and G = 3 so what you do you leave the second row and third column the second row and third column so left with 1 4 1 9 so this is and then when when you take out the determinant determinant of M2 3 that will be what 9 9 * 1 9 um 4 that will be nothing but 13 and as we shown 9 first of all the product of the diagonals and subtract it minus 4 so that will be what 13 okay so that is the minor of that Matrix now when you take out the minor of the Matrix how to calculate the coactor of a matrix so for calculating the coactor of a matrix I and j i and J and make sure that I I I matches with the minor minor of that and J matches with the minor of uh or uh whatever the uh J over here so these these two should match equals to 1 the power of I + J * the minor I and J okay so this is how you calculate so for example for this example let's calculate the coactor so how do how how do we calculate the coactor so the here I is 2 3 J is 3 is which which is nothing but equals to 1 1 2 + 3 * 13 okay so whatever the value will be over over here we don't care of that it will be minus one because it's five mean min1 to ^ 5 what it will be 1 of course because if it if it is 6 then it will be 1 so it is 1 * 13 so the output will be the the answer will be Min 3 will be the coactor of this Matrix given I to be two and J to be three okay of that minor okay of that of the two three ENT three okay so what how you write the coactor the coactor of 23 entry where two is the row and J is the uh two is the row and three is the column entry is min 3 okay so this is how you C calculate the the coactor of a particular Matrix okay so let's see one more example to make intuitive sense to you so that it would not left in sense oh my God what is happening with with some example so let's take an example that you have 2 4 6 21 2 1 one1 so this is your Matrix so you select okay you are selecting the third row you're selecting the third row you to take out first of all minor so going to take out the third row and the first column okay so third row and the First Column okay so you to take out the minor of that so first of all you leave the third row and the First Column so the minor will be the minor will be the deter the The subm Matrix will be at what uh 4 61 2 and when you try to take out the determinant of this what it will be 4 * 2 6 * 1 which is nothing but 8 6 which is nothing but 2 okay so the minor of 3A 1 entry is nothing but equals to 2 okay so after after you calculate the minor if you calculate the minor you wanted to calculate the the coactor because you want to calculate the coactor so for calculating the coactor you need a minor so you taken out the minor now when you calculate the coactor of 2x3 entry of the two two sorry it's 3x 1 now it's 3x 1 3x1 entry which will be what uh which should what so c i j so formula is c i the coactor of three I and J entry is nothing but 1 to the power of I + j i + J times the minor Matrix I and G so in this example 2 3 = 1 2 + 3 * 2 * 2 so what it will be it will 1 to the^ of 5 * 2 okay so that will 1 time 2 which will nothing but minus 2 is your coactor of that Matrix okay so the coactor of that Matrix for that ENT 3 3 3A 1 is 2 so this is how you calculate the coactor of a matrix so I'm just just going to ride the steps for you to calculate the coactor of a matrix so first of all you take out you take out you take out the minor you take out the minor take out the minor m i j how you take out the minor so first of all you take out the sub Matrix so you remove the one row I throw I'll just remove I just going to write remove I row remove I row and J column okay and then whatever the sub Matrix would be left and then sub Matrix sub Matrix and whatever the sub Matrix take take out the determinant of that sub Matrix okay that will be a minor after you take out now you can simply calculate the co factor which will be nothing but C J which is nothing but 1 the power of y + J * m j okay so this is how you calculate the coactor of a matrix c i j for that I entry okay so I hope that you understood what's the coactor and what's the com Miner of that so some some of the applications so some of the application so here over here what you're trying to actually do is to uh so we can write it's basically this Co coactors are basically used in prominently in lapl formulas for for the expansion of the larger determinants okay as as we have already seen so the formula of determinant of a which is nothing but I = to 1 all the way around to the n a i j a i j 1 I plus j m i j okay so this is for taking of the determinant as as I told you one of the most best application of uh of the coactors it is used in is a laplus formula for taking out the determinant of larger determinant so this is the formula for taking out the determinant of a by using the coactor so I = to 1 all the d m a i j times uh uh times of course minus one this is the coactor of your Matrix and this is a minor of that okay for that to I I and J entry now so we had seen the co coactor we have seen the coactor and we have also seen the minor and we have talked a lot about determinant and we talked a lot about determinant now let's talk about now let's talk about uh the Agate of a matrix so the adjugate of a matrix or some sometimes call it as a adjoint of a matrix so adjugate Matrix okay so we have to take out the aggate of a matrix so let's write it out so when how we take the adjugate for Matrix so first of all what is the adjugate of for Matrix so the adjugate of a matrix or the or sometimes there are lots of names sometimes we call it as the adjugate sometimes we call it is a classical adjoint classical adjoint and a lot more okay so this is a classical adjoint we also call it as that so how do we take out the adjugate of a matrix is nothing but the transp of its Co transpose transpose of its coactor Matrix coactor Matrix so Agate of a matrix nothing but the transpose of its coactor Matrix okay so here's how you define it so the aggate aggate of a matrix a is nothing but transpose of that coactor Matrix okay so this is a coactor coactor matrix which you taken out for every for every I and J in your Matrix okay so Agate is what is the transpose of that coactor Matrix so here are some properties so over here the adjugate adjugate of a is nothing but C transpose and it is nothing but uh minus one of course I'm writing the formula I + j i + J time M this is this is the formula for Cal calculating okay and and I and J should go from for all the elements so I and J okay so this is the I and J and this is how you take out the aggate of a matrix this is the definition of the aggate of a matrix okay one of the property is a times the inverse of the Matrix It it means you have a matrix a and you have a matrix inverse of that so the output will be of always the identity Matrix when you multiply the a matrix and the inverse of that Matrix so what is inverse of a matrix so here's how we deal with it so inverse of Matrix of inverse of a matrix is nothing but the AI how you C calculate it if you know about 1 / a this is this this is what we write but here's how we do that so 1 over the determinant of a one over the determinant of a times the Agate of a times the Agate of a and how do we calculate the Agate of a we calculate the aggate of a by taking the transpose of our coactor Matrix and you all know the for taking out the coactor Matrix we have this equation 1 the^ I + J * m sub subscript I and J okay so this is how you calculate the uh inverse of a matrix you calculate the 1 over the determinant of a times the adjugate of that a adjugate of that Matrix a this is how you calculate the inverse of a matrix it's not a big deal it's just a small stuff which you are seeing over here okay so so we had talk so we have we have a talk about inverse of Matrix so I'm just going to write it out in one detail about what we had a talk on this now and one one more thing over here if your the determinant of a the determinant of a is equals to zero then your Matrix The Matrix is not invertible if your determinant of that a is zero then your Matrix then your Matrix a is not invertable then it will be nondefined okay so let's uh let me write it out what we had studied we have our coactor we have our coactor of a matrix coactor of a matrix is nothing but uh minus 1 I + J to the power of time m i j okay and then you take out the the the adjugate of a matrix the adjugate of a matrix a is nothing but what the the coact the transpose of our coactor Matrix and for taking all the inverse of that for that Matrix for for for for Matrix a we have 1 over the determinant of a times times your uh Agate Agate of a and if your if your in uh the determinant of a is zero if determinant is zero then then your your Matrix is not invertible your Matrix is not invertible is not invertible okay so this is this is this is what the whatever we have studied in this video and I hope that you are able to understand so in the next session we started started talking about uh the systems of equations and and and and and we also talk about uh and then we will talk about igen vectors and igen values we'll also talk about Rank and rank of a matrix and choice of a matrix so I think we should end this video I'll be catching up your next video till then byebye have a great day hey everyone welcome to this next next lecture on trace of a matrix so in this video I'm going to talk about trace of a matrix okay uh Trace will will will will I I I will talk about how do we calculate the trace of a matrix and this video is not only about Trace we'll we'll talk about something called as hardart product which is one of the most important stuff we'll talk about its properties we'll talk about a cyclic property of a trace as well we'll talk about some more properties of a dress trace and then we'll talk about what are some exceptions in Trace and then we'll talk about our uh ner product I don't know how to pronounce it I think think a k is U silent over here so I think Ron roner products we'll talk about that and how with one with one example okay so this is the agenda for this video and the notes for this video is in description box so I hope that you will be able to go in the description box and see there uh the the notes of this video and maybe if and if if you're wondering whom to whom I'm writing on it's the updated Microsoft whiteboard and and I and I simply think that's that this is amazing this is going to beat every whiteboard available in the market uh so this is just an information from my side uh of the stuff so let's get started talking about a trace of a matrix but before that I want to I I want to give you some some class announcement the first announcement is we are removing around five students from the course by personally emailing them first of all understanding their what they're facing and if we think okay we can remove if we don't get a response from that student we are going to just remove them from the LMS okay so new seats are being available so you can go ahead and and draw for that LMS for absolutely free it's absolutely free so you can go ahead and assignments are also being released okay so if you're not able to do the assignments you are most likely uh not going to get uh you are not getting the grades and you'll not most likely not able to pass the course okay so so let's talk about trace of a matrix so so so these are some of the class announcements so let's get get ahead and talking about a trace of a matrix so so for for taking out the trace of a matrix it should be a square Matrix so for example let's say you to calculate the trace want to C calculate the trace trace of a square Matrix so you want to calculate the square of you want to calculate the trace of a square Matrix a okay and usually write that and and usually write that TR R and so this is the this is the formal notation for denoting that we are taking out the trace of the Matrix a so this is read as a uh taking out the trace this this this one is nothing but your Trace there's nothing but your Trace okay so so you're actually taking out the trace of a matrix a cool so I think that this is this is good to go so the trace of a matrix a is is defined by T TR and then in bracket you write a okay so so how do what's so how do we calculate the trace of a matrix so how do we even bother calculating the trace how do we even bother calculating the trace of a matrix for calculating the trace of a matrix we do sum of elements sum of elements sum of elements sum of elements sum of elements on the main diagonal of the Matrix a on the main diagonal of the Matrix a of the Matrix a or you can say of a okay and a is a matrix or a square Matrix so so the the how do we calculate the trace of Matrix the trace of a mat is calculated by the sum of elements on the main diagonal of the A and B say for example say for example you have a matrix a you have a matrix a you have a matrix a so let's assume that the M Matrix is 3x3 Matrix so let's assume the Matrix is 3x3 Matrix so just I'm going to take the maybe this color this shoots okay so you have uh so let's write it out a11 a12 a13 so let me just draw it over here as well okay so we have that and then let's draw A2 1 a22 a23 and let's pick this green color a maybe 31 a32 and a33 so you have this Matrix so you have this Matrix a so you have this Matrix a and you want to calculate the trace of this Matrix so for calculating for calculating the trace of a matrix a what you will do you will just um sum the elements of the main diagonal so a11 + a22 + a33 okay so the main diagonal is this this one and this one then this one okay so and then you'll be getting your scaler as an output it can be C uh whatever the number is okay so this is how you calculate the trace of a particular Matrix and and formally I can Define this I can Define this I I can Define this as uh I can Define this as a oh my God I think sumission is not written correctly I = to 1 all the way down to the three means in this case we are we want for a 3X3 Matrix so I = to 1 all the way to the three all the way to the 3 a i i okay so if we go on first of all a i = to 1 then that will be a11 + a22 + a33 okay so so this is how this is what the formal notation for uh uh for submission notation or the for Loop for particular for taking of the trace of a matrix okay so so let's take one example to perform the to perform the necessary accents accents so let's say let's say for particularly you have a data you have the particular Matrix you have particular Matrix a so just drawing the same Matrix and let's draw uh let's take one3 and let's take this yellow and let's write 11 5 2 let's take U uh green and let's write 62 5 and then let's draw a matrix so you have this Matrix now you want to calculate the trace of this Matrix the trace of a matrix is nothing but you want to calculate the trace of the Matrix a the trace of a matrix a is nothing but uh is nothing but it will go from I = to 1 all the way around to 3 a i * a I I think it's a a i * I okay uh this is what you and then sumission will will will this is the submission so it will it will add it up so 1 + 5 + 5 which will be nothing but 1 and 1 is the trace of a particular Matrix okay so this is the the one is a trace of the particular Matrix and I hope that you are able to make sense out of it and why do we bother studying Trace because because when you when when you go further me solving some some linear equations or or or systems of equations and and this is this is this is mostly used and in other formulas U mainly not in specifically linear algebra mainly some other formulas they are these straight race of magics helps the computation to be easier or help the too too much anotations as well as it has some cool properties okay so this is very very helpful not in only the con not in only the context of deep learning or machine learning it is most in the concept of context of maths and and and Mathematics and and and you you get in linear algebra and you get in other maths field or discrete M applications okay so this is what the trace of a matrix so we have a show you one example so I hope that you able to make sense out of it cool so let's see some some of the properties some of the properties some of the properties of the trace of a matrix so so some some of the properties of a trace of a matrix I'm just going to write in Black back so the properties properties so the first property which want to highlight the first property which I want to highlight is the trace of the Matrix a plus b can be written as the trace of the individual Matrix a plus trace of a matrix B so so I can write in this particular format it now the second so you can you can just think think think about it in a bit means let's let me just show you if if I can and I will just ask you to do experiment with it but I will just take an example to prove it example okay so let's say let's assume that that you're given a matrix a you're given a matrix a which is 2 2 2 2 and you're given a matrix B and and and you're given a matrix B 333 3 okay and you want to calculate the trace trace of a matrix A+ B okay A + B so what you can do first of all you can C calculate the trace of a matrix a you can calculate the trace of Matrix a so the trace of a matrix a is nothing but 2 + 2 okay which is four okay and the trace of a matrix B which is nothing but 3 + 3 6 and then and then trace of a plus trace of B which is nothing but 4 + 6 but 10 okay so so this is this is how what you you can take it out or in other words now if you first of all now this is the 10 we have proved we have taken out the uh this side rhs side let's see the LH side so in LHS so let's say let's say for a particular example for a particularly for for for example let's say let's let's take these one Matrix and then try to add it first so first of all We'll add it so let's add this Matrix A and B so 2 2 + 3 which is 5 then 2 + 3 which is five five five so we got 2x two Matrix when adding these two Matrix Two element wise addition so you get a matrix a plus B A + B which is this one now if you calculate the the the the the the trace of this particular Matrix a plus b so the trace is the sum of the elements in in the main diagonal so 5 + 5 is what 10 and we had proved therefore LHS is equals to rhs and hence proved okay so this property is proved proved like this okay so this is how you can prove the properties if you want and this is very very useful if you if if you want to get the clarity in your particular Matrix okay so another property which I want to show show showcase to you all is another property which is the trace of a matrix when it when it is Multiplied with some scalar C with some scalar C okay so so what is the trace of this Matrix so a is a matrix and C is some scalar C is some scalar so that is equ equivalent equals to C trace of a okay so first of all you take out the trace of a and then multiply with that c okay and that actually C what it does it just stretches it it simply means it stretches your Matrix okay and if if you talk about a geometric view okay so it's exactly what it's trying to do it's you you can hence prove it as well say for example you want to you have let's say for for for a particular example let's say you have a matrix a which is 2 2 2 2 okay and you have the scaler C which is two okay which is two so this is and then you want to take out the trace of CA so first of all you can take out the trace so the trace of a is so first of all let's take out the trace of a the trace of a is four the trace of a is four 2 + 2 4 and then when you when you multiply the trace of the two times the trace of a which is 4 * 2 which is nothing but eight is a particular answer of that question okay so eight is the particular answer for that question and it makes sense you can this is this is the this is the this is the your uh rhs proved and sorry yeah rhs proved you can prove it the same so when you take out the trace of the mrix which is the mrix is eight sorry so sorry it's four plus oh my God 4 4 4 4 and then we take out the trace of this Matrix which is 4 + 4 8 so and hence proved so r and E equals to LHS okay so these are the properties which I'm not going to prove all of the properties I've just showed you how I'm how how I proceed pro pro proving this stuff okay so another another property another property another property is a trace of a matrix a is equals to trace of a transpose of that Matrix it gives you a Clarity whether we transpose it that the main diagonal will be the same and the trace will be also the same okay so I I I need to prove it to Showcase to you all to see the interesting property of this the interesting property of this let's say for for for example you have a 2222 my favorite Matrix so so it is just Square Matrix because for particular Trace you need to have an Square Matrix and yeah it it it would work well okay so you you have a square Matrix 2x two okay so when you do the transpose of this Matrix when you not exactly you don't need a square Matrix to be done transpose you just need to D some some of the elements of the main di diagonal you don't need to do the uh you need you need not to have only the square m I'm just taking an example to make it convenient for you so so first of all you take out the trace of this so let's first of all take out the trace of this Matrix the first uh which is four trace of this will be four because the main diagonal 2+ 2 is four okay when you transpose it when you transpose It 2 2 2 two okay 2 two okay so this is this this is how it looks and and when you when you when when you go ahead and then uh add it it is also four okay the transpose does not matter and of course I have taken very very easy easy examples and exactly and Hance proved okay so whether you do the transpose the diagonals will remain same now next next property is uh the trace of a product so so so let's let's let's talk about an another greatest stuff which is a trace of a product trace of a product and the trace of product is the trace of a transpose B okay is equals to a trace of ab transpose which is also equals to trace of B transpose a which is also equals to trace of B A transpose okay so these are equivalent equal whatever we have written these all are equivalent equal whatever I have written over here and is nothing but uh this is we are we are we are multiplying the two mates the trace of the product okay so I hope that you're able to make sense out of it it's not a big deal for you okay so this is this this is just the basic properties which I'm highlighting okay so we had seen some some of the properties and and I want to talk about some more properties which are which are more useful means means we are also going to gather some information about uh a b other kind of products okay that we are going to to deal with Okay so let's let's let's talk about the the the hardart product okay and hardart product is is one of the most important product which you will see so let's talk about hardart product and it's very very important as well because if anybody go on Wikipedia and see some properties what is hardam product and what is um raw Necker product we will see that but before that I want to highlight one more property of a trace one more property of a trace the the trace the cyclic property which I don't which which which I can't forget means I I can't forget it if I forget it no one is going to to leave me so the the the I'm just highlight the cyclic cyclic property cyclic property of Trace okay and then and then we'll talk about two two kind of product which is hardat product and roniker product okay so we'll talk about that two kind of product in detail okay so the cyclic property says that let's say let's say for example going to calculate the trace of a matrix a b c d okay A B C D is just a multi M multiplication of a four Matrix so it is nothing but equals to you can just you can just take the you can just b a c d which nothing but equals a trace of CB uh I think it's c d a c d AB which is nothing but trace of d a b c okay so first of all B Gone to the um B B Gone first then C gone first then D gone first okay that's a cyclic property it's it goes in cycle okay but but but arbitrary permutations are not allowed but arbitary but these are not allowed I I think they done wrong over here it should be something like U first of all it was B and then it should be uh c b a d I think so like this yeah I hope so it should be like this and then when you go ahead um yeah so it should be something like this and then you keep on rotating after to recheck the thing but it but it keeps on so the trace of the cyclic property says over here so b c da a then you have a c d AB so first of all B Gone first and then we have CD and a a going last okay and then this is a cyclic per mut ations that Trace is an invariant under the cyclic permutations and the permutations you all know you can check this out this is the property this this property is known as cyclic permutation and one more thing I want to highlight over here that no arbitrary permutations are allowed you cannot do something like this okay so you have to have the symmetric matrices are considered it means if the product of three matrices symmetric me are considered then any permutation is allowed so for example if it is symmetric Matrix and for being a symmetric Matrix it should be equal to its transpose the the the the matrix product should should should be equals to transpose and the trace of a matrix should be equals to transpose of that matx trace of the transpose of that Matrix then it's called the symmetric Matrix and and then any permutations is allowed okay any permutations is allowed except this this is the only case where the any permutations allowed otherwise we don't allow any permutations are allowed in cyclic property okay so this is this is Clic property which you have to remember okay I may have written it wrong but I hope that you will correct it out by the Wikipedia page so this is this this this this was the cyclic property which I want to highlight and I hope that you are able to uh understand it a much better way so let's talk about um let's let's talk about hardart product so let's talk about hardart product which are B because we haven't we hadn't have a chance to talk on this hard demand hard demand product as as as as it seems to be a bit useful for me because it is also used in various Quantum Computing as well as machine learning as well as deep learning so let's assume let's assume that you want to take out the dot sorry I'm I'm saying dot product it should be hamat product hamat product when it multiplies with the two matrices it just do the element wise product okay and the and the Matrix and just just to the element wise product and gives another Matrix of this of of of the same Dimension Okay so so let's say for for example we given a matrix a multiply The Matrix B okay and this is the sign for the har product is just a small O Okay so so so let's say you have a matrix a we have a matrix a a11 a12 a13 A2 1 a22 a23 a31 um a32 a33 so so you have this Matrix a you have this Matrix a and you have Matrix B and you have another Matrix B and another Matrix B is maybe b11 B12 B13 B B21 B22 b23 b31 b32 b33 okay you have the two matrices and when you take out the dot sorry hardat hardat product hardat product between these two Matrix which will be nothing but the product wise multiplication so a11 b11 a12 B12 a13 B13 then a21 B21 a22 B22 a23 b23 okay a31 B2 b31 um um it's it should be a32 b32 and then a33 b33 okay so this is your Matrix uh the heart of our product which is Tim element wise product okay so I hope that you are able to get means element wise product it's not a big deal it's very very easy to understand okay so some s some of the properties of this some of the properties which I want to highlight properties properties which I want to highlight some of the properties of this particular uh product is is you can write it out in this format which is uh which is equals to this sorry B * A B B BB do B Circle A and then a and then b c which is nothing but equals to A B do c I hope that you all remember the properties name okay another is a uh b + C which is nothing but equals to A B + a c okay so these are some some of the properties which are highlighting over here A C nothing but equals to uh okay you can write it out something like this a z which is not nothing but equals to 0 time with 0 a which is nothing but equals to zero okay so this is these are some of the properties of a hardar product which you which I want you to think about it and and solve it on your own and prove it and and then prove these properties if you want okay and just like I've Tak taken some examples and proved it so let's talk about another so one so so let's so I'm just just going to show you what are the property of the trace of a trace of a uh trace of a hardat product the trace of the hardat product where is the where is that trace of a hardart product I I'm not seeing where it at but yeah it's it's very it's one of the important stuff but yeah let's go ahead and talking about um um and droner product I don't know how to pronounce it I'm pronouncing it good or bad way but I want you to think about this means uh you you wanted to the something called as roniker product roniker roniker product so I think we have to talk on this so roniker product let's let's talk let's talk about this Ron roniker product so you have a matrix a you want to calc you want to do the Rona product A and B between A and B okay so let's say and we we this is the roniker product sign for showcasing okay this a r roniker product product and this is how it looks 1 2 3 4 and then uh like this 0 5 6 7 okay and then you and this is how the raw product works so what you do you make a big Matrix means a big Matrix like this and you take this first element and multiply with all as a scaler and multiply The Matrix on Under The Matrix B so something like this 0 5 6 7 okay and then you and then you go on as a two as a scal in the second element you have 0 5 6 7 okay third element you three 0 5 6 7 okay uh 4 0 5 6 7 okay this is how it should work and and and and then what you do and then what you do you simply uh and and then you follow your whole procedure the whole procedure is uh which is nothing but one 1 * 0 then then 1 * 5 1 * 5 then we have 1 * 6 1 * 6 1 * 7 for for for this one I'm I'm doing like this okay and then you keep on doing this 3 * 0 3 * 5 3 * 6 3 * 7 I'm doing doing for this and then let's do for the same 2 * 0 then 2 * * 5 2 * 6 2 * 7 4 * 0 4 * 5 4 * 6 4 * 7 okay and then when you when you do this stuff you'll be left with you'll be left with 0 5 0 10 uh 6 7 12 14 0 5 15 actually 0 20 and in 18 21 24 28 and this is what you get as a 4x4 Matrix this is this is nothing but called the roniker product and one of the property and trace this is the the trace of uh the trace of this Ron product is can be written like this can be written like this can be written like this uh trace of trace of a and then times the trace of B okay okay so you can write it something like this but but but one thing which I want to highlight over here you cannot you cannot uh do something like this in a regular case in your in in any your if I could show it to you if if it could have a chance to show show it to you so let me just show it on a Wikipedia page I think that exactly not equals to H so the trace if if if you see the trace trace of a matrix product okay the trace of matrix product it can cannot be written like a * b as a matrix product usual matrix product which is Trace of a which is which is which is completely wrong it it cannot be written but there's a case which is Ronica product which we can write it something like this but in magx product we are not allowed to do that okay so so this was the bit about this uh Ronica product and I hope that you understood the hard demand product and the trace of a matrix so I hope that that we are done with the Core Concepts of linear algebra from the next session we'll be starting talking about systems of equations solving systems of equations lud de composition gion mixtures and then we are done with this uh chapter and I hope and and I and I really appreciate your patience throughout this course and then and and I I guarantee that you will learn calculus and the easiest way that I can and and I also hope that I will providing so much Valu to you all uh even I'm I I think uh it it it motivates me like if if I see the watch hours and it's bit increasing and it motivates me a lot okay uh peoples are watching these videos so please keep watching please share this course it's very very important for me so that it could reach to lots of viewers and I know this course will be boom in upcoming future and many people are going to take benefit from this and some of the comments are say said that you're that please don't stop uploading the videos this course will be boom on future if I just complete the course and then I show it to the viewers hey see the course is completed complete this you are it is more than any premium course you're you're getting for absolutely free you're getting the assignments you're getting everything for free so I think that it is so so much of uh the features which are available for free the people do not give when they when you when you pay for that okay so I hope that's that's it for this video I'll be catching up in next video till then byebye have a great day so let's get started talking about systems of equations as this is the last video on linear albra series and I hope that you are able to understand everything in linear algebra as well as some of some of the videos of linear algebra is already released before around nine nine videos and I hope that you are able to make sense out of it uh so we had a talk on everything which is required for deep learning for further deep learning is we have already talked about that and one concept which I haven't talked is igen vectors and igen values I'll I'm going to talk about I have already talked about this in my PCA videos so as I haven't seen much of use in deep learning but yeah surely you can go to my pcab video or principal component analysis video in ml1 and you can see there about igen vectors and igen values if you're interested in learning that so this is the last video on systems of linear equations so I'm just going to write linear as it does not make sense so linear equations so we'll try to solve we'll first of all see what the systems of linear equation means and we will see also how to solve this um this linear equations and every stuff okay so what are linear equation so as the as first of all let's let's start with the what is a linear equation a linear equation is it's an equation for a line so so as you have seen in linear regression so your this is the this is this will this will be your linear equation whatever the hypothesis is maybe the hypothesis h of xal to Theta 0 + Theta 1 * X X1 okay so this this this this equation is your linear is your linear equation the reason why I'm saying is a linear equation because over here it does not have any powers it it has a power of a one and and it is a straight line or or or or a line okay so so some of the examples of linear equations some of the examples of linear linear equation so I'm just going to give some some examples so maybe y = to 3.5 0.5x okay another example may be y = 0.57 x okay but they they both mean the same they both is actually equivalent whatever you write with in that form or you write in that form and you can also write this out in y+ 0.5x = 3.5 you have written that into another mode you can also write y + 0.5x 3.5 = 0 you have written that into a new mode or you have written this this into new mode so they are equivalently the same linear equation equivalently they are same linear equation same linear equation they are not different linear equation as you can see over here uh so let's let me just show you so first of all let's see this one okay y = 3.5 0.5 * X and if if you compare y = 0.5 and when you do the manipulation of algebraic manipulation you will get this and you can check that exactly if we will L to that okay uh which you can see 0.5 * 7 0.5 * 6 exactly what what what what what that second equation is telling and then you just you you you have just converted this 0 Plus 0.5x on the on on on the left on the left hand side okay and making that equ equals 3.5 you just manipulated this is a manipulation and they mean the same okay so this is these are the linear equations which I want to show it to you so so we had a talk on linear equation that is an equation for a line okay so it's an equation it's an equation of a line or a for a line whatever okay so what is systems of linear equation so systems of linear equation is when we have two or more linear regession so so sorry it's linear equation they are working together okay so so as as from from from from history we have seen particular statement is called something and a group of particular statements are called something okay so in the same way for example if if you have seen my linear combination videos so we have we had told the set of all the linear combination I think yeah the linear combination is called span and only one that is is called a span okay so the set of they are working together so in the same way um we have the systems of equations are the set of or when we have two or more linear equations working together okay so system of equation the definition of a systems of equation is when we have two or more 2 plus 2 plus linear equation 2 plus linear equation 2 plus linear equation when we have 2 plus L linear equation um when we have two plus linear equation uh they are working together they are they are working together then that's called system of equation working together together they are nothing but called they are called system of linear equation okay so so systems of linear equation means there are two or more linear equations working together that's nothing but the systems of equations so for example so let's say for for for example U 2x + y = 5 x + y = 2 that is you have the set system of linear equation and you need to find the value the value of X and Y by solving this system of equation we'll see how to solve system of equation using substitution elimination and Al algebraic manipulation we'll try to see how to solve how to find X and Y so they both are working together they they both are they both this is a a system of linear equation these both equations are linear equation and and and and using the system of L or we need to solve this uh system of equation by finding the value of X and Y okay so so so let's so let's see how do we proceed further in solving the systems of linear equation so I'm just going to see the recordings is being recorded yeah correct it is being recorded let's let's go ahead and solving the linear or so sorry it's systems why I'm saying the linear and linear regession uh just want to share one incident I was giving giving a speech and there was a kind of St stuff deep meaning okay so the I was having a quot and then I was telling the the beautiful line is is the title of this and and the deep meaning so I was so I was in a stage and and and and told uh the deep learning of this Cote so I was like so seriously I told the Deep learning but no problem again I corrected it but yeah it was a funny no one was knowing about deep learning in my school that uh at that point okay so solving so so so we need to solve let's let's take one example let's take one example to solve one linear equation to to to to solve a linear equation so let's go ahead let's let's go ahead uh to to to solve the linear equation so let's say solving you need to solve this x + y = 6 3x + Y which is nothing but equals to what uh 2 okay so you need to find the value you need to find the value of X and Y so how you're going to proceed further what we can do is simply merge them together merge them into one question into one uh one one equation by making the LHS to the LHS side and rhs to rhs side okay so x + y 3x + y okay so what I'm going to do is have over here we subtract it so let's sub subtract it out okay 6 2 so we are subtracting the linear the the equations okay so from the the second equation from the first equation sub subtracting the second equation from the first equation and and then we are done so we are sub subtracting with this now we if we x + y + 3x y = 4 and then you proceed further is 4X and then it is cut okay 4X = 4 and x = 1 okay so when you when you say that you you you got one what you can do so we now know the value of x you now know the value of x so after knowing the value of x you can put them together 1 + y = 6 when you put X x = 1 x = 1 1 + y = 6 and 3 * 1 + Y = 2 and Y = to I think 5 and maybe y = to 5 okay so uh so y = 5 of course you you can you you you you check with both the equations and both the equations yields the same y when when when you put X X's okay so so you can solve any of these uh equation and maybe the first equation or the second equation and then take y I've just done to to to show you the the check that okay the second equation means the same what exactly the first equation means okay so the value of of X and Y will be x = 1 and y = 5 is the solution to the system of equation okay so when you when you so you find the value so the question was to find the value of x and y and you are done with finding the X and Y values and and it is nothing but a beauty of algebra and and I seriously like algebra in these cases and and we'll see some of the some of these geometric examples that how it is working and and and and geometry geometrical interface of the systems of equations okay so so so like I think that we are able to make sense out of it and I also hope that you are able to understand it now we had seen the numerical understanding now let's try to Simply understand the geometrical aspect of this okay the geometrical aspect of this let's see let's say you have an X so sorry you have an X and Y AIS so sorry this is X and this is y AIS okay so let let let me to not write this out okay but it make more sense in in in in context of linear algebra so let's write 2 4 6 8 10 12 okay so let's go ahead with solving the same thing 2 4 6 8 10 12 okay now let's plot both of these equation let's plot both of these x + y = 6 and 3x + Y = 2 and graphing these is not a big deal if you to graph if you to graph the equations the equations graphing the equations I would like you we we'll cover that in a precalculus but I would like you to have some context understanding of how we graph the equation and the way we graph the way I used to just just just in this example let's assume that you want to grab the function you want to grab the function maybe F ofx = to x² how how how you going to graph it so you can just put the values of different different x's and point and keep keep mark on the point and and then and then and then after certain certain X's value just just join that point or not in the other words let's say for for example you do the same you you try the different different values of X you try the different different values of Y and keep pointing over there so you then you are able to graph the equations and if if if you still not if still confused I would like you to send to one video which will the link in description like graphing the equations which is not a big deal they just form a table and they just try different different X's values and then they get the Y values and then to try the yv value and then they get the x value okay and they keep on doing it until un they found some pattern in it and and it's not a big deal to found a pattern to to to have maybe three to four uh input values of x's and Y then you'll be getting your the the graph of that equation okay so I think uh let's plot this X and Y so the plot of this X and Y table will be nothing but uh will be nothing but what uh six and8 I'm just going to have a good uh so sorry I'm not able to plot clearly let me just touch this it's starting it is touching actually eight okay but it's looking very bad actually so let me try Okay so let's assume that it touches 8 okay so this is8 okay and this is six and this is an equation for your x + y = 8 let's try to plot uh maybe this one um this this particular ter stuff 3 minus 3x + Y = 2 okay so how you going to plot it so the so the plot of this would look something like this so looks something like this so when you go ahead and touch this so this is your the the graph of 3x + Y = 2 this this is a graph of that equation and when you see over here so now you have these two equations now if you if you if if you go closer and closer to this uh if if you go a little bit closer to this you'll you'll be noticing that okay I think I done a little bit wrong over here here I've done little bit wrong over here of course and I have done little bit wrong over here I should give two over here rather than giving two over there and one over here okay just just but in if if you if you draw a graph formally on the graph paper you'll be getting exact stuff but I'm just assuming okay some something like this now the the intersection the intersection which you're seeing over here the intersection the intersection which you're seeing over here is actually Your solution of x or or or the values okay or the values or the values of X and Y or the or or you find the solution to both of the you you already solved using graph of the value of X and Y how where where the points intersect so I'm just going to write it out let's remove it first of all so the point intersect so it is nothing but five okay at Y and there nothing but one at X so X = to 1 and y = 5 so you get the answer from here as well by geometry purpose the point of intersection the point of intersection is where where the lines intersect is actually the solution to that linear to that system of equation okay so when you when you can plot it uh maybe three linear equation and it will be in in in if if if you three linear equation with with the two variables okay so maybe it may be on uh uh on a a threedimensional plane and where the plane intersect exactly that's a solution of x y and z if if there are three values X Y and Z that's not a problem of a two two dimensional variable that is for of a problem with threedimensional variable where you have where you you plot the plane you have a plane something like this of x value y value and Z value and then you have this is this is one of the linear equation maybe you have another L linear equation and maybe you have another linear the point of intersection maybe this one is actually Your solution to that X and Y and Z okay so we'll see one geometric purpose later on uh but this is how the the 2D 2D geometry in 2D plane this is if you want to find the value of X and Y the point of intersection of the the the linear equations is actually the solution to the values of we able to find we we want to find it okay so I I hope that you're able to make sense out of it and it's not a big deal to understand it okay so I would like to go further I would like to go further into we had a talk on this is the two variables which is two 2D ples stuff so now let's go ahead now let's go ahead now now let's go ahead and talking about what that linear equation when we call a particular equation as a linear equation and when we call the particular equation as a nonlinear equation so who can tell me that 2x + y Z = 4 is it a linear equation yes or no I think this is a linear equation but is this a linear equation but is this a linear equation so sorry I think uh I have done a wrong example but no problem 2x + Y 2 Z = to 4 is this a linear equation yes or no it's no because here the power is through as a quadratic equation okay so this is not a linear equation over here but this is a linear equation okay so I'm just going to get started with with talking about variables which we are dealing up okay so variables which you're dealing it up so if if you see this example if you see this example of this x + y = 6 and 3x + Y = 2 and then you add minus over here so over here here we the the the the system equation is a two Dimension system equation is a twood dimension two Dimension uh variables okay why because you have only two variables okay X and Y but what if if I add x + y = to or plus maybe Z = 6 and 3x + y + z = two so you able to find x y and what about Z so this is a threedimensional problem through the system equation is a threedimensional the system of equation is a threedimensional stuff and uh okay so so this is how we are able to make sense out of it as two Dimension and threedimensional variables okay so let's get started solving up our problem so we have this two Dimension and three dimension Okay so I hope that you're able to understand what I'm trying to convey over here but in let's let me show you some other stuff so here we are seeing only two two system two two linear equation in the system equation your this just identify 3x + Y = 2 and maybe 2x + 2 y = um 16 okay so just let me know what uh is this a two two two dimension or three dimension it is a two Dimension because there are only two unique variables which you need to find okay but what if I changed 2 Z and maybe 2 Z I just change it and 2 Z + 2 y it's a threedimensional problem then okay so it's a toally based upon the unique variables you have in your problem statement so here we have more variables you you you you in a system of equation you have more variables and it's AR one okay so I hope that you're able to make sense out of it and and I also hope that you are you are able to understand it okay so let's go so so I hope that uh it's just not a big deal for you at least and uh uh please recapture it if if if if you're not even com uh comfortable please rewatch the video again I would ask you okay so we had seen that maybe the the line of intersection is called the solution to the process equation but there are three types of possible solution cases so the first type is no solution second type is one solution third type is infinitely many solutions so for the system of equation you can have either you can have either no solution you can have either no solution you can have either one solution and you can have either infinitely infinitely many solution many solution okay so you are have you can have one solution no solution exactly no solution for that system equation you can have one solution for that equation and then you can have infinitely many equation many solutions for that okay so if there if when there is a no solution for that equations that's called inconsistent or if there is a one or infinitely many solutions that called consistent that's called consistent okay so let's I will just go through the note again but let's see what that geometric mean no solution one solution infinitely many solutions okay so so let's here is our graph here is our graph here is graph so over here let's say for for the second example this is your one so this is a this is maybe two two two variable Stu and two dimensional Stu so you have a two linear linear equation in that system of equation and you have the first equation and you have the second equation and what you can see over here they do not intersect they do not intersect they're parallel to each other so that's why they have the no solution they have the no solution okay so if if there's a point so this is an example of no solution another example is if you have this one and the and you have the another equation something like this the point of intersection the point of intersection which you see over here is this is this this this gra an example of one solution because there is one intersection at that point okay the infinitely Min solution graph will would look like something like this if you have if you have something like X and Y graph X and Y graph and you have the on the you have the first equation and the second equation is is on the same line of that first equation uh then that's called then there where we we will be having the infinitely many solution to that linear equation Okay cool so I hope that you are able to make sense out of it and it's not a big deal to understand it okay so so these are three types of solutions and and and when we call this when there is a no solution we call that as a inconsistent we we call this as a inconsistent and these two are called the consistent these two are called the consistent okay so I hope this is a two these are diagrams of two equations in a variable and in two variables uh not a big deal to understand it cool so you this now now we are done with it and I think I spoke in just just before no problem again so solving system of equation so now let's let's let's start solving system of equation we had solve on system equation using regular rbra but basically real world you not get a system equation in very very easy mode you have to make you have to understand it and I and I will ask you to solve this system equation just to make sure that you're able to understand everything so let's start with solving uh system of equations so uh uh solving system equation so let's go ahead uh 3x + 2 y = 19 so let's take one let's take one example let's solve this algebraically we'll solve this algebraically okay man manipulation we'll do a lot of manipulation in this so let's take one example and the example States 3x + 2 y 3x + 2 y = 19 and x + y = 8 okay so over here you have this and and what we specifically do we make make that to um I I would say make that y or whatever the variable we to find into one side and other other other stuff in the in the left uh opposite side of that which is in rhs so what I'm going to do is 3x + 2 y = 19 y = 8 x so I so y = 8 x so now my trick will be I'll replace the Y over here with 8 x okay replace y with 8 x with 8 x so 3x + 2 8 x + = 19 and you have y = of course 8 x okay 8 x now what I will do I will expand this I will expand this okay so after you expand this 3x plus just write expand okay 3x + 16 16 2x = 19 okay so now you have this y = 8 x okay so you have expanded it now what you will do you will try to solve this okay you'll try to um try to solve this this one so 3x 2x which is of course X and we have us left with 16 so which is nothing but x + 16 = 19 and Y = 8 x so x = 19 6 and Y = to 8X and this will be yield yielding to your favorite uh uh 16 okay it's 16 no yeah 16 which is with three and then we we got the value of three we got the value of three now you put the value of three over here so so over here 8 3 which is nothing but 5 so you found x = 3 and Y = 5 as a solution to this linear equation of the system of equation I hope that this is very very easy not very hard to understand this we are sub this is the method called substit tion solving by substitution okay so we sub substitute the values the so what I did my technique was first of all make X on the one side all the all the variables on one side and other all the things on the opposite side of that variable which is the rhs in this case um and then try try to take out and try to do the algebraic manipulation and then you're done you just find one variable and other variable is in front of you okay so now we have seen these two variables now let's try to solve let solving the system ification of three equations in three variables so we'll be having the three equations in three variables okay so now let's see that how how we are going to proceed Sol further solving that okay so let's let me just make that this this this point and let's see let's let's go further so let's say x + z = to 6 okay X+ Z = 6 Z 3 y okay Z 3 y y = 7 and 2x + y + 3 Z = 50 I will try to be as much neat as possible to make sure that everything is going very very fine okay so what I will do what in my hand is to do is to I I'm seeing this x I can make this x equal to 6 Z okay I can I can do something like that okay so what I'm going to do is I can I can just make that X = to X = to 6 Z so let's go further x = 6 Z okay x = 6 Z now we have 3 y 3 y + z okay 3 plus Z because I'm I I or in for so let's let's assume let's try for for or what I'm going to do is to give you give you a good idea about how to be clean so what what I will do I'll I'll I'll I'll make a separate PES for for all of them so this is for X Y plus Z because we we don't have y in this case so we just leave this space for it which is six okay Z which is z okay so what I'm going to write z + 3 y okay minus 3 y = 7 and we have 2X + Y which is for this plus 3 Z = 15 this is called the neat and then you what you go further you go further then what you do you you first of all first of all x equals to Alo sorry I don't have to first of all you have to go further okay X then then you leave all the space for Y and Z okay equals to 6 Z okay 6 Z okay so that the equation left alone the the the variable left alone and the and our LHS side okay you now you write the same thing again 3 y + z = 7 and 2x x + y + 3 Z = 15 okay now what I'm going to do is to substitute the values of X over here now I'm going to do is to substitute the values of X over here so after substituting the so let's sub substitute the values of X over here which will be nothing but uh X then we leave this space equals to 6 Z 3 y 3 y + z = 7 and 2 6 Z + y + 3 Z which is nothing but 15 okay now you sub substitute the values of X and if we try to solve it if if you try to solve it uh if if you go ahead and try to solve this we be getting this um if you go ahead and try to solve this uh let's go ahead and solving this uh so uh over here if if if I just just solve it which is nothing but y + z = 3 okay just I'm solving this the last one which is the last equation I'm I'll be getting y + z which you can check it out okay so if we are solving this 2x + y + 3 Z = 15 you're getting 6 Z okay now now we are left with so now we left with x = 6 Z okay 3 y + z = to as usual now we write y + z = 3 okay now because this is the which we have solved okay now we repeat the process again now over here I can just make Z = to y y 3 yes or no yes or no 3 y + z = 7 and z = 3 y I can do something like this to make the variable alone so that we can sub substitute it now if you go ahead and substitute it so if you go ahead and put put put the values of Y so x = 6 z u maybe maybe 3 y + sorry Z values so Z what what we were having 3 y isn't it we were having or not exactly we're having the Y cut I think 3 y okay = to 7 = to 7 we go ahead which is but Z = to 3 minus uh uh what do you say you go further okay so you are left with 4 y 4 y + 3 okay so you'll be uh and and Z = to 3 Y which which which you are seeing over here okay now we have this now we have now we have substituted the values now we'll try to solve it so when we solve it when we solving when we solving 3 y + 3 y = 7 when you simplifies 2 4 y = 4 okay or Y = to 1 okay when when you try to solve this linear equation and it's not not a big deal to solve what you going do you can just add you just say okay uh this is uh 4 y + 3 which is 4 y = 7 3 and left with 4 and when you try to and and and and and then you will be left with minus one okay so so this is this is what you get y = to minus1 okay now you you got the value of one value which is x = 6 z y = to 1 and z = 3 y 3 y okay 3 Y which you're having now we go the values of Y we can substitute the values in z x = 6 z y = 1 and z = 3 1 okay nothing but 4 now we got the values of Z so x = 6 4 which is 2 okay so we are done with x = 2 y = 1 and Z = 4 which is is our solution solution to our system of liation in a higher Dimension or in three variables okay so this is a process called sub solving that by substitution and it works nicely okay it works very very nice to every most of the cases but what it's a very long process okay it's a very very long process but I would recommend you to have a hands on on this if you're not able to solve uh the system equation using regular algebra okay so we have Sol using this kind of stuff now let's try to solve the system of equation by elimination so I'm just going to write solving by elimination elimination okay so how are we going bother solving it example 3x + 2 y = 19 and x + y = 8 okay so this is your this is going we are going to solve it okay so what do elimination means we are going to either multiply add or subtract the the the equations from each other so we'll see how to do that so let's say so over here what you so we we are going to multiply the equation so we are going to multiply so you can see that over 2 Y is over here so here is also y so we are going to multiply the second equation with the two the second second second equation with the two so we are going to multiply the second equation by two 3x + 2 y = 19 and 2x + 2 y = 8 okay so when you go further you you you have this and and what you do and what you do you simply uh have the particular now you subtract the second equation from the first equation you subtract this equation from the first equation okay now now you do the sub subtraction over here subtraction over here so actually you're are eliminating the variables from the first one so 3 3x + 2 y 2x + 2 y which but 3x + 2 y 2x 2 y so it get cut down so we will be left with X = to what 3 okay X = because when you when you do this uh 19 minus uh what are you saying uh uh it's I think it's 19 or it's 16 yeah so so this this this will be also it I don't know why I haven't multiply the both both side by two now we have to multiply both side by two so 6 19 16 will be 3 okay so 9 19 6 6 16 will be 3 so now we got the values of X so X = to x = 3 and when you do this 2x + 2 y = 16 put the values of 3 2 * 3 + 2 y = 16 the Y value would be 5 and you found x = 3 and Y = 5 this is called the solving by elimination you first of all you multiply the number by two for the second equation and then you subtracted the second equation from the first equation and you're done with solving your linear equation or system of linear equation okay so so this elimination is little bit faster but it needs a neatness it needs you to have good logic and it needs to have a good strategy but um you can solve whatever you like uh in the system of equation you can you can make use of any any of them maybe system of equation using elimination Al manipulation May or maybe substitution it totally makes sense to you okay so thanks for seeing this video I hope that you're able to make sense out of it this this this was the last lecture on lar algebra I'm very very excited to to start with uh calculus okay I'm going to teach the first time Cal calculus and whatever I know about calculus I'm just going to put put that in front of you so that it could recapt youate whatever I had studied as well okay so and it makes sense okay I'm able to teach then I'm able to understand that as well cool so I'll be catching up in the next video till then byebye have a great day hey everyone so let's get started with a new lecture on lecture number seven which is on determinant and this is one of the one of the again I would say important concept to study because in principal compon analysis or whether you uh it it it it comes a lot in your machine Learning Journey as well as well as in deep Learning Journey because it tells you how to solve or solving the linear equations or or or or if I if I talk about in terms of linear transformation it just tells tells you how the how the how the change in area or a volume occurs okay and and determinant is nothing when you it's nothing but you just TR you just give some Matrix and then you get one number so we'll be talking about that in detail in this session uh I I think you you'll get a lot from this session and and you you can make your own notes or the notes is in description un box below either it would be updated soon but yeah uh I it is it is already been made it's just sent for processing and that it will be into your description if you like this video please be sure to subscribe this channel as well as like this video and comment because YouTube algorithm knows okay this is a good video to recommend because many many other the people say uh your channel is underrated so I want you I want this channel to be a rated Channel because I work a lot on this channel Okay cool so let's get started with solving uh what is determinant so we'll we'll get onto geometric meaning soon but uh in in in determinant what you do if you know about a square Matrix if you know about a square Matrix which which we talked about and and I have told that is very important Square Matrix are very important is used extensively in linear algebra to use this term terminology so Square Matrix is nothing where your where your number of a rows is equals to the number of a columns for example uh your Matrix a is is maybe it can be 2 2 22 okay so this is a 2X 2 where n = 2 and M = 2 so n * n Matrix where your Square Matrix is equals 2 where your number of rows is equals to the number of columns okay so that is the so this is so what you do you take your Square Matrix and determinant takes one square Matrix where the number of rows is equals to the number of columns you write determinant of uh an A and A A should be the square Matrix a should be the square Matrix and then you get get one scalar or or or a number as an output when you apply the determinant function or or or when you take out the determinant of that Matrix okay so now how this is useful we will see how do we take out the scalar a just in a second numerically but but U but when you um how how the determinant is useful this is this is one of the most important concept to know so the determinant is useful in in solving and solve linear equation in linear equation it's used very very extensively solving linear equation or maybe it can be useful in and in in in in knowing okay in knowing how linear transformation and knowing how linear how linear transformation transformation change their area or the volume okay change their area transformation change their area change their area over volume or volume okay not over it's or volume and it is also and it is also useful uh in other stuffs like uh when solving some some computationally it it it it it does reduces some comput not exactly me doing efficiently not exactly I would say efficiently I would say very precisely so solving the particular linear equation and is used a lot in that so that's why we take out the determinant of a matrix and that when you take out the determinant of a matrix you simply give a squar matrix root to the determinant and then after when you take out the determinant you will get one scaler okay so this is what the this is this is this is what we use and and if if you talk about um in machine machine learning use case so in machine learning if if you know about machine learning in machine learning you have something called as dimensionality reduction method and and in that you take out the determinant of that covariance Matrix so covariance Matrix okay so when you take out the determiner of that covariance Matrix and then you and then and and and and then go further into solving the particular problem okay so not exactly covariance yeah so you take out the termin and then you go further into uh into other stuffs like uh uh the igen vectors and igen values and they are extensively used the determinant are extensively used in the igen vectors and igen values in principal component analysis okay so I hope that this is clear why we use determinant and and and what's the determinant is now now we need to care about how do we take out the scalar value because we give a function because we just give a a square Matrix into that determinant and then we will we are going we are we are just getting a scaler as an output so how do we even do that uh so for for doing that assume that you have a matrix a you have a matrix a which is nothing but 2x two so I'm just going to write um a a b c and d okay so you have a matrix a b c d which is a 2X 2 Matrix so when you take out the determinant of that Matrix a which is nothing but which is nothing but so a d means you take out the product of the diagonals you take out the product of the diagonals a D minus BC a D minus BC so for example you have a matrix uh 2 3 4 6 and and then you want to take out the Matrix the determinant of that Matrix 2x2 matrix which is nothing but 2 * 6 2 * 6 3 3 3 * 4 3 * 4 which is nothing 6 6 2 12 3 4 12 that will be nothing but zero zero is the answer or determinant of this Matrix okay so determinant of a matrix can be zero we have we don't have any conditions but yeah the determinant of this Matrix is zero okay so this is how you take out the determinant of how Matrix geometrically speaking okay so one thing that I want to highlight over here let's say for for example uh what does it mean geometrically what does it mean geometrically so so let's uh let me make one more page so that I could explain you what does it mean geometrically speaking what does it mean geometrically speaking either I could just go on some website to mean to mean what is actually trying to tell so let's go on one website let's go on one website which I want to show you all is this one okay so assume that over here of over here you have let me choose my black color okay here it is so you have um a matrix a matrix a b c d okay you want to take a determinant of this so this this is this is what you take out so for taking out the determinant you just write either in this A B C D giving a pipelines like this okay or or you write determinant of this uh a a matrix and this a matrix is either uh a b c d like this okay so this is the notation for swing that you want to take out the determinant of this Matrix okay that pipeline that big big pipeline okay pipe uh line okay now over here your a is 1 your B is zero your C is zero and your D is 1 okay you want to take out the determinant of this you want to take out the ter determinant of this you want to take out the determinant of this so how do you take out so what does it mean geometrically speaking so geometrically what it's trying to tell is when you plot this Matrix over here first of all you take this and then you go over here so this is nothing but the determinant of a 2x2 matrix is the area of a parallelogram with the column vectors AC and BD okay so this is the the determinant is nothing but the area of this parallelogram of this parallelogram where the column vectors are AC and BD okay so when you when you plot the 2x2 matrix which is which looks like this and and and this the the the determinant which means geometrically speaking is nothing but area of that parallelogram which formed by joining everything and then and that area of that parallelogram is nothing but determinant of that Matrix okay this is what does it mean geometrically speaking uh I would ask you to watch one video on three blue one brown to see how how he shown geometrically but yeah uh the the det terminal is nothing but the area of that parallelogram whatever forms so for example your par so let me reduce the a a bit and then let me do something with this I don't know what how it is working yeah so let me do something like this and let me increase the area okay let me increase the B okay here it is so when you have the column Vector when you have a column Vector as 0.86 and Z okay and then you have another column Vector which is 0.52 and the the parallel gram is formed is nothing but your favorite the determinant okay so this is what the determinant means and you can play with it by just going to demonstration W frame and this with this website so let's go on the 3D view so how does it look 3D so 3D is nothing but area area of that parallel Zoid okay so if you just see over here the area of the paraloid is is nothing but a determinant we'll see how to solve how how to solve this deter this determinant one okay we'll see how to solve um three for the how to take out the determinant of a 3X3 Matrix and we'll also see how to take out the determinant of uh n by n Matrix okay so it's a it's a bit hectic task but we will try to do it so this is this is what the geometrically means and for 2D the area of a parallelogram and for 3D area of a parallel Zoid okay which you can see from the diagrams which are shown over here so if you just if if I could zoom like I can't zoom in but yeah I can just show you this is this this is what you have your uh 3x3 Matrix and then you this is the parallel Zoid which is formed and then when you try to take out the determinant of this is nothing but the area of this paraloid okay so this is what it means and the determinant geometrically is nothing but the area of a parallelogram or paraloid in 3D dimension okay so this is this is what you need in in a geometric intuition just just just to make sure that what the geometrical it it means okay so now let's see now one of one of the important thing which I want to show you up is is is we have seen we have seen how do we take out the determinant of a 2x2 matrix so the determinant so here's your a here is your here's is your a and you have and then you want to take out the determinant of this A B C D and I'm just writing pipe to denote okay this is a determinant so when you take when you try to take out the determinant of this so it's nothing but equals to uh uh a a minus oh my gosh it's a minus BC that's uh then when when you take up that's a uh simple scalar which is e not exactly that not 3+ 3.71 1 it's e okay so let's let's give it any scaler which is e okay so this is this is what it means in 2x2 matrix I'm talking a specifically 2x2 matrix now now let's talk about how do we take out the determinant of uh 3x3 Matrix so determinant determinant determinant of 3x3 Matrix 3x3 Matrix matx so how do we even approach we taking out so you have want to take out the determinant of a b c d e f g h i okay so GH I is this is your Matrix this is the determinant of this Matrix okay so how do you take out how how do you take out the determinant of this Matrix and of course your it should be a one scaler okay it should be one scaler or a number or number okay so how do we take out the determinant so can't we do a * a * e * I and then it will not work this is this is not you can you you can just guess how do we do it just try and comment maybe I can just see and be a bit funny in job so please be sure to write it and I will try to see what you write it okay so so let's start approaching how do we even approach this problem so what we do we simply so so what we do just just make sure that first of all we go to the a11 okay so me first element in that Matrix and then what we do we simply leave this uh column and this row and write a minor Matrix or a submatrix of that of of that uh big Matrix or you can say that we take out the minor of this Matrix how do we take out the minor of this Matrix you simply when for for example you choose this number okay so what you do you you leave this call column and you leave this row and then you write and then what you do you take out the minor and then you take out the determinant the determinant of that by multiplying by a okay so the first element is this and then you have e f h i we left this column and this row and then we write EF hii okay we want to take out the determinant of EF hii okay now what you do now what you do here is your plus sign now it will be a minus sign over here okay you go to the B you leave this column and you leave this row okay which is nothing but B and D FG I DF GI because we left this column this row and this column just d f g i okay and then here is your minus then here will be plus plus uh you write C now we left this column this this this column and the first row which is which will be left the determinant of d e g h okay and then we have convert now these are called the minor or a submatrix submatrix or the minor of our Matrix a these These are called the minor these are called nothing but the minor these are nothing but called the minor minor of our Matrix of our Matrix a okay so when you try to now it is very easy a * a * uh EI now you can just apply your 2 x two a EI and FH EI minus FH okay minus b d FH d i minus FH okay plus C and then you have DH EG okay DH minus EG okay and then you'll be left with some scalar and then you can simply do do this thing and then you simply multiply with this and then you do do some calculation and then you'll be getting your output at has maybe some some scalar some scalar value okay so let's see one of one of the one of the one of the problem or or the stuff to to see how how it looks like Okay so let's let's assume that you have a a matrix or 3x3 Matrix so here's a question for you okay maybe you can try try to approach it uh the you want to take the determinant of I'm writing this pipe that denotes that you want to take out the determinant of that uh for example 0 1 1 2 uh 1 2 0 uh let's let's write 1 1 Z okay just a random random I'll be walking you through it so take out the determinant of this this is a 3X3 Matrix try to take out the determinant of this so how do you take out so first of all we go to the first element and then what we do we take out the minor of this Matrix so the minor so we leave this column and this row so we'll be left with zero and then we and then we write out minor and then we take out the determinant of our sub Matrix okay plus now no no no it will be not plus over here it will be minus because here is our plus minus okay one you leave this column and this row which is 1 0 1 0 okay so 1 1 0 1 0 okay and then you write plus 2 and then you have uh you leave one 2 1 1 okay you leave this column and this row okay you leave this column and this row you'll be left with 1 2 1 one okay and then you do the sum and then you do the sum so and then you take and then what you do you try to take out zero 2 * 0 which is 0 0 okay uh 1 * 1 * 0 of course 0 and 1 * 0 0 okay plus 2 uh 2 okay 1 * 1 1 2 * 1 two okay then you'll be left with of course zero then it will be done then it done it will be also 1 * 0 which is nothing but Z okay we'll be leftt with 2 * 2 2 * 2 that that will be 4 okay which will which is your determinant of this Matrix so 4 is your determinant of this Matrix which you are seeing over here so sorry here is 1 2 it's not it's it's simply 1 so 2 * 2 is the determinant of this Matrix so I'll be so here you got the determinant of this Matrix which is nothing but min2 okay so this is this is how you take out the determinant of a 3X3 Matrix as well so there are there are some problem for you to work on so I'm just just going to write it out so there's one problem which which which you can approach okay so you want to take out the determinant of uh 371 4 please answer the HW please answer in the comment box it's just a quiz which you will see in your attendance as well okay so this is what the determinant of that Matrix of the 2x2 matrix or 3x3 Matrix we'll try to solve the determinant of a 3X3 Matrix using lianes formula or the rule of surus okay so it's is very good formula to work on so we'll see that but before that I want to highlight some of the properties some of the properties of that properties properties of determinant Matrix of determinant of a matrix determinant of a matrix the first first one is the first one is the first one is for example you want to take over the determinant of this Matrix for example you want to take out the determinant of 1 z uh 0 1 okay so try try try to take out the determinant of this 1 * one which is one uh and then and then minus Z okay what what it will it is one and can you identify this is an identity Matrix even if you have uh 3x3 identity Matrix then that will be nothing but that will be nothing but one so whenever you have identity Matrix whenever you have if if if it is if it is identity Matrix if it is identity Matrix identity Matrix then then then the then the then the determinant of that identity Matrix will be one okay this is this is one of the property second property if the if the rules are the same okay so for for example if the rows are the same are the same for for example a a b b okay a a b b then AB minus ba a will be nothing but zero okay so this is another property third property is you have a scaler multiplied with some uh a and you have another scal c and b d okay so what it will be it it just makes sense r a D minus R CD or r r CB okay you can write write it down like this and then it's it's not it's nothing you just you just take that out of out of okay you just uh take take that as a common r a D minus BC okay so we can write this as a we can write this we can write this as a r * a b c d either we can write it now so we proved it so it is just equivalent you can write this so for it will be easily for us to solve okay so it is so it is R times as either it is same as over here so we a D minus BC so it's just equivalent to that so this is another property which you see a lot in while taking out the determinant of a matrix okay so this is these are the some of some of the properties which I want to highlight in front of you so now let's go on to the another stuff is how do we take out the deter DET minant how do we even bother taking of the determinant determinant determinant of 3x3 Matrix so you can just say okay I'm just going to just going to take out the minor of the sub Matrix of the Matrix and then I will do that so here's another another trick which is called the rule of suus I think it's it's just not a I would say uh okay it's a good technique but okay you can try it out but eventually I like that my approach but yeah it is very very straightforward approach which I'm going to tell over here okay so assume that you have a matrix M and that you have a matrix M okay so a11 a12 a13 okay a21 a22 a23 a23 a31 a32 a33 okay so you have this 3x3 Matrix now when you wanted to take out the determinant determinant of this Matrix M so how do we even bother doing that so for for for for so you can use the rule of suus rule of suus I think the funny name he has but yeah again I'm no no one to comment on on his name he's a again a great people okay so not I'm not even a one 1% of these people so these are amazing people who give a lot to the world so I think about I'm no one to say about but yeah amazing name so what you do you take out the determinant so you want to take out the determinant of a11 a12 a13 okay so this Matrix I'm just writing this Matrix a21 a22 a23 okay a31 a32 a33 okay and then what you do you take out the first two column and write it in another format like this a11 a12 A2 1 this is a trick for solving a 3X3 Matrix a22 and a 31 okay so a31 and a32 so this is a22 okay it say a21 okay so this is what you now you write this now what you do now what you do so what you do you you simply take out the product you simply take out the product like this the first diagonal okay so this is the diagonal so what you do a11 8 22 a33 okay a11 a22 a33 plus plus a12 a22 A2 3 okay A2 3 and a31 so what you do you take out the product of these three you take the product of these three so a12 a uh uh 2 3 okay A 2 3 uh and a 31 okay now what you do you simply uh do this simply multiply the next next diagonally okay so plus plus a13 a a 2 1 okay I'm I'm I'm I'm doing a bit messy so let me do a31 uh if I'm not wrong a13 a a a a13 a21 and a32 okay A a a13 a21 a32 now you are done with this now what you do now what you do you now go from bottom to top here you are going from top to bottom now you'll go from bottom to top by changing the sign now okay now you go from bottom to top so here's how you do here's how you go further okay so so the way you go is you have a31 so from here so from here a31 okay and then a22 and then you go to a a13 so now you start going at this side like like this okay so a31 I'm just going to write a31 a22 a22 and then a13 3 a13 okay and then what you do and then plus no uh you you minus because you change you go from bottom to top so here you minus it now now you go at this one now you do this and then a32 okay this one this one and this one a32 * a23 yeah if I'm it's a it's a a23 if I'm not wrong yeah a23 and a a11 a11 okay now minus now this is done now you go at last one which is this one a33 a21 a12 okay and here's how you take out the determinant of a matrix using suus rule okay or a rule of suus okay so and then you'll be getting after after doing this all those stuffs you can just cancel it out something if it is so you just you just come do do the computation then here's your Matrix and this just a scaler number or other stuffs so here's the rule of suru so here's how you do you do simply you do you do write the first two column uh at at the side so that it could be easily so uh so what you do you simply take of the product from top to bottom for the first three and then you take out the bottom to top for the second three starting from the last okay so here's here's here's what the full the rule of suus means here here's how you take out the determinant of that Matrix like this okay so now I'm going to talk talk about is uh you can see the Wikipedia Pages for a libanese rule because they write a very very kind of libanese stuff so you can just go there and see more see more about this rule okay so the next thing which I'm going to talk about is the next thing which I'm going to talk about is how do we take out the determinant how do we take out mean bother of taking out the determinant of n by n Matrix the determinant of n by n Matrix so how do we even take out that and how do even bother taking out that okay so here's so I'm just going to write the N by n Matrix I I'm just going to write the N byn Matrix or let's start with a particular example let's start with a particular example so it would to totally makes sense okay so let's let's start with a particular example and then at at last we'll just write a definition and then we'll end this video okay the example is bit long so I'm just going to maintain my handwriting so the example is and you want to take out the determinant you want to take out the determinant of a 4x4 Matrix 1 2 3 oh my gosh 3 4 okay 6 6 9 2 1 and then we have a 4 9 2 1 and then you have a 0 1 1 1 okay so here's here's your determinant of this Matrix so to take out the determinant of this Matrix so how do we even approach taking out the determinant of this Matrix so how do we take out the determinant of this so for taking out the determinant of this so for taking out the determinant of this for for taking out the determinant of this you take out you take out you first of all take out the minor or the submatrix of this okay so you you go to the First Column first element and then you leave this column and this row and then write the sub Matrix so you just one and then you take out the determinant of 9 2 1 9 2 1 1 1 1 okay this is plus sign so it will be minus sign now you go at this take take of the sub Matrix leaving this row this column and this row so it would be two first of all you take product it so you multiply with that two two times uh the determinant of 6 2 1 6 2 1 4 2 1 and 01 1 okay and then you uh change the Sign Plus and then go through with three and then you leave this column and this so it will be nothing but uh three and then you have 621 I also just write okay 66691 I I just leave it so I'm three 691 691 uh 491 491 and 01 1 okay 0 1 1 now the last one is there four okay so there is four and then you take out the terminant of leaving all the all the uh column one and then row 692 2 692 492 and uh okay I think it's wrong 492 and 01 1 okay these are the sub Matrix of that Matrix let's name it as a m okay so this is a matrix M and then you have to take a determinat of that Matrix M so here's the 4 4x4 now you do this now you have this now what you do now here you convert it to 3x3 determinant now what you do you convert that to a 2X two here's how you do so you you don't want to use a s rule because I I eventually don't like that rule it's very hectic rule sometimes it maybe cause you error but no problem in that so here's how you do it so first of all what you do so first of all what you do you simply uh multiply uh one okay you simply go ahead and take take your uh one as a so if you can see I just want to take that one as an uh uh this one and then what I and then I go and approaching this so here is your nine so first first of all go at this element take out nine we want to take out the sub Matrix of this Matrix so 9 and then you take out the determinant of 2 1 1 1 so what how this came 2 1 uh you leave this this row and this column 2 1 1 1 okay now you simply change the sign minus okay you make sure that that you're doing only doing for this you're only doing for this we'll come to this later on but we are only doing for for this a21 okay minus now now we go to this two now we go this two we leave this column and this row which is 911 1 okay so which is aray what happened yeah so which is nothing but uh what do you say uh two because here is our two leaving this row this this this column and this row 9111 okay uh the determinant of 91 1 1 okay and then what you do plus now you change the sign and then you go at last one leaving this 9 21 1 okay so the 1 and the 9211 okay so you take out that okay now this was plus now you make it minus okay now here is your two so I will take that outside I will take take that outside okay and then I will just go ahead into solving this so you take this take this as a now you take out the minor of this Matrix or the sub Matrix of this Matrix so here's how you do it so you simply add it minus so here is minus is 6 uh 21 1 1 so here here's how you go with this you ignore this column and this row 211 1 now you go to this you ignore this 41 0 41 01 and then you go over here ignore this 42 01 okay so this is how I'm I'm I'm going to write minus 2 okay because you go over here two over minus 2 CH changing the sign take out the determinant of 41 0 1 4 1 0 1 okay and then you simply plus u now you change the sign you go one go to go to 1 42 0 1 42 0 1 uh + 1 uh 4 2 0 1 okay and then what do you do and oh my gosh yeah so then what do you do now you now you converted that 3x3 Matrix for this one and for this one now you go to this one okay by changing the sign plus plus and then you go and then you write separate three now you write separate three and then you take out the first one six okay so you leave this column and this row which is nothing but six and then 9111 which is the sub Matrix of that Matrix 9 because this plus 9 and how how how 9 came you go with this column leaving this column and leaving this row 41 01 which is nothing but 41 01 okay plus changing the sign 1 4901 how this 4901 came is you have this you leave this and this you leave this column and this the row which is 4901 okay so this is how you came it and then and then you're done okay now what do you do you you do for the last one you do the for the last one this because you you done for this you're done for this you're done for this now you converted that to a 2X 2 m which is easily deter which which we can easily take out the determinant now you go to this okay so here's how you do it so 4 okay and then what do you do and then what do you do you leave this column and this row taking know the first element so six the determinant of I was I think it's it was 9 9211 9211 9211 minus minus 9 okay so over here it was living this the SE going to this and leaving this column and this row which is 4201 I I I think about it yeah that's 4201 4201 and then you change the sign plus 2 49 I think it's it's it's more about you leave you go over here 4901 okay that is 4901 okay now you're are done now this is what you have written so you converted the first Matrix the first Matrix this one this one and this one as well uh into a minor Matrix which is 2x2 determinant so you can easy take out and then do the product and then you take out okay so let's do over here if if if I have a chance to do over here but no work problem I will do over here okay so here's how you do it here's how you do it so for doing it first of all you have the one available which is over here you have the one available which is over here what do you do you simply 9 16 + 7 how how how we came so you have the particularly 9 times because of course you want to always want to uh multiply it out okay so I think about this is you have uh if if you go over here 2 * 1 okay and 1 * 1 so 2 2 2 * 1 how much 2 * 1 how much it would be uh 1 1 I would say uh 2 1 which is 1 so it will be 9 okay minus minus over here uh 9 9 * 1 which is 9 1 which is 8 16 16 done and then you have 9 * 1 and then minus 2 * 1 so 9 2 which is 7 okay so here's how it came okay and then you then this the left minus 2 now you go on second one two over here so when you when you take a 2 * 1 how much 2 * 1 how much 2 * 1 2 1 okay that is 6 over here so we write uh 6 8 + 4 okay so here's how you do so it is 6us 4 * 1 of course 4 * 2 it's 0 * 1 of course zero so 4 * 1 4 * 2 which is 8 which which you have written over here okay then you go over here 4 * 1 how much 4 * 1 4 and then you four so here here is a plus 4 now now you go the next + three because you go over here now 9 * 1 how much 9 * 1 how much uh 9 * 1 9 of course 1 uh which is nothing but 8 8 * 6 48 so you have 48 36 + 4 okay so here's our 48 9 uh so 4 * 1 4 so 4 9 4 9 36 because this 2 * 0 is 0 then you go over here then you have this 9 9 9 * 0 is of course 0 4 * 1 so 4 2 8 so over here uh I think I'm wrong over here um you have this 4 * 1 4 910 0 so four uh it's it's it's it's four okay so it's four it's it it it it it should be eight okay it should be eight which is nothing but what you do you 4 * 1 4 and then you simply multiply with two which is nothing but 8 okay now you simply minus 4 4 uh 28 36 + 8 so how you take out 9 * 1 how much 9 2 7 7 7 6 7 7 6 how much uh oops it's it's uh it should be 9 * 1 2 9 8 7 7 6 14 42 okay so which will be nothing but 42 what the hell i' written over here so I have to just couple it out so I have to just return it it will be nothing but 42 okay minus minus minus uh minus 4 * 1 of course 4 9 are 36 plus 2 4 4 1's 4 4 2 are 8 okay so here's how you do it now you simply multiply with this and then first of all do the calculation do the the calculation then you'll be getting one a scaler as your output so please feel free to put your answer in the description box below I know the answer but yeah I want to leave it to you to do the rest of the calculation because I have done a lot so here's how you take out the determinant of n byn Matrix and how you do it and how you do it's very very easy you just uh keep converting that to a lower or a sub Matrix and then you and then after that you are done okay so here's how you do it so you you define one you define one submatrix a J you define one sub Matrix a i j which is nothing but the Matrix the n n 1 * n one Matrix n minus1 n minus1 Matrix if you ignore the I row and I column if you ignore if you ignore the I row which which you were doing I row and J column which which you were doing okay that is your new Matrix which we which we were forming that is a recursive this is a recursive you can write a Python program for write a recursive solution for this okay so you were writing A1 one and then you were adding the determinant of that submatrix and then you are doing so and so on so on that is a recursive solution so you are writing the recursive recursive stuff so we we we already written a lot so I hope that you understood for formal definition which you can see yeah we have gone through one of one of the example which is very very much important for us to know okay so I hope that you will uh get a lot from this video and determine it I hope that concept is clear with my examples and I also hope that you enjoyed this video I think I have to wrap up with wrap up with this video I'll be catching up your next video till then byebye have a great day meet you in the next lecture hey everyone welcome to this first lecture on single variable calculus from this video I know it's bit 6 days till the video is not being uploaded it's because uh I was I was having some examination that I will that I Was preparing for and I Was preparing for the content for single variable calculus so in this series of lecture we'll be talking about single variable Cal calculus it's U I would say will will come to calculus but uh we had a talk on on linear algebra and some of the videos like I vector and I values to be added okay so that is to be added and that is sent into that is we are making some good examples so that everyone could be could could get some uh fam familiarity with the igen vectors and I values so we always want to deliver you the quality content not the in a fast way or whatever the cont so that everyone could understand igen vectors and igen values that is left and soon it will be added so over here now we'll be starting off with single variable calculus I don't know what happened just in front of my screen but that's a my SQL update no problem just just just for those who wants to know so we'll be starting with the lecture number one uh which is a single variable calculus and this VAR I'll be talking about uh I'll giving you an introduction to calculus and what are the content which you're going to cover okay and what is the next series of videos in this ml2 okay and from now on the video lectures will be around 3 to four videos per week now my everything has got over I'll be focusing on the completing this series as soon as possible cool so uh so in this video we'll talking about that uh friendly introduction to calculus I have prepared my own lecture notes so that I could not miss any topics the first thing which I talk about is giving you what is calculus many people think there is some misconceptions about that I will I will say you uh I will I will just I will just give you some examples a geometric examples of what calculus is and what calculus is not okay uh what are the problems that can be solved using calculus what are the problems that cannot be solved using Cal that that does not require calculus okay we'll talk about two big ideas in calculus such as differentiation and integration now you may think here you you're going to start with differentiation integration don't worry my friend it's very very very I will say I will just give you an I I would say a ta a trailer means 0.1% what is going to come in differentiation and what is going to come in integration okay the geometric view only okay so then then we'll end this lecture talking talking about this lecture now what will be the content which will be in this lect uh single variable calculus course so first of all I don't want anyone to be missed for example many uh high school students watch watch this video so we are going to start with the precalculus we are going to start with a precalculus so I hope that everyone will be comfortable with the precalculus I'll be not covering logarithms I'll be not covering basic algebra because I require you to have a knowledge till class 10th math and uh I hope so you'll be having that like logarithms like uh I'll be talking about trigonometry as well and don't worry about that so I want logarithms and basic algebra like factoring GC F and a lot more so these are the algebra 1 and Algebra 2 which are required like exponential functions as well so uh in precalculus we'll be talking about the two two main Concepts which will be not two main Concepts I'm talking about functions talking about functions mainly the functions functions and we're talking about uh the second one is uh we'll be after after talking about functions we'll be talking about trigonometry Trigon metry okay so just review these two topics from a scratch so that everyone could be familiar it with what's going on and the and and in the further lecture so we'll be covering precalculus in two videos okay so we'll be talking about calculus then we'll going then then we'll talk about limits okay and after talking about limits we'll talk about differentiation solve taking out the derivative some of the power rule power rule then the there are lot lot lot of the rules which which we which are going to learn for taking out the derivative we'll talk about integration solving taking taking on integral for a function and then in infinite series okay so these are the things which will be taught uh in this single variable calculus Series so let's get started with this video so basically in this video we'll Define what is calculus anyone could Define me what is calculus just go in Des description box and tell me what what is calculus what is calculus so anyone could give it a try so calculus is is is is is is you you may think it's a different subject it's not a different subject it's not a something oh my God what is this a rocket science it's a it's a very Advanced version for your algebra and geometric problem okay so calculus I think is a Zach Newton and other the great great physicist mathematicians invented it for solving the some of the great problems so calculus is nothing but Advanced version of your regular algebra Advanced version Advanced version of your regular algebra which you see regular regular algebra and geometry and geometry which you see in real world okay so we'll we'll see some examples so what do you mean with this we'll I will give you a formal definition of calculus when we go further in the course the reason why I'm don't want to give you the formal definition to so that you canot confuse you could not confuse with a definition because I want to be very very clear in the first lecture so I'm going to I want to take some example of it okay so let's take let's take an example let's take one one example let's take one example so example can be example can be let's say that you want to find that you want to find the slope of this line slope of this line so slope of this line can be fin over uh like like this so you run so this is called run and this called rise okay rise okay the slope can be found using a reg regular math of the straight line by the slope can be can be regarded as a rise over run rise over run rise over run okay or Y2 y1 / X2 X1 okay so it tells uh it it just tells how much the Y changes when X changes okay that's how we this uh this is this is telling which will come to that we'll talk about slope briefly in our future videos but over here you can calculate the slope of this line by simply your regular math problem your reg regular math problem isn't it you're using a regular regular mathematics okay mathematics but you told but you're told to find maybe you you select this point and want to find the slope at this point and at every point in this for example over here the slope will be same for this line so over here if you to find the slope at this point you can take another point and then take out the slope for example it goes one uh one at the rise and goes up at the top so that will be slope for every point the slope will be of same okay so so this is a this is this is you can calculate so for every point in that line the slope will be the same now coming to the another this is this is your first first figure this is your first figure number one okay so this you can calculate the slope of this line using your regular math which you can see over here now I if I if I just tell you if I just give you a a line so let me just uh oh my God where is this okay so this is your this is your thing and and you have me just draw a little bit big line uh something like this you want to calculate you want to calculate the slope of this curve line I'm to calculate the slope of this curve line you cannot use you cannot use slope over here slope over here will be not used your regular math your regular math is can cannot be used because every time the slope is changing every time the slope is changing every time the slope will is changing so so you cannot use your regular mathematics over here okay you may think I will just take out the slope and then take out the average but what if I tell you take out the exact slope okay so that it could be same at every Point how do we do that how do we even do that so for example if I try do it over here okay I try to I try to take take out the slope at this point you may think okay I'm going that side but it's but isn't this changing if I go over here here this the the the the slope is changing the slope is changing so what if I to say you to calculate the slope of this line you'll be not able to using reg you'll be not able to calculate using regular algebra okay so it is solved so mathematicians has comes up comes C up with Calculus they have came with Calculus so calculus using calculus you can take out that slope on on on on any particular point in that curve okay so using calculus you can calculate the slope of uh what slope of a maybe curve okay so using calculus there's topical differentiation which will come to that so using you can calculate the slope of a curve okay so that's that's what the one of the calculus problem is okay so uh you can see that I differentiated the regular math problem with a calculus problem this is a calculus problem this is a regular math problem now let's see you have a one you have a one Tower okay so assume I'm taking another example I'm taking another example okay I'm taking another example over here let's let's say let's for for for the sake of an example let's say that you have a tower over here so assume this is a tower of yours okay so this is a tower I'm adding T1 which Tower number one and this is your Tower number two okay Tower number two now you need to calculate now this is your straight line okay this is your straight line cool so you can calculate the length of this using your regular math okay using your regular mathematics problem the length of this can be calculated using a regular math problem but what if I give you there's one Tower there is one Tower and there's another Tower and the wire is something like this the wire is something like this and then and you're given given this stuff C calculate the length you're given the specific stuff and you calculate the length how are you going to calculate it you cannot calculate using regular mathematics whatever you have in your toolbox so there the calculus comes there the calculus comes for calculating the the the slope of a curvy line okay so this is there's the calculus plays an important role so let's example you want to calculate the slope of of of course over here the slope will be zero but you told to calculate the slope over of this it will be you'll be not able to calculate using a regular problem here you need calculus okay so these are some of the instances where the calculus are being used extensively in real world examples okay so there are two big ideas in calculus which I want to highlight there are two big ideas in calculus which I want to highlight the first the ideas in calculus let let me write what are the ideas in calculus the ideas in calculus the first is differentiation different differentiation second is integration integration okay so there are two two big problem which are differentiation and integration so so I will just go go over it very quick way to make you understand about this what does these term means so let's talk about differentiation let's talk about what does it mean differentiation and of course there will be long long Le lectures coming on this briefly talking about this a lot with a lot more examples Okay so uh differentiation is a process for finding the derivative of a curve derivative of a curve so it's the process it's the process for finding for finding the derivative the the derivative of a curve of a curve okay so differentiation is the process of finding the derivative of curve but what is a derivative what is a derivative derivative in calculus derivative in calculus is just a fancier version for saying slope derivative is nothing but is just a fancier version is just a fancier version fancier version for slope okay so in calculus we we we call D we call slope as a derivative okay derivative for the curve okay so you find the derivative of a curve and derivative is nothing but a slope of a curve okay so derivative in in a nutshell a derivative is nothing but derivative is nothing but I is equals to slope and is as well as it is a rate it is a rate of instantial instanes change okay so we talk about that but you can think of it as a derivative as a rate so what is rate over here so let's take an example you want to calculate miles per hour miles per hour okay how much this miles changes when hour means it means what it if you go 1 hour then the miles is bigger like that miles per hour taken or a profit per item these are the rates okay profit per item so these are the derivative is that okay so derivative is just a fancier version for a slope because in slope we take out we can we can't take out the slope of a curve line but using the derivative you can calculate the slope of a curve line okay so uh this is this this is what I want to tell is over here you have uh this problem and you have this problem this problem so this is the ra this is a run you run X x uh X on in X direction and you go uh and then then you rise okay so the slope is nothing but rise over run so it just tells how much the how much the Y changes when X changes okay so this is what the slope is and the derivative is nothing but the slope of a curve okay so what's the slope of a curve what's the slope of a curve what's the slope of a curve what is the slope of that the slope of a curve is the uh let's let's let's take one example let's take one example I'm just I just believe in taking an example okay so we have an example something like this you have a X and Y plane and you have a X and Y plane something like this okay so you have something like this oh my God what is this okay so this is this is your curvy line you take out the slope okay so you select any two point A and B okay you select any two point A and B and join as made a secant line over here so we'll talk about this later on so you joined it okay now what you do now over here the slope or the steepness of this curve the slope the slope or the steepness steepness of the particular curve is changing is changing every is constantly changing between your point A and B okay so if you go over here the slope and steepness is constantly is very steep over here but it is so over here is the slope or the steepness is constantly changing for this curve between your two points which is a and b okay so let's take an example that you want to calculate the SL slope at the at at the at the exact point exact slope at the point C so how you're going to calculate so the geometric view which which I want to tell what you do in basic differentiation you zoom in as infinitely you zoom in infinitely far zoom in infinitely far in that at at that point so for for example you you zoom in infinitely far I'm telling you the geometric view telling you the geometric view so what what you do you zoom in so when you zoom zoom in infinitely far this curvy line becomes your formal straight line okay so when you zoom at this reason you zoom in extremely far infinitely far although you cannot get too much infinite but just assume as a geometric you zoom in a lot there is something called limit property which which we'll talk about but when you zoom in too much what it so it would this curve line will become a straight line okay so when when you zoom it so this curve line will become a straight line so let's take an example that be zoomed in too much too much and and now over here this curvy line becomes a straight line now here's a point C now this is a regular calculus problem so sorry regular math problem you can just uh go over rise okay rise over run okay now you'll be calculating the slope at that exact point okay so what you exactly doing is zooming zooming infinitely far infinitely far okay Zoom infinitely far that that that curve okay uh at for at that point at that point C so the curve becomes curve becomes straight curve becomes straight at that point okay and then you can solve using the regular math problem which is rise over run okay it's not a big deal to understand it just geometric View but we will again revisit differentiation these topics again in in our later videos okay so that is differentiation which is finding the derivative or the slope of a curve at any point C okay so now so we have seen differentiation now now let's see integration the definition of integration so we'll talk about integration means it is not if you're getting confused bear with me these concepts are too too much uh I'll explain in too much easy way so you don't need to uh to to worry about these but if you're being worried don't worry I'm just giving you a geometric view we'll again revisit these concept times and times so let's say for an example that your it's is the area under the curve it's the area under the curve actually okay so I'll just give you what is happening integration uh so this is your favorite uh X and plane and this is your oh my God so this is your thing and then you have a line something this and you're told to calculate the area of the Shaded region area of the Shaded so this is a rectangle this is this is a rectangle you can just calculate the area of this sh line so sorry what you can do actually do so say say for example this is your y5 and you want to calculate the area of this C line okay length in the breadth okay not a big deal to to talk about because maybe this is this very easy to understand it isn't it so so over here we are we can this is reg regular math problem this is a regular math problem regular math problem regular math problem but what if if if you get something like this what if if if if you get something like this so you have this and you have this and let's say for example that you have the curve like this and you are told to calculate the area of the Shaded portion over here it's it does not make any sense that how we going to calculate the area of this C line because not anything we cannot use our regular math over here so here's what we Cal integration is nothing but area under the curve area under the curve okay area under the curve and differentiation nothing but the SL derivative or the slope of the curve slope of the curve okay so different in integration helps you to find the area under the curve okay by adding the we we do is it is nothing but addition okay it's not a big deal we understand it much better way so you write a differentiation for a function we we'll talk talk about I'm not going to introduce it as of now so this is your integration which is the area under the curve and that is your difference differentiation the first topic will be differentiation we're talking about and then we'll go on integration okay so I hope that's not a big deal and I also hope that you understood this now this is this was the first lecture if you have understood a little bit about calculus and I hope that you understood a little bit about this so please give a thumbs up to this video and I hope that you liked this video I'll be catching up your next video in talking about precalculus about functions and then trigonometry and then talking about differentiation and I hope that you enjoyed this tutorial I'll be catching up in the next video till then byebye have a great day hey everyone welcome to this second lecture on single variable calculus uh so in this video we'll be talking about precalculus basically we'll be completing our pre precalculus uh and the in two videos as I think that I should review functions and I should review trigonometry so that everyone who has lost touch in this uh they can review it and the one who is who has at least seen these functions in trigonometry before they can uh just refresh up it is not a full full videos on functions and tri trigonometry I'll recommend watching some of lectures on trigonometry and functions if you want to learn actually trigonometry and functions and solving it but basically it will Pro provide you everything you need for calculus mainly the what you need for functions and you will whatever the trigonometry you need for for for taking of the trig values about uh we we'll talk about sakova we'll talk about one one more thing which is uh uh the special right triangles we'll see some of the tricks for solving the for for for solving it uh we'll also see the unit circle where we'll be seeing the radiance degrees and every stuff will give you the graph of cosine and uh uh cosine and S we'll try to give you the graph of that so I hope that you will enjoy the these two set of videos on precalculus if you're completely new to math and your class I think class 10 then I think that you're good to go just review the lecture of this if you are a little bit go ahead but means you can if if you're in college or or other set of classes I would say uh these these will be good refresher for you cool so we'll revie we'll be reviewing our function so what is the function if if if you can go ahead and write in the description box uh basically the functions is tells you the relationship it's a relationship between your two things in which one depends on other for the for for for for doing some processing okay so basically I think about the functions it's it's a relationship it's a relationship Rel relation it's a relationship just let me write relation relationship between two things between two things in which and between two things in which one depends on other in which one depends on other in which one depends on other it may it it may happen that you're that you're not able to get this what the definition is so let me write a function which takes any value X and Returns the square of that X if you are into programming and I hope that you are in so uh it's it's it's okay that you are not in so you may have seen functions before and and I really hope that uh in in in your programming you just Define the function so what the function does it takes some arguments okay so over here it takes some input values okay and the function do something and Returns the output okay so what is trying to do what is doing is taking the value in input value input value and returning the and doing the square of this so what is doing in the box so we are giving the giving x to the function which is f it is doing the square of this it's doing the square of this and giving your output y Okay so for example the function f let's say we give two to this function it will return it will it will take out the four and then it will give y as a four okay so basically it is the function f is doing some processing and giving some output okay the same way you think about the functions it's it's um it's it's it's kind of a processing what it is doing for example in this F ofx = to x² so it is taking the input value and then squaring them up the function can be let's take an example it takes uh maybe a set okay a set I would say giant X and Joint X is nothing but x equals to the X1 X2 and X3 okay and and and it doing what what it does this function tries to multiply with some Theta okay for example it multiply with some constant to 2 * X1 + 2 * X2 + 2 * X3 so it take the set of input values okay so I hope that that function is making sense what the function actually is so it takes one input value and do do the square of it and give the output value so Y is consider over your output value and X is considered as input value Okay cool so so so let's so let's let's go ahead um basically uh input value is the one which we input anything for example in this case we input two into that and then it does some processing as I said and Returns the square of that or gives the output value cool so I hope that that you that you're making that you made sense out of it so now let's talk about some of the terminology which I want to introduce to you first of all domain of a function the domain of a function so set of all the inputs the set of all the inputs of a function set of all the inputs of a function f let's take F function f is called the domain and what is the range what is the range range is set of all the outputs set of all the outputs set of all the outputs of a function is the range of a function okay so so say for say for an example that that you have a function that you have a function f that takes the value first of all that do the square of that do square of that so you give one and you give min2 over here you return 1 and four okay the domain is 1A 2 and uh range is 1A 4 okay it may happen that you may enter the third two and it will give four so it may it it it cannot be it it can be the same thing four four it can be the same thing okay so these are the certain terminology which you will see a lot in the uh up upcoming videos okay so I want to introduce you two things which is independent independent variables so all the input values are independent variable or values okay so basically so basically all your input values input Val values input values are the independent independent variables independent variables okay all the input values are the independent variables and all the and and the output values are called the dependent variable output values output values it it makes sense are the I I just want to include this one this one this one this one you can just write it out okay are the dependent variables so so it makes sense that for example we give the function f maybe X okay and X is not dependent on anything to be relied because X we have to give it is independent it is not dependent on anyone but this Y is dependent on these X on the input value so that's why we say output value is dependent variable and input values are independent variables okay these are two things so let's take an example I'll take I'll take one small example let's say let's say you have a function you have a function uh F ofx okay x² and you have another function G of X okay which is 5x 8 okay so you have two functions f ofx g of x 8 now now there is a chain of function so let's say F of G of3 if someone ask you to evaluate this function so how will you do so this is these functions are called as composite function composite functions these type of functions are called the composite functions Okay so what we do first of all as as you already might have been guessed okay I'm going to just evaluate G of3 and whatever the output G of3 will give I will give the input to the f of x okay so what how this work workflow will do first of all we'll evaluate the G of 3 okay so 5x 8 we give 3 to the input and we get output as a 7 now we give this seven as a input to to our F x² now that is uh F and then we get the output 49 49 okay and 49 is our uh output value okay so 49 is the answer or or or the result when we apply the G of3 with the output uh uh f f on the output of the G of3 okay so these the these functions are called composite function okay and I hope that you you will see a lot when when when we started change rule of differentiation and it's very very used there you will you'll make use of composite function you make use of the terminology of composite function cool so uh so maybe if you just you can you can just go ahead we have a line so let's let's let's talk about uh slope okay it's always better to talk talk about slope okay so uh let's go ahead talk about the slope say for an example you have the line you have the line okay so I I just draw a very bad line so let me just pick up this and let me just I don't know how to do it leave it so you let's let's take an example that you have a line okay that you have something like this and you have a line and you're told to find the slope of this straight line and you can do with regular math it is nothing but the slope of this line will be rise over run okay so so you can take any two values any to values you Y2 y1 / X2 X1 so how much y changes when X changes okay so it is just telling how much y changes when X changes okay so this is nothing but the slope okay slope is not a big deal to understand it so I hope that it's that it makes sense out of it now let's try to do one thing let's try to do one thing is Let's uh let's let's plot one line so graphing of a function how we do the graphing of a function let's let's let's talk about that a little bit so graph of a function graph of a function so let's take a let's let's say you to you have a function y and you write a function yal 3x + 5 you can also write the function something like this let's take an example you want to make a graph of this function so how are how are we going to do that so basically what I'm going to do let me just remove these straight line I'm just going to draw every everything over here so let's let's take let's take an example 2 4 6 8 10 12 14 uh let's let's do 14 14 uh 16 18 and 18 and 20 okay you have you have 2 4 6 6 oh my God this is 8 10 12 14 16 18 18 and 20 Okay so so how do we even plot our function y = 3x + 5 so we make a table we make a table so let's make a table let's make a table some some something like this let's make a table okay some something like this and you give X and Y okay so Yi you put the values of X so for example you choose x = 0 so when you evalue 3 * 0 means the output will be five okay so the so the first point you got 0a 5 so you put the point on that okay so on y value we have some over here okay now you put 1 and when you 3 * 1 3 + 5 which is 8 okay so 1A 8 is another another point so one and 8 okay one and 8 so maybe you go ahead my diagram is not good so you you will you will keep doing that and and just is uh just to draw a s straight line over there okay just just just draw a straight line over there uh just joining the points and you'll be getting your graph of a function I know it's I think because of my X and Y plane it is failing to draw but you can just take a graph paper and work on that okay so this so we had a talk on slope and we'll revisit the slope in times and times when we talk about differentiation so don't worry if if if you don't understand it but I but I can just hope that you understand a little bit so a straight line is if if you know about slope intercept form slope intercept for okay so over here you have y = mx + b is an equation for a straight line okay uh straight straight line okay where m is the slope m is the slope m is the slope if you all have already been seen and B is the y intercept B is the Y intercept okay so what is the inter Y intercept and x intercept if your line crosses some from here so the Y intercept of this will be four because it intercepts the the the Y AIS at the point 4 okay so that's the that's that's called Y intercept and slope is nothing but rise over run okay which is Y2 y1 / X2 X1 cool so these these are some of the talk on functions and I hope that that you are able to get the sense out of it now let's do one thing let's try let's let's let's try to talk about these the two two kind of functions which you will see a lot in your different in your Calculus journey is parabolic function and absolute value function okay so let's let's talk about that main the graph so U so let's take F ofx = x² this is is an equation for your Parabola okay so maybe let me let me take my black pen I think that that works okay okay okay so now when you when you try to plot it when just just just make a table X and y1 and try to put put the values and get the values so you put the values of X you'll get the value of y put the random Val of X put the Rand get the get the value of y and keep keep putting on the over here so you will get the parabola something like this let me touch it I think it's just not a it's it's not a good good Parabola but yeah okay so this this this is the diagram of your f ofx equals to or a graph of f ofx = to x² okay so you you may you'll think that this function is a continuous function we talk talk about that I I I just don't want to introduce you over here okay so uh this function is a continuous function we we see we'll see how to uh this it is also differentiable so we will talk about this so so that's why I'm in introducing to the graph of these plots so that it would be easy to understand that at at that point so this is your don't worry if you don't understand what is continuous and what is differentiable just I've told you we'll talk about in detail in our later videos okay so this is a function for your F ofx this this is a graph of your F ofx = to x² okay so I just want to draw another make a graph of another function f ofx equals to absolute value of x so absolute value of x so the the the so the the graph of the function will look like straight line let me join it over here join it over here okay so this is your graph of the function okay this is your graph of the function f ofx this is this this is a graph of function a g of X let's name a g of X because it will be confusing G of X Okay g of X where this this is your graph of function okay you can put the values you will get the exact graph of this okay but one thing which I want to tell you that this is a continuous function this is but this is nondifferentiable you may think hey why you telling this now you can tell me later on so just for your information I'm just telling so that why we are introducing these kind of graphs and in front of you so first of all I told you one property that the which is continuous and which is differentiable if you don't understand ignore it the second the second conclusion which you can make out of it the second Insight which you can get of this is both are symmetric both both function fun are symmetric both functions are symmetric with respect to two with respect to Y AIS okay so if you know symmetrical okay this makes them even function these are being symmetric this these are the these these functions are called the even functions okay another thing is the the polinomial function like this or let function like this F of f of f ofx okay = to 9 x^ 4 4x² + 3 so this function is also the even function can you guess why because the power the powers all the powers all powers are even okay all the powers are even okay so so that's why it is a even function so so so so that's why it is an even function if the powers are odd okay then that will be a odd function okay so I hope that you that that you understood either it's not important to know what is even function what is odd function but just for a general purpose I I told you cool so we had a talk on functions now let me see how much the time is being there I think 20 minutes we can go ahead talk about trigonometry now so we had a we had a brief talk on functions and I hope that you had got a very good sense on that so now what I will do I will just start off with uh with the basics of trometry uh either we can get started in the next video I think yeah so let's so I think it the I will just pre prefer in the next video for trigonometry because because there there's a lot to talk I can't continue with this video so I'll be catching up your next video in trigonometry because this is a two pages so I need to and I need to make around your 40 minutes to talk about trigonometry because trigonometry is a big topic to understand so we after after trigonometry we'll talk about limits we will talk about there are a two three videos coming on limits first of all what is limits the evaluating limits squeeze theorem so we'll talk about that mean the sandwich theorem so we'll talk about that so I hope that you had understood this very properly I'll be catching up your next video till then byebye have a great day so now we'll start with the trigonometry I think we'll just review trigonometry in such a way so that you could be from comfortable with upcoming calculus videos and this is not a math Channel I don't know why I'm feeling like I'm teaching mathematics it's it is a part of our machine learning course sorry deep learning course which I'm teaching so that's why I want everyone to go from very scratch to such an advanced level Because deep learning is a b speed so so that's why you have to first of all make your fundamental strong okay so we'll we'll be getting it started with the trigonometry okay we'll just review uh some of the trick functions we'll also review the unit circle maybe some of you have far good in it or I don't know about that I will also review how to graph sign and cosine okay so basically what is trigonometry first of all maybe you can just uh it's it's it's a study of triangles or maybe the right triangles which you see so mainly the trigonometry is the study of right triangle and there are total of three main trigonometric functions so there are total of three main Tri Tri trick functions I'm just going to write trig functions so they are total of three main trick functions the first one is the F the first one is sign okay s second one is cosine the third one is tangent okay so these are the three uh main trigonometri functions and there there there there are some more which which are called cosecant secant and cotangent so these three and the reciprocals they they reciprocals they reciprocals are also there some some important trick functions so they reciprocals the reciprocals are also the very important trick function such as uh cosecant Co coant secant second one is secant and third one is cotangent okay so we will talk about these in detail in in our whole session we'll talk about these six Str trick functions as these three are the reciprocal of this so we'll will study everything today uh these three six functions our our whole lecture will go around these St functions okay so I want to introduce to you something called a some some I would say um uh u i it's I I would say a good weapon for you okay a weapon that helps you helps you to solve uh trick trick trick functions very easily and very very very very very important uh the the sign to remember the what is a sign of the particular angle so that is nothing but cira so TOA so maybe some of you some of you have seen this and some of you have not so this so SOA is very very important in your trigonometry Journey it's very again I'm saying very very important many many people has different different stuff but very very easy to understand and very very easy to uh it it it helps you to remember the definition of your s cosine and tangent so using CA you can remember the the the definition of what that sign means what the cosine means and what the tangent means and it is very very useful and very very important about this okay many people say different different Rhymes to to to identify the definition of s cosine and tangent but I think this is the best and one thing which is easily remember rememberable to anyone okay so what I'm going to do is to I'm going to make a a right triangle okay so but before that I I I just introduce you in that right triangle what does it mean so first of all let me make the pen a little bit lower yeah so so let's take let's let's say let's say for an example that you have this one and you have this one okay I I just don't know how to draw a good line and and this one is and this is your right triangle let me okay so basically this this is called the hypotenuse so let me just give a name so this this is called the hypotenuse hypotenuse which is given Edge okay and hypotenuse is the largest side in your triangle okay this is uh this this this is your angle let's say x and uh and this is this is called the adjacent adjacent side adjacent side to that angle X okay adjacent side to that angle X as it's the right triangle and this is called opposite side to that angle opposite side opposite side to that angle okay so basically what I'm saying that this is a hypotenuse H okay um and we have an angle X where this uh let's let's give some name ABC okay and BC is adj side to this angle X and AC is an opposite to the angle X okay so which is opposite of the angle which which is a side which is opposite to angle X so now what I'm going to do is to take out is just to give you the definition of s of x s of x cosine of x and tangent of X okay so what I'm going to do is to using the definition of Circ K so what I'm going to do so to TOA so so s of theta and Theta is an angle Theta is an angle over here it will be X we we'll solve it but before that over here which you can see s of theta okay cosine of theta and tangent of theta tangent of theta so let's go with the definition of so so so let's try to Define s of theta so when you define the S of theta so so opposite over hypotenuse so s is s of theta is equals to the opposite side over the hypotenuse so opposite opposite opposite over hypotenuse so that's a definition for your sign of an angle the sign of an angle is nothing but the opposite over hypotenuse what's what is a cosine what is the cosine of an angle cosine of an angle is adjacent over hypotenuse adjacent adjacent over hypot tenous okay or in other words a over H as we given the name A and H okay so uh this is a sign this is a cosine and the the the the for for taking of the sign of an angle the definition for sign of an angle is nothing but the adjacent over hypotenuse if you want to calculate the tangent tangent of an angle which is nothing but uh which is which is nothing but opposite over adjacent opposite opposite over adjacent opposite over adjacent which is O and H okay so with the with the definition opposite over adjacent so using this rhyme SOA you able to uh to to to Define these three trick functions okay so now let's use this CA to take out the trig values okay so but before that let's give let's say the adjacent side over here the adjacent side uh let's let's assume adjacent side is four whatever the centimeters or meters let's let's assume centimeters the hyp the the the AC which is which is opposite side to that angle or the height so it will be three and may you can take out the hypotenuse how you can take out the hypotenuse this is very very easy using Pythagorean theorem Pythagorean theorem tells a² + b² = c² a sare is your BC b square will be your AC equals to c² so when when you go ahead when you go ahead 3² so sorry 4² + 3² = to what c² okay 4² + 4² 16 + 9 = to what c² 616 25 = c² and when you when you when you just remove this so it will be < tk25 = C which will be C = 5 so the hypotenuse will be five okay so you given the sides of an of of a right triangle now what you need to do you need to take out the S of the x s of x s of X so how are you going to do take out the S of X but taking out the S of x the sign the sign definition is so K so sign is opposite over hypotenuse so opposite over here is three and hypotenuse over here is five is your s of that s of s of the x s of X is nothing but the 3 over 5 so what is cosine of x so what is cosine of x so what is cosine of x cosine of x is C so so C A so adjacent over hypotenuse so adjacent side over here is a thing BC and this is 4 ided H which is 5 so cosine of x is 4X 5 what about tangent of X so tangent of X is nothing but opposite over adjacent and opposite to that angle X is three and adjacent to that angle is four okay so which is which is nothing but 3 over 4 is the tangent of X okay so these three s cosine and tangent we are taking out and this giv you definition what is a sign what is a cosine and what is a tangent s is nothing but the opposite over hypotenuse cosine is nothing but adjacent over hypotenuse tangent is nothing but opposite over adjacent how you remember it using the SOA definition and I think this is this is pretty much easy okay so we had used this s we have used thisa to identify this s of the particular x cosine of the X and tangent of X okay so we have seen this uh s cosine and tangent so we have seen the definition of these three now what about the reciprocals of it now what about the reciprocals of this so the same way the other three functions are the reciprocal of s cosine and tangent so let's let's let's let's talk about that so other trig functions other trig functions other trig functions are the reciprocals are the reciprocals of these existing these s of x cosine of theta and tan tangent of theta so let's give the Theta name Theta angle okay so so uh let's let's let's talk about the first one which is the cosecant okay cosecant cosecant cosecant so what is this cosecant is a reciprocal cosecant is the reciprocal cosecant is the reciprocal reciprocal of s cosecant is a reciprocal of sign we'll see what the reciprocal means just in a second second one we have which is secant and we denote this as SC SEC okay set so secant is nothing but is the reciprocal reciprocal reciprocal of your cosine of your cosine and the the the the last one which is the over here is cotangent cotangent which which we didn't denote is a cot okay is the reciprocal of your tangent is the reciprocal of tangent and how do you define cosecant cosecant is SC CC CS C okay secant SEC C and what about cotangent Co T okay so in sign we write s n cosine cos tangent t Okay the these are sh form given to them so now what you mean by reciprocal so let's take out the let's let's take the definition of cant cosecant of theta for for for for for any angle okay which is nothing but the reciprocal of s sin Theta so 1 / sin Theta so reciprocal of the S of that angle sin Theta okay so it will be one over s let's let's remember our favorite definition from one and only so so CA SOA so sign is opposite over hypotenuse s is opposite over hypotenuse s is nothing but opposite over hypotenuse it goes up of it will be H over o h over o which is nothing but your cosecant so definition of a cosecant of a Theta uh is nothing but a hypotenuse over uh over opposite of that angle okay uh the next one is the next one is secant of theta secant of theta is the reciprocal of cosine which will be 1 / cine of theta so it will be nothing but so what is the cosine adjacent over hypotenuse 1 over adjacent over hypotenuse it goes above so it nothing but h of a h over a which is hypotenuse over adjacent okay so the the definition of secant of the Theta is nothing but the hypotenuse over adjacent the last one over here is cotangent so what's the time so the last over the last one is over here is cotangent cotangent which is cot of theta is is the reciprocal of a tangent 1/ tan of theta So Tan of theta is nothing but opposite over adjacent so one over uh opposite is over over here o by a okay so that is so it goes above 1 over so sorry a over o which adjacent over opposite angle okay so opposite to that angle Theta so cotangent the definition of a cotangent is adjacent over opposite of that Theta okay so we have a g given you definition of CA s cosine and tangent and we have also given you the the definition of cosecant secant and cotangent of theta okay so we have also we have given you everything about this now I hope that you had got a good idea about what is these angles tells you but before that what I'm going to do what I'm going to do is to solve for the triangle take out we have to have taken out the sign of that triangle which we have built up S of this was 3 over 5 cine 4 over 5 tangent 3 over 4 so what will be the cosecant of these uh of of of this uh triangle okay so let's let's take out the cosecant the cosecant of X of the angle X which will be 1 over the S of theta okay so which is nothing but again when you when you go ahead so so let's let's not write it again let's let's use our formal definition so cosine cosecant of theta is hypotenuse over adjacent hypotenuse was F five and adjacent was four so F 5 over 4 is the answer of this so sorry it's 5 over3 okay because adjacent because uh over over here your uh adjacent opposite angle so this is this this is uh opposite okay I don't know where it is yeah so it is opposite so 5 over 3 okay which is the reciprocal of your sign if if you told to take out the cotangent or the secant first of all let's L secant of X which will be nothing but secant of X which will be nothing but let's let's let's say for for for take example secant of X is hypotenuse over adjacent hypotenuse was five adjacent was four okay so that is just the reciprocal of your cosine what is the tangent of X what is the tangent of X so tangent of X is adjacent over opposite okay so adjacent so adjacent will be uh four and uh opposite is three four and three okay so we have taken out for for for for this triangle we taken out the cosecant secant and who told tangent it's cotangent okay so mainly we would add C okay so um so these are the things which I which I want to remember and I hope that you remember it as well okay now I hope that that you have got a better idea about what is this SOA means and the form form formal definition of this uh CA and these six definitions I want to introduce you to to to two special right triangles it is very very important to know okay so I'm I'm just going to introduce to you about two important uh two special that helps you to remember these stct values because these will very handy in your uh calculus Journey so I'm just going to introduce to that so there are I'm just going to write two special right triangles two special right triangles right triangles so the first one is the first one is 45° 45° 90° triangle 90° triangle this is a cute angle okay so over here when you when you when you try to make a make a that so let's make a right there is one angle which is of 90° and other two angle are 45° okay so I'm just going to write 45° and 45° okay so this is the following and you have let's say for you you you'll be getting these base so base will be one it will be also one and the hypotenuse will be 1.41 which is < tk2 okay and the hypotenuse will be < tk2 because when when you take the hypotenuse how you'll do 1 + 1 1 s + 1 s = c² when you when you do this 2 = to c² when your < tk2 okay that is your hypotenuse now now let's apply Saka and their reciprocals to take out the sign of the 45° okay so let's apply the ca over here to take out the sign of 45 Dee okay of this angle let's let's apply that uh let's let's let's go ahead uh yeah so s of 45° will be first of all SOA let's let's remember our definition so TOA so opposite over hypotenuse so opposite 1 over hypotenuse < tk2 okay so which will which will be nothing but when you rationalize it when you rationalize it it will be nothing but < tk2 by 2 I hope that you how you you know how to rationalize you simply multiply something this I think that's some of you do not know about it so < tk2 * < tk2 < tk2 will beunk 2 by two that's what you get okay so that so you you get this and then when you when you go ahead that it is approximately equals to 0.71 you can calculate using your calculator okay so that is your s of your 45° what is the cosine of 45° cosine cosine of 45° cosine of 45° so adjacent over hypotenuse so adjacent is one and hypoten puts two okay so adjacent is 1 and < tk2 1 / < tk2 which will be again < tk2 / 2 by rationalizing approximately equals to 0.71 that is a cosine of 45° now what is the in in in this in the part for this triangle what is the tangent of 45° 1 / 1 how do you say opposite over adjacent which will be around 1 so 1 is the tangent of your 45° for this triangle so what about the reciprocals of it so what about the reciprocals of it so the reciprocals of it will be first of all secant first of all let's let's let's talk about uh co co cosecant cosecant of 45° which will be uh the the the reciprocal of it so hypotenuse over uh opposite angle so hypotenuse is < tk2 < tk2 by 1 which is nothing but approximately equals to 1. 4 1 okay what is the secant of 45° so secant will be h/ a h/ a hypotenuse hypotenuse is < tk2 over 1 which will be also approximately 1.41 what is a cotangent what is cotangent so what is a cotangent is will be 1/ 1 because adjacent over hypotenuse which will be one okay so these are the reciprocal the trig values so so we got the trig values of you can see over here that using this special these these special right triangle will be so so much in your journey so that's why I'm introducing to you this one okay now let's go ahead let's let's talk about another now you got to know these stct values let's talk about another uh special Tri triangle which which is nothing but um 30 60 90° triangle 90° diory angle and of course this is a acute angle okay so when you when you have this uh let's let's let's print it out yeah so over here you have this and let's say let's say for for sake of an example your uh uh your uh 30° 60° 90° okay adjacent side of 30° is < tk3 which is approximately equals to 1.73 and it is one the the the the the the height or AC is sorry the height will be one and the hypotenuse is two because this is all because mainly people what they do they put that into the base and that is wrong because the hypotenuse is also always the longest side okay now let's do one thing let's apply the ca so K TOA on 30° okay for the 30° angle for the 30° angle okay so go ahead apply the ca for the 30° angle then go ahead and apply the Circa and the reciprocals as well and the reciprocals are well for the 60° angle and you are good to go and you are and you'll be getting your trick values and that that that will be yielding to around so much of trig values in no time which you got it okay so I want you to do these two things taking out and write the answer in description box uh sorry I'm saying in comment Box about the the the the the all the all the trig value and the reciprocals for the 30° and all the all the all the tri function with the reciprocals for 660 de okay cool so we had done lots of practice okay but you may think yeah you can't we do for greater angle because all these are cute angle can't we go ahead talk about the uh the the the large angles like maybe 270° okay so maybe maybe bit more than that something like cosine of pi/ 3 cosine of maybe 3 Pi or sine of 3 pi/ 3 sine of 7 High over six uh maybe anything anything can be happened okay so how you go ahead and do that how you go ahead and do that so you can just uh maybe what is the sign of 2 210° what is this cosine of 120° there a lot more things so for that for that we have something called as unit circle you have we have something called as unit circle which which is very very very very important and it may happen some of you may think he you I cannot remember unit circle because many many of you has an enemy with a unit circle in your I think class 9th and and class 10th I think most mostly in class 10th uh so so don't worry about it very very trick is available online about remembering it and it's very it's you don't need to remember it it makes sense as well sometimes okay if if you think about geometrically don't think about remembering everything you can just make sense of it because it's it's it's it's it's kind of U very very important in your Calculus Journey because these are the very very frequent radians which you get or degrees which you get in your whole calculus Journey okay so what I'm going to do is to introduce to you a unit circle I'm not going to give you a trick for remembering the unit circle but I will leave a link in the video description about the trick for remembering this but as always Google is up for you you don't have to worry about it okay cool so um let's go ahead let's let me show you uh let me show you the unit circle let me show you the unit circle where is my unit circle here here we go so just going to take in between make it a big okay so this is your unit circle this is your unit circle I'm just going to make it a little bit big yeah so this is this this is your unit circle and very very uh uh scary or disastrous unit circle and and it's very very important as well so uh I'll just introduce you so that every everyone is familiar here you have three things the first thing is coordinates second thing is you have the uh the radians okay of the coordinates and then you have the degrees angles angles in radians and angles in degrees okay so you have the coordinate so we talk talk talk about that coordinates you have angles in radians in radians and you have angles in degrees in degrees okay so first of all let's let's let let's talk about this one uh let's let's talk about this one so this these go first of all over here you have y AIS and xaxis so uh the the the radi with this origin to this point the the radius of the circle is r = 1 R = to 1 so the coordinate at this point will be 1 comma 0 will be 1A 0 why 1 comma 0 1 on xais and it's 0 XIs 0 okay so 1 comma 0 it is the same 0 0a 1 on x axis is z so radius is areal to 1 everywhere so it will be 0a 1 so this is y axis this is one this is xais this is 0 comma minus1 so these are coordinates the same way these are also coordinates this for xaxis and this is for y axis this is for xaxis this is for y AIS okay and you may think how you have come up with these kind of uh coordinates how you come up with this it's it's basically uh more about geometric understanding you can go ahead see if if you go in much more detail as a bit out of the bound of this video because we just reviewing it so over here which you're seeing over here X and Y coordinates so these these are all the things which you're seeing outside the circle are the coordinates inside which you are seeing this Pi what whatever the fraction which you're seeing uh is the radians is the radian okay so over here you have 2 pi you have 2 pi and then pi over 6 pi over 4 pi over 3 for for that coordinate so there are three over here and Pi / 2 then go ahead we have these all the things are the are the for the for for where the radian is 4 Pi or 3 the coordinates are this 5 5 pi over 4 the coordinates are this okay so these are the things the next thing is you have the degrees you have the degrees you have the radian you have the degrees 120° 135° 90° 6 60° maybe you all remember using a pro protector okay it is very very uh familiar to you so I hope that's not a big deal now but there's one thing which I want to tell you is how do you remember these things so first of all what you do you have the three three points over here you give two two two two all the denominator will be given two okay and now uh 1 2 3 1 2 three okay you you do the square root every numerator if you put it does not require it < tk3 < tk2 < tk2 < tk2 and of course it does not require so you get this so these these some kind of tricks for remembering it it's very very useful in this okay so it's it's very very useful as well okay uh so you you remember these coordinates these Pi is the 1/3 Pi / 6 is 1/3 of Pi / 2 so you need to only remember some of the coordinates and some of the radians only that and using that you are getting uh using that you can get more radi or whatever okay so these are there are some tricks which you which you need to remember uh maybe you can search online otherwise the link is in the description Box about the trick for remembering this these things okay so you have the radians you have the degrees okay now what so every coordinate X and so every coordinate is a pair of X and Y okay x coordinate and y coordinate but it is indicating this is this cosine of theta and S of theta so X indicates the cosine of that maybe the radian or or or an angle and S maybe this Theta can be the radiant or that uh or that angle okay so let's let's say for for the S sake of an example let's take an example you told you want to find out the cosine of Pi / 3 of Pi / 3 how are you going to find it so you search 5 over3 over here what is the x coordinate is what is the x coordinate because x coordinate indicates the cosine of theta which is 1 / 2 what is the S of < / 3 so y coordinate is root < tk3 / 2 okay so these are thing these are the trig values okay which is which is very very useful maybe in your examination or in calculus you can take out anything with this say for an example you're told to find cosine of pi/ 6be cosine of pi/ 6 so over Pi / 6 is will will be the the x value is < tk3 < tk3 / 2 let's take an example you told s of 3 piun / 6 uh 3 3 piun over 4 so what it will be it will be nothing but minus sorry < tk2 /2 because that indicates the y coordinate so coordinates indicates the pair of the cosine and the sign of that angle or the radian okay so some of the interesting thing which I'm to show you to you is what is the tangent of 3 piun / 4 what is the tangent of this 3i 4 so tangent you all know tangent is equals to S of that 3 piun 4 / the cosine of 3i 4 okay so s of theta divid by the cosine of theta so it will be the S of theta 4 will be < tk2 / 2 okay < tk2 / 2 because that's Co sign is which which is min2 which which you can see over here so the the the the denominator cancel which will be minus 1 the tangent of the 3 Pi 4 will be minus one so this all about manipulations more all all about other things which you need to which you need to just uh apply some logic and you will getting getting your trig value so this gives you lots of trig values okay now what I'm going to do now what I'm going to do now what I'm going to do is um is to do one thing uh first of all uh what is what is the cosine what is the cosine what is the cosine of 3 pi over here we we only still have 2 pi okay but what is 3 Pi so it it here the radian is 2 2 2 pi so what is the cosine of 3 Pi so how how is construct at 1 Pi okay then 2 pi then 3 pi over here it will come over here the 3 Pi the cosine is always referred to the minus one so the answer will be minus one how first of all here is your Pi okay you rotate there is two first of all you go over here you rotate half circle Pi then 2 pi then 3 pi over here so 3 Pi will becoming minus so what is the S of 3 Pi which is zero because the y coordinate is zero so it is not matter whatever you you can have it does not matter okay now there is one last thing which I want to tell which I want to tell is there is one last thing which I want to tell is what is the S of piun / 4 what is the S of piun / 4 which will be nothing so over Pi but this is pi but there is minus so you here it is in second one so you what you do you start going you start going into the negative Direction okay so leaving one you go to two Okay because here is here the second coordinate you go over here 2 one and that is that is so sign of my piun 5 will be sine of 7 piun over 5 4 okay so that is nothing but two okay so uh root so sorry it's minus 2 2 over 2 okay so this is how you go in the negative direction if it is over here then you go from here so this is how everything is working you using the unit circle and I hope that you understood very very intuitively and I hope that's very not a not a big deal to understand these things so this is a whole thing and and and and basically you will see whole calculus and revolving around this okay most of the calculus problems TR TR calculus problems cool so I just want to talk about the last thing which is very very useful over here is how do you graph s cosine and tangent maybe you can search online because but I I just show you the graph of s of yal s of X looks like so here is your thing so it it it will look something like this it will look something like this it will look in cyclic form okay in cyclic form it it will look in cyclic Forum something like this okay so it looks in cyclic form but the same way your your your cosine will be will be different will be different because over here it it it is some some something y s of X something like this but what is the cosine cosine will be let let let let little bit different a little bit different it will be okay it will be okay so it will is cosine y = cosine of x will be something like this and and and this these are the graphs which you will see later a lot when when when when I will teach I will take these kind of graphs about I'll go in different differentiations very very important okay so I hope that you understood the trigonometry whatever I taught and um it's just a review there there a lot to learn in trigonometry there a lot to practice in trigonometry like inverse strict functions and ASM tootes we have lot more things which you can explore please please please go ahead see some tutorials if you're going to go ahead deeply in trigonometry these are things which I need to revise before going to the calculus now we are ready now we are ready to go in calculus from the next very we'll be talking about limits and we completing I think limits in three to four videos I think only three videos will required for limits short short videos we completing limits and then we are good to go and then we start with differentiation okay so and then we will go ahead talking about solving different take take Taking out the derivative like quotient rule chain Rule and a lot more and then we'll go ahead uh there's also power rule which I can't forget about that so there's um so about different op after differentiation I think we'll go ahead with integeration okay so I hope that you understood I'll be catching up the next video till then byebye have a great day hey everyone welcome to this lecture from this lecture we'll be actually starting off with uh limits and and limits is one of the most important topic which I cannot leave we will not too much talk about limits we'll not do that much kind of examples but yeah I will give you a a brief about limits and what the limits are and there there are lot more lot of the things which we'll talk about as well as we'll talk about uh uh how to evaluate the limits using various various techniques which we'll study in the next video basically in this video I'll just give you what exactly the limits is okay so we'll we we we see lot lot of examples to understand uh why this is called the microscope of maths so it's this very very nice uh nice things which have invented so far uh in the history of uh math and I think that differentiation the definition the formal definition of differentiation will'll Define using the limits property okay so now let's get started with one of one of the example which is in front of you which is that maybe you are familiar with this plot it is the the parabola plot if you know because we had a talk on this and how you plot you just plug in the x value just put the point on the X x coordinate and the ycoordinate and then you join that the points so this is what you get and this is not a best Parabola diagram in the history of math uh but yeah this is the basic uh basic stuff which you seeing over here now what I'm going to what I'm going to do is uh uh make a pie wise function but before that what I'm going to do is to just just uh talk about the this function and then we'll make a PE wise function but before that what I want what I want is to find the value I I I I want to find F of two F of two F of two so what it will be so we give to the X and this will be nothing but four assume that this is a four okay so just want to make it some over here so it is a four okay so this is nothing but four but what do you I say you have a function G of X you have a function G of X you have a function G of X you have a function G of X where you uh first of all uh what it X squ for every values except xal to 2 except x = 2 there is x² okay and another is when when when uh the input value is two then the output will be one when X = to 2 then the Y value will be be one uh in the first is telling x² in every input values except two and one for for the input value equals to two so that is these functions are nothing but called the pie wise function pie wise function uh which you which you might have observed already and I hope that you had already so now if I want to plot it over here this plot G of x if I try to plot it the G of X so what it the what the oh my God yeah so uh what if I if I want to plot it so for plotting it let me just remove this one because I oh my God I don't want to do this uh I don't know how to select the good uh the Eraser let's use very simple eraser let's first of all remove this and there is a discontinuity okay so if fals to two if fals to 2 then if input value equal to two we just saying that it is it should it should be one okay so this is your function of your G of X this is your function of your G of X where we are saying it this is a function this is a graph of x² this is a graph for X squ but we had told there is one discontinuity which we'll talk about today about discontinuity but there is one discontinuity in this function and you're saying if at xal to 2 make that to two and this x² does not work for x x is equals to 2 okay okay so this is a function this is a graph of your function G of X okay uh this this pie wise function this is how it looks like now coming to the next part is what is I'm just asking one thing what is the limit what is the limit what is the limit of your function G of X of your function G of X when when X approaches to when X approaches two not exactly two when X approaches two from the right hand side and the left hand side okay so it it may be confusing to you what exactly I'm saying what exactly I'm saying what is the limit of G of X when X approaches 2 from the right hand side and from the left hand side so let's start with the from the from the left hand side so let's say the let's say the input values the input so let's start from the from this value so it gets closer and closer to two okay so it approaches Clos and Clos it so we simply write limit of G of X when X approaches to so this is this is your limit uh notation to write it out so basically we are telling uh here we are telling what is the limit of when X approaches two from the left side okay so when X approaches two from the left side it seems to be approaching four it seems to be approaching four we are not taking exactly F of two because that is undefined at that point the function x² is undefined but basically there is discontinuity and we are we are getting closer and closer to two from the left side for for for example we are taking the value 1.9 1.999 1.999 1.999 N9 we are not getting exactly to two but we taking these values and taking the square of it okay so it seems to be approaching fold how I'm perceiving that so let's do one thing let's try to give the let's try to do from the left hand side first of all so we have an x value and and we have a g of X okay we have a g of X so you give 1.9 to XX when you square it up you get 3.61 okay and because it is defin else everywhere except X exactly equals to two so at 1.99 it will be 39601 that is a square of it okay so we give 1.999 so that is 3.99 601 okay we are not taking exactly it's technically do not can get exactly two four but it if you you use the calculator 1 1999999 it will just take out the square it will just round of that but actually this technical not true okay so over here you you're are getting you when when you're are approaching two when you're are approaching two when you are approaching xal 2 from the left hand side it seems to be approaching four your function G of X seems to be approaching four let's take an example of this so when you're approaching two from this side when you're approaching two from the left side it seems to be approaching four okay it's not exactly taking as a two as an input we saying if the the the the limit of a g of X tells you when X approaches 2 from the left hand side and when from from the left hand side so over here when the limit uh of G of X when X approaches 2 is approaching okay limit of of G of X when x x approaches 2 is approaching four the fun is approaching four so it is approaching four it is approaching four it is approaching four so when you when you get Clos and closer to two from the from from from the left hand side it seems the function G of X is approaching Clos and Clos it's it's it's approaching four it's not exactly four but it's approaching four okay now let's see the same way going from the right hand side so let's go with the right hand side so when we try X and G of X we give 2.1 4.41 2.01 uh 4.41 so from the right hand side as well from this side as well from this side as well from this side as well it seems from this side as well we are approaching to from this side this seems like it is approaching so it seems like it is also approaching four it is also approaching four so the limit from the right hand side limit of f ofx f ofx = to 2 of G of x from the right hand side is equals to 4 and the limit limit of x approaches 2 from the left hand side is also four so the limit of G of X when X approaches 4 2 is exactly equals to 4 and this is how you take out the limits of a function okay and this is damn easy we have we had to Made made use of table or Forum using calculator and so sorry and then we are done okay this is what limits are limits are nothing uh it's it's it's it's a function limit of a function limit of a function if it exist because limit cannot exist as well I will tell you the reason lat later on so limit cannot exist as well for some x value C and we denote so we're just going to write the formal let's write let's make a new page yeah so I'm just going to write the for formal definition the formal definition says that limit not no it is not a formal definition but um this is an informal definition but just now for the definition okay the limit of a function limit of a function limit of a function if it exists a of course if it exists if it exists limit of function if it exists for some x value for some x value C for some x value C okay for some x value C to the height of a function to the height of a function to the height of a function the height of a function gets closer and closer gets closer and closer as we saw that is gets closer and closer closer and closer to uh as X gets Clos and closer gets Clos and closer to to the to see gets closer and closer to see from the left side from the left and the right side from the left and the right side okay so for example this is your function this is this is your function assume that this is your function and this is there is a discontin it over here and you're telling that okay what is the what is the value of x so for example it's something like this okay and you're telling that's two so what is g of X when the what is the limit of what is the limit what is the limit of x approaches to of your function G of X okay so when X approaches to from the negative side sorry the left side and the positive side from the from the from the right side it seems to be approaching from the first of all from here and then it seems to be approaching for exactly it is four okay so it's check the gets the x value gets Clos and closer from the left and the right side and then you decide and then if the both are equal then you say okay that's the limit which exactly equals to four okay so now we had seen the one example and I hope you had got a very good idea about limits and we'll I will just give you the formal definition of a limits because it's very important for you as well to understand what exactly a limit is okay so so what I'm going to do now is to take another example so that you could get a more uh more more good definition for you at least uh we we we'll see that okay so we'll just give you the good good definition of the limits and it's very very important as well uh mainly if you uh for for for for for forgetting more about what exactly the limits is okay cool so uh what I'm going to do is take another another function take another function and the function is uh the function is the function is uh F ofx = 3x + 1 so what I'm going to do is to take a y i just to take a function three y = 3x + 1 y = to 3x + 1 so this is your function uh F ofx this is your function f ofx okay and you will identify the need of the limits from this function and you you will get to know the need what's exactly the need of limits from this function so let's get stress so let's what what do I ask you to do take your graph paper try to plot this function by putting X values and try to put the coordinates on the your graph paper and then you are done okay so what I'm going to do is to make the make a quick quick uh diagram of this function I don't know it it is it is a bit hard to make but no problem I'll try to make it as much as I can so just want to is it is uh just wait for a second I'm just checking the diagram yeah so what I'm going to do is to make an X and Y plane so I'll just put this make let's make in blue because it works okay so this is your X sorry y cord y plane y AIS and this is your xaxis okay what I'm going to do now is to just plot it now is two is approach a 7even so one two oh my God what is this oh I I I I got to know about this now one two uh let's let's let's make it a little bit more we two 3 4 5 1 2 3 4 5 6 7 okay so this is your uh let's let's make it inal way now when I'm going to try to put the value of one so let's put the value of one over here that is four and it is so that is four okay put the value of two that is that is nothing but uh seven I think so yeah that is seven okay so when I when I try to just join the points I'm going I try to just join the points I'm just trying to fully join the points okay so that is this is what I get when joining it okay so it is not a best function in the history of math but yeah that's what I can draw in just now with quick graph so this this this is the function y = 3x + 1 so this is this this is a function y = 3x + 1 coming to that now this is a function now what I want to know is limit limit limit of your function let's say f ofx let's give the name of this function f ofx limit of function limit of a function f ofx when X approaches when X approaches 2 what what what it will be you can simply it it is not it is continuous it is we we have this this is not a pewi function it is continuous and over here we don't have any discontinuity we don't have any whole over here okay it is just a point just just for Dra drawing the diagrams but but over here your uh this function is continuous and if you go when when when when when when we approach from the negative side as of the positive side and even when we get when we get x = 2 even when we get x = 2 so you'll be getting uh y = to 7 and that is the fully valid according to your function that is defined that is defined so the limit of your F ofx when X approaches to from the both the side is seven when X approaches to from the both the side from the from from this side and from that side even it gets two it will be defined because your function is not set it that okay your Y is 3x + 1 is not equal to 2 so it is not something like this it's not something like this it is defined at at X = to 2 so the limit of your function you can simply put the value of x and you'll be getting your limit value over here so you can simply put the value over here and then you'll getting your uh the limits even when when you say it it goes from the left side and the right side and it seems to be approaching seven okay coming to that now we have seen this now what I'm going to do is a little bit twist it so that so that you could get uh at least the idea of why limits are too much useful in this era so what I'm going to do what I'm going to do is um is uh say say for say for an example say for say for an example uh uh I'm just going to take a take a take this and then I'm just make a hole over here okay so just make a hole over here oh my God this is what I made the hole okay so now now the function is defined this function y is defined everywhere except X = to 2 and when it's xal to 2 it is one it is one okay so so so so what is so what is the limit of f of x when X approaches 2 it's seven It's s of course it's seven but you you will see the but you'll see the the main idea that if your function has discontinuity it will tell you that if we get closer and closer to two from both the side from the left side and the right side say say say for say for an example you have X so you put 1.5 so you put it you go from left side that is you have uh y function so that is one when you put one you'll get four when you put 1.5 you get 5 5.5 when when you put 1.9 you get 6.7 you get 1.99 you get 6.97 you get 1.999 you get 6.99 97 so this is this is getting closer and closer to seven if if you get closer and closer to from the from the left hand side it is getting Clos and closer to seven coming to that from the negative side coming to that from the sorry from the right right right hand side so that is that is that is nothing but 2.01 that is 7.3 2.01 that is 7.03 2.1 7.3 so this is also seems to be approaching 7 from the from from the right hand side so the limit is seven okay so this is how you you you you take out the limits and I hope that it's completely making sense at least to you okay coming to that later on now now what I'm going to do what I'm going to do is uh is to just go ahead give you the the the the topic which is onesided limits okay so you all have seen the onesided limits but I this it's my job to make you familiar with these stuff okay so I'm just going to take this one this diagram I'm just going to take only this diagram just going to take only this diagram okay now uh just let me just make it in the new page so that I could make you understand a better way okay so when X approaches to from this side and then that side okay let's make a new function let's make a new function let's make a new function don't ask me what's the name of the function just just let's draw a random function so that just just we can understood visually let's call that function P of X okay let's call that function P of X of course this is a peie wise function this is a pie wise function um it will be like this x² for and I will just tell you later on okay coming coming to that so let's make uh let's let's make a function this one uh let's make a function let me draw a DOT line at least a line uh something like this and then something like this I want it a little bit big so that everyone could be able to see it now uh just going to take this now what I'm going to do one uh it's it will be something like this okay so I just let's do it and this is from this side okay so this assume that this is your function uh this is your function it's just not a big big big function which you have ever seen and your eror okay let me make a little bit so that everything is yeah okay so you have the okay so you want to you have a piecewise function have peie wise function so assume that this is a p of X this is a p of X which is a which is a piecewise function which you ever seen in your life uh again just just just assume that that is a function and and over here and and over here uh over here uh you have limit what is the limmit of X approaches uh 2 okay so what is the limit of x approaches 2 from the negative side of your function f of e of X so when you when you get closer and closer to two from the negative side means from the left side it gets Clos and closer to five that gets Clos and closer to five it gets Clos and closer to five that is nothing but five okay so when you get Clos and closer to two from the negative side that's got Clos and closer to five and when you get closer closer to seven then when you get closer limit of P of x from the right hand side from the right hand side you get it is getting closer and closer to two that is nothing but two so over here you you you're taking the limit from the from the left hand side and taking the limit from the right hand side and these both uh right right hand limit and lefthanded limit are not equal are not equal are not equal okay are not equal and so so that's why uh this limit does not exist this limit does not exist why because the limit because the limit from X approaches 2 from the negative side of a function P of X is not equals to the limit of your P of X when X approaches 2 from the positive side it's not equals to which which you can clearly over here which you can clearly see over here that this the X approaches to from the left hand side and X approaches to from the right hand side is not equal but in previous cases which you have seen they both were equal so there the limit exists but here the limit does not exist okay so this is what the whole idea of the onesided limit when someone ask you to evaluate something like this get to get the limit from the negative side you're just going to go go uh get Clos and closer make a table format make a TBL format and then go ahead but we'll see how to evaluate the limits in our next section of of of of a function so don't worry about that okay I hope that everything is clear and now what I'm going to do is uh have have a small understanding of uh of of of a function again a pie wise function this is a peie wise function so let's let's let's take another pie VI punch so what I'm going to do uh it's the it will tell you the idea of infinity which will talk about we will talk about this in our next video a little bit more way I just going to go to give you the intuition about this okay so this is your this is your something like this okay assume that this is 1 2 3 4 5 6 okay um this is also 1 2 3 4 5 and this is also 1 1 2 3 4 5 5 1 2 3 4 okay now what I'm going to do is to just make uh is to just make us or here okay to make something like this make something like this and when we say when we say that when X approaches when the limit when the limit of when the When when when when the limit of f ofx let's say the function f ofx when X approaches negative uh from the from from the right right hand side from the right hand side even even from the left let's say from the left hand side from the left hand side when the limit get close and close to left hand side so it says that it's it's getting to Infinity it's getting to Infinity okay because you have a vertical asymptotes over here vertical asymptotes okay so it's getting to infinite so we say that that is a Infinity that is the infinity okay so this is a general idea when a limit can be in okay so don't worry about this will talk about vertical ASM tootes because this is a vertical ASM toote so we say that when when when when when the X approach is three it gets the limit of function gets gets to the infinity Okay cool so what I'm going to do now is talk about the last things of the topic which which which which we are left on is about continuity I think so yeah we'll talk about continuity but before that I want to give you a formal definition of a limit so that you could be under you could just write some somewhere so that everyone is familiar with it so the formal definition of a limit the formal definition of a limit the formal definition of a limit let F be the function let F be the function okay so the let let F be a function let F be a function let F be a function and C be any real number and C be any real number any real number okay limit limit of x approaches C of your function C is then C is the arrow number mainly we call it an arrow number which we which we have to approach exist this limit exists exists if and only if if and only if three conditions are made first condition limit of f ofx when X approaches C from the negative side exists exists second limit from X approaches see from the positive side exists and and third one is limit of x approaches C from the negative side of f ofx equals the limit of x approaches C of function f from the positive side so they should be both equal okay so this is so these are the three conditions where you all have seen in a real life examples which I just showed you about this example or maybe or this example Maybe Okay cool so now let's talk about the last thing which is nothing but continuity which is nothing but Contin nity so what I'm going to do is just is just spend some minutes talking on this uh talking on this uh so we'll talk about continuity so let me just give a definition continuity okay continuity a a continuous function basically the definition States a continuous function uh cont newest function is simply is simply a function with no gaps with no gaps or holes in between holes okay so a function which you can draw without without tick for example I can draw something like this so with it has no gaps okay or end holes in between okay so this is a this this is a this is a continuous function okay so we'll see some of the examples where where the functions are continuous and where the functions are not continuous so let's see some of the examples where the functions are continuous where the functions are continuous so just going to make it something this oh my God it's not correct so let's assume oh my God what is this what is this I think that I'm losing my mind I think so yeah so over here this this this this is a function this is function what I let's let's say p of X let's say p of X and this is continuous because you are drawing without taking out your hand and since one of one of the main thing about continuous function if you are making it making it draw without taking your pencil off the paper then it's continuous that's that's a that's another cool definition which is provided in some of the book okay so this this is some of that this is what you this is a continuous function let's draw let's draw uh something like this something like this it is also the continuous function so this is also the continuous function of the absolute absolute value of x okay so this is also continuous function but what are not continuous functions so let's take an example let's take an example of this function of this function the function states you have uh maybe something like this you have something like oh my God you have something you have something like this and let's do one thing let's just make a hole in between okay let's just make a hole and this is and this is not a continuous function okay another stuff is I will tell you the the good definition another another can be another can be uh let's take example of this okay this is uh I'm going to make a piecewise function I'm going to just make a pie wise function and this function is also not continuous function okay this this this function is also not a continuous function let draw vertical ASM tootes so this is also not a uh the the the the function which is continuous okay because we are you're taking off your hand and another stuff which I'm going to talk about is another pie wise function another pie wise function which is this one okay make whole there and let's make there's a hole over here so this is also not a continuous function okay so uh so when Whenever there there is some kind of piece the constraints it's it's it's difficult so uh basically you you're seeing the two functions so let me draw another function another same function which you all had already seen so that I could take one example of this uh something like this one and something like this one okay so these are the two functions which are not continuous so these two functions with the gaps are not not continuous everywhere these functions are not not I not say but the more precisely I would say the these two functions with the gaps are not continuous everywhere these two functions are continuous everywhere but these two functions are not continuous everywhere but sometimes a function is continuous everywhere it's defined okay such a function is described as a being continuous over its entire domain so if the function is continuous that that means that the function is continuous over its entire domain okay which means that it it's it's it's it's gaps or gaps occur at X values where the function is undefined okay so which you're seeing the function P of X is continuous over the entire domain is continuous over the entire domain which you're seeing over here it's continuous over the entire domain it has it don't have Gap in between but over here G of X on the other hand it's not continuous over its entire domain over its entire domain okay so G of X is not continuous over its entire domain maybe let's say for say for example you have this diagram so this this diagram so this function y is continuous over it in in entire domain except at Value except at xal to 2 because at xal 2 it has a whole so the function y is not continuous everywhere except x = 2 okay so this is how you say if if the function is a continuous or not maybe you say the it's you you say say that the function is continuous over its entire domain but basically as a more precise would say that okay the function is continuous everywhere except at this point okay so this was all about this uh continuity and I hope that you got to know what limits it's it's it's it's a good definition to to to to just take in your mind and and I hope that you will consider uh this uh in your own and I also hope that this limits got in your mind in the next section we'll talk about the evaluating limits and then we'll get on differentiation my favorite topic and then we'll talk about integration calcus over okay and then we'll go to probability Theory because it's probability theory is also very very important uh in this era and let me know that I'm that I was reading a book uh not what you want gift in a Christmas because I'm thinking that I should make a very very very comprehensive data science onee plan master plan or road map to complete and in in depth to get in Fang what do you say I'm not in Fang but I think many many of my students got into Fang uh so I think I'll be I'll be eligible I'll be help taking help from a lot of people to make that road map available okay so I think that if if you want that please comment it down below I'll be very happy to give you as a Christmas gift to make a full data science plan data scientist plan which goes from very scratch mass and this go go step by step with book recommendations with every stuff let me know in the comment box below I'll be catching up your next video till then byebye have a great day hey everyone welcome to this video um basically in this video we'll talk about how do we evaluate the limits because in the previous video I've just given you an introduction to limits we haven't solved as I've just shown you the numerically how uh sorry geograph maybe graphically how do we even uh go ahead and take out the limits with only few examp examples in this video what I what I will do is I will try to show you some of the methods for how we evaluate the limits we'll talk about substitution main the plugging one and then we'll talk about how do we evaluate limits using calculator and even if does not work there is algebra which is always there for you in maybe factoring evalu EV evaluation of a limits using factoring and rationalizing and conjugate and Sarah will talk about that today okay and I will just give you a a brief about squeeze theorem so that you could be you could be familiar with what is squeeze theorem is okay just a sandwich theorem cool and uh the frequency of the video will be bit low these days the reason why um uh cs001 is preparation is going on and there a lot of works on is on my head so it's bit slow but yeah I it's just like three videos per week is coming as men mention as promised but previously there was five videos Six videos were coming but I think it's my apology that the I'm not going to upload uh more than three videos per week okay and and the homework set says updated sorry the homework set will be updated on your LMS uh please go there and try to solve it I'll just uh have our T to to to to to grade your uh assignments Okay cool so uh plugging and using the calculator so let's talk about plugging let's let's just talk about plugging so limit of x approaches three okay and of a function of of a function x² 10 okay so what is telling this is your function f ofx so function f ofx is nothing but x² 10 okay so what is the limit when X approaches 3 okay so maybe just when when when you plot this fun function and when when you when you go when you approach X from the left hand side and when you approach X from the right hand side it seems to be approaching minus one okay but but it's not possible for every time for you to draw a graph and then go ahead so what you do you first of all try the substitution you first of all try the substitution so over here it seems to be approaching three and it is a it seems like a continuous function okay and it is a continuous function so over here when you what you do you put three into that X you put the three input to the F ofx so what do you do you simply put 3 over here okay so 3 S 10 which is equal to 1 so the limit of x approaches 3 of of this function is equals to minus1 it says that it's the the the function when X approaches 3 from the left hand side and the right hand side seems to be approaching minus1 okay or is approaching minus1 so what you do first of all is to try direct substitution you just give give the value of x to the function which is over three over here and then you take out the answer of it okay so this is called substitution method or plugging and it only works with a function which is continuous it only works with a function only works when your function is continuous otherwise it it works very well in most of the cases okay but but do don't you think that these this is just like a function no this is this is not like a function it is just a method which we use everywhere I will tell you how it is being used everywhere but every time it is not possible that this case happen let's take another example let's say an example that the you want to take the limit of the function which is 10 / x 5 when X approaches 5 when you try this plugging method over here when you try this plugging method over here what what will happen what will happen happen it it will simply this is this this is a function to 10 5 5 so you put x 5 that will be 10 /0 that is indeterminate form we say that is indeterminate form and this is undefined and the limit does not exist but the limit do exist in this case okay may maybe in some time we'll we see the tools that will say the limit that will that that will help help us to evaluate these kind of limits but over here this is undefined so that's why the substitution does not work everywhere does not work everywhere because it is giving you the indeterminant form if it is even a continuous Okay cool so now what we now this is the first method which you have seen let's see the second method which is of using the calculator so using calculator let's let's say for for the sake of example I'll take another example limit of X approaches the limit of x approaches 5 of the function X x² 25 / x 5 equals to what okay so this is this this is the thing so what I will do I'll make use of calculator I'll make use of calculator what you can do first of all is to put the values of five over here put the values of five so maybe 5 S 25 5 5 so 5 2 25 25 over 5 5 that will be 0/ 0 and when you divide something by 0 that is undefined that is undefined so the plugging method is not working over here so what method would work work over here so what you do you you take your calculator okay you start approaching you start putting the values of x from the left hand side and you start putting the values on the right hand side and see and make a table of it and see to whom it is approaching okay so say for an example this this is this this is your X this is your X this is your X and this is your y this is your X and this is your y so X maybe let's say 4999 uh 8 okay so 4998 8 that the when when you put this 4.9 and 8 when you when you go from the left hand side when you put this in function 4.9 and 8 that will be 99.998 okay now you put 4.99 9 okay that will be 99.999 okay and when you when you put five it gives it is undefined okay now you have seen from the left hand side it seems to be approaching what why is approaching what it seems to be approaching 10 so the limit of x approaches 5 from the neg from the left hand side of this function x 5 is equals to what 10 it seems to be approaching 10 from the left hand side okay but the but the definition of the limit what we have seen the limit I'll just draw it the limit the limit of x approaches a from the negative side of the function f of x should should be equals to limit of x approaches a from the positive side to be the limit called as so that the limit X approaches a is equals to whatever okay so if this is 10 this should be also 10 10 so that it could be called as a so that the limit from the both both both sided limit should be same okay so over here when we put five when when we put 5.01 5.1 okay so when when we go from the right hand side when we go from the right hand side it it is seems to approaching also 10 to 10.1 so when we put 5.2 from the right hand side I'm telling 10.2 so 5.2 it seems to be approaching 10.2 okay so basically the limit of x approaches five from the positive side of this function f ofx let's let's assume this function f ofx is also 10 is also 10 so as as for the definition of a limit the limit of a function f ofx when X approaches 5 from both the sides it seems to be approaching of the function f ofx it seems to be approaching 10 so the using the pluging our limit was undefined but using the calculator the limit we Define the limit okay the limit do exist okay so this this is this this is the second method using calculator but every time calculator does not works okay maybe uh the the advance calculator may may work but basically your computer calculator mobile calculator is not work May maybe some of the cases so that is using the calculator now let's see another thing another uh method for solving of it is using algebra but I I do want to see the timing of it so what's the timing is 9 minutes okay I just want to complete it very fast way so that I could not take mod of your time cool so what I'm going to do is to solve limit I just to solve limit solve limit using your basic algebra okay uh first of all what we'll do we'll try factoring we'll try factoring okay and then we'll do the rationalizing or conjugation okay so what's the limit what's the limit when X approaches five when X approaches 5 of the function x² 25 of x 5 that's the same function you know that the it should be 10 10 it should be 10 x 5 we have we have seen that now we'll we'll make use of factoring to uh evaluate this limit so so what so basically the limit what it tells when X approaches five when when in this function when X approaches five from the both side what it is approaching okay okay so that's what it is the Lim the the limit tells what happens when X approaches five from the right hand side and the left hand side and what will be the Y value of it okay so the first thing which you'll try is plugging the first thing always you have to try is plugging okay if now the plugging will be the indeterminate form or undefined okay so plugging is undefined plugging is undefined over here so first of all every time you you have to try the pluging first second what you have to do is to factor x² 25 x² 25 so it is basically x² 5² so a² B squ so what it will be so a squ b square will be uh A b + a a minus B and then a + b okay so that's the that that you have already studied so what I'm what I'm going to do the limit of a function X approaches 5 x² 25 / x 5 will be what the limit of x approaches 5 and that we have factored it out okay that is x 5 and x + 5 and in the denominator we have x 5 isn't it isn't it so what so what we will do we'll cancel out these two so what will what we have left with X limit of x + 5 now what you do now you put your x value over here now you use sub substitution limit of x approaches 5 of your function 5 + 5 so that will be 10 so limit of x approaches 5 is nothing but 10 so what you have done you factored it you do some algebra man manipulation and then you at last you plug in the now you left with something now you plug in and then you now you plug in or use the substitution method and then you are done with the the limit of this function will be 10 that exactly what you have seen before okay I hope that this is this is pretty much clear to you what I'm going to do just recap recapture Rec capsulate once you again is basically what we have done is to first try the plugging method first try the tried the plugging method second what we have done is to factor x² 25 and 25 is perfect square of 5 okay so x² 5 S okay and then using the a a b and a + b okay we can write something like this so that it could be cut in like this okay so the just and then we'll be left with this and you can plug in the values of X and then you are done with this okay that's that's that's that's pretty much easy this way for evaluation of the limits is what I'm going to do now is to take another example to Showcase you the rationalization stuff and uh so that it would be very easier for you at least so another example is what is the limit when X approaches 4 of this function fun otk x 2 by x 4 okay so this is your basically the the the the you have to EV evaluate the limit it's just simply telling when X approaches 4 from the both side what it is approaching on the Y AIS okay so first of all what what I have told to you what I have told to you is to try out is to try out uh a plugging method plugging so let's try out so so it will be nothing but 4 < TK 2 okay 4 4 whatever above comes 4 2 that is undefined that is undefined okay so plugging does not working at a first second thing what I've told to do the second thing what I've told to do is to multi uh now we have to think of something like this so limit of x approaches 4 x 2 X 4 what we can do so that uh this x 4 cuts off okay because this is what is causing the problem at the denominator so if if you know about the conjugate the please see the definition of conjugate simply changes the sign of it okay please see the conjugate over the Internet it's very one one minute definition you multiply with the conjugate of x x 2 you multiply the conjugate of x 2 so times x + 2 the conjugate of x 2 x + 2 x + + 2 okay now what now what you will do so you are left with x otk x 2 otk x + 2 so a b and a + b that will be what a s b s so otk x² b² okay and this will be left with x 4 and this otk x + 2 otk x + 2 now when you when you do this x 4 above will be x 4 x 4 < TK x + 2 otk x + 2 okay I think I'm correct so this is cutting out so the the left will be root x + 2 root x + 2 now this is your now what you do you put the value of uh four over here okay so 1 / < TK4 + 2 so that will be nothing but 1 / 2 + 2 that's 1X 4 is the limit the limit when X approaches 4 of this function of this function x 2 x 4 is nothing but equals to 1 1X 4 that is your answer okay so you have tried plugging mainly the substitution the second thing which you have done is to um this the second thing which you have done is simply uh uh factoring main not not exactly factoring multiply the conjugate so that the so that what is causing the problem will will be eliminated and then you put put in the value then again you substitution to take out 1x4 is the answer of this EV limit okay I hope it's pretty much clear to you at least now what I'm going to do is to talk about nothing but a squeeze theorem I think that I'm very bad at this means a drawing squeeze theorems but I'll fully fully try to just help you understand what this exuse theorem tell you I'm just going to not spend so much time on it but yeah it's it's good theorem so basically this this is also called a sand theorem sand Wich theorem okay so say you have a function f g and H so let's draw a function FG and H so let's draw a function f g oh my God this is this is not f and g Okay so this is your function so let's draw some something like this oh what the hell this is just want to take simply this this one okay so this is your thing and what I'm going to do is to add wait for a second okay and this is your another function okay so this is your I think I think that this is your F and this is your G and this is your H okay so I hope that this is a squeeze theorem something like this so I I can just explain you this so you sech search online for the exact picture so that the so that it would be better for you at least so you have a function you have three functions f g and H where G is sandwiched below your function f and H okay so basically this G is stand pitched below between your function f and H and you can see and you can see the when this let's let's let's assume this is one this is two okay and this is also one 2 and this is three okay so what is the limit of x x approaches 2 of a function f of x of a function f ofx okay what is the when when when it goes from this side and this side it seems to be approaching three it seems to be approaching three okay okay so this seems to be approaching three but it seems to be approaching three what is the limit of H of X when x x x approaches 2 it it is also three because when you because H is lower than the your F okay but when when we see see when when when when we go from right hand side and left left hand side it seems to be also approaching three so the limit of x approaches 2 of a function f ofx is equals to the Limit of X approaches to of your h of X is equal to the Limit and if if they both are equal then the that is also the limit when X approaches 2 of your G of X is also equals to 3 so these three will be equals to three okay so this is what the scuse theorem tells you the rigorous proof of this theorem can be seen on the internet if if you want to dive in okay so the basically what it's telling the limits of these three theorems will be the same okay don't don't worry about this it's it's it's it's kind of uh not a big deal just a basic example to help you understand when X approaches to from the left hand side and both right right hand side in all the three functions cool so we have if you haven't understood uh understood squeeze theorem don't try to understand it's not even on your deep deep learning syllabus okay so we have seen this uh very precisely and I also hope that you understood it as well uh in the next video we'll start off with the differentiation I will try to complete differentiation as much as I can I'll will try to there is three to four videos on differentiation I just want to talk in in a very cool way so that everyone could understand what the differentiation is and it's it's nothing but the change mathematics of change when one changes just a fancy slope we'll talk about that in detail I hope you will understand it I'll be catching up your next video till then byebye have a good day hey everyone welcome to this lecture uh in this lecture we'll talk about differentiation we I think we'll just uh get get get some idea we'll reach till the formal definition of derivative we we'll try to Define derivative with which is uh using the different coecient uh definition so we'll try to Define derivative we'll try to take out the derivative of a function which you which you can think of something like 1/ 4x2 we we will derive these kind of function f ofx = to or we'll derive the function like f ofx = x² you will get the tools after this video where you will be able to derive these kind of uh the the functions which are not too much uh funky functions something like this okay so so it is important as per the AI and ml Ai and ml uh if you are just want to get an idea about how differentiation works and how do we take out the derivative or you to do your back propagation because in deep learning we are going to do everything from a scratch maybe deriving the back propagation U um using uh derivatives and we we'll try to derive the we'll try to take out the derivative of the function when doing the back propagation and then we'll do the lot more stuffs from very scratch okay so so that's why I want you all of you to just take a very good attention at this in the next couple of uh very videos as well as because in this video I'll give you a geometric intuition as well as the intuition of the definition of differentiation the next video we'll see the differentiation rules like quotient rule power rule chain Rule and a lot more we'll also see the vector Cal calculus after we complete differentiation and integration because uh I'm just going to cover up integration as well it's the only reason because in probability and Theory you probability Theory and statistics you in probability density function CDF thetic require you to have a knowledge of integration and as well as some some of the deep learning research papers it is not too much found integration in deep learning but yeah it is uh in behind of the the con concepts of PDF CDF which you usually do okay so we had a lot more talk now let's get started with uh with with the defining with journey of defining differentiation and I hope that you will be able to understand but before that what I want you to do is just recall the slope and then using the slope I just if you if if you have seen my previous videos I told you differentiation uh is is is the process of taking on the derivatives and derivatives are nothing but the slope of a curve okay or slope of a function so let me just take one example let me take one example example States oh my God yeah example States example states that you have uh for example example you have this line okay so let me just draw the line you have this straight linear um straight line okay uh this is this this is on your graph paper where it where it goes on xaxis one unit and it goes above 1 and half unit okay and it does it is the linear so it is same at everywhere so one 1 / two etc etc so you pick any two point you pick any two point and you see okay it is running towards uh it it is running one unit and is going above Rising one 1 and A2 units okay so it is uh running one unit and Rising 1 and a half unit so slope formula is nothing but the rise overrun rise over run okay or Y2 y1 and X2 X1 we are seeing that is running one unit and is going Rising it is running one unit and it's rising 1 and/ half unit okay so it is Rising 1 and/ half unit over 1 okay so the final slope the final slope which you will get which will be 1/ half okay that is the slope of this function let's give this function name as G of X okay so the the slope of this function is g 1/ 2 and simply means how much y changes how much y changes when X changes you can think something like this as X is changing one unit X is going one unit and Y is move U rising up 1 and half unit okay when X changes okay so that's that t you can think of slope or the steepness of your line that's also the that's that's also you can understand by the slope okay the steepness of your line okay so uh this is your slope now coming to the calculus term what is the derivative what is the derivative of this function G ofx what is the derivative of this function G of X with respect to x with respect to X that will be nothing but 1 / 2 that that will be nothing but 1 / two why it is one 1 / two is over you because the derivative as as as I told derivative is nothing but the slope nothing but the slope that how much how much this G of X or Y changes how much this y changes when X changes how much this y changes when X changes okay so that is nothing but a slope but the fancy term in calculus we we we give which is derivative of course there's a lot more difference between the your regular slope and the derivative which we'll see later on but as of now what you can see that the derivative of this function G of X is nothing but 1/ 2 2 is nothing but 1/ two and how this 1/ two came is we are seeing that okay it is uh it is a linear line so derivative is nothing but a slope but the slope is rise over run and we are going one one at the this at running towards one and Rising 1 and half okay so the derivative of this function G of X with respect to X how much X Change or how much y changes or it is just equivalent to y 2 y1 over X2 X1 you can think of like that because this is a slow formula okay so that is uh that is the derivative of this function is nothing but 1/ two and this derivative is nothing but telling exactly the same how much this function or G of Y changes how much this y changes when X changes a little bit okay so you can think of like that okay so now I I'll take another example so you are more comfortable with okay so you have a l line you have a line let let me just draw a straight line something like this okay you you're moving towards you are running one one unit let's assume you running one unit okay and you're running one unit and you're Rising three units you're Rising three units okay so you're running to one one unit you're running one unit and you're rising and you're Rising Three three units Rising three units so what is the slope of this what is the slope of this um function so the slope of this graph or the function is uh maybe 3 over 1 3 over 1 which will be nothing but three okay what is the slope slope is three okay what is the derivative what is the derivative and derivative of this linear we we don't require derivative over here but what is the derivative so derivative Dy by DX Dy by DX which is which is just tells how much y changing when X is changing X is changing one it is going rise it is going one and it it it it is Rising one then Y is rising three okay so that is how much y changes when X changes okay so that that will be nothing but uh uh 3 over 1 okay 3 over 1 y changes 3 when when X goes 1 then y goes up three when X goes x x run one then y run Rises three okay so that the derivative will be also three so we write this is this is a formal this is a formal way we write Dy by DX Dy by DX okay so derivative of this function y with respect to X derivative of y with respect to that point x okay with respect to X so for example if you want want to take out derivative of this point derivative of this point so you'll get the same thing you're going one at this side and you're moving up okay so I'm I'm not taking the exact stuff but the but in the the function you'll be going so this is uh this is a point okay so that will be three okay so the slope of the slope of the linear lines will be always constant so any point you go on X for example if I go any point on this the slope will be same it goes the slope will be same of course the slope will be same the derivative at this point for say X1 the derivative of y with respect to X1 will be also maybe it goes one unit and 1 one that will be one okay so it is it is writing Dy by DX which is the derivative of this function y we we we we can we can name the function but you know the function for example we can remove this F ofx we can just write y okay so the derivative of this function with respect to X okay and X can be Point okay over here okay so this is so this is a formal way of representing Dy by DX another way to represent U derivative uh it it is read as Dy by Dy DX okay Dy DX which is nothing but uh uh uh derivative of the function with respect to that X okay another way we we can write in various ways we can write the derivative of function in various ways let me make you familiar with the ways as well okay so uh so we can write let's let's assume we have a sum function f ofx okay which is let's assume uh any function okay any function let's let's assume F ofx so when you want to take out the derivative derivative of the function f ofx with respect to X you you can just write derivative Dy DX d y DX so sorry Dy DX okay the derivative of function ex it is equivalently equals to this it is we we can also write F Prime X or fx that exactly that tells you the derivative of this function derivative of the function f with respect to X it is ALS we can also write this as a Y dash because Y is a function and we say what is the derivative of the function y with respect to X or we can write f F like like this or we can write y this there are lot more techniques we can write d x f something like this so they all are equivalent but we'll follow the formal notation which is this one okay Dy by DX which is nothing but how much y changes when X changes and that is nothing but the slope that is nothing but a slope but don't worry I'll give you the formal definition of a limit in in just a second when be try to Define differentiation with the help of a rate okay so we will try we'll come on that don't worry okay so I'll just give you the formal definition of a limit so that you could uh you could just Define limit what it is but as of now how much y changes when X changes okay so there so there are tons of not exactly tons of ways there are so many of ways for writing the derivative notation okay as you know the peoples are very smart on the earth they they just invent these kind of stuff I don't know why they haven't stick with that Dy by DX because I like that notation I don't know what the researcher have thought of maybe ISAC Newton has thought of this one and then a lot of researchers that thought this one and they all fight it together to get the work done but I don't know what they have done I have to go at the past and then ask them okay cool so we have seen the derivative of a line and I hope that you understood it as well so uh one thing which I Let's uh do we want to go ahead yeah let's go ahead let's let's let's talk about one similar example so that everything u i I don't think we should go ahead and uh see one more simple example because we have already seen it now we have seen the derivative of a line now we have seen the derivative of a line which is nothing but a slope of the line okay so you may think hey I Ed Dera is that simple no it's not that simple we'll we'll see uh we'll see but yeah it it it it it is not it is doable if you uh keep your good mind concentrated over here okay cool so one of the example which I want to highlight is from calculus for dummy's book okay example is from calculus for dumy book you can check out this book it's amazing book which I ever seen on calculus for beginners is is what I use for teaching as a reference uh there there there's an example where there there are two students there are two students let's Assume My Name aush let's assume uh let's assume there are two students aush and let's assume there second student which is Risha okay which is Risha uh maybe R is my best friend over the school but these are the two uh students these are two friends okay whiches around I think they are they are they are um I I don't know the the name of this seesaw yeah that is seesaw when when one goes down then up for example this one okay when um one is sitting above on the bench one goes down and when uh this goes down this goes up okay let see so I I don't know how what what what the name of the game is but that is the game okay so here it is telling that the IOU this aush aush is the the weight of aush is twice as as much as Laurel okay as sorry as much as Risha okay so iush weight is a twice as much as rishab okay so the rishab age is 20 sorry weight is 20 then aush weight is 14 okay so if the rer age is X then iush weight is 2x okay that is twice of Russia now they are they are on they are on seesaw they are on seesaw and this is something like like this this is a ground this is a ground and uh uh rishab is sitting here and aush is sitting here okay and a a is sitting here and rishab this is rishab and this is aush and of course I just changed the name so that uh uh it it it is not uh too much so so so that it would be understandable uh to to you all because I find those names very hard to pronounce and you can more connect with the Indian names if you're an Indian okay so there are two students rishab and Laurel where aush weits is twice as much as rishab now rishab has to has to sit twice as much closer so they have to sit as much closer to uh rishia okay for them to balance up okay because it is to much heavy is very hard to balance okay uh and for every inch for every inch aush goes down for every inch aush goes down rishab goes up by rishab goes up by 2 in okay so when uh when the aush goes down by 1 in okay then rishab goes up by 2 in okay so if the aush goes down by 10 in the Risha will go over by 20 in okay so aush weights of course higher so that's so so just listen my thing is when aush goes down by 10 Ines then rishab goes up by 20 in when a goes down by 1 in then rishab goes up by 2 in down up down up something like this okay so when aush when aush goes down by down by 1 in then the rishab goes of above by 2 in so it is so rishab mov moves twice as much as a so Ria moves twice as much as a us okay so then you got this is this that's a derivative you got the derivative so so we'll see it you got the derivative and you may recall this is nothing but your favorite rate okay we'll see it so uh rishab moves twice as much as hardi uh twice as much as aush I'm just taking the book books example because I have written my notes over there uh that the the name of the hard over there but uh basically uh rishia moves twice as much as a reserve moves twice as much as a so uh Dr okay so I didn't denote Ria with r is equals to the D 2 D A okay so the r goes R moves twice up as um uh aush okay so with the calculus symol you can write it something like this and when you uh when when you divide both side by da or you move it over here Dr by da is equal to 2 and that's nothing but your favorite derivative which you got that's nothing but the favorite derivative which you got and over here this derivative this this your your your your your basically this what what what you're seeing is the derivative of R okay so the derivative of your R okay derivative of rer r with respect to ause derivative of rer r with respect to ause so Dr can be thought of as a change in uh r position as dhda can be thought of as a change in a position okay so if hardi goes down is if aush goes down by 1 in then rishab goes up by 2 in so Dr can be thought as change in rishop position and D can be thought of as a change in a position okay so that this is this is what you got when you the calculation this is what you got as uh this is what you call it as a derivative and this that D DN by DH it's just telling derivative of r with respect to a okay derivative of rishab with respect to aush so what does it mean that rishab is moving two times as much as aush rishab rishab moves two times as much as aush okay that's the that's exactly what you're telling that this is uh this is dy by DX which is derivative of r with respect to a is nothing but telling the rishab moves two times as much as Hardy aush okay so that's that's how you got the derivative you can take a look this is this is the derivative which we taken out from the from the uh Ria point of view okay because R was moving up but over here but over here uh you you can take out the derivative from the aush point of view from the aush point of view okay so so the so the da a the derivative da can be thought of as a change in a i position is equals to 1 / to DH when aush it is just telling it is just telling uh aush moves half as much as uh this guy Rish Okay so aush moves half as much as uh risham okay so da equal to Dr R Dr so when when do the calculation da by Dr is equal to2 and that's your derivative which you got which which you got and that is nothing but the derivative of aush with respect to rishab and it simply means that a moves 1 and2 inch for every inch Laurel uh that rishia moves okay so so so basically what is doing is uh it is the derivative saying that uh aush is moving 1 and half when I aush is moving moves 1 and2 in for every inch Ria okay so how much as as I told how the change in position so how much a changes when R changes okay okay so a um when a a a is changing 1 and half when R is changing so when aayush moves 1 and/ 12 in when uh rishab moves 1 in okay that's that's pretty much clear I I I hope so uh it is if if you're getting confused please recall the video again so that you could be easily understandable it is very very easy to understand again I'm recalling that uh this is d a by Dr da by Dr is is the derivative from the aush point of view so it is just sing how much iuse changes when R changes when rup changes and it's how much rup changes when iuse changes okay so this these are two from two point of views so it is just in this one how much how much I use changes how much I use changes how much I use changes how much I use changes when how much I use changes oh my God how much Rish up changes when iuse changes and how much iuse changes how much iuse changes when Rish up changes okay so this is from different point of view and we taken out the derivative from the same okay so I hope that everything is clear I'm not going to talk about too much on that because we have already taken a lot of time on it and I hope that is very very understandable to you as well coming to different thing I I I think I'll just I'll just give you a proper definition of uh not I'll not able to give you the proper definition because it is already let me the timing that this one HS up so just see the timing it's 24 okay I can continue it I think I can continue it yeah so uh let's talk about let me give you the formal definition let me give you the formal definition of uh just just just just a English definition the English definition which you which the example you saw this example you saw is the derivative so as I already told you thousand times a derivative a derivative is simply is simply a measure a measure of how much how much one thing changes compared to another compared to another okay so d a by Dr saying how much a use changes compared to Russia okay that's exactly called the derivative cool so another we can another we we can think of Let's uh uh uh another another example can be uh when when a person driving at a constant speed of 60 M hour for for example there there is a car I don't know how to draw a car oh I I got to know about the car okay so this is a car this this is a car and this is going with a constant speed of 60 M hour 60 m per hour okay 6 6 60 M hour and it's driving at a 60 M hour so uh what is the derivative derivative of p with the D DP by DT so is telling how much how much how much position changes when time changes if there's 2 hour then where's the car is okay so that we we can think of it as a derivative as well okay so we had said it we had talked a lot about linear only and I hope that you understood that understood it as well okay so we have only talked about linear over here it is going to the constant speed it is just going at a constant speed and we have we have talked about only the straight lines we we are not talk about the the the the the derivative of a curve okay so the derivative of a curve where the slopes are constantly changing at every point where the slopes are constantly where the slopes are constantly changes so this is a parabola where the slope are constantly changing at each and every point so let's say you want to take out the slope at this point C how you going to take out let's say you want to take out a point F how how you going to take out say you H how you going to take out so there are so we so we'll see today about this okay so uh what I'm going to do what I'm going to do is just just tell you what's the derivative okay just going to tell you what's the derivative but how I take out how I take out I will talk about in the next videoi or in the in in the next video where we'll learn about the difference coecient or I'll try to cover the different coecient in this video only okay let's let's let's let's try to cover the different coecient in the next in this video only say you have a function f ofx = to 14x squ so the diagram for this will be like uh will be like uh something like this I I'm just going to make it something like this so that's a um a graph of this function that's a graph of this function that's a graph of the function I hope so that's a graph of function so you have an X you have an Y and the derivative of d by DX so derivative so say for say say for an example say for an example you want to take out the derivative of this point and this x is 2 and Y is what Y is what uh let's assume uh or X is one so let's assume X is 1 and y is0 0.25 okay so that is derivative 0.25 D D1 okay so what it will be what it will be it will be I'll just take uh what is what is the slope at this point uh that is we we can use the derivative to take out the slope at at at at that point uh you you I already explained you how we will we'll see how it how how how it does but later on but over here the derivative of this function is nothing but 1 / 2x you can plug in the values of X you can plug in the values of X say for an example that I plugged in uh I plugged in 1/ 2 1 / 2 * or x/ 2 * uh I plug in two okay so it will be one and the derivative and the derivative at this point one at this point one and it will be one it will be what it will be one the slope at this point the slope at this point the slope at this point D the slope at this point will be one okay you have the derivative and you can put any point you can put in any x value you will get the slope at that point okay because we make use of derivative to take over the slope at at exactly that point okay so you can make use of you can make use of 1x 2x we'll come to that how we evaluated 1X 2X and the later videos but in differentiation rules thought I will cover in this video only uh so you can this is how you take out the Dera we have we have saw using the derivative you can put in any value of x after taking out the derivative of function which is how much y changes when X changes okay so which is one 1 1/ 2X and X which the value which you have to input in to get the slope at that point or to get the derivative at that point or to get the rate at that point okay or you you all know okay so we had this we we we'll see how do we how do we got 1/ 2x but before that let's talk about the difference coent let's let's talk about the difference coecient because it is the most important concept which one need to know okay because most of the most of the things in calculus is based on it's just because it is it is needed because uh I have think I I I think uh if it it just tells you how it gets infinitely closer to make the line from curve to a straight line okay so we so we'll see today one okay let me find uh the copy which I want so that I can at least go ahead and I'll have help you out okay because I usually make notes before teaching in video because I can just hope that I don't make any uh mistakes in the videos see say for say for an example you have a graph something like this okay you have a graph something like this okay let me just uh draw this one graph at this point yeah that's good okay and you pick two points and you pick two points let's say this one X and this one okay so you pick the two points and you assume that this is a point x there on the x coordinate is X and there's a h distance between this point so this is x + H so this is H so for example this was 2 and the distance from this to this is uh h = 2 then over here it will be four okay so you can assume like this so H so H depends on H okay so uh the distance is H okay the distance is H so this is a point this the x coordinate is is x and y coordinate so and another point so this is X1 one coordinate and this is the second coordinate x coordinate okay so what is the coordinates of this point it will be X which is on X and this is your f of x function okay so this is f ofx f of x okay and what is the coordinate of this point the coordinate of this point will be X+ H is your X is the point is xais and that will be nothing but f of x + h f of x + H because we want this is also the distance of H okay so f of x plus h uh which you're seeing over here okay now we we got the we got the coordinate of this we got the coordinate of this so the coordinate of XA f of x and there H distance where this there on X X Plus H okay and this one is X the the coordinate of the xaxis and this is of Y AIS okay so I hope so it is making sense try if if it is if it is getting confused try to think on yourself is if you if you move this X Plus H and if you if you try to put put in that function f ofx and we can add some value H so that's it that's that's that's pretty much the common sense uh this is a very common sense to understand X+ H okay now coming to this now coming to this what we can do we can draw a secant line we can draw the secant line so let me draw a secant line something like this let me draw a secant line something like this a secant line the definition of a secant line is a line which intersects two points two points on a which which intersect two points on a graph which intersects two points on a graph that's a secant line or a more formal definition of a secant line let me just have you the more formal definition of a secant line a secant line is a line with that intersects a curve at two points okay to intersect a curve at two points uh this is where we draw the seant line and the secant line is something this now now what you can do if you take if you take this point if you take this point if if you take this point oh my God I think I have to choose the pen yeah if you take this point okay you make it closer to this point okay so when you slide this point with your secant line you slide this point to over here with your second line okay so let's do do do the thing now you you'll be getting just make sure that you're following me just make sure you will be having something like oh my God the SEC L do does not work correctly over here let me make something touching over there okay now what you do you you now this this point has come to over here now what you do you you take this point you slide a little bit with the SEC line you slide a little bit om mg uh you slide this a little bit okay now let me just draw it okay you you slide with the secant Point as well you slide with the secant line as well okay you you now what you do you take this point again you slide it again you slide it again you slide it okay now let's slide it again uh it will be make sure that it intersect the both the point okay now again take this red one again take this red one slide a little little bit over here and let's continue doing that okay that will be intersecting at the two points okay at some point at some point it will be so let's let's draw the graph again again so what you're actually doing you are making this you are making this H you're making this H to be to to approach to to approach zero okay you're making this H we can make use of limit we can make as as we want to go uh closer and closer to not exactly X but we but what we can do we can make this H approach is zero when X when h approaches zero when H approaches zero it will be F ofx so oh my God uh when f of x okay f of x and this f of x plus h when this H becomes H approaches zero then then that will be so let me just make you familiar let me complete that thing okay so we will come to this will come to this okay so this was yours and this was your two points this was your two points now when you do this when you do this you will be left with something like this a secant line from us from sorry a tangent line a tangent or over here from the secant when you when when when when you take your H approaches zero when you when you just make your H when you may just make your P um make use of limit make use of limit when H approaches zero when it approaches zero okay so this point will go over here here here here here so at some point will become a tangent it will become a tangent and the derivative of any function of a function is the is is is the slope okay of a tangent the slope of the secant would be if if I draw a secant line the slope would be we go at this point so we go something this so rise of run so it is very easy so secant line becames the tangent line becames the tangent line and the tangent the slope of the tangent line is nothing but the slope uh the derivative the derivative the slope of this tangent line is nothing but the derivative so what you do so the so the slope of the secant line so let's let's start writing the formal definition let's start writing the for formal definition I think uh it would be very easy for me at least to draw it again okay so everyone is on same Pace OMG let me mix little bit up yeah so this is your one point and this is your second point Point okay let's draw a secant line over here okay and uh so what is what what will be the slope what will be the slope of this of this of the secant line so we have we were having our x coordinate okay and then we were having x + H and this was a distance of H okay and this was uh X comma f of x and this was nothing but X XA f of x + H okay so this is what you have so what so what would be the slope the slope of this would be Y2 y1 X2 X1 okay so f of x + H f of x ided by x + H x okay so this plus minus this cuts down okay so you'll be left with f of x + H f of x / H okay so uh now uh now you got the you got the slope of the secant line that is nothing but called the difference coecient difference coent okay quotient okay why I'm pronouncing it wrong so that is the slope this is the slope of your secant line so when you when you make your H approaches zero when you you make your hedge approaches zero so this will become a tangent line so it will go slide over here you slide it over here you slide over here so it will be something like this and then it will something like this and it some it it will somewhat become like this it will it will only touch uh okay it will become something like this this is the tangent line This is a tangent line so what how you can do this you can make use of limits to make your H approach zero because if the H gets if the f of x so it make in in other words you can you're sliding this point to this point and that will be equivalent the coordinate will be equivalent so you just want H H to be zero so H approaches zero not not exactly zero technically you can you can't get exactly zero f of x plus h f of x plus s f of x / H okay and that's the the derivative of your function with respect to X that's the derivative that's the definition of a derivative with respect to X okay so the seant line becomes the tangent line when you approach H to the zero so I hope that this this this this makes a complete complete sense okay so it just tells how much the function is changing when X is changing it it what it does so what it does again I'm again I'm telling it slides you can see the slides over here it started to become the tangent line so it started making it h h approaching zero okay you made use of limit to Define uh this uh this this this a derivative and the formal definition of a derivative let me just Define it for you let me just Define it for you let me just Define it for you um okay uh I think I have to find that definition of it yeah so the definition so the definition the definition is derivative of f ofx with respect to X is equals to limit when H approaches zero f of x + hus f ofx ided h okay that's the formal definition and what it does what it does this slope or this simply the shrinking we are shrinking rise over run we are shrinking the rise over run okay we we are making shrinking we are making of limit property we shrinking over here sliding this uh this this this coordinate this coordinate okay so we had a talk a lot and I hope that you are uh enjoying it as well even I am enjoying it okay but one thing the just one question which I want to solve is take out the derivative of the function f ofx which is 1 / 4x² which is 1 over 4X s we are going to take out the derivative now okay which you have seen we have taken the derivative okay one one 1/ 2X I will just show you how I taken out so this this is your function now let's go ahead now let's put in let's we have a definition now we can limit of H approaches zero limit of H approaches zero we can put this uh X this x is the coordinate which which he wants so we just going to give X and we can take this 1 over 4 as a coent okay because that that only wants 1 / 4 okay x + H and we don't have to leave a square because this is the one X we want okay minus your whole function 14 x² okay 14 x² cool divided by H divided by h okay so this is your now we are going to evaluate this limit so the limit of H appro H approaches zero 1 / 4x² + 2 x h / 4 + h² A + B whole Square you know 1/ 4² 1 / 4 x² so this one cuts out this one this one cuts out out this one so you are left with you are left with limit of edge approaches Z 2x + H over 4 plus uh maybe uh I I yeah I can write something H Square okay you can write the four over here as well you can write this four over here as well okay because when you when you when you multiply this oh my God why have left this when when you multiply 1 over4 with every member of this so that will be nothing but uh let me be little bit transparent let me be a little bit transparent let me be a little bit transparent then I think that I've done wrong a little bit I don't have to do wrong over here yeah let me be a little bit transparent so oh my God sorry for delays but 1 / 4 just expand in x² + 2x H + h² 1 / 4x² now you simply limit of H approaches Z 1 / 4x² + 2 x h by 4 + h² by 4 1 / 4x² this cuts out with this limit of H appro H approaches 0 1 and now it cuts down so 2x H 4 + h² 4 okay let's uh let's let's keep keep keep on working on it so that let's write the same thing again H approaches zero 2x H by 4 + h² by 4 by H I think I forgot in everything H we don't have to forget H okay by H so H is there don't forget okay now what I will do I will try to add it when H approaches 0 4 okay so that will be nothing but 2 x h + h² okay the limit of h h approach is zero we take common H 2x + H uh by 4 by H and this cuts out this and this cuts out okay so limit now it's very easy 2x + H by 4 now you can do the substitution you can do the substitution limit uh now we don't have to write limit 2x + 0 by 4 and that will be nothing but 2X by 4 and that will nothing but X by 2 or 1 / 2 x okay this was that much easy to calculate the derivative of a function okay using our formal definition of a limit and we showed you the geometric interpretation of the differentiation I and I hope you really enjoyed it try to take out the derivative of x² so just just just for your convenience the limit it it will be x + h² x² / H okay divid by H so let me see if it is exactly equals the same yeah it is divided by H okay so this now try to take out now this take out the derivative of a function f ofx which is x² and using the limit formal definition okay so I hope that this is pretty much clear to you at least uh I hope so it it is very much clear although you get a very very complex function complex function it is not a we we we we can't use our uh we have a different differentiation rules which will help us to do take out the derivative very quickly which we'll talk about in the next video till then byebye have a great day you have done a great job today meet you in the next video to this uh video uh in this lecture basically we'll talk about differentiation rules in our previous video we talked about uh differentiation which is approximately 40 minutes of video so sorry for that much long the only reason is uh I just completed the whole differentiation definition because in the previous video we given you the geometric intuition of differentiation and in this video we'll try we'll we'll see some of the rules for taking out the derivative of a function of even a complex functions we'll try to take out in the in the in this video and the next couple of videos so try to see that and then we'll see a local Minima and Maxima we'll try to see that uh and then future varibles and then we'll go on integration we'll complete the integration the same way as we completed differentiation however we'll not focus on that much on integration because it is not being used too much but but we'll surely take a look because sometimes it comes integration comes over the research paper then we'll go to the probabil probability Theory and a statistics and then we'll start with deep deep learning and you may be thinking hey why we are learning these much things and just I let let me tell you what happens if the beginners get very confused oh my God what is happening behind them behind the algorithms when they study deep learning so when we are going to start with deep learning then we are going to go very fast way okay um it's because you'll be understanding each and everything each and everything whatever I'm going to write it out and if you don't see these videos if you don't know what is calculus if you don't know what is uh uh what is differentiation integration you been not getting the actual point of view of deep learning and you'll be not understanding deep learning so that'sa we are doing all those things and it's very very useful to learn these as well for your professional career because it will maybe if if you want to go in research it will very uh you'll be able to understand hardest research papers in this era okay so let's get started with the uh differentiation rules I'm not going to recap the previous video but I want you to see the previous video uh which which which you have already had a talk on that but we have we had formulated a definition we have for formulated a definition of differentiation so let me just give you what what definition we have formulated if you want to take out the derivative if you want to take out the derivative Dy by DX and Y is a function okay so we we are solving for y so we if you to take out the derivative Dy by DX we were have we were having a limit definition okay this a using the difference coent when H approaches zero limit sorry f of x + H minus F ofx by H okay and this is this is the this is and You by solving this by using the this we are we are able to get it but when you have a too much complex functions it would very tedious and almost impossible for you to achieve using this okay so that's how we have differentiation rules which will help us to uh take out the derivative of the functions okay so let's get started so let's get at least get started with the today's video uh the the first rule which I'm going to talk about is the first rule which I'm going to talk about is uh the the the constant rule okay so every everyone should should have to should should be familiar with it it's a constant rule the first rule which I'm going to talk about about is the constant the constant rule so what this rule tells if you have a function let's say F ofx equal to 5 okay f ofx equals to 5 and and this and the derivative will be zero the derivative will be zero and the slope will be also zero because derivative is nothing but a slope and over here the it the the derivative the change the it is nothing is changing so derivative will be zero so the derivative the derivative of this function so basically we'll write y = 5 so derivative of f ofx with respect to X so we write f ofx is equals to Z okay for this function the derivative for this function f ofx is with respect to X is equals to zero nothing is changing okay nothing is changing uh just I'm to introduce to you the note notations just I want to introduce to to the notations we can write this in a short form as well F Prime x equals to Z so this is this this not notation this notation d uh F ofx uh by DX D of Dy by DX is equivalently equals to this equivalently equals to this okay so it is just telling fime X just a short form which will use interchangeably we we we'll use this uh consistently both the formats so that everything is crystal clear so we'll use both the things uh very frequently at the same time okay so that it saves the time and I like this notation much better but for the definitions of the con of the rules we'll use this notation so that everything is clear okay so we'll use that these two notations very frequently over this video so let's take some more examples so this was the one of one of the example the another example which I want to talk about is G of x = to 7 G of x = to 7 so what is the derivative what is the derivative of this function G of X of this function G of X with respect to X which is also equals to Z or F G Prime X is equals to Z okay so this is the constant rule which we had a talk on okay now let's go ahead now let's go ahead now let's go ahead let me just uh finish this uh this stylist now let's go ahead let's talk about the functions like let's take an example uh you want to you want to take out the derivative F ofx = to X the power 5 x to the power 5 you can use this limit definition to to do this but but there is a very quick way to take out the derivative of this function okay how how much X is changing when uh Y is changing changing okay so uh you see the definition in in my previous video because you'll be getting more bro broader view what this derivative is trying to do okay so we we we can use a rule called power rule okay there is this is a very very extensively used in other rules as well so please make sure that that you understand each and everything so f of x = to x to the power 5 okay so so let's take an example you take out the derivative of this function f ofx of this function f ofx with respect to X so how how will you do it so derivative of this function f ofx with respect to X which will be nothing but what you do you take this power you take this power and bring in front of the bring as a coefficient of x so you will take you'll take this power okay bring that in the front of the X okay and reduce that power uh by 1 okay so 5 1 so it will be nothing but 5 x ^ 4 okay 5 x X to the^ 4 so there are two things which you have done there are two things which you have done the first thing which you have done is to brought the power brought the power brought the power to the to the just as a coefficient of that X okay and reduce used the the the the power at that x value at that x value by one by one okay let's see some some more examples of it so that it would more make sense at least to you okay so let's take an example that you have a function again let's use the function name of f ofx which is equals to x² which is equals to x² okay so how you're going to take out the derivative of this function the derivative of the function derivative of the function so how you will take out so what we do we take this two bring in front of x 2 x 2 1 so it will be left with 2X and this is one so we we we can neglect it but this is your actual derivative this is your derivative of your uh x² okay so uh this is the D derivative of this function f ofx okay if if you use this limit definition it would take around 2 to 3 minutes or maybe 5 minutes for you at least to take out two to reach till 2x but in two steps or two seconds or even one seconds we we completed it okay so so so that's why these rules are very very important as well let's talk about one more example you have another example x^ 2 x the^ 2 so what will be the derivative ative of this function so the derivative of the function the derivative of this function which is D Dy by DX which is nothing but equals to you bring this minus 2 in front of it you bring this minus two in front of it okay uh now what do you do uh x the^ 2 2 1 so it will be nothing but 2 x 3 and this is the derivative of this function this is the derivative of this function okay bringing the power in the front of the X and then uh reducing That Power by one okay so that is called the the the the power rule okay the another uh rule which which is which is related to power rule which is nothing but the constant multiple rule the constant the constant multiple multiple rule okay the constant multiple rule so let's take an example that you have f of x okay you have you have a function you have a function you have a function let's name it a y because I I have to write f of x times and times so let's use y equal to 4X to the^ CU okay the X sorry 4X CU and you want to take out the derivative you want to take out the derivative of y Prime or Dy by DX the derivative of the function with respect to X okay so what you do what you do you simply take this scalar okay that multiple four times the d uh uh maybe uh the derivative of x CU okay derivative of x Cube okay so what you will do Dy by DX of uh this x Cube or wait for a second let me just make it more transparent at least so that it will be very very useful for you at least Dy DX xq okay so four * the derivative of x CU will be the derivative of x CU will be what you can use the power rule bring the three in front of it and reduce the power by two 3x² 3x2 it will be nothing but 12 X2 is the derivative is the derivative of the function y okay so Dy by DX which would be nothing but 12 x² let's take another let's take another example let's take another example Y is equals to uh 2x² okay what you what you do you take out the derivative of this and after you taken our derivative and then multiply with two okay so the derivative is equals to uh two * the derivative X squ will be what bring two in the front sub subtract which is 2x which is 4X is the derivative of this function okay so that's called the constant multiple rule are are you getting what I'm trying to say yeah so uh this is the two example let's see one last example so that it's it make completely sense to you uh 4X ^ 3 okay 4X to the power oh my my god I've already taken it uh 5 x to the power maybe 4 okay so so how do you take out the derivative of this so what you do five * the derivative of X4 will be bring three in the front sorry bring four in the front then subtract X subtract 4 1 which is 3 which is 20x cu okay so this is the derivative of this function the derivative of the function will be 20 x Cub okay so this is this this is called the constant multiple rule another rule which which are which which we are going to see which is called the sum rule okay which is which is again the useful rule uh is very fairly simple rule okay so let's see the sum rule so the sum rule states the sum rule states um let's say you have a function you have a function f ofx which is equals to x ^ 6 + x ^ 5 + x ^ 3 + x ^ 2 + x + 10 what is the derivative what is the derivative of this function or what is the derivative of this function what is the derivative of function so the derivative of the function will be so what is the derivative function so what you do you take out the derivative of every terms okay take out the derivative of every terms okay so what is the derivative of x CU sorry x^ 6 so we we can use the power rule so what is the derivative which will be bring 6 in the front Okay 6X to the power 5 plus bring five in the front 5 x 4 plus beinging three in the front 3x² plus bring two in the front 2x 1 plus it will be what it will be 1 okay so because um this is 1 1 which will be what uh zero okay so x x to the power Z which will be equals to one so one and this is called this this was the the the constant rule okay so so this the this what is the derivative of 10 which will be zero because the the the there is nothing is changing over there in that for that function okay now this is your actual derivative so we can simplify it 5x ^ 4 + 3x 2 + 2x and this is the derivative of this function okay so this is called the sum rule okay uh the same with diff uh the difference rule works okay so instead of plus there will be minuses okay instead of plus there there will be minuses the one example which I want you to work on is uh G of X which is equals to 2X x ^ 5 + 6 x ^ 8 + 10 x the power 1 okay try to work in it and take out the derivative if the function is differentiable take out the derivative of it okay maybe it it is differentiable maybe make it plus okay so you want to take the derivative of this function G of X Okay cool so uh try try to do this just pause this video and try to do this if you can go ahead otherwise let me try to do this for you so so um so let's do this so 2 * uh it is 4 uh X so sorry I just m it's it's my habit I don't know why 5 x ^ 4 + 6 * uh 8 x ^ 7 plus uh May over here it would be 10 * 1 okay okay 10 * 1 and over here the left will be the left will be uh 10 X4 + 68 48 X7 + 10 okay so this is the derivative of this function okay so try to work on several examples the problem set which will be released through your LMS okay you you can see from there uh the soon the the LMS will be updated with the prior information the certificates of the the chapter number one and the chapter number two will be given to you okay please see the elements for the same and the email if you have enroll elements because there there are only around 100 students which have enrolled I want more of the people to enroll at least 400 okay another uh now we have seen this the constant rule power rule multiple sum Rule and difference Rule now what I want you to do is to remember some the how do we differentiate trig values and it would very tedious start for me to at least showcase so what what I want I have I have seen a lot of people to me memorize this because this is very very handy okay so we are going to talk about differentiating differentiating just just don't memorize it you don't have to see how it came and how we derived it okay this is um most of the people just remember this means who who whoever is is uh good at this okay uh just you can uh what is the derivative of sinine of X with respect to X which is nothing but cosine of x okay the derivative of s of X with respect to X is cosine of x derivative of tangent of X with respect to X which is nothing but secant SEC SEC squ X okay uh another is uh derivative of SEC x with respect to X which is nothing but SEC X Tan x okay another is derivative of cosine of x with respect to x minus sin of x minus sin of X and another is derivative of cotangent of X with respect to X which is nothing but equals to minus cose Square X okay and the last one is derivative of cosecant of X with respect to derivative with respect to X which is nothing but minus the S cosecant x times the cent of X okay so the these are the six trig values which I want you to remember okay because it will be very very useful when we talk when when we talk for further rules and very very handy as well okay you can always view means if if you do the lot of practices you will it will be automatically memorized in your mind because I haven't seen the lecture notes for writing these even what whatever I'm teaching I'm not seeing my lecture notes because I practice a lot of it I remember everything okay so uh try to just practice it out it will very easy for you to at least understand it these are six streak values which is very very Handful in your Calculus Journey till now okay and this is taken from one of the most most famous book on calculus are calculus for dummies okay so now what I'm going to now what I'm going to do is we have seen the some of the diff how do some of the different some of the derivatives of the some of the trick values which is six Tri values uh there you can search for online if you want to see the steps how they have come up with and if you want me to do just feel free to comment it below maybe I can make a next video on it but again this is a toally optional one okay okay so now what I'm going to do is uh talk about the last thing differentiating exponential and logarithmic functions okay at least this is very very important as well how do we differentiate a logarithmic function and how to differentiate uh how to differentiate exponential functions and logarithmic functions and then we'll see uh in the next video we'll try to see uh the product rule and the quotient Rule and in the next and in and then in the next video we'll try to see in the next to next video we'll try to see the chain rule of differentiation we'll try to see how to use different different like products rule in chain rule how it is very very useful in that then we'll see the diff implicit differentiation we'll see the logarithmic differentiation which is very which is very very useful in in your Calculus Journey at least okay let's get started with uh exponential functions okay exponential functions let's get started with that uh exp differentiating exponential exponential uh functions exponential functions let's start with it uh so what is the derivative what is the derivative of e ^ x by sorry with respect to with respect to X which is itself okay so that the derivative of this e to the^ X is itself the derivative is is its own function okay so e to a to the power x okay if you want to see how it came I would just link some video some resources which I really really like to because if I again go ahead and teach you it will very very uh boring um and also it is out out of the bound so what I suggest you to see the link in the video description okay so the the derivative of any exponential function means the e e to the power x is equals to the itself okay but what if we have a function f ofx which is equals to 5 E power x 5 e the power x so what is the derivative what is the derivative of function what is the derivative of this function what is the derivative of this function what is the derivative of the function so the derivative of this function will be what you can do what you can do you can simply you can use the constant multiple rule over here okay you can use the constant multi multiple rule so here's how here's how I'm going to do it so Dy d by DX of 5 e x so what I'm going to do is to take that five outside to it 5 * d by DX e to the power x okay so which will be nothing but 5 * uh which would be uh which will be nothing but five times and it the the derivative of the E to ^ x is itself e to the^ X so that will be nothing but 5 e x and which yields the same thing okay so the derivative of any exponential function most of them which have ever seen it's its Alpha function okay so I hope that pretty much everything is clear and here we use the constant uh constant multiple rule multiple root okay uh to to to to take that five outside the derivative and then multiply with the derivative of e^ x cool so we have seen some exponential functions you can try out 6X 10 x whatever you you want to try try it man okay so we have seen exponential function now let's see uh now let's see another thing which is a logarithmic function which is Game Changer over here and I if you don't know about logarithms I'll link a video I link a I link a simple uh uh Nancy p video which I really like her videos because uh she's she she is an MIT graduate and she teaches very well these things she teaches very well and uh now she stopped making the videos but uh his her videos on calculus I know the it is it is bit a half incomplete but she she taught these things very well like logarithms some the rules like chain rule I even watched her video amazing videos by her so you can check out the channel I'll just give a link in the description okay so uh another thing which I'm going to talk about is lo logarithmic functions how do you differentiate the logarithmic functions okay so let's take an example you to differentiate the function uh which is f of X which is nothing but natural logarithm of X okay so how do you take out the derivative of this function which is natural logarithm of X okay so which is nothing but log base ex isn't it log base e so which will be nothing but um which which which which will be nothing but 1 /x so the derivative of the natural logarithm of X which is nothing but 1 /x okay uh we will see when the when your base is not when when when your base is something because what we do we tend to me memorize some of the differ derivatives okay and then we'll solve further things using that whatever we had memorized and it's not a big deal to remember four to five derivatives okay so this is your derivative of natural logarithm of divided with respect to X which will be nothing but One /x one/ X so if your if if the log base if the log base B is a number is a number other than e other than e if it is other than e you can tweak this derivative a little bit okay so let's take let's take let's for for for the sake of an example you have you have a function f of x make it f ofx equals to uh oh my God my hand is painting now so F ofx which is nothing but equals to uh what say uh what say uh log base 2x okay uh log base 2X and uh how we do it so what we do what we do we simply if you take the derivative of this of this function which is nothing but 1 /x okay which is your natural logarithm by okay ln2 natural logarithm of two okay so natural logarithm of two okay so when you do this 1 / X natural logarithm of 2 this is how you do it for if the log Bas is different let's take let's take another example if you want to take out the derivative of a function which is log base 10 x you you you also write as log X which is by default the the base is 10 okay so I'm going to take out the derivative of this Dy by DX X of this function log base 10 x which will be nothing but equals to first of all 1 /x you all agree 1 1 /x divided by natural logarithm of 10 and uh when you do this 1 /x Ln 10 okay and that is the derivative of this function so this is how you take out the the the the derivative of the logarithmic functions and I hope this is pretty pretty pretty much clear to you okay okay what if we try to cover this in this video our product Rule and quent Rule just wait for a second I have to see uh the timing which we have covered we can do yeah um yeah let's cover the product rule let's cover the quo rule okay so uh no no no let's let's get on to the next video to cover these two the product rule and the question rule uh it it would be better for us at least so that the video duration should be short okay so we have seen a lot in this video I'll be catching up in next uh so now let's get started with uh product rule because in the previous video we had a talk on this uh various basic differentiation rules Which is far more very very easy to understand and uh from this video the things will get started a little bit difficult but as a sum rule as the as the difference rule the same will be the product rule but the product rule is bit important and quent Rule is also bit important for solving the derivatives questions and as well as uh in a when and the the reason why I'm teaching these because this is this product rule and question rule is important in your chain Rule and implicit differentiation and chain rule is the the far most important thing which you have to cover uh for deep learning and then so so so so that's why I don't want to leave this product rule and the quti rule otherwise I would have definitely skipped but these two important rules which I can't forget the reason why you all know is uh these are my favorites uh because it is e easy to teach okay so assume that's what I can say to you that's what I can say to you assume that you have two functions oh my God my hand is painting again I don't know what happened to my hand and what do you think I should uh take some break or what no let's not take break let's continue working on otherwise tomorrow I will die and uh no one knows what will happen uh so you have a two function f of x and G of X which are differentiable okay which are differentiable means it is we can take out the derivative of this we'll see when the function is differentiable and when the function is not in our later videos or you can search online for the same but say you have function which is differentiable and and now if you take now if the function are in this format so you have H ofx which is f f of x time G of X okay so you have like a product of kind of thing for example you have a function y x Cub time sin of x s of X so how how how you going to take out so this is your f of x this is your f of x and this is your G of X okay so how do you take out the derivative of this so derivative the derivative of this kind of function Dy by DX of a function f ofx g of X which will be nothing which will be equals to F ofx times okay the derivative the derivative of G of X of G of X okay of G of X Plus G of X time the derivative of f ofx of f ofx okay so so this this is how you do it first of all um you you you leave the first thing and then you multiply with the derivative of the second function then you plus it then the you leave the second thing you multiply the first one okay it get you can write in this way because just to make sure that you will be not be confused I can write in different different ways I can write in different different ways is first of all I can take this derivative and then this one okay so for example d d by DX f of x okay uh times G of X leaving the second function alone plus then leaving the first function alone times taking of the second function changing the position which does not matter at all okay because you will see it that I will do it because that's my way of doing this everything interchangeably coming to the next notation we can write this out let's assume let's assume if let assume let assume u b f ofx and uh V I think V yeah let's assume V be G ofx let's let's assume V be G ofx so you can also do this but you you'll just replace u u f f of x with u and G of X with v okay another notation which which which I'm going to write it out which is very very uh important F ofx times G Prime X the so this we can write the derivative in this way so this this can be written this way plus G of x * F Prime X okay so this is the quotient rule U which we which is the definition of a quotient Rule now let's see some of the examples so that this is pretty pretty pretty much clear to you every every every one of you so y = x Cub * sin of X sin of X okay so what is dy by DX it will be nothing but so you take this x Cube so so X CU * d by DX sin of X okay plus uh you leave the you now now what do you do you leave the second one as it is times you take out the derivative of the first function first okay first f of x which is X the derivative of x CU which will be something like this coming to now now uh X CU times the derivative of s of X remember remember the derivative of s of X which is what equals to cosine of x cosine of x plus now you leave the S of X as it is times the derivative of x Cub so you bring three in the front of it x² okay now it will pretty much everything is clear everything will be clear which will be nothing but X Cub cosine X Plus sin of x uh okay so you can do something like this maybe bit more cleaner 3x² sin of X okay so this is the derivative this this is the derivative of your function okay so let me just make it a little bit more transparent so that everyone is AAL to so this is a derivative and this is the product rule from which we have achieved let's take another simple example so that uh it would very very very much clear to everyone uh 6 x² + 7 x CU so what is the derivative of this function Dy by DX so first of all you take out the first one and leave the second one so 6x^2 Prime okay so this Prime I'm saying this the derivative of this function 6 S Plus okay let's let's multiply we have to multiply 7 x Cub plus now you take the second second one prime uh times you leave out the first one and then when you when you do this uh you just simp 6 * 2x² which is nothing but uh 2x which is 12x uh * 7 X3 7 X3 okay * 7 X3 uh now you simply you can simplify further it and then you can do it and now you take out the Dera derivative of this and then you can simplify it further on which is very very easy for you at least okay uh try to do with and answer answer me in the comment box coming to the this this was a product rule now let's talk about another rule which is the I rule which is the quotient rule where you may you may have already been guessed your functions are being divided okay so let's do this let's let's let's let's do this so let's say you have uh you want to take out the derivative of a function like y = to sin of x / X to power 4 so how you how you're going to take up so we have something called as the uh uh quotient rule the quotient uh rule which which you can use to take out the derivative of these kind of function uh d by DX uh let me write the formal definition first because I do believe in formal definition so let's write a formal definition and then we'll come up with then then we'll Sol solve this example okay so what you will do uh so the derivative of this the quent rule state some something like this G of x time F Prime x f Prime x minus you leave the now you leave the f of x now you simply multiply with uh G Prime X okay and then divide it with GX s GX s okay leave G of X multiply with the derivative of f ofx minus f of x leave f of x and then multiply the derivative of G of X and then multiply with G of x² okay sorry divide by G ofx Square okay and this is the your quotient rule we can write it in this way format as well G of x * the derivative of um f ofx okay minus F ofx time Dy by d uh GX okay so you and then divided by G x² okay now you now you now using this you can take out the the the the the the the the the the derivative okay uh of these kind of functions so now now let's do one of one of the example now let's do one of one of the example let's do one of one of the example uh which is d by DX okay you going to take out of sin of X over X x the^ 4 okay so how you will do it how will you do it so uh first of all this is your F ofx and this is your G of X which is first of all you take out F ofx sin of X derivative of this times leave x ^ 4 minus uh s of X leave that times take out the derivative of this okay divided by x^ 4² great okay uh now what do you do the co the that the derivative that you all remember cosine of x time uh x ^ 4 sin of X so times the it is nothing but uh 4X ^ 3 okay and divided by x ^ 8 you keep doing this now let's simplify it bit a little little bit x to the power 4 cine of xus uh 4 x CU sin of x / x ^ 8 okay now you just keep on doing this H keep take as a factor x^ 3 okay and then x cosine of x for this minus uh 4 sin of X okay and take that as X to the^ 8 now you can simply use the EXP exponents you can simply uh use uh you can simply minus it out okay you can simply minus it out so it will be left with X cine of x 4 sin of X okay uh when you X which is will be which will be x^ 5 okay this is X and this is your final derivative after after you simplify everything this is your derivative either you can go with this but I suggest some simplify as much as you can so at least it is doable just just you can put in the values of X and then you are done okay so we are we are done with this quent rule as well as we are we are done with this period rule okay I hope that this is pretty much clear to everyone if you have any problem any doubt you can ask in our Discord server uh or you can see the student manual if if you don't know how to enroll in LMS just to update uh let me write in enroll in LMS so just enroll enroll in LMS everything is Free Your da support is free everything is free so you can go there enroll there and I hope that you will take this opportunity to uh learn deep learning from a scratch and learn as as compr as as comprehensive as you can okay so thanks for seeing this video I'll be catching up your next video talking about the chain Rule and implicit differentiation till then byebye have a great thing hey everyone in this lecture we are going to talk about chain rule of differentiation we'll try to know about more about it and it is very very very very very used in deep learning or whether we learn back propagation we'll we'll be we'll be using this this rule which is a chain rule to take out the derivative when we back propagate okay so I ask you to pay a special attention at this rule so basically so this rule tells if you have if you want to take the derivative of a function okay so let's assume Dy by DX it's nothing but the derivative the derivative derivative of inner function leaving the outside function and the derivative of the inside function okay so derivative of the outside function leaving the inside function time times the derivative of inside function but but it I know it does not make any sense to you so let's talk about uh uh wait wait for a second let me just delete it out so let's let's let's let's take one example and then we'll formulate this definition then we'll formulate this definition of chain R so the example States the example States let's say you have uh y = 3x + 1 to the^ 7 okay so this is your function this is your function which you want to take out the derivative so you can actually use a power rule and then you can use a lot of rules available but um it it will be very uh hectic for you to do it so we have a chain rule of differentiation which will help you to do this so what is the derivative of this function so Dy by DX or Dy by DX so using the chain rule what what we do first of all let's use the power rule okay so let's use the uh let's use the power rule to just make that seven into the front of 7 so what we are doing is taking out the derivative of the outside of function so the derivative of outside of a function okay we we are not touching inner one this is our inner one so we are taking the derivative of the outer one okay so outer one is some value okay seven so what we will do derivative of this will be we bring seven into the front of it whatever will be and this will be 6 7 1 using the power rule okay so leaving the inside function as it is leaving the inside function as it is and then using the power rule we have bring the seven over here and subtract one from there and this is the derivative of the outside function derivative derivative of outside function okay derivative of outside function okay and out outside function was uh we have some value and just to the power 7 okay that's the derivative now the time the derivative of inside function times the derivative of inside functions so what is the derivative of over here 3x the derivative of 3x will be what will be 3 plus we'll use a sum Rule and derivative of constant is simply zero so that will be 3 okay so the derivative will be U 7 * 3 21 3x + 1 to the^ 6 and this is the D derivative of your function which is this 3x + 1 the^ 7 again I'm recapitulating so that it could make more sense to you okay uh let's take an example another example let's say you have G of X which is nothing but um let's say 2x plus maybe 4 okay and we are squaring this up we are squaring this up so um so so now let's try to solve it so first of all what we what we will do we'll take out the derivative of the outside of function the derivative outside of function which will be you bring two in the front of it two leave this as it is subtract one from there add one that's fair enough times we have taken our derivative now we times 2x + 4 constant this is this is a constant so we this is a zero the derivative of this will be zero and this will be two okay time 2 so it will be 4 2x + 4 is the derivative of this function G of X okay so this is what the chain rule says now let's take another now let's take another example so that it could make more and more sense to you otherwise it would be very U hectic for you at least so let me take let me just remove it out let me take another example um so another another example is uh let's say you have f ofx X okay which is equals to sin 3 okay uh 5 x² 4x so what is the derivative of this function so the derivative of this function we can write in this way so the derivative of the function will be sin 5x^2 4x we can write in this way as well okay so we can write in this way okay because you all know that trigonometry is not just on this uh so it will be three over here now we can go ahead and taking out the outer derivative we can take out we can go ahead and take out the U out outside of this function so outside of the function will be so outside of a function will be uh this will be three we bring three in the front of it we bring three in the front of it s f of x² 4X the same inside we leave inside one okay subtract 3 from two as a power rule states times the derivative of inside of a function derivative of insert inside of a function so the derivative of of inside of function sin 5x² 4x okay take out the derivative of inside of a function okay so I hope that this is pretty much clear as of now now you can go ahead and take out the derivative of this inside of a function and you will be good to go okay and you will be good to go you just take out the derivative and this is your homework you all know how to take out so just try to take out and please give me in the comment box okay now another example you can just take out the derivative as s um is cosine you can actually use the product rule which you all know so go ahead and try it out coming to coming to the next example is let's say you have uh let's say you have um let's let me take another example you have a k of X or G of X let's say you have a function G of X where you have a function sine x² 3x so what you will do first for first thing is take out the derivative of outside of a function so the outside of function so the that is nothing but there is a sign of something okay so you then when when we take out the derivative outside of function so what is the derivative of a sign we have studied the cosine okay and leaving the inside as it is times the derivative of inside of a function okay so the x² the derivative x² is 2x 3 okay so the derivative so the derivative will be what the derivative will be cosine of x² 3x x * 2x 3 is the derivative uh G derivative of a function G of X okay so I hope that chain rule is giving you some of the definition or some of the uh some of the way to think about it how this and this works as you as you see these are composite functions okay so so that's why CH this chain rule works you can see online why this chain chain rule works because it is important to know about that another example which I want to put in front of you is here I I want you to work in this in this example you have H of X which is equal to X2 + 5x 5x 6 to the^ 9 so what you do you bring 9 into so first of all you take out the derivative outer of a function that will be 8 x^2 + 5x 6 to the power uh sorry this is 9 this is 8 okay uh now times the derivative of inside of a function so the derivative of inside of a function be 2x + 5 okay that is 2X and five and this constant is zero okay so 9 X2 + 5x 6 ^ 8 and 2x + 5 is the derivative of H ofx okay so now I hope that is pretty much clear so let's write write the formal definition let's write the formal definition let's write the formal definition of chain R so let's say you have a function if you have a function okay which is f g of X which is just like composite function okay and it is a comp composite function then then the derivative of y of of a function will be uh the derivative of will will be first of all we take out the derivative out Outer side okay okay I'm noting this is f Prime Times the derivative times the derivative of the inside of a function G Prime X okay so we are taking the derivative of outside of a function leaving the inside as it is multiplied by the derivative of inside of a function okay so over here this is of derivative derivative of outside leaving the inside and this is the derivative of inside of function okay inside of a function so we can we can we can write on different notation so let's WR let's write it out in different notation so the different notation will be let's say if if uh Y is equals to the F of U okay and U is equal to G of X so basically it is just saying F of of G of X okay so U is equals to this now we can write that out into the other formal notation which you mostly see which is not nothing but just wait wait wait for a second let me just drag it over here the derivative of function this Dy by DX will be Dy by du okay so so first of all take taking out the Dera of a function okay times du by DX okay so first of all taking out the outsider then taking out The Insider okay so this is what the formal definition States so I hope that that this is pretty pretty much clear with a lot of examples which you did now coming to the uh chain rule can be used with different different rules okay chain rule with product rule quent rule so that's why chain rule makes it easy to for for for for you to work on so let's let's go let's go ahead okay let's go ahead and and do one uh example which is example uh 4x² okay s of xq so in this example we'll make use a product rule product rule and chain rule to take out the derivative to chain rule take out the D derivative so just have to remind about the product rule if you to take out the derivative of the two functions which is being multiplied this one and this one okay when when when two functions being multiplied okay so which is f ofx f ofx time G of X so that will be the derivative will be first of all we take we we leave the first function as it is times the derivative of the second function which which is G of X plus the derivative of the first function the derivative of the first function times the leaving the second function as it is so this is the product rule which we have started now coming to this what if we let's let's name this function as F ofx now we want to take out the derivative of it so the derivative of it will be 5 Prime X which will be nothing but U first of all we'll by by going we can we can have this as a first and we can have this as a second so it does not it matter order does not matter first of all let's take out the derivative of the F ofx so let's assume that this is the G of X and this is your f of x okay so let's let's let's take out the 4X s okay so the derivative uh 4X s okay take out times leaving the leaving the other function as it is leaving the other function as it is okay plus taking all the derivative taking all the derivative so multiplying F ofx first of all 4x2 times uh the derivative s of X Cub okay derivative of the SEC second function okay uh second function now now let's let's take out the derivative so after we take out the derivative which will be nothing but 8 x sin xq so when we multiply so that derivative will be 8X and when we multiply with this so 8X sin x Cub plus now over here which which will get as uh the derivative of s of something is cosine of something but before that over here which you're seeing there is some chance of chain rule there is a some chance of chain rule which in this case which are seeing we'll use the chain rule in this case because we have outer function and we have an inner function and we're a bit confused because we are not seeing any power we we can't do easily with the power rule on anything okay so we'll leave 4X squ times what is the derivative of outside of function so we have a sign of something we have a sign of something so this is outside of function so what is the D derivative of outside of function which would be cosine leaving the inside function times the in the the derivative of inside function the X cube is 3x okay and we and using the chain rule we got the derivative now you're good to go now the derivative is which of this function 8X let me just use a different color so which will be 8X sin x Cub okay + 12 x to the^ 4 cosine of x when you simplify it you will be getting this x to the power 3 okay and this is your derivative of a function so use product rule and then in one of one of the functions or one of the part we use the chain rule to differentiate nice so we are we we are good to go with u chain rule of different differentiation and I hope that you uh that you got got to know about it much better and I hope you will be uh remembering these stuffs now let's talk about logarithmic differentiation the last thing which I'm talk about is logarithmic differentiation using logarithms you will be easily able to take out the uh to differentiate okay and and these are used to quickly take out the derivatives okay so the last thing which will talk about is logarithmic logarithmic logarithmic differentation okay so we want to take out the logarithmic differentiation now coming to this you have a function f ofx which will be equals to uh let's this this is this is the question we have to take out so X Cub 5 3x^ 4 + 10 4 x 2 1 2x ^ 5 5 x 2 10 we want to take out the derivative of this function and you may be thinking here you oh my God what the hell this is and you'll be also in oh like oh my God how how I'm going to approach this problem so let me tell you this is very very easy problem but uh we will use something called logarithms to solve this so what we are going to do now is to take out the logarithm we just take out the logarithm on both the side okay so take out the log logarithm logarithm of f ofx equals to logarithm just the same thing okay so let me just um snap it out how I'm going to snap it out no no no wait wait wait let me just do it for me at least so that I could not add to the current page over here okay so you take out the logarithm so you take out the logarithm both the sides so oh my God I think it's I have done wrong yeah so now you take out the logarithm of above the side now coming to this now coming to this now what you do you know the property of a logarithm if if you have a lot of fun um in this you you you know all the property of logarithm uh which will be logarithm of X Cube to the individual U terms inside it and to individual uh these things okay so times uh * logarithm 3x ^ 4 + 10 * logarithm 4X 2 1 * logarithm 2x 5 5x 10 okay now now when we uh when you you all know the when how when we take out the uh the the when we take out the derivative of a logarithm and you all have studied because I've already talked about this it it is nothing but when when you take out it it will be nothing but uh 1 by F ofx times F Prime X okay so we we can write it out F Prime X over f of x at this side and all and in this side what you do you simply one over 1 / x ^ 3 5 * * the derivative X x3 5 derivative so it will be nothing but 3x 2 3x 2 by x3 5 x3 5 okay now times do the same thing with this with when then we'll get 12 x CU 12 x Cub by 3x 4 3x^ 4 + 10 * uh 8X so it will be 8X by 4x^2 1 so you can recheck how how we got it it's simply 1 / 4X 2 1 * 4X 2 1 derivative so 8 there will be 8X and this is zero so that is 8x by so times uh the last one which will be 10 x ^ 4 10 x and we can write it out below things 2x the power 5 5 + uh sorry 10 okay so this is what you got when you take out the when you just take out the derivative of the natural logarithm okay basically uh you when you when you when when when you take out the derivative of a natural logarithm you will do something like this okay and it should be plus when you when you go ahead uh when you take out the natural logorithm of uh of the this this is a property you it will be becoming plus okay will becoming plus now then you differentiate it and then you get it now at last now at last let me just go ahead or let me just keep working on it now you will get F Prime X which is equals to the step number four which is equals to what you will do what you will do uh you will use something called as uh we we'll simply multiply with the logarithm and the actual thing this one okay and this one Whatever the differentiate we got it okay so that will be nothing but first of all we write whatever we got after taking the derivative of our logarithm six this one + 12 x Cub / 3x^ 4 + 10 + 8 x by 14 x 2 1 + 10 x^ 4 10 x / 2x^ 5 5 x^ 2 10 times big times the question x x ^ 3 5 3x^ 4 + 10 4 x^ 2 1 2X to^ 5 fx^ 2 10 and this is the your derivative of your large functions function which you saw over here and you may think yeah use this is this this is also a long process I agree this is a long process but if you go and solve with other process I assure you you will be getting you can use product rule to solve it but it will be very this is the very easy to solve this particular Problem by logarithmic difference iation so we talked about chain Rule and the most important thing was chain rule if if you haven't understood the logarithmic you can safely ignore this okay so I hope that you got the problem and I'll be catching up the next video with the introduction to integration and then we'll start off with probability and statistics thanks for watching this video I'll be catching up in next video till then byebye hey everyone from this lecture we are going to start off with deep learning we'll talk about we'll start off our journey with actual deep learning which you all are uh w from lots of time and I just want to make a disclaimer is I assume you that you have already covered the previous videos and some videos of probability uh which is not yet uploaded which will be uploaded in coming days but I assure that whatever the videos is uploaded like mathematical prerequisite please be sure that you complete those lectures first you can was in watch that in 2x lectures because all the things which you will study will heavily depend on the previous one okay so so please make sure that you have a better understanding of mathematical intuitions which I already taught if you faced any problem just join our Discord server we'll get or maybe we can get on a live with some Tas you can just discuss that and make your doubt clear and you can simply ask at M our Discord server which will be in the description box uh the link will be in the description box okay so uh what is deep learning so we'll try to answer in this lecture um so first of all I'll try to answer what is machine learning machine learning is an artificial intelligence domain where we extract patterns from the data and analyze the data and make intelligent predictions on the new data according to the pattern which we has already learned or our machine has already learned okay so we have a function f that that takes some input values and that map that input values to an output variable y okay so I'm talking about in the context of supervised learning so basically so basically you have a function f that that takes the input value X and that Maps the input value to Y so you want to construct a function so what so you learn a function f that that ex that extract parents from the data that extract parents from the data or loan learn from the data and analyze the data and make intelligent predictions so we can give X to this input value it will give us y variable for for for for for getting the desired y variable we do need a a very good function which actually does that specific job so exactly what machine learning tries to do is to make a function f that Maps the input value x to the output value y if you haven't already familiar with machine learning I'll ask you to take an introduction video or introdu int int introductory course of machine learning maybe you can go to ml1 or you can go to some other machine learning course but I highly S Suggest mll1 is the best course for you to consider at this point cool so um so you you got to know about the definition but there are different machine learning algorithms so you may ask that how you how we are going to extract patterns from the data and how we analyze the data so that is the first question which may come in your mind how we extract the patterns from the data and how we make intelligent predictions or how we do how do we analyze the data okay so so for making intelligent predictions first of all you need to extract patterns from the data so how are you going going to extract patterns from the data which we'll see uh in a review of machine learning in the course but there are different set of algorithms which will help you to extract patterns from the data or analyze the data some of the examples are logistic regression logistic regression logistic regression we have linear regression we have linear regression and I'm preparing I think a four to five hours of lecture four to five hours of lecture on linear regression is the mainly video titled as multivariate analysis you can go on New Era YouTube channel you can go on newa YouTube channel to maybe it will be uploaded till the end so maybe you can just go there you'll be seeing a title multivariate and regression analysis I have taught linear version in great detail there cool so uh so back to our topic so there are different set of algorithms which will help you to extract patterns from the data say for example legis ression and logistic regression is a classification algorithm classification algorithm we can use for classification tasks classification tasks and linear regression is a regression is a regression algorithm which will help you to perform regression tasks and many people just think about linear version is very simple algorithm uh it does not perform well but but but you but just go in statistics uh term and then you will see the power of linear regression mainly people forget to write to use Lear regression in a right way so you can watch the multivariate regression analysis which will be uploaded soon so um n base which is another yet powerful classification learning algorithm which can be used to predict whether an email is a spam or ham so using nbase you can construct a function f that takes an email that takes a email and classify that as a Spam or not non spam or non spam and let's denote spam with zero and let's denote non spam with one okay so you want to learn a function f that takes the input value X and that Maps either zero and one or or in other words we can say y be the member of zero and one okay so there is n base cool so the performance of these simple algorithms heavily depends on the representation of the data which you have okay you should have a good data to for your machine learning algorithm to form to perform well if you have a bad data your machine learning algorithm will not perform well but this ex this extract or this this this paragraph is taken from book written by Deep learning uh by Ian Goodfellow um I I only know the one one one auor name and others are I think U um I I don't know exact name but but y yosua Benjo so so you can just go there and see the book by Yan Goodfellow so this is an extract from that book and what is it indicates the performance of these simple algorithms depends heavily on the representation of the data which you have the representation of the data which you have or which you're given say for an example you're using linear regression to predict the house prices okay so so basically what exactly is trying to tell uh you you're using using Lear ression um to predict the house of the prizes so on time of prediction say uh so you train an algorithm you train an algorithm which takes size and as an input okay and uh and number of a fans of a house number of a fans of a house and then using these two features it predict your y variable y variable okay so this is how you train a function f that takes size and number of fans and then give you the pricee of the house so basically basically it is saying that you are using linear R to predict the house prices so now you trained it using these two features now when the time of prediction came user have to provide one size and the number of a fans okay so in let's take a bedroom ra rather than fans so number of bedrooms to make predictions okay but when you give number of a fans and then ask your model to predict the price your model will not be able to predict do you do you do you know the reason why it's very simple because you have trained your model on the size on two feature on based on two features and the number of a bedrooms and then you getting price what if if you give only the fans you'll be not able to predict it isn't it so that's the major problem of representation that that that that these performance of these machine learning algorithms depends heavily on the representation of the data which you have given and these are features so size is also a feature number of a fans is also a feature number of a bedrooms is also a feature so I hope that this is clear we'll we'll come to that we'll we are coming to definition of uh uh what machine learning sorry deep learning cool so the problem of feature representation so as I've taken one example to start the to start with a problem of feature representation or feature uh uh uh learning so we will talk about that AI task can be solved if we have right set of features and make a mapping from feature to desired output so all the AI tasks which is available today okay so say say say for an example that you wanted to predict the price of the house okay want to predict the price of the house you want to predict the price of the house so so so for for predict in this you should have the right set of features for this to get to particular uh answer so you want to learn a function f okay that correctly maps from some feature X1 X2 all the way around to the xn given these n features it predicts your y variable and these n features should be right set of features should be right set of feat if you have bad set of features where our AI model will not not able to perform very well okay so we do want the right sort of features so that uh we can train our model on these rights of features and then get the desired output one of example which I want to represent to the real world example say for example that you're learning some science concept where you have feature of the particular thing so uh say for an example I'll take a simple example so that uh it would be very easy um let's take a example in in real world concept um car okay so a car and what are the features of it what what what are the features of it so the features of it can be it should have four wheels it should have four wheels um it should be not too much big it it we have we have steering we have a steering we have seat belts you have seat belts so these are some of the features we used to identify cars okay and they they they come in different different color so there are a set of features we need a right set of features to uh to understand that is a car and how do we learn it uh how do we learn it say for an example that uh these seats and then fourwheel steering these are the right set of features that we that we used to identify whether that's a car or not okay so that's how a baby learns it identifies some set of Fe features okay uh my my my my father is telling that okay it is the steering So based on that I will identify okay that's a car because I've identified some of the features of that because I've identified some of the features of that uh one of one of the main main thing which I want to highlight is identifying the the brand of a car okay brand of a car identifying the brand of car that's that's a very very that's that that I face in real world is for every car we have identified feature okay say when I when I when when I see a car I'll take a look at logo of that in in front of the car so in front of that there is a logo and if it is logo in something like this then it's Audi okay then it's AA then it's AA because of this logo I'm able to identify what's the because of this feature mainly the logo we let's this because of this feat feature I'm able to identify okay that's a car so this is the right or unique set of feature that this car have so so for example so when you can train your model okay so you need right set of features okay um so so your model is able to identify that I hope that this this give you pretty much lot of sense to you at least okay say for example you want to detect the cat in in an image so you want to build a model that given a image eye that will detect whether that is a cat or noncat okay so a cat can have can have different set of features so you can hand design right set of features by yourself like you can design you can just the the values are in pixel format the values are are in pixel format okay the the picture so you can take the only the pixel which are of ears you can take only the pixel which are of ears so you can design the right sort of features like ears nose and which are very consuming time consuming task because for because you need to be very good at the the the at the background of that domain or or or or or you should have a good domain background for identifying the right sort of features because in this example it's very easy car image but what if uh in real world you don't have a problem like this you do have a problem like this but but like weave detection um like in weather you you should have a great background inside that so you should write you should design right set of features and that will be very timec consuming task and getting the right set of features for your model is very very challenging in today's era for many tasks around us okay say for an example um like uh wave detection or or whatever um that we should we we don't know uh what are the right sort of features we we think okay every every feature is the right right set of feature but getting the right set of features in today's era for many task is also very challenging okay so so what so so to in using machine learning using machine learning what we are able to do what we are able to do we are hand designing the rights or features we are hand designing the rights or features so we get an input we get an input we get an input and then we hand designed right s of features so this is a car so we we taken out the pixel of maybe wheels for that image and then we we taken our right s of features like Wheels steering logo okay so there are some sort of features we hand designed by ourself by programming and whatever and then we train a machine learning algorithm on these pixels or whatever on this whatever our features which we' have extracted let X1 X2 all the way around to the xn okay we train a Model F let's let's put in a giant X okay and then we give this set X and then we train our F maybe it can be uh classification algorithms like legis regression it takes the value of x and then classify that either a car or a not car okay so in machine learning what you're doing is you're hand designing the right set of features by yourself because when you see your machine learning data data which you have you're given the in several input values several input values 1 2 3 4 5 okay and you are given the the target label and these these Fe features are hand designed these features are hand designed okay so this is how uh this is what today's uh today's machine learning is able to do is it also require feature extraction it also required to extract features from the data by human human human human will do it human will Design right of features so this is what the problem which we have is the is hand designing the rights s of features which are very challenging and is very time consuming task for Designing the right right set of features so so what is the solution to the pro problem the one solution to this problem is making machine learning not only learn the mapping from features to Target labels also learn the representation to y okay so basically we are asking our computers to not only learn this classification stuff not only make a model given the set of features given the set of features also learn features by itself okay so given a car given a car learn feature EXT extract feature automatically and make a model on the feature which you extracted Okay so say for example uh in machine learning you are hand designing set of features and then you're giving giving to the model whoever the right set of features is but in deep learning what what what exactly you were doing is given a car an image to your to to your system what it does first of all it extract features that's called feature extraction or feature learning or representation learning it extract right set of features and extract right set of features let's say Wheels let's say anything okay and based on extracted features it builds a classification model or whatever learning algorithm and then is able to classify so so there's a huge amount of time which is preserved and this actually perform very well in today's ERA with stateoftheart models which are doing very perfectly so what I again I'm going to repeat that one solution is to not only learn mappings from X to Y but also learn X's as well also learn the features of it as well okay of course you're going to going to give you give the input which is the car but but but in but for if we have to identify that problem that can be solved using deep learning or that problem that that can be solved using machine learning you have to identify that okay the model takes the raw data as input which is the image and perform feature extraction by learning the feature representation so it ex automatically extract the features and learn the parameters to perform the necessary task or to build a model f that maps from features the Learned features to Output but one thing which you will ask question that how do we go on performing feature extraction so how it performs and what is what these uh three three things means don't worry about it just just I will will briefly talk about in this future detail don't worry about anything just understand what exactly deep learning trying to solve okay so I think it's very very very very much clear here I just want to move forward cool so what is the definition of machine learning so this is a definition by the book again I'm saying this is by Ian goodfella yosua benju book and deep learning methods aim at learning feature hierarchies with features from higher level of features form by composition of lower level features and automatically learning features at multiple levels of distraction allowing a system to learn complex function mappings from the input to the output directly from the data without depending completely on human crafted features and learning deep architectures for AI and that and and I personally know that you are very confused with this statement so let's get ahead and make you understand that what the exactly definition trying to tell maybe you know about hierarchies okay the hierarch means like we have this hierarchy of a feature so you have different different feature different different levels so basically this is the this is in this level you are extracting some level of features some lbel of features you are extracting so that is visible layer that is the input pixel so you give this pixel this pixel this pixel this pixel this pixel so these are input level fixes which is given to your model which is the raw in raw raw input raw raw input and then the then it is passed don't worry about what is this what is this what this Arrow just assume whatever just one you understand what exactly I'm telling first the input which we give our which we give to our model or whatever algorithm is the pixel value is the pixel value is the pixel value okay um is the pixel value which which we given in which is visible to us now now the we we give to the second level we second layer which you usually call as a hidden layer but don't worry about it just just don't just just don't worry worry about it then we give to the next level which extract some patterns some feature like this some feature like this okay then we go then then we give to the next feature whatever the information contained whatever the information is G given to the next feature and then we learn some sort of features again then we extract extract from this image some set of features then again next next l level we extract some set of features we extract some set of features and then based on these learned features you're getting whether that person is a car with person or animal okay uh so basically what exactly you are doing is extracting features till this level you're extracting features and at last you're making a model here and then making prediction whether whether that is a car person or animal don't worry about it what exactly um what what exactly is trying to what what exactly these Arrow represent and how how are we making classification model just don't worry worry about that the only thing to worry about that you are understanding the right thing here the right thing is that you are learning features at different different levels and you are learning feature hierarchies with features from higher levels of the hierarchy so here you have you have some set of features learned and then the second and the third so have multiple levels where you're are learning different different features and because of this you're able to make complex functions we'll talk complex mapping from the pixels to the car person or animal okay you're here no human Loop is needed it is automatically creating it is automatically extracting his her uh glasses or whatever it is auto automatically no involvement no human crafted features at level one it extracted some sort of features at at level two extracted some some set of features at level one it extract edges okay mainly this edges edges level two extracted Corners mainly this one and Contour level three it extracted object Parts like this okay level level three so it is learning complex features now your model is able to make complex function of comp complex function f that takes the input image and then then powerfully making or making a very good prediction comparable to human U score or accuracy so I hope that that this is very clear on the definition of deep learning and and general idea what the what exactly deep learning trying to solve is learning feature hierarchies okay with features feature hierarchy from higher levels of the hierarchy formed by the composition of lower level of features so you're not involving any any human Loop just you're learning feature learning exactly what the Deep learning is trying to solve cool so um we we we we'll again precisely Define uh you will learn when when you will learn neural networks then you will understand what exactly de printing is but I hope so that that that that give you a better sense over here coming to the different applications there are thousands of applications which can be solved which are which are in production today which are using deep learning to solve the real world problem the first one is um op classification problem given an image classify that whether that image is of cat or a noncat so how does it helpful and maybe and when you go to the company you just show your face and that allows gate opens and then you go inside that's face recognition where it identifies whether you are that person or not okay uh classification plus localization to localize that where that object is for if for example it can be used in cell driving car which you're seeing in front front of you it it is able to localize the stuff uh so that it would make their own decision to where to go further and then we have art generation using from text so given a a beautiful woman it will generate the image of beautiful woman it can be possible I'll show you one demo today and then we have chat Bots uh chat Bots like AI coder uh AI Q&A system using gpt3 and then uh lots of things uh search recommendation system like Google is using San Francisco whatever is there um so I think these are some of the examples another examples are speech recognition task you can just make use of Alexa how are you and then it will give you the answer music generation using deep learning you can actually generate the music using deep learning and then brain tumor detection it is used also used in medical industry so these are some sort of applications which I highlighted for you at least to get to know much uh in in in much more detail so I hope that this is very clear now and now what I'm going to do now is to show you some of the cool application which I built as a project by myself so I hope that that will give you more sense to you so I'm just going to show it to you um the first one which I want to show to you is the chatbot which we had developed at Anton so let me just uh let me just make it little bit oh wait for a second I'll just make it 1080p so that it would be more precise for you to at least see that so you can see over here uh and chat which is which is able a Discord chatbot which is able to chat with me uh like uh which you can see no problem let's talk about something else and then I seriously haven't heard about this song we did it and this is an chat which you can of course opt for it you can just make you make that in your own Discord server if you have just email us we will uh my company has launched this product which is anat which uses gpt3 so this is one of one of the example of deep learning the another example is I just show you another example we have ancer which actually given program for taking the factorial it gives a Python program for that which you're seeing in front of you that it it it is able to solve any kind of programming question which you presenting in front of your screen and I I hope this is anodo which is again you can email us to get the access of it now coming to the next part is n Q&A q& is a system which you can see over here that we have n q you can just ask what is a circle and whatever it is able to answer your question and that is we even answering uh in machine learning tasks and I hope that this this is what the C assist says system we have developed and it is available for uh which you can email at Anon will be happily will'll be happily giving you in your own Discord server to make your uh server produ productive coming to the next project is Art generation using deep learning which is generated adversarial networks which we will do in this course uh using Gans and which you can see that how this is generating the image and this is the image which is generated by an this is on my own YouTube channel this is a image which is generated by uh peoples okay coming to this we have another another art which is generated which you can see over here uh using text to image and I really hope that it made a lot of sense to you you can just go at my YouTube channel and uh just sub subscribe this YouTube channel if you want and then uh go to playlist go to M2 and also also what I what I want to highlight is uh you can just uh have you can just enroll in LMS you can just go there and enroll in LMS by going to this Doc and uh then going to uh reading all the course and then assignments and a lot of lots of stuffs you can go there and uh enroll in your LMS which which I written over there you can enroll in your Neo LMS okay so you have so I hope that this is pretty much clear and I also hope that you have enjoyed this video If you haven't uh please leave a comment at how I can improve this uh these lectures uh otherwise uh I hope that this video give g g given a lot of intuition of behind deep learning I'll be catching up in next video till then byebye have a great day everyone welcome to this lecture we'll be finally starting off with introduction to neural networks uh so let's get started with this lecture um so uh first first of all we we will talk about biological neural network as it is very important um that how our neurons in our brain work so I'll try to relate the biological neural network with our neural networks specifically we'll talk about perceptron so in this particular lecture here's the outline of this lecture we'll start off with the biological neural network we will will get into the the the that how we can relate this to uh something called as perceptron on and then we'll go ahead talk about different things kind um activation functions and whole outline can be found on a course website as we had detailed uh sections and subsections provided there cool so uh let's start with biological neural network so here in front of you you're seeing is a neuron in a human brain so basically human nervous system contains which is something called as cells and you know cells are the basic unit of a life you have already studied in class so so so human nervous system contains cells called neurons and the neurons are connected to one another with the use of exons and the dendrites and the connecting regions between exons and dendrites are referred as synapsis so what exactly it is telling that you have a several neurons let's assume this as a specific neuron so you have a neuron and this is connected with another neuron another neuron okay with the help of Exon and dendrites and dendrites and these are the the reasons which connects the exons and the dentrites are called synapses but you may be thinking hey uh you are not in Biology class then why why you all are teaching this so I'll try to relate this with the particular stuff like this the the the keywords the dentes are the input terminal which takes the some input from other new neuron say say for an example you have a neuron over here so this is your particular neuron and this gives some input to the another neuron so they are connected to each other so this is a neuron which gives which which which gives an output which is connected to another neuron and this the output is taken the output of another neuron for for for for this particular neuron it it act as a input input terminal where takes the output of the previous neuron okay and so that's why we have a dendrite which is nothing called the input terminal okay the next thing is the Exxon which is the output wire which is the output wire so Exon terminal is an output wire so this so whatever it outputs the neurons whatever it outputs so what dite received from the neuron denr is called the input terminal and whatever it is outputed is from the Exon terminal because it's the it's the Exxon which is the output wire and which output something and Exon terminal are the output terminals which output something let's say let's say output something let's say this and this is again transferred and taken input from using the dendrite so again I'll Recaps youate this stuff as Dent rites are the input terminals the Exon is the output V and the Exon terminals are the output Terminals and this is a particular structure of a neuron this is a particular structure of a neuron so I hope that this this this gives you a better sense of um moment cool so we will not um want talk about this biological neuron we'll go ahead and talk about we'll try to relate this with artificial neural networks okay so so let's take an example uh you have a neuron which gets an input say for an example you're getting X1 you are getting X2 X3 X4 which is an input received in dendrites so you have a dendrite which is a dendrite terminal which is the input terminal so let's say you get four um inputs don't see this diagram as of now just only see this diagram and listen to what I'm saying so the information which then right receives is called X1 X2 X3 X4 okay so uh for for for for for this example you have X1 X2 X3 X4 which are the input which is received in D rights and the information the information X1 X2 X3 and X4 are weighted by some weights and W1 W2 W W3 and W4 and it determines the effect of your input so uh say for example X1 how much this effect of an input in this neuron on okay so we'll take a specific example say say for an example you saw something okay so you you have a human eye so just and you saw something like um like a dog like a dog okay so you saw different different thing like its eyes it's eyes let's let's assume this eyes is X1 its ears X2 it's tail X3 and its legs X4 so all these in your mind all these contain some weights let's say example that you saw that you are quite familiar with the mouth or the eyes of the dog to be similar so let's take W1 X1 okay so this input how much weight it contains how much effect to so because you want to identify that dog so that's why you want to identify how your your your maybe mind knows how much effect your eyes there okay so eyes of a dog how much effect does that ear of a dog effects uh on for for for for you so that you are a successfully able to classify that as a dog okay so so most important thing say for example it's its tail may be having the the biggest weight because by seeing the tail you're able to identify whether it is a Dober man or anything so tail would be the containing the highest weight so the whole whole whole phen or whole thing of weight is that that the W1 W2 W3 it contains the information is weighted we give some weights to every information whatever we get in the D right so that we'll be able to identify the effect of each input in your dendrite okay so basically in in an in an overview it determines the effects of the input so W1 X1 so what we do we simply multiply with the input so all X X1 has has has its respective weight say for example X1 X2 and X4 so we have four features we have four features and let's say that X1 is more important in I in doing that task so X1 is weighted more W1 W3 W4 and W5 let's say X1 is important so its weight will be high because it is weighted and this X1 has the the most important effect of the input okay so we'll come to that weight or uh a ton of time the whole deep deep learning is based on that getting good weight so we'll talk about that okay just assume that you have any weights we don't we still don't know how do we get the actual weights we still don't know about the formal definition I'm just defining it informally so the weighted information now what you do you multiply the weights with the inputs so you can determine the the effect of the input uh using that weights okay so the weighted information are aggregated in the nucleus so basically this is a nucleus which is nothing but the PowerHouse okay so it's aggregated in the nucleus as a weighted sum as a weighted sum as a weighted weighted sum as a weighted sum so what you specifically do you simply um multiply the weights with the respective inputs and add it up okay um so W1 X1 plus W2 X2 + W3 X3 + W3 * X3 + W4 X4 okay simply add it that's it that's what what what what you do you simply aggregate in it in a sum variable or a nucleus let's call it as a z okay that's a nucleus that's a nucleus where you aggregate all the information at one point plus you add some biom okay let's let's let's add some bium let's add some bium and don't do uh that is a bias which we will have a detailed talk uh in this uh section please don't worry about that we'll have a detailed talk on that just you can ignore this or assume some constant like 0.1 okay don't worry about that what W1 how how how we'll choose W1 we'll see in the whole section ction then what then what we do so here we came to a nucleus and nucleus perform one action to it so this now nucleus now this is down here you apply the the the what do you say the nonlinear function the nonlinear function so on Z so you so you on Z you applied some kind of uh some kind of function don't worry what that function do okay just just just assume that we apply some kind of function may maybe you have already seen something called as sigmoid function I'm just taking an example of a Sig word function there's a lot of functions which which we apply which we'll formally Define in this section don't worry about that the only thing which you need to worry is to understand the whole process of uh neural networks and I'm just trying to for informally Define the neural networks with the help of your neuron so so where we are we have set of inputs which we got from a d right then carrying these input carry carries the the the weights or the or the or the information is carried by the weight okay weighted by the weights which is W1 all the way to W4 why all the W4 because we have four inputs which we got okay it determines the effect of each input by taking or or the product between their respective um inputs and then what we do and then we reach to the nucleus where all the information is aggregated by taking a sum where we add one bias term we'll talk about it l later on what exactly that bias term do okay and then and then after after a aggregation now some processing applies like nonlinear function which is a sigma function in this case maybe if you know about it if if you know about legis ression then you might be knowing that and then it is further sent and then it is further sent for further processing to Exon y okay so it is further sent to another now we got our output which is some non non on some function on Z and then we get our output y okay and then it is sent to as a input to another neuron as an input to another neuron or we get a final output or we reach to the destination what we wanted to reach so what does it mean it reaches to destination so let's take an example you saw something you saw a dog okay you saw a dog and here the the information uh let's say you want to identify whether whether that dog is a is a German shepher or a doberman okay Doberman pure so specifically what you will do you all know that a German shefer has a long long long hair and kind of wolfy um nature kind of stuff and Dober man is very uh thin and have a and don't don't don't have a long tail as well as they are very um they very strength and kind of stuff you have a specific mind in your uh your specific picture in your mind so when you see Doberman or when when you see a dog you need to and that dog is either German shepher or dman pin okay so here is a dog that dog is either German Shepherd so how how your mind will work and I'm just taking an example of it so what happens is you see his hair you see his tail so all the information get into your into your eyes and through eyes you get into into your mind through eyes it gets into your mind and all the things like hair Dober Manel these are your input these are your input hair tail ear eyes these are your input and over here the that's a hair is carried by some weights carried by some weights all the weights so that we can understand the effect of our inputs so say for an example that uh the your hair has the the the weight of a hair because in your mind in in at least in my mind I I'll I'll I'll be having more influence of hairs so w one here will be very large okay so I'll be able to identify that and then what and then what happens after after we aggregate the information we simply aggregate this information in a nucleus so every feature is very important so you aggate the information as a nucleus and then you apply a nonlinearity or some some kind of function don't worry about what is non nonlinear Etc what what this function does just simply we apply some kind of function or some kind of processing and then we reach to the T destination whether that dog is a doberman PTI or German Shepherd okay so or or maybe that output whatever whatever output we get after applying that function it it will be sent to another neuron so that we so maybe if required I don't know about much more about brain cool but is this exactly happens in our maybe um um a specific neuron it's it's kind of no absolutely no is just an inspiration from a neuron it's not like that how these kind of exact maths is being calculated it's not that it's just an inspiration so that like his scientists previously just take an inspiration of it and made a mathematical Morel out of it okay so do not relate it to exactly how neuron works you can see other videos on how neuron and how brain works but but specifically this is taken for inspiration for this kind of uh statements that I've described up Okay cool so I hope that this gives you a better sense about how everything works let's go ahead um over here the pictorial representation of a workflow so exactly this is the artificial neural networks so this is in mathematical model this is whatever you're seeing is not a real neural network or or or a brain is just an artificial neural networks where we have we will talk about what is neural what is networks later on but this is a pictorial representation of what our workflow States so just recapitulate you you have a set of inputs you have a set of inputs X1 X2 X3 all the way around to the X4 okay and all these inputs are carried by or all all the information is carried by the weights which we denote with the W1 W2 W3 W4 and a biased term okay and um and these weights determine the effect of your input and this is a nucleus this is a nucleus where all the information is aggregated by taking out the sum of and this is this is the summation notation uh W1 X1 + W2 X2 + W3 X3 + w4x 4 and then we had the bias term and then in that and then let's assume that this is the nucleus and then we apply and in nucleus we apply some kind of a function on it so let's say Sigma on Z okay and then we get some output Y and then it it it is either your destination it is either your destination which you want to achieve or or it is or it is sent to another another neuron the same neuron like this where it have where it again multiply without which which will'll see later on okay cool so I hope that this gives you a very specific sense about artificial neural networks and I hope that uh you'll you'll you are able to understand how how exactly it is working so now what I will do I'll just just make a remark that neural networks whatever artificial neural network which you see is called exactly this is what artificial neural network is okay so neural network basic units is inspired from machine learning and machine learning is obviously spy from a brain and stuff so so here which you're seeing if if I know this the course prerequisite is machine learning fundamentals I either recommend in my mll1 course or anything but you're exactly seeing that this logistic regression or or or whatever we have studied is exactly logistic ression if you see okay so basically uh this is the one unit so this is what you what you're seeing this is a logistic regression what do you do you simply multiply with the given weights and the inputs and then we agre create it and then we and then we apply a nonlinearity or the sigmo function which you usually know 1 / 1 + e to the^ minus g z and then you will get some output let's say a and then if a is greater than 0.5 you identify okay that that value um uh let's uh one or if it is or else if it is smaller than or equals to 0.5 then zero okay then zero so this is this is usually used for classification problems and why do we apply this sigmod function we'll talk about in detail in this deep learning we'll talk about this Sigma function in very detail but what this sigment function tells you um so what it does it you you get your output you get your output in a range in a continuous manner okay so it simply squeezes that output into bit between 0 to one between 0 to one now this was your basic unit which you already seen in machine learning what do you do you put take that basic points and you put several units and stack them up okay and take that stacked layer and make several layers okay so so you this is a particular unit which is exactly what exactly computation is doing is simply multiplying with different different weights and then applying processing Etc and putting up different different basic units and then at last with that kind of stack or basic units are able to generate predictions so this is the Deep neural so we will talk about that later on but the basic idea that this slides want to give you that neuron networks are inspired from basic units of machine learning or puts up puts up lots of basic unit together and then get your output but don't worry what this exactly diagram States forget about it about it if you don't understand okay please rewatch the video if you don't understand otherwise uh you can simply ignore what what exactly each neuron is doing will will cover that in detail in a multi neural network uh or perceptron section cool so the perceptron now what we'll do is formally Define a perceptron so that it would very helpful for you at least so let's take an example so now we will formally Define the perceptron so um here you can see the simplest neural network is referred as the perceptron as the perceptron where uh where we have the basic which is the simplest neural network which we which we refer as a perceptron so what we are given we are given the information from X1 to X dimensional okay so I'll take an example I'll take a simple example because I just want to may have a conversation with you so that at least you can understand everything and please make sure that you watch in kind of a 2X manner it's okay for you I do I I don't care about that so uh let's take an example of a diabet or maybe huh yeah so diabetes predictions system diabetes predictions I'll just take a different pen so that at least it gives me good a feeling so uh what's your favorite color I don't know it's blue so diabetes prediction system diabetes prediction system so in machine learning you're given set of features and you're told to take that set of features and map to or make a fun F that takes a particular set of features and maps to an output variable y that's it that's that's exactly what in machine learning you're trying to do so these so let's say your X1 denote U maybe some kind of a um let's say the BP okay and X2 denotes BMI okay X3 denotes your age X4 denotes your maybe um uh symptoms which have a certain SE or the height okay X4 Den notes your Heights so these are your information these are your information okay and these information are carried by the weights are are these information are weighted by weights that how much this how much this have effect or or or or how much um uh how much how how how much our input effects okay so or how much our information carries information okay W3 and W4 so w 1 W3 W4 okay so you have a respective weights which tells you how much it affects okay how much how much your information have weights or how much how much weightage or how much information it has Okay cool so what I will do I'll just have my X and uh this is X1 X2 all the way around to the XD okay and that is transpose so that it's become it's a column Vector I think yeah it's a column Vector it should when when you do the transfers it becomes the row Vector that is a x which is the input value and output value output value Y is either zero or one I'm taking an example of a classification case of a classification case so you are your output is either zero or one cool so specifically over here you have a weights you have a weights um which is carried by the weights your information where what is learning so where learning occurs how you learn the particular system how you make a diabetes prediction system so how you learn it so learning occurs by changing the weights and the goal of changing the weights is to modify the computer function to make the predictions more correct in future iterations so what is the learning tells you how do we how do we learn it okay so the whole learning stuff is getting the right set of weights or identifying how much X1 contains or or identifying the right set of Weights so that we'll be able to identify which input have more effect okay which input have have more effect if you don't understand from this point that how learning occurs don't worry about it we'll talk about in detail what exactly we call as a the the learning problem okay sure so um what exactly how what what is learning and in high level overview what we do the learning occurs by changing the weights W1 W2 W3 and W4 so we change the weights until and unless our prediction uh be more correct in our future iterations okay and then uh this and then you you strive to find the the best weights the best weights by choosing algorith like gradient descent algorithm or stochastic gradient descent and there are lots of optimization algorithm which are out there cool so now after now what you do you you simply multiply or or or or multiply the or take out the product of the inputs and the weights and then you aggregate the weighted information okay the whatever the information which you contain whatever the information which you contain you simply multiply with the inputs just order to X so um simply aggregate the information XI and wi so um W1 X1 so we identify so we are able to uh say okay in a nucleus you'll be having the weighted information or the aggregated information so that we are just adding it up and then we are adding the bias STM and then we apply some kind of activation function which is some nonlinearity okay which is some kind of function here here it is a sigma function before sending it to the destination y okay so now we are we are ignoring too much about bias STM we are ignoring bias term and then we are ignoring nonlinearity or activation function okay let's talk about that and then we'll end this video cool so over here the perceptron with the bias term so what exactly do what exactly bias term help you to do say for example take a simp simple example y = mx + b y = to mx + b so you all know about it very very very carefully so you all know know know about this specific term called y = mx + b where you have this where let me just erase it out so that it would be much better because I don't know why I'm not able to draw very good kind of stuff oh my God no problem so you have this and then you you have this Okay cool so this is the the equation for this straight line Y = to mx + b okay and M here is the coefficient or yeah coefficient of x coefficient of x and B here is your Y intercept you B here is Y intercept or we can say m is slope of this particular line okay so what if just I'll I'll I'll not go into y MX plus b because you already know because it scores expects you to go go at Algebra 1 and Al Algebra 2 so um what is B tells you if we B if we make that b to be equals to zero what will happen if we make that b equals to Z what will happen so over here your y your y will be zero okay your y will be zero and always pass through the origin which is 0 and Z where X is z and y is z and Depends and your straight line or your or your or your or your line will depend only on one parameter so y = to MX where if you have b equals to Z then then we don't need to write it so over here it only depends your slope okay it only depend you can either make this way or either make this way but the only parameter which it which which it it will depend is M okay is M now over here you're only able to make a function you're only able to make a function you're only able to make a function which just Maps which which is which is non complex function which passes only through origin and just only depends on one parameter but when we add B there when we add B there y = mx plus let's say 2 so over here let's assume two so you you able to make more complex you're able to make more complex function and shift the graph okay say for example it it's if we add B if we have zero then it does not shift the graph when we had the bias stor it shift from here to here so what it does it shifts the graph and hence it is able to represent more complex situations which it was not able to um make it before so from this y = mx plus C example it is simply saying it is able to represent the more complex example or shift the graph a little bit up the same way it will work in your neural network bias allows you to shift the activation function by adding a constant to it that is the given bias to the input you can think of it as a linear constant which we learn as a weight okay which we need to find a good bias okay we we we of course we don't just just just we do do not only need a simply add a bias we do need to find a very good bias which we'll talk about L later on that how do we get the W 1 W2 or how do we get B1 and Etc so and make sure that W1 W2 W3 uh W1 W2 these are the weights these are the weights or we sometimes call the parameters and you I I I have already discussed these weights are are the weights for your inputs for your inputs or every input has it certain weights okay so specifically in this case your bias storm help you to shift the graph of your activation function so if you have seen your logistic regression if youve seen your logist logistic regression the graph of this the graph of logistic regression is s shaped s shaped okay s shape like like this so when bias is zero it the the origin is over here and bias is zero now when we say bias one then it is able to shift the graph and hence it is able to represent more complex situation and when we do plus one then it is able to represent much more complex situation so basically bias term shifts the graph a little bit or a linear as a linearly and then it is it is able to represent more complex situation so that's why bior is absolutely necessary in neural networks it's not kind of it it will not work without biom but it's very very necessary part of neuron networks cool cool so uh over here which you're seeing is neural network which is let's say you want to build a house price predictor because we have all we we we are only seeing out the examples of maybe kind of stuff like um we are only seeing examples of maybe kind of a classification problem but what if you if you get a regression problem where we want to predict the house prices so let's say we are given the information X1 X2 so these information is carried by the W1 W2 W3 and for a bi and okay so so then we apply a linear activation function we don't apply any any kind of nonlinearity we don't apply any nonlinearity we don't apply any nonlinearity you don't apply that you simply apply the linear activation function which is just the identity function okay we just it's it's a function when you give your Z which Z is aggregated information of nucleus which just gives Z the input okay which is a linear activation function okay and then you and then you reach the output and then we have your familiar m e meanus square error okay and then you take the partial derivative of it but don't worry about it we'll we'll talk about talk about gradient descent in Greater detail okay don't worry ignore it just I I have just written it out so that how training occurs you'll get to know but just ignore this cool so the last thing which I'll discuss in this video is intuition behind activation function why do we need this n nonlinearity which you're seeing that why do we need a sigma function and why do we need activation function after after we aggregate the information why do we even need that next step which is nonlinearity why do we even need to think about that so don't worry about what the Picture Tells don't worry about what the Picture Tells worry about some uh a story okay so if you smell something delicious delicious okay I'm pronouncing it correct so if you smell something delicious think of your favorite food my favorite is let's say an example of pizza okay take any example your favorite food so if we smell something delicious which is your favorite food in your neurons your sum neurons which which learn to identify that favorite food will get activated and will help you to get sense or tast it so your so you have several neurons you have millions of neurons and some neurons will get activated when you smell it okay to uh when when when when you smell some when you smell your delicious food your some neurons will get get activated and then it will give the signals to your mind to to just get a sense of something or taste something okay and if you s smell something which is not good the same neurons do not get activated okay now this is kind of a critical in this case it's very very kind of inspiration from brain like say for example you have a neuron 1 you have a neuron 2 you have a neuron three you have a neuron 4 you have a neuron 5 neuron 6 neuron 7 neurons eight okay so several neurons you smell something delicious you smell something delicious so what will happen so what will happen maybe the first neuron get activated the first neuron get activated this neuron get activated and this neuron get got activated these three neurons got activated and then it helps and then it it it it tells your hand or tells your mind to just uh taste something or kind of that but but but now now what will happen I'll just have this now the same neurons which we are having the same neurons which which we are having now when you Sim something which is not delicious the same neurons will not get activated other neurons which will get activated which is not the same which which is used for kind of so it will not get activated the same will will not get activated but other neurons will get activated that will help you to leave that food so in general same happens with your neural network that can happen as one activated and zero nonactivated okay so we can say if the value closer to the value to zero the Lesser it is activated so in this example in this in this example in this example um say for an example you have X1 X2 X3 and X4 now these are your inputs now over here this this neuron got activated this this neuron got most and most activated because it for for for this task these two got more activated these two got more activated as compared to this these two are completely zero these two are completely zero and these two are completely one these two are completely one and maybe this is kind of maybe 0 um 75 okay more one okay so um this is how the interation of activation function which activates you which activates a particular neuron so that it is able to do one task this is a this this is kind of a relation to your uh brain kind of stuff uh so uh using linear activation function so why our why our um activation function should be linear why do we need nonlinear so here your data is linearly separable so we do not need any nonlinear activation function we are happy with because here your data is easily separable by a straight line so it is linearly separable but in real world your data is not nonlinearly separable so that's why you have your favorite nonlinear activation functions one of the example of a nonlinear activation function is sigmoid function okay which is 1 / 1 + e^ minus Z and all null linear activation functions are differentiable when you take out the derivative of this you take out the derivative with respect to Z is nothing but time 1 C okay but don't worry about it we'll see the derivations later on so over here your data you cannot fit a straight line to this or fit a straight line like like this to to separate it out so so so for that you need a this you need a or you need a line or a circle which separates it out so so it separate the green and this blue one which is a non linear which which exactly nonlinear activation function achieves is is using to find a nonlinear decision boundary okay so here your data is nonlinear it helps you to achieve the nonlinear activation function to help you to build a nonlinear decision boundary okay so I hope that this gives you a better sense of it so I hope um um so I'll just start off with the last of uh I'm saying from a lot of time so so basically using nonlinear activation function so you have a set of uh inputs X1 and carried by the weights and then you aggregate the nucleus and whatever get net J and then what you do you apply to a non a nonlinear activation function because your data is non non non L linear okay so provide that into a nonlinearity okay and then and then it gives your output and then using a threshold like Z if it is greater than 0.5 you'll just say Okay this the person has a what's say the person has U diabetes so if if it a classification problem but what if it if it is a regression problem we use a linear activation function okay just uh pass that uh the output or or or or the net J rather than applying activation function okay and then and then we get an output but you may be thinking you may be having some questions around it hey hey hey I use you told in can we have a nonlinear data in regression problem yes absolutely we can have but but you will soon realize in neural networks you don't have only one neuron you have a several neuron and and at last you have one output layer and in that output layer uh where there you don't apply any activation function but other neurons you do apply activation function but in rest but in output neuron you don't apply neuron Network sorry activation function and then you simply apply a linear and then you get your output for regression problem but don't worry about it we'll discuss that in very detail so I hope that this is very clear on activation functions and I hope that you understood it very well now we'll talk about the training how do we how do we make it learn or how do we make it learn a real world neural network or how how do we make it how do we train this neural network so that it would be able to cor correctly classify uh the particular neuron and what exactly learning means and what is learning problem so let's talk about that in the next lecture hey everyone welcome to this lecture so in the previous lecture we had a talk on introduction to neural networks we had defined artificial biological neural network we also talk artificial um neural network in inspired by biological neural network we had also told you about um what are weights and Etc we had also given an intuition behind what are activation functions and why why do we need it we talked about linear and nonlinear activation function so now we had seen that how we we we we go ahead and make prediction like with the information carried so what eventually we do so we'll just just do the quick recap of it so basically basically what exactly we do is this is what the perceptron looks like so you have the input feature X1 X2 X3 okay and X4 so these are the four input features and each feature is carried by the weights and these weights like W1 W2 W3 W4 and we have an bias term please see the previous video to know why do we require bi so these are some of the weights so these all the and we have x0 which is always set to one okay so basically an x0 is being multiplied with this so X1 be multiplied with its weight and it identifies the effect of our input so X1 + W1 + X2 + W2 all the way down to the X4 + W4 and these information are carried into a nucleus if you talk about in biological way or is we we do the some of it and then we apply some kind of processing on it which usually called as activation function whether to activate the activate this particular neuron or not okay and if if it is gives zero then then we say okay or or maybe if let's say you are working on some classification problem it g 0.4 so it if it is smaller than our threshold if it's smaller than our threshold H let's say epsilon then we say Okay um say that returns zero and maybe zero can be the person has a diabetes and if it is greater than or equals to maybe greater than or equals to Epsilon then you can say return one and that one can be the person does not have ades so it it is totally dependent on your case so that's just that was a particular neuron where we talked about that how the forward thing work okay so how we do the forward propagation how we do the this is called a forward propagation this is called the for forward propagation where we are prop propagating forward to make prediction okay so we we where we are forward where we are doing for propagating forward to make prediction or get our particular variable Y what exactly we are doing is multiply with these input feature with the weights and do the weighted sum of it and then apply some n activation function and then we get our output okay and now who will tell us that we we are getting the good output we are getting the good output and how do we how do we get these weights how do we get this W1 W2 W3 W4 and B because these are the input feature which we'll get from the user these are the input feature but how we will identify these weights how we identify these weights because it is playing a crucial role over here when when being multiplied so we need to identify what is the effect of our input okay because uh we we need to know how do we get these weights so let's get started talking about this so now let's get started what exactly do we mean by learning this perceptron so so how we can make this perceptron learn to make good prediction or can be using in in in or or okay so what we'll do now is to formalize our learning problem so what exactly does it mean is you have your input feature and you have your input and this is a giant X so we'll say x is a set or giant set which contains the input features and please note I'm not adding the design Matrix if you wanted to know what exactly we will what what is design Matrix I would recommend you to watch my machine learning fundamentals or ml1 videos like first video which was multivariable or multivar uh like multivariable um linear regression it would be very nice for you to know about how we structure our inputs uh so that you could get to know that exactly how the input is structured cool so you have X1 X2 X3 and X4 okay so these are the input features an input feature can be n features X1 all the way around to the xn okay so we have n features in our input set X cool now given these input feature given by the user so this this this will be given by the user this will be given by the user and we need to and we need to and then we'll be having our output if this is a case of supervised learning problem if this is this this is a case of supervised learning problem then we'll be having our label or we can say then we'll be having our supervisor which is Target variable Y which will tell us is which will tell us that how we which will help us our to our model to learn to okay so I hope that you know supervised learning so if it is a case of supervised learning problem then you're giving the given the input set feature X and then you given output variable Y and there's a ground Ru that that is a ground Ruth which we know because these are the these x and x X1 y1 X2 Y2 these are your training examples which is given to you now we need to learn a function f which takes the input value X and we want our function to map to Target variable y okay so to map a Target variable y so you want to learn a function f that takes these input feature X and Maps this Maps this to a Target variable y hat and this y hat this y hat should be equals to why don't worry about other cases like overfitting Etc worry about that what exactly we need to learn we need to learn a function X we need to learn a function f that Maps your input variable to some Target variable okay and then um so and this target variable should be equals to your actual Target variable while training and then you can use this learn function to predict on new new examples where you don't have these your ground Thro okay so I hope that you're getting this this this is what machine learning tries to do it tries to find a function f that Maps your input variable to some output variable and and now so here's some diagram of it so you have unknown function you have not learned any kind of function which is f which Maps your input variable to Y and now what you do you have your training example with that training example you give that training example to the learning algorithm let's say in this case neural network or we can say linear regression or we can say logistic regression or we can say support Vector machine so so you give to the support Vector machine or whatever and then it will output and then it will output a learn function G learn function G which Maps these input variables to our output variable and this course comes with the machine learning fundamentals prerequisite so I hope that you know about these algorithms previously okay cool very nice now let's go further so this was the formal formalization of our learning problem so what exactly is learning so so how our perceptron or any algorithm learns so learning happens by changing the values of our weights so what exactly we do what I'm going to do is X1 and X2 okay and I'm neglecting the bias term as of now okay so neglecting the bias term as of now you have X2 and X1 we give to a neural network which is a perceptron what it does and each information is carried by the weights carried by the weights and we say X1 W1 + X2 W2 and then you apply the procing or the nonlinearity on this Z and Z is this one okay so so so you may be thinking okay you you will you are getting X1 and X2 but who will give you W1 and W2 whole learning depends on getting these good W1 and W2 please listen me carefully whole learning means in all the learning what exactly you need to do is find good find fine good weights F good weights and some or parameters parameters is something if someone if someone is telling you that that this neural network is learning eventually what is doing is getting good weights or getting the good set of Weights that's getting the good set of Weights that that for the particular problem so let's say in starting when we start in starting what your perceptron will do the perceptron will set your W1 to be zero and W2 be0 okay in starting we'll set all the all our weights to zero okay now it will output some y okay that that will be also zero because when you simply multiply with 0 it will be eventually zero um so what what exactly it is doing is it change the value of Weights in every iteration so now you when you initialized your weights let's say W1 to be Z and W to be Zer and we'll see initialization techniques later on but let's say for this example we had initialized our weights now now we now we'll start training it eventually what this training means is getting the good WR rights or the getting the good set of weights or right set of Weights like W1 and W2 so in first iteration an iteration number we have to check we have to give as a hyper hyper parameter let's say that we want to do for th000 iterations okay so so what essentially it will do it will change the value of weights from 0 to maybe 0.2 and W2 to be 0.3 okay that will change the value of weights and check okay because you will get to know that we have something called as loss function and this loss function in this case it's if we apply the nonlinearity which is a logistic loss then then it is a log logistic regression or log loss which we'll use over here so basically we want uh so now after updating the weights we'll check if the loss is decreasing or the error is decreasing by changing those weights if it is decreasing then we update these weights from 0 to 0.2 and 0.3 that's first iteration is done we will do the second iteration and in second iteration what we will we'll change those weights like 0.4 and W2 by 0.5 and I'm doing this completely because some people will ask how are you writing these These are completely random okay you will get to know in late later on that eventually when we do the back propagation I'll do a step by step one example to show you how to show you every cases but over here I'm just sticking as a random okay now over here you up in the second this is a second iteration this is a second iteration this is a second second ration where W1 is this and W2 is this where it changed the weights or tweaked the weights from 0.2 to 0.4 and from for W2 to from 0.3 to 0.5 now we check if the loss function loss function now we check the L loss function is decreasing if it is decreasing then we update from 0.2 to 0.4 and 0.5 okay So eventually what we are doing is changing the values of weights until and unless we get our good or or or or our loss function is equals to zero or our error is zero so in brief we'll use our batch gradient descent algorithm maybe you already know about batch gradient descent algorithm to minimize our loss functions to minimize our loss function So eventually our goal our goal what is our goal to minimize to minimize our error our error with respect to for with respect to um yeah so we need to minimize the uh error um we need to minimize our error that's our end goal so we'll use gr gradient desent algorithm to find good weights don't worry you don't have to do by your hand will you make use of the algorithm which is gradient descent algorithm to find the global Minima of our function J of theta and in the case of regression problem your J of theta is 1 / 2 m i = 1 all the way down to the m s h of x y n² so that is a mean square error and the and what you do you have a function and you need to find the global Minima in this function okay and when you when you find the global Minima in this function that will be here okay and we will we'll cover this so we'll we need to find the until and unless our loss is equals to zero or it it reach to Global Minima in our function J of theta cool so I hope that this is pretty much clear about what exactly learning means learning simply means is changing the values of our weights in every iteration and checking if the all f is decreasing if it is then update those values to the new weights from the old weights to the new weights cool and uh in brief what we do is use bash gradi algorithm in this example but there are lots of optimization algorithm that that that covers the limitation of these gradient descent which which will help us to find the global Minima of our function or to converge or to get the good values okay so in anation of gradient descent what exactly is doing don't worry we'll cover gradient descent we have already covered actually gradient descent in our machine learning course if you want to go there you can go there and watch that what exactly does it mean but don't worry if you don't want to watch that we'll cover gradient descent again to to to let you know how exactly the working uh means how exactly the geometric purpose of gradient desent works cool so let's go ahead let's go ahead talking about the gradient descent so that you could understand if you haven't understand don't worry about it you will you will understand it later on cool so we'll use our standard gradient descent algorithm to train our perceptron means what do we mean by training we mean by training is getting those good weights W1 and W2 okay and and of course we do need a bi as well we do need a bias as well so we need a bi so we need to get W1 good we need to get a good W1 W2 and the B we need to get these three parameters or weights uh which are good for your problem so here how it works so in every iteration okay in every iteration it changes the value of Weights okay of her parameters and check how well your model is performing with the current weight if it is performing well it simply update the weights with the current value it keeps on doing this until unless you convert first to your Global Minima okay so over here over here let's say over here let's say you have your loss which is very high loss very high error very high error very high error which is J of theta let's say um 500 okay that is your error which is pretty big now over here when the when when your loss was 500 your weights were Theta 0 oh my God W1 to be Z and w 2 to be zero and bias to be also zero so these are all our zeros when you're for when your all thetas were or sorry when your all weights are zero then you your loss was very high okay now when you update these laws when you update these laws now what do you do using gradient sent what exactly it's doing it's it's it's changing the weights of this it's changing the weights of this of all of these like now it changes the weight to W1 to be 1.2 and W to be 0.4 and B to be 0.1 so it changed the weights now checked if the loss function is decreasing so you have a less loss and now when we updated it loss function seems to decreased loss function seems to decreased it CES to 400 now we update our weights to be not this to be this okay now again what we do in next iteration it updates these weights to be the new weights and checks if the loss one decreasing and over here when updating those weights again tweaking this means uh let's say 4.1 so in this over here it by tweaking these weights we are getting the loss function loss function to be 200 which is eventually very low okay so keep on doing this until unless you reach to be here because here your loss function is zero and you get good set of parameters W1 W2 W3 all the W1 or in this for example W1 W2 and B so when you reach at this where your loss is zero you have a good parameters and this is come some this this is a parabola and something like that because it should look like Parabola um just for showcasing you all so I hope that what what does it mean if you want to know this in depth I have a video on my ml1 ml1 course please go there and watch that and on my YouTube channel um New Era so this was a geometric purpose of it this this was a geometric purpose of it now let's get ahead now let's get ahead to talk about um like the the the formulization um like the mathematical part of it like what what is strategy do we use to update the weights or change tweak the weights and Etc so we have already studied the dead derivatives it is just a fancy slope so what do the slope means how much y changes when X changes so what what do we need to know in this case what do we need to know in the case of here we need to know how much how much loss is changing how much loss is changing when we change W1 okay the same question again how much loss is changing when we change W2 how much loss is changing when we change to W or B okay for this example so so so it will tell us how much loss is changing cool so I hope that this gives you pretty much sense about now this is exactly slope and that is a slope and in calculus term if you want to take out the slope of a of a nonlinear or a curve if you want to take a slope of a curve we call that as a derivatives which you have already studied if you don't know about derivatives something will pop up or you can just go on New Era YouTube channel New Era YouTube channel there is a playlist called single variable calculus single variable calculus okay go there and complete the all set of videos then you'll be knowing the geometric purpose of the derivatives is just a fancy slope is just tells how much that loss changes when that weight changes okay how much that loss changes when that weight changes so how much cost function is changing when feature weights changes so we have a lot over here and this example we have taken the log loss we have taken the log loss because we are dealing with the classification problem in this case let's be honest we are dealing with the classification problem in this case cool so over here you have 1 / M where m is the number of examples it is looping through I = to 1 all the way the m y I log of H of x i + 1 y I log of 1 s h of x i if you don't know about this log loss I again go recommend to complete mll1 course which is machine learning fundamental course now now what Z means Theta transpose X and H of X means um sigmoid of X and sigmoid is nothing but 1 / 1 + e^ minus Z and E has some value which you all know now we have a loss what our goal is we need to optimize our our we need to minimize our loss function and find the the good weights we need to find the good weights it should not be here it should be here we need to find the good weights good set of weight and that minimizes this loss function we need to find good set of Weights W1 W2 all the w n that minimizes the loss function J of Z okay so how do we solve this op optimization problem because you all know about this that um you maybe if you have already not studied calculus I'd again recommend you to do my calculus course because in this this course you will get to means it's to because I don't I don't ignore calculus in this course please please please go there and watch my single variable calculus videos and then it will it will make pretty much every sense to you now so now you need to solve this optimization problem so what exactly we do is the first step is to initialize all weights to be zero initialize W1 to be zero W2 to be zero all the way around to the w i to be zero with the bias term also equals to zero okay now you keep on doing this you keep on and now you keep on doing this step until and unless it converged or until unless it is great it is greater than or equals to that iteration the number of iterations you want to do or until unless it finds the global Minima and your loss function or until and unless your your JF s minimized okay so you have your gradient so this is your gradient which is which is which you already studied in that course of single variable calculus and this is maybe you will see in Vector calculus this is just a notational argument to write the derivatives so so what exactly we'll do for vectorization we'll put that all in in a column Vector so this is a column Vector it'll make that as a row Vector now so we so that is we need to that is the partial derivative of your loss function J with respect to W so that is the that is the vector of gradients uh gradient Vector now this Vector contains the partial derivative of J with respect to W1 why are we taking the partial derivative of J with respect to W1 if you don't know about the part difference between the partial derivative and the derivative it 90% means the same u means in it's it's it's kind of a don't worry if you don't know about the difference between partial derivative and derivative for this if you don't know about the difference please understand as a say okay so it's just saying how much the loss function is changing when we are changing this W1 okay when we are changing this W1 So eventually you're taking out the derivative of a loss function with respect to W1 we taking out the derivative of a loss function with respect to W1 okay uh now now when we take the derivative so let's say W1 equals to W1 which was old so I'll just write W1 old the old which in this case will be zero if it is the first iteration minus the alha Alpha which which which which I'll come to that let's assume that is 0 0.1 I will come to this Alpha in just a second and then you write partial der um partial der of with respect to W1 you said doj with by do by do W1 okay so this is what it is doing it simply it's simply simply what taking of the partial how much the loss function changes when W1 changes and multiply with the alpha okay and then subtract this particular whole operations with this old one okay and then update that W one to be the new whatever you get by doing this operation and you do this for every example and this keep changing the W new and W old will keep changing okay uh for example this is W1 is over now it will do for W2 all the way around to the W J that is a first sttion okay that is a frustration in second iteration it will do W1 W2 all the way under WJ okay for the second iteration for third iteration it will do the same okay so uh so this is what does it mean what do I mean with this so you take out the partial derivative of J with the respect to L one if you don't understand till now don't worry about it I'll again talk about keep on talking about this but just I just I I I can just hope that you know about this particular thing what exactly we are doing because the you already have studied in your machine learning course so I just don't want to focus more on this so the learning rate GS and general know some additional control over how large the step should be make so if the learning rate is too large you can't you can overstep the minimum and even diverge so so you you all know that we are that we are doing this and we are here so if we have a now we need to identify which rate we need to go to the minimum which rate we need to go to go to this local minimum but this rate is set by the Learning rate Alpha learn learn learning rate alpha or we or you will see sometimes this notation okay so you will uh you want to uh know the learning rate or the rate by which you can control how large your iteration or steps should be um for your change so if the learning rate is too large you you can either diverse so let's say you have this uh you have this loss function and this you you need to go here now learn learning rate is around one what if what what you will keep on doing is go there go there go there it will just do like this it will never convert in local minimum it will just keep on diverging in The Jig jaag manner okay and if you have a small you will I I think uh you'll never converge okay so you can go there um https developers.google.com machine learning classc filtergraph this gives you a geometri or grab view of the learning rate so so over here we are we have on solving this optimization problem so again I'll repeat what exactly we are doing or where we are we are initializing the weights to a zero and we are repeating this step until an convergence so what exactly is doing it is simply M it is what is exactly doing it's first of all taking the partial derivative of of of what do you say of J with respect to W and that is W is old okay not the current on means what w is obviously whatever the W you have so you take out the partial Dera of clause function with respect to W1 and then what you do is what exactly is telling how much the loss changes when W1 changes and then we get to know about that and then we multiply with the learn with the alpha which is a learning rate which gives you additional control over how large the step should be and then you're subtracting from the old weight whatever you get that whatever you get from this operation which is this one and what which is this one and then you get your new weight cool I hope that this this gives you a pretty much Clear sense about it but the the next step is update rule so we are updating our old one with a new one and you may think hey a why are we subtracting it so one way to understand it is the way I understand it is we are subtracting why are we sub subtracting this operation from the W1 is because if you need to reach Global Minima when you if you need to reach the global Minima so let's take an example of this so you want to move down not up you want to not go above you want to go go so sorry you want to go down right so you are subtracting it to go down okay not you want if you add it it will go above it will it will go above okay so I hope that this this gives you pretty much sense sense sense about it so why we are Sub Sub sub subtracting it is the one reason to go down it's kind of a one way to understand there are several ways when you when you would do the partial derivative by your own hand you'll understand in more detail uh you'll get in problem sets so we are updating our old old weights with the new weight and instead of addition we are subtracting is the only reason because we need to reach our Global Minima over here you want to minimize your cost function so you need to reach to Global Minima so we need to subtract to go down so why gradi isent use derivatives so you may be thinking why are needing this kind of a partial derivative or derivative why do we even need it so what exactly is doing let's take an example that you are at this you are at this point and this is this this is this is your function of your loss or let's say this of course this is a parabola and let's assume that this is a function of J of Thea where your loss is very high at this point your loss is very high at this point but you need to reach over here but you need to reach over here but you need need to reach over here so when you take out the so what what it does it draws a tangent line on this it draws a tangent line on this okay T tangent line on this point when you take out the partial derivative of J with respect to W1 it takes out the in geometric view I'm saying it is a tangent line at this so what is this telling what it is telling if we draw a tangent line at that so let me just say if we draw a tangent line at the green point we know that if you're moving forwards okay okay so if we draw a tangent line or how does tangent line is drawn at that point it's taking out the derivative of your loss function or whatever that function is so if you take out if you draw a tangent line at that green point if we say if we moving upwards we are we are seeing that we are that we are going away from our local Minima sorry Global Minima or the Minima of the function where we need to reach where our loss function is zero that that we exactly want and when your loss function is zero you know that you have a good weight so if you draw a tangent line at the green point we know that if we are moving upwards we moving away from the Minima and if we if we are if we go down we are going close to our Minima okay so a tangent gives us a sense of the steepness of our slope of a function okay another thing it tells another thing which is a very crucial this derivatives tells another thing which this which is very CR crucial this derivative STS is the first is the direction to move Theta in or the or the parameters in so over here over here if you see that we are moving upwards we are going away from the Minima but if we go downwards we are moving or we are getting closer to Minima so it will tell you okay we need to go down ra rather than going upwards we need to go downwards okay the first thing it tells is to move s in the second is how big a step to take okay that how big a step how big step to take if the slope is large we need to take a large step because we are far away from the minimum and if the slope is small we need to take a smaller step because there the slope is smaller okay and for this illustration What do I recommend you to do maybe you if if you go to this website or the link when you put it it first of all it takes a larger steps but when when it start reaching to the Minima it start taking a small small steps because there your slope is small okay so it it tells you how big the step to take so these are the two things which this derivatives tells and what exactly we do need okay so I hope that this the calculus is again the very good thing which you need to know are again Toom to know about the geometry purposes by going on my YouTube channel Numa it would very helpful for you cool so now over here you have below is a perceptron which you all see which you which we haven't se which we have already seen it now what we will do is see how we will try train this perceptron to perform well and what do learning means or what does training means is getting good set of Weights W1 W2 all the way around to the W4 with the bias term so this is a perceptron which you're seeing over here and now I hope that uh you know I hope that you got to know about uh the perceptron and everything out here in the next video what I'll try to do is to make you train everything give you equations that how we will do and do we'll do one work example as well as we'll work on some activation functions for taking of the Deb we'll we'll do the lots of derivative steps in our next video till then byebye have a great day so in the previous lecture we had a talk on um the introduction to neural networks and I hope that you really uh like that lecture and understood every concept because most of the concept later on will be dependent on the previous one so basically um now we'll start talking about the learning problem and you don't need to worry about if you don't understand it it's just for giving you an intro if you know about machine learning you just know you know about grade in descent how gr grade in descent works then it will be pretty much clear to to understand this section if you don't know about it I will ask you to watch my calculus videos to understand it better and if you don't understand even if you know calculus I'll ask you to wait for the next lecture because we'll be having we'll be doing a stepbystep example uh using numeric example to help you out cool so so what is so over here what I'm going to do is to formulate the learning problem so basically in this we are formulating the learning problems so here you're given the input variable X and then you have input variable Y and then you want to make a function f which takes this big or or the big X or set X and Maps it to Output VAR varable and we want our function to map our input to Output variable so you you all know in machine learning what we do we take a set of inputs let's say you want to you you wanted to build a diabetes prediction system so you take uh let's say the age of a person the height of a person the BMI and Etc whatever Foods they eat and you take the inputs and then you make a function f and then using that and then it learns to predict a given those input features or those input values or those information to map it to Output variable whether the person has a diabetes or not okay so you want to learn a function f and your data if you if you I I assume that you already know machine learning and and a bit of machine learning the machine learning fundamentals so basically in Mach what what what what you do you you you have a you have data so X1 and y1 which is the one training example X2 and Y2 which is second training example X X3 and Y3 which is third training example and x and YN but over here you have for example input and the target variable input and the target variable okay so this this is your data now you want to learn uh F which takes that input variable X and then maps to Output variable y so so you so when you pass it through this algorithm which is NN and you pass through this algorithm you pass F to this algorithm you will get G which is trained function or which is learned function which is uh which is what what do you say which is very good function to Maps your input variable x to the output variable y so that's a case about learning formulation so I hope that you already know about nonlinearity I won I don't want to jump into this so what is learning so learning happens by changing the values of our Ates and in every iteration of it we should decrease in brief we'll use our bat grent descent algorithm to minimize our cost function so we had already had a talk on this we all we we also talked on our gradient descent algorithm we had also also seen our this and now we also also seen the learning rate we have also seen the update rule we we have also seen the Y gcent users derivative now what we will do is um jump on the training of perception so you have a perceptron where you have a set of inputs you all know what what exactly perceptron does it takes set of inputs with a biased term with a biased term and then pass it to nucleus or a neuron which we say in neuron it do two things first of all aggregate the information and take the weighted sum exactly what it is doing there and the second half it applies the processing in it which is 1 / 1 + e^ minus Z which is an active ation function whether to activate this neuron or not and then we get our output by hat and then we check now we got a y hat which is a model prediction now we check whether this y hat is equals to that Y which is a ground through and if it is uh we we take a loss uh using the log loss which is the negative log log likelihood which you're seeing over here okay so now what we have planned to do is to use grid algorithms so we have already seen what what exactly learning means learning means changing the values of weights until and unless we get a good model until and unless um our loss is minimized so basically W1 W3 all the around of the B are the parameters or the weights which we need to find good which we need to find perfect weights which works very well so basically in whole deep learning era what do we need to do is to find the set of weights which is W1 W2 W3 W4 and all the way around to the B um and B to get if we have good weights we'll be we'll be able to get good output and if you have bad weights we'll be able to get the bad output so you may be thinking hey you how we are going to get this weights if uh so so basically we initialize the weights so before training our neuron or before before training our neuron or before learning these weights what exactly we do what what exactly is we do we initialize all our weights so basically we initialize W1 to be zero W2 to be0 W3 to be0 W4 to be0 and B to be0 okay and then we start training it because now we have the weights now you now your model will performing very bad because everything is zero so you'll get your final output to be zero okay and that is very bad right so so what you do you take out the derivative because you need to find the local sorry Global Minima but in this case it will be able to find the local Minima so you need to find the Minima of that function uh of that function loss okay of of of of the loss so what you need to do you need to take out the derivative of L with respect to W1 so basically what is saying how much the loss changes or J of theta changes when W1 changes when you weight changes okay so basically it's just it's just a pretty much easy version like how much how much your loss or how much your error changes when you change W1 a little bit if it decreases if the if the loss changes when one when W1 is 0.2 then you update that way W1 okay so that's why we take out the derivative because it tells lot of things to us which direction to move Theta in and uh yeah so and and and it all tells us the slope andc etc etc okay so basically so that's why we take a look at we in the previous video we have seen that why grad desent use derivative please see that so over here we get our output now what we need to do is to take out the derivative of L with respect to W1 okay that tells how much l changes when W1 changes a little bit so let's let's go ahead and so over here you need to take a losses there and then you and there is a path a full path okay so basically this is uh here we'll use the multivariable chain rule of calculus so for taking out the D do L by do W1 which is partial derivative of L with respect to W1 what do you do first of all partial derivative of L with respect to sigo of Z okay the sigo of Z because we can't directly go to we we can't directly take out the derivative of L with respect to W1 we have to go through the path so over here we'll use the chain rule of differentiation multivariable chain rule of differentiation what exactly we are doing this computational graph is derivative of L with respect to that Sigmar function which is um Sigma of Z okay and the sigment of Z is 1/ 1 + e e the^ minus Z okay so you take out the derivative of L with respect to Y hat because that's a y hat not that not the it is basically this basically this um this particular sige of Z is nothing but your prediction what you're getting as an output right so you're taking the derivative of L with respect to Y hat equivalent it does not matter and then you take the derivative of this um Sigma function with respect to this one or or or you can say with respect to this one and with respect to this one but it does not need need to be because this is your one neuron this is your one neuron which is okay you take out the derivative and then you can simply go to w one okay without even going here just just have marked a line to make you understand but this whole process takes in one one line Okay cool so uh so here we have you so we'll simply Dera of L with respect to this neuron and then using this the derivative of this neuron with respect to W1 okay cool and then for taking out the derivative of um now over here you the derivative of sigar function with respect to W1 which which are over here so now let's go ahead and take out the the mathematically about let's solve this particular problem so we'll be getting our D derivative okay so let's solve it so let's get ahead so we want to take out a derivative of L with respect to W1 the first parameter so we need to choose because we need to find W1 W2 W3 and W4 okay these four parameters to be good parameter okay and then we'll getting our good output so basically what you do derivative L using the chain rule of calculus okay so so let's take out the first of all let's take out the derivative the derivative of loss with respect to Y hat okay with respect to Y hat or prediction okay so basically your log loss is nothing but minus I'm writing the negative log likelihood y log of H of X which is your prediction or we can say the sigma of X which is a prediction plus 1 y log of 1 minus sigmoid of Z okay so basically you have this and then when you take out the derivative of it when take out the derivative of L with respect to uh this Z so Sig of Z so basically what you'll do so just for your information the derivative of of log X with respect to x with respect to X is nothing but 1 / X so that's your derivative 1 /x so you'll you you'll be left with y Sigma of Z + 1 y 1 Sig of Z when you take out the derivative of this if you don't know calculus don't worry about it um you can just think about okay when we take out the D derivative you'll get this calculation or you can learn learn calculus or see on some YouTube videos to understand how it came but basically is very very easy to understand first of all the derivative this is a constant this is uh this is a very varable uh this is uh your your um so when you take out the what do you say 1 over uh this that you'll be left with the derivative of log X so that will be this and plus 1 + 1 Y and then you simply add 1 by 1 Z okay so that's it that that exactly how the how we take all the derivatives now let's go further and uh and now now we now we have had found we had found this one which is this one we have we we done with this one now we need to find the this one so basically we're taking out the derivative of Sigma of Z with respect to W1 so Sigma of Z Sigma of Z which is the function 1 / 1 + e^ minus Z so that's your function which is a sigo function so you need to take out the derivative of that with respect to this Z with respect to Z because we can't directly go to W1 but with respect to Z and then and then and then we'll be having this this one so from here to here and using this Z we'll get our W one okay so basically how you take out so 1 + 1 1 + e^ minus Z to the to the^ minus 1 because we had converted to this fractional form to aine form so you can t take take take a look like this and then you use the power rule of differentiation so what you did is to transfer this over here transfer this over here so let me just how I come up come come up with this you can simply feel free to um ask me over comments or see any online video because I assume that you already know these uh what do you say Sigma function or or or the derivatives uh calculus don't worry we'll derive this in our next lecture now you have taken out all the now we have now we have taken out all the derivative of L with respect to we can simply multiply with this derivative of Z is this and derivative of Sigma Z with respect to is this one and how this X1 came how how this x X1 came so because the derivative of Z with respect to W1 and Z was nothing but W1 * X1 * X1 okay uh we just assume that W1 * X1 over here over when you take out so over here we need so you'll be left with X +0 okay so you you're left with W1 uh sorry X1 X now let's go further we so that's why we got X1 and then you got the derivatives of respect to one but if you have in a rest don't worry about it pretty much easy we'll again re re revisit the same thing but we'll do the all the calculations by ourself so that you get a very good understanding of it but don't worry if if you haven't understood is just for those who knows calculus for those for those who knows calculus who knows calculus okay to to take out an Al who who are who are good in mathematics so we have t so now we have seen our so now we'll see some of the activation functions so now we'll see some of the activation functions and then take out the derivative of those activation function okay so this first activation function which you have already seen is the logistic function the main main reason why we use Sigma function is because it exist it it it makes that the output of Z into a range of 0 to 1 okay so when you take out the derivatives which you're seeing in front of you we had already taken out by derivative of Z with respect to uh Sigma of Z with respect to Z okay cool uh let's go further so we have another activation function which is the tan activation function the hyperbolic tangent activation function is often referred as a Tage activation function it is very similar to activation function it's just similar to activation function but rather than making that Z to in the range of 0 to one it makes in the range of minus 1 to 1 it range in the range it it makes in the range of minus one to 1 rather than making it to the 0 to one so that's why we Define as a tan activation function what is the function formula the formula for that e^ x e ^ x / e^ x + e^ x so you just instead of Z over here it's X so you can just put the x value or input value to convert into range of Z sorry minus one to 1 when you take up the Dera it's a bit hard and I don't want you to get into this because it's kind of you just need need need to know about this particular stuff you don't need to you you need to know about the derivative but if you are seeing over here the Der ative of that e^ minus Z is nothing but 1 t h squ okay so all the derivatives which be here we are using the chain rule differentiation exponential uh derivative Etc to get it to 1 minus t z ² which is the derivative so why why I'm telling you the derivative because we'll use it in our later CL later lectures so that's why if if you're not not getting it why we are why we are using it please be sure to uh please be sure to complete the whole class class and then you'll be understanding why do we need to use these so then the next activation function is widely used activation function Rel activation function which is a rectified linear unit the main advantage of this reu activation function over another activation function it does not activate all the neurons at at the same time so basically if if what it does it takes the max of zero and Z okay assume that your Z your input value Z is 1 so when you take the max of 0 and 1 what it will be answer will be zero so so that particular neuron is not activated because in neuron we have a particular Z and then you have some kind of activation function so Z is let's say one and when you apply this Max then the neuron is not activated okay at the same time because it help it really helps uh and and and and the special property of all the activation function reu activation function whether whether it be Tage activation function or whether it be what do you say Sig mode activation function all these um functions are nonlinear in nature and is differentiable so all are differentiable so that's why we are able to take out the derivative but what do you think about the derivative of this so that so we have the equation Max of 0 comma Z so the derivative of activation function is one if x is greater than zero and zero if x is smaller than zero okay so so so the the derivative is this uh which we can represent through this diagram cool so we'll discuss about it advantages and disadvantages later in the course and we continuously revisiting these activation functions a lot to help us better understand and we'll be introducing later activation sorry extensions of some activation functions to overcome their advantages so if you don't know about any of these things and if you didn't got it really don't worry directly move to the next lecture you'll be understanding everything because there I have gone very slow in understanding step bystep gradient an example so here's an recap so exactly perception learns is by changing the ways and here the path you follow from going to L to the W1 by first of all the Der partial derivative of L with respect to that Sigma of Z and then partial derivative of Sigma is Sigma of Z going to Sigma of Z and then partial Dera of s of going to W1 okay and this is the same for every particular what do you say um the every particular feature over here okay so I hope that's that's that's pretty much clear to you and I hope that you are able to understand everything out here in the next lecture we'll start off with multier perceptor and I hope you'll enjoy that let's go on next video hey everyone welcome to this lecture on multilayer neural network in our course mlo2 and I hope that you are really liking this course so let's get started with uh multilayer uh neural Network so what is multilayer neural network so in in perceptron which we have seen we have a particular unit I would say we have particular unit like this and and and in multilayer percepton we include multi uh lots of perceptron and stack them together okay so let's take one example so taking one example which you're seeing that we have a particular so so which which we have seen in previous videos we have several inputs several inputs and we give to a particular Neutron and then we get our output y okay so what we will do what we will do is have mulp on perceptron like this say for example we have information X1 X2 and X3 we give to the to this particular neuron and then the output the output from this neuron out output from this neuron is given to another neuron and then we get our output right so this is a multilayer perceptron this is a multilayer perceptron where we have a multiple perceptron or or more than one computational lab okay in perceptron you have you were having an input layer and an output layer right so you have a input layer which is X1 all the way down to the X4 and then you were you you were having the out output like this right so yeah so you're having the neuron and then you're getting output so from this particular neuron you are getting the output so this was called as an output layer right output layer and because we here we are having the because from this neuron we are getting the output so that's why we call this as a output layer where all computation was happening in the only in output layer right so all the computations like the aggregation of the information like this I = to 1 all the way around to the J XJ and WJ and then we have apply the nonlinear activation function over here we are applying the sigmod activation function like this and then we are and then we were getting our output right that is that all the all the all the computation was happening in this neuron now over here in in multier perceptron we our input is passed through several layers so we have our input so we have our input X1 X2 and X3 it is passed to one neuron one neuron and then the output from this neuron the out put after doing the computation and computation what it does first of all the aggregation of I = to 1 all the around J XJ and WJ and then the preprocessing okay or the activation function so this processing is same okay so we app do the all the computation and then the output from this is passed to another neuron right and then it performs aggregation and the processing then passed to another neuron that we have to specify now you may ask me ask me a question here use and how many number of layers which we have so over here in this example and then we get our output so here in this example we have total of three layers you have total of three layers um which is called the uh these two are called The Hidden layer and then we have them then then we have an output lay so let's discuss about the terminologies which we will eventually use a lot in this say for an example you you have this example and then you have an input layer you have input layer and then you have a weight for for with respect to every input and then we have one bias right and then we give to the and then we give to the one neuron and then the output from this neuron is carried by some weights with the with the information from the previous neuron so this this weights is need to be learned because because over here we need to learn these weights as taught in the previous video we need to learn these weights so basically the information from the from this neuron is carried by some weights which we denote by w we we talk about what this 21 means later on but over here the E information is carried by the weights right and then and then the it is passed through in over here now what what will the computation the computation in this this neuron will be w21 times times uh the h11 okay the information which we got from here right and plus the bias term plus the bias term okay and then we get our output then then we get our output which is the which is our y so over here we have a total of two uh layer neural two two layer so we in this particular example we have two layer Network and here we have one hidden layer we have one hidden layer the reason why we call this because we are not see the user is not seeing this right what the the hidden layers that's why KOB we call this as a hidden layer but we are seeing the output so that's why this output layer this is called output layer we do not count our input layer because eventually we are not doing any computation in input layer but in in in Hidden layer and output layer we are doing computation right so that is our favorite output layer and hidden layer so again I'm again I'm repeating the through several layers which is not visible to the user that's what we call that call it as a hidden layer right so that was pretty much easy to understand it in much better way now let's talk about uh we will talk about these termin ologies to or notational arguments what exactly this 14 and 15 means and what this just relates to what this relates to and what this relates to and all the stuff but over here this was an introductory kind of a slide for making you understand about multilayer perceptron like how we just extend the idea of perceptron to multilayer perceptron right so let's go on the next slide to to to understand much better better way let's go over here so let's take a let's let's take one example over here but before that I want to make you familiar with ter logies so let me just show it to you so here it is so over here you have an input layer then you have one hidden layer the layers which is not visible of course the and this is called the output layer the L which which is the output layer right so information is carried by some weights so information is carried by the weights W W1 W2 W1 W3 and all the way did bias and then we have four input layer and then we have an layer and then we have an output layer right so so the the notation which you're following to be very much consistent is we have a w to denote a weight and then we seeing lay number lay number and weight number okay so this this this this weight this weight is of which lay it's for layer number one so we write layer number one and what is the what is the number what is the weight number is the first weight right so we write as a land number and the uh weight number so that we have lay number one because it's going in layer number one and then we have a second second weight first layer third weight first layer fifth weight right so over here we'll change the argument but for being precise I'm doing like five we have fifth uh weight which we need to learn but a but as we go further we'll slightly change our notations to help you much more better on more concrete or industri stand wise right so just for information you can assume that a 15 okay and this is layer number this is layer number which is going for layer number two and this is the weight number there is only one weight since since there is only one weight so we have to one right and over here you have an hidden layer so we denote by at which denotes the hidden layer and then this is your unit number this is your unit number and this is your layer number okay so this is a first hidden layer so we denote by one and this is a one there is one perceptron layer we will see that we will having a several perceptron like this and each each denotes a unit right so over here we have the first unit right so we have the first unit okay and then over here we have an output layer which is denote by o and then we have and then this is this is this is nothing but your layer number or output number you can understand in this way so sorry that yeah that is an output number or we can think of in different way that that is an output number and this is your layer number okay but we can slightly change it to our formation is like this I think uh we can slightly do the changes like this where this denotes the unit number where this denotes I think there's a typo it should be one over here and there it should be two over here okay that denotes the unit number and that denotes the layer number to be consistent over here okay I hope that that's pretty much clear about the notations so let's go on our example which we have seen previously so let's go on examples so here we have a very good example to understand a much better way so diabetes prediction system given age height and BMI okay so you have three information or three input value and there is one bias term where we always set x0 = to 1 okay and then here we take an example then let's take an example AG is 4.5 understand this way H can be 4.5 years old and then height is 5 cm or and BMI is six and then x0 is obviously one by the convention so we taken a default value for this example though no one need to understand how this example came it's just a random example which I've taken now I have given the random weights don't worry how these weights come we'll learn how to get good weights just for just a random stuff right just to make sure that is you're clear about the process which is going on so over here over here you have weight 1 one which is 0 0.1 we had just defaulted this okay so we have w11 weight 1 weight 2 weight 3 weight four and then we have one bias term like this okay which is randomly initialized now we go to the first hidden layer in the first hidden layer we have one Z and then we have an H so what do Z does Z Aggregates the information right so it just multiplies the in the weights with the input values so over here if you multiply 0.1 times your uh your input feature 4.5 plus the 0.2 * 5 0.3 * 6 0.4 * 1 okay so that is 0.4 so sorry 0.4 times uh I think I've done did wrong over here I think this should be removed oh my God this should be removed uh please ignore this and then we have an bium please ignore this okay uh and then we have an h11 which is over here 1 1 by 1 1 + e^ minus Z because here we are applying the sigma activation function which squeezes your output into the given range right so over here we have 0.9 9 okay so you get your uh the output from the first hidden layer and this is your output from the first hidden layer okay and then we give to the let me just uh do this little bit concrete it should be 0 2 and 1 the output layer now you simply have initialize a random bait 0.3 times the information from the previous layer and then we add the bias term and the bias term which you have taken over here is 0.2 as a randomly as a randomly the bias term is 0.2 and then we get our output and then we apply the activation function which is 0.62 and then we get our output now if this is a diabetic prediction system we can have a rule which is we can the our prediction is zero the person is not having a diabetes if the predi if the probability is smaller than 0.5 okay and over here it is greater than 0.5 so it is nothing but one the person is having a diabetes with these information okay however don't just think this is wrong right uh means this is just I've taken one example a very random example a very abstract example a very imaginary example okay don't worry this is for actual case this is just to show you how we are going forward in our network with multier perception so how we are going how we are doing how how we are going forward in our Network so we call this as a forward forward propagation we call this as a forward propagation which we call because we are we are propagating forward in our multilayer perceptron right so that is our uh that that is our just an example to help you can to help you better understand these things cool so we have talked about rotations so now we can do for multiclass classific prediction as well with with lots of hidden layers in between like this okay so over here so I'll just make you understand much more detail so over here you have a example of multiclass classification prediction you're given the color size and the and the size of the bird so you're given the color of a bird and then you're given the size of the bird you need to predict you need to predict you and and you're given the sound of a bird right so so so you're given the color of a bird size of a bird and sound of a bird you need to predict uh whether that bird is a sparrow whether that bird Sparrow or parrot okay given these three information you need to predict whether that bird is a sparrow or parrot okay so you're given these three information every information is carried by those respective weights with with one bias St right so so over here the number of Weights which you will get and and so you have a one you have of of over here over here you have one hidden layer you have second hidden layer you have third hidden layer okay and then we can increase and then we can increase lots of hidden layers that's that's that's what we have to choose now you may ask one question H is there any way we can have we can def we can have the number of layers defined like how are how how are we going to choose the number of layers which we want right the number of hidden layers which we want so this is a total very active area of research that there is there is no fixed number of hidden layers which we can use to get optimal results but it is just activate your research we just tune our we just check with two layers it works or not we just tune our hyperparameters like this to get our output right to do to analyze which number of head layers are working well right so here you have the first here and there you simply do the information aggregation and the pro processing step right so act activation step so over here you have a total of so the the the so the the size of an input layer the size of the the size of a input layer is n n n * 3 the the the number of weights at uh at this hidden layer is 4 * 1 because there are four weights times the the the there is only one uh unit because further we'll see we'll be having several units so that's why that is your four * 1 which is the number of Weights okay and the information is carried by the the the output from the edge11 is uh at subscript one to the power superscript one is carried by the weight w subscript 2 one and over here you have un biased term as well so you have a 2 * 1 so over here you have w31 and then it is carried by and then the output from this Le is carried by some weights with one bias term like this now over here we need two outputs first of all for Sparrow and second for parrot okay so every so every so we we we every every uh o output is carried by some weights or the information is carried by different different weights for different different units right and then we get our particular output the maximum so we say let's assume that that this is for a sparrow and this is for parot and this is for 3 * 2 over here we have three weights for the for the sparrow for the for the parot and the bias okay and the information scried by the the the information the weights for the the the information got or the output got from this hidden layer is carried by three three weights the first weight is for the sparrow second is for the parrot third is the bias term okay and then we have a to Total two units over here right so there are a total of six weights there total of six weights over here okay so I think there there there there's a typo have total of three weights over here but but we eventually don't count it right so over here so over here you take out the maximum of Pi right so you take out the maximum of of the probability of 012 so so let's let's let's assume we get 0.7 and this is 0.4 so what is the maximum 0.7 so we classify given these three information our uh uh the the bird is a sparrow okay so that is your with multiple uh hidden layers as well as with multiple output so now we'll see one hidden layer neural network now what we will do is to to stack different perceptron in a particular layer for getting our output right so over here assume that you have two inputs so and just make you familiar with it so you have two inputs X1 and X2 okay and then you have a the one unit second unit third unit and fourth unit okay so that is your first neuron second neuron third neuron and fourth neuron and then we are having the output layer so the this is a one her and layer neuron l so over here let's go with the so so now your information X2 is scattered by weights like w21 for the first neuron for the first neuron in layer number one it's scattered by the weights right and these weights are need to be learned right weights are the thing which we need to learn right which which we use optimization algorithms to learn these which we which we'll do in our couple of next videos to understand how we learn these weits okay so X2 information X2 is carried by the W2 carried by different different weights for different different neuron say for example in this the W2 is the the the weight for H1 by this X2 is carried by this the for the X2 the information is carried by this weight for this particular neuron and the information of this weight the information of this is carried by this weight for this neuron and for this okay and then we get our output like this and then uh the out outut from this is carried by this the output from this is carried by this way the output from this is carried by this way the output from this is carried by this way and then we do our formal uh in in every step we do our formal uh what say Pro multiplication or aggregation and activation stuff right so in every neuron this is this is what is happening okay these two these two steps which is happening in a particular neuron and assuming currently I'm neglecting so I'm I'm and over here for Simplicity for Simplicity neglecting neglecting a biasm neglecting bias okay so I'm currently neglecting biasm over here so that is for the second now let's let's go further to understand this this this way so for this for the first we have the first information scattered by this way for this particular neuron and then for f for for this we have this particular particular neuron and for this this information scattered by this weight for this particular neuron and for this information scattered by this weight for this particular neuron okay so you may have and then we do the formal process to get our output so over here if you combine those things this is how it looks like this this is how it look like so let me just show it to you so you have W1 1 and 1 w 1 2 superscript 1 W2 so sorry so sorry W13 right 1 and w14 1 okay so this is this indicates the layer number this indicates the L number this indicates from where it is coming right so from which from where it is coming the unit one the input input unit one or input unit 2 so it is coming from input unit one that that indicates the input unit okay input unit input unit right and uh uh this is the weight number the weight index so this is the first weight right so over here weight index so the same goes with the the the the the input number so let me just change the color so that it makes sense and W2 is is your weight index and this is your land number same goes with this same goes with this this and then goes with this okay so this is how the notation is working for this particular example we have w21 and then we have w22 w23 and w24 right so you have a w21 so over here this is the input unit 2 this is the because X2 and this is the the the the the the weight index and this is your land number right so this is how we denote the weights over here and if you see this example this has changed the the this one this one over here w11 to this indicates your uh the the the hidden unit right hidden unit and this indicates your weight index and this indicates your layer number so that is your notation arguments now in every particular um uh what is happening in every in every uh this neuron what is happening so I've done a very good uh very good analysis over here so I'll just tell you that what exactly is happening in every neuron or the unit unit hidden units okay so first of all I have given different different names so for so for aggregation steps aggregation steps aggregation steps I'm using Z okay and for post active post the the post activation I'm using H as our final output right so so over here so let's assume for this let's assume for this what is what is happening over here so basically this is this this indicates the the hidden unit the hidden unit and this indicates the layer number the layer number okay so for the first unit and in the first hidden layer we are multiplying the weights of the the first w11 this weight okay and then we multiplying for the first information and the second information right so we are not just uh because uh the first information and the second information is carried by different different weights and then given to this neuron okay because we have several neurons so for the the the two neurons is given to this the two input values is given to this neuron right okay and then we add the bias over here and then this is what the aggregation step means and then we apply the re activation function value activation function on the on on on the output which we get and then over here what what is happening in this it we are simply multiplying the weight from the for the the information X1 is carried by the weights with to the H the hidden unit number two same goes with the information number two is carried by some weights to the to the second second hidden unit okay and then we add the bias and then we get our output same goes with j3 j3 is our this this unit right so over here this is your pre preactivation preactivation preactivation in preactivation the information is aggregated okay with the respective weights and then applying the r for Z4 we are for this one we are doing the pre aggregation step and then applying value okay and then we have and then we get our output which is simply which simply multiplying the multiplying the weight with the output which you get from the particular unit this is also the out this is the weight which is carried by the information by the edge2 this is the weight the this one is scattered by the weight the inform the the the the information is scattered by this weight the information of this scattered by this weight and this is the B and then we get our output and then we apply some kind of activation function over here like reu or sigmoid or tan whatever we apply over here and then we get our output right so I hope that makes sense let's go further so if we can write in vectorial format to have it much more better way so I'll just see so we'll cover till uh different Laya so let's cover over here and then we'll uh go from the next video Let's cover the vectorial format and then we'll uh cover the rest in the next videos so we can write in this way we can write in vectorial format so we can have our big Z we can have a we can have a uh a matrix or or a vector which is z okay and we can store all our weights in a matrix and in know for this particular example we can store this uh our our weights in a particular Matrix so we store this we store for the first uh we store for the uh first information we store this weights and for the second for this for for the weights for this information and the weights for this information as stored like this okay where we have for this for this which are for first first information weights these are second information weights or the second put weights okay I store them into a particular weight which is weight Matrix one okay for this indicates the L number now we have now we make for the second which is which is St as a as as as a column Vector is 112 which indicates which which you can see over here which is weight number two and this weight number two contains the weights for the particular for for every um or or or you would say like this okay and we can write in this way we can write in this way uh in the vectorial format and then we can simply uh take out Z okay so what you can do you can uh simply multiply the weights with the with with the with the column Vector which is X1 and X2 we can put that into a vector as well and when you apply a matrix vector product with with a bias term so we'll we we'll add the bias term in every neuron the bias term is like this so when you add this when you add this so you'll you'll be getting you'll be getting your when you do the Matrix Vector product and then add the BM then add the B term you'll get a uh you'll get a vector is 1 2 3 4 for this particular example and then you apply the Sig mode function and then you have a big H1 and then you apply the activation function and then you apply the activation function like this uh like this and then you and then you'll be getting your output Vector Big H one where you'll be having these things so this is more efficient or more computationally efficient over here and then what you do and then you do the same for12 where you apply the Sig word function every element Sig word function on these on on on the particular vector vector which you'll get from here okay so that's what we have written in a vectorial format so so there's no any kind of a theoretical concept which needs to be explained over here everything is very very clear in terms of writing these things out here main part will be your back propagation stuff which needs a little little bit of more explanation but these examples are pretty pretty much easy to understand I hope that this makes to sense to you I'll be catching up in next video till then bye have a great day so uh welcome to this lecture on uh on on neural networks so in this lecture what we will do is maybe we'll talk start talking about um chain rule of calculus um and try to try to start off with that propagation but before that I'll just take some examples or worked examples of of multier perceptron just to be very clear in uh in calculating all those things and maybe we'll talk about different applying different application uh sorry act activation functions on on on your in in neural networks and how things work and F prop so maybe we'll revise it and then we'll go on computational graph which is of chain rule of calculus and then we'll try to see through the through the computational graphs like how do we calculate the how do we about great great info about chain Rule and then we will go ahead and and and end this video on starting introduction to to back propagation cool so let's get started so till till previous videos we talked till uh nine slides so I'll just want to make you familiar with it right so we talk till n9th class uh sorry ninth slides so what we will do we'll just Recaps youate it so that it would be better for you so we have a z variable we have a variable Z and then we have a variable X and then we have a biased term B right and and then if these these are our weights these are our weights this is the input feature this is Matrix Vector multiplication plus the biased term and then uh and then you get your and the biased term is the vector then you get your uh pred uh a vector which is where we apply the the the the post post activation sorry sorry the activation functions on each Z right so when you when you take out the dot product between uh your weights and and input value use X or the information then and then you apply the preprocessing stuff like this sigmoid in this case and then you get your output which is a vector uh a vector which you will get uh in over here and then this Vector is Multiplied with uh another uh weights Vector weights Vector this Vector H1 is Multiplied with another weights Vector plus the bium so basically we're taking the dot product between these two uh vectors maybe yeah so we are taking uh we are taking the dot product between these two vectors and then uh we we were getting this 012 okay uh so this was our basic uh basic revision of you of our previous lecture and I urge you to to basically have a good understanding of these things by watching our previous lecture because we had discussed a lot in that cool so now let's start seeing the worked example of one layer neural network so basically the the current neural network which we have is one lay one layer neural network one layer neural network Network so basically we have 1 two um three and four neurons four neurons right we have four neurons and then we have and then we have uh one particular one particular output neuron right so we have output neuron and over here we have two input features and x0 is always equals to one so so we don't need to worry about that all right and and and for assumption for Simplicity we have taken our bias term bias term all equals to zero for all uh neurons in this case so basically we have a weights we have weights like 0 0.1 so we have um so we have the first weight in the F from so W uh from the first to the first neuron so we have 1 one to be 0.1 for the first layer for sure so 0.1 then for 0.2 for the first information so the first for the first information X1 is carried by these weights into the four neurons for the first neuron for the second for the third for the fourth and then X2 for the first for the second for the third for the fourth so you have a weight Vector we we call this as a weight Vector we call this as a we call this as a weight Vector right and this is your input Vector which you're seeing which is 21 right so so the in we have taken a very dummy example to Showcase this is two and this is one and the and the weights for the second layer is 0.91 1.1 and one2 that is that are just random and just for the sake of example of a forward propagation like how do we multiply and do all those stuff right so first of all we we when we reach when we do calculation till here we get uh the vector uh for this we have 0.7 for this we have 1 uh after doing all the processing and the processing which is over here in every neuron is the first one is your I uh the the aggregation wi I * XI and then you apply the the the processing currently we're not applying the processing over here but the the first one is the is the weighted average or or the or what do would say the dot perk between your input features and the and the weight values right and then you have done the first part of it the second part is applying applying the the activation function so in this case we for this for for particular getting edge1 one then we apply the activation function on Z which we get like Z1 Z2 Z3 and Z4 you apply that and then you get a final Vector which is the output from this particular hidden layer so this hidden layer is this H H1 okay the first hidden layer outputs this and then this is Multiplied with with weights over here um we have another Vector weight and of H1 * times the the the weight for the second layer we take out the dot per between both of both both of them and then and then uh we get our final output okay so this was your applying the sigmoid activation function where it squeezes the output in the range of 0 to 1 so this one is squeezed into 0.73 1.6 into 0.832 1.3 into 0.78 5 Okay so this this was your basic example of applying sigmo function let's go ahead and see uh the the the next neuron so for the for the we have we we are done with the H1 so we are done with H1 now we do for 01 um for for for this so basically what I can do uh that indicates it's it's the second layer and this is the first unit right and uh this is what it does and now if if you see this is this is your uh output this is your uh the weights this is your weights W2 so this is your weights for the for the second layer and this is your your H1 which you got from the previous layer so information from the previous layer so this information for the previous layer is Multiplied with the given weights and then you get the particular output and then over here first of all you did the uh what do you say weighted average Now by taking the dot product between those those two vectors now what you do you apply the sigmoid function you apply the sigma function and on 2.5 it gives you a range of 0.93 right and then you get your 0 93 as an output so that's a one head and layer neural network right so where we are using the sigmoid activation function now let's go ahead and talk about Ru activation function the same the same example carries out so we have a 4x2 Matrix Matrix where it contains I think um a 4x2 Matrix of weight Matrix we have 2x1 input Matrix and we have four 4x1 bias bias uh sorry not Matrix 2x1 vector and 4 4x1 uh uh 4 * 1 uh bias vector and then we are getting the output and then we have an H1 which is what it does it over here we instead of uh applying uh the sigmo activation function we are applying reu activation function so basically what you mean and relu what it does it squeezes your output between one and zero right so so B basically not one and Zer it just takes out the max it just take out the max of zero and uh and zero I think yeah so it takes the max of zero and Z so whoever is bigger it just gives that as an output okay so 0.7 is bigger than zero 1 is bigger than Z so if there is minus one and your and and and let's say uh let's say 0 and minus one so so your Z is minus one then the output will be uh Z so there will be zero right so reu just what it does is the equation is Max of 0 comma Z and and whoever is bigger it just uh information for the next headden lay it is widely used Rel activation function and we'll see the advantages and disadvantages later on uh here we apply the tan activation function the tan activation function what it does over here which you can see um it's uh oh my God I think there's a typo over here we are applying the tan activation function uh tan activation function over here and this is the formula for it so we we urge you to watch the previous lectures if you haven't seen about these these activation functions in detail so we had a talk on on on a one layer hidden hidden neural network now here we see see an example of two hidden neural network two layer hidden neural network where we have for the for the first hidden layer for the first hidden layer our weight Matrix look like this our weight Matrix look like this we can also write in this format W1 right and then we can write in this format transpose we can write uh in this format so for the first uh weights right so we can simply write this as a this this row Vector into our column Vector right so sorry row Vector this column Vector into a row vector and then we can either transpose over here but but but for Simplicity let's let's stick with our example so basically we have W1 and then we have another this is a weight Matrix right and over here this this is a weight Matrix for for the first layer the weight Matrix for the second layer looks like this where for this you have the particular uh column Vector for the second uh hiden unit you have and you have this red Vector for this which you can see in detail so we have this uh weight Matrix for the second layer and then we have a third uh third weight Matrix or the or the column Matrix for the third layer right so basically for the output neuron okay so you can see it out just pause this video if you want to go ahead and see it out how it just works so we have different layer of neural networks so basically here we have a two layer neural network we have I think uh in in previous previous example this is three layered neural network uh but in if we include this particular output neuron so this is three layer neural network this is three layer neural network Okay cool so over here we have several kind of different layers neural network and and every uh and these are all carried by the weights and our primary goal is to optimize these weights to get good Theta okay or the primary or the weights over here so we can have L layer neural networks like this and we will formally introduce you to the Larn neural network maybe at the end of this video U maybe at the end of the back propagation when we'll formally give you the tools or the definition of neural networks both forward propagation and backward propagation for L layer neural network and when we'll Implement our own neural networks right so that's why I think we can script this L neur neural network right now so let's get ahead so now we'll talk about the chain rule of calculus so the chain rule of calculus so first of all we will'll start off with the derivatives derivatives of the computation graphs and over here let us assume that you want to take out the derivative of P the partial derivative the partial derivative of p with respect to X1 right so the partial derivative of p with respect to X1 okay the partial Dera of p with respect to X1 so what it will be over here we can't directly take out the partial derivative of p with respect to X1 we need to go with the path and this is where the chain rule comes in right so the chain rule comes in first of all you take the partial derivative of p with respect to Z with respect to Z1 times the partial derivative of of Z1 with respect to X1 then you will get to this so basically first of all you you you make a relation with this by taking the partial derivative of p with respect to Z1 and then times right so that is the CH of calcul multi multiv ch ch R of calculus times U uh the partial derivative of Z1 with respect to this particular X1 all right so for for reaching from here to here you take you you you need to follow the path but there is one more way we can reach to this X1 okay so that's why we add a plus sign here the partial derivative of p with respect to Z2 times the partial derivative of Z2 with respect to X one so there's one more way so there's one One path to go here and there is another path to go here right so basically and then we'll and then we'll be having our partial derivative of p with respect to partial derivative of X1 the first path is this path and the second path is going by this path right so basically you you over here this is the come this is the parti the derivative of a chain rule of Al so basically again I'm repeating the partial derivative of this particular p don't worry about these things what they are just assume that all are differentiable and and we are able to calculate the partial derivative or the derivative the partial derivative is quite similar to derivative because the difference between the partial and partial derivative and and derivative is just uh you you can see my Matrix calculus course for this but basically the difference between them is um the difference uh it's it's it's it's more uh partial derivative is involved in vectors right so that's why we we add the partial derivative over the partial derivative of this p P right with respect to X1 so it is it is asking how much this P changes when X1 changes so this this is basically change saying how much how much P changes how much P changes when X1 changes so that's what the derivative is D derivative is a slope right so change in y over change in X right so for for for understanding the change in y change in P over change in X you need to follow the root to to understand the change in X when you change P right so when you change X1 how much P changes okay or how much P changes when X1 changes so basically for taking the for taking out the change you for uh for for for for the for for this with respect to uh for for this with respect to this then you need to follow two parts the first part is the partial der of p with respect to this Z1 and then multiply the partial D of Z1 with respect to X1 this tells how how much for in this tells the how much P changes when Z1 changes and how much Z1 changes when X1 changes so inly it is telling how much P changes when X1 changes right but there is one more path which is the partial of p with respect to Z2 and then multiply it by the partial derivative of Z2 with respect to X1 this is telling how much P changes when Z2 changes and how much Z2 changes when X1 changes okay and then we get our final partial derivative of p with respect to X1 now coming to the next the coming to the next part the next part State the partial derivative of p with respect to X2 the partial der of p with respect to Z2 right uh and so basically if if you want to calculate the partial der of P me how much P changes when X2 changes so there are a total of two paths the first path which we can go is by this and the second part which we can go is by this right the partial der of p with respect to Z2 and then by following this path the partial der of Z2 with respect to X2 right uh with respect to X2 and then you get your how much P changes when X2 changes for the first path for the second path the partial derivative of p with respect to Z1 and partial derivative of Z1 with respect to X2 right so how much P changes when Z1 changes and how much Z1 changes when X2 changes and then you'll be getting a partial der of p with respect to X1 and partial derivative of p with respect to X2 so it's basically if you have seen a great recent thing it we we are what we doing what we were doing we're simply taking the partial D of g z with respect to with respect to Theta or W1 so it just tells how much J changes or loss changes when W1 changes so I'm just teaching you the concept so that you could understand the back propagation in much easier way when when we study about back propagation later on right so that's that's the first thing about the derivatives of the computation graph uh when when we talk about the chain Rule now let's get ahead I hope that this this this makes sense to you and this is now let's let's keep talking about the uh the Deep computational graph if you have a deep computational graphs how things work right so we had a talk on forward propagation all these things which which we talk on forward propagation but now we'll start talking about the backward propagation how we do the how we back propagate okay so the first thing we want to talk about is chain rule in deep computational graphs so over here you have this this H1 this this H1 a takes information it's h h H1 is is is is is is the function of X1 and X2 and this h12 is a function of X1 and X2 right and this h21 is a function of h11 and h12 and this H22 is a function of h11 and h12 and P is a function of h21 and H22 so it takes these two values to compute their output right so that's what the that's what these these indicates over here so now so now I want to talk about is what is the the partial derivative of p with respect to the partial derivative of p with respect to X1 so first of all the partial derivative of p with respect to h21 so first of all we go this way times the partial derivative of h21 with respect to H1 U sorry it's X1 yeah so the so that's X1 so basically we we do this way right so we this all for sure the path but using this we are able to reach X1 right and the partial der of p with respect to H22 right and the partial der of H22 with respect to X2 right but over here if you see we can calculate but there's when calculating h21 h21 to we have one one object over here there there there's one blockage so first of all what you do partial Dera of h21 with respect to with respect to partial derivative of of what do you say h11 h11 times partial der of h11 with respect to partial with respect to X1 right this is how you calculate it this is how you will calculate it okay just for Simplicity let's let me showas you to you how we Cal how we calculate the partial der of p with respect to X1 is partial der h21 with respect to with the partial D of p with respect to h21 first of all we get two here and then with this and then with this we calculate the partial der of h21 with respect to X1 for the we have another path partial der of p with respect to H22 times the partial derivative of H22 with respect to X1 right so there's two path but in in in in in the second path h21 with respect the partial derivative of S21 with respect to X1 and partial D s22 with respect to X1 has a block in between right so so basically for calculating the partial derivative of S21 with respect to X1 what you will do you will calculate the partial derivative of h21 the partial derivative of h21 times the partial D with respect to uh part with respect to h11 times the partial derivative of h11 with respect to X1 so the the blockage clears out so the blockage so the blockage clears out right plus there is one more way we can get to this X1 so basically the partial derivative of h21 with respect to h12 and partial with and and and partial derivative of h12 with respect to X1 and then we calculate this particular partial derivative of h21 with respect to X1 so there's two path over here which we can get to uh that how much h21 changes when X1 changes over change in h21 over change in X1 right so this is how we calculate for X1 right and then we'll do the same for this particular because there's some blockage in between right so let's let's do for that as well so basically when we do the partial derivative of H22 the partial derivative of H22 with respect to X I think X1 okay so we are calculating for X1 so for X1 it is nothing but the partial der H22 with respect to h11 so there's a blockage in between right so there's the blockage in between and then the partial derivative of of what do you say okay so over here I think uh I I did a little bit wrong over here I think so yeah so basically over here is B basically telling you have another path which we can go in this way so basically if if you see over here it starts Cal calculating but over here if if you see the partial derivative of H22 with respect to h11 and the partial and times the partial derivative of h11 with respect to X1 it should be X1 over here right and then it goes with the same with with this for s22 I think the the the diagram is bit wrong I'll just put the picture right so it understand much much well you can simply ignore this you can simply ignore this this is kind of um misleading diagram so for calculating the partial der s22 with respect to X1 so first of all the partial derivative of s22 with respect to h11 right um H h11 h11 right and then times the partial D h11 with respect to X1 right plus the partial derivative of H22 with respect to H H22 with respect to h12 right so we we have another another path which is like this times the partial der of h12 with respect to X1 so we calculated this as well and then we were having two two parts this path and this path and then we eventually will get our output as our final output right so this this was little bit misleading this was also little bit misleading but I will change change it um so basically this is the two paths which you calculate right so that's uh that's a particular thing which I talk about uh and and then same go and then we can write in this format and then like which which you're seeing over here and then we can calculate our partial of p with respect to how much P changes when X1 changes so that's a that's a that's a lot of things which is over here I hope that you underst to this uh deep computational graphs so in the next video what I will do I I'll I'll go through I I'll make you go through the the iteration of back propagation so we we'll solve an example of an iteration of back propagation out here to understand much very well like how everything works and how everything does not works all right so we'll so we'll do in our next video of an iteration of back propagation and we'll try to formally Define the back propagation in deta so thanks for watching this video I'll be catching you next so everyone in our previous lecture uh we had a talk on propagation we had talked about comparation graphs we had talked about how do we back propagate in computational graph and I and I hope that given you a so much sense about back propagation and back propagation is not an algorithm it's uh it's it's a part of gradient descent which we do which you'll come to that later on but basically today what I'm going to do is to do an duration of back propagation uh mathematically by solving a a neural network where I where I'll be doing one iteration of back propagation and show you the updated weights uh which which optimizes the loss a little bit okay so I'll be doing only one iteration but in but in but in but in practice we usually have the parameters of how many iteration which you're which you're going to do which will talk about later on in our hyperparameter optimization sessions so basically the back propagation which I'm going to talk about is just uh is taken from a Blog which I link in description box below so the explanation of this written is also available in the form of blog which I link to the description box below I would like to give give a big shout out to that guy and I'll try to explain it much more detailed way so that you could also understand by watching this video so so basically this this is the basic uh stuff which is in front of you which is a basic a twed neural network where where we are having uh one hidden layer and one output layer with two outputs or two outputs from your neural network so basically let me take my pen so basically you have several weights uh which is carry your information is CED by several weights so basically this uh X X1 is CED by this W uh 1 one or w11 okay so basically what I what I'm going to do is to vectorize this neural network and write this into the form of linear uh algebraic format or I would say majores format or vectors so basically I'm going to what what I'm what what I'm going to do is to store our input so basically the input which which it which it will go is just 0.05 and 0.10 and they're just I I've just taken a sake sake sake sake of an example this is this is nothing much more to worry about so basically this is your inputs X1 and X2 0 0.05 and 0.10 that is your input to your model and uh that's your that's that's our column Vector which is the X and I should denote with the small X rather than big X so basically small X and you have a weight Matrix for the first hidden layer the weight Matrix for the first hidden layer so for for for the layer this is a layer so so basically you have 0.15 and 0.20 right so 0 0.15 is a weight uh is is a weight for X1 and 0.20 is a weight for x uh of course uh X1 going to uh if you see X2 okay okay so X 0.15 is X1 0.20 is for X2 so basically your information is scattered X2 X1 is scattered by 0.15 uh from the first information to the first hidden layer or the neuron and your second uh way and second information is carried by some weight to the first hidden lay so basically that is 0.20 okay now we have the another neuron as as as I said we stack the neurons or the basic units right so we stack the so so basically this h21 basically this is a hidden layer and we have two neutrons in it and this H this X1 is CED by weights and being aggregated at h21 which is the second hidden layer second hidden neuron in the first hidden layer and then this uh and then X the S2 then X2 information is also C by some ways for particularly this neuron so 0.25 and 0.30 which is weights for this for the first layer weights for the second layer weights for the second layer is W2 is equals to uh 0 0.40 which is going uh just is carried this h11 H1 uh H superscript one sub subscript one is carried by some weight 0.40 and for and then this this this this this information is also carried by 0 050 for the second neuron and this the second H2 the second second neuron also gives out some information which is carried by some weights which goes in the which which which goes in the second out the first output neuron and this and the same information is Cared by another another weights for the second neuron okay so this is this is the basic neural network which you have and you have a bias term and you have a bias term which is first of all for B over here where for this you have 0.35 and for this you have 0 0.35 for this you have 0.60 for this you have also 0.60 pretty much it okay so now what I'm going to do is to do a forward propagation so basically that's that's that's what I'm what I'm going to do is to do a simple forward propagation and basically uh uh what what I'm going to do for propagation is get some predicted value o o sub superscript 2 subscript one and o superscript 2 and O sub subscript 2 so that's for what I'm going to to get so let's go so here let's let's go in our first uh hidden lay and in the first unit or neuron so when when you go there as as as I already say to you a neuron is Con a neuron in a hidden lay consists of two things first it gets aggregated in as uh in a nucleus which we call this as a z and then we have something called is post post uh sorry AC activation function which activates this neuron which decides whether to activate this neuron for the particular task or not okay so so basically if this is zero then the neuron is not activated but if this is something then a neuron is activated and that contributes some information that can go into the second layer of network so basically if you're seeing that z z superscript one sub subscript one is can be calculated by using something like w transpose X plus b and basically your W if you if you think for for but for this W was 0.15 and 0.2 right so this this this was your uh for the first the first information carried by two ways into two two different neuron right but but basically uh but basically over here uh which which you're seeing over here which which which we have this this over here this is carried by two weight 0.15 and 0.2 right uh 0 0 0.15 0.2 from two two different inputs and then we add a bu term which is 0.35 and multiply by 1 because x0 is always equals to one and then you get your output so basically this is your weights this this is your input X1 this is your W 21 okay for the first and this is w11 okay and this is your bias term right so this is this is basically your W transpose x + B which is your hypothesis and then this this this is aggregated and then what you do you simply apply an activation function here we are applying a sigmoid activation function sigmoid activation function but we'll see that this sigmoid activation function has a really big fall of Vanishing gradient isent problem so which will talk about it that later on but for Simplicity let's go with sigmoid so when you apply the Sig sigmoid to this output of Z1 the it it it is it say 0.59 it means this neuron is activated uh right and then you get your final output for this is nothing but 0.59 blah blah blah okay so that's your out that's your output from this for for for this you have again the Z1 and A1 and and this is this this in this newon X1 is carried by some weights and X2 is carried by some weights and then we aggregate that and then we add the bias term which is 0.35 and then we get our output and then we apply an activation function which is Sigma activation function and then you get your output right so that that that that is for the first H h11 and h12 now let's let's go ahead and talk about the the the output neurons right so that's that's a production so basically uh if you're seeing over here Z2 if so basically if you're going over here so this is a second layer of course and the first neuron right so basically when you when you calculate that this this output output neuron is carried by two information from the previous layer which is uh this information and this information so this information is carried by some bits and this information is carried by some bits and please note that these informations or the weight value are random as of now okay your your algorithm main goal of your algorithm to learn these weights okay uh that's that's that's that's what which is super important and this a12 is then you apply an activation function on top of it and then we get our o o o21 as an output okay same we do with o22 for which which we get an output as like this so now so now we have we have got our output now what we want to do we want to calculate the error as well like with these weights with these particular weights how well our models performing or how is the loss function or how is the cost function or or what's the what's the error is or how well are models performing right so basically uh let us assume that a ground Thro is for that a ground Thro is 0.01 and o22 for o22 we have 0 0.99 but our model given 0.75 for the for the first one and and 0.1 uh 0.77 for the second output neuron which is your model model output so what you what your model will do uh so basically we can calculate the error by by Sub Sub sub subtracting uh 0.75 then this one and then 0.77 with this one okay so basically you know how to take out the loss which is a mean square error what what whatever you can use over here and then you can uh and then we and then we make use of mean squ error to C calculate the error which is .20 29837 blah blah okay so that's what we are done with for propagation where where we got the error J with all of these parameters now we need to optimize these uh this this J so basically what I told you in our previous lectures or our previous grent lectures is we take the the the partial derivative of of of our C function J with respect to our parameters whoever over there and what we do we simply adjust these weights and check if the loss function is decreasing if the loss function is decreasing we update those weights right so that's that's what we do uh when when when we talk about uh derivative so derivative tells you two things how much weight to change and in which direction your your your your your weight will go in so please note that you was the lecture of gradi and descent so that you have understood in much more uh good way so what I'm going to do is to take out the partial first of all we need to update this weight W1 w211 so what I'm going to do is take of the partial derivative of J cost function with respect to w21 which which tells us how much loss changes how much loss changes or cost changes when that w21 that that W changes or that parameters Chang or that weight changes okay so so how we can calculate that so that's quite easy to calculate so if you want to take out the partial derivative of cost function with respect to your W12 so you have some blockage in that so first of all you need to calculate the calculate the the the the the the partial derivative of J with respect to a12 so the activation function of this so if you're seeing that you to calculate the first of all you have to go over here multiplied by as your as we invoke the chain rules so basically this a12 so first of all you take out the partial de derivative of J because over here over here if you if you want to go back then over here you have two things Z and a okay and Z this this is nothing but A2 like this okay so that's what we need to do so that's what we need to do is is is just take out the partial derivative of J with respect to a12 after that now we need to C because we still haven't reached there we still haven't reached there to our W12 so basically we calculated the partial derivative of A1 this one this uh this this one with respect to this okay because we need to calculate the partial der of that so we calculate that and then using Z1 and then using Z1 we can go to this parameter to know how this is changing okay so basically let's let's so basically here's the calculation which is involved so here's our cost function where where I have for Simplicity I have written this of course and then you take out the partial derivative of J with respect to a12 so basically what I what I'm doing over here so basically what what I'm doing over here is to using the the chain rule of calculus and the power rule ofal calculus to perform this calculation so basically what what I'm doing is is performing uh performing the first of all the the partial derivative of the inner function of the inner function inner function and the partial derivative or The Taking of the derivative not partial derivative the derivative of inner function leaving the outside function as it is sorry so sorry partial derivative outer function leaving the inner function as it is and and then taking out the times the partial the derivative of inner function so first of all when we take the the derivative of outer function so basically here is two so using the power rule we take it over here right and subtract Min 1 from this right so when when we 2 2 * 1 by 2 that is cuts down so we are left with uh y h of X now that that is our the derivative of outer variable now outer the outer function somehow because over here we have something but there is something which is one / two something something and this is two so the derivative of outer function which where we take two over here multiply with it as I said as you have listened me that I told in gradient when when when when do in gradient send it canceled out so that's why we do that and then we leave this as it is times your your the derivative of inner in inner function okay so when when you take out the derivative of inner function when when you take out the derivative of inner function this why this this this this y have uh IID say uh i' say if you to take out with a12 so this so This eventually becomes 1 1 + 0 and 0 is nothing but I would say uh the the the reason why we got zero because the the derivative of any uh inner in inner uh constant is zero and here we are using the sum rule because we are applying the derivative of n on every function and then the reason why we have minus one because uh when you multiply this this this minus one with this so you so it'll be left with this final formula which is a partial derivative of J with respect to a12 so that's that's your that's your the the partial derivative which you got okay now this just tells us how much cost changes when that activation function or a to1 changes okay that's what what you got over here now you can plug in the values to get that value right so when you plug in the values so basically put put the minus sign as it is this this is a ground Ruth and this is a model predicted which you get which is 0.74 right so that's you you got the value of this one now you go the same for the derivative of A1 2 with respect to Z1 so basically this is your as usual sigmo function and we have already derived this in our intro to NN chapter so when you take out the derivative of your activation function this something looks like like this and then when you take out the and then you put in the values of sigmoid of Z1 so basically your Z1 was what what what what was your Z1 we have just calculated priorly when doing for propagation so it it was this times minus 1 1 minus this and then you get your final output which is 0.186 which you got the value of this as well now the last one which you're left with is z12 where we are simply the the the the the the z12 is this so we going to take out the partial Dera of z12 with respect to this variable sorry this parameter so basically when you calculate that when you calculate that you are one left with you're one left with uh this this particular value which is uh H1 one okay that's that's that's that's that's what you're left with let let me just make you sure that what exactly your left with here so when you when you go back here so the when then you take out the partial derivative of z12 with respect to w11 then you left is f 5 93 because uh that that uh because that's that's a constant right and and you have XI which is left so when when you do the calculation you'll be getting this 0.59 which is quite well to understand okay so that's a that's the first that's the first parameter updation now what I what I'm going to do is to just change the little bit of calculation so that that you are you up to the standards of the calculation which are going on so Delta rule the Delta rule is when you take out the partial derivative of J with respect to W1 as how much cost changes when that changes right so that is which we have just taken out which we have just taken out as you're seeing over here we are just just taken out we have this so we can fill in the formulas W12 that is a ground Thro minus your actual value sorry sorry yeah mod predicted value and we have minus over just we add little bit of notations times this one which is uh which is partial der Z1 with two and then Etc which is the which is the partial derivative of your segment function which you got okay this this formula and this formula and then in this when you take out the partial derivative of Z1 with respect to whatever with respect to your parameter you have the output as a uh a12 so basically when you when go there uh uh this a12 which is a11 so basically that uh that particular value so yeah so basically you're you're left with a11 which is over here okay so basically you left with a11 and then you using that a11 you you get that W12 so basically using that A1 because this h11 have the a11 of course you you h11 is an output of your z11 and and a11 right so your a11 is the final output of course so basically you have this a11 over there if you're confused a little little bit on that so let me just make sure that you're that that that you're that you're understanding what I'm saying so over here I think this there is a slightly change it should be 7 six whatever but but basically we are we are going here and this is where we are we we are using uh this is from this is where we have this uh h11 basically where we're going backwards right so with here we have Z as w11 uh W2 * whatever uh the the output from a11 right so this is your constant of this so that is your a11 is which is an output when if you studied about calculus then then you might be familiar with this so that's your partial derivative of Z1 so sorry U yeah so so basically uh that that that that was a part derivative of z11 with Z with respect to your parameter W and please be sure that I'm making lots of Errors over here because I think that it's it's very tough to at least pronounce their name but no problem in that but now we can write this into Delta notation and Delta notation here we can do we can take this uh and put that into a Delta 012 which is the which for that variable right and then multiply it by a11 and that's your partial deriva of J with respect to w211 okay so that's that's a Delta rule which you also see in real world so now we can just go ahead and and and and do all the calculations so we had taken out all all all of these in over here now what we can do we can we can multiply with that and then we get a final output and now what we can do we can update our uh weight because that because now we need to update our previous weight right so that's the that's the first update which we going to do let let us assume we have taken a learning rate which is 0.5 which is fairly a large learning rate but previously our weight was 0.4 but minus uh which 0 0.5 times the derivative okay and then you get the new weight okay previously 0.4 now it was now it is 0.35 which optimizes the error now what I'm going to do now what I'm going to do is um is just have something which is partial derivative of J with respect to w21 and basically over here what we are doing is take out first of all go back and then first of all take out a21 because over here you have something like this yes right so so basically you need to go back and this this is for a21 right so basically this is uh this is for uh second uh second weight going into first neuron as far as I know right so basically you have this a12 which is the second uh new second activation function so basically you take out a partial deriva of J with respect to Second activation functions you can do the calculation Now by your own and then using that you go to over here and using Z you take out the partial derivative of Z with respect to w21 which which tells how much G changes when this parameter changes and when you do that you'll get this 0.822 and when you when you when you update your parameter by the old one by minus 0.5 which is the learning rate you get the 0.40 which is uh pre previous one was 0.45 now was 0.40 right you do the same for the other two for the second for the second layer neural network for the second layer uh out for for for the output layer weights so you can do in this way the same way the same will follow up right and and there is something called a memorization there is something called memorization which I just just want to tell is when you take out say say for an example you take out the partial derivative of J A1 to and and your previous iteration you will will come to that the instance where you'll see the partial derivative already taken which which can be used in next iteration so we just make use of that rather than read read read redoing the partial uh the partial derivative of that so so that's what I we just make use of that of the of the one res all cre but but there will be a separate video coming up coming up on this so we had now updated our all uh parameters which is all the parameters of this till now now we need to update the parameters of this because we are back propagating back propagating over here so now we're updating the values of the parameters of the first hidden layer so basically we have to calculate the partial derivative of J with respect to w11 uh with for the first uh parameter so basically what I'm going to do is partial derivative of J with respect to a11 right first of all we need to go there times the partial derivative of J with respect a11 with respect to Z1 because we thebr we also go to Z1 and then using Z1 we go to W1 okay so as you know we first go here we first go here we first go here the partial derivative of J with respect to the G with respect to uh A1 a11 and then and then using this we go over here uh you take out the partial derivative of this with respect to Z uh 1 one right and then using this we take out the partial derivative of Z with respect to this and this this this this tells us if and using the chain rule as I told this is a chain Rule and there is a some notion called intermediate variables and and all those stuff which I which I'll come to that later on but basically this is the basic idea if something is causing blockage that's that's the chain rule which if you have already known about that so you take up when when when you do the calculation you'll get your partial Del of J like how much cost changes when w11 changes one superscript one and Sub sub subscript one one changes so now if you if you go over here if if you go over here uh for calculating the partial derivative of J with respect to a11 a11 so there a one blockage which is causing over there so we need to calculate the partial derivative of e eo1 because that's that's our output with respect to a11 right plus the the the partial derivative of e O2 with respect to A1 okay because as we know as we go over here and from this side as well as I told in your previous section uh in in in our previous lecture so that's that's the path because there are two paths which which we can go further so basically the part should derivative of Z with a11 so here's here's your two paths the first path for for going to a11 and this is the second path for for for for going to the Z1 like this and like this right as I told so first let's take out this partial derivative E1 with respect to a11 so when you when you take out the partial dtive E1 that is your a that that is nothing but uh a one 2 that's that's that's what it is right that's that's the output of it right uh as far as I know that's that's a soorry so sorry uh uh just just wait for a second let me confirm it if I'm correct over here maybe I should uh yeah I guess that I'm correct 0.75 yeah I'm correct I think so basically this is this is not this is not an error I think you can ignore this this is an output output of the of the of the out this is basically an output not an error please ignore this as of now this is an output not an error okay so basically this is an output not an error between the ground throw this just an output which is a21 a12 okay so and this is a22 okay so that's that's activation for which you get from output now basically when so basically you have to go from here to here so basically you are here so you you take the partial derivative of this with respect to this and then using this you go over here that's what we need to do right so you do the same and then for taking of the partial derivative of eo1 with respect to z12 right so z z z12 we need to first of all take out the partial derivative of a11 with respect to a12 times the partial derivative of A1 to divided by so sorry with with respect to z12 so basically this is quite confusing as of now I know but basically uh what we are doing we have two different paths to go above okay we have two different paths to go above sit down and see where it is going okay sit down and see where it is going and then you'll understand that we have a two paths where it going from this side uh from from from from from this side and from this side and you'll be seeing that we have taken from the this side this is partial derivative E1 with respect to a11 and this is the and and and when you take out the partial derivative for from this side as well okay from this side as well right so so so basically what what you can do is uh have this particular which you're seeing which is your uh which you can calculate that and then you got your final so basically you can plug in the values of that you can plug in the values of that because we need two these two for calculating the Z how much a changes when this changes plug in the values and and then we get our values and then we simply plus that because because that's the two path to go at that a11 okay so after that we done with this now we Cal so basically we we done with calculating the partial J with respect to a11 partial Dera of J a11 because it involves lots lots of calculations not only from this side it will go from this side as well if there's a two paths for that and that's why back back propagation is one of the hardest stuff in deep learning but it but but but it comes over uh learning all the stuff I suggest you to maybe read a Blog with it as well as watch the video that would make more sense uh so basically now we done with that so we take out the partial derivative of a11 with respect to z11 because without that we can't go so basically you all you the partial derivative of your Sigma function is always you know and then you have a z when you take out that which gives X1 which is output now you can simply plug in the values which is 0.036 time 0.24 which is another derivative then times 0.05 and then you get your output as a final output okay so that's the derivative of how much J changes when w11 changes and now what you going to do now what you going to do is just is just update your values 1 one is equals to W is new one this is the old one right this is the old one minus you're learning at Alpha and the D derivative of J with respect to 1 one okay and this is your out put which is 0.04 and then you can calculate for 1 one same you can do with W uh W uh W one two for the first layer but you need to identify the paths right the number of paths and then do it do it in that way okay so that's what the the iteration of back propagation means so we did a one one iteration of back propagation and I hope that this this this was a really nice session uh basically I just uh basically uh this was a a really really nice session on back back propagation uh if you like this lecture let me know and if if you have any questions or any doubt
