With timestamps:

00:00 - in this comprehensive course on
00:01 - algorithmic trading you will learn about
00:04 - three cuttingedge trading strategies to
00:07 - enhance your financial toolkit latchezar
00:10 - teaches this course he is an experienced
00:13 - quantitative researcher and data
00:15 - scientist in the first module you'll
00:17 - explore the unsupervised learning
00:19 - trading strategy utilizing SP 500 stocks
00:23 - data to master features indicators and
00:26 - portfolio optimization next you'll
00:28 - leverage the power of social media with
00:31 - the Twitter sentiment investing strategy
00:34 - ranking NASDAQ stocks based on
00:36 - engagement and evaluating performance
00:38 - against the QQQ return lastly the
00:42 - intraday strategy will introduce you to
00:44 - the gar model combining it with
00:46 - technical indicators to capture both
00:48 - daily and intraday signals for potential
00:52 - lucrative positions hello and welcome to
00:54 - this free code Camp course on
00:56 - algorithmic trading machine learning and
00:59 - Quant strategy with python my name is
01:01 - Lazar and I'll be your instructor toout
01:03 - the course and in this course we are
01:06 - going to develop three big quantitative
01:09 - projects from start to end and the
01:11 - course overview would be the following
01:14 - first we're going to talk about
01:15 - algorithmic trading Basics then we're
01:18 - going to talk about machine learning in
01:20 - trading some obstacles and challenges we
01:23 - may face while using machine learning in
01:25 - trading then we are going to develop the
01:27 - first project which would be an unsup
01:30 - rised learning trading strategy using
01:32 - stocks from S&P 500 the next project
01:36 - would be Twitter sentiment and it would
01:38 - be using data from NASDAQ 100 stocks and
01:41 - the third one would be focusing on one
01:43 - asset it would be an intraday strategy
01:46 - using a gar model to predict the
01:48 - volatility it would use simulated data
01:52 - and after that we are going to have a
01:54 - quick wrap up and with that we'll finish
01:56 - the tutorial but before we continue I
01:59 - would like to mention that this tutorial
02:02 - should not be ConEd as a financial
02:04 - advice it is for educational and
02:07 - entertainment purposes only we are going
02:09 - to develop some Concepts and come up
02:11 - with strategies in the end but it's not
02:14 - a financial advice and you shouldn't
02:16 - make any decisions based on it as well
02:18 - for this course I would assume that you
02:20 - have at least some python knowledge and
02:22 - understanding because down the road will
02:24 - deal with some complex problems and if
02:27 - you're new to python you may get B down
02:29 - nevertheless you learn some very
02:32 - interesting Concepts so stay along and
02:35 - let's get into it okay algorithmic
02:37 - trading Basics so what is algorithmic
02:39 - trading it is trading on predefined set
02:42 - of rules which are combined into a
02:44 - strategy or a system it is developed by
02:47 - a programming language and it is run by
02:49 - the computer it can be used for both
02:52 - manual and automated trading by manual
02:55 - what I mean is that you may have a
02:57 - screener which comes up with a set of
02:59 - stock stocks you want to trade on a
03:00 - given day or you may have a algorithmic
03:03 - strategy which is developed into an
03:05 - alert system and whenever the conditions
03:07 - are triggered you get an alert but you
03:09 - execute it manually on the other hand
03:11 - you may have a completely automated
03:14 - complex system which does a lot of
03:15 - calculation comes up with uh positions
03:18 - and sizing and then executes the trade
03:22 - automatically okay so what is the role
03:24 - of python in algorithmic trading python
03:27 - is the most popular language used in
03:29 - Alor trading quantitative finance and
03:32 - data science and this is mainly due to
03:35 - the vast amount of libraries that are
03:36 - developed in python as well the ease of
03:39 - use of uh python it is mainly used for
03:43 - data pipelines research back testing
03:46 - strategies as well it can be used to
03:48 - automate strategies but python is a slow
03:51 - language and it can be used to automate
03:53 - low complexity systems if you have a
03:56 - really high-end system which is really
03:58 - complicated and it needs to execute
04:01 - trades really quickly you would use Java
04:04 - or C++ for those strategies our gmic
04:08 - trading is a great career opportunity
04:10 - it's a huge industry there are a lot of
04:12 - jobs with hedge funds Banks Prop Shops
04:15 - and I've just checked the average yearly
04:18 - base salary for Quant researcher is
04:21 - around
04:23 - $173,000 and this is not including the
04:26 - yearly bonus it is a great career
04:28 - opportunity and if you're interested
04:30 - into it the main things you need to know
04:32 - are python you need to know how to back
04:35 - test strategies you need to know how to
04:37 - replicate papers and you need to know
04:40 - machine learning in trading if you're
04:42 - interested into it I'll definitely
04:44 - advise you to go for it okay so let's
04:47 - move on and talk a little bit about
04:49 - machine learning in trading and some use
04:52 - cases of machine learning when we talk
04:54 - about supervised learning we can use it
04:57 - for Signal generation through prediction
05:01 - for example we can come up with buy or
05:03 - sell signals on a given stock or a given
05:06 - asset based on predicting the return or
05:10 - the sign of the return of that asset we
05:13 - can as well use it in risk management
05:16 - for example we may use a prediction to
05:19 - determine the position sizing or the
05:21 - weight of a given stock in our portfolio
05:24 - or to predict where exactly should our
05:26 - stop loss be and with unsup learning we
05:30 - can use it to extract insights from the
05:33 - data for example we can discover
05:35 - patterns relationships or structures
05:38 - within the data for example clusters and
05:41 - use it in this way to uh help our
05:44 - decisions what are some of the
05:45 - challenges that we may face while trying
05:47 - to apply machine learning in trading and
05:50 - the first theoretical challenge we may
05:52 - face is the so called reflexivity
05:54 - feedback loop and it is referring to the
05:57 - phenomenon that if we for example have a
05:59 - machine learning model that is uh
06:02 - predicting that a stock is going to go
06:04 - up each Friday and we can form a
06:06 - strategy around to profit from that
06:08 - phenomenon for example we are going to
06:11 - buy each Thursday and then sell on
06:13 - Friday to capture that price increase
06:15 - move if we find this strategy throughout
06:18 - predictions and start trading it with
06:20 - time other Market participants as well
06:23 - are going to find this Market phenomenon
06:26 - and start to exploiting it as well which
06:28 - would cause the price to start going up
06:31 - on Thursday because everybody's buying
06:33 - now on Thursday instead on Friday and
06:35 - then this strategy is going to be
06:37 - Arbitrage away so it's this reflexivity
06:40 - feedback loop which is making
06:42 - predictions quite hard what is most hard
06:45 - while applying machine learning the most
06:47 - hard thing is to predict returns and
06:50 - predict prices the next quite hard thing
06:53 - to do is to predict return signs or the
06:57 - direction of a given asset is it going
06:59 - to to go up or down the next thing is to
07:02 - predict an economic indicator for
07:04 - example it is quite hard to predict
07:06 - nonfarm payrolls or weekly jobless
07:09 - claims and a thing which is not that
07:11 - hard or quite straightforward is to
07:14 - predict the volatility of a given asset
07:17 - furthermore there are some technical
07:18 - challenges like overfitting a model or
07:22 - generalization overfitting is that the
07:24 - model is learned the train data too well
07:26 - and it fails on the test data and
07:29 - generalization is that the model is not
07:31 - performing the same as on the real data
07:34 - as well we may have nonstationarity in
07:36 - our training data and regime shifts
07:39 - which may ruin the the performance of
07:41 - the model and the last thing is that if
07:44 - we have a really complicated model or um
07:47 - neuron Network it is like a black box
07:49 - and we are not able to interpret it
07:52 - correctly what is the usual workflow
07:54 - process in algorithmic trading and
07:56 - machine learning that would be to
07:57 - collect and prepare the the data then
08:00 - develop a hypothesis for a strategy then
08:04 - you have to code the model and train the
08:06 - model and then finally back test the
08:09 - strategy okay guys and some key
08:11 - takeaways from this course so you learn
08:14 - high level Concepts in quantitative
08:15 - Finance as well practical machine
08:18 - learning in trading you develop a
08:20 - project from idea to back test final
08:23 - results however we will not automate or
08:26 - execute any trades we'll just develop a
08:29 - strategy this course is all about
08:31 - developing a strategy from start to end
08:34 - so you see the workflow and this is just
08:37 - a purely research project for
08:40 - educational purposes I repeat it should
08:42 - not be construed as any Financial advice
08:46 - whatsoever and with that we can move to
08:48 - the first project and let's get into it
08:51 - okay so first project is about
08:53 - unsupervised machine learning trading
08:56 - strategy we're going to use data from
08:58 - S&P 5 500 stocks let's talk a little bit
09:01 - about unsupervised learning in trading
09:03 - so it involves machine learning
09:05 - techniques to analyze financial data and
09:07 - discover patterns relationships and
09:09 - structures within this data without
09:12 - predefined labels or Target variable
09:15 - unlike supervised learning where the
09:17 - model is trained to make predictions
09:20 - unsupervised learning focusing on
09:22 - extracting insides from the data and
09:24 - some use cases would be clustering which
09:27 - we are going to use in the first project
09:29 - dimensionality reduction anomaly
09:31 - detection Market regime detection and
09:34 - portfolio optimization in this project
09:36 - what we are going to do is first we're
09:38 - going to download all the prices data
09:40 - for all S&P 500 stocks then we're going
09:43 - to calculate different technical
09:44 - indicators and features for each stock
09:47 - next we're going to aggregate on a
09:49 - monthly level and filter only the
09:53 - top50 most liquid stocks from the S&P
09:56 - 500 for each month next we are going to
09:58 - calculate PL monthly returns for
10:01 - different time Horizons to add up to the
10:03 - features and the next step would be to
10:05 - download the F French factors and
10:08 - calculate rolling Factor betas for each
10:10 - stock as well to add to the feature set
10:13 - and at this point we'll have enough
10:15 - features to fit the model and either
10:18 - make predictions or in our case we're
10:21 - going to fit a c's clustering algorithm
10:25 - an unsupervised learning model and we
10:27 - will use it to group group the stocks
10:30 - into similar assets into clusters and
10:33 - from those clusters then we will be able
10:35 - to for each month select stocks from a
10:38 - given cluster and we are going to
10:41 - analyze the Clusters and select a
10:42 - particular cluster and then for each
10:45 - month we are going to select those
10:46 - stocks within this cluster and form
10:48 - portfolios however these portfolios will
10:51 - be optimized so the weights of the
10:53 - stocks within the portfolio we're going
10:55 - to find them by using the efficient
10:56 - Frontier Max Sharpie ratio isue
10:59 - portfolio weights and then we are going
11:01 - to form the portfolio hold for one month
11:04 - and rebalance at the end of the month
11:06 - and you know form another Max sharp
11:08 - ratio portfolio in the end we'll have
11:11 - the strategy returns for each day and we
11:14 - will be able to compare our portfolio
11:16 - strategy returns to the S&P 500 returns
11:19 - themselves actually not small limitation
11:22 - is that we're going to use the most
11:24 - recent S&P 500 stocks list which means
11:26 - that there may be a survivor buyers in
11:29 - this list this is a huge issue actually
11:32 - in reality you should always uh back
11:34 - test strategies using survivorship free
11:37 - bias data what is survivorship bias it
11:40 - is the condition when a stock which have
11:43 - actually went out of the S&P 500 because
11:46 - it was failing is currently not in the
11:49 - list right so last year for example
11:51 - there was a stock which was failing and
11:53 - going down down down so at some point in
11:55 - December last year they removed it from
11:57 - S&P 500 and they included a new stock so
12:01 - if we have made the optimization last
12:03 - November we could end up with having
12:06 - this stock into our portfolio and
12:08 - actually it affecting our portfolio
12:11 - results but if we use the most recent
12:14 - S&P 500 symbols list this stock would
12:17 - not be there so that's survivorship bias
12:20 - and for the given project we are not
12:22 - going to deal with this survivorship bu
12:24 - so the list we are going to work with
12:25 - has survivorship bus most probably so
12:28 - yeah that is limitation you need to know
12:30 - in the second project we are going to
12:32 - develop a Twitter sentiment based
12:34 - investing strategy we are going to use
12:37 - the NASDAQ 100 stocks and Twitter
12:40 - sentiment data what is sentiment
12:42 - investing this approach focuses on
12:44 - analizing how people feel about certain
12:46 - stocks Industries or the overall Market
12:49 - it assumes that the public sentiment can
12:51 - impact stock prices and for example if
12:55 - many people are positive about a
12:57 - particular company on Twitter it might
12:59 - indicate potential for that company
13:01 - stock to perform well what we are going
13:04 - to do first we're going to load the
13:06 - NASDAQ stocks Twitter sentiment data
13:09 - then we're going to calculate a
13:10 - quantitative feature of the engagement
13:13 - ratio in Twitter for each stock after
13:15 - that we're going to rank all the stocks
13:18 - cross-sectionally for each month and
13:20 - create an equal weight portfolio in the
13:22 - end we're going to compare the return of
13:25 - this portfolio to the NASDAQ itself the
13:27 - second strategy is much smaller than the
13:29 - first strategy there is no machine
13:31 - learning modeling in this strategy but
13:33 - the idea here is to show you how
13:35 - alternative or different data in this
13:37 - case sentiment data can help us to
13:39 - create a Quant feature and then create
13:42 - potential strategy out of it that's the
13:44 - idea of the second project and the third
13:46 - project is about an intraday strategy
13:49 - using a gar model in this one we are
13:52 - going to focus on a single asset and
13:54 - we'll be using simulated data actually
13:57 - we'll have daily data and intraday 5
14:00 - minute data but what does an intraday
14:02 - strategy means this approach involves
14:04 - buying and selling Financial assets
14:06 - within the same trading day to profit
14:08 - from the shortterm price movements
14:11 - intraday Traders usually use technical
14:13 - analysis real time data and different
14:15 - risk management techniques to make
14:17 - decisions and profit from the strategies
14:20 - what exactly we're going to do in this
14:22 - project first we are going to load the
14:24 - simulated daily data and the simulated 5
14:27 - minute data then we are going to define
14:29 - a function which would fit in a rolling
14:32 - window a gar model to predict one day
14:35 - ahead volatility of the asset after we
14:38 - have that we'll calculate prediction
14:40 - premium so we'll predict the volatility
14:42 - and we'll calculate the prediction
14:43 - premium and form a daily signal from it
14:46 - after we have uh calculate that then
14:48 - we'll merge the daily data with the
14:50 - intraday data and calculate intraday
14:53 - technical indicators to form intraday
14:56 - signal so we will have a daily signal
14:59 - and then on top an intraday signal so
15:02 - two signals and after we have that we'll
15:04 - generate position entry and hold until
15:08 - the end of the day in the end we're
15:09 - going to calculate the final strategy
15:11 - returns and this is it for the third
15:14 - project the idea is to show you how
15:16 - predicting volatility Works in uh
15:18 - intraday strategies and yeah with that
15:21 - guys we are ready to jump into the first
15:23 - project so let's get into it okay let's
15:26 - start with the first project the unup
15:28 - rised learning trading strategy but
15:31 - before we continue with the coding guys
15:33 - the first step would be for you to pause
15:35 - the video and install all the needed
15:38 - packages for this project so I prepared
15:41 - a small list up here with all the
15:43 - packages we are going to need for the
15:45 - project so those are pandas numai MP lip
15:48 - stats models pandas data reader daytime
15:51 - wi Finance SK learn and buy portfolio op
15:55 - how you can install the packages you can
15:57 - do it the following way you can open on
15:59 - Ana prompt like that right and then just
16:02 - write pip install and the package name
16:05 - pip install pandas for example and do
16:07 - that for each package that's one way
16:09 - another way to do it is through the
16:11 - notebook itself you can uh type the
16:13 - following so pip install and then the
16:17 - package name so pip install pandas you
16:19 - do control shift and run the cell and it
16:21 - will install the package pause the video
16:23 - take your time install all the packages
16:26 - and let's continue okay so the first
16:29 - step would be to download the S&P 500
16:32 - constituents prices data but before that
16:34 - we'll have to import all the packages we
16:37 - use throughout this project I've already
16:39 - prepared that those are all the packages
16:41 - we've just installed and now we are
16:43 - importing into the Jupiter notebook The
16:46 - Next Step would be to download S&P 500
16:49 - constituents data to do that we can go
16:51 - to this link on Wikipedia and up here
16:55 - you can see that they have a table
16:57 - containing a S&P 500 component stocks
17:00 - like the symbol the the name of the
17:02 - security sector industry date added as
17:05 - well so quite some data uh so we would
17:07 - like to load this table into our Jupiter
17:11 - notebook how we can do that we just call
17:14 - a pandas read HTML function and let's
17:17 - see what this would return us okay so
17:20 - this is returning a list containing two
17:23 - elements it looks like two data frames
17:26 - so actually we are interested in the
17:28 - first frame yeah exactly we will assign
17:31 - that to an object called S&P 500 and the
17:34 - next step would be to grab this symbol
17:38 - column and extract all the symbols in a
17:40 - list but before that I think we would
17:43 - have to make a little cleaning on some
17:46 - of the symbols because I know that one
17:49 - or two of the symbols contain a DOT and
17:53 - this would give us an error while we
17:55 - download data from wi Finance so we have
17:58 - to actually replace all dots with
18:03 - Dash and that would do the job next step
18:07 - would be to you know just grab list with
18:11 - all the stocks so yeah we can just use
18:13 - symbol. unique to list we will assign
18:17 - that to a object called
18:20 - symbols list and yeah that would be our
18:24 - symbols list of all S&P 500 stocks as we
18:28 - talked uh in the beginning this list of
18:31 - stocks is not Survivor ship bias free so
18:36 - you need to know that it's uh quite some
18:38 - of limitation all right so we would like
18:42 - to download the data up to a few days
18:46 - ago let's define an object which is end
18:49 - dat and we'll use
18:51 - 2023 September 27th and the start date
18:57 - would be
18:59 - P to date
19:01 - time and
19:03 - date it would be exactly 8 years ago so
19:07 - we can just use the end date and we will
19:12 - substract eight years out of it how we
19:14 - can do that we just say pandas to
19:17 - datetime we'll convert to daytime the
19:19 - end date and then we'll just substract
19:21 - pandas date of set and then Supply 365 *
19:27 - 8 all right so now we have the end date
19:31 - which is yeah 27th of September which is
19:34 - a string and the start date would be a
19:36 - time stamp but that's all right because
19:39 - we are going to download data from y
19:42 - finance and this function we use from y
19:46 - Finance download function which takes
19:48 - tickers as an argument so that would be
19:51 - our symbols list then start which would
19:53 - be our start date and end which would be
19:57 - our end date and now this will download
20:01 - all the S&P 500 constituents if we run
20:04 - it yeah it will take some time all right
20:07 - so we've downloaded all the data for the
20:09 - S&P 500 stocks we got it up here so the
20:12 - next step would be to actually yeah
20:15 - we'll comment out everything and have uh
20:17 - this object printed out as you can see
20:20 - up here we have the column and then we
20:22 - have like a multi index column so first
20:25 - we have the adjusted close and then we
20:27 - have the adjusted close for each stock
20:29 - that's not really convenient to work
20:30 - with the whole data frame at the moment
20:32 - we have 2012 rows and 3,8 columns that's
20:37 - really inefficient how we can overcome
20:39 - that we would just use the stock method
20:43 - which now creates a m index the first
20:46 - level would be the date and then for
20:48 - each date we have the corresponding 500
20:51 - stocks adjusted close close high low so
20:54 - that's much more convenient we have six
20:55 - columns and almost 1 million rows so we
20:59 - have to change the datea frame to be
21:00 - stocked or actually we can move this
21:03 - method right up here after the download
21:06 - directly and yeah now we have our data
21:08 - frame stacked Next Step would be I would
21:11 - always when I have a multi-index I would
21:13 - always want to have labels on both of
21:16 - the index levels so we'll say index.
21:18 - names and we'll assign the new name so
21:21 - date and thicker that will be our new
21:25 - multi-index names as you see they change
21:28 - step here next step would be to fix the
21:31 - columns a little bit I I would like to
21:33 - fix the column names to be not as titles
21:36 - but with a lower letters so that would
21:39 - be DF do columns DF do columns do string
21:44 - lower and that's pretty much it with the
21:47 - downloading and fixing data a little bit
21:50 - before we move to the next step which
21:52 - would be to start calculating technical
21:54 - indicators and features of all those
21:58 - 53 stocks okay so in the second step we
22:02 - can start calculating the features and
22:04 - Technical indicators for each stock we
22:07 - are going to calculate the garm class
22:09 - volatility RSI Ballinger bands ATR macd
22:13 - and dollar volume for each stock let's
22:15 - first start with the garm class
22:17 - volatility what is garm class volatility
22:19 - it is volatility measure usually used in
22:22 - Forex Trading but it works for stocks as
22:24 - well it is approximation to measure the
22:27 - intraday volatility of a given asset and
22:29 - that is the formula right here so what
22:32 - do we do we Define a new column called
22:35 - garmon class V and it would be the
22:40 - following so log from the high minus log
22:46 - from the low so that would be yeah from
22:49 - the low this whole thing is squared and
22:53 - then it is divided by two then from this
22:56 - we substract 2 * log 2us one multiplied
23:03 - by subtraction of log from the adjusted
23:07 - close yeah
23:09 - minus log from the
23:12 - open again the hold think is squared and
23:16 - we just close another bracket and that
23:19 - should be it that should be the gar
23:22 - class volatility yes so we now have
23:24 - calculated the gar class volatility for
23:26 - each stock it is calculated on a given
23:29 - role we don't need to do any fancy
23:32 - calculations for this one the next one
23:34 - is RSI so how do we calculate the RSI on
23:38 - each stock what we're going to do we are
23:40 - going to group Buy on the thicker level
23:44 - so level one the multi- index has level
23:47 - zero and level one which is the thicker
23:50 - level zero is the date level one is the
23:51 - thicker so we grew by on level one then
23:54 - we are selecting the column which would
23:55 - be adjusted close and apply the
23:58 - transform method and within the
23:59 - transform method we just apply a Lambda
24:01 - function which would be so now we have
24:04 - grouped grouped by on each thicker and
24:07 - what do we want to do we want to
24:09 - calculate the RSI to calculate the RSI
24:12 - we are going to use the pandascore TA
24:16 - package which is the package to
24:18 - calculate pretty much all of the needed
24:21 - technical indicator so we use the
24:23 - pandascore TA package and from the
24:25 - pandascore we use the RSI function the
24:28 - side function we have to supply the
24:30 - close price which would be X and the
24:33 - length would be 20 and yeah if we do
24:37 - that you see now we have the
24:40 - RSI for each stock how we can double
24:44 - check our work we select
24:47 - apple and then RSI and then we'll plot
24:51 - it and yeah as you can see the r side
24:53 - goes up and down up and down so we have
24:56 - worked correctly the next indicator we
24:58 - would like to calculate is Ballinger
25:01 - bands and actually we would want to have
25:04 - the lower band the middle band and the
25:07 - upper band but there is one
25:10 - specification for each indicator from
25:14 - now on we would like to normalize and
25:18 - scale the indicator itself so for the
25:21 - Binger BNS will will supply the log from
25:25 - the close price first the function we
25:27 - are going to use is from Panda CA B
25:29 - bands and it is taking the clothes just
25:32 - for presentational purposes we Supply a
25:35 - upper adjusted close and the length will
25:39 - be 20 when you run this function it
25:42 - returns five columns so the first one is
25:44 - the lower band the second one is the
25:46 - middle band and the third one is the
25:48 - upper band we have to take this into
25:50 - account and what we are going to do is
25:53 - pretty much you know Define a new column
25:56 - BBL low Ballinger band low and we use
25:59 - the the same idea as for the RSI we are
26:02 - going to group by each thicker select
26:05 - the adjusted close column and then we
26:06 - use the transform method Lambda function
26:09 - and in the Lambda function we say pandas
26:11 - ta B bands close would be equal to X
26:14 - actually it will be equal to log of x so
26:18 - MP log 1 p and then the length would be
26:23 - again 20 when we run that it will return
26:27 - those five columns what we actually want
26:29 - to do is assign to B below the First
26:32 - Column which we know is the B the lower
26:34 - Ballinger band we can say iock all the
26:37 - rows and the First Column we can repeat
26:41 - the same operation for the midband and
26:44 - for the upper band okay but this happens
26:48 - when I forget something and I think I
26:50 - have forgot I forgot the second curly
26:53 - bracket to close the curly brackets
26:55 - let's see what this would return yeah
26:57 - guys after you return and calculate
26:59 - something you can just select all the
27:02 - the code you've used and comment it out
27:04 - okay we forgot to change the names that
27:07 - would be BB mid and first index that
27:11 - would be BB High the second index okay
27:14 - let's run that again and see what we get
27:18 - awesome now we have the lower bager Bond
27:21 - Middle Ballinger Bond and upper
27:23 - Ballinger Bond and we have the data
27:25 - scaled and normalized next step is to
27:29 - calculate the ATR for each stock however
27:33 - the ATR function needs three inputs so
27:35 - three columns not only one column and
27:37 - when we use transform method in pandas
27:40 - it is actually working when you select
27:42 - only one column it would not work if you
27:45 - have three columns as an input so we
27:48 - would have to use another approach more
27:50 - specifically that would be a group by
27:52 - apply and to do that we need to Define
27:55 - our own custom function to calculate the
27:58 - ATR we can uh double check what the ATR
28:01 - function from pandas ta requires as
28:03 - input and it is the high so we have to
28:05 - supply the high then we have to supply
28:08 - the low and the close price furthermore
28:12 - we can supply the length for example 14
28:16 - and yeah it will mess mess up with the
28:18 - data if we run it like that because we
28:20 - have to select the data for a given
28:22 - stock but yeah you see that it requires
28:25 - three columns so here we we going to
28:28 - define a function called compute ATR it
28:31 - will take stock data and here we will
28:34 - just calculate at which would be pandas
28:38 - ta. high would be stock data
28:42 - High the low then we have we need a
28:45 - close the length would be 14 all right
28:48 - and a little detail we are going to add
28:50 - of here is we are going to normalize the
28:52 - data while we calculate it so that would
28:55 - be ATR do substract first first we are
28:57 - going to the mean and then we are going
29:00 - to divide it by the standard deviation
29:02 - so ATR do standard deviation and yeah
29:05 - that will be our ATR indicator function
29:10 - now we can just say ATR create a new
29:12 - column called ATR and group by level one
29:16 - again so we are applying that for each
29:18 - stock however up here when we use the
29:20 - group by apply we need to add an
29:23 - additional argument to the group bio
29:26 - which is group key is equal to false
29:29 - because if we don't do that it will
29:32 - double the the date column so it will
29:34 - return another date colum and we have a
29:36 - triple multi-index with two date columns
29:38 - we don't want that so we just say group
29:40 - Cas equals to false and then apply this
29:44 - function and this will now calculate the
29:46 - ATR index normalized for each stock the
29:49 - next indicator we are going to calculate
29:51 - is the macd indicator and for the macd
29:55 - we're going to follow the same logic as
29:57 - as for the ATR indicator we're going to
30:00 - Define our own custom function to
30:02 - compute the macd so it will be called
30:04 - compute macd it will take the close
30:07 - price and up here we are going to say
30:10 - magd is equals to pandas ta. magd close
30:15 - equal to close length is 20 and then we
30:19 - would like to get the First Column which
30:21 - would be returned however in the end as
30:24 - well we are going to normalize the dat
30:27 - we're going to the meain the series and
30:29 - then we're going to divide by the
30:31 - standard deviation why do we do that
30:33 - right away we're normalizing the data
30:35 - because we are going to use it into a
30:37 - machine learning model we are going to
30:39 - Cluster the data we want to do that
30:41 - straight away and don't think about it
30:44 - later in the future so here we are going
30:46 - to do again group buy level one each
30:49 - scker on each stock group Keys equal to
30:52 - false and then apply compute Mark D this
30:56 - would calculate calate the mag the
30:58 - indicator for each stock all right we
31:01 - have an error why do we have this error
31:04 - that's quite
31:05 - strange oh okay I'm sorry I'm sorry I
31:08 - forgot to add the adjusted close column
31:11 - up here and this is driving the eror all
31:16 - right so we will return back I lock in
31:21 - the first corn that should be guys all
31:25 - right yeah now we have the macd as well
31:27 - calculated and normalized as you can see
31:31 - we have the data looks pretty good so
31:33 - far the only indicator we are not going
31:36 - to normalize is the RSI and there is a
31:39 - particular reason for that but you uh
31:41 - understand more about it when we come to
31:44 - the clustering part all the other
31:46 - indicators we are going to normalize and
31:48 - the final one is the dollar volume so
31:50 - we're going to create a new column
31:53 - dollar volume which would be equal to
31:55 - the adjusted close multiplied by the
31:58 - volume however we may want to actually
32:02 - divide that by 1 million for each stock
32:05 - because we know that millions of shares
32:07 - are traded each day and this would make
32:10 - sense as you can see now the data looks
32:12 - much better for the dollar volume right
32:14 - and that's pretty much it with uh
32:16 - calculating the first batch of features
32:19 - our technical indicators now we have a
32:21 - really beautiful data frame for each day
32:23 - we have all the 500 stocks we have the
32:26 - close price low open volume garment
32:29 - class volatility RSI Ballinger band ATR
32:32 - macd and the dollar volume for each
32:36 - stock and we are now ready to move to
32:38 - the next and the third step in the third
32:41 - step what we want to do is to aggregate
32:43 - on a monthly level the data and filter
32:46 - the top 150 most liquid stocks for each
32:49 - month why do we do that we do that to
32:51 - reduce training time for any potential
32:54 - machine learning model and experiment
32:56 - with features and strategies what is my
32:59 - idea here I would like to aggregate all
33:02 - the indicators so those five I would
33:06 - like to take the end value the end of
33:09 - the last value for the month as well the
33:11 - same for the adjusted close price and
33:13 - for the dollar volume I would like to
33:16 - get the average dollar volume for the
33:18 - whole month for each stock we can start
33:20 - actually what we can do first is uh take
33:23 - the data frame un stock the thicker
33:26 - level so we'll unstack thicker and then
33:29 - we're going to select the dollar volume
33:32 - column and if we run that we have the
33:35 - dollar volume for each day for each
33:38 - stock now right and what we can do is
33:42 - just resample to monthly and take the
33:46 - mean this should resample to monthly now
33:49 - as you can see we have monthly index end
33:52 - of each month and we have the average
33:54 - dollar volume for the month what we can
33:57 - do now is just stack it back into a MTI
34:01 - index like that and we can say two frame
34:05 - to make it uh a data frame with one
34:08 - column two frame dollar volume beautiful
34:12 - that would be the first step however for
34:15 - the indicators what we can do is we can
34:19 - follow the same logic but we need to
34:22 - select the exact columns actually we may
34:27 - create a list of columns so last call
34:31 - and this would be our list of columns
34:34 - for which we want to do the same
34:36 - operation however we would use the last
34:39 - method up here instead of mean and those
34:42 - columns would be C for C in DF do
34:46 - columns.
34:48 - unique and yeah the first element C for
34:51 - C in do in DF columns. unique if C is
34:57 - not in the following columns list so if
35:01 - the column is not dollar volume it is
35:04 - not volume it is not open it is not high
35:10 - low or close pretty much we want to do
35:14 - that only for the technical indicators
35:17 - columns so for those columns we don't
35:19 - want to use these columns for our
35:22 - aggregation we want just the Fe we are
35:24 - creating the features data frame in the
35:26 - end right we would use the dollar volume
35:28 - to filter out the most liquid stocks the
35:30 - dollar volume would not be featured in
35:32 - our model as well NE neither the volume
35:35 - or open low high close after we have
35:37 - defined the last columns uh we can
35:40 - actually proceed with the next
35:43 - aggregation we have done the dollar
35:45 - volume aggregation the next one would be
35:48 - for DF last C we are going to
35:54 - unstack actually we are going to un
35:56 - stack before that so we are going to
35:59 - unstack we're going to unstack and
36:01 - select those columns and then resample
36:05 - to monthly and then just use the last
36:08 - volume like that and again we're going
36:11 - to stack backwards into a multi-index
36:15 - voila and now what we can do is concut
36:19 - those two together we can say the
36:23 - following pandas cona and that would be
36:28 - axis axis
36:30 - one and boom we have the dollar volume
36:34 - the average dollar volume and the last
36:37 - value for adjusted close ATR and other
36:40 - technical indicators for each month now
36:42 - we have aggregated the data to monthly
36:44 - level for the features we would need
36:48 - actually what we can uh what we can add
36:51 - up here is a small drop and that looks
36:55 - much more beautiful
36:57 - and we can call that
36:59 - data
37:00 - and let's visualize what we have yeah
37:05 - that's our data all right and that's the
37:09 - first step with the aggregating to a
37:11 - monthly level The Next Step would be to
37:14 - calculate the fiveyear rolling average
37:17 - dollar volume for each stock and then
37:20 - you use this uh aggregated dollar volume
37:24 - to filter out only the top
37:27 - 150 most liquid stocks for each month
37:32 - how do we approach that first we can
37:35 - start by selecting the dollar volume
37:39 - like that so what we can do select the
37:41 - dollar volume and stack the thicker
37:44 - level and now we can use a rolling
37:47 - function with a window of 5 * 12 so 5
37:51 - years and then we can calculate the mean
37:54 - as you can see now we have the rolling
37:56 - average
37:57 - mean for each stock rolling average
38:00 - 5year dollar volume for each stock and
38:04 - again we can just stack backwards and
38:07 - that's pretty much our dollar volume
38:09 - column what we can do now is we can
38:11 - assign it to the dollar volume column
38:13 - update to The fiveyear Rolling average
38:17 - for each stock awesome after we have
38:20 - that the next step is to calculate the
38:23 - dollar volume rank cross section
38:27 - for each month how do we do that we can
38:30 - say data Group by level zero or date so
38:34 - we can Group by on date for each month
38:36 - we're going to select the dollar volume
38:40 - and just rank ascending equal to false
38:46 - let's see what this would give
38:48 - us
38:50 - ascending and now as you can see we have
38:53 - all the stocks ranked by Dollar volume
38:56 - and the the ones the the guys who have
38:59 - the smallest dollar volume have the the
39:02 - highest rank so we want the top
39:05 - 150 and from here we can pretty much
39:09 - very easily select the stocks which are
39:14 - below 150 for each month and those would
39:18 - be the top50 most liquid stocks for each
39:22 - month after we have selected them we can
39:26 - just drop the two columns we can drop
39:29 - the dollar volume and the dollar volume
39:32 - rank because we are not going to need
39:34 - them anymore access equals to one and
39:38 - that's pretty much it for our we can
39:41 - assign that to
39:42 - data and that's pretty much it with our
39:45 - third step where now we have aggregated
39:48 - monthly data for all the features we
39:52 - would need plus the adjusted close price
39:55 - and we can move move on with the next
39:57 - step the fourth step and that would be
40:00 - calculating monthly returns for
40:02 - different time Horizons and add them as
40:05 - additional features to the ones we
40:07 - already have here okay so let's move to
40:10 - that step I'll just cut those sales in
40:13 - the middle all right why do we want to
40:15 - calculate the monthly returns for
40:17 - different time Horizons and uh add them
40:19 - to the feature set because we may want
40:22 - to capture time series dynamics that
40:24 - reflect for example the momentum
40:26 - patterns for each stock to do that we
40:28 - can just use the pandas data frame
40:31 - method uh percent underscore change and
40:34 - Supply the different logs my Approach
40:36 - would be to use logs for 1 month two
40:39 - months 3 months 6 months 9 months and 12
40:42 - months that's uh like six different uh
40:46 - LS to really capture the momentum
40:49 - patterns how do we approach that let's
40:51 - first start by for example selecting the
40:54 - Apple stock I just want to make a little
40:57 - example so we'll select the Apple stock
41:00 - and let's see what we have here all
41:03 - right now we would like to calculate the
41:07 - returns for the following lcks for one
41:09 - month two months 3 months 6 months 9
41:13 - months and 12 months right as well we
41:15 - may want to have an outlier cut off
41:18 - because we are dealing with a lot of
41:19 - stocks there will definitely be outlier
41:22 - values in the returns of those stocks
41:24 - what do we want to do we want to with
41:26 - them by clipping them what clipping does
41:29 - is that for all values which are above
41:32 - the outlier threshold they will just be
41:35 - assigned the threshold of that percent
41:38 - up the cut off value we may want to have
41:41 - it as a 0.05 which means the
41:45 - 99.5 percenti that would be our outlier
41:48 - cut off and now what we do is just for
41:51 - each log so for log in logs for each log
41:55 - we are going to create a column which
41:58 - would be the return for the given log
42:01 - for the given log month right that would
42:03 - be the column and then we grab the
42:05 - adjusted close then we just do percent
42:10 - change Lo so for each lck we'll
42:12 - calculate the following column which
42:14 - would calculate the given return and
42:17 - then we want to deal with the outliers
42:19 - right so that would be pipe Lambda so
42:22 - far we have the adjusted close and then
42:24 - we calculate the return for the given L
42:27 - and then we input that into the pipe
42:29 - Lambda X we can clip now we clip the
42:33 - return and we can clip the lower band
42:37 - the lower cut off we'll use x
42:40 - quantile and we supply for the lower cut
42:44 - off just outl cut off and for the upper
42:48 - one we Supply x quantile one minus
42:53 - outline cut off then we add one to the
42:57 - power of 1 / by the log and we substract
43:02 - one in the end and that should be it
43:05 - guys now we can see that for our Apple
43:08 - stock which is G right we have the one
43:11 - month return two months return 3 months
43:14 - 6 months 9 months 12 months Etc how we
43:16 - can extend that we just use the same
43:18 - approach we use for the at and the macd
43:21 - indicators we will create our own custom
43:24 - function and then we you just use the
43:26 - group by apply methodology for the bound
43:29 - data frame so calculate returns it takes
43:32 - DF for example and just move that a
43:36 - little bit return DF in the end however
43:39 - up here we have to change that to DF
43:42 - this one to DF as well and that's pretty
43:45 - much it our function and now we can say
43:48 - the following data equals to data. group
43:51 - by level one because we Group by on the
43:54 - thicker level so we can say uh Group by
43:57 - ticker or level one let's say level one
43:59 - and then we want Group keys to be false
44:03 - so we don't have two uh date indexes
44:06 - assigned to the new date frame so Group
44:08 - keys false then apply we apply the
44:13 - calculator T function and in the end we
44:16 - may want to drop the na values and yeah
44:20 - this would take some time and yeah
44:23 - that's pretty much it I mean now we have
44:26 - added the return features as well which
44:31 - we would use to capture momentum
44:34 - patterns for each stock and that's
44:37 - pretty much it with the fourth step the
44:40 - calculating the monthly returns for
44:41 - different on Horizons and we can move to
44:43 - the next step which is really
44:45 - interesting actually adding even more
44:47 - features to our data set that would be
44:50 - to download the F French factors and we
44:53 - are going to calculate the rolling
44:55 - Factor better for each stock in our data
44:58 - all right let's move to it okay so in
45:00 - this step we're going to download the
45:02 - farm French factors data and calculate
45:05 - the rolling Factor betas for each stock
45:07 - in our current data set so we want to
45:10 - introduce the F French data to estimate
45:13 - the exposure of our assets to commonly
45:15 - known risk factors and we're going to do
45:17 - that using a regression rolling OS model
45:20 - the five round French factors namely
45:22 - Market risk size value profitability and
45:25 - invest M have been shown to empirically
45:28 - explain asset returns in the past and
45:31 - are commonly used in the asset
45:33 - management industry to assess the risk
45:35 - return profile of different portfolios
45:38 - so it kind of makes sense to include
45:39 - them in our current feature data set how
45:42 - we can do that we can use the pandas
45:46 - data reader package we which we imported
45:49 - as web we can use this package to
45:52 - download the F French Factor models but
45:54 - before that we may want to to take a
45:56 - look at the we might want to take a look
45:58 - at the data so we can Google it Farm
46:01 - French factors and up here you can find
46:05 - the canid French data Library just click
46:07 - on it and this is the part we are
46:09 - interested in in FAL French 5 research
46:12 - factors 2 * three so those five factors
46:15 - that's the data we are interested in if
46:17 - you scroll down a little bit you can
46:18 - find the daily data right here you can
46:20 - download it as txt or CSV file as well
46:22 - you can check the details they have
46:24 - monthly returns and annual returns we
46:27 - are interested in the monthly returns as
46:29 - our data is already on a monthly level
46:32 - right so what we can do is uh we can say
46:35 - the following web. data reader data
46:38 - reader and up here we have to supply the
46:41 - name of the exact Factor so the exact
46:44 - file so I have prepared it already
46:46 - that's the name then we say F French
46:49 - then the start date we want 2010 this is
46:53 - returning a dictionary with two keys the
46:55 - first one is the monthly factors and the
46:58 - second one is the yearly factors that's
47:00 - awesome so we want the monthly factors
47:02 - only and that's the data we have 164 uh
47:06 - months up to August 2023 that's pretty
47:10 - good however I think we don't even
47:12 - though the risk-free return is pretty
47:16 - solid right now it is out of the scope
47:18 - of this tutorial so we'll just drop it
47:21 - we say drop RF AIS equal to one and and
47:26 - that's perfect we can call this uh
47:28 - assign it to factor
47:31 - data and
47:35 - yeah that's our Factor data let's check
47:37 - the index the index is uh monthly yeah
47:41 - all right so I think we have to fix the
47:43 - index as well so we can call of pandas
47:46 - to daytime and Supply the index let's
47:49 - see if this would
47:51 - work okay we have to use to time stamp
47:54 - all right
47:59 - okay so now we have fixed the index as
48:01 - well as you can see now it's it has the
48:04 - year the month and the beginning of
48:07 - month date however our data is end of
48:10 - month that's one thing we have to fix
48:13 - another one is that I see that the
48:16 - factors are in
48:18 - percentages so we would have to divide
48:21 - them by 100 how we can fix that we can
48:24 - just say resum P to monthly and get the
48:28 - last
48:28 - value which would fix immediately the
48:33 - issue yeah with the beginning of mandate
48:36 - next we can say divide by 100 and that's
48:40 - perfect we just assign
48:42 - that to factor
48:45 - data comment it out yeah next we want to
48:51 - fix the name of the index to be just
48:55 - date
48:56 - and the next step would be to join with
49:00 - the pretty much join with the one month
49:06 - return why would we want to do that
49:08 - because at the beginning of each month
49:11 - we have the factors and then we have the
49:15 - return of each stock at the end of the
49:17 - same month so now we have fixed the date
49:20 - of the factors we can just join with the
49:23 - end of month return and then we can can
49:26 - regress them and take the beta right if
49:28 - the factor is predictive we have it at
49:30 - the beginning of the month and we will
49:33 - regress it with the return of the end of
49:34 - the month so we'll get the the the beta
49:37 - how we can do that factor data join and
49:41 - then from our
49:44 - data we can select the one return column
49:48 - let's see what this would give is that's
49:51 - perfect now we can sort the
49:54 - index
49:56 - and
49:59 - just assign to factor data what we may
50:02 - want to do here is to double check our
50:05 - work we can select two
50:08 - stocks for example
50:11 - apple and let's say
50:16 - Microsoft and we can double check the
50:20 - return yeah the return is different but
50:22 - the factors stay the same looks like we'
50:25 - worked
50:27 - correctly and we are ready to move to
50:29 - the next step in this step we are going
50:33 - to filter out stocks that have less than
50:37 - 10 month data why are we doing that
50:40 - because we are going to use rolling
50:43 - window for the regression of around 2
50:45 - years 24 months and stocks that don't
50:49 - have enough data would actually break
50:52 - our function so we have to remove them
50:55 - from the data set how we can do that we
50:57 - can say Factor
50:59 - data. groupby level one and then just
51:04 - call the size method and now we can see
51:07 - how many months of data we have for each
51:12 - stock okay guys and I just realized that
51:15 - we have made a small mistake somewhere
51:18 - because we have only 23 months of data
51:21 - for each stock and I had to go back
51:23 - through the code and actually f figure
51:25 - out that on this step uh this part was
51:28 - missing which was the data dolog
51:31 - selecting all the rows and then the
51:34 - dollar volume column you can just rerun
51:36 - this sale and you can see that here our
51:39 - our first month is in 205 November and
51:43 - after that it was 2020 so when you add
51:46 - the dot loog and selecting the all the
51:50 - rows and the dollar volume column you
51:51 - get the fixed data then we can rerun as
51:54 - you can see up here it's 2021 the first
51:57 - date if we run it it should be much
52:01 - backwards in the past yeah so
52:04 - 2017 31st of October and we have to do
52:08 - that again for the factor data two and
52:12 - if we run now yeah now we have 71 months
52:16 - right and the idea here is uh that we
52:20 - remove all the stocks that have less
52:23 - than 10 months of data how do we do that
52:26 - we can just assign that to
52:30 - observations and then we can save valid
52:33 - stocks that would be
52:37 - observations that would be all the
52:39 - stocks that have more than 10 months of
52:46 - data observations yeah those are our
52:49 - valid stocks and now we can just use
52:51 - that as a filter so Factor data
52:55 - would be
52:57 - Factor
52:58 - data again Factor data. index do get
53:04 - level
53:05 - values we get the thicker values maybe I
53:08 - can show you what this is returning this
53:12 - is returning uh yeah an object with all
53:15 - the the stocks we have in the thicker
53:19 - index part of our M index and then we
53:21 - can just say is in the valid stock
53:25 - stocks and this would filter out pretty
53:29 - much
53:34 - everything we have to add this
53:44 - part
53:50 - okay okay so I
53:53 - think oh yeah
53:56 - I missed this part should
53:58 - be all stock
54:01 - index those are the stocks we are going
54:03 - to remove so if we take that
54:10 - out yes so now as you can see we hit
54:13 - before we hit 10,21 rows now we have
54:17 - 10,250 we've removed around 51 rows and
54:22 - yeah with this step we are now ready to
54:26 - calculate the rolling Factor betas we
54:30 - would like to do that simultaneously for
54:32 - all the stocks in our Factor data we can
54:35 - just use the same methodology with Group
54:39 - by and apply a function that would be
54:43 - Factor
54:44 - data. Group by and then we're grouping
54:48 - by level one by ticker Group keys should
54:54 - be false
54:55 - and we can put that into brackets
54:59 - so we can continue on the next row so
55:04 - apply Lambda X and now we want to
55:09 - use the rolling
55:12 - regression actually we may want to
55:14 - explore the rolling
55:17 - regression rolling ORS
55:23 - python it takes the endog and exog all
55:28 - right we here Supply endog and exog so
55:32 - the endog would be our return column
55:36 - right so
55:38 - X return one month and our xog would be
55:44 - everything else what we can do here is
55:48 - say x do
55:50 - drop the return one month column and
55:53 - this would return all the other this
55:56 - would give all the other columns without
55:58 - the return and actually we can add a
56:03 - constant here on the spot so we add a
56:07 - constant and I think the next one we
56:11 - have to supply the window we decided the
56:13 - window to be 2 years right so that would
56:15 - be 24 right 24 months and there is
56:19 - another mean
56:22 - observation the minimum number of
56:24 - observation required to estimate the
56:26 - model we have to supply this one as well
56:29 - this one is a little bit more tricky we
56:31 - have to supply here to have at least the
56:35 - total number of columns plus one so that
56:38 - would be Lan x. columns plus one and
56:43 - this should be our model now we have to
56:46 - say fit and then
56:50 - params and then we can drop the we can
56:53 - drop the constant because it will return
56:55 - constant as we added a constant up here
56:58 - right and I think this should pretty
57:01 - much work except that maybe sometimes we
57:06 - would not have exactly 24 months of
57:11 - observations but we still may want to
57:14 - run the regression so what we can do is
57:16 - use as a window the value which is
57:19 - smaller than two Val so it either 24
57:22 - months or we can use the number of rows
57:26 - we have in the for the given stock right
57:29 - and we know that we have stocks with
57:31 - more than 10 months of data so if one of
57:33 - the stocks have like 15 months of data
57:36 - you just use the 15 months as a window
57:39 - instead of 24 and yeah I think that
57:44 - should pretty much be it
57:48 - right okay series object doesn't have
57:51 - fit
57:53 - why right
58:05 - maybe okay so that's pretty
58:08 - much that's pretty much uh our rolling
58:12 - Factor betas we have calculated them now
58:15 - we can assign them to
58:18 - betas
58:22 - and that's our rolling Factor BS guys in
58:26 - The Next Step what we want to do is to
58:29 - join them to our current features and
58:34 - with that we have our Full Features data
58:36 - set but before we join them we have to
58:40 - think about a little bit now we have the
58:42 - rolling Factor betas where we used the
58:46 - factor at the beginning of the month and
58:48 - the return at the end of the month so
58:51 - this beta we would actually know at the
58:56 - next month right at the end of the month
58:58 - we go we'll be able to run the
59:01 - regressions and have the betas but we'll
59:04 - have them in the next month so we cannot
59:07 - just blindly join them to the features
59:09 - data we have so far what we have to do
59:13 - is to shift them
59:17 - with one month forward before we join
59:21 - them to the data because these values we
59:24 - would have no not known in the same
59:26 - month we would know uh we would know the
59:29 - mon the the rolling Factor better for
59:31 - example for the end of October we would
59:34 - know them in November what we have to do
59:36 - is we have to shift with one month
59:39 - forward on the ticker level so for each
59:42 - ticker not like the whole data frame if
59:44 - we just say
59:46 - betas do shift it will run it will shift
59:50 - with one row downwards and for example
59:53 - the value of Verizon will come up up
59:56 - here right just see that so Verizon is
60:00 - 0.3 if we apply
60:03 - shift it's now here so that's obviously
60:06 - not correct so what we have to do is we
60:08 - have to first group
60:09 - by and we can do that group by thicker
60:12 - and then we can
60:14 - shift that would now uh do it correctly
60:18 - we can grab that and Supply it
60:23 - here
60:33 - and data will be equal to
60:42 - this and the next step that we want is
60:47 - to impute the missing values of each
60:50 - factor with the average for that factor
60:53 - beta how we we can do that we can say
60:56 - first we can create a a list with the
60:59 - factor columns factors would
61:03 - be just those five
61:18 - columns we can say data. log all the
61:22 - rows factors
61:25 - we'll just select all the factors and
61:27 - all the rows and then we can say the
61:30 - following data. Group by ticker for each
61:34 - ticker we'll select the factors again
61:38 - and then we'll just apply a Lambda
61:42 - function Lambda X where we will F all
61:49 - the missing values with the mean of this
61:52 - Factor but I think because we are doing
61:54 - grp Group by apply we have to add the
61:58 - group
62:01 - keys to be
62:03 - false and
62:06 - now we should have fixed our missing
62:10 - vales
62:11 - issue voila so now we don't have any
62:15 - missing Valu so all all the nas are
62:17 - imputed by the average for this factor
62:21 - and yeah now we can just say drop a if
62:25 - there are
62:26 - any and we can say data. info to see our
62:31 - final
62:32 - result
62:34 - and that is now beautiful guys this is
62:37 - our features data set however I see a
62:41 - column we don't need here which is the
62:42 - adjusted close we just have to drop it
62:46 - data is equal to data drop adjusted
62:49 - close axis equal to
62:53 - one and
62:55 - and this is our features data set guys
63:00 - so we have 18 features at this moment we
63:03 - are now ready to apply machine learning
63:05 - models from here on what we have to
63:08 - decide usually is for each new month we
63:11 - have to form a portfolio with some of
63:15 - the stocks from our uh data set so as we
63:18 - know we have for each month we have the
63:20 - top 150 most liquid stocks and now now
63:24 - at the end of each month we have to
63:26 - decide the stocks we want to have in our
63:29 - portfolio for the next month that's
63:30 - where we can use a machine learning
63:32 - model first we can use a machine
63:34 - learning model to predict which stocks
63:37 - to include in the portfolio as well if
63:39 - we have a long short portfolio we have
63:41 - we can predict which stocks to be long
63:44 - and which stocks to be short but in this
63:46 - course we are just focusing on Long Port
63:49 - foros so we can use a machine learning
63:51 - model to predict which stocks as well we
63:53 - can use machine learning model to
63:55 - predict the magnitude of the position in
63:58 - each stock so what is the weight in the
64:01 - portfolio and the other way is to use a
64:04 - machine learning model in our case a
64:06 - nonsupervised model to decide which
64:09 - stocks to use in the portfolio based on
64:12 - grouping that's why we are going to use
64:14 - a clustering algorithm a KES clustering
64:16 - to keep things simple because from this
64:19 - point on things can get really
64:21 - complicated but yeah that's so far was
64:24 - the preparation to get the data to fit
64:27 - into a machine learning model guys and
64:30 - yeah I'm really excited in the next step
64:31 - we are going to fit a c's clustering
64:34 - algorithm and split the data in in a few
64:37 - clusters each stock will be assigned a
64:39 - cluster and then we'll uh be able to
64:42 - analyze the Clusters and decide what we
64:44 - do further let's move to this step okay
64:47 - so in this step we're going to fit the C
64:50 - clustering algorithm for each month and
64:53 - split the stock stocks into four
64:56 - different groups based on their features
64:59 - y four I've already did a a little work
65:03 - beforehand and I've estimated that the
65:06 - optimal number of clusters roughly on
65:09 - average for each month is around four so
65:12 - we'll use four clusters for each month
65:14 - uh from now on the specifications about
65:17 - the K's clustering algorithm is that it
65:20 - uh it may assign the centroids of the
65:23 - Clusters around randomly and then it
65:25 - assigns a given point to the cluster
65:28 - based on the distance from the centrate
65:31 - to that point to uh help you understand
65:34 - a little bit more so this is an example
65:37 - with the same data but different number
65:40 - of clusters specified when fitting the
65:43 - model so in the first one we have two
65:44 - clusters as you can see really well
65:47 - defined three four and five whenever we
65:50 - initialize the model we have to specify
65:52 - how many clusters we want the data to be
65:55 - grouped in and the algorithm would go in
65:59 - assign random points and then the
66:01 - closest points to the Cent read so
66:03 - Random Cent the closest point to the
66:06 - Cent would be assigned to that cluster
66:09 - and until uh that will be repeated until
66:12 - all points have an assigned cluster how
66:14 - do we start here let's first import the
66:18 - C's class from
66:21 - sklearn do cluster we are going to
66:24 - import K
66:26 - means and now what we want to do right
66:29 - is fit uh K means model for each month
66:34 - and assign a cluster to each stock so
66:37 - get the label we can do that at once
66:40 - using the group by apply logic we've
66:43 - already used for ATR and macd we can
66:45 - define a function which should be get
66:48 - clusters to take data or DF it's up to
66:53 - you and then we'll create a new column
66:56 - called
66:57 - cluster and here
67:01 - we'll fit the CES model so the CES class
67:05 - takes first argument number of clusters
67:09 - that would be four as we already
67:12 - know I've done my research
67:15 - beforehand and then we'll use the random
67:18 - State argument to be zero this would
67:21 - ensure it works like random seed this
67:23 - would sure that we have the same results
67:26 - through different calls and there is
67:28 - another really important argument here
67:31 - they need this is the initialization
67:34 - method of the centroids actually here
67:37 - for now we'll use a random
67:40 - initialization but we can actually
67:42 - Supply here the initial Central points
67:46 - for the Clusters and we may use that in
67:49 - the future so let's see the first result
67:51 - we fit the model follow to the data
67:54 - frame and then we'll get the labels we
67:58 - will return the labels and we will
68:00 - assign them to the cluster column and we
68:03 - just return the data frame in the end
68:06 - and now we
68:09 - can say the following data drop na we
68:12 - will ensure that we don't have any na in
68:15 - the data Group by level one or date and
68:19 - then Group keys equals to false because
68:22 - we do group by apply
68:24 - and then apply get
68:29 - clusters this will take some time but it
68:32 - will now go through each month and fit
68:36 - the K clustering algor with four
68:38 - clusters and assign the given cluster to
68:41 - each stock let's wait for this to run
68:45 - okay so the clustering is finished let's
68:47 - take a look at the results now we have
68:49 - the cluster column and in it we have the
68:53 - assigned values from 0 to three so four
68:56 - clusters 0 1 2 three for each stock and
69:00 - that is done every month now the next
69:03 - step would be to visualize actually our
69:05 - clustering job which is pretty hard when
69:09 - we use more than two features and in our
69:12 - case we have 18 but that's why we didn't
69:15 - apply we didn't apply normalization to
69:17 - the RSI because now we can use it to
69:21 - visualize the clustering job in Better
69:24 - Way all right so I've already wrote down
69:27 - a small function for that up
69:29 - here this is a plot function uh we just
69:33 - select the stocks for the first the
69:36 - second the third and the fourth cluster
69:38 - and then we just visualize them with a
69:42 - with a scatter plot and my idea here is
69:45 - to do that for each month so we can use
69:48 - this function it will select for the
69:50 - scattering it will select the first
69:52 - column which is the 8 R and the seventh
69:55 - column which is the
69:58 - RSI and let's do that now p. style. use
70:05 - we use the ggplot style always and now
70:08 - for I in data. index get level
70:17 - values and we'll get all all the months
70:21 - so
70:22 - unique to
70:24 - list for each month we are going to
70:27 - select pretty much this month right so
70:31 - data. x uh Xs and then I level
70:36 - zero so we are going to select the month
70:40 - and we're going to
70:42 - use this function so we for each month
70:45 - we will just plot the
70:47 - Clusters
70:49 - however we may want
70:52 - to
70:56 - add the month to the title so we can do
70:59 - that in the following way
71:02 - date all let's run this thing
71:06 - and check our clustering job all right
71:10 - guys so now for each month we have uh
71:14 - our clustering job so those are all the
71:17 - stocks and on the Y label we have the
71:19 - RSI values and here we have the ATR
71:22 - values and those are our clusters as you
71:26 - can see they're pretty well defined
71:28 - throughout the different
71:30 - months and if you go through it the
71:34 - first observation is that actually the
71:36 - clustering is working in such a way that
71:39 - all the stocks which are around 60 to 70
71:41 - RSI are in one cluster then you have the
71:44 - two middle two midles from 50 to 60 and
71:47 - from 50 to 40 and then you have the down
71:50 - cluster here around 35 and what we can
71:54 - see is that this assumption or this
71:58 - observation holds throughout each month
72:00 - of clustering however the problem is
72:04 - that for example in this month cluster
72:06 - zero is around 30 35 RSI cluster one is
72:11 - around 40 to 50 right but then uh this
72:15 - is the same for the next month but at
72:17 - some point cluster zero is around 50 and
72:21 - cluster one is around 60 because the
72:23 - Cent RS are random our clustering gets
72:27 - random and my idea for the strategy
72:29 - we're going to apply is that we would
72:31 - like to follow stocks momentum by stocks
72:36 - momentum I would use the RSI as an as
72:40 - the main indicator right the stocks
72:41 - which are around 70
72:44 - RSI are in an upward momentum and I
72:47 - would like our strategy to invest every
72:51 - month in the stocks that have have the
72:54 - highest upward momentum throughout the
72:56 - previous month for that job I would like
72:58 - to focus on the stocks that are
73:01 - clustered around RSI of 65 to
73:06 - 7075 but using the random initialization
73:10 - of the Clusters would not work and as we
73:15 - can see so what we have to do now is to
73:19 - help a little bit the CIS clustering Alm
73:22 - by
73:23 - supplying the initial centroids but how
73:29 - we can do that let's check out the K's
73:41 - documentation okay so that's the init
73:43 - argument so the default is C++ we used
73:47 - random but if an array is passed it
73:51 - should be of shape so this shape and
73:54 - clusters and features and gives the
73:56 - initial centers of the Cent that's
73:59 - exactly what we want to do and we
74:03 - want the initial centers to be based on
74:06 - our RSI indicator pretty much what we
74:10 - want is that all the stocks which are
74:13 - around 70 RSI to be for each month to be
74:17 - in the same cluster then 55 the same
74:20 - cluster then 45 the same cluster and 30
74:24 - the same cluster obviously that would be
74:27 - a little bit better than just deciding
74:30 - those thresholds ourselves for example
74:32 - if you want to select all the stocks
74:34 - above 70 every month we'll have a
74:37 - different result than using this
74:39 - clustering algorithm to do the
74:41 - clustering around 70 for US based on all
74:44 - the features we have in the data set now
74:46 - we have to supply to the clustering
74:48 - algorithm
74:51 - this this array in in this shape with
74:55 - the initial
74:56 - centers okay so let's do that so first
75:00 - we would like to have the target RSI
75:03 - values and that would be a list with the
75:06 - first Target RSI value would be 30 45 55
75:10 - and 70 then our initial
75:15 - centroids would be n Pi zeros and we
75:20 - were we are going to use the target Val
75:22 - so the length to the Target
75:24 - values which is
75:26 - four and then the number of features
75:30 - right
75:31 - four and
75:33 - 18 because we know that we have 18
75:37 - columns right 18 features and that would
75:40 - be our initial sent rats however let's
75:46 - run
75:50 - that yeah I think I forgot that part
75:54 - yeah so that's our array however up here
75:57 - we have to supply the sent R what we
76:01 - were going to do is the following all
76:02 - rows and this column as we know that the
76:07 - sixth the the seventh column so the
76:10 - sixth index the seventh column is our
76:11 - RSI column in the features data set
76:15 - we're going to
76:17 - just change that to the RS Target RSI
76:21 - values and we'll get the following here
76:23 - we have the initial sent RS and that's
76:26 - our array we're going to use for the
76:30 - clustering job we have to supply it here
76:34 - but before that we would have to first
76:37 - drop the cluster colum and rerun the
76:40 - whole clustering so data is equal to
76:42 - data
76:47 - drop this will now drop the cluster
76:50 - column and rerun the the clustering and
76:55 - assign the centroids as we have supplied
77:00 - this will ensure that throughout each
77:02 - month we'll have the same cluster label
77:07 - assigned to the stocks corresponding to
77:10 - the the same cluster as you can see
77:13 - again in the first month in October we
77:15 - have assigned cluster two to all stocks
77:18 - with RSI around 70 in the next month
77:20 - it's cluster one we want that to be
77:23 - always cluster
77:25 - two that's why we are doing it okay
77:28 - let's run it we have to wait again for
77:30 - the model to fit okay and it's
77:34 - done we have fit now the model with our
77:38 - initial
77:39 - centroids and we can just now rerun the
77:45 - visualization let's do that now okay so
77:48 - now cluster three is assigned for stocks
77:52 - uh with our around
77:54 - 70 and this is consistent for each month
77:58 - it's again cluster three cluster three
78:01 - cluster three cluster three and now we
78:04 - can use that down the road to select
78:08 - every month the stocks which are for
78:10 - example in cluster three to form our
78:13 - portfolio site and we know that cluster
78:15 - 3 is corresponding to stocks that have
78:19 - had a good momentum through the previous
78:23 - month with that we are ready to move to
78:26 - the next step which is at the beginning
78:29 - of each new month we're going to select
78:32 - the stocks we want to invest in for the
78:35 - given for that
78:36 - month okay guys so we are at the most
78:38 - exciting part now we are going to select
78:41 - stocks based on our clustering and then
78:44 - we are going to form a portfolio using
78:48 - an efficient Frontier optimized Max
78:51 - Sharpie weights for those those stocks
78:54 - we want to choose a cluster based on our
78:57 - hypothesis and as we talked earlier my
79:01 - hypothesis here is that stocks which had
79:04 - an RSI around 70 are having a good
79:08 - momentum and my idea is that this
79:11 - momentum should keep outperforming in
79:13 - the next month and for this job I've
79:16 - analyzed all the Clusters throughout the
79:18 - months and I saw that cluster 3 is
79:20 - always stocks around 70 R side and in
79:24 - this case I would like to select those
79:26 - stocks for every month for my portfolio
79:29 - how do I do that we just select stocks
79:33 - corresponding to Cluster I'm sorry to
79:36 - Cluster
79:38 - three for each month okay so here we can
79:43 - call that
79:45 - filtered data
79:51 - frame and
79:55 - assign
79:56 - that okay so now after we have our
79:59 - filter data frame now the first step is
80:02 - to actually get so at the end of October
80:08 - right on 31st of October we had those
80:13 - stocks that's our list of stocks which
80:16 - we would like to use to invest in
80:19 - November my idea here is to create a
80:21 - dictionary with the first day of the
80:25 - next month and all the stocks for the
80:28 - for the next month in a list to do that
80:31 - we have to do the following so filter DF
80:35 - we first reset the index we reset
80:39 - actually only the first
80:45 - [Music]
80:49 - level
80:51 - okay
80:57 - all right so we've done that and now now
81:01 - we can do the following we can just use
81:03 - the
81:04 - index and add one date to it so pandas
81:08 - date of set one which would
81:11 - move each index with one day in the
81:15 - future as we know all the indexes are
81:18 - the last day of the month so we'll have
81:19 - the beginning of the next
81:21 - month
81:23 - like this and the next step would be to
81:26 - do the following so we we can reset the
81:29 - index and
81:30 - then we can set the index again to be a
81:34 - multi-index so we have date and
81:39 - thicker let's see all right perfect if
81:43 - that doesn't work to you you can do
81:45 - unstuck and then stuck
81:49 - but it's not necessary at the moment we
81:53 - do this step and then the next step
81:56 - would be to create the dates object so
82:00 - dates would be index. get level
82:05 - values
82:06 - we get all the
82:13 - months and then what we have to do is to
82:17 - create this dictionary which is having a
82:20 - key the date so the beginning of the new
82:22 - month and value would be a list of all
82:26 - the stocks for that month we can call
82:28 - that fixed
82:30 - dates it would be a dictionary and then
82:33 - for D in
82:35 - dates we do the
82:38 - following fix
82:41 - dates do
82:43 - D here we'll fix it to be string have
82:47 - time in our format year
82:51 - month day
82:54 - and this would be equal to filtered data
82:58 - frame pretty much we will
83:01 - select each month right we select each
83:06 - month and then we get the index after we
83:10 - select the month the index will be the
83:11 - tickers and then we call it to
83:15 - list and that's pretty much it this
83:18 - should give
83:21 - us
83:24 - this should create for us this
83:27 - dictionary and voila we have it so now
83:30 - we have the first day of the next month
83:32 - right and the stocks list we the stocks
83:36 - we want to invest for the next month
83:39 - into a list and actually we can now
83:42 - really easily use that to create our
83:46 - portfolios and with that we are ready
83:48 - for this step so the next step is to
83:52 - Define find the portfolio optimization
83:54 - function we're going to use the
83:56 - portfolio opt package and the efficient
83:59 - Frontier to have on a portfolio which
84:02 - maximizes the Sharpie ratio so okay
84:05 - let's move to define the portfolio
84:07 - optimization function we'll use to do
84:09 - that first we will import from pfolio op
84:13 - package a few different classes so from
84:18 - P PF
84:20 - op do effici efficient
84:24 - Frontier we import the efficient
84:28 - Frontier class
84:31 - then
84:33 - from ppf opt we import risk
84:38 - models and again
84:41 - from ppf op we will
84:44 - import the
84:47 - expected returns object okay so now what
84:51 - do we want to do so at the beginning of
84:54 - each month we have the list of stocks we
84:56 - are going to invest and those stocks we
85:00 - have to assign weights to them we have
85:03 - to find those weights we are going to
85:05 - use this package to optimize and find
85:10 - those weights for every month we'll
85:13 - Define this
85:15 - optimize weights function which will
85:17 - take two argument prices actually one
85:20 - argument for now and and uh we'll add
85:24 - another one in the later on so first
85:27 - we'll the the function will take only
85:30 - the the prices of all the stocks we want
85:33 - the weights to be optimized for first we
85:36 - have to calculate the returns of those
85:38 - stocks so we will use the expected
85:42 - returns from expected returns we use a
85:44 - method called mean
85:48 - historical return which take an argument
85:52 - price PR and we will supply the prices
85:55 - as well it takes an argument
85:58 - frequency and that would be 252 days so
86:02 - one year of trading data then we have to
86:06 - calculate the co variance and this time
86:09 - we use the risk models so from risk
86:12 - models sample Co
86:14 - variance again we have to supply the
86:17 - prices and the
86:20 - frequency again one year of data after
86:24 - we have calculated the returns and the
86:26 - co variance we can now initialize our
86:30 - efficient Frontier object efficient
86:35 - Frontier and now we can take a look by
86:40 - portfolio op
86:43 - efficient from here let's read the
86:51 - docs okay but that's the
86:55 - general we want the mean variance
86:59 - optimization okay so the efficient
87:01 - Frontier takes the following argents so
87:04 - expected Returns the covariance Matrix
87:07 - weight bounce and then the
87:10 - so all right the expected
87:14 - returns maybe we can just c
87:21 - those
87:22 - the expected returns would be our just
87:25 - calculated Returns the C coverance
87:28 - Matrix would
87:30 - be C and we will keep this weight bounce
87:35 - for now I will show you
87:38 - why and we use the S CS
87:42 - store and after we have that we can just
87:46 - calculate the weights weights would be
87:48 - from efficient Frontier we will use the
87:51 - max Sharpie
87:53 - method that should give us the weights
87:57 - and then in the end we can get the clean
88:00 - weights which will be round the weights
88:02 - and Clips near zeros so we we want the
88:04 - the clean weights we will just return
88:09 - ef. clean weights and that this
88:13 - our portfolio optimization function so
88:16 - obviously we have to supply the onee
88:19 - prices to this function and it will
88:22 - calculate the returns of those stocks so
88:24 - we have a data frame with one year
88:26 - prices of all the stocks for a given
88:28 - month this function will calculate the
88:30 - returns it will calculate the co
88:32 - variance then it will fit the efficient
88:34 - Frontier optimization and come up with
88:38 - the optimized Max Sharpie ratio weights
88:41 - and return the weights however there is
88:44 - a small specification and that is the
88:46 - weight bounce pretty much this is the
88:49 - bounce for a single stock what can be
88:52 - the constraint for weight of a given
88:54 - stock at the moment it is from zero to
88:57 - one so after the optimization we may
89:00 - have some stocks with zero weight and we
89:03 - may have a single stock with 100% weight
89:06 - obviously we don't want that so we will
89:08 - come back and fix it but yeah that's uh
89:12 - pretty much our function for
89:14 - diversification
89:16 - purposes we would like to actually have
89:18 - a maximum weight of 10%
89:22 - of our portfolio in a single stock so we
89:25 - can already assign that to be
89:28 - 0.1 but for the lower bound we may use a
89:30 - more Dynamic metric so we'll create a
89:34 - argument here lower
89:36 - bound which would be equal to zero
89:40 - but for example for the lower bound when
89:43 - we do the optimization we may use half
89:46 - the weight of an equally weighted
89:49 - portfol for example for a given month we
89:51 - have
89:52 - 20 stocks an equally weighted portfolio
89:55 - would have 5% in each stock so we may
89:58 - assign half of that weight let's say
90:01 - with 20 stocks we may have um the
90:04 - smallest weight to be 2.5% half of equal
90:07 - weight and the maximum to be
90:10 - 10% and with this we will ensure that we
90:13 - have a diversified and well balance
90:16 - portfolio and yeah guys with that we're
90:19 - ready for the next step and in The Next
90:22 - Step we're going to download fresh daily
90:24 - prices of all the stocks that may end up
90:27 - in our portfolios as we know the first
90:31 - portfolio we are going to form at 1st of
90:34 - November
90:35 - 2017 and from our optimized weights
90:39 - function we know that we need at least
90:40 - one year of data prior to the
90:43 - optimization so the starting date of our
90:46 - uh of the download should be 2016 1st of
90:49 - November at least and
90:52 - to download the prices we can just use
90:55 - wi Finance package the first step would
90:58 - be to create the stocks list actually we
91:00 - can go back and use the data object that
91:03 - would be the 150 most liquid stocks
91:06 - because any one of them may end up in
91:08 - our portfolio we want the data for them
91:10 - so data. index. get level
91:15 - values get level
91:18 - values that will be thicker right or
91:20 - thicker values
91:22 - unique and then to list so we have the
91:25 - stocks list and then we can create the
91:27 - new data frame uh we can yeah the new
91:30 - data frame we download from y
91:34 - finance thers will be our stocks and now
91:38 - the start date yeah as we know the first
91:42 - date we want at least one year before
91:43 - that so we can actually to make things
91:46 - Dynamic we can use the data do
91:50 - index
91:52 - get level values but here we we'll get
91:55 - the the the months then we take the
91:58 - first value so the first month that
92:00 - would
92:00 - be October
92:03 - 2017 but we can just
92:06 - say minus Panda's date of set and then
92:12 - months equal to one so minus one month
92:16 - this would return us exactly end
92:20 - of September 2016
92:26 - actually I went to 2017 okay so we want
92:30 - 12 months we want one whole year I'm
92:32 - sorry my bad
92:35 - guys that's exactly what we want so that
92:37 - would be our start date and our end date
92:42 - would be pretty
92:43 - much the same however that would be the
92:46 - final value from our unique month index
92:51 - of the day date
92:52 - object and that's pretty much it this
92:55 - will download the data we need for our
92:58 - optimization and after this is
93:00 - downloaded we are ready for the next
93:02 - step where the most interesting part is
93:04 - coming we are going to Loop over the
93:07 - fixed dates dictionary and for each new
93:11 - month we're going to get the stocks
93:14 - optimize the weights calculate the daily
93:17 - portfolio return and then uh calculate
93:22 - the whole time period portfolio return
93:25 - by rebalancing and optimizing every new
93:27 - month the download is done and we're
93:30 - ready to move to the next
93:33 - step first we are going to calculate the
93:37 - returns the daily returns for each stock
93:39 - from the fresh new data we just
93:41 - downloaded and then again we're going to
93:44 - Loop over each month select the stocks
93:47 - for the month calculate the weights form
93:49 - the portfolio and calculate the daily
93:52 - return for our portfolio and for our
93:54 - strategy all right so let's
93:56 - start first you want to create the
93:58 - returns data
94:01 - frame would be we calculate log returns
94:05 - so from new data frame we'll select the
94:07 - adjusted Clause column take a log and
94:10 - then the difference that would give
94:14 - us the
94:16 - returns data frame
94:20 - awesome after we have the returns we'll
94:22 - create
94:24 - portfolio DF which would be an
94:27 - empty data
94:29 - frame and then we will Loop over so for
94:34 - start date in
94:37 - fixed dates. keys so we are going to
94:40 - Loop over our dictionaries for start
94:43 - date let's so for each date we are going
94:48 - to first get the end date so the end
94:53 - date would be should be the end of the
94:56 - month right we're investing for one
94:57 - month and then we rebalancing so at the
95:00 - beginning of uh November 2017 we want to
95:04 - select the returns from this return data
95:08 - frame we want to select the returns for
95:10 - the next month right so the end date
95:12 - would be the end of November
95:14 - 2017 that will be P to date time start
95:18 - date
95:20 - Plus
95:21 - pandas date
95:24 - offsets actually I think here we'll use
95:29 - offsets and then
95:32 - month
95:36 - end and we can call the string F time
95:41 - method so we have
95:45 - the right format and we can print that
95:49 - now actually we can print both the start
95:52 - date and we can
95:56 - print the end
96:01 - date for each month we have the start of
96:03 - the month and the end of the month and
96:06 - now the next step is to get the columns
96:08 - for the month to do that we can just say
96:11 - the following so columns would be equal
96:16 - to fixed dates and we are just going to
96:21 - select the star
96:24 - date and those are going to be our
96:26 - columns right so those are going to be
96:28 - the stocks for each month that uh we are
96:32 - going to form portfolio portfolios with
96:35 - after we have the start date and date
96:38 - and the stocks for the given month now
96:40 - we want to calculate the weights but to
96:42 - calculate the weights we need to do the
96:44 - optimization and as we already know to
96:48 - find the optimized weights we need a
96:50 - onee data prior to the start date and it
96:55 - has to be daily so for that data we'll
96:58 - use the new data frame that we've just
96:59 - downloaded which contains the prices and
97:03 - pretty much what do what do we want so
97:04 - to calculate the weights for all stocks
97:09 - at the beginning of November 2017 so for
97:12 - all those stocks we would have to input
97:16 - the prices for one year prior of those
97:20 - stocks into this function which will
97:23 - calculate the weights and then we'll
97:24 - have the weights for uh for the first
97:27 - month so for the November 2017 in this
97:31 - case we would have to create an
97:34 - optimization data frame first we have an
97:37 - optimization optimization start
97:41 - date and it would be pandas to date
97:45 - time
97:47 - start date it would be the given start
97:50 - date 1st of November 2017 minus Panda's
97:54 - date off
97:56 - set and we will supply
97:58 - months minus 12 months exactly so
98:01 - exactly 12 months ago would be the
98:04 - optimization start
98:06 - date
98:08 - and we
98:12 - will create an optimization end date as
98:15 - well so the end date would be
98:19 - actually again start dat minus however
98:23 - one date and now we can print to
98:27 - visualize everything we here so far so
98:29 - we have the start
98:32 - date we
98:35 - have we have the start date then we have
98:37 - the end date then we have the stocks
98:40 - that would be included in a given month
98:44 - then we have the optimization start date
98:46 - and optimization end
98:50 - date
98:52 - date of
98:55 - set my bad so date of
98:58 - set and now for this month so for 2017
99:02 - from 1st of uh November until the thir
99:04 - 30th of November that would be the
99:06 - stocks in our portfolio and we'll have
99:09 - an optimization so to calculate the
99:11 - weights for those stocks we have an
99:13 - optimization data frame which would be
99:15 - for exactly one year prior to the
99:19 - starting date so from uh 1st of November
99:23 - 2016 up to 31st of October
99:27 - 2017 and now the next step is to
99:32 - calculate the weights for the weights we
99:40 - use we are going to use that was my
99:43 - point one so we're going to use our
99:45 - optimize weight
99:50 - function
99:54 - and and here we have to supply the
99:59 - prices of the optimized weight and the
100:02 - lower bound right the prices would be
100:04 - actually an optimization data frame this
100:08 - optimization data frame we can use the
100:10 - new data frame which contains prices
100:13 - right and we can select the following
100:17 - dates from the optimization start date
100:20 - to optimization and date then we will
100:24 - select the adjusted close and we want
100:26 - only the columns that we are going to
100:29 - use for a portfolio for this month and
100:33 - that would be pretty much our
100:34 - optimization dat frame so in this case
100:39 - yeah when I run it so that's that
100:42 - because in the for Loop the the last
100:45 - values for the start and end date would
100:46 - be the last values based on the last
100:49 - value of the fixed date key so it it's
100:51 - fter out from uh 3rd of October 2022 up
100:54 - to 29th of September 2023 that's around
100:58 - almost one year but what we are going to
101:01 - do for the sake of Simplicity because
101:04 - our first start date is 200 yeah
101:08 - 2017 1st of November we are going to
101:13 - select the following date
101:15 - 2016 1st of
101:18 - November until 2017
101:22 - 30 to
101:24 - October and this is going to be our
101:27 - optimization data frame to calculate the
101:30 - weights we are going to supply that up
101:33 - here as well we have to supply the lower
101:36 - bound that for each stock what would be
101:39 - the lower bound the upper bound we know
101:40 - it's
101:41 - 10% maybe we want to make it this one
101:44 - dynamic as well but for now let's keep
101:46 - it 10% but the lower bound we want to be
101:50 - half of an equally weighted portfolio so
101:52 - a equally weighted portfolio is going to
101:55 - have weights which are pretty
101:57 - much equal to one divided by the number
102:01 - of Assets in the portfolio so the number
102:04 - of columns in this case we can
102:08 - use we can so we would have 10 columns
102:11 - so 10 assets for a given month so the
102:15 - equally weighted portfolio would have
102:17 - 10% for each stock half of that that
102:21 - would be the number of stocks multiplied
102:23 - by two I mean one divided by the number
102:25 - of stocks multiplied by two and that
102:28 - would be 0. uh
102:30 - 0.05 and now we can wrap that around in
102:33 - a round function so we are always sure
102:37 - we have around weight and now we can run
102:41 - it to optimize our weights however I
102:45 - think that up
102:46 - here our columns are actually not the
102:51 - right columns because we would have to
102:55 - select
102:56 - the first optimization date so that
102:59 - would be
103:01 - 11 this is the list of our
103:07 - columns and our optimization data frame
103:10 - will be the following and now we can
103:12 - input it here and check out the
103:16 - weights this now
103:19 - looks quite
103:22 - interesting
103:25 - first let's take a look at the lower
103:29 - bound so the lower bount is
103:33 - 0.01 if we
103:36 - apply it will be 0.012 okay maybe we can
103:40 - use a rounding of three instead of
103:45 - two okay now this looks better those
103:48 - will be the weights of our port
103:50 - portfolio in November
103:53 - 2017 as you can see for stocks they have
103:57 - a weight of 0.01 so 1.2% of our total
104:01 - portfolio and then we have this stock
104:03 - which has 10% of our portfolio this one
104:06 - has seven this one has
104:10 - 6.8%
104:11 - 5.9 3% 4% 4% that's what we are going to
104:16 - do at the beginning of each month we are
104:19 - going to optimize and find the weights
104:22 - for our stocks for for that month okay
104:25 - now we we can Implement that into our
104:30 - optimization so up here we are going to
104:34 - provide the weights and Next Step would
104:37 - be to create the returns date frame but
104:42 - actually before that I think we have we
104:44 - can turn that into a data frame so we
104:47 - can say the following weight equals to
104:49 - pandas data frame frame and then weights
104:54 - and index is uh Panda series and now we
104:58 - have the weights as a data
105:00 - frame and we can now potentially use
105:05 - that with the returns data frame to
105:12 - multiply the way to Fe stock for the
105:16 - given month by the return for that stock
105:19 - for every day and this way we'll be able
105:23 - to calculate the portfolio return for
105:28 - each day okay next step would be to
105:32 - create a temporary datea frame which
105:35 - would be equal to returns data
105:38 - frame and here we'll use the start
105:43 - date up to the end date we are going to
105:47 - filter the beginning of month so that
105:50 - would be
105:53 - 2017 first until
105:57 - 2017 13th that will be our temporary
106:01 - data frame let's see what we have
106:04 - here yeah so we pretty much selected the
106:07 - stocks and the the returns of the stocks
106:10 - for the given month the selected stocks
106:12 - and now what we would like to do is just
106:17 - merge with the weights data frame to get
106:21 - the stock return for each day and the
106:23 - weight for that day we can do the
106:26 - following first stock we are going to
106:28 - stock and then we're going to say to
106:32 - frame to frame
106:35 - return now we have for each day all the
106:37 - stocks and then the return column and
106:40 - now we want to reset the
106:43 - index we are going to reset the date
106:47 - index
106:49 - and
106:52 - next step is to
106:55 - merge with the
106:59 - weights are looking something like that
107:03 - we can
107:05 - say
107:07 - stock to frame
107:13 - weight and we can remove we have now a
107:17 - multi index up here so level zero level
107:20 - one so we want to remove the
107:22 - zero we can say reset
107:25 - index level zero drop equals to true
107:29 - because we want to just drop it
107:32 - directly
107:36 - and we are going to merge those two left
107:41 - by index so left index true right index
107:45 - true as
107:48 - well and we'll get something which looks
107:52 - like that the
107:55 - next step would be
107:59 - to reset the
108:02 - index and set a new
108:07 - multi-index date
108:10 - and I think this one doesn't have a name
108:13 - right now so it would be just called
108:15 - index and now we have almost what we
108:19 - want so we can say
108:22 - unstack and then
108:25 - stock and now we have the data frame
108:29 - with the return for the given date and
108:34 - the weight of the stock for the whole
108:37 - month of our stock and we can call this
108:42 - our temp dat
108:45 - frame the temp dat frame was initially
108:48 - the start
108:51 - date to the end
108:55 - date we can grab that put it up over
108:59 - here and
109:00 - [Music]
109:02 - now we can grab this part as
109:06 - well put it over
109:08 - here and let's take a look
109:12 - again now we can just simply use a
109:15 - vectorized operation we can multiply the
109:18 - return column by the weight column
109:21 - and then we can get the waited return
109:24 - for each stock for every day of the
109:27 - given month and let's do that but before
109:32 - maybe we would like to fix the index so
109:34 - the index to be the index names actually
109:37 - so date and thicker to keep things
109:43 - tidy
109:48 - names and now now we would like to
109:51 - calculate the
109:54 - weighted return for each stock which
109:57 - would be
110:00 - just the return return
110:04 - column multiplied by the weight
110:14 - column and now we have the weighted
110:17 - return for each day for each stock and
110:20 - the next step would be just to calculate
110:22 - the daily the sum of the daily weighted
110:25 - return which would be our portfolio
110:27 - return for the given day and we can do
110:29 - that by saying the following Group by
110:31 - level one or date level zero or date
110:34 - whatever
110:36 - you prefer so awaited return do
110:41 - sum that will be the portfolio return
110:44 - for every day and then we can just say
110:47 - to
110:48 - frame and and call it strategy
110:55 - return and that will be our strategy
110:58 - return for the for the selected month
111:01 - obviously we are looping over each month
111:03 - and we are doing the the operations for
111:05 - every month we can assign that to the
111:07 - temp
111:09 - DF
111:10 - and move it over
111:14 - here and the next operation would be
111:16 - just to you
111:18 - know concut this to the portfolio data
111:22 - frame so portfolio data frame will be to
111:25 - pandas concut portfolio data frame with
111:29 - temporary date frame axis equal to zero
111:32 - or the default axis and that should be
111:36 - it guys we have our so we Loop over each
111:39 - month we select the optimization dat
111:43 - frame we are doing the optimization
111:44 - calculating the weights then we are
111:47 - merging the weights with the returns of
111:49 - those stocks for the given month and
111:51 - we're uh calculating the waited return
111:55 - for each day for our portfolio which is
111:58 - our strategy return for every day let's
112:00 - run that and see if it's going to work
112:06 - out okay so we
112:09 - have an error which is
112:13 - return okay so I found how to overcome
112:16 - the error we'll just Implement and try
112:18 - and accept cloud
112:20 - so
112:21 - [Music]
112:24 - try
112:25 - and we'll try to run that
112:28 - except exception s
112:32 - e and we'll just print the exception as
112:37 - well I was digging through the code and
112:38 - I figured out guys that we have forgot a
112:41 - really important part of it and up here
112:44 - we don't specify the optimization data
112:47 - frame we would have to add that
112:51 - the optimization data frame was pretty
112:53 - much our new data frame where we select
112:59 - the optimization start date and the
113:02 - optimization end
113:03 - date and then we select the adjusted
113:06 - close and the columns for the given
113:09 - month and with that I think now the code
113:14 - the optimization part should be pretty
113:16 - much fixed and we have something so
113:21 - please check your objective constraint
113:22 - or use different
113:24 - so so status invasible okay yeah so uh
113:28 - this is actually happening from time to
113:30 - time and pretty much the solver is not
113:35 - able to optimize the max Sharpie ratio
113:39 - weights and instead it's failing if we
113:42 - visualize the portfolio returns you can
113:44 - see those periods where there is no data
113:48 - so pretty much the so over has not
113:50 - optimized the portfolio we didn't have
113:52 - any weights and we end up with no
113:55 - investment for those months a work
113:58 - around would be actually to do the
114:01 - following I've really spent some time on
114:03 - that part and unfortunately there is no
114:07 - easy solution for this uh optimization
114:10 - failing for the max sharp ratio the only
114:12 - workaround I figure out is to implement
114:14 - the following whenever the optimization
114:16 - fails and we don't have the max Sharpie
114:19 - ratio weights we will use equal weights
114:22 - for our portfolio as simple as that
114:24 - we'll do the following so we'll try to
114:27 - calculate the weights and
114:29 - then if this doesn't work out we'll do
114:33 - accept and then we'll print that
114:36 - Max sharp
114:40 - optimization failed
114:43 - for
114:44 - start
114:48 - date
114:52 - continuing with equal
114:55 - weights and let's see if this is the
114:58 - issue if we run that now it should print
115:00 - yeah exactly so for those months the max
115:03 - Sharpie ratio optimization failed and we
115:06 - have to continue with equal weights so
115:07 - how do we do that we'll do that the
115:09 - following way we'll create a new
115:11 - variable which is
115:12 - success and it will be equal to false
115:16 - however if the weights have been
115:19 - optimized it will be equal to
115:23 - true all right and then we can just do
115:26 - the following so if
115:29 - success is
115:32 - false our
115:34 - weights would be pretty much we would
115:38 - create a pandas data frame with our
115:41 - weight so pandas data frame but let me
115:44 - take this out of here let's put it down
115:48 - here we'll create a pandas data frame
115:51 - and then we
115:55 - want equal weights so we can just use
115:58 - this part right here we want equal
116:02 - weights so that would be the equal
116:04 - weight right one weight but we want it
116:06 - for for the number of stocks in our
116:10 - portfolio so it will be for I in range L
116:15 - optimization DF do
116:18 - columns
116:20 - yeah that would be our equal weights
116:24 - and then we have to supply the index and
116:27 - the index would be just the
116:31 - columns is a list so columns to
116:36 - list
116:38 - and the
116:41 - columns would be a pandas
116:45 - series zero and this would give us a
116:49 - data frame containing the stocks for the
116:52 - given month with uh their weight
116:55 - assigned as an equal uh equal weight and
116:57 - we can just transpose
116:59 - that and have it like that so this would
117:02 - be our equal weights data
117:06 - frame
117:07 - if our optimization
117:10 - fails and we'll have only equal weights
117:14 - whenever the optimization fails and
117:17 - after implementing that
117:24 - so yeah pretty much you can Implement a
117:29 - print for print statements throughout
117:31 - the code to double check if uh we work
117:34 - correctly but I can assure you that it's
117:36 - working correctly and now if you want to
117:39 - visualize the returns just do a simple
117:41 - plot you see that we have the returns
117:44 - throughout the whole time frame and with
117:48 - that pretty much guys we are ready with
117:50 - our strategy portfolio building we may
117:53 - want to apply to drop duplicates up here
117:55 - if we have any duplicates I think we
117:58 - don't but just in
118:00 - case and yeah with that guys we are
118:03 - ready for the next step and in The Next
118:04 - Step we're going to visualize the
118:07 - returns of our portfolio and compare it
118:09 - to a benchmark which in this case would
118:11 - be the S&P 500 Index itself it will be
118:14 - really interesting to see the results of
118:16 - all that hard work so far so let's move
118:20 - to that
118:21 - part and to compare our portfolio to the
118:25 - S&P 500 first thing would be to just
118:29 - download the returns for S&P 500 so we
118:33 - can say for so spy and we have uh we
118:38 - download
118:39 - the from wi Finance
118:42 - stickers AR tier would be spy the ATF on
118:46 - S&P 500 the start date we can just
118:51 - Supply like
118:53 - 2015 1st of January and the end date we
118:58 - can supply daytime date
119:03 - today and now we
119:06 - would pretty much create a new object
119:08 - let's call it spycor red so spy return
119:12 - and here will just take the log return
119:16 - of
119:17 - spy and just
119:21 - close then
119:23 - difference then drop an
119:25 - name
119:28 - rename and yeah
119:31 - first let's check
119:33 - spirate
119:34 - before okay so this is the log return of
119:38 - spy for each day and now I'll would just
119:40 - like to rename the adjusted close to
119:44 - Benchmark return or spy Buy and Hold
119:47 - would be adjusted close
119:51 - to
119:52 - spy Buy and
119:55 - Hold AIS equ
119:59 - one and then we can use the portfolio
120:03 - data
120:06 - frame to merge the Spy return to the
120:11 - port for dat FR so left index would be
120:14 - true right index would be
120:17 - true by the 4 is an inner joint so we
120:20 - don't have to worry
120:26 - about uh okay so we'll just you know
120:30 - we'll just rerun
120:32 - this sell and then we'll run rerun this
120:36 - one and now we have the strategy return
120:42 - and the Spy Buy and Hold return for the
120:44 - same dates and we can move to the next
120:47 - step where we would like to you know
120:50 - calculate the cumulative return for both
120:53 - the S&P 500 and for our strategy and
120:56 - plot them to compare them in time we'll
120:59 - do the following M lip style. use ggplot
121:03 - I think we've done that previously but
121:06 - just in case and now we want to
121:07 - calculate the portfolio cumulative
121:10 - return cumulative
121:12 - return and that would be from npy
121:15 - expanding window so NY
121:18 - XP
121:20 - log from portfolio data frame and then
121:25 - after that we call the
121:27 -  and this should
121:31 - calculate the cumulate is starting from
121:34 - one but we have to say minus
121:38 - one and now we would have the cumulative
121:41 - return for our strategy and the S&P 500
121:46 - itself now what we may want to do do is
121:49 - just to select for example that we want
121:52 - the portfolio return up to a given point
121:56 - let's say we want it up to 29th of uh
122:02 - September so we just say up to 0.9 20
122:09 - 29 and then we can plot that we can say
122:13 - fix
122:15 - size 16x 6 and
122:22 - and and that's uh pretty much uh that's
122:26 - our comparison guys we can add a title
122:29 - so p. title
122:34 - unsupervised learning trading
122:39 - strategy returns over time as well we
122:44 - may want to PTY
122:47 - label return return
122:49 - and maybe we would like to fix a little
122:52 - bit the axis up here to be in
122:55 - percentages so we can call the following
123:00 - GCA do ya
123:03 - axis set
123:06 - major
123:08 - formatter and here we can supply a
123:10 - formatter like there is a format from
123:14 - MTI but I think we have
123:17 - to first sent format
123:21 - there I think we have to import it up
123:24 - here so we have to import from
123:28 - modb forat B Li
123:32 - thicker same thck uh so import map yeah
123:37 - as MTI and this would do
123:41 - it uh Y
123:44 - axis set major
123:47 - format and
123:49 - that is pretty much our strategy guys up
123:51 - here we have selected the 150 most
123:54 - liquid stocks for each month we have
123:56 - calculated 18 different features on each
123:59 - stock then we used a c's clustering
124:01 - algorithm to assign a cluster for every
124:04 - stock we supplied our custom sent we
124:09 - initialized our sent with a custom array
124:12 - and then for each month we optimized the
124:15 - portfolio to have the maximum Sharpie
124:17 - portfolio weights for every new month
124:20 - and form this strategy using S&P 500
124:22 - stocks and now we compare to the return
124:24 - of the S&P 500 nevertheless this is not
124:28 - a financial advice you shouldn't use
124:30 - that anyway if you come up here and just
124:33 - change the logic for uh selecting the
124:36 - stocks from third cluster to zero uh to
124:38 - Cluster zero everything will be changed
124:41 - on the spot and you see that the results
124:45 - would change dramatically as well that's
124:47 - just example of how we can Implement
124:50 - machine learning into trading strategies
124:53 - and forming portfolios Etc so that's
124:57 - pretty much it for this strategy and
124:59 - into the next one we're going to form
125:02 - portfolios using Twitter sentiment data
125:05 - I'm looking forward to that and let's
125:07 - move to the second
125:10 - project in the second project we are
125:12 - going to build a Twitter sentiment
125:14 - investing strategy for this project we
125:16 - are going to load a data set which will
125:19 - be available for you to download from
125:21 - the description of the video there is a
125:23 - link where you can download the data set
125:25 - used in this project and in the third
125:28 - project as well the notebook or all
125:30 - three projects would be available on
125:32 - that link too so we would be loading
125:35 - this data set containing Twitter
125:38 - sentiment data on stocks from NASDAQ 100
125:41 - this strategy would be focused on NASDAQ
125:43 - 100 stocks and we are using Twitter
125:45 - sentiment data the idea here is that I
125:48 - would like to to it will be a smaller
125:50 - project than the first one but I would
125:52 - like to show you how we can create a
125:55 - derivative feature from the Twitter
125:58 - sentiment data to create potential value
126:02 - for our investments first we'll start by
126:05 - importing the
126:07 - packages those are the packages we're
126:09 - going to use panda snai m clip datetime
126:13 - wi finance and the operational system
126:15 - package we import daytime is DT that's
126:18 - the I think we're going to change and
126:20 - again we are going to use ggplot as our
126:22 - mpot lip style first let's define our
126:25 - data folder so this would be the path to
126:29 - the sentiment data file you're going to
126:31 - download so it would be a CSV file this
126:35 - is my folder which contains the data and
126:39 - you would have to assign the path to
126:41 - your folder up here let's load the data
126:43 - so
126:44 - sentiment data frame would be uh pandas
126:48 - read
126:49 - CSV and here we have to supply the puff
126:51 - os. puff.
126:54 - jooin data folder and the file name so
126:58 - the file name is sentiment data.csv
127:02 - and let's run that and see what we have
127:05 - here that is the the sentiment data we
127:08 - have we got the the date the symbol and
127:12 - the corresponding Twitter posts Twitter
127:14 - comments Twitter likes Twitter
127:16 - Impressions and the Twitter sentiment
127:19 - uh the Twitter sentiment is calculated
127:21 - uh by the methodology used by the data
127:24 - provider we just use it as it is here
127:27 - the first thing we want to fix is the
127:29 - date we want the date column to be date
127:33 - time so pandas to date
127:37 - time and next step is
127:40 - to set the indexes again we are going to
127:44 - work with the multi-index in two levels
127:47 - so sentiment dat data frame set
127:51 - index this is going to be the date and
127:54 - the
127:55 - symbol
127:57 - date and
128:01 - symbol so far so good now I would like
128:06 - to calculate a Quant feature out of this
128:08 - data and it would be called actually I'm
128:12 - interested in the engage the Twitter
128:16 - engage each stock here yes not only just
128:19 - the raw sentiment or the RO Impressions
128:22 - likes and comments but I'm interested in
128:25 - the engagement that people have on the
128:28 - posts on Twitter for this company
128:30 - because we know that there are a lot of
128:32 - bots on Twitter and they are messing up
128:35 - the data skewing the data on some post
128:38 - or some symbols and actually I would
128:41 - like to have engagement ratio which may
128:44 - be more
128:45 - informative than having just the live
128:48 - likes or the comments or the sentiment
128:50 - on its own and this engagement ratio
128:52 - will be pretty much the Twitter comments
128:54 - divided by the Twitter likes if a given
128:57 - company has a lot of likes but no
129:00 - comments we know that this is Bots
129:02 - activity but if there are quite some a
129:05 - lot of comments and a lot of likes as
129:07 - well and actually the comments are as
129:10 - much as the likes are even more we know
129:12 - that people are engaged on this stock
129:15 - let's calculate the engagement ratio
129:20 - engagement ratio and this engagement
129:22 - ratio will be pretty much Twitter
129:24 - comments divided by Twitter likes so how
129:29 - much are the
129:30 - comments compared to the
129:34 - likes
129:37 - and that's our engagement ratio for we
129:40 - have this engagement ratio for each
129:42 - stock for every day and now we may just
129:45 - want to filter out the data frame to to
129:49 - contain only stocks that have for
129:53 - example more than u a given amount of
129:56 - likes so let's say we want stocks that
129:58 - have more than 20 likes for a given day
130:01 - and more than 10 comments on a given day
130:04 - because otherwise it's just random
130:09 - noise so Twitter
130:12 - likes we would like to have more than 20
130:17 - likes
130:21 - and You' like stock to have more than 10
130:25 - columns to be kept into our investment
130:31 - universe and with that we are pretty
130:34 - much ready with the first step we are
130:37 - following pretty much the similar logic
130:39 - to the previous project however here
130:41 - we're going to create an equally
130:43 - weighted portfolio we are not going to
130:45 - implement the efficient Frontier
130:47 - optimization ation every month we would
130:50 - decide an investment Universe of top
130:53 - five stocks and just invest into them
130:57 - and now after we have uh calculated the
131:00 - engagement ratio now we want to pretty
131:02 - much calculate the average engagement
131:06 - ratio for each stock for every month and
131:09 - we can do that the following way so we
131:12 - the reset the symbol index the symbol
131:16 - part of the index we reset symbol and
131:19 - then we'll Group
131:24 - by we'll group byy for every
131:29 - month and every stock we'll Group by on
131:33 - the monthly level and the symbol level
131:35 - and then we are going to maybe we can
131:38 - put that into
131:40 - brackets then we are going to select the
131:44 - engagement ratio
131:46 - column and calculate that the average
131:48 - engagement ratio for the whole month
131:52 - pretty
131:53 - much and now we can assign that to uh
131:59 - new variable new data frame called
132:02 - aggregated
132:05 - DF and now we can calculate the
132:08 - cross-sectionally the rank for each
132:11 - month of all the stocks based on this
132:14 - engagement ratio how do we do that we
132:17 - say a new column rank we are going to
132:19 - group by level zero or date so we are
132:23 - grouping by each month because already
132:25 - we have uh aggregated to the monthly
132:27 - level right so we group up group on each
132:30 - month and then we select the engagement
132:32 - ratio and then we are doing the
132:35 - following so on the next row we're going
132:38 - to say transform
132:41 - Lambda but maybe we can put that into
132:46 - brackets and move that on the next row
132:49 - so Lambda X and here we'll just say x.
132:54 - rank
132:56 - ascending equals to
132:59 - false and this would create a new column
133:02 - called rank cross-sectionally for each
133:05 - month based on the engagement ratio and
133:08 - the stocks with the highest engagement
133:10 - ratio would have the highest rank and we
133:13 - can obviously see that already so now we
133:17 - want to pretty much select for each
133:20 - month the top five stocks for each month
133:25 - based on this Rank and grab them to use
133:29 - similarly to what we did in the previous
133:31 - project grab the start date and the
133:35 - stocks that we want to include in the
133:37 - next month and then calculate the
133:39 - returns but this time in an equally
133:40 - weighted fashion so let's do that I'll
133:43 - cut those
133:45 - celles and yeah now we'll just select
133:49 - the top five stocks for each
133:52 - month so we'll do the
133:56 - following aggregated data frame rank
134:01 - where is below six and now we have the
134:06 - stocks with the highest engagement ratio
134:08 - for each month we can assign that to
134:13 - filtered DF data
134:16 - frame
134:19 - and after we have the filter DF data
134:22 - frame we can just reset the symbols part
134:27 - of the index so reset index level equals
134:29 - to one and
134:32 - then we
134:37 - just fix the index again the same way we
134:40 - did last time so we'll just add one day
134:44 - to your index so plus day time uh
134:48 - Panda's date of set one day let's see
134:51 - what this would give us now we have the
134:54 - beginning of the next month and the
134:56 - stocks we want to invest in for the next
134:58 - month so those five stocks for each
135:01 - month and yeah pretty much next step is
135:05 - to uh reset the index and set again the
135:08 - multi
135:11 - index we'll set the multi index again
135:14 - date
135:15 - symbol and
135:19 - visualize top 20 rows so far so good now
135:23 - for for the beginning of each new month
135:26 - we have the top five stocks by
135:29 - engagement ratio and now we would like
135:33 - to create the fixed dates object like we
135:37 - did in the first project from dates
135:40 - would be good to filter data frame index
135:45 - get level values
135:50 - date and here yeah unique and to list so
135:54 - we make a
135:56 - list and fixed dates would be again a
136:00 - dictionary empty
136:02 - dictionary for D
136:07 - dates pretty much we are following the
136:10 - exact same logic we did in the previous
136:13 - projects so it's quite straightforward I
136:16 - hope this stuff will guys however uh
136:19 - yeah in this project we are not doing
136:21 - the efficient front here optimization
136:23 - just rather use a weekly weighted
136:25 - portfolio the idea here is to show you
136:27 - how we can extract value from different
136:30 - uh data
136:34 - sets and yeah as you can see now for
136:38 - each new month we have the list of five
136:41 - stocks we are going to invest in and the
136:44 - next step would be to download the fresh
136:48 - prices to do that we can first create
136:51 - our stocks list it would be equal to
136:54 - sentiment data frame do index
136:59 - get level values
137:02 - symbol and
137:03 - again unique to
137:06 - list this is our stock list and we can
137:10 - create a new data frame pric DF where we
137:13 - are going to
137:14 - download again from yaho
137:19 - final our start date we can use as you
137:23 - you can see the we can use just the 1st
137:26 - of January
137:27 - 2021 to be sure we have enough
137:31 - prices and the end date would be
137:35 - 2023 1st of
137:38 - March so
137:41 - 2023 1 of
137:44 - March awesome after we have our our list
137:49 - of stocks for each month and the prices
137:51 - we can move to the next step where we
137:53 - are going to calculate the portfolio
137:55 - returns following similar logic to what
137:57 - we did in the last section so from
138:01 - prices
138:05 - DF we can select the adjusted close
138:09 - column and
138:12 - calculate the log returns for each stock
138:16 - and yeah pretty much call that returns
138:20 - date frame so after and we can drop the
138:24 - na values in the
138:26 - end after we have the returns we can now
138:30 - create portfolio data frame which is
138:32 - just an empty data
138:35 - frame and for start
138:38 - date in fixed dates.
138:45 - Keys print start date just to visualize
138:48 - what we have so far so for each for each
138:50 - month we have the start date and now we
138:55 - want the end
138:57 - date for our
138:59 - portfolio so end date would be pandas to
139:06 - datetime start date plus
139:12 - pandas
139:13 - offsets month
139:16 - end which will
139:19 - give us
139:23 - the which should give
139:25 - us which should give us the end date of
139:27 - the same
139:29 - month uh okay pretty
139:31 - much we have that however we would have
139:36 - to add the string F time method in the
139:46 - end like that that's pretty awesome so
139:50 - what was the next step The Next Step was
139:52 - to select the columns or for the given
139:55 - month that will be the stocks we are
139:56 - going to invest for that month and these
140:00 - will be fixed
140:02 - dates start
140:08 - date so for each month the stocks we
140:11 - have to invest for that month and now we
140:14 - can create our temp data frame
140:17 - where we would be calculating the
140:21 - equally weighted return for our
140:24 - portfolio for every day this would be
140:27 - returns date frame here we'll save from
140:29 - start date to the end date and then for
140:32 - the selected columns we're going to
140:35 - calculate the mean the
140:38 - average by columns so the average daily
140:42 - return and we call that in the end we
140:46 - say to frame and then portfolio return
140:49 - portfolio return next step would be just
140:52 - for each month to
140:59 - concut this portfolio
141:07 - return and with that we have calculated
141:10 - the equally weighted portfolio return
141:13 - for every month for those stocks we have
141:15 - selected using our methodology about
141:19 - aggregated uh mon monthly
141:22 - average engagement ratio for every stock
141:26 - and then ranked
141:27 - cross-sectionally and now we are ready
141:31 - to pretty much visualize the return the
141:33 - cumulative return of our portfolio and
141:36 - compare it for example to a benchmark
141:38 - like NASDAQ right those stocks are
141:41 - constituents of NASDAQ so we may want to
141:44 - compare it to the NASDAQ indexes a
141:47 - benchmark again guys I've said it in the
141:50 - beginning of the tutorial and the course
141:52 - we're using stock lists which are not
141:55 - survivorship bias free and that is
141:59 - really important and most probably
142:02 - skewing the results upwards so you
142:05 - should never use you should always use
142:08 - survival ship bias free stock list when
142:10 - you're researching strategies so now
142:12 - what do we want we want to just download
142:16 - the
142:19 - prices for the NASDAQ or the NASDAQ ETF
142:22 - which is QQQ so we would use start date
142:25 - to be 2021 1st of January and the end
142:29 - date would be pretty much the same as
142:33 - for our portfolio so first of
142:37 - March and now we can calculate the
142:41 - NASDAQ return so
142:42 - QQQ return that would
142:45 - be log
142:48 - from the adjusted
142:52 - close
142:53 - differen and we may want to rename it to
142:59 - Mazda
143:01 - return and with that we're ready to
143:04 - merge
143:08 - together the can return to
143:10 - [Music]
143:13 - our portfolio date frame so
143:19 - we merge by
143:20 - index by theault the merge is always
143:24 - using uh in our joint so we are not
143:27 - obliged to specify
143:33 - that or maybe we
143:37 - are no we are
143:40 - not
143:41 - and now we have the portfolio return for
143:44 - every day and the NASDAQ return for
143:46 - every day and the next step would be to
143:49 - just visualize so we can say the
143:53 - following
143:54 - portfolios
143:56 - cumulative return will be equal to uh
144:00 - we'll calculate that in a n expanding
144:02 - window where we take uh
144:07 - log from the return of both portfolios
144:13 - and then
144:14 - we apply the cumulative some over the
144:17 - whole expanding window so for for each
144:20 - new uh each new addition We Run The
144:24 - accumulator some in the end we have to
144:27 - substract one and that's pretty much
144:30 - ready for plotting so we create a plot
144:33 - with a fix size equal to 16 and six and
144:37 - we can set a
144:39 - title like Twitter
144:44 - engagement ratio Str
144:48 - return over time as
144:52 - well P
144:55 - GCA Y axis set major format that would
144:59 - set the the y- axis to be a
145:08 - percentage so we'll use the percent
145:12 - format as well we give a name to the Y
145:16 - uh label
145:23 - and we
145:25 - just show
145:28 - and that's pretty much our second
145:31 - project guys about the Twitter sentiment
145:35 - investing strategy with monthly
145:37 - rebalancing of an equally weighted
145:39 - portfolio our uh stock selection
145:42 - criteria is based on the engagement
145:45 - ratio which we calculators the Twitter
145:47 - comments divided by the Twitter likes
145:50 - for each stock and we select the top
145:52 - stocks which had the highest average
145:53 - engagement ratio for every month if you
145:56 - want to change the selection criteria
146:00 - for example if you want to select the
146:03 - stocks which have on average the most
146:05 - likes for every month you just we can
146:08 - just simply do that so cop this com put
146:12 - it up here then put it up
146:15 - here and now we have the new we have
146:19 - pretty much the ranking done by average
146:21 - T likes through throughout the month and
146:23 - if we run the symbols down the sales
146:26 - down
146:28 - below it will be redone using the
146:33 - likes
146:34 - and yeah as you can see when we use the
146:37 - likes our strategy
146:40 - underperforms however if you use the
146:43 - comments maybe we can try with the
146:45 - comments to see what what would be the
146:47 - results with the
146:52 - coms similar results similar results
146:56 - however a little bit um higher
146:58 - performance so the idea here is to show
147:00 - you that if we have alternative data we
147:03 - may need to implement some calculations
147:06 - to get a derivative features which then
147:09 - may be valuable for our research and for
147:14 - our strategies that is the main idea
147:18 - here and if
147:21 - we return the engagement
147:26 - ratio let run the whole thing we are
147:28 - able to see the difference in the
147:32 - results obviously the engagement ratio
147:35 - by using the engagement ratio we have
147:38 - some we've created a little bit of value
147:41 - above the NASDAQ itself and that is
147:44 - pretty much it about the second project
147:46 - and we are able to now move to the third
147:49 - project where we are going to create
147:52 - intraday trading strategy on a simulated
147:54 - one asset data using a gar model so a
147:59 - model to predict the one day ahead
148:01 - volatility and it will be a really
148:03 - interesting one so I'm really excited to
148:05 - move to the third project and let's get
148:08 - into
148:09 - it in the third project we are creating
148:13 - an intraday strategy using gar model
148:16 - we're going to use simulated daily data
148:20 - and simulated intraday 5 minute data on
148:23 - a single asset and we are going to
148:26 - create two signals on two different time
148:29 - frames on the daily level and on the
148:30 - intraday level so on The Daily level our
148:33 - signal would be driven by the gar model
148:35 - volatility prediction we are going to
148:38 - fit in a rolling window for every day a
148:40 - gar model and predict the volatility for
148:42 - the next day and from that we're going
148:44 - to calculate the prediction premum
148:46 - premium and derive our daily signal out
148:49 - of that on the intraday level we are
148:52 - going to calculate technical indicators
148:55 - and create a signal following a intraday
148:58 - price action pattern and merge with the
149:01 - daily signal and calculate our final
149:03 - signal intraday only for those days
149:06 - where simultaneously we have the
149:08 - intraday and the daily signal and
149:10 - whenever we have the the intraday signal
149:12 - we are going either into a long or into
149:15 - a short position and we're holding until
149:18 - the end of the day and that's pretty
149:20 - much the strategy guys let's start by
149:24 - first loading the data we are going to
149:28 - use so the data again it will be
149:30 - available for you to download from the
149:32 - link down in the video description first
149:34 - we start by importing all the packages
149:37 - we'll be using for this project they are
149:39 - MP lip Arc we'll use the arc project for
149:43 - the gar model then uh Panda snai and the
149:46 - operation system package again our uh
149:49 - data
149:50 - folder you have to provide the path to
149:53 - your data folder and we can start by
149:56 - loading the data so daily uh data frame
149:59 - would be pandas read
150:01 - CSV all spot
150:06 - join and then the daily date frame which
150:09 - is
150:10 - simulated daily data.
150:15 - CSV and this should be our daily data
150:20 - here we would have to fix the date we
150:22 - use pandas to
150:24 - daytime
150:26 - and we Supply the
150:29 - date as well then we are going to set
150:33 - the index for to the Daily date frame to
150:35 - be the
150:41 - date pretty much standard stuff and our
150:46 - I have already calculated the log return
150:49 - but to repeat that I'll just calculate
150:52 - the log return which would be n
151:03 - log from the adjusted close and then we
151:06 - just do the dot difference method and so
151:10 - far so good that's our daily data and
151:12 - our intraday data would be intraday 5
151:16 - minutes
151:18 - DF again we are going to load it using
151:21 - pandas stre CSV so p.
151:25 - join and the name of the file is
151:28 - simulated 5 minutes data.
151:33 - CS we can commment out this
151:36 - code this one as well and here first
151:40 - first step we need to fix the daytime
151:44 - column to be date time so pandas to date
151:49 - time that's the first
151:56 - step then we want to assign it as an
152:04 - index and the next step would be to
152:06 - calculate this date column so we can
152:10 - just redo that real quick or we
152:13 - just cast it to data we already have it
152:17 - so we'll cast it to daytime but if
152:20 - you're wondering what it is it's just
152:23 - selecting the index and then
152:27 - date which which would give us the date
152:35 - column and with that we are pretty much
152:38 - ready with loading our daily and
152:41 - intraday 5 minute data and we're ready
152:44 - to move to the second step so in this
152:47 - step we are going to define a function
152:49 - which would fit a gar model and predict
152:52 - the volatility for one day ahead in a
152:54 - rolling window we'll use a six months
152:56 - rolling window for that but the
152:59 - specification here is that whenever you
153:01 - fit a gar model you need to supply the
153:04 - auto regressive and moving moving
153:06 - average orders you need to actually find
153:10 - those orders and to do that I've used a
153:13 - Brute Force approach where I fit 16
153:16 - models with a combination for the outo
153:20 - aggressive and moving a average orders
153:22 - from one to four and combinations
153:26 - between them so I fit 16 models then
153:28 - I've extracted the mean square error and
153:31 - the beian information Criterion and
153:33 - pretty much I chose the best model to be
153:36 - the one who is minimizing the beian
153:38 - information Criterion so I found that
153:42 - the best G model on this data is having
153:45 - Auto regressive order of one and moving
153:47 - average order of three so P = to 1 and Q
153:50 - = to 3 so that's a really important um
153:54 - clarification first you start by
153:56 - calculating on The Daily data frame the
153:59 - variance or actually the rolling six
154:02 - months
154:05 - variance of the log
154:10 - return that would be the
154:15 - variance
154:24 - and we may want to visualize
154:30 - that this is the variance of our data
154:34 - the variance of the log return of our
154:36 - data through time and now we would
154:40 - actually want to for the sake of
154:42 - Simplicity I'll just filter out the date
154:45 - to be be from 2020
154:51 - onwards and now I'm going to create a
154:56 - predict volatility function so we are
154:57 - going to predict the
155:00 - volatility for every day so so far we
155:04 - have the Vance pretty much the
155:05 - volatility is predicting the Vance with
155:07 - a G model when you fit a g model You can
155:10 - predict the variance or the uh the mean
155:13 - of the series we're going to predict the
155:17 - variance of our asset for every day and
155:24 - we would have to do that in a rolling
155:25 - window so first we create the best model
155:29 - which would be where you going to use
155:31 - the arc
155:33 - model and here we have to supply Y which
155:37 - will be our X or the series then p is
155:41 - the order the regressive order the
155:43 - moving average order Q so those values
155:48 - we have to find with the Brute Force
155:49 - approach as I did I mean fitting 16 or
155:53 - 25 models collecting the metric the mean
155:56 - squar error the be information Criterion
155:58 - after you have run the model and
156:00 - choosing the one which is minimizing the
156:03 - for example the bean information
156:05 - criteria that's the best model so after
156:08 - we have supplied arguments to the arc
156:10 - model next we are going to fit the model
156:13 - here we would say the update frequency
156:15 - to be every five steps and this position
156:18 - to be off and this is our best model and
156:22 - now we can forecast right VAR forecast
156:26 - so from the best model we are going to
156:29 - forecast Horizon equals to 1 so for one
156:32 - day ahead our data is daily so for one
156:34 - day ahead and then after the forecast is
156:37 - done for next day we have two columns or
156:40 - two vales the mean or the Vance so we
156:44 - choose the Vance and and we're going to
156:46 - choose the last value so last row of the
156:50 - First Column we may want to print the
156:53 - date so x. index last date the one we
156:57 - are going to forecast for then we'll
157:00 - just return the VAR forecast and this is
157:03 - our prediction function guys now we'll
157:05 - just say the following
157:08 - predictions and our predictions are
157:11 - going to
157:12 - be based on the log return
157:16 - we're going
157:18 - to apply a function in a rolling window
157:21 - of 180 days the same way as we did for
157:24 - the Varan 180 days and here we say
157:28 - Lambda x equal to predict volatility
157:33 - X and we can now pretty much run
157:39 - that okay yeah my bad there is no do up
157:44 - here
157:47 - what is the
157:53 - problem okay I think yeah it's
157:57 - iock here and this will now just run in
158:02 - a rolling window fit the best G model we
158:05 - know that the best G model for our data
158:08 - is auto regressive order of one moving
158:10 - average order of three and yeah we just
158:14 - have to wait for this thing to run
158:16 - and predict the
158:18 - volatility for the whole data set after
158:23 - we are done with that we will move to
158:25 - the next uh step where we are going to
158:28 - calculate the prediction premium and
158:31 - form a daily signal out of it okay so
158:34 - the model is done fitting and predicting
158:37 - the Vance and we can now take a look we
158:41 - have the V and the predictions actually
158:43 - if you look closer then the Val are
158:46 - pretty close and similar actually we may
158:49 - want to visualize them if you're
158:51 - interested guys let's see what we have
158:54 - here so we have the Vance and the
158:56 - predictions and yeah maybe we can just
158:59 - plot them so the Vance is more stable
159:02 - than the predictions and as you can see
159:04 - the predictions are exploding here or
159:07 - there but we may deal with that if we
159:09 - apply a rolling average or a CA filter
159:12 - model on the predictions they will
159:14 - smooth them out
159:16 - considerably however for the sake of
159:18 - this strategy and this course I'm not
159:21 - going to implement that but usually you
159:23 - may think about it you know like signal
159:26 - reduction techniques okay so the next
159:29 - step we are going to calculate feature
159:33 - called prediction premium and after we
159:35 - have the prediction premium we'll
159:37 - calculate the six months rolling
159:39 - standard deviation of that uh prediction
159:41 - premium and from there we'll be able to
159:43 - calculate and create our daily signal
159:45 - the prediction premium would be a very
159:49 - simple formula it would be equal to
159:53 - predictions
159:55 - minus so from the predictions we are
159:58 - going to substract the Varan and then
160:01 - we're going to scale the whole thing by
160:03 - the varas itself it's a simple
160:07 - formula then we're going to calculate
160:10 - the premium standard
160:12 - deviation premium standard
160:16 - deviation which would be just a rolling
160:20 - 6 months standard
160:24 - deviation and let's run that and see
160:27 - what we have now we have the prediction
160:29 - premium let's visualize it real
160:34 - quick so this is the prediction premium
160:37 - and we have the standard deviation of
160:40 - this prediction premium awesome from now
160:43 - on we can create our daily signal right
160:47 - so our signal daily will be the
160:50 - following our daily signal will
160:52 - calculate by doing apply Lambda over
160:55 - each roll we have one if prediction
160:59 - premium is higher than the premium
161:03 - standard deviation multiplied by 1.5 so
161:08 - 1.5 if the prediction premium is higher
161:10 - than 1.5 standard deviations then we
161:13 - have long signal or one else we would
161:18 - have minus one if the prediction premium
161:21 - is smaller than minus 1.5 standard
161:25 - deviations and here we have to say the
161:29 - following else MP
161:33 - n and
161:36 - then we'll do
161:38 - that for axis equal to one we're doing
161:42 - that for each row right let's see what
161:46 - this would give us this would create
161:49 - this signal daily column so whenever you
161:52 - know the prediction premium is higher
161:55 - than 1.5 standard deviations or lower
161:58 - than 1.5 standard deviations we get the
162:01 - signal we may want to explore that a
162:04 - little
162:05 - bit equals to
162:09 - one we have quite some signals for long
162:12 - but actually you know what we may do the
162:14 - following we may just plot a histogram
162:17 - instead of uh you know obing the we want
162:21 - to understand how much short and how
162:24 - much long signals we have so kind
162:28 - histogram and then p.
162:31 - show this would create histogram and
162:34 - we'll be able to understand how
162:40 - many okay so what is the
162:44 - problem
162:49 - funny okay so now we can see that we
162:52 - have around 50 long signals and then
162:55 - around 35 short signals and yeah with
163:00 - that we are ready with our daily signal
163:02 - guys and we can move to the intraday
163:04 - part of the strategy and here we are
163:06 - going to merge our daily data with the
163:10 - intraday data based on the based on the
163:13 - column on the date column have created
163:16 - actually to do that we need to shift the
163:18 - daily data one day ahead right because
163:21 - at the end of the 29th for example we
163:25 - have the daily data but we'll be using
163:27 - it on the 30th so we need to do the
163:31 - following up here we need to say signal
163:36 - daily will be
163:38 - just
163:40 - shifted one day
163:42 - forward that's pretty much what we have
163:45 - to to say and we can move to forming the
163:49 - final data frame here we can use our
163:52 - intraday data frame we're going to reset
163:56 - the index and then we're going to merge
163:59 - so do
164:00 - merge actually we may move that on the
164:03 - next row so we are going to merge with
164:05 - the daily date
164:08 - frame and actually only with signal
164:11 - column because we are not interested in
164:13 - the other daily columns we interested
164:15 - only in the signal column we're going to
164:17 - reset the index on The Daily data frame
164:20 - as well and here we'll be using the left
164:23 - on so left on date left on this this
164:28 - column and right on date as well or on
164:34 - the index column pretty much this one
164:37 - after the reset it will be available as
164:39 - a
164:39 - column
164:42 - and that's pretty much our join now we
164:46 - have the five uh five minute intraday
164:49 - data right and we have the daily signal
164:54 - derived by the gar volatility prediction
164:58 - now we may just want to set the index to
165:00 - be the DAT time again and that's pretty
165:04 - much our final data
165:09 - frame we can call it final data
165:14 - frame
165:20 - comment that
165:21 - out okay so now we want to drop those
165:25 - two columns we don't need the those two
165:27 - date columns so we'll say the
165:30 - following final DF do
165:34 - drop and yeah we'll drop date and
165:43 - date and we are ready now to calculate
165:47 - all the technical indicators potentially
165:49 - we can use for this strategy to do that
165:52 - we we will need the pandas ta package
165:55 - which I forgot to import so we are going
165:57 - to import the pandas ta
166:00 - package and the first indicator I would
166:03 - like to calculate is the RSI indicator
166:06 - from Pand ta we are going to use the RSI
166:09 - my to and we'll just apply the close
166:13 - price and the
166:16 - length would be 20 next one is uh the
166:20 - Ballinger bands I would like to
166:22 - calculate so b or lower
166:26 - band
166:29 - first here it's B bands again it takes
166:33 - the close price as an argument however
166:36 - the B bands are returning five columns
166:38 - instead of one we have to specify that
166:42 - we want the First Column to be returned
166:44 - so iock all the rows and the First
166:46 - Column that would be our lower band and
166:49 - for our upper band it would be the third
166:53 - column lower middle
166:56 - [Music]
166:58 - upper those are going to be Ballinger
167:01 - band lower upper band and now I would
167:04 - like to have potentially to use the ls
167:08 - of the lower and over band but let's
167:10 - continue with d and for after we have
167:12 - calculated the indicators we can just
167:16 - run that part check out the
167:21 - indicators and now we can move on to
167:25 - calculate the intraday signal so signal
167:27 - intraday would be the followings so we
167:31 - have a intraday signal whenever the RSI
167:35 - is above 70 and the close price closes
167:40 - above the upper Ballinger band so
167:42 - whenever the RSI is above 70 and and the
167:45 - price closes above the upper boringer
167:47 - band we have a burst of momentum upwards
167:50 - or downwards that's my strategy idea and
167:55 - this would be our Buy Signal and the
167:57 - sell would be when at RS SI is below 30
168:00 - and simultaneously we have a close below
168:02 - the lower bar this price action pattern
168:04 - is a pure momentum pattern and we can
168:07 - again uh calculate using the applied
168:10 - Lambda function so one if RSI is above
168:15 - 70 and if the close
168:20 - price is above the upper band else we
168:24 - would like minus one if the opposite
168:28 - pretty
168:29 - much
168:30 - right R side below 30 and close below
168:35 - the lower band else we would like to
168:38 - have MP n if those conditions are not
168:41 - met and the axis will be equal to one we
168:45 - can run that and see what we
168:48 - call we have a small Arrow up
168:53 - here
168:56 - why okay so there's something wrong with
169:00 - our
169:01 - data we can call the info
169:06 - and everything is
169:09 - float what is the problem here
169:13 - guys
169:26 - I found the error guys I have forgot to
169:29 - add the X up here for both the upper and
169:33 - the lower bound and that
169:35 - was doing the error it's going to
169:38 - calculate the intraday strategy signal
169:42 - as well now and
169:45 - with that we are actually pretty much
169:49 - ready with our intraday signal and our
169:53 - daily signal and we can move on now to
169:57 - generating our entry points and our
169:59 - final signal the final signal that we
170:02 - are going to use to calculate our daily
170:05 - returns from to do that we are going to
170:08 - do the following so final DF return
170:11 - signal or return sign that will be the
170:14 - of our return and actually the direction
170:16 - of the position it would be the
170:18 - following we would have a short position
170:21 - whenever the daily signal is one so the
170:26 - prediction premium is above one standard
170:32 - 1.5 standard deviations High prediction
170:35 - premium let me grab that signal daily so
170:39 - whenever the signal daily is equal to
170:41 - one and the signal
170:46 - intraday is equal to one as well the
170:50 - intraday situation is that the price is
170:54 - super overextended our RSI above 70 and
170:57 - simultaneously the close is above the
171:00 - upper Binger band so I assume a mean
171:03 - reversion and on the day we know that
171:06 - the prediction premium the volatility is
171:09 - above the rolling standard deviation or
171:13 - 1.5 1.5 rolling standard deviations
171:17 - whenever we have those conditions met we
171:20 - are going into a short position else
171:23 - we're going into a long position
171:25 - whenever we have the opposite so both
171:28 - the daily signal and the intraday signal
171:33 - are minus one and else we have MP
171:38 - none and the axis would be equal to one
171:42 - as well and this would be our intraday
171:46 - signal to get into a long or a short
171:49 - position invalid syntax
171:52 - okay I forgot the com up
171:56 - here that's
172:03 - enough okay I'm not sure guys what's the
172:06 - problem here
172:10 - but all right I think I had forgotten
172:14 - this something you know okay now we have
172:17 - the intraday signal for each position my
172:21 - idea here is the following so whenever
172:24 - intraday whenever in the in a given day
172:27 - we have this signal the first trigger of
172:30 - that position we are going into a
172:31 - position and we'll be holding for the
172:33 - whole day so the first time it can be in
172:35 - 10:00 but we can have another signal in
172:38 - the same day uh at 5:00 we don't care
172:41 - about the second one we are going into
172:43 - full position position on the first one
172:45 - and we are going to hold for the whole
172:47 - day in this return sign column whenever
172:51 - we have the first occurrence of the
172:54 - signal what do we want to do we want to
172:56 - forward feel until the end of the day
172:58 - the same position sign to do that we can
173:01 - use the following trick return sign
173:03 - would be first let's explore what we
173:05 - have I'm sorry let's explore what we
173:07 - have so far so the return sign is our
173:13 - intraday
173:15 - final
173:16 - signal uh
173:19 - wow okay so this one should be
173:28 - one all right so whenever we have to go
173:32 - into a long position we have long signal
173:34 - right here the return sign we can
173:36 - observe that this happened at uh in 2021
173:41 - 18th of October at 605 then we had
173:45 - another signal on the 23rd of October
173:47 - then on the 28th on the 29th etc etc so
173:52 - in some days we may have only one signal
173:55 - but in some days we may have two or more
173:57 - we don't care about the second or the
173:59 - third signal we care only about the
174:00 - first one so at this point in time we
174:03 - had the signal and on the next bar so on
174:06 - the next 5 minutes on the open price we
174:09 - are going to go into a position and we
174:11 - want to hold it until the end of the day
174:13 - at the end of the day we close the
174:14 - position an easy way to calculate the
174:19 - strategy return would be to use just the
174:23 - vectorizer return right so we are going
174:25 - to forward f for every 5 minutes after
174:29 - the entry time we're going to forward f
174:32 - with the same sign the return sign
174:35 - column how do we do that final DF do
174:38 - group by pandas grouper we are going to
174:41 - group for every day we are going to
174:43 - group on a daily basis we are going to
174:46 - select the return sign and now we'll use
174:49 - the transform
174:51 - method and because we have supplied one
174:56 - or MP9 we are able to use a transform
174:59 - method and do Lambda X forward F and
175:03 - this would now forward F this value
175:06 - until the end of the day for every day
175:09 - where we have a Val it will be done by
175:12 - the first occurrence of the Val after we
175:14 - run that we can now select where the
175:18 - return is equal to one where we have the
175:20 - entry signal and as you can see now we
175:22 - have the first signal at 605 6010 615
175:27 - Etc ET until the end of the day we can
175:30 - now calculate the 5 minutes returns
175:34 - shift them backwards with one row for
175:37 - every day and multiply the return sign
175:39 - column by the signal intraday let's do
175:43 - that right now that would be The
175:47 - Following
175:51 - return percent change and we would like
175:53 - to get the forward return right so
175:56 - forward
175:58 - return and this would be equal to our
176:02 - return column shifted we don't need to
176:06 - apply Group by daily here because our
176:09 - data is continuous it's 5 minutes data
176:11 - but it's uh from the beginning of the
176:14 - day until the end of the day it is
176:15 - continuous for every day including the
176:18 - weekends and that's why we we can just
176:21 - calculate the 5 minute return and then
176:24 - shift it backwards with one row and our
176:27 - final strategy return will be the
176:29 - following so strategy return would be
176:32 - equal to final data frame forward return
176:36 - multiplied by the return
176:41 - sign that would be our strategy return
176:44 - but it will be on the 5 minute level so
176:47 - we want to get the daily strategy return
176:49 - so we'll calculate daily return data
176:52 - frame and it would be the final data
176:55 - frame do
176:58 - groupby we're grouping on the daily
177:00 - basis so for every day we're going to
177:03 - group by
177:05 - select the strategy return column and
177:09 - just
177:13 - Summit after we have the daily return
177:16 - data frame we can move on to the final
177:20 - step and we'll just calculate the
177:23 - strategy cumulative
177:26 - return that would be equal to expanding
177:29 - window so numi expand
177:34 - log and then cumulate
177:38 - sum and we just substract one in the
177:42 - end and we can plot the strategy
177:48 - return we set title to be
177:52 - intraday strategy
177:58 - returns we fix the
178:01 - yaxis uh values to be percentages again
178:04 - so set major
178:08 - format to be from
178:10 - anti percent format
178:16 - one the Y label we set it to return and
178:21 - we're pretty much ready with
178:24 - our graph and this is our strategy G as
178:28 - you can see we have an inaday uh
178:30 - strategy we may not trade for quite some
178:33 - time up here we haven't trade for almost
178:35 - six months but then we had some trades
178:38 - and yeah whenever we have uh the match
178:41 - of the daily signal and the intraday
178:43 - signal
178:44 - that's when we are going into trades and
178:47 - we have the returns that's a strategy
178:49 - using the machine learning model the gar
178:51 - model to predict one day ahead
178:53 - volatility in combination with simulated
178:56 - intraday data and with that we're
178:57 - finishing the third project and I really
178:59 - really hope you've enjoyed it guys you
179:01 - can find the data and the notebook Down
179:04 - Below in the link if you're interested
179:07 - to learn more how to back test Quant
179:09 - strategies how to do algorithmic trading
179:12 - there will be a link in the description
179:13 - ion to my website where you can learn
179:16 - more about algorithmic trading and quad
179:17 - trading with python

Cleaned transcript:

in this comprehensive course on algorithmic trading you will learn about three cuttingedge trading strategies to enhance your financial toolkit latchezar teaches this course he is an experienced quantitative researcher and data scientist in the first module you'll explore the unsupervised learning trading strategy utilizing SP 500 stocks data to master features indicators and portfolio optimization next you'll leverage the power of social media with the Twitter sentiment investing strategy ranking NASDAQ stocks based on engagement and evaluating performance against the QQQ return lastly the intraday strategy will introduce you to the gar model combining it with technical indicators to capture both daily and intraday signals for potential lucrative positions hello and welcome to this free code Camp course on algorithmic trading machine learning and Quant strategy with python my name is Lazar and I'll be your instructor toout the course and in this course we are going to develop three big quantitative projects from start to end and the course overview would be the following first we're going to talk about algorithmic trading Basics then we're going to talk about machine learning in trading some obstacles and challenges we may face while using machine learning in trading then we are going to develop the first project which would be an unsup rised learning trading strategy using stocks from S&P 500 the next project would be Twitter sentiment and it would be using data from NASDAQ 100 stocks and the third one would be focusing on one asset it would be an intraday strategy using a gar model to predict the volatility it would use simulated data and after that we are going to have a quick wrap up and with that we'll finish the tutorial but before we continue I would like to mention that this tutorial should not be ConEd as a financial advice it is for educational and entertainment purposes only we are going to develop some Concepts and come up with strategies in the end but it's not a financial advice and you shouldn't make any decisions based on it as well for this course I would assume that you have at least some python knowledge and understanding because down the road will deal with some complex problems and if you're new to python you may get B down nevertheless you learn some very interesting Concepts so stay along and let's get into it okay algorithmic trading Basics so what is algorithmic trading it is trading on predefined set of rules which are combined into a strategy or a system it is developed by a programming language and it is run by the computer it can be used for both manual and automated trading by manual what I mean is that you may have a screener which comes up with a set of stock stocks you want to trade on a given day or you may have a algorithmic strategy which is developed into an alert system and whenever the conditions are triggered you get an alert but you execute it manually on the other hand you may have a completely automated complex system which does a lot of calculation comes up with uh positions and sizing and then executes the trade automatically okay so what is the role of python in algorithmic trading python is the most popular language used in Alor trading quantitative finance and data science and this is mainly due to the vast amount of libraries that are developed in python as well the ease of use of uh python it is mainly used for data pipelines research back testing strategies as well it can be used to automate strategies but python is a slow language and it can be used to automate low complexity systems if you have a really highend system which is really complicated and it needs to execute trades really quickly you would use Java or C++ for those strategies our gmic trading is a great career opportunity it's a huge industry there are a lot of jobs with hedge funds Banks Prop Shops and I've just checked the average yearly base salary for Quant researcher is around $173,000 and this is not including the yearly bonus it is a great career opportunity and if you're interested into it the main things you need to know are python you need to know how to back test strategies you need to know how to replicate papers and you need to know machine learning in trading if you're interested into it I'll definitely advise you to go for it okay so let's move on and talk a little bit about machine learning in trading and some use cases of machine learning when we talk about supervised learning we can use it for Signal generation through prediction for example we can come up with buy or sell signals on a given stock or a given asset based on predicting the return or the sign of the return of that asset we can as well use it in risk management for example we may use a prediction to determine the position sizing or the weight of a given stock in our portfolio or to predict where exactly should our stop loss be and with unsup learning we can use it to extract insights from the data for example we can discover patterns relationships or structures within the data for example clusters and use it in this way to uh help our decisions what are some of the challenges that we may face while trying to apply machine learning in trading and the first theoretical challenge we may face is the so called reflexivity feedback loop and it is referring to the phenomenon that if we for example have a machine learning model that is uh predicting that a stock is going to go up each Friday and we can form a strategy around to profit from that phenomenon for example we are going to buy each Thursday and then sell on Friday to capture that price increase move if we find this strategy throughout predictions and start trading it with time other Market participants as well are going to find this Market phenomenon and start to exploiting it as well which would cause the price to start going up on Thursday because everybody's buying now on Thursday instead on Friday and then this strategy is going to be Arbitrage away so it's this reflexivity feedback loop which is making predictions quite hard what is most hard while applying machine learning the most hard thing is to predict returns and predict prices the next quite hard thing to do is to predict return signs or the direction of a given asset is it going to to go up or down the next thing is to predict an economic indicator for example it is quite hard to predict nonfarm payrolls or weekly jobless claims and a thing which is not that hard or quite straightforward is to predict the volatility of a given asset furthermore there are some technical challenges like overfitting a model or generalization overfitting is that the model is learned the train data too well and it fails on the test data and generalization is that the model is not performing the same as on the real data as well we may have nonstationarity in our training data and regime shifts which may ruin the the performance of the model and the last thing is that if we have a really complicated model or um neuron Network it is like a black box and we are not able to interpret it correctly what is the usual workflow process in algorithmic trading and machine learning that would be to collect and prepare the the data then develop a hypothesis for a strategy then you have to code the model and train the model and then finally back test the strategy okay guys and some key takeaways from this course so you learn high level Concepts in quantitative Finance as well practical machine learning in trading you develop a project from idea to back test final results however we will not automate or execute any trades we'll just develop a strategy this course is all about developing a strategy from start to end so you see the workflow and this is just a purely research project for educational purposes I repeat it should not be construed as any Financial advice whatsoever and with that we can move to the first project and let's get into it okay so first project is about unsupervised machine learning trading strategy we're going to use data from S&P 5 500 stocks let's talk a little bit about unsupervised learning in trading so it involves machine learning techniques to analyze financial data and discover patterns relationships and structures within this data without predefined labels or Target variable unlike supervised learning where the model is trained to make predictions unsupervised learning focusing on extracting insides from the data and some use cases would be clustering which we are going to use in the first project dimensionality reduction anomaly detection Market regime detection and portfolio optimization in this project what we are going to do is first we're going to download all the prices data for all S&P 500 stocks then we're going to calculate different technical indicators and features for each stock next we're going to aggregate on a monthly level and filter only the top50 most liquid stocks from the S&P 500 for each month next we are going to calculate PL monthly returns for different time Horizons to add up to the features and the next step would be to download the F French factors and calculate rolling Factor betas for each stock as well to add to the feature set and at this point we'll have enough features to fit the model and either make predictions or in our case we're going to fit a c's clustering algorithm an unsupervised learning model and we will use it to group group the stocks into similar assets into clusters and from those clusters then we will be able to for each month select stocks from a given cluster and we are going to analyze the Clusters and select a particular cluster and then for each month we are going to select those stocks within this cluster and form portfolios however these portfolios will be optimized so the weights of the stocks within the portfolio we're going to find them by using the efficient Frontier Max Sharpie ratio isue portfolio weights and then we are going to form the portfolio hold for one month and rebalance at the end of the month and you know form another Max sharp ratio portfolio in the end we'll have the strategy returns for each day and we will be able to compare our portfolio strategy returns to the S&P 500 returns themselves actually not small limitation is that we're going to use the most recent S&P 500 stocks list which means that there may be a survivor buyers in this list this is a huge issue actually in reality you should always uh back test strategies using survivorship free bias data what is survivorship bias it is the condition when a stock which have actually went out of the S&P 500 because it was failing is currently not in the list right so last year for example there was a stock which was failing and going down down down so at some point in December last year they removed it from S&P 500 and they included a new stock so if we have made the optimization last November we could end up with having this stock into our portfolio and actually it affecting our portfolio results but if we use the most recent S&P 500 symbols list this stock would not be there so that's survivorship bias and for the given project we are not going to deal with this survivorship bu so the list we are going to work with has survivorship bus most probably so yeah that is limitation you need to know in the second project we are going to develop a Twitter sentiment based investing strategy we are going to use the NASDAQ 100 stocks and Twitter sentiment data what is sentiment investing this approach focuses on analizing how people feel about certain stocks Industries or the overall Market it assumes that the public sentiment can impact stock prices and for example if many people are positive about a particular company on Twitter it might indicate potential for that company stock to perform well what we are going to do first we're going to load the NASDAQ stocks Twitter sentiment data then we're going to calculate a quantitative feature of the engagement ratio in Twitter for each stock after that we're going to rank all the stocks crosssectionally for each month and create an equal weight portfolio in the end we're going to compare the return of this portfolio to the NASDAQ itself the second strategy is much smaller than the first strategy there is no machine learning modeling in this strategy but the idea here is to show you how alternative or different data in this case sentiment data can help us to create a Quant feature and then create potential strategy out of it that's the idea of the second project and the third project is about an intraday strategy using a gar model in this one we are going to focus on a single asset and we'll be using simulated data actually we'll have daily data and intraday 5 minute data but what does an intraday strategy means this approach involves buying and selling Financial assets within the same trading day to profit from the shortterm price movements intraday Traders usually use technical analysis real time data and different risk management techniques to make decisions and profit from the strategies what exactly we're going to do in this project first we are going to load the simulated daily data and the simulated 5 minute data then we are going to define a function which would fit in a rolling window a gar model to predict one day ahead volatility of the asset after we have that we'll calculate prediction premium so we'll predict the volatility and we'll calculate the prediction premium and form a daily signal from it after we have uh calculate that then we'll merge the daily data with the intraday data and calculate intraday technical indicators to form intraday signal so we will have a daily signal and then on top an intraday signal so two signals and after we have that we'll generate position entry and hold until the end of the day in the end we're going to calculate the final strategy returns and this is it for the third project the idea is to show you how predicting volatility Works in uh intraday strategies and yeah with that guys we are ready to jump into the first project so let's get into it okay let's start with the first project the unup rised learning trading strategy but before we continue with the coding guys the first step would be for you to pause the video and install all the needed packages for this project so I prepared a small list up here with all the packages we are going to need for the project so those are pandas numai MP lip stats models pandas data reader daytime wi Finance SK learn and buy portfolio op how you can install the packages you can do it the following way you can open on Ana prompt like that right and then just write pip install and the package name pip install pandas for example and do that for each package that's one way another way to do it is through the notebook itself you can uh type the following so pip install and then the package name so pip install pandas you do control shift and run the cell and it will install the package pause the video take your time install all the packages and let's continue okay so the first step would be to download the S&P 500 constituents prices data but before that we'll have to import all the packages we use throughout this project I've already prepared that those are all the packages we've just installed and now we are importing into the Jupiter notebook The Next Step would be to download S&P 500 constituents data to do that we can go to this link on Wikipedia and up here you can see that they have a table containing a S&P 500 component stocks like the symbol the the name of the security sector industry date added as well so quite some data uh so we would like to load this table into our Jupiter notebook how we can do that we just call a pandas read HTML function and let's see what this would return us okay so this is returning a list containing two elements it looks like two data frames so actually we are interested in the first frame yeah exactly we will assign that to an object called S&P 500 and the next step would be to grab this symbol column and extract all the symbols in a list but before that I think we would have to make a little cleaning on some of the symbols because I know that one or two of the symbols contain a DOT and this would give us an error while we download data from wi Finance so we have to actually replace all dots with Dash and that would do the job next step would be to you know just grab list with all the stocks so yeah we can just use symbol. unique to list we will assign that to a object called symbols list and yeah that would be our symbols list of all S&P 500 stocks as we talked uh in the beginning this list of stocks is not Survivor ship bias free so you need to know that it's uh quite some of limitation all right so we would like to download the data up to a few days ago let's define an object which is end dat and we'll use 2023 September 27th and the start date would be P to date time and date it would be exactly 8 years ago so we can just use the end date and we will substract eight years out of it how we can do that we just say pandas to datetime we'll convert to daytime the end date and then we'll just substract pandas date of set and then Supply 365 * 8 all right so now we have the end date which is yeah 27th of September which is a string and the start date would be a time stamp but that's all right because we are going to download data from y finance and this function we use from y Finance download function which takes tickers as an argument so that would be our symbols list then start which would be our start date and end which would be our end date and now this will download all the S&P 500 constituents if we run it yeah it will take some time all right so we've downloaded all the data for the S&P 500 stocks we got it up here so the next step would be to actually yeah we'll comment out everything and have uh this object printed out as you can see up here we have the column and then we have like a multi index column so first we have the adjusted close and then we have the adjusted close for each stock that's not really convenient to work with the whole data frame at the moment we have 2012 rows and 3,8 columns that's really inefficient how we can overcome that we would just use the stock method which now creates a m index the first level would be the date and then for each date we have the corresponding 500 stocks adjusted close close high low so that's much more convenient we have six columns and almost 1 million rows so we have to change the datea frame to be stocked or actually we can move this method right up here after the download directly and yeah now we have our data frame stacked Next Step would be I would always when I have a multiindex I would always want to have labels on both of the index levels so we'll say index. names and we'll assign the new name so date and thicker that will be our new multiindex names as you see they change step here next step would be to fix the columns a little bit I I would like to fix the column names to be not as titles but with a lower letters so that would be DF do columns DF do columns do string lower and that's pretty much it with the downloading and fixing data a little bit before we move to the next step which would be to start calculating technical indicators and features of all those 53 stocks okay so in the second step we can start calculating the features and Technical indicators for each stock we are going to calculate the garm class volatility RSI Ballinger bands ATR macd and dollar volume for each stock let's first start with the garm class volatility what is garm class volatility it is volatility measure usually used in Forex Trading but it works for stocks as well it is approximation to measure the intraday volatility of a given asset and that is the formula right here so what do we do we Define a new column called garmon class V and it would be the following so log from the high minus log from the low so that would be yeah from the low this whole thing is squared and then it is divided by two then from this we substract 2 * log 2us one multiplied by subtraction of log from the adjusted close yeah minus log from the open again the hold think is squared and we just close another bracket and that should be it that should be the gar class volatility yes so we now have calculated the gar class volatility for each stock it is calculated on a given role we don't need to do any fancy calculations for this one the next one is RSI so how do we calculate the RSI on each stock what we're going to do we are going to group Buy on the thicker level so level one the multi index has level zero and level one which is the thicker level zero is the date level one is the thicker so we grew by on level one then we are selecting the column which would be adjusted close and apply the transform method and within the transform method we just apply a Lambda function which would be so now we have grouped grouped by on each thicker and what do we want to do we want to calculate the RSI to calculate the RSI we are going to use the pandascore TA package which is the package to calculate pretty much all of the needed technical indicator so we use the pandascore TA package and from the pandascore we use the RSI function the side function we have to supply the close price which would be X and the length would be 20 and yeah if we do that you see now we have the RSI for each stock how we can double check our work we select apple and then RSI and then we'll plot it and yeah as you can see the r side goes up and down up and down so we have worked correctly the next indicator we would like to calculate is Ballinger bands and actually we would want to have the lower band the middle band and the upper band but there is one specification for each indicator from now on we would like to normalize and scale the indicator itself so for the Binger BNS will will supply the log from the close price first the function we are going to use is from Panda CA B bands and it is taking the clothes just for presentational purposes we Supply a upper adjusted close and the length will be 20 when you run this function it returns five columns so the first one is the lower band the second one is the middle band and the third one is the upper band we have to take this into account and what we are going to do is pretty much you know Define a new column BBL low Ballinger band low and we use the the same idea as for the RSI we are going to group by each thicker select the adjusted close column and then we use the transform method Lambda function and in the Lambda function we say pandas ta B bands close would be equal to X actually it will be equal to log of x so MP log 1 p and then the length would be again 20 when we run that it will return those five columns what we actually want to do is assign to B below the First Column which we know is the B the lower Ballinger band we can say iock all the rows and the First Column we can repeat the same operation for the midband and for the upper band okay but this happens when I forget something and I think I have forgot I forgot the second curly bracket to close the curly brackets let's see what this would return yeah guys after you return and calculate something you can just select all the the code you've used and comment it out okay we forgot to change the names that would be BB mid and first index that would be BB High the second index okay let's run that again and see what we get awesome now we have the lower bager Bond Middle Ballinger Bond and upper Ballinger Bond and we have the data scaled and normalized next step is to calculate the ATR for each stock however the ATR function needs three inputs so three columns not only one column and when we use transform method in pandas it is actually working when you select only one column it would not work if you have three columns as an input so we would have to use another approach more specifically that would be a group by apply and to do that we need to Define our own custom function to calculate the ATR we can uh double check what the ATR function from pandas ta requires as input and it is the high so we have to supply the high then we have to supply the low and the close price furthermore we can supply the length for example 14 and yeah it will mess mess up with the data if we run it like that because we have to select the data for a given stock but yeah you see that it requires three columns so here we we going to define a function called compute ATR it will take stock data and here we will just calculate at which would be pandas ta. high would be stock data High the low then we have we need a close the length would be 14 all right and a little detail we are going to add of here is we are going to normalize the data while we calculate it so that would be ATR do substract first first we are going to the mean and then we are going to divide it by the standard deviation so ATR do standard deviation and yeah that will be our ATR indicator function now we can just say ATR create a new column called ATR and group by level one again so we are applying that for each stock however up here when we use the group by apply we need to add an additional argument to the group bio which is group key is equal to false because if we don't do that it will double the the date column so it will return another date colum and we have a triple multiindex with two date columns we don't want that so we just say group Cas equals to false and then apply this function and this will now calculate the ATR index normalized for each stock the next indicator we are going to calculate is the macd indicator and for the macd we're going to follow the same logic as as for the ATR indicator we're going to Define our own custom function to compute the macd so it will be called compute macd it will take the close price and up here we are going to say magd is equals to pandas ta. magd close equal to close length is 20 and then we would like to get the First Column which would be returned however in the end as well we are going to normalize the dat we're going to the meain the series and then we're going to divide by the standard deviation why do we do that right away we're normalizing the data because we are going to use it into a machine learning model we are going to Cluster the data we want to do that straight away and don't think about it later in the future so here we are going to do again group buy level one each scker on each stock group Keys equal to false and then apply compute Mark D this would calculate calate the mag the indicator for each stock all right we have an error why do we have this error that's quite strange oh okay I'm sorry I'm sorry I forgot to add the adjusted close column up here and this is driving the eror all right so we will return back I lock in the first corn that should be guys all right yeah now we have the macd as well calculated and normalized as you can see we have the data looks pretty good so far the only indicator we are not going to normalize is the RSI and there is a particular reason for that but you uh understand more about it when we come to the clustering part all the other indicators we are going to normalize and the final one is the dollar volume so we're going to create a new column dollar volume which would be equal to the adjusted close multiplied by the volume however we may want to actually divide that by 1 million for each stock because we know that millions of shares are traded each day and this would make sense as you can see now the data looks much better for the dollar volume right and that's pretty much it with uh calculating the first batch of features our technical indicators now we have a really beautiful data frame for each day we have all the 500 stocks we have the close price low open volume garment class volatility RSI Ballinger band ATR macd and the dollar volume for each stock and we are now ready to move to the next and the third step in the third step what we want to do is to aggregate on a monthly level the data and filter the top 150 most liquid stocks for each month why do we do that we do that to reduce training time for any potential machine learning model and experiment with features and strategies what is my idea here I would like to aggregate all the indicators so those five I would like to take the end value the end of the last value for the month as well the same for the adjusted close price and for the dollar volume I would like to get the average dollar volume for the whole month for each stock we can start actually what we can do first is uh take the data frame un stock the thicker level so we'll unstack thicker and then we're going to select the dollar volume column and if we run that we have the dollar volume for each day for each stock now right and what we can do is just resample to monthly and take the mean this should resample to monthly now as you can see we have monthly index end of each month and we have the average dollar volume for the month what we can do now is just stack it back into a MTI index like that and we can say two frame to make it uh a data frame with one column two frame dollar volume beautiful that would be the first step however for the indicators what we can do is we can follow the same logic but we need to select the exact columns actually we may create a list of columns so last call and this would be our list of columns for which we want to do the same operation however we would use the last method up here instead of mean and those columns would be C for C in DF do columns. unique and yeah the first element C for C in do in DF columns. unique if C is not in the following columns list so if the column is not dollar volume it is not volume it is not open it is not high low or close pretty much we want to do that only for the technical indicators columns so for those columns we don't want to use these columns for our aggregation we want just the Fe we are creating the features data frame in the end right we would use the dollar volume to filter out the most liquid stocks the dollar volume would not be featured in our model as well NE neither the volume or open low high close after we have defined the last columns uh we can actually proceed with the next aggregation we have done the dollar volume aggregation the next one would be for DF last C we are going to unstack actually we are going to un stack before that so we are going to unstack we're going to unstack and select those columns and then resample to monthly and then just use the last volume like that and again we're going to stack backwards into a multiindex voila and now what we can do is concut those two together we can say the following pandas cona and that would be axis axis one and boom we have the dollar volume the average dollar volume and the last value for adjusted close ATR and other technical indicators for each month now we have aggregated the data to monthly level for the features we would need actually what we can uh what we can add up here is a small drop and that looks much more beautiful and we can call that data and let's visualize what we have yeah that's our data all right and that's the first step with the aggregating to a monthly level The Next Step would be to calculate the fiveyear rolling average dollar volume for each stock and then you use this uh aggregated dollar volume to filter out only the top 150 most liquid stocks for each month how do we approach that first we can start by selecting the dollar volume like that so what we can do select the dollar volume and stack the thicker level and now we can use a rolling function with a window of 5 * 12 so 5 years and then we can calculate the mean as you can see now we have the rolling average mean for each stock rolling average 5year dollar volume for each stock and again we can just stack backwards and that's pretty much our dollar volume column what we can do now is we can assign it to the dollar volume column update to The fiveyear Rolling average for each stock awesome after we have that the next step is to calculate the dollar volume rank cross section for each month how do we do that we can say data Group by level zero or date so we can Group by on date for each month we're going to select the dollar volume and just rank ascending equal to false let's see what this would give us ascending and now as you can see we have all the stocks ranked by Dollar volume and the the ones the the guys who have the smallest dollar volume have the the highest rank so we want the top 150 and from here we can pretty much very easily select the stocks which are below 150 for each month and those would be the top50 most liquid stocks for each month after we have selected them we can just drop the two columns we can drop the dollar volume and the dollar volume rank because we are not going to need them anymore access equals to one and that's pretty much it for our we can assign that to data and that's pretty much it with our third step where now we have aggregated monthly data for all the features we would need plus the adjusted close price and we can move move on with the next step the fourth step and that would be calculating monthly returns for different time Horizons and add them as additional features to the ones we already have here okay so let's move to that step I'll just cut those sales in the middle all right why do we want to calculate the monthly returns for different time Horizons and uh add them to the feature set because we may want to capture time series dynamics that reflect for example the momentum patterns for each stock to do that we can just use the pandas data frame method uh percent underscore change and Supply the different logs my Approach would be to use logs for 1 month two months 3 months 6 months 9 months and 12 months that's uh like six different uh LS to really capture the momentum patterns how do we approach that let's first start by for example selecting the Apple stock I just want to make a little example so we'll select the Apple stock and let's see what we have here all right now we would like to calculate the returns for the following lcks for one month two months 3 months 6 months 9 months and 12 months right as well we may want to have an outlier cut off because we are dealing with a lot of stocks there will definitely be outlier values in the returns of those stocks what do we want to do we want to with them by clipping them what clipping does is that for all values which are above the outlier threshold they will just be assigned the threshold of that percent up the cut off value we may want to have it as a 0.05 which means the 99.5 percenti that would be our outlier cut off and now what we do is just for each log so for log in logs for each log we are going to create a column which would be the return for the given log for the given log month right that would be the column and then we grab the adjusted close then we just do percent change Lo so for each lck we'll calculate the following column which would calculate the given return and then we want to deal with the outliers right so that would be pipe Lambda so far we have the adjusted close and then we calculate the return for the given L and then we input that into the pipe Lambda X we can clip now we clip the return and we can clip the lower band the lower cut off we'll use x quantile and we supply for the lower cut off just outl cut off and for the upper one we Supply x quantile one minus outline cut off then we add one to the power of 1 / by the log and we substract one in the end and that should be it guys now we can see that for our Apple stock which is G right we have the one month return two months return 3 months 6 months 9 months 12 months Etc how we can extend that we just use the same approach we use for the at and the macd indicators we will create our own custom function and then we you just use the group by apply methodology for the bound data frame so calculate returns it takes DF for example and just move that a little bit return DF in the end however up here we have to change that to DF this one to DF as well and that's pretty much it our function and now we can say the following data equals to data. group by level one because we Group by on the thicker level so we can say uh Group by ticker or level one let's say level one and then we want Group keys to be false so we don't have two uh date indexes assigned to the new date frame so Group keys false then apply we apply the calculator T function and in the end we may want to drop the na values and yeah this would take some time and yeah that's pretty much it I mean now we have added the return features as well which we would use to capture momentum patterns for each stock and that's pretty much it with the fourth step the calculating the monthly returns for different on Horizons and we can move to the next step which is really interesting actually adding even more features to our data set that would be to download the F French factors and we are going to calculate the rolling Factor better for each stock in our data all right let's move to it okay so in this step we're going to download the farm French factors data and calculate the rolling Factor betas for each stock in our current data set so we want to introduce the F French data to estimate the exposure of our assets to commonly known risk factors and we're going to do that using a regression rolling OS model the five round French factors namely Market risk size value profitability and invest M have been shown to empirically explain asset returns in the past and are commonly used in the asset management industry to assess the risk return profile of different portfolios so it kind of makes sense to include them in our current feature data set how we can do that we can use the pandas data reader package we which we imported as web we can use this package to download the F French Factor models but before that we may want to to take a look at the we might want to take a look at the data so we can Google it Farm French factors and up here you can find the canid French data Library just click on it and this is the part we are interested in in FAL French 5 research factors 2 * three so those five factors that's the data we are interested in if you scroll down a little bit you can find the daily data right here you can download it as txt or CSV file as well you can check the details they have monthly returns and annual returns we are interested in the monthly returns as our data is already on a monthly level right so what we can do is uh we can say the following web. data reader data reader and up here we have to supply the name of the exact Factor so the exact file so I have prepared it already that's the name then we say F French then the start date we want 2010 this is returning a dictionary with two keys the first one is the monthly factors and the second one is the yearly factors that's awesome so we want the monthly factors only and that's the data we have 164 uh months up to August 2023 that's pretty good however I think we don't even though the riskfree return is pretty solid right now it is out of the scope of this tutorial so we'll just drop it we say drop RF AIS equal to one and and that's perfect we can call this uh assign it to factor data and yeah that's our Factor data let's check the index the index is uh monthly yeah all right so I think we have to fix the index as well so we can call of pandas to daytime and Supply the index let's see if this would work okay we have to use to time stamp all right okay so now we have fixed the index as well as you can see now it's it has the year the month and the beginning of month date however our data is end of month that's one thing we have to fix another one is that I see that the factors are in percentages so we would have to divide them by 100 how we can fix that we can just say resum P to monthly and get the last value which would fix immediately the issue yeah with the beginning of mandate next we can say divide by 100 and that's perfect we just assign that to factor data comment it out yeah next we want to fix the name of the index to be just date and the next step would be to join with the pretty much join with the one month return why would we want to do that because at the beginning of each month we have the factors and then we have the return of each stock at the end of the same month so now we have fixed the date of the factors we can just join with the end of month return and then we can can regress them and take the beta right if the factor is predictive we have it at the beginning of the month and we will regress it with the return of the end of the month so we'll get the the the beta how we can do that factor data join and then from our data we can select the one return column let's see what this would give is that's perfect now we can sort the index and just assign to factor data what we may want to do here is to double check our work we can select two stocks for example apple and let's say Microsoft and we can double check the return yeah the return is different but the factors stay the same looks like we' worked correctly and we are ready to move to the next step in this step we are going to filter out stocks that have less than 10 month data why are we doing that because we are going to use rolling window for the regression of around 2 years 24 months and stocks that don't have enough data would actually break our function so we have to remove them from the data set how we can do that we can say Factor data. groupby level one and then just call the size method and now we can see how many months of data we have for each stock okay guys and I just realized that we have made a small mistake somewhere because we have only 23 months of data for each stock and I had to go back through the code and actually f figure out that on this step uh this part was missing which was the data dolog selecting all the rows and then the dollar volume column you can just rerun this sale and you can see that here our our first month is in 205 November and after that it was 2020 so when you add the dot loog and selecting the all the rows and the dollar volume column you get the fixed data then we can rerun as you can see up here it's 2021 the first date if we run it it should be much backwards in the past yeah so 2017 31st of October and we have to do that again for the factor data two and if we run now yeah now we have 71 months right and the idea here is uh that we remove all the stocks that have less than 10 months of data how do we do that we can just assign that to observations and then we can save valid stocks that would be observations that would be all the stocks that have more than 10 months of data observations yeah those are our valid stocks and now we can just use that as a filter so Factor data would be Factor data again Factor data. index do get level values we get the thicker values maybe I can show you what this is returning this is returning uh yeah an object with all the the stocks we have in the thicker index part of our M index and then we can just say is in the valid stock stocks and this would filter out pretty much everything we have to add this part okay okay so I think oh yeah I missed this part should be all stock index those are the stocks we are going to remove so if we take that out yes so now as you can see we hit before we hit 10,21 rows now we have 10,250 we've removed around 51 rows and yeah with this step we are now ready to calculate the rolling Factor betas we would like to do that simultaneously for all the stocks in our Factor data we can just use the same methodology with Group by and apply a function that would be Factor data. Group by and then we're grouping by level one by ticker Group keys should be false and we can put that into brackets so we can continue on the next row so apply Lambda X and now we want to use the rolling regression actually we may want to explore the rolling regression rolling ORS python it takes the endog and exog all right we here Supply endog and exog so the endog would be our return column right so X return one month and our xog would be everything else what we can do here is say x do drop the return one month column and this would return all the other this would give all the other columns without the return and actually we can add a constant here on the spot so we add a constant and I think the next one we have to supply the window we decided the window to be 2 years right so that would be 24 right 24 months and there is another mean observation the minimum number of observation required to estimate the model we have to supply this one as well this one is a little bit more tricky we have to supply here to have at least the total number of columns plus one so that would be Lan x. columns plus one and this should be our model now we have to say fit and then params and then we can drop the we can drop the constant because it will return constant as we added a constant up here right and I think this should pretty much work except that maybe sometimes we would not have exactly 24 months of observations but we still may want to run the regression so what we can do is use as a window the value which is smaller than two Val so it either 24 months or we can use the number of rows we have in the for the given stock right and we know that we have stocks with more than 10 months of data so if one of the stocks have like 15 months of data you just use the 15 months as a window instead of 24 and yeah I think that should pretty much be it right okay series object doesn't have fit why right maybe okay so that's pretty much that's pretty much uh our rolling Factor betas we have calculated them now we can assign them to betas and that's our rolling Factor BS guys in The Next Step what we want to do is to join them to our current features and with that we have our Full Features data set but before we join them we have to think about a little bit now we have the rolling Factor betas where we used the factor at the beginning of the month and the return at the end of the month so this beta we would actually know at the next month right at the end of the month we go we'll be able to run the regressions and have the betas but we'll have them in the next month so we cannot just blindly join them to the features data we have so far what we have to do is to shift them with one month forward before we join them to the data because these values we would have no not known in the same month we would know uh we would know the mon the the rolling Factor better for example for the end of October we would know them in November what we have to do is we have to shift with one month forward on the ticker level so for each ticker not like the whole data frame if we just say betas do shift it will run it will shift with one row downwards and for example the value of Verizon will come up up here right just see that so Verizon is 0.3 if we apply shift it's now here so that's obviously not correct so what we have to do is we have to first group by and we can do that group by thicker and then we can shift that would now uh do it correctly we can grab that and Supply it here and data will be equal to this and the next step that we want is to impute the missing values of each factor with the average for that factor beta how we we can do that we can say first we can create a a list with the factor columns factors would be just those five columns we can say data. log all the rows factors we'll just select all the factors and all the rows and then we can say the following data. Group by ticker for each ticker we'll select the factors again and then we'll just apply a Lambda function Lambda X where we will F all the missing values with the mean of this Factor but I think because we are doing grp Group by apply we have to add the group keys to be false and now we should have fixed our missing vales issue voila so now we don't have any missing Valu so all all the nas are imputed by the average for this factor and yeah now we can just say drop a if there are any and we can say data. info to see our final result and that is now beautiful guys this is our features data set however I see a column we don't need here which is the adjusted close we just have to drop it data is equal to data drop adjusted close axis equal to one and and this is our features data set guys so we have 18 features at this moment we are now ready to apply machine learning models from here on what we have to decide usually is for each new month we have to form a portfolio with some of the stocks from our uh data set so as we know we have for each month we have the top 150 most liquid stocks and now now at the end of each month we have to decide the stocks we want to have in our portfolio for the next month that's where we can use a machine learning model first we can use a machine learning model to predict which stocks to include in the portfolio as well if we have a long short portfolio we have we can predict which stocks to be long and which stocks to be short but in this course we are just focusing on Long Port foros so we can use a machine learning model to predict which stocks as well we can use machine learning model to predict the magnitude of the position in each stock so what is the weight in the portfolio and the other way is to use a machine learning model in our case a nonsupervised model to decide which stocks to use in the portfolio based on grouping that's why we are going to use a clustering algorithm a KES clustering to keep things simple because from this point on things can get really complicated but yeah that's so far was the preparation to get the data to fit into a machine learning model guys and yeah I'm really excited in the next step we are going to fit a c's clustering algorithm and split the data in in a few clusters each stock will be assigned a cluster and then we'll uh be able to analyze the Clusters and decide what we do further let's move to this step okay so in this step we're going to fit the C clustering algorithm for each month and split the stock stocks into four different groups based on their features y four I've already did a a little work beforehand and I've estimated that the optimal number of clusters roughly on average for each month is around four so we'll use four clusters for each month uh from now on the specifications about the K's clustering algorithm is that it uh it may assign the centroids of the Clusters around randomly and then it assigns a given point to the cluster based on the distance from the centrate to that point to uh help you understand a little bit more so this is an example with the same data but different number of clusters specified when fitting the model so in the first one we have two clusters as you can see really well defined three four and five whenever we initialize the model we have to specify how many clusters we want the data to be grouped in and the algorithm would go in assign random points and then the closest points to the Cent read so Random Cent the closest point to the Cent would be assigned to that cluster and until uh that will be repeated until all points have an assigned cluster how do we start here let's first import the C's class from sklearn do cluster we are going to import K means and now what we want to do right is fit uh K means model for each month and assign a cluster to each stock so get the label we can do that at once using the group by apply logic we've already used for ATR and macd we can define a function which should be get clusters to take data or DF it's up to you and then we'll create a new column called cluster and here we'll fit the CES model so the CES class takes first argument number of clusters that would be four as we already know I've done my research beforehand and then we'll use the random State argument to be zero this would ensure it works like random seed this would sure that we have the same results through different calls and there is another really important argument here they need this is the initialization method of the centroids actually here for now we'll use a random initialization but we can actually Supply here the initial Central points for the Clusters and we may use that in the future so let's see the first result we fit the model follow to the data frame and then we'll get the labels we will return the labels and we will assign them to the cluster column and we just return the data frame in the end and now we can say the following data drop na we will ensure that we don't have any na in the data Group by level one or date and then Group keys equals to false because we do group by apply and then apply get clusters this will take some time but it will now go through each month and fit the K clustering algor with four clusters and assign the given cluster to each stock let's wait for this to run okay so the clustering is finished let's take a look at the results now we have the cluster column and in it we have the assigned values from 0 to three so four clusters 0 1 2 three for each stock and that is done every month now the next step would be to visualize actually our clustering job which is pretty hard when we use more than two features and in our case we have 18 but that's why we didn't apply we didn't apply normalization to the RSI because now we can use it to visualize the clustering job in Better Way all right so I've already wrote down a small function for that up here this is a plot function uh we just select the stocks for the first the second the third and the fourth cluster and then we just visualize them with a with a scatter plot and my idea here is to do that for each month so we can use this function it will select for the scattering it will select the first column which is the 8 R and the seventh column which is the RSI and let's do that now p. style. use we use the ggplot style always and now for I in data. index get level values and we'll get all all the months so unique to list for each month we are going to select pretty much this month right so data. x uh Xs and then I level zero so we are going to select the month and we're going to use this function so we for each month we will just plot the Clusters however we may want to add the month to the title so we can do that in the following way date all let's run this thing and check our clustering job all right guys so now for each month we have uh our clustering job so those are all the stocks and on the Y label we have the RSI values and here we have the ATR values and those are our clusters as you can see they're pretty well defined throughout the different months and if you go through it the first observation is that actually the clustering is working in such a way that all the stocks which are around 60 to 70 RSI are in one cluster then you have the two middle two midles from 50 to 60 and from 50 to 40 and then you have the down cluster here around 35 and what we can see is that this assumption or this observation holds throughout each month of clustering however the problem is that for example in this month cluster zero is around 30 35 RSI cluster one is around 40 to 50 right but then uh this is the same for the next month but at some point cluster zero is around 50 and cluster one is around 60 because the Cent RS are random our clustering gets random and my idea for the strategy we're going to apply is that we would like to follow stocks momentum by stocks momentum I would use the RSI as an as the main indicator right the stocks which are around 70 RSI are in an upward momentum and I would like our strategy to invest every month in the stocks that have have the highest upward momentum throughout the previous month for that job I would like to focus on the stocks that are clustered around RSI of 65 to 7075 but using the random initialization of the Clusters would not work and as we can see so what we have to do now is to help a little bit the CIS clustering Alm by supplying the initial centroids but how we can do that let's check out the K's documentation okay so that's the init argument so the default is C++ we used random but if an array is passed it should be of shape so this shape and clusters and features and gives the initial centers of the Cent that's exactly what we want to do and we want the initial centers to be based on our RSI indicator pretty much what we want is that all the stocks which are around 70 RSI to be for each month to be in the same cluster then 55 the same cluster then 45 the same cluster and 30 the same cluster obviously that would be a little bit better than just deciding those thresholds ourselves for example if you want to select all the stocks above 70 every month we'll have a different result than using this clustering algorithm to do the clustering around 70 for US based on all the features we have in the data set now we have to supply to the clustering algorithm this this array in in this shape with the initial centers okay so let's do that so first we would like to have the target RSI values and that would be a list with the first Target RSI value would be 30 45 55 and 70 then our initial centroids would be n Pi zeros and we were we are going to use the target Val so the length to the Target values which is four and then the number of features right four and 18 because we know that we have 18 columns right 18 features and that would be our initial sent rats however let's run that yeah I think I forgot that part yeah so that's our array however up here we have to supply the sent R what we were going to do is the following all rows and this column as we know that the sixth the the seventh column so the sixth index the seventh column is our RSI column in the features data set we're going to just change that to the RS Target RSI values and we'll get the following here we have the initial sent RS and that's our array we're going to use for the clustering job we have to supply it here but before that we would have to first drop the cluster colum and rerun the whole clustering so data is equal to data drop this will now drop the cluster column and rerun the the clustering and assign the centroids as we have supplied this will ensure that throughout each month we'll have the same cluster label assigned to the stocks corresponding to the the same cluster as you can see again in the first month in October we have assigned cluster two to all stocks with RSI around 70 in the next month it's cluster one we want that to be always cluster two that's why we are doing it okay let's run it we have to wait again for the model to fit okay and it's done we have fit now the model with our initial centroids and we can just now rerun the visualization let's do that now okay so now cluster three is assigned for stocks uh with our around 70 and this is consistent for each month it's again cluster three cluster three cluster three cluster three and now we can use that down the road to select every month the stocks which are for example in cluster three to form our portfolio site and we know that cluster 3 is corresponding to stocks that have had a good momentum through the previous month with that we are ready to move to the next step which is at the beginning of each new month we're going to select the stocks we want to invest in for the given for that month okay guys so we are at the most exciting part now we are going to select stocks based on our clustering and then we are going to form a portfolio using an efficient Frontier optimized Max Sharpie weights for those those stocks we want to choose a cluster based on our hypothesis and as we talked earlier my hypothesis here is that stocks which had an RSI around 70 are having a good momentum and my idea is that this momentum should keep outperforming in the next month and for this job I've analyzed all the Clusters throughout the months and I saw that cluster 3 is always stocks around 70 R side and in this case I would like to select those stocks for every month for my portfolio how do I do that we just select stocks corresponding to Cluster I'm sorry to Cluster three for each month okay so here we can call that filtered data frame and assign that okay so now after we have our filter data frame now the first step is to actually get so at the end of October right on 31st of October we had those stocks that's our list of stocks which we would like to use to invest in November my idea here is to create a dictionary with the first day of the next month and all the stocks for the for the next month in a list to do that we have to do the following so filter DF we first reset the index we reset actually only the first level okay all right so we've done that and now now we can do the following we can just use the index and add one date to it so pandas date of set one which would move each index with one day in the future as we know all the indexes are the last day of the month so we'll have the beginning of the next month like this and the next step would be to do the following so we we can reset the index and then we can set the index again to be a multiindex so we have date and thicker let's see all right perfect if that doesn't work to you you can do unstuck and then stuck but it's not necessary at the moment we do this step and then the next step would be to create the dates object so dates would be index. get level values we get all the months and then what we have to do is to create this dictionary which is having a key the date so the beginning of the new month and value would be a list of all the stocks for that month we can call that fixed dates it would be a dictionary and then for D in dates we do the following fix dates do D here we'll fix it to be string have time in our format year month day and this would be equal to filtered data frame pretty much we will select each month right we select each month and then we get the index after we select the month the index will be the tickers and then we call it to list and that's pretty much it this should give us this should create for us this dictionary and voila we have it so now we have the first day of the next month right and the stocks list we the stocks we want to invest for the next month into a list and actually we can now really easily use that to create our portfolios and with that we are ready for this step so the next step is to Define find the portfolio optimization function we're going to use the portfolio opt package and the efficient Frontier to have on a portfolio which maximizes the Sharpie ratio so okay let's move to define the portfolio optimization function we'll use to do that first we will import from pfolio op package a few different classes so from P PF op do effici efficient Frontier we import the efficient Frontier class then from ppf opt we import risk models and again from ppf op we will import the expected returns object okay so now what do we want to do so at the beginning of each month we have the list of stocks we are going to invest and those stocks we have to assign weights to them we have to find those weights we are going to use this package to optimize and find those weights for every month we'll Define this optimize weights function which will take two argument prices actually one argument for now and and uh we'll add another one in the later on so first we'll the the function will take only the the prices of all the stocks we want the weights to be optimized for first we have to calculate the returns of those stocks so we will use the expected returns from expected returns we use a method called mean historical return which take an argument price PR and we will supply the prices as well it takes an argument frequency and that would be 252 days so one year of trading data then we have to calculate the co variance and this time we use the risk models so from risk models sample Co variance again we have to supply the prices and the frequency again one year of data after we have calculated the returns and the co variance we can now initialize our efficient Frontier object efficient Frontier and now we can take a look by portfolio op efficient from here let's read the docs okay but that's the general we want the mean variance optimization okay so the efficient Frontier takes the following argents so expected Returns the covariance Matrix weight bounce and then the so all right the expected returns maybe we can just c those the expected returns would be our just calculated Returns the C coverance Matrix would be C and we will keep this weight bounce for now I will show you why and we use the S CS store and after we have that we can just calculate the weights weights would be from efficient Frontier we will use the max Sharpie method that should give us the weights and then in the end we can get the clean weights which will be round the weights and Clips near zeros so we we want the the clean weights we will just return ef. clean weights and that this our portfolio optimization function so obviously we have to supply the onee prices to this function and it will calculate the returns of those stocks so we have a data frame with one year prices of all the stocks for a given month this function will calculate the returns it will calculate the co variance then it will fit the efficient Frontier optimization and come up with the optimized Max Sharpie ratio weights and return the weights however there is a small specification and that is the weight bounce pretty much this is the bounce for a single stock what can be the constraint for weight of a given stock at the moment it is from zero to one so after the optimization we may have some stocks with zero weight and we may have a single stock with 100% weight obviously we don't want that so we will come back and fix it but yeah that's uh pretty much our function for diversification purposes we would like to actually have a maximum weight of 10% of our portfolio in a single stock so we can already assign that to be 0.1 but for the lower bound we may use a more Dynamic metric so we'll create a argument here lower bound which would be equal to zero but for example for the lower bound when we do the optimization we may use half the weight of an equally weighted portfol for example for a given month we have 20 stocks an equally weighted portfolio would have 5% in each stock so we may assign half of that weight let's say with 20 stocks we may have um the smallest weight to be 2.5% half of equal weight and the maximum to be 10% and with this we will ensure that we have a diversified and well balance portfolio and yeah guys with that we're ready for the next step and in The Next Step we're going to download fresh daily prices of all the stocks that may end up in our portfolios as we know the first portfolio we are going to form at 1st of November 2017 and from our optimized weights function we know that we need at least one year of data prior to the optimization so the starting date of our uh of the download should be 2016 1st of November at least and to download the prices we can just use wi Finance package the first step would be to create the stocks list actually we can go back and use the data object that would be the 150 most liquid stocks because any one of them may end up in our portfolio we want the data for them so data. index. get level values get level values that will be thicker right or thicker values unique and then to list so we have the stocks list and then we can create the new data frame uh we can yeah the new data frame we download from y finance thers will be our stocks and now the start date yeah as we know the first date we want at least one year before that so we can actually to make things Dynamic we can use the data do index get level values but here we we'll get the the the months then we take the first value so the first month that would be October 2017 but we can just say minus Panda's date of set and then months equal to one so minus one month this would return us exactly end of September 2016 actually I went to 2017 okay so we want 12 months we want one whole year I'm sorry my bad guys that's exactly what we want so that would be our start date and our end date would be pretty much the same however that would be the final value from our unique month index of the day date object and that's pretty much it this will download the data we need for our optimization and after this is downloaded we are ready for the next step where the most interesting part is coming we are going to Loop over the fixed dates dictionary and for each new month we're going to get the stocks optimize the weights calculate the daily portfolio return and then uh calculate the whole time period portfolio return by rebalancing and optimizing every new month the download is done and we're ready to move to the next step first we are going to calculate the returns the daily returns for each stock from the fresh new data we just downloaded and then again we're going to Loop over each month select the stocks for the month calculate the weights form the portfolio and calculate the daily return for our portfolio and for our strategy all right so let's start first you want to create the returns data frame would be we calculate log returns so from new data frame we'll select the adjusted Clause column take a log and then the difference that would give us the returns data frame awesome after we have the returns we'll create portfolio DF which would be an empty data frame and then we will Loop over so for start date in fixed dates. keys so we are going to Loop over our dictionaries for start date let's so for each date we are going to first get the end date so the end date would be should be the end of the month right we're investing for one month and then we rebalancing so at the beginning of uh November 2017 we want to select the returns from this return data frame we want to select the returns for the next month right so the end date would be the end of November 2017 that will be P to date time start date Plus pandas date offsets actually I think here we'll use offsets and then month end and we can call the string F time method so we have the right format and we can print that now actually we can print both the start date and we can print the end date for each month we have the start of the month and the end of the month and now the next step is to get the columns for the month to do that we can just say the following so columns would be equal to fixed dates and we are just going to select the star date and those are going to be our columns right so those are going to be the stocks for each month that uh we are going to form portfolio portfolios with after we have the start date and date and the stocks for the given month now we want to calculate the weights but to calculate the weights we need to do the optimization and as we already know to find the optimized weights we need a onee data prior to the start date and it has to be daily so for that data we'll use the new data frame that we've just downloaded which contains the prices and pretty much what do what do we want so to calculate the weights for all stocks at the beginning of November 2017 so for all those stocks we would have to input the prices for one year prior of those stocks into this function which will calculate the weights and then we'll have the weights for uh for the first month so for the November 2017 in this case we would have to create an optimization data frame first we have an optimization optimization start date and it would be pandas to date time start date it would be the given start date 1st of November 2017 minus Panda's date off set and we will supply months minus 12 months exactly so exactly 12 months ago would be the optimization start date and we will create an optimization end date as well so the end date would be actually again start dat minus however one date and now we can print to visualize everything we here so far so we have the start date we have we have the start date then we have the end date then we have the stocks that would be included in a given month then we have the optimization start date and optimization end date date of set my bad so date of set and now for this month so for 2017 from 1st of uh November until the thir 30th of November that would be the stocks in our portfolio and we'll have an optimization so to calculate the weights for those stocks we have an optimization data frame which would be for exactly one year prior to the starting date so from uh 1st of November 2016 up to 31st of October 2017 and now the next step is to calculate the weights for the weights we use we are going to use that was my point one so we're going to use our optimize weight function and and here we have to supply the prices of the optimized weight and the lower bound right the prices would be actually an optimization data frame this optimization data frame we can use the new data frame which contains prices right and we can select the following dates from the optimization start date to optimization and date then we will select the adjusted close and we want only the columns that we are going to use for a portfolio for this month and that would be pretty much our optimization dat frame so in this case yeah when I run it so that's that because in the for Loop the the last values for the start and end date would be the last values based on the last value of the fixed date key so it it's fter out from uh 3rd of October 2022 up to 29th of September 2023 that's around almost one year but what we are going to do for the sake of Simplicity because our first start date is 200 yeah 2017 1st of November we are going to select the following date 2016 1st of November until 2017 30 to October and this is going to be our optimization data frame to calculate the weights we are going to supply that up here as well we have to supply the lower bound that for each stock what would be the lower bound the upper bound we know it's 10% maybe we want to make it this one dynamic as well but for now let's keep it 10% but the lower bound we want to be half of an equally weighted portfolio so a equally weighted portfolio is going to have weights which are pretty much equal to one divided by the number of Assets in the portfolio so the number of columns in this case we can use we can so we would have 10 columns so 10 assets for a given month so the equally weighted portfolio would have 10% for each stock half of that that would be the number of stocks multiplied by two I mean one divided by the number of stocks multiplied by two and that would be 0. uh 0.05 and now we can wrap that around in a round function so we are always sure we have around weight and now we can run it to optimize our weights however I think that up here our columns are actually not the right columns because we would have to select the first optimization date so that would be 11 this is the list of our columns and our optimization data frame will be the following and now we can input it here and check out the weights this now looks quite interesting first let's take a look at the lower bound so the lower bount is 0.01 if we apply it will be 0.012 okay maybe we can use a rounding of three instead of two okay now this looks better those will be the weights of our port portfolio in November 2017 as you can see for stocks they have a weight of 0.01 so 1.2% of our total portfolio and then we have this stock which has 10% of our portfolio this one has seven this one has 6.8% 5.9 3% 4% 4% that's what we are going to do at the beginning of each month we are going to optimize and find the weights for our stocks for for that month okay now we we can Implement that into our optimization so up here we are going to provide the weights and Next Step would be to create the returns date frame but actually before that I think we have we can turn that into a data frame so we can say the following weight equals to pandas data frame frame and then weights and index is uh Panda series and now we have the weights as a data frame and we can now potentially use that with the returns data frame to multiply the way to Fe stock for the given month by the return for that stock for every day and this way we'll be able to calculate the portfolio return for each day okay next step would be to create a temporary datea frame which would be equal to returns data frame and here we'll use the start date up to the end date we are going to filter the beginning of month so that would be 2017 first until 2017 13th that will be our temporary data frame let's see what we have here yeah so we pretty much selected the stocks and the the returns of the stocks for the given month the selected stocks and now what we would like to do is just merge with the weights data frame to get the stock return for each day and the weight for that day we can do the following first stock we are going to stock and then we're going to say to frame to frame return now we have for each day all the stocks and then the return column and now we want to reset the index we are going to reset the date index and next step is to merge with the weights are looking something like that we can say stock to frame weight and we can remove we have now a multi index up here so level zero level one so we want to remove the zero we can say reset index level zero drop equals to true because we want to just drop it directly and we are going to merge those two left by index so left index true right index true as well and we'll get something which looks like that the next step would be to reset the index and set a new multiindex date and I think this one doesn't have a name right now so it would be just called index and now we have almost what we want so we can say unstack and then stock and now we have the data frame with the return for the given date and the weight of the stock for the whole month of our stock and we can call this our temp dat frame the temp dat frame was initially the start date to the end date we can grab that put it up over here and now we can grab this part as well put it over here and let's take a look again now we can just simply use a vectorized operation we can multiply the return column by the weight column and then we can get the waited return for each stock for every day of the given month and let's do that but before maybe we would like to fix the index so the index to be the index names actually so date and thicker to keep things tidy names and now now we would like to calculate the weighted return for each stock which would be just the return return column multiplied by the weight column and now we have the weighted return for each day for each stock and the next step would be just to calculate the daily the sum of the daily weighted return which would be our portfolio return for the given day and we can do that by saying the following Group by level one or date level zero or date whatever you prefer so awaited return do sum that will be the portfolio return for every day and then we can just say to frame and and call it strategy return and that will be our strategy return for the for the selected month obviously we are looping over each month and we are doing the the operations for every month we can assign that to the temp DF and move it over here and the next operation would be just to you know concut this to the portfolio data frame so portfolio data frame will be to pandas concut portfolio data frame with temporary date frame axis equal to zero or the default axis and that should be it guys we have our so we Loop over each month we select the optimization dat frame we are doing the optimization calculating the weights then we are merging the weights with the returns of those stocks for the given month and we're uh calculating the waited return for each day for our portfolio which is our strategy return for every day let's run that and see if it's going to work out okay so we have an error which is return okay so I found how to overcome the error we'll just Implement and try and accept cloud so try and we'll try to run that except exception s e and we'll just print the exception as well I was digging through the code and I figured out guys that we have forgot a really important part of it and up here we don't specify the optimization data frame we would have to add that the optimization data frame was pretty much our new data frame where we select the optimization start date and the optimization end date and then we select the adjusted close and the columns for the given month and with that I think now the code the optimization part should be pretty much fixed and we have something so please check your objective constraint or use different so so status invasible okay yeah so uh this is actually happening from time to time and pretty much the solver is not able to optimize the max Sharpie ratio weights and instead it's failing if we visualize the portfolio returns you can see those periods where there is no data so pretty much the so over has not optimized the portfolio we didn't have any weights and we end up with no investment for those months a work around would be actually to do the following I've really spent some time on that part and unfortunately there is no easy solution for this uh optimization failing for the max sharp ratio the only workaround I figure out is to implement the following whenever the optimization fails and we don't have the max Sharpie ratio weights we will use equal weights for our portfolio as simple as that we'll do the following so we'll try to calculate the weights and then if this doesn't work out we'll do accept and then we'll print that Max sharp optimization failed for start date continuing with equal weights and let's see if this is the issue if we run that now it should print yeah exactly so for those months the max Sharpie ratio optimization failed and we have to continue with equal weights so how do we do that we'll do that the following way we'll create a new variable which is success and it will be equal to false however if the weights have been optimized it will be equal to true all right and then we can just do the following so if success is false our weights would be pretty much we would create a pandas data frame with our weight so pandas data frame but let me take this out of here let's put it down here we'll create a pandas data frame and then we want equal weights so we can just use this part right here we want equal weights so that would be the equal weight right one weight but we want it for for the number of stocks in our portfolio so it will be for I in range L optimization DF do columns yeah that would be our equal weights and then we have to supply the index and the index would be just the columns is a list so columns to list and the columns would be a pandas series zero and this would give us a data frame containing the stocks for the given month with uh their weight assigned as an equal uh equal weight and we can just transpose that and have it like that so this would be our equal weights data frame if our optimization fails and we'll have only equal weights whenever the optimization fails and after implementing that so yeah pretty much you can Implement a print for print statements throughout the code to double check if uh we work correctly but I can assure you that it's working correctly and now if you want to visualize the returns just do a simple plot you see that we have the returns throughout the whole time frame and with that pretty much guys we are ready with our strategy portfolio building we may want to apply to drop duplicates up here if we have any duplicates I think we don't but just in case and yeah with that guys we are ready for the next step and in The Next Step we're going to visualize the returns of our portfolio and compare it to a benchmark which in this case would be the S&P 500 Index itself it will be really interesting to see the results of all that hard work so far so let's move to that part and to compare our portfolio to the S&P 500 first thing would be to just download the returns for S&P 500 so we can say for so spy and we have uh we download the from wi Finance stickers AR tier would be spy the ATF on S&P 500 the start date we can just Supply like 2015 1st of January and the end date we can supply daytime date today and now we would pretty much create a new object let's call it spycor red so spy return and here will just take the log return of spy and just close then difference then drop an name rename and yeah first let's check spirate before okay so this is the log return of spy for each day and now I'll would just like to rename the adjusted close to Benchmark return or spy Buy and Hold would be adjusted close to spy Buy and Hold AIS equ one and then we can use the portfolio data frame to merge the Spy return to the port for dat FR so left index would be true right index would be true by the 4 is an inner joint so we don't have to worry about uh okay so we'll just you know we'll just rerun this sell and then we'll run rerun this one and now we have the strategy return and the Spy Buy and Hold return for the same dates and we can move to the next step where we would like to you know calculate the cumulative return for both the S&P 500 and for our strategy and plot them to compare them in time we'll do the following M lip style. use ggplot I think we've done that previously but just in case and now we want to calculate the portfolio cumulative return cumulative return and that would be from npy expanding window so NY XP log from portfolio data frame and then after that we call the and this should calculate the cumulate is starting from one but we have to say minus one and now we would have the cumulative return for our strategy and the S&P 500 itself now what we may want to do do is just to select for example that we want the portfolio return up to a given point let's say we want it up to 29th of uh September so we just say up to 0.9 20 29 and then we can plot that we can say fix size 16x 6 and and and that's uh pretty much uh that's our comparison guys we can add a title so p. title unsupervised learning trading strategy returns over time as well we may want to PTY label return return and maybe we would like to fix a little bit the axis up here to be in percentages so we can call the following GCA do ya axis set major formatter and here we can supply a formatter like there is a format from MTI but I think we have to first sent format there I think we have to import it up here so we have to import from modb forat B Li thicker same thck uh so import map yeah as MTI and this would do it uh Y axis set major format and that is pretty much our strategy guys up here we have selected the 150 most liquid stocks for each month we have calculated 18 different features on each stock then we used a c's clustering algorithm to assign a cluster for every stock we supplied our custom sent we initialized our sent with a custom array and then for each month we optimized the portfolio to have the maximum Sharpie portfolio weights for every new month and form this strategy using S&P 500 stocks and now we compare to the return of the S&P 500 nevertheless this is not a financial advice you shouldn't use that anyway if you come up here and just change the logic for uh selecting the stocks from third cluster to zero uh to Cluster zero everything will be changed on the spot and you see that the results would change dramatically as well that's just example of how we can Implement machine learning into trading strategies and forming portfolios Etc so that's pretty much it for this strategy and into the next one we're going to form portfolios using Twitter sentiment data I'm looking forward to that and let's move to the second project in the second project we are going to build a Twitter sentiment investing strategy for this project we are going to load a data set which will be available for you to download from the description of the video there is a link where you can download the data set used in this project and in the third project as well the notebook or all three projects would be available on that link too so we would be loading this data set containing Twitter sentiment data on stocks from NASDAQ 100 this strategy would be focused on NASDAQ 100 stocks and we are using Twitter sentiment data the idea here is that I would like to to it will be a smaller project than the first one but I would like to show you how we can create a derivative feature from the Twitter sentiment data to create potential value for our investments first we'll start by importing the packages those are the packages we're going to use panda snai m clip datetime wi finance and the operational system package we import daytime is DT that's the I think we're going to change and again we are going to use ggplot as our mpot lip style first let's define our data folder so this would be the path to the sentiment data file you're going to download so it would be a CSV file this is my folder which contains the data and you would have to assign the path to your folder up here let's load the data so sentiment data frame would be uh pandas read CSV and here we have to supply the puff os. puff. jooin data folder and the file name so the file name is sentiment data.csv and let's run that and see what we have here that is the the sentiment data we have we got the the date the symbol and the corresponding Twitter posts Twitter comments Twitter likes Twitter Impressions and the Twitter sentiment uh the Twitter sentiment is calculated uh by the methodology used by the data provider we just use it as it is here the first thing we want to fix is the date we want the date column to be date time so pandas to date time and next step is to set the indexes again we are going to work with the multiindex in two levels so sentiment dat data frame set index this is going to be the date and the symbol date and symbol so far so good now I would like to calculate a Quant feature out of this data and it would be called actually I'm interested in the engage the Twitter engage each stock here yes not only just the raw sentiment or the RO Impressions likes and comments but I'm interested in the engagement that people have on the posts on Twitter for this company because we know that there are a lot of bots on Twitter and they are messing up the data skewing the data on some post or some symbols and actually I would like to have engagement ratio which may be more informative than having just the live likes or the comments or the sentiment on its own and this engagement ratio will be pretty much the Twitter comments divided by the Twitter likes if a given company has a lot of likes but no comments we know that this is Bots activity but if there are quite some a lot of comments and a lot of likes as well and actually the comments are as much as the likes are even more we know that people are engaged on this stock let's calculate the engagement ratio engagement ratio and this engagement ratio will be pretty much Twitter comments divided by Twitter likes so how much are the comments compared to the likes and that's our engagement ratio for we have this engagement ratio for each stock for every day and now we may just want to filter out the data frame to to contain only stocks that have for example more than u a given amount of likes so let's say we want stocks that have more than 20 likes for a given day and more than 10 comments on a given day because otherwise it's just random noise so Twitter likes we would like to have more than 20 likes and You' like stock to have more than 10 columns to be kept into our investment universe and with that we are pretty much ready with the first step we are following pretty much the similar logic to the previous project however here we're going to create an equally weighted portfolio we are not going to implement the efficient Frontier optimization ation every month we would decide an investment Universe of top five stocks and just invest into them and now after we have uh calculated the engagement ratio now we want to pretty much calculate the average engagement ratio for each stock for every month and we can do that the following way so we the reset the symbol index the symbol part of the index we reset symbol and then we'll Group by we'll group byy for every month and every stock we'll Group by on the monthly level and the symbol level and then we are going to maybe we can put that into brackets then we are going to select the engagement ratio column and calculate that the average engagement ratio for the whole month pretty much and now we can assign that to uh new variable new data frame called aggregated DF and now we can calculate the crosssectionally the rank for each month of all the stocks based on this engagement ratio how do we do that we say a new column rank we are going to group by level zero or date so we are grouping by each month because already we have uh aggregated to the monthly level right so we group up group on each month and then we select the engagement ratio and then we are doing the following so on the next row we're going to say transform Lambda but maybe we can put that into brackets and move that on the next row so Lambda X and here we'll just say x. rank ascending equals to false and this would create a new column called rank crosssectionally for each month based on the engagement ratio and the stocks with the highest engagement ratio would have the highest rank and we can obviously see that already so now we want to pretty much select for each month the top five stocks for each month based on this Rank and grab them to use similarly to what we did in the previous project grab the start date and the stocks that we want to include in the next month and then calculate the returns but this time in an equally weighted fashion so let's do that I'll cut those celles and yeah now we'll just select the top five stocks for each month so we'll do the following aggregated data frame rank where is below six and now we have the stocks with the highest engagement ratio for each month we can assign that to filtered DF data frame and after we have the filter DF data frame we can just reset the symbols part of the index so reset index level equals to one and then we just fix the index again the same way we did last time so we'll just add one day to your index so plus day time uh Panda's date of set one day let's see what this would give us now we have the beginning of the next month and the stocks we want to invest in for the next month so those five stocks for each month and yeah pretty much next step is to uh reset the index and set again the multi index we'll set the multi index again date symbol and visualize top 20 rows so far so good now for for the beginning of each new month we have the top five stocks by engagement ratio and now we would like to create the fixed dates object like we did in the first project from dates would be good to filter data frame index get level values date and here yeah unique and to list so we make a list and fixed dates would be again a dictionary empty dictionary for D dates pretty much we are following the exact same logic we did in the previous projects so it's quite straightforward I hope this stuff will guys however uh yeah in this project we are not doing the efficient front here optimization just rather use a weekly weighted portfolio the idea here is to show you how we can extract value from different uh data sets and yeah as you can see now for each new month we have the list of five stocks we are going to invest in and the next step would be to download the fresh prices to do that we can first create our stocks list it would be equal to sentiment data frame do index get level values symbol and again unique to list this is our stock list and we can create a new data frame pric DF where we are going to download again from yaho final our start date we can use as you you can see the we can use just the 1st of January 2021 to be sure we have enough prices and the end date would be 2023 1st of March so 2023 1 of March awesome after we have our our list of stocks for each month and the prices we can move to the next step where we are going to calculate the portfolio returns following similar logic to what we did in the last section so from prices DF we can select the adjusted close column and calculate the log returns for each stock and yeah pretty much call that returns date frame so after and we can drop the na values in the end after we have the returns we can now create portfolio data frame which is just an empty data frame and for start date in fixed dates. Keys print start date just to visualize what we have so far so for each for each month we have the start date and now we want the end date for our portfolio so end date would be pandas to datetime start date plus pandas offsets month end which will give us the which should give us which should give us the end date of the same month uh okay pretty much we have that however we would have to add the string F time method in the end like that that's pretty awesome so what was the next step The Next Step was to select the columns or for the given month that will be the stocks we are going to invest for that month and these will be fixed dates start date so for each month the stocks we have to invest for that month and now we can create our temp data frame where we would be calculating the equally weighted return for our portfolio for every day this would be returns date frame here we'll save from start date to the end date and then for the selected columns we're going to calculate the mean the average by columns so the average daily return and we call that in the end we say to frame and then portfolio return portfolio return next step would be just for each month to concut this portfolio return and with that we have calculated the equally weighted portfolio return for every month for those stocks we have selected using our methodology about aggregated uh mon monthly average engagement ratio for every stock and then ranked crosssectionally and now we are ready to pretty much visualize the return the cumulative return of our portfolio and compare it for example to a benchmark like NASDAQ right those stocks are constituents of NASDAQ so we may want to compare it to the NASDAQ indexes a benchmark again guys I've said it in the beginning of the tutorial and the course we're using stock lists which are not survivorship bias free and that is really important and most probably skewing the results upwards so you should never use you should always use survival ship bias free stock list when you're researching strategies so now what do we want we want to just download the prices for the NASDAQ or the NASDAQ ETF which is QQQ so we would use start date to be 2021 1st of January and the end date would be pretty much the same as for our portfolio so first of March and now we can calculate the NASDAQ return so QQQ return that would be log from the adjusted close differen and we may want to rename it to Mazda return and with that we're ready to merge together the can return to our portfolio date frame so we merge by index by theault the merge is always using uh in our joint so we are not obliged to specify that or maybe we are no we are not and now we have the portfolio return for every day and the NASDAQ return for every day and the next step would be to just visualize so we can say the following portfolios cumulative return will be equal to uh we'll calculate that in a n expanding window where we take uh log from the return of both portfolios and then we apply the cumulative some over the whole expanding window so for for each new uh each new addition We Run The accumulator some in the end we have to substract one and that's pretty much ready for plotting so we create a plot with a fix size equal to 16 and six and we can set a title like Twitter engagement ratio Str return over time as well P GCA Y axis set major format that would set the the y axis to be a percentage so we'll use the percent format as well we give a name to the Y uh label and we just show and that's pretty much our second project guys about the Twitter sentiment investing strategy with monthly rebalancing of an equally weighted portfolio our uh stock selection criteria is based on the engagement ratio which we calculators the Twitter comments divided by the Twitter likes for each stock and we select the top stocks which had the highest average engagement ratio for every month if you want to change the selection criteria for example if you want to select the stocks which have on average the most likes for every month you just we can just simply do that so cop this com put it up here then put it up here and now we have the new we have pretty much the ranking done by average T likes through throughout the month and if we run the symbols down the sales down below it will be redone using the likes and yeah as you can see when we use the likes our strategy underperforms however if you use the comments maybe we can try with the comments to see what what would be the results with the coms similar results similar results however a little bit um higher performance so the idea here is to show you that if we have alternative data we may need to implement some calculations to get a derivative features which then may be valuable for our research and for our strategies that is the main idea here and if we return the engagement ratio let run the whole thing we are able to see the difference in the results obviously the engagement ratio by using the engagement ratio we have some we've created a little bit of value above the NASDAQ itself and that is pretty much it about the second project and we are able to now move to the third project where we are going to create intraday trading strategy on a simulated one asset data using a gar model so a model to predict the one day ahead volatility and it will be a really interesting one so I'm really excited to move to the third project and let's get into it in the third project we are creating an intraday strategy using gar model we're going to use simulated daily data and simulated intraday 5 minute data on a single asset and we are going to create two signals on two different time frames on the daily level and on the intraday level so on The Daily level our signal would be driven by the gar model volatility prediction we are going to fit in a rolling window for every day a gar model and predict the volatility for the next day and from that we're going to calculate the prediction premum premium and derive our daily signal out of that on the intraday level we are going to calculate technical indicators and create a signal following a intraday price action pattern and merge with the daily signal and calculate our final signal intraday only for those days where simultaneously we have the intraday and the daily signal and whenever we have the the intraday signal we are going either into a long or into a short position and we're holding until the end of the day and that's pretty much the strategy guys let's start by first loading the data we are going to use so the data again it will be available for you to download from the link down in the video description first we start by importing all the packages we'll be using for this project they are MP lip Arc we'll use the arc project for the gar model then uh Panda snai and the operation system package again our uh data folder you have to provide the path to your data folder and we can start by loading the data so daily uh data frame would be pandas read CSV all spot join and then the daily date frame which is simulated daily data. CSV and this should be our daily data here we would have to fix the date we use pandas to daytime and we Supply the date as well then we are going to set the index for to the Daily date frame to be the date pretty much standard stuff and our I have already calculated the log return but to repeat that I'll just calculate the log return which would be n log from the adjusted close and then we just do the dot difference method and so far so good that's our daily data and our intraday data would be intraday 5 minutes DF again we are going to load it using pandas stre CSV so p. join and the name of the file is simulated 5 minutes data. CS we can commment out this code this one as well and here first first step we need to fix the daytime column to be date time so pandas to date time that's the first step then we want to assign it as an index and the next step would be to calculate this date column so we can just redo that real quick or we just cast it to data we already have it so we'll cast it to daytime but if you're wondering what it is it's just selecting the index and then date which which would give us the date column and with that we are pretty much ready with loading our daily and intraday 5 minute data and we're ready to move to the second step so in this step we are going to define a function which would fit a gar model and predict the volatility for one day ahead in a rolling window we'll use a six months rolling window for that but the specification here is that whenever you fit a gar model you need to supply the auto regressive and moving moving average orders you need to actually find those orders and to do that I've used a Brute Force approach where I fit 16 models with a combination for the outo aggressive and moving a average orders from one to four and combinations between them so I fit 16 models then I've extracted the mean square error and the beian information Criterion and pretty much I chose the best model to be the one who is minimizing the beian information Criterion so I found that the best G model on this data is having Auto regressive order of one and moving average order of three so P = to 1 and Q = to 3 so that's a really important um clarification first you start by calculating on The Daily data frame the variance or actually the rolling six months variance of the log return that would be the variance and we may want to visualize that this is the variance of our data the variance of the log return of our data through time and now we would actually want to for the sake of Simplicity I'll just filter out the date to be be from 2020 onwards and now I'm going to create a predict volatility function so we are going to predict the volatility for every day so so far we have the Vance pretty much the volatility is predicting the Vance with a G model when you fit a g model You can predict the variance or the uh the mean of the series we're going to predict the variance of our asset for every day and we would have to do that in a rolling window so first we create the best model which would be where you going to use the arc model and here we have to supply Y which will be our X or the series then p is the order the regressive order the moving average order Q so those values we have to find with the Brute Force approach as I did I mean fitting 16 or 25 models collecting the metric the mean squar error the be information Criterion after you have run the model and choosing the one which is minimizing the for example the bean information criteria that's the best model so after we have supplied arguments to the arc model next we are going to fit the model here we would say the update frequency to be every five steps and this position to be off and this is our best model and now we can forecast right VAR forecast so from the best model we are going to forecast Horizon equals to 1 so for one day ahead our data is daily so for one day ahead and then after the forecast is done for next day we have two columns or two vales the mean or the Vance so we choose the Vance and and we're going to choose the last value so last row of the First Column we may want to print the date so x. index last date the one we are going to forecast for then we'll just return the VAR forecast and this is our prediction function guys now we'll just say the following predictions and our predictions are going to be based on the log return we're going to apply a function in a rolling window of 180 days the same way as we did for the Varan 180 days and here we say Lambda x equal to predict volatility X and we can now pretty much run that okay yeah my bad there is no do up here what is the problem okay I think yeah it's iock here and this will now just run in a rolling window fit the best G model we know that the best G model for our data is auto regressive order of one moving average order of three and yeah we just have to wait for this thing to run and predict the volatility for the whole data set after we are done with that we will move to the next uh step where we are going to calculate the prediction premium and form a daily signal out of it okay so the model is done fitting and predicting the Vance and we can now take a look we have the V and the predictions actually if you look closer then the Val are pretty close and similar actually we may want to visualize them if you're interested guys let's see what we have here so we have the Vance and the predictions and yeah maybe we can just plot them so the Vance is more stable than the predictions and as you can see the predictions are exploding here or there but we may deal with that if we apply a rolling average or a CA filter model on the predictions they will smooth them out considerably however for the sake of this strategy and this course I'm not going to implement that but usually you may think about it you know like signal reduction techniques okay so the next step we are going to calculate feature called prediction premium and after we have the prediction premium we'll calculate the six months rolling standard deviation of that uh prediction premium and from there we'll be able to calculate and create our daily signal the prediction premium would be a very simple formula it would be equal to predictions minus so from the predictions we are going to substract the Varan and then we're going to scale the whole thing by the varas itself it's a simple formula then we're going to calculate the premium standard deviation premium standard deviation which would be just a rolling 6 months standard deviation and let's run that and see what we have now we have the prediction premium let's visualize it real quick so this is the prediction premium and we have the standard deviation of this prediction premium awesome from now on we can create our daily signal right so our signal daily will be the following our daily signal will calculate by doing apply Lambda over each roll we have one if prediction premium is higher than the premium standard deviation multiplied by 1.5 so 1.5 if the prediction premium is higher than 1.5 standard deviations then we have long signal or one else we would have minus one if the prediction premium is smaller than minus 1.5 standard deviations and here we have to say the following else MP n and then we'll do that for axis equal to one we're doing that for each row right let's see what this would give us this would create this signal daily column so whenever you know the prediction premium is higher than 1.5 standard deviations or lower than 1.5 standard deviations we get the signal we may want to explore that a little bit equals to one we have quite some signals for long but actually you know what we may do the following we may just plot a histogram instead of uh you know obing the we want to understand how much short and how much long signals we have so kind histogram and then p. show this would create histogram and we'll be able to understand how many okay so what is the problem funny okay so now we can see that we have around 50 long signals and then around 35 short signals and yeah with that we are ready with our daily signal guys and we can move to the intraday part of the strategy and here we are going to merge our daily data with the intraday data based on the based on the column on the date column have created actually to do that we need to shift the daily data one day ahead right because at the end of the 29th for example we have the daily data but we'll be using it on the 30th so we need to do the following up here we need to say signal daily will be just shifted one day forward that's pretty much what we have to to say and we can move to forming the final data frame here we can use our intraday data frame we're going to reset the index and then we're going to merge so do merge actually we may move that on the next row so we are going to merge with the daily date frame and actually only with signal column because we are not interested in the other daily columns we interested only in the signal column we're going to reset the index on The Daily data frame as well and here we'll be using the left on so left on date left on this this column and right on date as well or on the index column pretty much this one after the reset it will be available as a column and that's pretty much our join now we have the five uh five minute intraday data right and we have the daily signal derived by the gar volatility prediction now we may just want to set the index to be the DAT time again and that's pretty much our final data frame we can call it final data frame comment that out okay so now we want to drop those two columns we don't need the those two date columns so we'll say the following final DF do drop and yeah we'll drop date and date and we are ready now to calculate all the technical indicators potentially we can use for this strategy to do that we we will need the pandas ta package which I forgot to import so we are going to import the pandas ta package and the first indicator I would like to calculate is the RSI indicator from Pand ta we are going to use the RSI my to and we'll just apply the close price and the length would be 20 next one is uh the Ballinger bands I would like to calculate so b or lower band first here it's B bands again it takes the close price as an argument however the B bands are returning five columns instead of one we have to specify that we want the First Column to be returned so iock all the rows and the First Column that would be our lower band and for our upper band it would be the third column lower middle upper those are going to be Ballinger band lower upper band and now I would like to have potentially to use the ls of the lower and over band but let's continue with d and for after we have calculated the indicators we can just run that part check out the indicators and now we can move on to calculate the intraday signal so signal intraday would be the followings so we have a intraday signal whenever the RSI is above 70 and the close price closes above the upper Ballinger band so whenever the RSI is above 70 and and the price closes above the upper boringer band we have a burst of momentum upwards or downwards that's my strategy idea and this would be our Buy Signal and the sell would be when at RS SI is below 30 and simultaneously we have a close below the lower bar this price action pattern is a pure momentum pattern and we can again uh calculate using the applied Lambda function so one if RSI is above 70 and if the close price is above the upper band else we would like minus one if the opposite pretty much right R side below 30 and close below the lower band else we would like to have MP n if those conditions are not met and the axis will be equal to one we can run that and see what we call we have a small Arrow up here why okay so there's something wrong with our data we can call the info and everything is float what is the problem here guys I found the error guys I have forgot to add the X up here for both the upper and the lower bound and that was doing the error it's going to calculate the intraday strategy signal as well now and with that we are actually pretty much ready with our intraday signal and our daily signal and we can move on now to generating our entry points and our final signal the final signal that we are going to use to calculate our daily returns from to do that we are going to do the following so final DF return signal or return sign that will be the of our return and actually the direction of the position it would be the following we would have a short position whenever the daily signal is one so the prediction premium is above one standard 1.5 standard deviations High prediction premium let me grab that signal daily so whenever the signal daily is equal to one and the signal intraday is equal to one as well the intraday situation is that the price is super overextended our RSI above 70 and simultaneously the close is above the upper Binger band so I assume a mean reversion and on the day we know that the prediction premium the volatility is above the rolling standard deviation or 1.5 1.5 rolling standard deviations whenever we have those conditions met we are going into a short position else we're going into a long position whenever we have the opposite so both the daily signal and the intraday signal are minus one and else we have MP none and the axis would be equal to one as well and this would be our intraday signal to get into a long or a short position invalid syntax okay I forgot the com up here that's enough okay I'm not sure guys what's the problem here but all right I think I had forgotten this something you know okay now we have the intraday signal for each position my idea here is the following so whenever intraday whenever in the in a given day we have this signal the first trigger of that position we are going into a position and we'll be holding for the whole day so the first time it can be in 1000 but we can have another signal in the same day uh at 500 we don't care about the second one we are going into full position position on the first one and we are going to hold for the whole day in this return sign column whenever we have the first occurrence of the signal what do we want to do we want to forward feel until the end of the day the same position sign to do that we can use the following trick return sign would be first let's explore what we have I'm sorry let's explore what we have so far so the return sign is our intraday final signal uh wow okay so this one should be one all right so whenever we have to go into a long position we have long signal right here the return sign we can observe that this happened at uh in 2021 18th of October at 605 then we had another signal on the 23rd of October then on the 28th on the 29th etc etc so in some days we may have only one signal but in some days we may have two or more we don't care about the second or the third signal we care only about the first one so at this point in time we had the signal and on the next bar so on the next 5 minutes on the open price we are going to go into a position and we want to hold it until the end of the day at the end of the day we close the position an easy way to calculate the strategy return would be to use just the vectorizer return right so we are going to forward f for every 5 minutes after the entry time we're going to forward f with the same sign the return sign column how do we do that final DF do group by pandas grouper we are going to group for every day we are going to group on a daily basis we are going to select the return sign and now we'll use the transform method and because we have supplied one or MP9 we are able to use a transform method and do Lambda X forward F and this would now forward F this value until the end of the day for every day where we have a Val it will be done by the first occurrence of the Val after we run that we can now select where the return is equal to one where we have the entry signal and as you can see now we have the first signal at 605 6010 615 Etc ET until the end of the day we can now calculate the 5 minutes returns shift them backwards with one row for every day and multiply the return sign column by the signal intraday let's do that right now that would be The Following return percent change and we would like to get the forward return right so forward return and this would be equal to our return column shifted we don't need to apply Group by daily here because our data is continuous it's 5 minutes data but it's uh from the beginning of the day until the end of the day it is continuous for every day including the weekends and that's why we we can just calculate the 5 minute return and then shift it backwards with one row and our final strategy return will be the following so strategy return would be equal to final data frame forward return multiplied by the return sign that would be our strategy return but it will be on the 5 minute level so we want to get the daily strategy return so we'll calculate daily return data frame and it would be the final data frame do groupby we're grouping on the daily basis so for every day we're going to group by select the strategy return column and just Summit after we have the daily return data frame we can move on to the final step and we'll just calculate the strategy cumulative return that would be equal to expanding window so numi expand log and then cumulate sum and we just substract one in the end and we can plot the strategy return we set title to be intraday strategy returns we fix the yaxis uh values to be percentages again so set major format to be from anti percent format one the Y label we set it to return and we're pretty much ready with our graph and this is our strategy G as you can see we have an inaday uh strategy we may not trade for quite some time up here we haven't trade for almost six months but then we had some trades and yeah whenever we have uh the match of the daily signal and the intraday signal that's when we are going into trades and we have the returns that's a strategy using the machine learning model the gar model to predict one day ahead volatility in combination with simulated intraday data and with that we're finishing the third project and I really really hope you've enjoyed it guys you can find the data and the notebook Down Below in the link if you're interested to learn more how to back test Quant strategies how to do algorithmic trading there will be a link in the description ion to my website where you can learn more about algorithmic trading and quad trading with python
