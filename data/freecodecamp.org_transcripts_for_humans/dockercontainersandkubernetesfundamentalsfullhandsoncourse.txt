With timestamps:

00:00 - gee Barrette teaches us Docker
00:01 - containers and kubernetes fundamentals
00:03 - course for beginners ghee is a developer
00:06 - and trainer with more than 25 years of
00:08 - experience he is a Microsoft MVP
00:11 - frequent conference speaker and was the
00:14 - leader of the montreal.net user group
00:16 - for more than 23 years all this is to
00:19 - say he is the perfect person to teach
00:21 - you about Docker and kubernetes
00:25 - [Music]
00:33 - welcome to this Docker containers and
00:35 - kubernetes fundamentals training course
00:39 - my name is gibaret and I'll be your host
00:42 - in your Learning Journey Into The
00:44 - Amazing World of containers
00:47 - I'm a full-time trainer with a developer
00:49 - background and I'm based in Montreal
00:51 - Canada and that's where my strange
00:54 - accent comes from
00:56 - I'm certified on kubernetes and also on
00:59 - terraform Azure AWS and Google Cloud
01:04 - I'm very honored to be a Microsoft MVP
01:07 - in the Azure expertise and a
01:10 - digitalocean navigator
01:13 - you can reach me via the contact page
01:15 - and on Twitter
01:19 - what should you expect from this course
01:22 - you will not become an expert just by
01:24 - taking this course this is an
01:26 - entry-level course that will provide you
01:28 - with a strong containers and kubernetes
01:31 - Foundation
01:32 - and you'll gain enough knowledge to make
01:34 - sound decision at work or in your
01:37 - projects
01:39 - throughout this course you will find
01:41 - lots of Amazon activities to practice
01:44 - what you've learned
01:46 - you will use Docker and kubernetes
01:49 - locally on your PC or Mac so there's no
01:53 - requirement to have an account with a
01:55 - cloud provider
01:58 - are there any prerequisites for this
02:01 - course not really if you're a developer
02:04 - a devops specialist an ID Pro or even a
02:08 - technical manager that's totally fine no
02:12 - previous Docker or kubernetes knowledge
02:14 - is required
02:18 - we will cover a lot of ground
02:21 - you will learn about containers Docker
02:24 - and D Docker registry
02:27 - you learn about the kubernetes objects
02:30 - like pods workloads and services
02:33 - that's a lot of material and the goal
02:36 - here is to get you from zero knowledge
02:38 - to a kubernetes ninja
02:41 - well at least provide you with enough
02:43 - knowledge to Aspire being a kubernetes
02:46 - ninja
02:48 - I want to say a big thank you for
02:51 - learning Docker and kubernetes using
02:53 - this course
02:56 - if you like the course you can help me
02:58 - by making a small donation this is the
03:01 - link to my buy me a coffee page
03:06 - you can of course buy one of my other
03:08 - courses where you'll learn to run
03:11 - containers on different Cloud providers
03:13 - services and use a managed kubernetes
03:16 - cluster in the cloud
03:19 - and finally I wish you all the best in
03:22 - your Learning Journey
03:27 - [Music]
03:33 - let's see how to set up your laptop or
03:35 - PC for this course
03:39 - you need a laptop PC or Mac with either
03:42 - Windows 10 Mac OS or Linux
03:45 - if you have a Mac with an apple silicon
03:48 - most tools should run perfectly
03:51 - I will use Visual Studio code with the
03:54 - docker extension to help build create
03:57 - and run containers
03:59 - vs code is a free IDE that runs on
04:02 - Windows Mac and Linux
04:05 - on Windows and Mac you'll need Docker
04:08 - desktop with kubernetes enabled
04:11 - on Linux refer to the documentation on
04:14 - how to install Docker and kubernetes on
04:17 - your distro
04:19 - you'll need a Docker of account
04:22 - and a few easy to install tools
04:25 - refer to the setup instructions located
04:28 - below this video
04:32 - the lab files are located in a git repo
04:35 - on GitHub
04:37 - simply open this URL in a browser
04:43 - click on the code button
04:46 - if you have git installed on your
04:48 - machine you can type git clone with the
04:52 - link displayed here
04:54 - and if you don't have git simply click
04:57 - on the download zip button to download
05:00 - the code as a zip file
05:03 - [Music]
05:08 - let's talk about the microservices
05:10 - concepts
05:12 - if we head to Wikipedia and take a look
05:15 - at the definition that we find over
05:17 - there it says that it's a variant of the
05:20 - service oriented architecture or SOA a
05:23 - structural style slash architecture
05:26 - and that it arranges in application as a
05:29 - collection of loosely coupled services
05:32 - so instead of a large monolithic system
05:35 - we have multiple smaller pieces
05:39 - in a microservices architecture services
05:41 - are fine-grained meaning that each of
05:44 - them have their own responsibilities
05:47 - and the protocol use are lightweight
05:50 - like an API exposed over HTTP or grpc
05:54 - for example
05:57 - if we look at the monolithic
05:59 - architecture
06:01 - these systems were usually built as one
06:04 - single unit
06:05 - an IDE would group multiple projects and
06:08 - we would compile the whole thing as one
06:10 - single unit
06:13 - they were also deployed as a single unit
06:15 - so we would need to copy everything all
06:19 - the files on on a server
06:23 - and if we had to scale the system we had
06:26 - to spin a new VM and copy deploy the
06:30 - whole system on that VM and same for a
06:33 - third and a Ford server
06:36 - an example of such a monolithic
06:39 - architecture is a treaty application
06:42 - even though the system was clearly
06:44 - separated into layers
06:46 - it was all tightly coupled from the web
06:49 - project we had to make a reference to
06:51 - the business layer project and the whole
06:54 - system would run in the same address
06:56 - space
07:00 - with macro Services we break our big
07:04 - system into smaller parts each with its
07:07 - own responsibility
07:10 - so let's say we have a class that deals
07:13 - with identity in our business layer we
07:16 - can extract that code and place it in
07:18 - its own microservice
07:21 - we can then scale each of these smaller
07:24 - pieces independently from each other
07:28 - there's no strong bound since we expose
07:30 - functionality through an API
07:34 - they can be written by smaller teams and
07:37 - each can use their own programming
07:40 - languages like go PHP C sharp
07:45 - and domain-specific data can be stored
07:48 - in separate databases
07:52 - so the way we would deploy a monolithic
07:55 - system is by deploying everything on a
07:58 - server all the dlls files needed to run
08:01 - the system
08:03 - we had to scale the deploy everything on
08:07 - more servers
08:11 - now let's compare that to microservices
08:13 - well
08:15 - microservices are deployed independently
08:19 - each can scale independently also
08:28 - need to scale back one service
08:30 - no problem
08:35 - so if you have an existing monolithic
08:37 - system how can you transform it into a
08:40 - microservices architecture
08:42 - well you need to break it into small
08:46 - units like the code that dealt with the
08:48 - identity in the in our business layer
08:53 - Martin Fowler author of the patterns of
08:56 - Enterprise application architecture book
08:59 - documented the way to achieve such a
09:02 - transformation using the Strangler
09:04 - pattern
09:05 - let's say our identity code is here in
09:08 - the Legacy system
09:10 - we can place a facade to Route the calls
09:13 - to it migrate the code
09:15 - and have the facade route the calls to
09:18 - the new macro service at some point
09:21 - as we go we will end up with less code
09:24 - in our Legacy system
09:26 - and when democracy and it's done we can
09:29 - get rid of the facade
09:31 - this pattern is very useful and you can
09:34 - learn more about it using the link in
09:37 - this slide
09:40 - and this concludes this lecture on the
09:42 - Marco Services Concepts
09:46 - [Music]
09:52 - let's talk about Marco Services
09:54 - anti-pattern because it's a thought
09:56 - Rosie I know it's kind of strange to
09:59 - talk about what can go wrong right away
10:01 - but I think it's very important
10:06 - first of all
10:08 - it's not some kind of magic pixie dust
10:10 - that you can sprinkle on top of a
10:12 - zigzing system and boom you get a
10:15 - beautiful Marco Services architect
10:17 - system
10:19 - it takes efforts and maturity to achieve
10:22 - this
10:24 - from one monolithic system you'll end up
10:28 - with a bunch of smaller pieces and that
10:30 - can add extra complexity
10:33 - a change to a microservice can have a
10:35 - domino effect and take your system down
10:39 - and what about securing all of these
10:41 - microservices
10:44 - it's also essential to use or introduce
10:47 - new processes in the organization
10:51 - like devops Ci CD and testing
10:55 - but be careful and don't try to
10:57 - implement everything at the same time
10:59 - it's a recipe for disaster
11:02 - take it step by step and make sure you
11:05 - have metrics in place to validate each
11:08 - of these steps
11:10 - and this concludes this lecture on
11:12 - microservices antibatterns
11:15 - [Music]
11:21 - let's talk about the microservices
11:22 - benefits and drawbacks
11:26 - since each microservice runs in its own
11:29 - address space
11:31 - there are less chances that if one of
11:33 - them goes down it takes the whole system
11:35 - down with it
11:38 - a microservice runs on open source
11:41 - Technologies so there's less Fender
11:43 - lock-in
11:45 - since they are smaller in most case they
11:49 - are easier to understand
11:52 - and that makes them faster to deploy
11:56 - and also easier to scale
12:02 - like we saw in the anti-pattern section
12:04 - there are some drawbacks
12:08 - complexity is added over mother's
12:11 - existence to resolve to her complexity
12:13 - issues
12:15 - so make sure your team is well trained
12:17 - and has made some proof of concept and
12:21 - make sure to start small adding one
12:25 - piece at a time
12:28 - testing might appear simpler since there
12:31 - are less functionality in a microservice
12:33 - to test
12:34 - but make sure to test the whole system
12:40 - deployment may appear simpler but one
12:43 - update can impact many Marco services
12:45 - and have a terrible domino effect
12:49 - ready to manage multiple databases
12:54 - calls between macro services will go
12:56 - through apis and this will add a bit of
12:59 - latency to all calls so make sure you
13:02 - test for that
13:05 - this key transition server will appear
13:08 - you'll make a call and it will fail or
13:11 - but try again 50 milliseconds later and
13:15 - it will work
13:16 - so make sure to implement some retry
13:18 - strategies in your code or by using a
13:21 - service mesh
13:24 - instead of one big point of failure
13:27 - you'll end up with multiple ones can
13:30 - your system survive if one microservice
13:32 - goes down
13:36 - and what about security
13:37 - are you okay for all these microservices
13:40 - can see and talk to each other
13:43 - so yes complexity is introduced for
13:47 - solving complexity issues
13:51 - and this concludes this lecture on the
13:53 - macro Services benefits and drawbacks
13:56 - thank you
13:58 - [Music]
14:04 - let's Now understand what is cloud
14:06 - native
14:08 - you may have heard the term Cloud native
14:10 - before
14:12 - but what it is exactly
14:15 - it's a way to architect and build
14:17 - complex systems taking advantage of
14:19 - modern development practices and the use
14:22 - of cloud infrastructure
14:25 - if we head to the cloud native
14:27 - Foundation website and look at the
14:29 - definition
14:31 - we see that it's quite a long one
14:35 - so let's break it into smaller parts
14:39 - Cloud native uses containers service
14:42 - meshes microservices immutable
14:44 - infrastructure and declarative apis
14:48 - we'll cover containers service meshes
14:50 - and macro services and the concept of
14:52 - immutability in this course
14:54 - but not how to build apis
14:58 - immutable infrastructure means that we
15:01 - usually never update something but we
15:03 - replace it with a newer version
15:08 - Loosely coupled systems mean that the
15:11 - functionalities are exposed through apis
15:15 - observable with the use of metrics
15:19 - creation and updates are automated
15:22 - and instead of making changes once every
15:25 - six months we deploy eye impact changes
15:28 - on a freaking basis
15:31 - and finally we use a series of Open
15:34 - Source projects to run our system
15:40 - when the cncf says to use open source
15:43 - projects they are not kidding
15:46 - this cncf landscape graph shows a ton of
15:49 - Open Source projects that you can use
15:52 - but don't worry you don't have to use
15:54 - them all
15:56 - the challenge really is to identify
15:58 - which one to use in the context of what
16:01 - you want and try to achieve
16:05 - and this concludes this lecture on cloud
16:07 - native
16:08 - head to the cncf website for more info
16:14 - [Music]
16:20 - let's go deeper in the cloud native
16:22 - Concepts
16:24 - Cloud native is about Speed and Agility
16:29 - the user wants new features right away
16:31 - without any downtime
16:34 - and the business wants faster release of
16:37 - feature to stay competitive
16:42 - a cloud native application architecture
16:44 - starts with clean code
16:48 - using domain-driven design techniques
16:52 - Markle services
16:56 - and kubernetes
16:58 - this course is all about microservices
17:00 - and kubernetes
17:02 - feel free to explore the concepts of
17:04 - clean code and DDD on your own
17:08 - with Cloud native we need to change
17:11 - mentalities
17:14 - infrastructure becomes immutable and
17:16 - disposable
17:18 - it is provision in minutes and Destroy
17:21 - as fast
17:23 - it is never updated but it's replaced
17:26 - with newer versions
17:28 - traditionally we would care about our
17:30 - virtual machines
17:32 - we would patch the OS update the apps
17:36 - with containers we create newer version
17:39 - with the software updates
17:42 - destroyed the previous running ones
17:44 - and replace them with the newer ones
17:48 - so the containers that you'll run will
17:50 - be more like cattle than pet
17:55 - of course this Cloud native thing is a
17:58 - lot easier when starting a new project a
18:02 - blank page or a green field
18:06 - however it's still possible with Legacy
18:09 - projects
18:13 - I really like the cloud native
18:15 - Foundation trail map because it breaks
18:19 - the journey to Cloud native into smaller
18:21 - measurable objectives
18:25 - you can set your own performance
18:27 - indicator to measure each steps to
18:29 - ensure a smooth Journey
18:33 - so let's take a look at the first steps
18:37 - your team must first learn how to
18:40 - cantonerize your application
18:43 - the developers and the IT Pros must know
18:46 - how to deploy and monitor containers
18:51 - you need to automate deployment through
18:53 - the use of continuous integration and
18:56 - continuous delivery techniques and tools
19:01 - you need to use an orchestrator like
19:03 - kubernetes
19:04 - and maybe deploy your application using
19:07 - L charts
19:11 - then you need to add observability so
19:13 - you can understand what's happening in
19:15 - your kubernetes clusters and be reactive
19:21 - use tools like service meshes to provide
19:24 - more functionalities inside your cluster
19:29 - Implement security through policies
19:34 - and wow these were just the first six
19:37 - steps Now understand that you don't have
19:41 - to implement all of this and especially
19:43 - not at the same time
19:45 - I really like this trail map because it
19:47 - breaks the journey into smaller steps
19:49 - that the management can understand and
19:51 - measure
19:55 - and this concludes this lecture on the
19:57 - cloud native Concepts
20:00 - [Music]
20:05 - let's take a look at the cncf website
20:08 - the website is located at cncf.io
20:13 - and from there you can take a look at
20:15 - the various projects maintained by the
20:17 - cncf
20:19 - information about on how to get
20:21 - certified
20:23 - Community Information like the
20:25 - conferences that the cncf organized each
20:27 - year
20:29 - so let's go back to the projects menu
20:32 - and you'll notice that projects are
20:34 - categorized in three categories sandbox
20:37 - incubating and graduated let's click
20:41 - here on this the second menu here
20:43 - and let's scroll down to the bottom and
20:48 - here you get the information about the
20:51 - meaning of these three categories and
20:53 - basically that's their maturity level
20:56 - sandbox projects are mostly newer
20:58 - projects uh well graduated projects our
21:02 - projects set conservatives Enterprises
21:04 - are more likely to use
21:07 - so let's take a look at the graduated
21:10 - ones we found here kubernetes
21:15 - Helm Jagger so let's click on kubernetes
21:18 - and basically that will show you the the
21:22 - project website
21:23 - let's take a look at the incubating ones
21:26 - here we find Linker D grpc let's click
21:32 - on grpc you can get more information
21:34 - about the grpc
21:37 - all right
21:38 - let's go back to this menu remember the
21:41 - trail map that I mentioned in the
21:43 - previous lecture well here it is cloud
21:46 - native trail map
21:49 - here's the nice diagram you can get more
21:52 - information you can get send that to
21:55 - friends colleagues
21:57 - and here's a link to the landscape
22:00 - diagram
22:03 - here it is it's super huge
22:07 - Let's uh let's click here on kubernetes
22:11 - and here we get very interesting
22:13 - information
22:14 - well you get the repository where the
22:16 - project is uh is stored you get the
22:19 - number of stars and the activity number
22:22 - of commits here
22:24 - you get you get the website address
22:28 - and also the Twitter handle here
22:32 - so you should follow the Twitter feed of
22:35 - the project that you're using so let's
22:37 - close this one
22:39 - let's click here on hell
22:42 - again
22:43 - website
22:45 - uh repository number of stars activity
22:49 - and the Twitter handle
22:57 - and this concludes this look at the cncf
22:59 - website
23:02 - thank you
23:08 - let's now talk about the containers
23:10 - Concepts
23:13 - containers containers containers
23:16 - they are everywhere but what are they
23:20 - exactly
23:23 - a container is a unit of deployment
23:27 - it contains everything needed for the
23:30 - code to run
23:31 - so the compile code the runtime system
23:35 - libraries and this is system tools
23:39 - you take a container push it on a server
23:42 - and it should run of course it can have
23:46 - some external dependencies like a
23:48 - database or a cache but the code
23:51 - deployed in it should run as is
23:57 - so why use containers
24:00 - because it is faster to deploy something
24:03 - small than something big like a complete
24:07 - monolithic system
24:10 - uh they use fewer resources they are
24:12 - smaller
24:15 - and since they are smarter you can fit
24:17 - more on the same server
24:21 - when using cicd techniques they are a
24:24 - lot faster to deploy
24:28 - you can run them anywhere
24:32 - and they are isolated from each other
24:34 - meaning that if one fails
24:37 - it will not take the whole system down
24:39 - with it
24:42 - so what exactly is virtualize
24:45 - let's compare virtual machines with
24:47 - containers
24:50 - a VM runs on some kind of Hardware where
24:54 - an OS is installed
24:56 - the OS hypervisor will let you create a
25:00 - virtual machine
25:01 - where you will install in OS
25:06 - so basically DVM virtualized the
25:10 - hardware
25:12 - and what's happening when a VM starts
25:15 - well you see the BIOS coming up and then
25:18 - the OS boots up
25:22 - and what about the size of that VM let's
25:25 - say we have a Windows Server VM
25:27 - it can take 12 gigabytes of RAM and 500
25:31 - gigabytes of hard drive space
25:34 - and how long does it take to boot
25:37 - well depending on multiple factors
25:39 - something like 5 to 10 minutes
25:44 - now let's compare that to containers
25:48 - we still have the hardware and the OS of
25:50 - course
25:52 - there's a container runtime installed uh
25:56 - in the OS
25:57 - and containers images are run in memory
26:01 - now compared to a VM a container does
26:05 - not have to boot
26:06 - because it will use the host OS kernel
26:11 - this means that container starts in
26:13 - seconds because they don't have to boot
26:18 - they also use a lot less memory and hard
26:21 - drive space since there's no OS
26:25 - a small container can take a hundred
26:28 - megabyte of hard drive space and run in
26:30 - 64 or 100
26:33 - megabyte of RAM
26:37 - so VM and containers
26:40 - virtual machine have a larger footprint
26:43 - they are slower to boot
26:46 - ideal for long running tasks
26:49 - container are lightweight they're quick
26:52 - to start they don't have to boot they're
26:54 - portable
26:55 - and they're ideal for short-lived tests
26:58 - because they you can spin one super fast
27:03 - so are containers replacing virtual
27:05 - machines
27:07 - our virtual machines obsolete
27:10 - absolutely no containers are just
27:13 - another tool in your toolbox
27:16 - and you need to find the right use case
27:18 - for them and also for VMS
27:24 - if you're old enough you must remember
27:26 - what the telephone booth is if not well
27:29 - before cell phones we used to make phone
27:32 - calls in these spoons by dropping a dime
27:35 - or a quarter anyways
27:37 - using a telephone boot analogy you can
27:40 - pack more containers on the same server
27:44 - than what's possible with virtual
27:46 - machines
27:49 - containers are made of layers
27:52 - you start with the base OS
27:56 - add customizations
27:58 - and add your applications
28:02 - let's take a deeper look at this
28:04 - screenshot
28:06 - the docker pull command retrieves and
28:09 - download a container image
28:11 - as you can see each layer is downloaded
28:14 - individually
28:18 - notice that each has a unique ID
28:21 - and that for the first ones Docker says
28:24 - that they already exist why is so
28:28 - Docker uses a local cache to store the
28:31 - container's images and if a layer
28:34 - already exists it will not be downloaded
28:37 - again
28:38 - the benefit is that if you pull version
28:40 - 2 of an image Docker will only download
28:43 - the layers not present in its cache
28:48 - one of the goal when creating container
28:50 - images is to create them with the
28:52 - smallest number of layers possible
28:55 - later on we'll see techniques on how to
28:58 - achieve that
29:00 - now can you write on these layers
29:04 - well no except for the top one because
29:08 - it is read right
29:10 - the lower ones are read only
29:15 - another concept is the container
29:18 - registry
29:20 - it's a centralized repository where you
29:23 - deploy the container images you create
29:26 - think Gita but for containers
29:30 - Docker as one called Docker up that
29:33 - provides public and private repositories
29:37 - and all major Cloud providers have
29:40 - container registry services
29:45 - the last container concept is the
29:48 - orchestrator
29:50 - an orchestrator allows us to manage
29:52 - scale monitor the containers that we run
29:56 - on our servers
29:58 - you can install your own
30:01 - or use a managed cluster offered by one
30:05 - of the cloud providers like AWS Azure
30:08 - Google Cloud
30:10 - we will come back to the orchestra
30:12 - Concepts after we have a better
30:14 - knowledge of containers
30:17 - and this concludes this lecture on the
30:19 - containers Concepts
30:23 - [Music]
30:28 - let's Now understand what is docker
30:32 - so what is stalker that may seem like a
30:36 - simple question but there's more to it
30:39 - there's Docker the company
30:42 - and Docker the platform
30:45 - document thinks the Mobi project an open
30:49 - source container runtime that follows
30:51 - the specs from the open container
30:53 - initiative
30:55 - doctors sold its Docker Enterprise
30:58 - division late 2019 to a company called
31:00 - mirantis
31:02 - so if you want to buy Enterprise support
31:04 - or get certified with docker you have to
31:08 - go through morentis
31:11 - Docker provides a container runtime
31:15 - that runs on Mac windows and Linux
31:19 - a command line tool to create and manage
31:22 - a containers
31:24 - a Docker file format for building
31:27 - containers
31:29 - and interestingly Windows lets you
31:32 - create both windows and Linux containers
31:36 - foreign
31:39 - if for some reason Docker doesn't seem
31:42 - to work on your machine try restarting
31:44 - it by using the restart menu from the
31:47 - system icon on Windows
31:50 - or by clicking on the debug icon in
31:53 - Docker desktop on Mac and windows and
31:56 - clicking on restart
31:57 - Docker desktop is very stable but I had
32:00 - some issues when my laptop was coming
32:02 - back from hibernation but that was a
32:06 - long time ago and I haven't had issues
32:08 - for a while
32:11 - and this concludes this lecture on
32:13 - docker
32:16 - [Music]
32:21 - the easiest way to run Docker on your
32:24 - machine is by running Docker desktop
32:27 - it's a free download available on
32:29 - docker.com so you click here on get
32:33 - started
32:35 - and you download the version for your OS
32:39 - so Windows Mac Linux here
32:42 - now if you're running Windows uh check
32:46 - the version of Windows that you're
32:48 - running
32:49 - if you're running Windows 10 version
32:52 - 2004 or a later version
32:55 - you can run what what's called windows
32:57 - subsystem for Linux version 2. so wsl2
33:03 - basically it allow you to run a Linux
33:06 - distribution right into your Windows
33:09 - installation so Docker desktop can run
33:13 - its container by installing a virtual
33:17 - machine inside hyper-v or if you have
33:20 - wsl2 install it will it will install
33:24 - that virtual machine
33:26 - inside the Linux distribution and
33:29 - everything will be a lot faster so
33:31 - that's the preferred way so just to
33:33 - prove a point here I'm going to launch
33:36 - my hyper-v
33:38 - miniature
33:41 - and as you can see I'm not running any
33:45 - virtual machine so my Docker desktop
33:48 - installation uses wsl2
33:52 - you'll find a link to this installation
33:54 - guide in the modules notes
33:58 - okay so let's take a look at the docker
34:01 - desktop I'm running Windows as you can
34:04 - see I can see my Docker desktop system
34:08 - tray icon here so for Mac User it'll be
34:12 - at the top of the screen of course I can
34:15 - right click on it and let's select
34:18 - dashboard
34:22 - all right so here I can see a list of
34:27 - um containers that are currently running
34:29 - I can stop them restart them delete them
34:33 - I can see a list of uh images that are
34:37 - installed on my machine here we'll come
34:40 - back to that later on there's a gear
34:44 - icon here that's the setting icon and in
34:49 - the general section here
34:51 - you can see that
34:54 - wsl2 is enabled so that's why Docker
34:58 - desktop doesn't use a Now preview
34:59 - virtual machine it uses the wsl2 to run
35:04 - the VM
35:06 - and there's the kubernetes menu here if
35:10 - I select it I can see that
35:13 - kubernetes is enabled so when you check
35:18 - that Docker desktop will download
35:20 - additional containers to run kubernetes
35:24 - right onto Docker desktop here so very
35:28 - useful
35:30 - there's a bug icon here which is the
35:33 - troubleshoot icon here so
35:36 - if at some point you're issuing Docker
35:40 - commands and the don't work or
35:44 - something's wrong uh running Docker
35:47 - Docker commands
35:49 - you can click here on the restart button
35:51 - here
35:53 - you can see my name here it means that
35:57 - I'm currently logged and if I right
35:59 - click down on the system play icon
36:03 - you can see that I'm currently logged in
36:06 - and I have the option to to sign out so
36:08 - what uh username and password did I use
36:11 - to to log in
36:14 - when you downloaded the uh the docker
36:17 - desktop you could have created a Docker
36:19 - account or a Docker Hub account so
36:22 - that's the same username and password so
36:25 - if you go to
36:27 - up.ducker.com and you logged in well
36:32 - that's the same account that is used
36:34 - here in Dr desktop
36:37 - thank you
36:39 - [Music]
36:44 - let's take a look at your first Docker
36:47 - CLI commands
36:50 - now throughout this course I will
36:52 - introduce you to various commands
36:55 - I will list them in what I call a cheat
36:58 - sheet list like what you find on the
37:01 - slide
37:02 - I will briefly explain what the commands
37:04 - are for
37:06 - and that will be followed by a concrete
37:09 - and Zone demonstration
37:11 - so when you install Docker desktop on on
37:14 - your Mac or PC
37:16 - it also installed the docker CLI tool
37:21 - our first command is Docker info
37:24 - this will display some information about
37:26 - the docker installation on your machine
37:30 - Docker version will display its version
37:34 - and Docker login will log you into a
37:37 - Docker registry
37:39 - by default this login command will log
37:41 - you into Docker Hub the registry from
37:44 - docker
37:47 - and this concludes this lecture on the
37:50 - docker CLI
37:53 - [Music]
37:58 - alright we need to open a new terminal
38:01 - window into Visual Studio code so let's
38:05 - select a terminal new terminal or you
38:08 - can use the shortcut Ctrl shift back
38:11 - tick key
38:13 - all right this will open a terminal
38:17 - window and let's take a look at the
38:20 - commands that we will run they're really
38:23 - basic
38:24 - it's just for testing that our Docker
38:28 - installation is working correctly so
38:30 - let's type Docker info
38:33 - this will give me some information about
38:35 - my current installation what's happening
38:38 - I have 47 containers three stuff 44
38:42 - running 25 images and so on and so on so
38:46 - information that that is quite useful
38:49 - for debugging purposes I can see that
38:51 - the virtual machine running that running
38:54 - Docker desktop is running as to CPU it
38:57 - has two gigabytes of memory allocated
39:01 - all right sounds good let's uh type now
39:06 - Docker version
39:09 - and this will uh give me some
39:10 - information about the version number of
39:13 - different parts of Docker desktop
39:16 - so again useful for debugging purposes
39:21 - these are not commands that you will run
39:23 - on a day-to-day basis but they're quite
39:26 - useful for troubleshooting the last
39:28 - command is Docker login so I'm just
39:33 - going to type Docker login without any
39:34 - username password see what's happening
39:37 - and well the command says that I am
39:40 - login successfully why is so
39:44 - well if I right click here on my Docker
39:49 - desktop I can see that I'm already
39:51 - logged in so that's why I didn't have to
39:55 - input or type any username password I
39:58 - was already logged in
40:02 - [Music]
40:07 - let's now see how to run containers
40:12 - the docker pull command lets you
40:14 - download an image from a registry
40:18 - Docker run will execute an image in
40:21 - memory as a container
40:24 - if the image is not present in the
40:26 - docker local cache it will be downloaded
40:29 - automatically
40:32 - using run with the Dash D flag will run
40:36 - the container in the background
40:38 - giving you your comment prompt or
40:39 - terminal back
40:43 - the start command will run a container
40:45 - in the stopped state
40:49 - Docker PS will list all the containers
40:52 - currently running
40:54 - and add the dash a flag to also lists
40:58 - all the stopped ones
41:02 - Docker stop will stop a container
41:05 - running but the container will still be
41:07 - in memory
41:08 - we will see how to remove them from
41:10 - memory in a few minutes
41:14 - Dr kill will well kill a container that
41:18 - might be stuck in memory
41:19 - you usually don't use this comment but
41:22 - it's useful to know
41:25 - Docker image and spec will give you some
41:28 - information about an image very useful
41:30 - for debugging purposes
41:33 - so you may notice that we have two
41:36 - parameters here
41:37 - one is called image name and the second
41:40 - one container name so what's the
41:42 - difference
41:44 - the image name is the name of the image
41:47 - as you find it in the container registry
41:50 - and the container name is the name of
41:53 - the running container
41:55 - so you run an image using its name and
42:00 - then interact with it using the running
42:02 - instance name
42:05 - the Run come in as an optional flag call
42:07 - dash dash name that lets you specify
42:10 - your name
42:12 - if you don't specify one Docker will
42:15 - Auto generate one for you
42:18 - you can set limits on the memory and the
42:22 - CPU that the container can use when
42:25 - using the Run command
42:29 - so how do you run a container
42:33 - using the docker run command you specify
42:36 - the image name as found in the container
42:39 - registry
42:41 - you specify a name for the running
42:43 - instance
42:45 - and with the published flag you map a
42:48 - port from your local OS to the port that
42:51 - the container is listening to
42:54 - you can list the running containers
42:56 - using Docker PS
42:59 - notice how we stop the container by
43:02 - using the running name and not by using
43:04 - the image name
43:06 - then we remove it from memory using the
43:10 - remove or RM command
43:13 - containers are not black boxes you can
43:16 - attach a shell to your terminal and run
43:18 - commands that will execute right inside
43:20 - the running container
43:22 - by using the dash it switch and the name
43:26 - of the program you want to run
43:29 - a Windows container well you can run
43:32 - Powershell
43:35 - using the docker container exact command
43:38 - you can attach to a running container
43:42 - here's a screenshot showing the docker
43:44 - run command
43:47 - and notice the terminal problem changing
43:49 - when attached to a container
43:54 - so how do we clean up things
43:58 - the remove command lets you remove a
44:02 - container from memory
44:04 - but first it must be in the stop state
44:08 - for the command to work
44:11 - here's the remove command getting a list
44:13 - of the stopped container and removing
44:16 - them all
44:19 - the images that you pull will be cached
44:22 - locally
44:24 - you can get a list of these images using
44:26 - the docker images command
44:31 - use the remove image or RMI command to
44:35 - delete an image from your machine
44:39 - after a while you may end up with a
44:41 - bunch of images
44:42 - to do some spring cleanup I use the
44:46 - system prune command
44:48 - this will delete all the image currently
44:51 - not in use so be careful using this
44:53 - command
44:58 - and this concludes this lecture on the
45:00 - docker CLI
45:03 - [Music]
45:07 - let's now run our first container we'll
45:10 - run an engine X web server so something
45:13 - pretty basic that'll be perfect for this
45:16 - this lab all right let's open a terminal
45:20 - so terminal new terminal or Ctrl shift
45:25 - back tick
45:28 - perfect
45:29 - so we'll run disk command so let's take
45:31 - a look at the command first
45:33 - so I'm going to run a Docker run
45:36 - and let's go at the end of the command
45:39 - this will be the image that will run so
45:42 - an nginx image
45:45 - we'll give it a name
45:47 - so the name will be web server that will
45:50 - be the name of the running instance
45:54 - will map local All Sport
45:59 - 8082d or that the container is listening
46:02 - to so port 80.
46:04 - n d 4G decimal so we can get our Command
46:08 - Prompt or terminal prompt back so let's
46:11 - run this
46:15 - ha something interesting is happening
46:17 - unable to find image nginx latest and
46:21 - you see that the docker as pull all the
46:24 - different layers locally here
46:27 - so now if I issue a Docker PS
46:32 - it will list the containers that are
46:34 - currently running so I have uh three
46:36 - containers running so the kubernetes
46:39 - dashboard and the metric server here so
46:42 - don't uh look at these let's focus on
46:46 - this one so this is the containers it is
46:49 - that we just launched the nginx image
46:53 - here
46:54 - it started 30 seconds ago
46:58 - we can see that the port 8080 local
47:01 - levels is mapped to the port 80 here and
47:06 - the name is web server
47:09 - so let's launch a web browser and let's
47:14 - type localhost 8080
47:19 - the container is actually running
47:22 - fantastic all right
47:27 - so we can get a list of the images
47:31 - installed on your machine here by using
47:34 - Docker images so I have a bunch I may
47:37 - have a lot more than you but let's focus
47:40 - on the last one here uh nginx the tag is
47:44 - latest image ID and this is the uh the
47:48 - size
47:50 - all right fantastic let's try to connect
47:54 - to uh to it so we'll issue a Docker
47:57 - container exact
48:00 - we'll give it the the name so the
48:04 - running instance and the program we want
48:06 - to run
48:11 - so look at the The Prompt now
48:14 - root at some some ID so the ID is the
48:20 - actual container ID so I'm logged in as
48:23 - root on that container so now I can
48:26 - issue some some commands so let's do in
48:29 - the last
48:32 - let's thy fell less been to see what's
48:36 - in there so different commands so I'm
48:39 - connected to to that running container
48:42 - that's pretty uh pretty cool so I can
48:43 - issue commands look at the logs if any
48:46 - and do some troubleshooting so this is
48:49 - super useful
48:51 - for debugging purposes uh we'll use that
48:54 - a lot in the various steps that we'll do
48:57 - together
48:58 - all right let's get out of there by
49:02 - typing exit
49:04 - all right you can just clear my screen
49:07 - here
49:08 - so our container is running how do we
49:11 - stop it we use the docker stop command
49:15 - but
49:17 - look at the the parameter that we use
49:20 - the name of the container that we use we
49:22 - use the running instance name not the
49:26 - name of the image so that's pretty
49:27 - important here
49:29 - Docker stop web server
49:32 - all right but the container is still in
49:36 - memory if I do a doctor PS
49:40 - it's not listed anymore as you're
49:43 - running container but if if I type
49:46 - darker es Dash a
49:50 - ha for all I can see that my my
49:54 - container is still in memory here
49:57 - so I need to remove it from uh from
50:00 - memory
50:01 - so we'll use the docker RM and the name
50:05 - of the running instance so RM for remove
50:10 - and now the container is no longer
50:15 - in memory awesome
50:18 - now
50:20 - the container is still well the image
50:23 - that was used to create the container is
50:26 - still on my machine
50:28 - so if I type darker images
50:34 - you see it's still it's still here
50:38 - so that takes 133 megabyte of this space
50:44 - if I want to get rid of that I use the
50:47 - RMI so remove image command and the name
50:52 - this time of the image not the name of
50:54 - the running instance because none are
50:58 - running right now
51:01 - and you see all the layers
51:04 - have been deleted
51:09 - [Music]
51:14 - let's now see how we can build
51:16 - containers
51:19 - Docker build that you create an image
51:21 - using a Docker file
51:24 - if you run the command in the same
51:26 - folder where the docker file is located
51:29 - simply use a DOT as the file name
51:34 - if the file is located in a different
51:37 - folder specify the location using the F
51:41 - flag
51:44 - the tag command let you assign a name to
51:47 - an image
51:49 - this tagging has two parts
51:51 - a name and a tag
51:54 - the tag is usually used to specify the
51:57 - version number
52:01 - so what is a Docker file
52:05 - well it's a text file listing the steps
52:09 - to build an image
52:11 - here's the simplest Docker file I can
52:13 - imagine
52:14 - two lines
52:16 - the from command specified the base
52:19 - image
52:21 - when building new images you always
52:23 - start from something already existing in
52:26 - this case an image with the nginx web
52:29 - server using the Alpine version
52:34 - and then the copy command copies
52:36 - everything from the current folder to a
52:39 - folder inside the container
52:43 - using the build command we create a new
52:47 - image specifying the docker file
52:50 - remember to use the dot when the file is
52:54 - located in the same folder
52:59 - here's another one this time a little
53:02 - bit more complex
53:04 - it is used to create an image running a
53:06 - node.js app
53:09 - let's take a look at it
53:12 - the from command specify the base image
53:18 - using the Run command we run the package
53:21 - manager inside the container to install
53:23 - node.js
53:26 - next we copy all the local files into a
53:30 - folder named SRC inside the container
53:35 - we use the Run command to do an npm
53:38 - install
53:40 - we then had some metadata in this case
53:43 - we tell the container to listen on port
53:45 - 8080
53:48 - and finally we tell the container what
53:51 - to run when starting
53:54 - so as you see this Docker file contains
53:56 - the stats needed to run our node.js app
54:02 - we saw what tagging was a moment ago
54:05 - let's explore this again
54:08 - using the docker tag command we name an
54:11 - image using a name and optionally a tag
54:17 - if you don't specify a repository name
54:21 - it will default to Docker Hub later on
54:25 - we'll see how to push images to
54:26 - different repositories and we'll have to
54:29 - specify the Ripple's Journey name when
54:31 - tagging our images
54:34 - and this concludes this lecture on how
54:36 - to create Docker images
54:41 - [Music]
54:45 - let's see how can Visual Studio code
54:48 - help us build and run containers
54:52 - so what is visual studio code and why
54:55 - talk about this tool in this course
54:58 - well it's a text and code editor
55:01 - it's free and open source
55:05 - it runs on Windows Mac and Linux
55:09 - and you can download it for free using
55:11 - this link
55:14 - so you will work a lot with text files
55:16 - creating Docker files and later on
55:18 - Docker compose and yaml files
55:22 - a tool like vs code will help you
55:25 - because you can install plugins that
55:27 - will make your life easier
55:30 - using a different text editor no problem
55:35 - in vs code you can install plugins by
55:39 - clicking on the extension icon in the
55:42 - left menu
55:44 - then you search for Docker and install
55:47 - the extension from Microsoft
55:52 - the extension lets you add Docker files
55:55 - to your projects using the command
55:57 - palette
55:59 - open it using the view menu or type Ctrl
56:02 - shift p
56:05 - type Docker add and select add Docker
56:09 - files to workspace
56:12 - the extension will ask you a few
56:15 - questions
56:16 - and we'll create the docker files for
56:19 - you
56:24 - vs code has a built-in terminal where
56:27 - you can type commands
56:29 - or you can run commands using the
56:32 - command palette
56:35 - here's another example when running a
56:38 - container
56:40 - and when using the command palette
56:43 - there's no magic there the extension
56:45 - will simply issue a command in the
56:48 - terminal
56:49 - but sometimes it's a great way to learn
56:55 - creating Docker files is okay but what I
56:58 - like the most is the UI provided by the
57:01 - extension helping me manage my container
57:06 - so if you click on the docker icon
57:10 - you can see the images installed on your
57:13 - computer
57:15 - and you can even see the containers
57:17 - currently running
57:21 - right click on an image to manage it
57:24 - same thing for the containers currently
57:27 - running very very very useful
57:32 - and this concludes this lecture on vs
57:35 - code
57:37 - thank you
57:43 - let's now use Visual Studio code to
57:45 - containerize a node.js express
57:47 - application
57:49 - so I already installed the docker
57:52 - extension in Visual Studio code but
57:55 - let's take a look at it so if I click
57:57 - here on extensions
57:58 - and let's search for docker
58:02 - here it is
58:04 - that's the one from Microsoft almost 6
58:08 - million install at the time of recording
58:11 - so that's the one
58:13 - all right so let's go back to our files
58:17 - so I have my node.js application here if
58:21 - I click here on package.json
58:23 - [Music]
58:25 - the name of my application is my Express
58:28 - app so that'll be important in a few
58:30 - seconds
58:32 - all right so let's first add the docker
58:37 - file to our project so we'll use the
58:40 - tooling provided by the extension to do
58:43 - that
58:45 - we'll go to the view menu command
58:47 - palette or you can use the shortcut Ctrl
58:50 - shift p
58:52 - right and we'll type Docker add and
58:57 - there are two options here Docker files
58:59 - or Docker compose files so we haven't
59:03 - looked at the docker compost file so
59:05 - let's select the first one Docker files
59:08 - the extension hack is asking us about
59:11 - the application platform so it's a
59:13 - node.js application but it can generate
59:15 - Docker files for net python Java C plus
59:19 - plus go and Ruby apps so let's select
59:22 - node.js
59:25 - all right and next next it's asking us
59:28 - where is the package.js file so here it
59:32 - is it set the root of my application so
59:34 - I'll select this one
59:37 - the port that the application is uh
59:40 - listening to 3000 perfect
59:43 - and do I need the optional Docker
59:46 - compost file no not at this time
59:50 - all right so the extension quickly added
59:54 - a the docker file and also the docker
59:58 - ignore file and also a folder for vs
60:02 - code so let's take a look at the docker
60:05 - ignore basically it's a list of files
60:09 - not to deploy in our container
60:13 - and the docker file has been created for
60:16 - me to use a node base image it copies
60:21 - everything into the Container the image
60:25 - and expose port a portrait 3000 perfect
60:29 - so let's now build this image
60:34 - so we'll go back to the command palette
60:37 - view command palette Ctrl shift p and
60:41 - this time we'll search for Docker build
60:44 - all right here it is Docker images build
60:48 - image
60:51 - all right so I just issue uh the command
60:54 - and look look at what's happening
60:57 - the extension is not a black box uh
61:00 - issuing some crazy or strange uh
61:04 - commands in the background it's just
61:05 - issuing a
61:07 - a Docker Bill command here
61:11 - and remember a few seconds ago I
61:14 - mentioned that the name of the
61:17 - application in the package.json file is
61:20 - is important well that's the name of our
61:23 - uh of our application here so the
61:25 - extension use it to generate the name
61:28 - for my image
61:31 - all right
61:33 - it's just a Docker Bill command nothing
61:36 - fancy but using this extension is a
61:38 - great way to learn about some uh some
61:41 - commands that you may want to use at a
61:44 - later time from the terminal prompt
61:47 - okay terminal will be reused by test
61:50 - press any key to close it let's press
61:53 - any key all right
61:57 - the image has been built let's run it
61:59 - now so view command palette
62:04 - okay Doctor run
62:08 - so we'll use the first one
62:11 - and uh there's a list of my images
62:16 - and here is my Express app
62:19 - perfect
62:21 - run that
62:22 - select image that's the latest the tag
62:26 - latest perfect
62:29 - and look at what's happening Docker run
62:37 - sport 3000 to the port that the
62:41 - container listening to so nothing fancy
62:44 - here is uh it's a comments that you can
62:46 - type yourself but uh the extension to
62:50 - that are for you so if I
62:53 - start my browser local ocean I go to
63:00 - Port 3000
63:03 - there it is
63:05 - so that worked
63:07 - perfect
63:09 - let's now close this
63:13 - let's now use the UI provided by the
63:16 - extension so in the left menu let's
63:19 - click on the docker icon here
63:22 - and from there you have a list of the
63:27 - running containers the images and the
63:31 - different Registries that you connected
63:33 - to all right so here's our Express hat
63:38 - my Express apply it is and it's running
63:42 - name the instance name is called magical
63:46 - underscore Direct
63:48 - uh why is so well when we issued a
63:52 - Docker run command using the command
63:55 - palette
63:56 - the extension didn't provide a name so
63:59 - uh one was generated automatically by by
64:03 - docker
64:04 - so from there I can view the logs attach
64:09 - shell
64:10 - inspect open in a browser so if I select
64:15 - that
64:16 - there it is that's my app listening on
64:20 - Port 3000
64:22 - uh I can stop it restart it remove it so
64:26 - let's uh let's remove it here
64:30 - so are you sure you want to remove
64:32 - container yes
64:34 - it's not in memory anymore it's not
64:36 - running anymore
64:38 - but if I look at the list of the images
64:42 - that I have here it is my Express app so
64:46 - I can right click on it
64:48 - and I have a few options I can run it
64:51 - inspect it pull push tag and even delete
64:55 - it so let's let's run run it
65:01 - okay and you see at the top my Express
65:04 - app
65:05 - and look at the name gifted underscore
65:08 - El beckyan
65:11 - because if you look if you uh inspect
65:15 - the docker command that was issued
65:18 - by the extension there's no name
65:21 - provided
65:23 - so Docker generated one for us
65:28 - all right so let's close this
65:31 - we can stop or remove it from memory
65:36 - and if we no longer need the image we
65:40 - can just remove it here
65:44 - are you sure you want to remove the
65:46 - image blah blah blah
65:49 - okay let's remove it
65:51 - and it's gone so
65:54 - what's happening in the background is
65:56 - that the extension is simply issuing a
65:59 - Docker remove image RMI to to remove the
66:04 - image from from this so nothing nothing
66:07 - really fancy the extension it's not a
66:10 - black box that is issuing some strange
66:13 - commands it's just regular and Docker
66:17 - commands but I really like the the UI
66:20 - provided here
66:22 - and so instead of uh typing uh okay
66:25 - let's uh let's try this let's
66:28 - um
66:29 - let's clear this and let's type Docker
66:33 - images
66:35 - here's a list of my all my images but
66:38 - here I have a nice UI listing all the
66:41 - the same images here so sometimes it's
66:44 - it's easier having a little UI to help
66:48 - you accomplish some tasks other times
66:52 - it's easier issuing the commands from
66:55 - the terminal or the command prompt so
66:58 - it's up to you you can use the UI or The
67:02 - Terminal
67:03 - thank you
67:04 - [Music]
67:09 - let's talk about data persistence
67:13 - containers are ephemers and stateless
67:17 - so you don't usually store data in them
67:21 - of course you can write data in a
67:24 - container but if you destroy one or if
67:28 - it crashes any data stored in it will be
67:31 - lost
67:32 - so it's okay to write some log files or
67:37 - scrap data that you don't want to keep
67:39 - as long as you understand that you will
67:42 - lose these files at some point in time
67:46 - to purchase data you need to store it
67:49 - outside the container in what we call a
67:51 - volume
67:52 - a value Maps an external folder or even
67:55 - a cloud storage service to a local
67:58 - folder inside your container
68:01 - so your app sees a volume just like any
68:03 - regular folder
68:06 - the OS in this diagram represent the
68:09 - server or the virtual machine where the
68:12 - container is running
68:13 - as you can see a local folder is mapped
68:17 - to the VM file system so they are stored
68:20 - in a volume where survive a container
68:22 - restart or crash
68:25 - there's still a chance that we can lose
68:28 - the data if the VM crashes so later on
68:31 - we'll see how we can use some type of
68:34 - external storage provided by the cloud
68:37 - provider
68:38 - but first thing first in the next
68:41 - lecture you'll see how to create a
68:43 - volume that maps to a VM file system
68:48 - and this concludes this structure on
68:50 - data persistence
68:53 - [Music]
68:55 - um
68:58 - let's see how to create volumes
69:01 - here's a cheat sheet listing the docker
69:04 - commands for managing volumes
69:07 - Docker create volume will create a new
69:09 - volume
69:11 - LS will list all the volumes
69:16 - volume and spec will get you information
69:18 - about a volume
69:20 - remove will delete a volume destroying
69:23 - all the files storing it
69:26 - and volume prune will delete all the
69:29 - volumes currently not mounted or not in
69:32 - use
69:33 - so be super careful careful using this
69:36 - command
69:40 - alright
69:41 - you first need to create a volume using
69:43 - the docker volume create command
69:48 - then when you run a container you need
69:51 - to use the V switch or to volume a
69:53 - parameter
69:55 - specifying the volume name a column and
69:58 - the name of a local folder that folder
70:02 - will be a logical folder in your code
70:06 - so your code will see just like any
70:08 - regular folder
70:11 - if you use the inspect command you'll
70:13 - see The Logical folder location in DVM
70:18 - instead of using a volume you can
70:21 - specify a local folder
70:23 - this is great for testing purposes let's
70:26 - say you started developing your service
70:28 - and that you want to test on your Dev
70:31 - machine
70:32 - if your code can read and write some
70:36 - files correctly you can use this kind of
70:39 - mapping but don't use that in production
70:43 - using the inspect command you can see
70:46 - the local folder path
70:50 - and this concludes this lecture on
70:52 - volumes
70:55 - [Music]
71:00 - a software system data outside of a
71:03 - running container
71:05 - so to do that we'll use a value
71:09 - so I'll open a terminal
71:13 - and we'll use the docker volume create
71:16 - and the name of the volume so this will
71:18 - create a volume
71:22 - perfect let's Now list the volumes on my
71:28 - machine
71:30 - so there's a few here the first four
71:33 - were created earlier
71:35 - this is the volume that we just created
71:39 - now let's run a ninja next image
71:44 - and let's attach a or Mappy a local
71:48 - folder to that volume so we'll use
71:50 - Docker run Dash D for detach we'll name
71:53 - our instance fault test with the Dash D
71:57 - for volume we'll use the volume name
72:01 - that we created earlier and map that to
72:04 - a folder called ATP on our nginx image
72:08 - all right
72:10 - to that
72:14 - okay excellent so that worked now let's
72:18 - connect to our running instance so
72:22 - Docker exec Dash it the name of the
72:25 - instance and it will run bash here
72:30 - perfect
72:31 - let's first do an LS to see if we see
72:36 - the app folder there it is so that's
72:39 - that folder is mapped to the volume so
72:43 - anything that we store or write in that
72:47 - folder will be written externally
72:50 - outside of the container so that will
72:54 - persist
72:55 - all right
72:57 - uh just for fun instead of
73:01 - doing a cat Let's uh install Nano inside
73:05 - the running instance so I'll do first in
73:08 - apt-get update
73:12 - remember this is uh this is running
73:14 - inside the the container and we'll
73:17 - install Nano app get the install Nano
73:21 - which is a small editor perfect
73:25 - LCD into the app folder
73:30 - in open Nano so Nano and we'll create a
73:36 - file called text test.txt
73:42 - hello
73:44 - volume
73:45 - [Music]
73:47 - all right and we'll use Ctrl o to write
73:52 - to to disk Ctrl o enter and control X to
73:59 - exit Nano perfect
74:03 - if I do an LS here
74:05 - I should see my file there it is
74:10 - test.txt so let's exit the running
74:15 - instance and what we'll do will stop the
74:19 - running instance and we'll remove it
74:21 - from memory
74:23 - stop it
74:25 - here
74:28 - and we will remove it from memory with
74:32 - our m
74:34 - okay so now the container or the
74:36 - instance is gone if we would have stored
74:39 - some data inside that container it would
74:42 - it would be lost at this point but we
74:45 - use a volume so the data was stored
74:47 - externally so let's try to create a
74:51 - second instance
74:54 - same thing using the same volume and
74:58 - let's exec to it
75:02 - perfect let's
75:05 - see what is in the app folder here is
75:10 - our file so let's do cat
75:13 - test.txt just to prove hello volume
75:18 - that work
75:20 - let's exit here
75:22 - this proved that by using a volume your
75:26 - data persists a container restart or
75:29 - crash here
75:32 - the data is still there
75:35 - until I remove the volume so if I issue
75:39 - a Docker volume removed and the name of
75:43 - the the volume you see doesn't work why
75:48 - the error says that the volume is in
75:51 - news
75:52 - interesting so I need to stop
75:56 - any container instance that is that is
76:00 - running
76:01 - and remove it from memory before
76:04 - deleting or removing
76:07 - a volume let's try uh again
76:11 - term Docker volume RM for remove by
76:16 - volume
76:16 - and this time it worked
76:19 - now another thing I want to show you let
76:25 - me create it again
76:30 - perfect and let's switch to the docker
76:35 - UI here if I click Docker here
76:39 - there's a section
76:41 - that list of the different volumes here
76:44 - so my vowel is listed here
76:47 - so here I can inspect I cannot look at
76:50 - the files but I can I can manage it so I
76:53 - can click here on remove that will
76:55 - delete the volume
76:58 - are you sure yes
77:04 - [Music]
77:09 - let's now see the yaml concepts
77:12 - yaml stands for yaml ain't markup
77:15 - language
77:17 - it's a way to serialize data so that
77:20 - it's readable by human beings
77:23 - it's the file format used by Docker
77:26 - compose and kubernetes
77:30 - here's the sample yaml file
77:34 - for a key value pair you specify the key
77:38 - a column
77:40 - data space and a value
77:44 - don't forget the space it's mandatory
77:49 - here are some nested values
77:52 - you specify child values using two space
77:55 - indentation
77:58 - and quotes are not needed for string
78:01 - values
78:05 - here's a list
78:07 - again the child elements are indented
78:10 - with two spaces
78:12 - and there's a space after the dash
78:18 - this is what we call the block style
78:24 - there's also a flow style that looks
78:27 - like Json
78:28 - so you may be tempted to use it if
78:31 - you're familiar with Jason but don't I
78:34 - never saw any sample or any
78:35 - documentation using this flow Style
78:42 - since it's easy to forget a space
78:46 - and you can spend quite some time
78:48 - figuring out why your yaml file doesn't
78:50 - work
78:51 - you can use tools like this linter
78:54 - available on yamlin.com
78:57 - it will parse your yaml and flag any
79:00 - errors very useful
79:04 - and this concludes this lecture on the
79:07 - yaml concepts
79:10 - [Music]
79:16 - let's now take a look at the docker
79:18 - compose Concepts
79:20 - let's say that your app is composed of
79:22 - multiple containers
79:24 - you run the front-end container using a
79:27 - Docker run command
79:29 - Docker run again for the backend
79:31 - container
79:33 - and again for the redis cache container
79:36 - so you end up issuing multiple Docker
79:39 - run commands to run your app
79:42 - would it be nice if you could deploy
79:44 - your app using one single command
79:48 - well that's the docker compost go
79:52 - to Define and run multi-containers
79:54 - application using a single yaml file
79:59 - there's a compost plugin that extends
80:02 - the docker CLI and let you run those
80:05 - Docker compose files
80:08 - these specifications are available here
80:12 - if you look at Docker compose before you
80:15 - may have seen that sometimes the
80:17 - commands are using an iPhone so Docker
80:21 - Dash compose
80:23 - and sometimes they do not
80:25 - why is this
80:28 - at the dockercon conference in 2022
80:32 - Docker announced the general
80:34 - availability of couples version 2. okay
80:38 - this means that there was a V1 before
80:41 - and the V1 command line tool was
80:45 - installed separately from the docker CLI
80:48 - it was built using python so you needed
80:51 - to have python installed to run compost
80:54 - V1
80:55 - and the syntax was Docker Dash compose
81:01 - couples V2 is a drop in replacement
81:04 - meaning that all the V1 commands are
81:07 - working as expected
81:11 - it's installed as the docker CLI plugin
81:14 - automatically by Docker desktop
81:17 - to use it you type Docker space compose
81:21 - no hyphen needed here
81:24 - it's written in go so no need to have
81:27 - python installed to run the command
81:31 - in summary it's simply a faster version
81:34 - of the docker compose tool shipped as a
81:38 - Docker plugin instead of a python
81:41 - application
81:43 - here's a Docker compose file
81:47 - there are three containers defined in it
81:50 - web API 1 web API 2 and API Gateway
81:56 - the name that you use here defined the
81:59 - network or host name for that container
82:03 - the code running inside your container
82:05 - can use these hostname to communicate
82:08 - between each container
82:12 - for each of them you specify the image
82:14 - to run you set the internal and external
82:18 - Port the container will listen on note
82:21 - that the API version is now optional so
82:24 - it's okay to skip it
82:28 - you may ask yourself should I use darker
82:30 - compose or not
82:32 - Docker compose is perfect for small
82:35 - workloads that don't require a full
82:37 - orchestrator like kubernetes
82:39 - it's perfect When developing and testing
82:42 - locally before deploying to kubernetes
82:46 - Some Cloud providers offer services that
82:49 - support Docker compose like app service
82:51 - on Azure and ECS on AWS
82:56 - and of course you can simply use a
82:58 - virtual machine or a VPS virtual private
83:01 - server with the digitalocean or linued
83:06 - and this concludes this lecture on the
83:09 - docker compost Concepts
83:14 - thank you
83:18 - LaSalle used Docker compose
83:22 - here's a cheat sheet listing some of the
83:24 - docker compose commands
83:26 - Docker composed build lets you build the
83:29 - containers as defined in your Docker
83:32 - compose yaml file
83:34 - if the file is located in another folder
83:37 - you can use the optional Dash F per
83:40 - meter and specify the files location
83:44 - start we'll start all the containers as
83:48 - defined in your yaml file
83:50 - step will stop them but they'll remain
83:54 - in memory
83:56 - up we'll do a build followed by a start
84:00 - this is super Endy
84:02 - use the Dash D parameter to run the
84:05 - command in the background and take back
84:07 - your terminal prompt
84:10 - PS will list what's running
84:14 - remove our RM we'll remove the
84:17 - containers currently from memory
84:20 - down we'll do a stop followed by a
84:25 - remove again this is super ND
84:29 - logs will display the logs for a
84:31 - container
84:34 - and you can open a session inside a
84:37 - container by running Docker compose exec
84:40 - the container name and the program to
84:42 - run
84:48 - the docker compose file is located
84:50 - inside a folder and if you run Docker
84:55 - compose up
84:56 - this will launch your application
85:00 - if you try to run a Docker compose up a
85:02 - second time from the same folder nothing
85:05 - will happen because the application is
85:08 - currently running
85:10 - if you want to run a second instance of
85:12 - your application it was impossible with
85:15 - Docker compose V1
85:18 - but with V2 you can use a project name
85:22 - to launch a second instance of your
85:25 - application from the same folder
85:28 - here's the cheat sheet for some of the
85:31 - new commands
85:32 - Docker compose dash dash project Dash
85:36 - name followed by a project name
85:39 - this will run a then sense of the
85:41 - application as a project
85:44 - the shortcut is much shorter to type
85:47 - Dash p instead of that Dash project Dash
85:50 - name
85:52 - you can list the project currently
85:54 - running by using compose LS
85:58 - CP will allow you to copy files from the
86:03 - containers so this is super ND to
86:05 - retrieve let's say log files
86:08 - and you can copy files to The Container
86:12 - so from your machine your desktop or
86:15 - laptop to The Container by using Docker
86:18 - compose CP The Source pad the container
86:21 - ID and the destination path
86:27 - here's an example imagine that the
86:29 - docker compost file is located in the
86:32 - same folder where you run these commands
86:36 - you simply use the up command to build
86:39 - and run the containers and to take them
86:42 - down simply use the down command
86:46 - and this concludes the structure on
86:49 - Docker compose
86:53 - [Music]
86:57 - in this lab we will deploy a Docker
87:00 - compose application
87:02 - let's take a look at our Docker
87:03 - compose.yaml file
87:06 - we have one section called services and
87:10 - under that section we Define two
87:14 - Services the first one is called Web
87:16 - Dash Fe F4 front end it's a python
87:21 - application and instead of using an
87:25 - image from Docker up will be building
87:28 - that image using the build parameter
87:32 - the dot means that the docker file is at
87:35 - the same level as the docker compose
87:37 - file so here it is
87:39 - it's a simple python application just
87:43 - one file app.pi and requirement.txt that
87:48 - we're copying on that base image
87:56 - it will be listening on Port 5000 and
87:59 - this is our second service it's the
88:02 - redis cache and this time we'll be using
88:05 - an image from Docker up
88:08 - all right let's open eight terminal
88:14 - and let's build the image using Docker
88:19 - compose build
88:26 - perfect my image was build now I can
88:29 - launch the application using Docker
88:31 - compose up and Dash D for detach now I
88:35 - could have skipped the build step
88:38 - because up does a build first and then a
88:44 - start so it's super ND
88:47 - so let's use Docker couples up
88:52 - and my application is up and running I
88:57 - can test it if I go to local OS 5000
89:02 - you visited me one time and do a few
89:06 - refreshes five time perfect
89:10 - okay I can list the running containers
89:13 - using Docker compose PS
89:19 - I can also use Docker PS since it will
89:23 - list the docker container currently
89:26 - running
89:30 - and I can look at the logs for my
89:34 - front-end service using Docker compose
89:37 - logs Dash F and the name of the service
89:44 - and if I
89:48 - move that a little bit
89:50 - and it F5 a few times you see
89:55 - new entries are logged perfect
89:59 - that works
90:01 - let's do a Ctrl C to terminate the log
90:06 - streaming
90:09 - and we'll use Docker compose LS to list
90:13 - the currently running projects
90:16 - I have one project running it's called
90:21 - l09-04 Docker compose well basically
90:24 - when I use Docker compost up I didn't
90:27 - specify a project name
90:29 - so Docker compose use the folder name as
90:33 - the project name
90:34 - now let's try to create a second
90:36 - instance of our application if we use
90:39 - Docker compose up Dash D again
90:46 - well Docker compose tell me that the
90:49 - application is running so it will not
90:52 - start a new version we can try to deploy
90:55 - our second version using a project name
90:58 - so Docker compose Dash p for project
91:01 - name
91:01 - we'll name it test up Dash D
91:06 - let's see what will happen
91:09 - starting oops we have an error here hmm
91:14 - bind for local OS 5000 failed Port is
91:19 - already allocated of course are my local
91:23 - old sport 5000 is in use right now
91:27 - so what I need to do
91:29 - I need to change that Port here the
91:32 - localhost port I'll use 5001 save the
91:36 - file
91:37 - and use the same command Docker compose
91:41 - Dash B project name test up Dash D
91:48 - and this time it worked awesome
91:52 - let's open our browser
91:54 - and let's go to Port 5001
91:59 - yes that worked
92:01 - a few refreshes 10 times let's go back
92:05 - to Port 5000 the first instance
92:08 - 15 times
92:10 - all right
92:16 - so let's do a Docker compose LS again
92:21 - to list our projects now we have two
92:23 - projects the first one that we deploy
92:26 - without specifying a project name and
92:28 - the second one with the project name
92:31 - test
92:32 - let's delete our for instance by using
92:35 - Docker compose down
92:38 - so I didn't specify a project name
92:40 - Docker compose use the folder name as
92:43 - the project name now if I do the docker
92:47 - composer list I should have only one
92:50 - project running yes it's test now I can
92:54 - delete that one using Docker compost
92:56 - specifying the project name and down
93:05 - let's list the projects again Docker
93:08 - composer less
93:11 - nothing running PS
93:14 - listening the containers nothing and why
93:18 - not try Docker PS
93:21 - nothing in memory
93:26 - [Music]
93:31 - in this lab we will deploy a Docker
93:34 - compose sample application
93:37 - that is composed of three services a web
93:40 - front-end build with react
93:43 - nodejs backend and the Maria DB database
93:48 - let's take a look at our Docker compose
93:51 - file
93:52 - this is a more complex than what we saw
93:55 - so far so let's try to break it into
93:58 - smaller pieces
94:01 - here we have the definition of our tree
94:03 - services if we start from the left we
94:05 - have the back-end service
94:08 - top right the database DB
94:12 - and the front end now if you look at the
94:15 - DB service you can see that we're
94:18 - referencing an image that will pull from
94:21 - Docker hub
94:23 - and the other two Services we're using
94:27 - the build parameter
94:29 - this means that we will build these two
94:32 - images
94:37 - looking at the backend service we can
94:39 - see that we specify build and context
94:44 - contacts with the value of backend
94:46 - backend is actually a subfolder where
94:51 - the docker file is located
94:56 - next we're defining two networks public
95:00 - and private
95:02 - we can see that our front-end service is
95:05 - using the public network
95:08 - the backend service is using public and
95:12 - private
95:13 - and the database service the DB service
95:16 - use only the private Network
95:21 - front end being in the public network
95:23 - cannot communicate directly with the DB
95:28 - service
95:29 - but backend being in both public and
95:33 - private can communicate with both DB and
95:38 - front-end services
95:42 - we're defining also two named volumes
95:45 - back in dash modules and DB Dash data
95:50 - and we can see highlighted in yellow
95:53 - that our DB service is using DB data and
95:57 - our backend service is using back-end
95:59 - modules
96:04 - highlighted in yellow we also see that
96:06 - we're using other volumes these volumes
96:10 - are scoped at the service level and are
96:13 - not shared between services
96:16 - to create an instance of our Docker
96:18 - composed application we simply use
96:20 - Docker couples app and to bring it down
96:22 - Docker compose down
96:26 - in Visual Studio code let's take a look
96:28 - at the docker compose file so it's
96:30 - called compose.yaml
96:33 - here we have our services section
96:36 - Network section where we Define two
96:39 - networks the name volumes back-end
96:44 - modules and DB data and secrets that we
96:47 - haven't seen yet where we're defining
96:49 - one key value pair so the key is DB
96:53 - password and we get the secret from a
96:56 - file the file is located in the folder
96:59 - called DB and it gets the value from a
97:03 - file called password.txt if we open the
97:07 - DB folder here's the file with the value
97:11 - the secret
97:13 - and that will be injected when we run
97:17 - Docker compose up
97:20 - okay let's go back to the services
97:23 - section and here we have our backend DB
97:27 - and front-end Services let's take a look
97:31 - at backend
97:32 - we're using the build a directive and we
97:36 - are setting the context to backend it
97:39 - points to this folder called backend
97:42 - here and the docker file is located
97:44 - inside that backend folder with the
97:48 - application
97:49 - same thing with the front-end service
97:53 - build context front-end and we're
97:57 - getting the docker file and the
97:59 - application from the front-end folder
98:04 - and what about the DB service well we're
98:08 - using an image that will pull from
98:10 - Docker hub
98:12 - okay let's build our two images
98:17 - we'll open a terminal
98:20 - and run Docker compose build
98:33 - our images were billed perfect now let's
98:37 - run the application by using Docker
98:39 - compose up Dash D
98:57 - ER perfect the application is this thing
98:59 - on Port 3000 so let's open a browser
99:04 - and type local OS 3000
99:09 - is our react application it's working
99:12 - awesome
99:15 - we can list the containers that are
99:19 - currently running by using Docker
99:21 - compose PS
99:23 - we should have three backend DB front
99:27 - end awesome
99:28 - let's take a look at the logs from the
99:31 - back-end service Docker compose logs
99:35 - Dash F backend
99:37 - and these are the logs for our backend
99:39 - service awesome
99:42 - we can type Ctrl C to stop the log
99:46 - streaming
99:47 - and we can take our application down by
99:51 - using Docker compose down
99:57 - this will stop and remove the containers
100:00 - from memory
100:02 - do we have something else in memory we
100:06 - should not
100:10 - perfect and even if I type doctor PS
100:15 - there's nothing however the volumes are
100:19 - still there when you do a doctor compose
100:21 - down it will remove the containers from
100:24 - memory but will not remove the volumes
100:27 - so if I open the docker desktop
100:30 - application and I click on volumes here
100:34 - I can see that I have a few well three
100:37 - volumes that were created a few minutes
100:39 - ago
100:41 - so I need to delete them manually I can
100:44 - select them right here and click on
100:47 - delete or I can do the same thing by
100:50 - clicking on the docker icon in vs code
100:53 - locating the volumes here
100:58 - and delete them
101:00 - find it easier to do that in Docker
101:04 - desktop because we see when the volumes
101:07 - were created so I'm pretty sure that
101:10 - these three are the ones that I need to
101:13 - delete I'll click on the delete button
101:16 - and confirm
101:20 - thank you
101:21 - [Music]
101:27 - let's take a look at some of the compose
101:30 - file features
101:33 - it's a good practice to set limits on
101:36 - the resources that your container will
101:38 - use
101:39 - in this example I light it in yellow we
101:42 - tell Docker to start the container with
101:45 - a quarter of a CPU and 20 megabytes of
101:48 - RAM
101:50 - the green section is the limits that we
101:53 - are allowing in this case half of a CPU
101:56 - and 150 megabytes of RAM
102:01 - to set an environment variable that will
102:04 - be injected in the running instance
102:07 - simply set the key value pair in the
102:10 - environment section
102:13 - those values can be overridden at the
102:16 - common line using the dash e per meter
102:21 - you can reference an environment
102:23 - variable using the daughter curly
102:27 - bracket syntax
102:28 - this way you can set the variable on
102:31 - your machine or server and use it
102:34 - directly in the compose file
102:38 - you can place the values in a file that
102:41 - you will name dot EnV located in the
102:46 - same folder as the compose file the
102:48 - compost command will automatically read
102:51 - the values from that file
102:55 - by default all containers specify in a
102:58 - compose file will see each other using
103:01 - their service names here we have two
103:04 - Services web and DB
103:09 - the code running in the web service can
103:12 - communicate with the second one using DB
103:16 - as the hostname and vice versa
103:23 - the web container is visible from
103:25 - outside of the docker network using the
103:29 - port number configured in the left
103:31 - portion of the ports value
103:34 - web is listening inside the docker
103:37 - Network on Port 80. DB can reach web on
103:41 - Port 80.
103:43 - finally DB only exposes one port number
103:48 - that's the internal port
103:51 - web can reach DB using Port 5432
103:56 - but DB is not visible from outside the
104:00 - docker Network
104:03 - if you have a compose application with
104:06 - multiple containers you can restrict who
104:09 - sees who by configuring networks in this
104:13 - example we're defining two Networks
104:16 - front end and back end
104:20 - proxy can see app because both are part
104:26 - of the front-end Network
104:29 - however proxy does not cdb because it's
104:34 - not part of the backend Network
104:40 - when using multiple containers you may
104:43 - want to start some of them first and
104:45 - wait until they are running before
104:47 - starting the other ones a typical use
104:50 - case is a database that you want to run
104:53 - before sorting the main application
104:58 - doing so is easy using the depends on
105:01 - parameter where you simply specify what
105:05 - is the service name that the service is
105:08 - dependent on in this example app depends
105:12 - on DB so compost will first start DB and
105:17 - when DB is running compose will then
105:20 - start app
105:24 - you can declare volumes in the volumes
105:26 - section
105:28 - these are called named volumes and they
105:31 - can be used by all these services that
105:34 - you are declaring in the compose file
105:37 - to use a volume from a service map it to
105:41 - a local folder using the volume name
105:44 - colon and the virtual path inside the
105:48 - container
105:50 - optionally you can make the mapping
105:52 - read-only by appending colon Ro to the
105:57 - mapping
106:00 - you can also create a mapping without
106:03 - using a named volume
106:06 - this mapping can't be shared across
106:08 - services
106:12 - it's also a good practice to set a
106:15 - restart policy
106:16 - let's say that you deploy your couple's
106:19 - app in a VM
106:20 - and at some point you need to install
106:23 - some OS batches and you need to restart
106:26 - or reboot the server
106:29 - what will happen to your compost app
106:32 - well if you don't specify a restart
106:35 - policy
106:36 - the one by default is no meaning that
106:39 - compose will not restart the containers
106:42 - if they were shut down by reboot
106:47 - you can set the policy to always this
106:50 - way compose restarts the containers
106:52 - until their removal
106:55 - on failure resource a container if the
106:58 - exact code indicates an error
107:01 - and lastly unless stop does a restart
107:05 - unless you stop or remove the containers
107:10 - and this concludes this look at some of
107:12 - the docker compose features
107:18 - [Music]
107:23 - let's now talk about container
107:25 - registries
107:26 - so what is the container registry it's a
107:30 - central repository for container images
107:34 - you build an image locally
107:36 - then you push and store the binary the
107:39 - different layers
107:41 - 2D Repository
107:43 - they can be private or public
107:47 - the default one is Docker hub
107:51 - Microsoft AWS and Google each offer
107:55 - container Registries as service
107:58 - the benefit of using a repository from
108:01 - your cloud provider is that the images
108:03 - are located near your app so no apps
108:08 - over the internet to retrieve the images
108:13 - so let's say we want to retrieve an
108:15 - image from Docker up
108:17 - we issue a Docker pull command and
108:20 - Docker downloads the images layers and
108:23 - store them in its local cache
108:27 - and this concludes this lecture on
108:29 - container registries
108:34 - [Music]
108:39 - let's see how to push and pull images to
108:42 - Docker up
108:45 - make sure you are logged in with your
108:49 - Docker user account to be sure simply
108:52 - type Docker login
108:54 - without a username and password
108:56 - Docker will tell you that you're already
108:59 - logged in if not enter your Docker
109:01 - username and password
109:04 - you need to tag an image with the
109:07 - repository name by default it's your
109:09 - username
109:11 - if you have created some organization in
109:13 - Docker hub
109:15 - prefix it to the name of the image
109:18 - in this example I want to push this
109:20 - image to my Kates Academy organization
109:24 - then use the push command and don't
109:28 - forget to specify the organization name
109:30 - it's part of the image name
109:34 - to retrieve the image we use the pull
109:36 - command with the image full name
109:42 - on Docker up public images are available
109:45 - for download to anyone
109:48 - if you don't want to share them you need
109:50 - to create a private Repository
109:54 - later on we will create one using our
109:57 - cloud provider
109:58 - and this concludes this lecture on
110:00 - Docker up
110:03 - [Music]
110:08 - let's now push our first image to Docker
110:11 - hub
110:12 - first thing first let's make sure that
110:15 - we can log in into Docker Hub so if you
110:18 - head to
110:20 - up.docker.com make sure you can log in
110:24 - also if I right click here on my Docker
110:28 - desktop icon I can see that I'm logged
110:31 - in
110:32 - perfect
110:36 - we will containerize a node.js Express
110:40 - application so we'll first add the
110:43 - docker file
110:44 - we'll use the tooling so view command
110:47 - palette
110:49 - if you can type Docker add and Docker
110:53 - files to workspace
110:57 - this is a node.js application
111:00 - the package.json file is located at the
111:03 - real so that's the correct one
111:05 - it's listening on power 3000 and we
111:09 - don't want Docker compose files
111:11 - perfect
111:13 - now we need to build the image so let me
111:16 - open a terminal
111:21 - perfect and we need to issue a Docker
111:24 - build command
111:25 - with the Dash D for tag parameter
111:30 - but notice here we need to prefix the uh
111:33 - the name of the image with our registry
111:36 - name
111:38 - if I select this
111:42 - I need to prefix that with
111:45 - my name here the name of my registry
111:51 - let me run that
112:06 - the image was built successfully now I
112:10 - need to use the push command to push
112:13 - that image onto Docker Hub again I'll
112:16 - select this
112:19 - and let's replace the registry name with
112:25 - mine
112:28 - use your home
112:32 - and see what's happening here Docker is
112:36 - pushing each layer to my Docker app
112:40 - account here
112:42 - all right
112:44 - no errors things went fine let's go back
112:48 - here I'm gonna refresh this page
112:52 - and here it is
112:54 - here's my
112:55 - um uh image here my Express image
112:58 - so I can click on this
113:02 - I can edit the
113:05 - um
113:05 - the information I can see that I have my
113:09 - tag is V1
113:11 - right I can get more information here I
113:15 - can here click on public view so by
113:18 - default the repositories on Docker Hub
113:21 - are public anyone can download and view
113:25 - and download your images this is the
113:28 - view for someone who would look at my
113:31 - image so with the pull command Docker
113:34 - pull
113:36 - let me go back here
113:39 - I can see the tags
113:42 - and so on and so on and if at some point
113:46 - you want to delete this you go into the
113:50 - settings tab here
113:53 - you scroll down and you can delete that
113:55 - repository here
113:59 - all right let's go back
114:03 - now let's try to pull that image to our
114:06 - computer here
114:08 - the first thing uh I will try to do is
114:12 - remove it from my computer
114:14 - let me
114:18 - type this RMI so remove image
114:22 - the image has gone completely so now
114:25 - let's try to pull it from uh from Docker
114:29 - uh
114:32 - okay
114:40 - Perfect Pull complete my image is is
114:44 - back here
114:46 - now
114:48 - you see that I use the V1 tag here to to
114:53 - tag my image with the with the version
114:55 - number so let's try to build a version 2
114:59 - of that image let me copy that
115:03 - and again let's replace this part this
115:07 - placeholder with my registry name
115:23 - image has been built so let's now push
115:26 - it
115:27 - two Docker uh
115:32 - same thing as we did before but this
115:37 - time we're pushing version two
115:45 - okay
115:47 - let's go back here let's go back to my
115:52 - Express or the general tab
115:56 - look here I have V1 and V2
116:01 - if I click here on tags
116:04 - I can see when the image was was pushed
116:09 - tags and people can download V1 or V2
116:20 - if I want to remove my images
116:23 - I use the RMI
116:25 - command so remove image I'll remove V1
116:29 - and also I remove V2
116:32 - and on Docker up if I no longer need
116:37 - this Repository
116:39 - I click here settings scroll down a
116:43 - little bit
116:44 - click on delete Repository
116:47 - and I need to enter the name of my repo
116:51 - just to make sure and click on delete
116:55 - and now it's gone
116:58 - [Music]
117:03 - time to introduce kubernetes
117:07 - so kubernetes or also known as Kates so
117:11 - the letter K followed by the number
117:14 - eight so eight letters
117:15 - and then the number s
117:17 - and it's pronounced Kate's so kubernetes
117:21 - is a project that was created at Google
117:25 - version one came in July 2015.
117:29 - it was the third generation of container
117:31 - scheduler from Google
117:33 - previous projects were Borg and Omega
117:37 - and Google donated the kubernetes to the
117:41 - cncf so now the development is
117:44 - supervised by the cncf
117:47 - is currently the leading container
117:50 - orchestration tool
117:52 - it's designed as a Loosely coupled
117:55 - collection of components for deploying
117:57 - managing and scaling containers
118:01 - it's vendor neutral so it's not attached
118:04 - to a single company and it runs on all
118:07 - Club providers
118:09 - and there's a huge community ecosystem
118:12 - around kubernetes
118:15 - so what kubernetes can do
118:19 - service Discovery load balancing
118:22 - it can bridge to the cloud providers
118:25 - storage services
118:27 - can provide rollout rollbacks
118:30 - capabilities
118:33 - and can monitor the health of the
118:35 - containers
118:36 - can manage configuration and secrets
118:40 - and the same API is available either in
118:45 - a on-premising solution or in every
118:48 - cloud provider
118:50 - so what can't kubernetes do
118:54 - it can deploy or build your code
118:58 - and it does not provide application
119:00 - Level services like databases service
119:03 - buses caches
119:07 - here's a quick look at the kubernetes
119:09 - architecture
119:10 - this diagram was taken from the
119:13 - kubernetes documentation
119:15 - we'll take a closer look at each
119:17 - component but for now let's just say
119:19 - that it's composed of a master node also
119:22 - called the control plane so that's the
119:25 - portion to the left
119:28 - and the control plane runs the
119:31 - kubernetes services and controllers
119:36 - and you have the worker nodes these node
119:39 - runs the containers that you'll deploy
119:41 - in the cluster
119:44 - so a container will run in a pod a pod
119:48 - runs in a node and all the nodes form a
119:52 - cluster
119:55 - and this concludes this intro to
119:58 - kubernetes
120:01 - [Music]
120:06 - let's see how you can run kubernetes
120:08 - locally
120:10 - so do you need to install a kubernetes
120:12 - cluster in the cloud or ask your it
120:14 - Department to install one in your
120:16 - Enterprise so you can test locally
120:19 - absolutely not there are many ways that
120:22 - you can run kubernetes on a desktop or
120:25 - laptop
120:27 - Docker desktop lets you run kubernetes
120:30 - macrocates from the makers of Ubuntu and
120:34 - minicube also let you run kubernetes
120:37 - altree requires that virtualization is
120:40 - enabled
120:42 - kind runs over Docker desktop and offer
120:46 - extra functionalities
120:49 - Docker desktop is limited to OneNote but
120:53 - it's usually not a problem
120:55 - Marco Cates kind and minicube can
120:59 - emulate multiple worker nodes
121:03 - on Windows Docker desktop lets you run
121:06 - both Linux and windows containers
121:10 - and you can't create and run Windows
121:12 - containers on Mac and Linux
121:16 - it runs on hyper-v or Windows subsystem
121:20 - for Linux
121:21 - so if you have Windows 10 version 2004
121:25 - or later
121:26 - it's the recommended way to run Docker
121:29 - desktop
121:32 - if fiber-v is enabled on your laptop or
121:35 - desktop you can't run another hypervisor
121:38 - at the same time
121:40 - and mini Cube used by default virtual
121:43 - box but it can also run on hyper-v
121:49 - you can install Docker desktop on
121:51 - Windows using App review and it will
121:54 - create a virtual machine named Docker
121:56 - desktop VM
121:58 - when you take the enable kubernetes
122:01 - checkbox in Docker desktop it will
122:05 - download additional containers to run
122:07 - kubernetes
122:10 - using Windows 10 version 2004 or later
122:13 - and if you have wsl2 installed you can
122:18 - tick the use the wsl2 base engine and
122:21 - Docker desktop will create its VM inside
122:24 - the Linux distro you install on your
122:27 - Windows machine
122:29 - note that this is the recommended way to
122:32 - run Docker desktop
122:35 - on Mac Docker desktop use the hyperkit
122:39 - lightweight hypervisor to run its VM
122:44 - mini cube is another popular option
122:48 - it does not require a Docker desktop it
122:51 - runs on Linux Mac in Windows and it
122:54 - requires an appervisor like a virtualbox
122:57 - here we can see mini Cube running on a
123:01 - Mac and its virtual machine in
123:04 - virtualbox
123:09 - if you need to install mini Cube on
123:11 - Windows but don't want to install
123:12 - virtualbox you can run minicube on
123:15 - hyper-v
123:17 - you need to create a network switch and
123:19 - start minicube with some extra
123:21 - parameters
123:25 - kind stands for kubernetes in Docker
123:29 - because it runs on top of Docker desktop
123:33 - kind lets you emulate multiple control
123:36 - planes and multiple worker nodes
123:39 - this is useful if you need to test node
123:42 - affinity
123:45 - and this concludes the sector on how to
123:47 - run kubernetes locally
123:51 - [Music]
123:56 - and this will be a super quick lab just
123:59 - to validate that our kubernetes
124:01 - installation is working locally
124:03 - so here I'm on Windows and I'm using
124:07 - Docker desktop and I installed
124:09 - kubernetes with Docker desktop
124:12 - in the system tray I can right click on
124:15 - the docker desktop icon on a Mac you can
124:18 - do that from the top of the the screen
124:21 - and I will select dashboard
124:27 - and from there we're going to click here
124:29 - on the gear icon the settings icon
124:34 - and I'm using wsl2 to run Docker desktop
124:38 - so that's the recommended way
124:40 - and here if I click on kubernetes
124:43 - uh enable kubernetes is checks so
124:45 - kubernetes is installed but is it
124:49 - running correctly let's find out let me
124:53 - go back here let's open a terminal
125:00 - and let's run uh this command uh Cube
125:03 - CTL cluster info
125:06 - that should give us a little information
125:08 - about what's running so kubernetes
125:10 - Master it as in green yay it's working
125:13 - so it's running at this address and Cube
125:16 - DNS is running at this address
125:18 - so by running qctl cluster info you get
125:22 - some information about the kubernetes
125:24 - installation and itself
125:29 - [Music]
125:34 - let's see how you can use the kubernetes
125:36 - CLI
125:39 - the kubernetes API server is a service
125:42 - running on the master node
125:45 - it exposes a rest API that is the only
125:49 - point of communication for kubernetes
125:52 - clusters
125:54 - you define the desired state in yaml
125:57 - files
125:58 - and let's say you want to run a x number
126:02 - of instances of a container in the
126:04 - cluster
126:06 - using the kubernetes CLI you then send
126:10 - that desired state to the cluster via
126:13 - the rest API
126:16 - other applications like a web dashboard
126:19 - can also communicate with the rest API
126:21 - to display the cluster state
126:26 - Cube CTL is the kubernetes CLI and it
126:29 - runs on Mac Linux and windows
126:33 - and you pick your choice of
126:34 - pronunciation cue control Cube cuddle
126:38 - Cube CDL doesn't matter
126:42 - it communicates with the API server
126:45 - and its connection information is stored
126:48 - in a config file under the dot Cube
126:52 - folder
126:56 - let's now see what a context is
127:00 - it's a group of access parameters that
127:03 - let you connect to a kubernetes cluster
127:06 - it contains the cluster name a user and
127:10 - a namespace
127:12 - the current context is the cluster that
127:16 - kubernetes commands will run against
127:19 - let's say that you can connect to three
127:22 - clusters cluster a cluster B and cluster
127:25 - C
127:27 - when you set the default context to
127:29 - Cluster B
127:30 - then all the cube CTL commands that you
127:34 - you will run will run against cluster B
127:40 - here's a cheat sheet for context
127:43 - commands
127:46 - Cube CDL config current context will get
127:50 - you the current context
127:53 - get context will list all of them
127:58 - Cube CL config use contacts and the
128:02 - context name Will Set the current
128:05 - contacts
128:08 - and delete contacts with the context
128:11 - name we'll delete the context from the
128:14 - config file
128:19 - there's a large ecosystem of free open
128:21 - source kubernetes tools that you can use
128:24 - Cube CTX is a good example
128:28 - it's a shortcut for the Cube's DL config
128:31 - use context command
128:33 - you simply type Cube CTX followed by the
128:37 - context name to quickly switch contacts
128:41 - it runs on Windows Mac and Linux
128:44 - very useful
128:47 - and this concludes the structure on the
128:49 - kubernetes CLI and the concept of
128:51 - context
128:55 - [Music]
129:00 - a context contains connection formation
129:03 - to a kubernetes cluster and you can have
129:06 - one or more than one context set on your
129:10 - machine it's super important to know how
129:13 - to figure out in which context you're
129:15 - currently in and how to change context
129:18 - so first thing first let's figure out in
129:20 - what context we're currently in I'm
129:22 - going to use cubectl config current
129:25 - context
129:28 - and this will print the name of the
129:31 - context we're currently in I'm currently
129:33 - in the docker desktop contact means that
129:36 - whenever I type Cube CTL commands it'll
129:38 - be applied to that kubernetes cluster
129:42 - all right I can have more than one
129:45 - context configure on my machine or on
129:48 - any machine or server to list them we
129:51 - use Cube CTL config get Dash context
129:56 - and here we can see that I have two
129:58 - contacts set on my machine Docker
130:01 - desktop which is the current context
130:03 - because it has that star in the current
130:06 - column and I have a second one called
130:09 - demo then one is a cluster that I
130:11 - created in the cloud
130:13 - what if I want to change from Docker
130:17 - desktop to demo
130:19 - okay let's use cubectl config use Dash
130:24 - context and the context name
130:29 - demo
130:32 - and now if
130:34 - I print again the current context
130:38 - there it is demo so whenever I'll be
130:41 - typing Cube CTL command
130:45 - DZ yaml files this command will be sent
130:48 - to My Demo cluster somewhere in the
130:52 - cloud
130:54 - a cool tool is Cube CTX because it
130:58 - allows you to well instead of using Cube
131:02 - CDL config blah blah blah blah blah
131:05 - basically it's a shortcut
131:08 - if I simply type a cube CTX
131:12 - that will print the context that I've
131:14 - configured on my machine so demo is
131:16 - green means that it's the current
131:18 - context and I can quickly change context
131:23 - using cubectx and the context name
131:28 - Docker desktop you know a little bit
131:31 - less keystroke to talk about anyway it's
131:34 - a fun too
131:36 - foreign
131:39 - context let's say uh you you your
131:42 - cluster has a long funky name and you
131:44 - want to rename it to make uh more sense
131:47 - you can use the cube CTL config rename
131:50 - contacts all name new name so let's try
131:53 - to rename our demo cluster here
131:57 - demo let's say
132:01 - it's on Azure so Azure demo
132:06 - okay let's use the cube CTX to print the
132:10 - contacts and there it is azure demo
132:15 - where is that context information stored
132:18 - it's stored locally on your machine so
132:21 - I'm on Windows let's see where it is
132:25 - it's on the C drive
132:28 - users
132:29 - your username
132:34 - under dot Cube deduct Cube folder
132:40 - and there it is config so if I right
132:43 - click on it
132:44 - and select opening with code
132:49 - we can see that it's a yaml file
132:54 - and it contains two entries here two
132:56 - clusters
132:59 - one is the Azure one and the second one
133:02 - is the
133:04 - Docker desktop one
133:06 - and this is the different context that I
133:10 - have that have so here is my Docker
133:13 - desktop context
133:15 - and here is my demo context
133:19 - well the cluster name is still demo but
133:22 - I renamed the context name to Azure Dash
133:27 - demo
133:28 - there it is
133:30 - all right
133:32 - let's go back here
133:35 - and let's say that I've deleted my
133:39 - cluster in the cloud I don't need it
133:41 - anymore
133:43 - I want to get rid of the contacts
133:46 - information I can use cubectl config
133:49 - delete contacts and the context name
133:52 - so let's try this
133:57 - Azure demo
134:00 - deleted context Azure demo from the
134:04 - config file so let's take a look here
134:08 - and you see that now I have only one
134:12 - contacts
134:14 - and here's the current context
134:17 - in this line here
134:19 - but let take a look here at the Clusters
134:24 - list
134:25 - My Demo cluster is still there
134:28 - right so it's not deleted automatically
134:31 - what you can do is simply edit that
134:34 - config file and remove the section no
134:37 - longer needed
134:40 - [Music]
134:45 - let's talk about the declarative and the
134:47 - imperative ways to create resources in
134:50 - kubernetes
134:52 - there are two ways that you can use when
134:54 - you want to create resources in
134:55 - kubernetes
134:57 - the declarative way and the imperative
135:01 - way
135:02 - using the imperative way you use cubectl
135:07 - to issue a series of command to create
135:09 - resources in the cluster
135:11 - this is great for learning testing and
135:14 - troubleshooting
135:16 - using the declarative way you define the
135:19 - resources needed in yaml files and use
135:23 - cubectl to send the content of these
135:27 - file as the desired state to the cluster
135:31 - so instead of a series of command
135:35 - this is reproducible and you can even
135:38 - store these yaml files in a source
135:41 - control system
135:43 - here we can see a series of command to
135:45 - create resources that's the imperative
135:48 - way
135:49 - you can create a pod using the Run
135:51 - command create a deployment or service
135:55 - using the create command
135:58 - using the declarative way
136:01 - you would use a yaml file to define the
136:04 - resource and then send the content of
136:06 - that file to the cluster to create these
136:09 - resources
136:10 - so what's a yaml file
136:14 - well it's a text file that contains
136:17 - properties that Define the resource
136:19 - it has some required properties like the
136:22 - API version
136:23 - the object kind that defines the type of
136:26 - object you want to create
136:28 - we'll take a look at these later on
136:32 - you can use the cube CTL create command
136:34 - to send the information to the
136:36 - kubernetes cluster
136:38 - we will take a deeper look at yaml files
136:41 - in the future lectures but right now you
136:44 - may be wondering
136:45 - do you need to type all that yaml
136:48 - manually
136:49 - the answer of course is no
136:53 - one way to get the correct syntax is to
136:56 - copy one from the official kubernetes
136:58 - documentation at kubernetes at IO slash
137:02 - Docs
137:03 - you then search for the object you want
137:06 - to create
137:07 - and click on the copy icon
137:11 - another way is to use templates offered
137:14 - with an editor like Visual Studio code
137:18 - let's say you create a new yaml file
137:21 - then you type control space and select
137:25 - the template to generate the Manifest
137:28 - that you can edit neat
137:32 - you can also use the kubernetes CLI to
137:36 - generate the ammo
137:38 - add dash dash dry run equals client and
137:43 - dash o for output
137:45 - specifying the yaml to Output the yaml
137:49 - to the console
137:52 - you can even send the output to a file
137:54 - using the greater than sign and a file
137:58 - name
138:01 - and this concludes this overview of the
138:04 - imperative and declarative ways to
138:06 - create resources in kubernetes
138:10 - [Music]
138:15 - let's deploy an nginx container using
138:18 - both the imperative way and the
138:20 - declarative way
138:22 - using the imperative way uh we're gonna
138:25 - type command called cubectl create I'm
138:28 - going to create a deployment I'm going
138:30 - to name our deployment my nginx1
138:33 - I'm going to specify a parameter called
138:36 - image where we will specify the image we
138:39 - want to run
138:40 - this will create a deployment
138:45 - and we can Cube CTL get a list of the
138:51 - deployments using Cube CTL get deploy
138:54 - and there it is
138:56 - now the second way is the declarative
138:58 - way
138:59 - instead of typing a command with all the
139:02 - parameters at the common line we're
139:05 - going to specify a yaml file where all
139:08 - the configuration options are stored
139:11 - cubectl create Dash f for file and the
139:15 - name of the file
139:17 - so let's run this
139:22 - okay
139:24 - it was created let's again type Cube CTL
139:28 - get deploy
139:29 - get both our deployments and we if we
139:32 - take a look at the yaml file well it's a
139:35 - yaml file the type deployment it has the
139:39 - my nginx2 name it has a bunch of
139:42 - parameters that will come back
139:44 - to that a little bit later but basically
139:47 - all the configuration parameters are
139:51 - stored in that yaml file what's cool
139:53 - with that concept is that it's you can
139:55 - put these files quite easily in a source
139:58 - control system
140:00 - all right uh let's do a little bit of a
140:03 - cleanup let's delete our deployment Cube
140:06 - CTL delete deployment my nginx one
140:13 - okay and I'm gonna use a the same
140:16 - command but using a shortcut this time
140:19 - Cube CTL delete
140:21 - instead of deployment
140:23 - and the name of our deployment
140:28 - okay and if I type again Cube CTL get
140:32 - deploy
140:34 - there's no deployments currently my
140:36 - namespace
140:39 - [Music]
140:44 - let's take a look at namespaces
140:48 - so what is a namespace
140:51 - it's a kubernetes resource that allow
140:53 - you to group other resources
140:56 - let's say you need to deploy your
140:57 - application in multiple environments
141:00 - like Dev test and prod
141:03 - well you can create a namespace for each
141:05 - of these environments
141:08 - they are like logical folders in which
141:11 - you group resources
141:13 - kubernetes creates the default namespace
141:15 - called well default
141:18 - objects in one namespace can access
141:21 - objects in different in a different one
141:23 - the kubernetes internal DNS assigned
141:26 - Network names to some resources
141:29 - deleting a namespace will delete all
141:32 - these child objects
141:34 - this is super useful when doing tests
141:37 - create a namespace in the morning
141:40 - create resources under that namespace
141:42 - during the day
141:44 - and at the end of the day simply delete
141:46 - that namespace
141:48 - this command lists all the namespaces
141:50 - existing in a cluster we'll take a look
141:53 - at this command in more detail
141:58 - okay
142:00 - first you create a namespace this yaml
142:03 - file Define a namespace called prod
142:08 - then you use that namespace when you
142:11 - create other resources
142:13 - in the metadata section you set the
142:16 - namespace key to the name of the
142:19 - namespace you want this resource to be
142:21 - created in
142:23 - so namespace colon prod
142:27 - you can assign Network policies and
142:30 - limit the resources that you can create
142:32 - in a namespace using the resource quota
142:35 - object
142:39 - here's a cheat sheet for the namespace
142:41 - commands
142:44 - so cubesatel get namespace list all the
142:48 - namespaces
142:49 - and if you don't want to type namespace
142:53 - each time you can use a shortcut an S so
142:57 - Cube CTL get an S is the same thing as
143:00 - cubectl get namespace
143:03 - you can set the current context to use a
143:06 - namespace in the next commands that
143:10 - you'll type by using cubectl config set
143:14 - contacts
143:15 - current with which will use the current
143:18 - context and then the namespace is called
143:21 - the name space that you want to use so
143:24 - the next command that you'll type will
143:27 - be under that namespace
143:32 - cubectl create NS and the name of the
143:34 - namespace so you create a namespace you
143:37 - delete it using cubectl delete in s and
143:40 - the namespace name
143:42 - and you can also list all the pods or
143:46 - the objects from another namespace so
143:49 - Cube CTL get parts or any objects and
143:53 - then you pass the flag dash dash all-
143:56 - namespaces that will list all the
143:59 - objects in all the different things
144:01 - spaces
144:05 - and this concludes this look at
144:07 - namespaces
144:10 - [Music]
144:15 - let's see how to list and switch between
144:19 - name spaces
144:23 - you can get a list of the namespaces
144:25 - using the cube CTL get namespaces
144:28 - command
144:30 - let me copy this
144:33 - and these are the namespaces currently
144:37 - created on my machine on my kubernetes
144:40 - cluster so default the cube node Cube
144:44 - public Cube system
144:45 - I can also use the shortcut for
144:49 - namespaces which is just few letters
144:52 - which is a lot faster to type
144:54 - and it will give you the exact same
144:56 - result here
145:00 - let me get a list of the pods running
145:05 - Cube CTL get parts
145:09 - no resources found in default namespace
145:15 - that I'm currently my contacts is using
145:19 - the namespace called default right and
145:23 - there's no pods running in there
145:26 - so what if I want to list the pods that
145:29 - are in the cube system namespace
145:33 - well I can use the same command qctl get
145:35 - pass specifying the namespace switch and
145:39 - specifying the name of the namespace
145:44 - let me copy this
145:48 - and there are many parts running here
145:51 - here I can use also the the shortcut so
145:54 - instead of typing dash dash namespace
145:56 - equal I can just use the shortcut dash n
146:01 - so only one dash here two dashes here
146:04 - and the name of the namespace so let's
146:07 - try this
146:09 - and it works
146:12 - perfect
146:13 - now what if I want to change from the
146:18 - default namespace or the namespace
146:20 - called default to a namespace called
146:23 - Cube system right and then apply all my
146:26 - commands to so all the objects will be
146:29 - created in that namespace well I can do
146:31 - that by using the cube CDL config set
146:35 - contacts dash dash current so here we're
146:39 - modifying the context
146:42 - and passing a switch called namespace
146:45 - and the name of the namespace we want to
146:48 - change
146:49 - so we want to change instead of being
146:52 - the default namespace all the time we
146:54 - want to be in the cube system
146:58 - okay and yeah let's see if uh if we run
147:04 - Cube City I'll get passed if we would
147:06 - get some pods of course because we're
147:10 - currently in that Cube system namespace
147:18 - okay now let's change back to the
147:21 - default namespace
147:26 - and let's get a list of the parts there
147:30 - should be none
147:31 - perfect
147:33 - okay so this is how you uh you change
147:36 - namespaces
147:39 - perfect
147:41 - of course we can create new namespaces
147:44 - by using the cube CTL
147:47 - uh create NS
147:50 - for short for namespace and a namespace
147:52 - name so hello
147:55 - okay uh Cube
147:59 - CTL get NS and my name space here was
148:05 - created seven seconds ago
148:08 - and I can delete a namespace using
148:10 - cubectl delete NS and the namespace name
148:16 - that and hello now
148:19 - a bit of a warning here uh if you have
148:22 - resources under that namespace these
148:26 - resources the buzz the containers
148:28 - whatever running will also be deleted so
148:31 - use this command with caution
148:34 - foreign
148:37 - take a couple of seconds
148:40 - okay let's get the name spaces and our
148:44 - hello namespace is gone
148:48 - [Music]
148:54 - let's now look at the masternode
148:58 - in nowadays the physical or virtual
149:00 - machine
149:01 - a group of nodes formed a cluster
149:05 - the masternode is also called the
149:08 - control plane
149:10 - the kubernetes services and controller
149:13 - are located on the control plane
149:16 - they are also called The Master
149:18 - components
149:20 - and you usually don't run your
149:22 - application containers on the masternode
149:28 - the key value data Store where the state
149:31 - of the cluster is stored
149:33 - the API server is the only component
149:36 - that communicates with its CD
149:41 - let's start with the API server
149:44 - it exposes a res interface and client
149:48 - tools like the kubernetes CLI
149:50 - communicates through that recipe
149:54 - it saves the state in a CD
149:57 - and all client interacts with the API
150:00 - server never directly with the data
150:03 - store
150:06 - hcd is the data store for storing the
150:10 - cluster state
150:11 - it's not a database but a key Value
150:14 - Store
150:16 - is the single source of truths inside
150:19 - kubernetes
150:23 - the cube control manager is the
150:25 - controller of controllers
150:28 - its job is to run the other kubernetes
150:30 - controllers
150:34 - the cloud control manager job is to
150:37 - interact with the cloud providers
150:40 - a check if nodes were created or deleted
150:44 - uh Route traffic create or delete load
150:48 - balancers
150:49 - and interact with the cloud providers
150:51 - storage services
150:55 - the cube scheduler watches for paths
150:58 - that are not created yet
151:00 - and selects a node for them to run on
151:04 - it checks for various rules and then
151:07 - assign the Pod creation to a node
151:12 - finally you can install various add-ons
151:15 - on the masternode
151:17 - these add-ons provide additional
151:19 - functionalities in your kubernetes
151:22 - cluster
151:24 - and this concludes this look at the
151:26 - masternode
151:30 - [Music]
151:35 - let's take a look at the worker nodes
151:38 - a node is a physical or visual machine
151:42 - a group of nodes forms a cluster
151:46 - there's a special node called the
151:48 - masternode sometimes called the control
151:51 - plane
151:52 - where the kubernetes services are
151:54 - installed
151:56 - the nodes running the containers are
151:59 - called the worker nodes
152:02 - when a worker node is added to the
152:04 - cluster
152:05 - some kubernetes services are installed
152:08 - automatically
152:09 - the container runtime
152:11 - the cubelet and the Q proxy
152:15 - these are Services necessary to run pods
152:18 - and they are managed by the master
152:21 - components on the masternode
152:25 - the cubelet manages the pod's life cycle
152:29 - and ensure that the containers described
152:31 - in the Pod specifications are running
152:33 - and are healthy
152:36 - the Q proxy is a network proxy that
152:40 - manages Network rules on nodes
152:43 - all Network traffic go through the cube
152:46 - proxy
152:48 - on each node you will find a container
152:51 - runtime
152:53 - kubernetes supports several container
152:55 - runtimes that implements the kubernetes
152:57 - container runtime interface
152:59 - specification or CRI
153:04 - one thing to note is that for kubernetes
153:07 - version previous to 1.19
153:11 - the Mobi container runtime was installed
153:14 - and was receiving the container runtime
153:16 - interface call through a shim
153:19 - because it did not fully implemented the
153:23 - specifications
153:25 - this is not ideal as it added an extra
153:29 - step
153:30 - starting with kubernetes version 1.19
153:33 - Mobi is no longer installed
153:36 - oh wait a minute
153:38 - Moby not installed that means that my
153:42 - Docker containers will no longer run if
153:45 - the docker container runtime is not
153:46 - installed right
153:48 - well the short answer is that your
153:51 - Docker images will run as is nothing to
153:55 - change its business as usual
153:59 - what change is that what you can do
154:01 - inside the cluster
154:04 - since the docker runtime is no longer
154:07 - installed
154:08 - you no longer can access the docker
154:11 - engine and issue Docker commands
154:13 - directly inside a node
154:17 - you'll have to use another tool called
154:19 - crease CTL but again that's if you SSH
154:23 - into a node and run commands directly on
154:27 - that note
154:28 - something that you don't do usually
154:33 - alright
154:34 - a node pool is a group of virtual
154:37 - machines all with the same size
154:41 - a cluster can have many node pools and
154:44 - each node pool can host virtual Machines
154:47 - of different sizes
154:50 - let's say that we have two node pools in
154:52 - our cluster the first one consists of VM
154:55 - without gpus and the second one with
154:58 - chip use
155:00 - remember that Docker desktop is limited
155:03 - to one node so basically you run the
155:05 - master components and all the
155:07 - application containers on the same node
155:12 - and this concludes this look at the
155:14 - worker nodes
155:18 - [Music]
155:22 - so let's get some information about our
155:25 - nodes I'm going to run a cube CTL get
155:28 - nodes
155:31 - and since I'm running on Docker desktop
155:34 - locally here
155:35 - I have only one node name is Docker
155:40 - desktop status is ready roll is master
155:44 - here's the revision the version number
155:47 - and it was installed 72 days ago all
155:51 - right
155:52 - I can get more information about the uh
155:55 - the node uh since I only have one node I
156:00 - can skip the node name per meter
156:05 - and here's a bunch of information
156:09 - the name the row some labels
156:13 - creation date
156:17 - um what else what else what else what
156:19 - else uh the capacity to CPUs
156:23 - uh number of PODS maximum number of
156:25 - parts that I can run 110
156:29 - the architecture the OS Linux
156:33 - architecture AMD
156:37 - the pods running and the CPU request CPU
156:42 - the Miss memory so all useful
156:45 - information when uh you need to
156:47 - troubleshoot and debug
156:50 - so that's pretty uh pretty interesting
156:52 - now I'm using a locally Docker desktop
156:56 - so I'm limited to one node Let's uh
157:00 - switch should
157:03 - to
157:04 - um My Demo cluster
157:08 - uh running in the cloud
157:12 - so let's do the same thing Cube CTL get
157:15 - notes
157:19 - I have more notes now so I have a
157:23 - cluster in in the cloud running three
157:27 - nodes in a node tool so one two three
157:31 - status ready agent and the version of
157:35 - kubernetes that is running that's
157:37 - interesting let me grab cubicity of the
157:41 - scrap node and let's uh let's say use
157:45 - this one
157:50 - so I can get
157:52 - the same information that I I was
157:55 - getting earlier but this one is running
157:58 - uh in the cloud
158:00 - that's the name
158:03 - role agent agent pool the different
158:06 - labels
158:08 - annotation
158:13 - number CPUs memory
158:18 - System Info it's running Ubuntu Linux
158:23 - AMD 64.
158:26 - uh what else will end the different
158:28 - parts
158:30 - that are running on this uh node
158:37 - [Music]
158:42 - let's take a look at parts
158:44 - so what are pots
158:47 - a pod is the smallest unit of work in
158:50 - kubernetes
158:51 - it encapsulate an application container
158:54 - and represent a unit of deployment
158:58 - paths can run one or more containers
159:02 - inside a pod containers shame the same
159:05 - IP address space and volumes
159:08 - and they communicate with each other
159:10 - using local laws that's inside the pod
159:15 - s are ephemeral deploying a pod is in
159:18 - atomic operation so it succeeds or not
159:23 - if a pod crashes it is replaced by a
159:27 - brand new one with a shiny new IP
159:29 - address
159:31 - you don't update the Pod that is
159:33 - currently running
159:34 - you create an updated version deleting
159:37 - the first one and deploying the new one
159:42 - you scale by adding more pods not more
159:46 - containers inside a pod
159:52 - I use this analogy in a previous lecture
159:55 - pods are like cattle there are ephemeras
159:58 - and you just replace them
160:02 - a note can run many pods
160:05 - and a part can run one or more
160:08 - containers
160:11 - if a pod runs multiple containers
160:14 - there's usually one that is the main
160:16 - worker where your application logic is
160:19 - located
160:21 - and the other ones are helper containers
160:23 - that provide services to the main worker
160:26 - we will come back to this concept in
160:29 - another lecture
160:32 - and this concludes this look at pods
160:37 - [Music]
160:42 - let's take a look at the Pod life cycle
160:45 - we'll start with the creation life cycle
160:48 - so when you issue a cube CTL create
160:51 - command to deploy a pod in your cluster
160:55 - the CLI sends the information to the API
160:58 - server
160:59 - and that information will be written
161:01 - into a CD
161:05 - the scheduler will wash for this type of
161:08 - the information
161:09 - look at the notes and find one where to
161:13 - schedule the Pod and write that
161:15 - information of course in a CD
161:18 - the cubelet running on the Node will
161:20 - watch for that information
161:23 - and issue a command to create an
161:25 - instance of the container inside a pod
161:28 - and finally the status will be written
161:31 - in a CD
161:34 - one thing to notice is that each time an
161:37 - operation is taking place inside the
161:39 - cluster the state is written in its CD
161:42 - so SCD is the single source of Truth in
161:46 - the cluster
161:48 - all right so let's take a look at the
161:51 - deletion life cycle now
161:54 - when you issue a cube City delete
161:56 - command to delete a pod from your
161:59 - cluster the CLI sends deformation of
162:02 - course to the API server
162:05 - that information will be written in SCD
162:10 - and notice that the grace period of 30
162:13 - seconds will be will be added
162:18 - so the cubelet picks that information
162:20 - and sends a terminate signal to the
162:23 - container
162:24 - if the container hangs it is killed
162:27 - after the 32nd grace period
162:30 - and finally the state is stored in a CD
162:38 - The Path State will give you a high
162:40 - level summary of Where the Pod is in its
162:43 - life cycle
162:46 - pending mean that the Pod is scheduled
162:48 - for creation
162:50 - but is not created yet
162:52 - if you run out of resources in your
162:55 - cluster kubernetes may not be able to
162:58 - create new paths and if this happens
163:01 - the parts will be in the pending state
163:07 - running means that the part is currently
163:09 - running
163:11 - succeeded means that the code exited
163:14 - without any errors
163:17 - fail means that the code inside the the
163:20 - Pod exited with the non-zero status
163:23 - so some kind of error occurred
163:27 - unknown means that kubernetes can
163:29 - communicate with the pod
163:32 - and finally crash loop back off oh I
163:36 - love this in his state name so krashu
163:40 - backup means that the Pod started then
163:43 - crash kubernetes started it again and
163:47 - then the Pod crashed again
163:48 - so kubernetes say okay hold on I'm
163:51 - stopping here
163:53 - so we'll take a look uh in the official
163:55 - letter of where to look for these states
164:01 - and this concludes this look at the bud
164:03 - life cycles
164:07 - [Music]
164:12 - let's see how to define and run pods
164:16 - to define a pod the declarative way you
164:19 - create a yaml file
164:22 - specifying but as the kind
164:25 - that's the type of resource you want to
164:27 - create
164:30 - you specify an image location in this
164:33 - case the nginx image will be pulled from
164:35 - Docker up that's the default container
164:38 - registry
164:40 - you set the port that the container will
164:43 - listen on
164:45 - you can add labels they are used to
164:48 - identify describe and group related sets
164:52 - of objects and resources
164:55 - you can set environment variables
164:58 - directly here
164:59 - might not be the best idea to place
165:01 - configuration values
165:04 - in a later lecture we'll see how we can
165:07 - externalize that by the use of a config
165:10 - map object
165:14 - you can even specify a command to run
165:17 - when the container starts
165:23 - if you have created a yaml file with
165:26 - your pod definition
165:28 - you use
165:30 - cubectlcreate-f specifying the yaml file
165:34 - location and name and this will create
165:37 - your pod the declarative way
165:40 - now if you don't have a yellow file and
165:43 - you just want to run a pod the
165:46 - imperative way you use qctl run you
165:50 - specify a name for your running pod Dash
165:53 - Dash image the image name
165:56 - you can specify a program to run in this
166:00 - case it's sh and dash C you can specify
166:04 - a parameter that you want to pass to the
166:08 - program
166:11 - qctl get parts will list all the pods
166:15 - that are currently running
166:17 - Dash o wide will get you the same
166:20 - formation but uh with a few columns more
166:25 - cubicity Of The Scribe part and the Pod
166:28 - name will show the pad information
166:34 - you can use qctl get part with the Pod
166:38 - name
166:39 - Dash offer output
166:41 - uh in yaml format and you can pipe that
166:44 - to a file name so this is pretty cool
166:47 - because
166:47 - it will extract the Pod definition in
166:51 - yaml and save it to a file
166:54 - so if in case you lost that yaml file
166:58 - that was used to create a pod well you
167:02 - can recreate it quite easily
167:05 - qctl exec Dash it specifying the Pod
167:09 - name and the program to run will get you
167:14 - inside that part in the interactive mode
167:19 - you delete a pod using cubectl delete
167:22 - Dash F specifying the yaml file or if
167:26 - you don't have the yaml file simply use
167:29 - cubectl delete pod and the path name
167:33 - that will result in the same as the
167:36 - previous command
167:41 - and this concludes this look at how to
167:44 - define and run pods
167:48 - [Music]
167:52 - in this lab we will run our first pods
167:56 - we'll start by using the imperative way
167:59 - we'll use the cube CTL run command
168:02 - specifying the image that we want to run
168:05 - in our case we want to run an nginx web
168:08 - server
168:09 - and we'll specify a name to the running
168:13 - instance
168:14 - my nginx
168:17 - let's run this
168:21 - pod my nginx created
168:24 - let's get a list of the running pods by
168:28 - using the cube CTL get pods command
168:32 - I have one pod running my nginx ready
168:36 - one of ones that is running it was
168:38 - created 11 seconds ago
168:41 - let's try to get more information by
168:44 - adding the dash o y per meter
168:48 - it's the same command but we'll get a
168:51 - little bit more information like the IP
168:53 - address of the Pod and the node where
168:56 - the Pod is currently running
168:59 - awesome
169:01 - if you want more information
169:03 - we'll use the cube CTL describe command
169:07 - with the type of object
169:10 - and the name of the running instance so
169:13 - cubectl describe the type pod and the
169:18 - name my nginx
169:22 - all right tons of cool information here
169:29 - the name of the object my internet the
169:32 - namespace where it's running
169:34 - uh the node where it's running start
169:37 - time any labels annotation the IP
169:40 - address information about the container
169:44 - any volumes restarts that happen
169:49 - and any volumes here
169:53 - and at the end we get the list of events
169:56 - that happened when the Pod was created
170:00 - it was first scheduled
170:03 - and then the image was pulled
170:07 - and then the image was successfully
170:10 - pulled
170:12 - and created
170:14 - and then started
170:18 - the cube CDL describe command should be
170:20 - the first thing you try when doing some
170:23 - troubleshooting here you will likely
170:26 - have some very useful information uh in
170:30 - case a pod doesn't start maybe
170:32 - the image is not available maybe the
170:36 - image didn't start correctly also Cube
170:40 - CDL describe command that's very very
170:42 - useful
170:45 - let's now delete our pod by using the
170:48 - qctl delete the object type pod and the
170:52 - name of the Running part
170:59 - should get my command line back in a few
171:03 - seconds
171:08 - all right
171:10 - let's now run a second pod this time a
171:14 - busy box image
171:16 - Cube CTL run the image busy box and the
171:21 - name of the running instance my box but
171:23 - we're adding extra parameters here Dash
171:26 - it dash dash the program we want to run
171:30 - this will open a session
171:33 - inside our running pod
171:39 - and look at the command prompt it's
171:41 - changed to a pound sign this means that
171:44 - now I can type commands that are running
171:47 - inside the pod LS for listing the
171:51 - folders and files I can run a
171:56 - base64 command here I can encode a
172:00 - string here and that happened inside the
172:04 - running container inside the Pod pretty
172:08 - cool
172:09 - to stop the session simply type exit
172:13 - and the this ends the session
172:17 - okay let's do a little bit of cleanup
172:21 - um in the busy box case uh it takes up
172:24 - to 30 seconds to uh to delete so we have
172:28 - two options here we can run the cube CTL
172:31 - delete the object type and the running
172:34 - AdSense and to get back our Command
172:36 - Prompt right away we can use the dash
172:40 - dash weight equal false per meter
172:42 - or if we simply want to kill the Pod
172:46 - brutally we use dash dash force and with
172:50 - a grace period of zero seconds
172:53 - let's run this
172:56 - and this will kill the Pod brutally
173:01 - all right
173:04 - let's now create a pod the declarative
173:07 - Way by using a yaml file
173:11 - we have a yaml file here called
173:12 - myhab.yamo
173:14 - the kind is a pod we give it a name my
173:18 - app dashboard
173:20 - a few labels
173:22 - the image we want to run
173:26 - any limits here four degree sources in
173:30 - CPU and memory
173:32 - the port that the container is listening
173:35 - to and some environment variable we're
173:38 - defining an environment variable called
173:40 - dbcon that will have a value of my
173:42 - connection string here
173:45 - awesome
173:46 - let's use the qctl create Dash F4 file
173:51 - and the name of our yaml file
173:54 - pod my
173:56 - app.pod created perfect let's run the
174:01 - qcl get pods command to get a list of
174:05 - our paths it's running
174:08 - perfect we can also describe
174:12 - our pod
174:15 - again
174:17 - same information that we saw earlier
174:21 - right the default names the namespace
174:27 - and here look we have our environment
174:30 - variable
174:33 - and the same events that we saw earlier
174:38 - previously we used the cube CTL run
174:43 - command with the dash it parameter to
174:48 - open a session to our BusyBox container
174:51 - now what is the Pod is currently running
174:53 - well we can use the cube CTL exec
174:57 - command with the same it
175:00 - per meter here
175:02 - we specify the Pod name and the program
175:05 - we want to run
175:08 - that will open a session to a part that
175:11 - is currently running
175:13 - okay let's output the
175:18 - dbcon environment variable my connection
175:21 - string awesome that worked
175:25 - let's exit or stop our session
175:29 - and this time we'll use the qctl delete
175:32 - command specifying our yaml file to
175:36 - delete our pod
175:44 - [Music]
175:49 - let's take a look at init containers
175:52 - let's say that your app has a dependency
175:55 - on something can be a database an API or
176:00 - some config files
176:02 - you want to initialize or validate that
176:05 - these exist before launching the app
176:08 - but you don't want to clutter your main
176:11 - logic with this type of infrastructure
176:13 - code
176:14 - so what do you do
176:17 - you can use an init container that lets
176:20 - you initialize a pod before an
176:23 - application container runs
176:25 - let's say that for the app container to
176:28 - run it requires a series of
176:30 - configuration files
176:32 - in the Pod definition you define a
176:34 - container that will run first this is
176:38 - the init container
176:40 - upon completion kubernetes will start
176:43 - the app container
176:45 - this is a great pattern for applications
176:47 - that have dependencies the init
176:49 - container job can be as simple as
176:52 - validating that a service or a database
176:55 - is up and running
176:56 - this keeps the infrastructure code out
176:59 - of the main logic
177:03 - init containers always run to completion
177:07 - you can have more than one and each bus
177:11 - complete successfully before the next
177:13 - ones starts
177:15 - if it fails the cubelet repeatedly
177:18 - restarts it until it succeeds unless
177:22 - it's restart policy is set to never
177:27 - probe are not supported as ended
177:29 - containers run to completion
177:33 - in this bad definition file we have the
177:36 - main application located in the
177:39 - containers section in green
177:41 - and the init containers in the init
177:43 - container section in yellow
177:45 - as you can see here we have two init
177:48 - containers they both watch for services
177:51 - to be up and running
177:54 - so the first one we will run to
177:56 - completion
177:58 - then the second one
178:01 - and finally kubernetes will start the
178:04 - app container
178:06 - and this concludes this look at init
178:09 - containers
178:12 - [Music]
178:16 - in this lab we will use an init
178:19 - container to modify the home page of an
178:23 - nginx container
178:26 - let's take a look at our yaml file
178:30 - it's manifest for a pod
178:34 - um we have two sections here containers
178:37 - and any containers let's first take a
178:39 - look at init containers
178:41 - we will use a BusyBox image
178:44 - and we'll run this command wget
178:48 - and will it that
178:51 - website right here and that website
178:55 - it's the ohm of the first website the
178:59 - home page of the first website is pretty
179:01 - simple just a few lines of HTML
179:05 - all right
179:07 - and we will save that HTML into a file
179:12 - called index.html into a volume called
179:16 - work directory
179:20 - and the nginx image
179:24 - will map that value
179:28 - and we'll serve that index.html page as
179:32 - its default web page
179:34 - so basically we're initializing our
179:38 - nginx
179:39 - container by using an init container
179:43 - here
179:44 - awesome
179:47 - let's open a terminal
179:54 - and let's deploy our application
179:58 - Cube CTL apply Dash F the name of our
180:02 - yellow file
180:04 - let's wait till the nginx image is up
180:11 - so if I do a doctor PS
180:16 - yet my nginx container is a
180:20 - oh let's uh open a session right into
180:23 - that nginx container
180:28 - and let's try to hit the default web
180:31 - page curl localhost
180:35 - and yep that worked we're serving the
180:39 - default uh web page of the CERN website
180:43 - here
180:44 - pretty cool
180:47 - let's type exit
180:50 - and let's
180:52 - do our cleanup
180:54 - cubesatel delete and our yellow file
181:01 - [Music]
181:06 - let's now look at selectors
181:10 - when defining kubernetes resources you
181:14 - can use labels
181:16 - these labels allow you to identify
181:19 - describe and group related set of
181:22 - objects or resources
181:24 - they are simply key value pairs that you
181:28 - define yourself
181:32 - in this part definition we see two
181:34 - labels
181:36 - app
181:37 - with the with a value of my app and type
181:41 - with a value of front end
181:45 - note that the app and type keys are not
181:49 - something defined by kubernetes they are
181:52 - defined by you for your application
181:57 - okay but what does labels have to do
182:01 - with selectors
182:04 - well selectors use labels to filter or
182:08 - select objects
182:11 - here we see a selector in this pod
182:13 - definition
182:15 - the selector type is node selector
182:19 - and the key value pair is this type
182:22 - equals super fast
182:25 - okay
182:26 - but how does that work
182:30 - here we have a path definition with the
182:33 - node selector set to this type equals
182:36 - super fast
182:39 - we're telling kubernetes that we want to
182:41 - run this bar on the Node that has a
182:45 - label set to this type equals super fast
182:52 - note a does have such label so
182:55 - kubernetes will schedule the Pod
182:57 - creation on that note
183:02 - the simplest way to picture what
183:04 - selectors do is by comparing them with a
183:07 - SQL query
183:09 - it would be something like select star
183:12 - from nodes where this type equals super
183:15 - fast
183:18 - and this concludes this look at the
183:20 - selectors concept
183:24 - [Music]
183:29 - in this lab we will test the selector
183:31 - concept
183:33 - we have two yaml files here the first
183:36 - one
183:37 - it contains the definition of a pod
183:40 - we will run an nginx web server
183:43 - listening on Port 80. and for the sector
183:47 - concept what is enter is is this section
183:50 - the label section so we have two labels
183:54 - defined here
183:55 - app set to my app
183:58 - type set to front dash n
184:03 - let's take a look at the second yaml
184:05 - file that's the service of name my
184:09 - service it's listening on Port 80 and
184:13 - targeting or redirecting the traffic to
184:16 - Port 80 that the Pod is listing on
184:20 - and let's look at this section selector
184:23 - app my app type front dash n so these
184:28 - are the same labels that we Define
184:31 - right here in our pod definition
184:36 - all right
184:37 - Let's test this concept
184:40 - I'll open a terminal
184:44 - and it will first deploy r
184:47 - by using cubesatel apply Dash F and the
184:50 - name of the yaml file
184:53 - foreign let's now deploy
184:57 - the service
184:59 - Cube CDL apply Dash f myservice.yaml
185:03 - all right how do we know that the
185:06 - service is connected to the Pod how do
185:10 - we know that the selection was
185:12 - successfully made
185:14 - to find that let's get the Pod IP
185:19 - address by using qcdl get pod Dash o
185:23 - wide
185:25 - and let's look at the IP address here
185:29 - 10.1.9.31 all right
185:33 - let's run this command cubectl get EP EB
185:36 - is short for endpoint and here's the
185:38 - name of our service my service so let's
185:40 - run this
185:44 - all right
185:45 - you see D in the endpoints column
185:50 - 10.1.9.31 that's the same IP address
185:54 - let's now try to port forward
185:57 - to that service
186:01 - okay we get an immediate result here and
186:05 - let's uh go to localhost
186:08 - port 8080 and that worked
186:11 - perfect let's go back here to our
186:14 - terminal and let's type Ctrl C to stop
186:18 - the port forward
186:20 - now let's try to break things a little
186:24 - bit
186:26 - we'll open the my app.yaml file and
186:30 - let's change the one of the labels so uh
186:34 - app will set it to my app to control s
186:38 - let's save that file
186:41 - and let's redeploy the file again the
186:46 - Pod again
186:47 - by using cubectl apply Dash F and the
186:50 - name of the yaml file
186:53 - all right
186:54 - let's check the endpoint again
186:58 - is it still working
187:01 - huh
187:02 - look at the endpoint colon none
187:07 - and if we try to pour forward again
187:13 - there's no immediate results so that
187:16 - doesn't work
187:18 - so disproved that both labels must match
187:23 - here the selector here boat labels must
187:27 - match the labels in in the Pod
187:30 - definition
187:31 - so that the selection can work
187:34 - all right
187:36 - let's type Ctrl C to stop this and let's
187:40 - do our little cleanup by deleting the
187:44 - service and then the pod
187:51 - thank you
187:56 - let's take a look at multi-container
187:58 - pods
188:00 - we sign a previous lecture that pass can
188:02 - run one or more containers
188:05 - that there's always a main worker and
188:08 - that the other containers are providing
188:10 - services to that main worker
188:12 - like saving data to a database
188:16 - and writing log files
188:18 - there are scenarios while
188:20 - multi-container Parts make sense and
188:22 - they are well documented in a series of
188:24 - patterns we will take a look at some of
188:26 - them in the next few slides
188:30 - with these sidecar pattern the helper
188:32 - container provides extra functionalities
188:34 - to the main worker
188:36 - let's say that our app writes some log
188:39 - files inside the pod
188:42 - the sidecar can copy these log files to
188:46 - some purchasing storage offered by the
188:48 - cloud provider
188:49 - this way the application code located
188:53 - inside the main worker is not cluttered
188:56 - with infrastructure code
188:58 - that code is located in the helper
189:01 - container
189:02 - and if you move from one cloud provider
189:04 - to another one well you simply replace
189:07 - or update that L per code
189:10 - this keeps your application code super
189:13 - clean
189:15 - our next pattern is the adapter let's
189:18 - say that our main worker outputs some
189:21 - complex monitoring information
189:24 - that the monitoring surface of our cloud
189:27 - provider cannot understand
189:29 - the adapter role would be to connect to
189:32 - the main worker
189:34 - simplify the data for the monitoring
189:37 - service
189:38 - again the code specific to the cloud
189:41 - provider service is located inside the
189:43 - helper container
189:46 - the Ambassador pattern is another type
189:51 - of the men of the middle role let's say
189:54 - that our application code needs to write
189:56 - to some nosql database but the code has
190:00 - no clue on how to do that
190:02 - no problem you can send that data to the
190:06 - Ambassador that in turn will write the
190:10 - data to the nosql data store
190:13 - the code specific to the data store
190:16 - service is located inside the helper
190:19 - container
190:21 - if you're curious about these patterns I
190:24 - suggest you get a copy of the design
190:26 - distributed systems book the other is
190:29 - Brennan Burns Brendan worked at the
190:32 - Google where he co-created kubernetes
190:35 - he now works at Microsoft
190:39 - so how can you define multi-container
190:41 - paths well if you remember the lecture
190:44 - about yellow files
190:46 - you see that the container sections is
190:49 - actually a list
190:51 - this means that you can Define multiple
190:53 - containers
190:55 - are added in yellow you see container
190:57 - number one and in green container number
191:00 - two and when you create a pod both
191:03 - containers will be created at the same
191:05 - time pretty cool
191:08 - here's a quick cheat sheet for
191:11 - multi-container parts
191:13 - so after you created your yaml file you
191:17 - simply use
191:18 - cubectlcreate-f specifying the yaml file
191:21 - name so same thing as creating a single
191:23 - container pod
191:26 - if you want to exec into one of the
191:28 - container you simply use Cube CTL exec
191:31 - Dash it the part name and specifying
191:36 - Dash C and the container name this way
191:39 - you can jump into one of the containers
191:43 - running inside the pod
191:46 - you get the logs for a container using
191:49 - cubectl logs the Pod name Dash C the
191:53 - container name
191:58 - and this concludes this look at
191:59 - multi-container pods
192:09 - let's take a look at some networking
192:10 - Concepts
192:13 - kubernetes is what we call a flat
192:15 - Network as a result most resources
192:18 - inside a cluster can see each other
192:21 - all containers within a pod can
192:23 - communicate with each other
192:25 - all pods can communicate with each other
192:28 - all nodes can communicate with all pods
192:31 - and all nodes
192:33 - paws are giving an ephemeral IP address
192:36 - while services are giving a persistent
192:38 - IP so this is quite important we'll come
192:41 - back to that later
192:43 - let's illustrate the cluster Network in
192:46 - blue here
192:49 - each pod gets an IP address and the
192:53 - containers inside a pod share the same
192:56 - address space
192:58 - the containers inside the same pod share
193:01 - the same IP address but each must be
193:04 - assigned a different port number
193:08 - they can communicate inside the Pod
193:10 - using localhost and the port number
193:16 - they can also share the shared volumes
193:22 - what about communication between pods
193:25 - can the container on the right talk to
193:29 - The Container inside the pod on the left
193:31 - using localhost
193:34 - no they can't
193:35 - they need to go through a service that
193:39 - will front the network traffic for the
193:42 - pod
193:44 - for external access to the cluster
193:46 - traffic goes through a load balancer
193:48 - service offered by the cloud provider
193:52 - in future lectures we'll look at
193:55 - different type of services that we can
193:57 - use in kubernetes
194:00 - and this concludes this quick look at
194:02 - networking Concepts
194:06 - thank you
194:12 - let's create a multi-container pod using
194:14 - a yaml file let's take a look at the
194:17 - yaml file
194:19 - so the kind is part
194:21 - the name will be two dash containers
194:25 - and in the container section we're
194:28 - defining two containers so the first one
194:30 - we'll use the nginx image and we'll name
194:34 - it my nginx and that web server will
194:39 - listen on Port 80.
194:42 - and we're defining a second container
194:45 - using the BusyBox image and we'll name
194:48 - it my box
194:50 - this one will listen on Port 81.
194:54 - and for the BusyBox container to stay up
194:57 - and running we need to tell it to to
195:00 - stay up by issuing a sleep command so
195:04 - it'll stay for an hour
195:06 - what we'll try to achieve is open a
195:09 - session on the BusyBox container and try
195:13 - to hit the nginx container so the the
195:17 - default web page served by the nginx
195:20 - container
195:22 - all right
195:24 - let's try to create our
195:29 - pod here using a cube CTL create and the
195:34 - name of our yaml file
195:36 - okay pad two dash containers created so
195:41 - let's try to see uh if they're running
195:46 - all right two out of two because we have
195:50 - two uh containers running in that pod
195:52 - status running 10 seconds uh the IP
195:56 - address so there's one IP address
195:57 - assigned to the to the pub
196:00 - and denote okay let's try to get a
196:03 - little bit more information by using the
196:06 - cube Ctrl describe pod and the name of
196:08 - the pod
196:10 - let's scroll up a little bit
196:14 - so the name is two containers then space
196:17 - is the default one the node where it's
196:21 - running uh the containers so the first
196:25 - one is my nginx which using a the nginx
196:29 - image
196:32 - we have information about the limits
196:34 - that we set earlier in the ml file my
196:38 - box using a BusyBox image listing on
196:42 - Port 81
196:43 - right there's a that sleep command
196:48 - so useful information here and also look
196:53 - at the events now
196:55 - so earlier when we add only one
196:58 - container inside a pod we would get just
197:01 - one set of events for polling and
197:05 - creating the container now we have two
197:08 - first one is the Pod was scheduled and
197:12 - the second one here is on the second
197:15 - land line
197:18 - um the nginx image was in the polling
197:20 - state it was Paul created started and
197:23 - then the busy box container was pulled
197:27 - and created
197:28 - okay
197:30 - perfect now let's try to open a session
197:33 - inside our BusyBox container so we'll
197:36 - use Cube CTL exec Dash it okay here's
197:41 - the trick now we need to specify the Pod
197:44 - name and then the container name that we
197:50 - want to to connect to
197:51 - and then the the program that we want to
197:55 - run that's the trick when you have
197:56 - multiple containers you need to specify
197:59 - the Pod name and then the container name
198:04 - all right let's try to do that
198:07 - oh looks good at last yeah
198:11 - it worked okay now we'll use wget to try
198:16 - to hit that default page serve ID nginx
198:21 - web server
198:23 - wget with the this flag and we'll call
198:27 - localhost
198:30 - all right welcome to nginx so that word
198:35 - now Outpost for a couple of seconds and
198:39 - why I'm gonna ask you why did that work
198:44 - why calling localhost worked
198:51 - so that work because let's go back to
198:55 - our yaml file
198:58 - the nginx container is listening on Port
199:01 - 80. right we don't have to specify a
199:06 - import number here
199:09 - if the nginx container would have
199:11 - listened to something different we would
199:13 - have to specify Here Local O's colon and
199:17 - the port number
199:19 - all right let's try to exit this
199:23 - perfect
199:25 - let's now delete our pod using Cube CTL
199:28 - delete and the name of the ml file and
199:30 - by using the force and grease grace
199:33 - period equals zero flag
199:35 - that will kill both containers
199:39 - immediately
199:42 - perfect
199:46 - [Music]
199:51 - this is a super short lecture just to
199:54 - introduce you to the concept of a
199:56 - workload
199:58 - so a workload is an application running
200:00 - on kubernetes
200:02 - all containers running in a kubernetes
200:04 - cluster must run in a workload
200:09 - the Pod is the atomic workload it
200:12 - represents a set of running containers
200:14 - and all workloads will create pods
200:18 - the replica set and the deployment will
200:20 - provide extra functionalities on top of
200:23 - the pod
200:24 - like the ability to Define how many
200:26 - instances of a part we want
200:30 - the stateful set and the Daemon set are
200:33 - specialized workloads and finally the
200:36 - job and crunch up offer tasks that run
200:39 - to completion
200:41 - these are short-lived tasks
200:45 - we will see each of these workloads in
200:48 - detail in the following lectures
200:51 - and this concludes this super short
200:53 - lecture on the workload concept
200:58 - [Music]
201:03 - let's take a look at replica set
201:07 - the replica set primary job is to manage
201:10 - d-pad replicas making sure that the
201:13 - desired number of instances are running
201:17 - this provides the self-e-link
201:19 - capabilities in kubernetes
201:22 - while you can create replica set the
201:26 - recommended workload is the deployment
201:28 - now welcome back to that in a moment
201:32 - so let's say that you want three
201:34 - instances of a pod to run at all time
201:38 - you create a replica set and specify
201:40 - that you want three replicas
201:43 - if for some reason one bird crashes
201:47 - kubernetes will replace it automatically
201:50 - without any human intervention I'm
201:53 - pretty cool eh
201:55 - let's see how to define a Rebecca set by
201:58 - starting with a pod
202:01 - we take the Pod definition except for
202:03 - the API version and kind
202:06 - and we place these values in the
202:09 - template section of the replica set
202:12 - the final result is a replica set yaml
202:15 - file
202:17 - so basically in the section highlighted
202:20 - in green
202:22 - you will find values specific to replica
202:24 - set and under the template section the
202:28 - values that Define the Pod you want to
202:30 - run
202:31 - here you set the desired number of
202:34 - instances with the replicas property
202:39 - again while you can create replica sets
202:41 - the recommended workload to use is the
202:44 - deployment because it provides extra
202:47 - functionalities on top of the replica
202:49 - set
202:51 - so why bother and learn about replica
202:54 - sets
202:55 - well in the deployment lecture you'll
202:58 - see that when you create a deployment
203:00 - that will also create a replica set in
203:03 - the background
203:04 - that's why it's important to learn about
203:06 - the replica sets functionalities
203:11 - here's a cheat sheet for replicasets
203:13 - command
203:15 - you create one using cubectl apply Dash
203:19 - F and the yabl file
203:24 - you get a list of the replicases by
203:26 - using cubectl get RS
203:31 - you get some information
203:33 - about the replica set by using Cube CTL
203:36 - describe RS and the replica set name
203:40 - you delete one using Cube CTL delete
203:43 - if you have the yaml file you specify
203:46 - Dash F and the yaml file name or if you
203:49 - don't have the yaml file simply by using
203:53 - the replica set name using cubectl
203:55 - delete RS and the replica set name
204:00 - and this concludes this look at Olympic
204:03 - assets
204:06 - [Music]
204:11 - let's create three instance of energy
204:14 - next container using the replica set
204:17 - template
204:19 - so let's take a look at our yaml file
204:22 - the type of object we want to create is
204:25 - the replica set
204:27 - this will be the name of the replica set
204:30 - rs-example
204:33 - we want three replicas running uh at the
204:37 - same time
204:39 - and uh we want three replicas of this
204:43 - containers
204:44 - uh name will be nginx and the image
204:47 - nginx uh colon Alpine so it's a smaller
204:52 - version and we Define the resources and
204:56 - also the ports that each is listening on
205:01 - all right
205:02 - let's try to uh create that so Cube CTL
205:07 - apply or create Dash F and the name of
205:11 - our EML file
205:15 - okay we'll pick a set created
205:18 - let's take a look at our running pods
205:23 - and have three pods running okay and
205:28 - look at the names that were
205:30 - um that were assigned to the each of
205:32 - these spots RS Dash example and then is
205:36 - some kind of a unique number here
205:41 - each one must have a different or unique
205:45 - name so each one is ready running and
205:48 - look here each one has a different IP
205:51 - address
205:52 - perfect so let's take a look at our
205:56 - replica sets that we've created so Cube
205:59 - CDL get RS
206:01 - so there's one
206:03 - rs-example
206:06 - um three desired three current three
206:09 - ready
206:10 - everything is green everything's okay
206:14 - Let's uh now describe our replica set so
206:20 - Cube CTL describe RS for replica set and
206:23 - the name of our replica set
206:25 - so let me paste that
206:29 - right let's scroll up a little bit
206:32 - so the name is RS example uh it's
206:36 - running in the default namespace
206:40 - um any labels or annotations are listed
206:43 - here number of replicas that we set so
206:46 - three current three desired and the the
206:49 - current part the status S3 running
206:52 - and zero failed
206:56 - to The Container information so nginx we
206:59 - want to run the nginx Alpine image the
207:02 - sync on Port 80 the limits and so on and
207:05 - this these are the events that were
207:09 - raised here so each pod was successfully
207:13 - created here
207:17 - all right so last thing we need to
207:22 - delete what created using qctl delete
207:25 - Dash F and the name of the EML file
207:31 - thank you
207:36 - let's take a look at deployments
207:40 - we'll start by comparing pods and
207:42 - deployments
207:44 - Buzz don't self-heal meaning that if a
207:47 - pod dies kubernetes will not replace it
207:50 - automatically
207:52 - using a pod definition only one instance
207:55 - can be created and you can't update and
207:58 - roll back pods
208:01 - deployment can
208:04 - a deployment manages a single part
208:07 - template so you create one for each
208:10 - microservice you want to run
208:13 - when creating a deployment this will
208:15 - also create a replica set in the
208:17 - background
208:18 - but while you can see it
208:21 - you don't interact with the replica set
208:23 - directly you let the deployment manage
208:26 - that for you
208:29 - to summarize the replica set provides
208:32 - the self-ealing and scaling
208:34 - functionalities while the deployment
208:37 - provides updates and rollback
208:39 - functionalities
208:43 - let's take a look at the deployment
208:45 - definition
208:46 - you define the desired number of
208:48 - instances with the replicas property
208:52 - this will be used by the underlying
208:54 - replica set
208:56 - you set the number of iteration you need
208:59 - to keep using the revision history limit
209:01 - property
209:04 - and you set the update strategy in the
209:06 - strategy section
209:08 - you can set the strategy type to a
209:11 - rolling update this way kubernetes will
209:14 - cycle when updating the pods
209:16 - the other strategy is recreate
209:19 - kubernetes will take all existing pods
209:22 - down before creating the new ones
209:26 - we'll have a dedicated lecture on this
209:29 - topic later on
209:32 - like a replica set we start with a pod
209:35 - definition
209:36 - and we start the metadata section in the
209:40 - template section of the deployment
209:48 - the final deployment definition looks
209:51 - like this
209:52 - highlighted in green we see the
209:54 - properties specific to the deployment
209:56 - and in yellow the ones defining the Pod
210:00 - we want to run
210:03 - here's a cheat sheet for deployments
210:05 - command
210:08 - so if you don't want to use a yaml file
210:12 - you can create a deployment using the
210:16 - imperative way so you use Cube CTL
210:18 - create deploy you specify a name
210:22 - then with the image property you you
210:25 - specify the image
210:27 - replicas the number of replicas you want
210:30 - to run and you can specify also other
210:33 - properties like the port number that the
210:37 - paths will listen on
210:39 - if you have a yaml file well you simply
210:42 - use Cube CTL apply Dash F and the yaml
210:45 - file
210:46 - you get a list of the deployments using
210:50 - cubectl get deploy
210:52 - and you get the deployment info using
210:55 - cubectl describe deploy and the
210:58 - deployment name
211:00 - since a deployment will create also a
211:03 - replica set you can get the list of the
211:06 - replica set using cubectl get RS and you
211:11 - delete a deployment by using a EML file
211:13 - so Cube CTL delete Dash F the name of
211:16 - the yaml file or if you don't have it
211:19 - simply use Cube CTL delete deploy and
211:22 - the deployment name
211:26 - and this concludes this look at
211:29 - deployments
211:30 - thank you
211:32 - [Music]
211:37 - let's use the deployment template to
211:39 - create three instances of an nginx
211:43 - container
211:44 - so let's take a look at the EML file
211:49 - this time we want to use the deployment
211:52 - Kai
211:53 - type of object we want to create is the
211:55 - deployment
211:56 - we name it deploy example we want three
212:00 - instances three replicas
212:03 - and we want to keep three versions three
212:06 - replica sets versions in the history in
212:09 - site kubernetes
212:12 - I know if you scroll down a little bit
212:13 - we see that we want to run an nginx uh
212:16 - the Alpine version because it's a little
212:18 - bit smaller and we name it nginx we set
212:23 - some resource limits and each pod will
212:26 - listen on Port 80. all right quite
212:30 - similar to the replica set template that
212:32 - we saw earlier except maybe for uh this
212:38 - parameter here
212:40 - all right
212:41 - now let's create our deployment using
212:45 - cubesatel apply or create Dash F deploy
212:49 - example
212:51 - and let's get a list of the pods that
212:55 - are currently running Cube CTL getbots
212:58 - Dash o wide
213:00 - and yes we have three parts three lines
213:04 - here so look at the names uh given to
213:08 - each bot so deploy example that's the
213:11 - name of the deployment object
213:14 - the the name of the deployment object
213:16 - Dash something unique so to make sure
213:19 - that each part has a unique thing
213:21 - kubernetes as a random number like this
213:26 - only one container is running inside
213:28 - each bot uh it's in running State each
213:32 - pod gets its own IP address
213:35 - so that's perfect
213:37 - okay let's now try to describe our pods
213:43 - so we can use the cube CDL describe pod
213:47 - and deploy example name but we just saw
213:52 - that the the names work a little bit
213:56 - different let's see if this works
213:59 - yep it worked
214:02 - it worked because uh the name is share
214:04 - across uh these three parts and so we
214:09 - get information about each one of these
214:12 - let's say I just need the information
214:14 - about that particular part I can use
214:16 - that name uh the unique name also so
214:19 - instead of deploy example I'm gonna use
214:23 - the full name
214:25 - and there it is f uh the information
214:29 - about that pod
214:32 - just that button
214:33 - all right
214:35 - let's now get some information about the
214:38 - deployment
214:40 - currently uh inside my cluster so
214:43 - cubesatel get deploy I have only one
214:48 - deploy Dash example three out of three
214:50 - ready up to date available so
214:53 - everything's uh looking good
214:56 - and I can describe my deployment using
215:00 - cubectl describe the name of the the
215:03 - type of the object and then its name
215:07 - all right so let's go a little bit we
215:11 - have the name the namespace where it's
215:13 - running any labels annotations number of
215:17 - replicas three desired three updated
215:20 - three total three available perfect
215:23 - strategies rolling update
215:25 - okay because we haven't set a running
215:28 - update strategy that's the default one
215:32 - we'll come back to that uh later on the
215:36 - path template
215:37 - nginx Alpine listening on Port 80.
215:41 - right and the events here
215:47 - all right
215:49 - since a deployment will automatically
215:53 - create a replica set let's take a look
215:56 - at the replica set that was created so
215:59 - Cube CTL get RS
216:02 - and for sure we have one replica set
216:04 - that was created by the deployment if we
216:07 - desired three current we're ready
216:08 - everything looks okay and we can also
216:11 - describe our Republic asset since we
216:14 - have only one we can uh use Cube CTL
216:18 - describe RS or we can use its full name
216:23 - we copy that
216:27 - and here I have the replica set
216:31 - description
216:33 - and finally we need to do a little bit
216:36 - of cleanup we delete our deployment
216:40 - using Cube CTL delete and with the name
216:44 - of the yaml file
216:49 - thank you
216:54 - let's take a look at demon sits
216:57 - the demon set is a specialized workload
217:00 - its role is to ensure that an instance
217:02 - of a pod is running on all nodes
217:06 - the paths are scheduled by the scheduler
217:09 - controller and run by the demon
217:11 - controller
217:13 - as nodes are added to the cluster
217:16 - the pods are added to them
217:19 - typical views are running some kind of
217:21 - helper service like a log collection or
217:25 - some kind of monitoring
217:27 - well let's illustrate that in this
217:30 - cluster we have two nodes
217:32 - and the demon workload ensures that an
217:35 - instance of a pod is running on each one
217:38 - of these nodes
217:41 - so you define a demon set in a yellow
217:44 - file
217:45 - you can specify that you don't want to
217:47 - schedule a pod on the master node by
217:49 - using a toleration
217:51 - same thing if you want to run the part
217:54 - on specific node pools
217:58 - here's a cheat sheet for the demon sets
218:00 - command
218:01 - so you create a demon set using a yaml
218:05 - file with the cube CTL apply Dash F
218:08 - command
218:09 - you get a list of demon set using Cube
218:12 - CDL get DS
218:14 - you get some information about the
218:17 - running demon set using cubectl describe
218:20 - DS and the name of the demon set
218:23 - and when you want to delete a demon set
218:26 - you either use a yaml file using cubectl
218:30 - delete Dash F and the name of the yaml
218:32 - file or the name of the running Daemon
218:36 - set by using Cube CTL delete DS and the
218:39 - demon set name
218:42 - and this conclude this look at demon
218:46 - sets
218:48 - [Music]
218:54 - let's run a BusyBox image as a demon set
218:58 - to ensure that we have one instance of
219:03 - that container running on each node in
219:07 - our cluster
219:09 - so let's take a look at the yaml file
219:13 - all right
219:14 - the kind is demon set we give it a name
219:18 - demon set Dash example
219:20 - and the container that we'll run is a
219:24 - busy box and here in the Toleration
219:27 - section
219:29 - we specify that
219:32 - we don't want to schedule
219:35 - a demon set on
219:38 - a node role
219:41 - that is mastered so we don't want to to
219:44 - run that demon said that the Busy box uh
219:48 - on the control plane on the masternode
219:51 - all right
219:54 - Let's uh
219:56 - deploy our demon sets so here I'm I'll
220:01 - be deploying that demand set on a
220:03 - cluster in the cloud that has three
220:05 - nodes because if I would try to do that
220:08 - on Docker desktop with just one note
220:10 - won't be interesting so let's try to do
220:13 - that doesn't matter what cloud provider
220:15 - I'm using right now
220:17 - okay uh let's uh get a
220:22 - a cube City I'll get parts we'll come
220:24 - back to uh the rest just after that so
220:29 - here I have three uh Parts demon example
220:33 - with a unique name here let's uh add the
220:38 - dash
220:40 - oh wide
220:44 - a flag
220:46 - Let's uh examine the node column so you
220:52 - can see that each pod is running on a
220:57 - unique node so 0 3 and 4 here
221:03 - and this selector is interesting because
221:06 - it if you have multiple objects you can
221:11 - basically filter out or just select what
221:15 - you want so here selector says the app
221:18 - uh equals the demon set example that's
221:22 - the name of our object here
221:25 - so that word one instance of our busy
221:28 - box is running on each node
221:32 - let me add a fourth node
221:43 - all right so my note count is now four
221:48 - let's go back to visual studio code and
221:52 - let's rerun uh the get paths command
221:56 - and for sure I have a Ford instance of
222:01 - my BusyBox container running as a demon
222:05 - set on the new note without me doing
222:08 - anything it was deployed automatically
222:11 - because I selected the demon set object
222:15 - type
222:19 - now we simply need to delete that demon
222:22 - set using a cube CTL delete
222:29 - [Music]
222:35 - let's take a look at stateful sets
222:38 - let's say that you run a database inside
222:41 - your kubernetes cluster
222:43 - traffic gets higher and you need to
222:46 - scale that database
222:48 - so you create more instances
222:51 - the main one can read and write while
222:55 - the other instances are in read-only
222:57 - mode and use replicas of the main
223:00 - database
223:01 - this is a complex scenario to implement
223:06 - the stateful set role is to help you
223:09 - solve this complex problem
223:12 - for parts that need to maintain state
223:15 - and unlike a deployment a stateful set
223:19 - maintains a sticky identity with a
223:22 - persistent identifier for each of the
223:24 - pods
223:26 - and if a part dies it will be replaced
223:30 - by another one using the same identifier
223:35 - the stateful set will create the parts
223:37 - in sequence
223:39 - and delete them in reverse order
223:44 - typical use are for Network related
223:47 - services that maintain some State and
223:50 - also for databases
223:55 - each pod gets a unique identifier using
223:58 - the stateful set name plus the sequence
224:01 - number
224:02 - and if one dies it is replaced by a new
224:06 - one but with the same identifier
224:12 - the pot creation is done in a ordered
224:14 - way meaning that the first one will be
224:17 - created then the second one and so on
224:21 - and when they are deleted they are
224:24 - deleted in the reverse order
224:27 - note that the persistent volumes are not
224:30 - automatically deleted
224:33 - foreign
224:36 - set you need to use a needless service
224:39 - you define one by setting the cluster IP
224:43 - value to none
224:45 - the stateful set will then refer to the
224:48 - atlas service
224:50 - and you define cloud storage in the
224:53 - volume claim templates section
224:57 - let's represent this slide as you can
225:00 - see only the first spot lets you read
225:03 - and write so how can you reach it if you
225:08 - want to write to the database
225:10 - well you use the pad network name in
225:15 - this case the instance name my SQL Dash
225:19 - zero
225:21 - plus the service name so dot MySQL
225:26 - when reading simply use the service name
225:29 - this will load balance the calls across
225:32 - all instances
225:36 - a bit of warning here
225:39 - containers are stateless by default and
225:42 - stateful sets are offering a solution
225:45 - for a stateful scenario but a lot of
225:48 - work has to be done on top of that
225:52 - a better approach might be to use the
225:54 - cloud provider database Services instead
225:57 - of trying to fit a stateful scenario
225:59 - inside your kubernetes cluster
226:03 - lastly deleting a sitful set will not
226:06 - delete the PVCs you have to do this
226:08 - manually
226:11 - here's a cheat sheet for stateful set
226:13 - commands so you create one using the
226:17 - cube CTL apply Dash F and the yaml file
226:21 - you get a list of set full sets using
226:24 - cubectl get STS you describe one using
226:28 - cubectl describe SCS and the set for set
226:31 - name
226:32 - and you delete a simple set using the
226:36 - delete command either using a yaml file
226:39 - or the name of the stateful set
226:45 - and this concludes this look at stateful
226:47 - sets
226:51 - [Music]
226:56 - let's now create a stateful set
226:59 - let's take a look at the state we'll set
227:01 - that EML file there are two parts in
227:04 - this CML file the first part is the
227:06 - creation of the Headless service
227:09 - the the kindest service
227:13 - but the trick here is to set cluster IP
227:18 - To None like this this will create an
227:22 - endless service
227:24 - all right second part is to create our
227:26 - stateful set so the kind set will set
227:29 - give it a name nginx Dash SCS and here
227:34 - we're referencing our Atlas service here
227:37 - so the name of our Atlas service
227:39 - assigned to the service name property
227:43 - we want three replicas
227:46 - right of a nginx image
227:50 - and we're creating a claim on a search
227:54 - class name called Azure file so the
227:57 - cloud provider here is not important
227:59 - that will work on any cloud provider so
228:03 - I already have a search class name
228:05 - called Azure file we'll use the read
228:09 - write once access mode and we'll use
228:12 - when when gigabyte of storage and we'll
228:15 - Mount this to a folder in VAR www
228:22 - all right
228:25 - so let's create our Seattle set here
228:28 - Cube CTL apply Dash if
228:30 - stateful set Dot yaml and quickly let's
228:34 - do a cube CTL get but a white and here
228:39 - you can see that the first
228:42 - instance is running
228:45 - the second one is running the third one
228:48 - is pending so each instance will be
228:52 - created in a sequence so the first one
228:55 - zero the second one one the third one
228:59 - two you can see by the age of each
229:04 - instances so 19 seconds 14 and 9 seconds
229:08 - they are created in a sequence and
229:12 - deleted in a reverse order also
229:16 - here we can see that um
229:19 - there's an IP address assigned to each
229:22 - of these
229:25 - let's take a look at the PVCs we should
229:28 - have a PVC for each of these instances
229:34 - and yes SCS 0 1 and 2. so uh we can do a
229:41 - mapping one to one here so for uh scs0
229:46 - we have a PVC call also scs0 to prove
229:50 - that let's uh describe the second one
229:53 - let's say so Q
229:56 - the L
229:58 - describe paths and the number two and
230:03 - let's see what the PVC is assigned what
230:06 - claim is assigned
230:09 - so here
230:11 - the volume claim name scs-2
230:16 - awesome
230:19 - okay
230:22 - what we'll do we'll create a file in the
230:25 - in that instance the instance number two
230:27 - and we'll delete the the Pod the Pod
230:31 - will be recreated automatically and
230:34 - we'll see if the file still exists we'll
230:37 - also modify the default web page served
230:41 - by nginx and see how we can reach that
230:45 - that file the default web page from
230:50 - another instance
230:52 - so let's open a session on the nginx CS2
231:00 - perfect
231:02 - Let's uh CD in VAR
231:07 - are volume and let's uh simply type Echo
231:12 - and pipe that to
231:15 - a file called echo.txt
231:19 - I do it LS my file is there get
231:23 - Hello dot txt yeah perfect
231:28 - okay first step creating that file uh
231:32 - Second Step modifying the default web
231:35 - page let's CD into the user chair nginx
231:39 - HTML folder
231:44 - and let's do an LS here
231:47 - so here's the default page served by
231:50 - nginx index.html
231:54 - we will brutally replace that file by
232:00 - using cat and piping that to the file
232:04 - name
232:06 - and typing hello
232:08 - enter Ctrl D on Windows to save the file
232:15 - the file is there if I simply do a cat
232:18 - index.html
232:21 - yeah okay the file has been brutally
232:24 - replaced
232:26 - okay we'll close our session on this
232:29 - instance so scs2 let's close our ins our
232:33 - session
232:35 - and let's open
232:38 - a session on the instance zero
232:43 - okay
232:49 - awesome
232:51 - let's try to it that default HTML page
232:55 - but on the instance number two so nginx
232:59 - as CS2 so to do that we'll need to use
233:04 - the web the web address nginx scs2 so
233:09 - that's the name of the instance dot the
233:12 - name of the atlas service
233:16 - and that word
233:18 - would only the
233:22 - hard name word
233:26 - no you need a combination of boat
233:29 - the instance name and the atlas service
233:33 - also
233:34 - all right let's exit our session here
233:40 - okay let's try to delete uh the uh
233:45 - instance number two
233:53 - okay let's do a q that's CTL
233:57 - get but
234:01 - we have a new instance
234:03 - uh that was created uh seven seconds ago
234:07 - but the uh the name of the instance is
234:10 - still the same
234:11 - so instead of a random number here by
234:16 - using a stateful set this will ensure
234:19 - that the names will be the same
234:22 - okay so let's
234:25 - open a session
234:27 - on the instance number two let's LS far
234:32 - and here's our file
234:37 - awesome
234:38 - let's do our cleanup so we'll delete the
234:41 - stateful set and we need to manually
234:45 - delete dpvcs because simply deleting
234:48 - this table set will not delete the PVCs
234:52 - so let's do that
235:00 - [Music]
235:05 - let's take a look at jobs
235:09 - are for short-lived task workloads
235:13 - you start a job it executes and succeeds
235:17 - or fails so job don't say memory they
235:20 - don't wait for traffic
235:22 - a job creates one or more pod and
235:25 - ensures that a specific number of them
235:28 - successfully terminate
235:30 - the job tracks the successful part
235:33 - completions and then marks the job as
235:36 - complete when the desired number of
235:38 - completion is reached
235:40 - when using multiple Parts the job will
235:43 - create them one after the other
235:46 - they can also run in parallel
235:50 - this is a job definition
235:53 - you define how many paths you want to
235:55 - run at the same time
235:57 - you can set a deadline if needed
236:02 - and the number of completions to reach
236:04 - to Mark the job as complete
236:08 - and you should set the restart policy to
236:12 - never
236:15 - so here's a cheat sheet for the jobs
236:17 - command
236:19 - you create a job the imperative way
236:22 - using Cube CTL create job the job name
236:26 - and the image name
236:28 - using the declarative way with a yaml
236:31 - file you use cubectl apply Dash F into
236:34 - the yaml file name
236:36 - you list the jobs by using cubectl
236:40 - GitHub
236:41 - you get some information by using
236:44 - cubectl describe job and the job that is
236:47 - currently running
236:49 - and you delete jobs with either a yaml
236:53 - file using cubectl delete Dash F or with
236:56 - the job name Cube CTL delete job and the
237:00 - job name
237:03 - and this concludes this look at jobs
237:09 - thank you
237:14 - in this lab we'll create a simple job so
237:18 - let's take a look at the chub yaml file
237:21 - so the kind is chopped so that's the
237:25 - type of object we're creating
237:27 - uh the name we're giving it hello and
237:31 - what we will run is a busy box container
237:36 - and when the container starts it will
237:40 - Echo uh hello front the job
237:44 - perfect
237:46 - something sip simple
237:49 - Let's uh run this Cube CTL apply with
237:55 - the name of our yaml file
237:58 - perfect the job was created
238:01 - okay let's get a list of the jobs
238:07 - there's one
238:08 - hello completion one so it ran
238:13 - and duration was two seconds okay
238:17 - let's do a cube CDL describe job since
238:21 - we have only one job that will do the
238:24 - work if not we can type the job name
238:28 - hello
238:31 - and uh but but let's take a look at name
238:34 - hello namespace default
238:38 - annotation and so on and so on it ran
238:42 - One Time One succeeded
238:45 - so that's our container
238:48 - successfully create and complete it
238:52 - perfect
238:53 - now
238:55 - we
238:58 - can get a list of the pods using cubectl
239:02 - get that and this is uh the Pod that was
239:07 - created to run the job it's still there
239:10 - you see the status completed
239:13 - but it's still there so we can get to to
239:19 - examine the log in case something went
239:21 - wrong
239:22 - so we can do Cube
239:25 - CTL
239:26 - logs and the name of the part here
239:33 - hello from the job
239:37 - that worked
239:39 - so let's do our cleanup let's delete the
239:41 - job
239:46 - is deleted do we have any uh Parts left
239:51 - none any chops left
239:56 - none perfect
240:01 - [Music]
240:06 - let's take a look at crunch apps
240:09 - a crunch up is a workload that runs jobs
240:12 - on a schedule
240:14 - it's an extension of the job that we saw
240:17 - in the previous lecture
240:19 - the schedule is defined using a crown
240:22 - like syntax in UTC
240:24 - and you can get more information about
240:26 - the crown syntax in this Wikipedia page
240:30 - here's a crunch out definition
240:33 - you set the scheduled parameter to a
240:36 - cron schedule
240:38 - so how do you know if a crunch up ran
240:41 - successfully
240:42 - well you need to look at the job history
240:45 - by default the last three successful
240:48 - jobs
240:49 - and the last fill job are kept
240:54 - the paths will be in a stop State and
240:57 - you'll be able to look at their logs
241:00 - foreign
241:01 - if you don't want to keep any history
241:03 - you can set the successful job history
241:06 - limit to zero
241:11 - so here's a cheat sheet for the crown
241:14 - job commands
241:15 - you can create one using the imperative
241:18 - way
241:20 - and if you have a yellow file you use
241:23 - the cube CTL apply Dash F and the name
241:25 - of the yaml file
241:27 - you can get the list of decron jobs
241:29 - currently
241:31 - running using cubectl get the CJ
241:36 - you can get some information with the
241:39 - cube CDL describe CJ and you delete the
241:42 - crunch up using its CML file using Cube
241:44 - CTL delete Dash F and the yaml file name
241:48 - and if you don't have the yellow file
241:50 - name you can delete it using cubectl
241:53 - delete CJ and the crunch up that is
241:56 - currently running
241:59 - and this concludes this look at run jobs
242:04 - [Music]
242:09 - so let's create a crunch up we'll take a
242:13 - look at the yellow file
242:16 - kind is the crunch up I'm going to name
242:20 - it hello Cron
242:22 - and we will give it a schedule
242:27 - only Stars which means that it will run
242:29 - every 60 seconds every minute that's the
242:33 - default
242:34 - and we're going to run a busy box image
242:38 - and we it will Echo this string
242:43 - okay
242:45 - so let's create our
242:49 - job using cubectl apply
242:53 - and we can get a list of the crown jobs
242:57 - using cubectl get print jobs
243:01 - okay so here we have the name the
243:04 - schedule if it's suspended it's if it's
243:07 - active the last last time it ran and we
243:12 - can get some information using cubectl
243:15 - describe run job and we pass the its
243:20 - name hello Dash Cron
243:25 - here
243:26 - this is super useful for troubleshooting
243:29 - the name the default name space it runs
243:32 - on in the default namespace
243:34 - we have the schedule here
243:37 - how many jobs does it keep in its
243:39 - history how many failed job is kept also
243:45 - the the command that will run
243:50 - oh
243:52 - you can get a list of the pods
243:55 - okay uh one per Ran So elocron with a
244:00 - unique uh value here so it has uh
244:04 - completed I'm gonna pause
244:07 - um to let it run a few times
244:11 - all right uh the job ran three times
244:15 - well it the last one is still uh running
244:18 - uh container creating Let me refresh
244:21 - that so it's completed okay now
244:25 - we can get the logs by using cubectl
244:30 - logs and then the
244:33 - job name
244:35 - dpod name so hello from the crown chart
244:39 - so by default the last three run of the
244:42 - job are kept in the history and you can
244:45 - configure that in the yaml file
244:51 - let's delete our crunch up here
244:58 - perfect and if again we type Cube CTL
245:03 - get pads uh all the parts in in the
245:07 - history basically are also deleted when
245:10 - you delete the crunch up
245:15 - [Music]
245:20 - let's take a look at rolling updates
245:23 - in a previous lecture we saw that using
245:26 - deployments you can set the number of
245:29 - paths instances using the replicas
245:32 - parameter
245:34 - and set a number of previous iterations
245:36 - of the deployment to keep in kubernetes
245:40 - we also saw that there are two update
245:42 - strategies running a date and recreate
245:47 - recreate is quite simple kubernetes will
245:50 - shut down all the running paths and
245:52 - create new ones after that
245:54 - running update will cycle through
245:56 - updating parts
245:58 - all right let's illustrate that using
246:01 - recreate all previous paths are deleted
246:06 - and the New Path will be created after
246:08 - that
246:09 - this means that there might be a small
246:11 - period of time where your microservice
246:13 - might not be responsive
246:17 - using the routing of this strategy
246:20 - a pod is deleted and replaced by a new
246:23 - one
246:24 - then the next one and so on
246:29 - there are two values that you can set to
246:32 - help you with this process
246:34 - Max search will tell kubernetes how many
246:38 - parts can be created over the desired
246:42 - number of pods
246:43 - let's say that you you want three
246:45 - instances in total
246:47 - setting Max search to 1 will allow
246:51 - creation of one additional pod on top of
246:54 - these three desired ones and this while
246:58 - the rolling update is running
247:01 - Max unavailable is the opposite
247:04 - is the maximum number of pulse that can
247:08 - be unavailable during the update process
247:13 - note that if you don't specify an update
247:16 - strategy in the deployment manifest
247:18 - communities will use a default strategy
247:21 - of running update with Max Surge and Max
247:24 - unavailable both set to 25 percent
247:31 - so let's say that we want three
247:33 - instances of a bot and we set max search
247:37 - and Max unavailable to 1.
247:40 - we're telling kubernetes that it's okay
247:42 - to create one additional part on top of
247:46 - the three desired one
247:48 - and that's it's okay to have one part
247:51 - less than the three desired one
247:56 - when done the previous replica set is
247:59 - kept
248:00 - you set how many you want to keep with
248:03 - the revision history limit property
248:07 - here's a cheat sheet for running updates
248:11 - you create of course your deployment
248:14 - using cubectl apply Dash F and the name
248:17 - of your yaml file
248:19 - you get the progress of the update using
248:22 - cubectl rollout status
248:26 - you get the history of the deployment
248:28 - using cubectl rollout history deployment
248:31 - and the deployment name
248:35 - you can roll back a deployment using
248:37 - qctl rollout undo and the deployment
248:41 - name that will roll back to the previous
248:45 - version
248:46 - or if you want to roll back to a
248:49 - specific revision number
248:51 - you add the two dash revision parameter
248:58 - and this concludes this look at running
249:01 - updates
249:04 - [Music]
249:09 - in this lab we will create a deployment
249:11 - and later on update it to a new version
249:14 - using a rolling update
249:17 - so let's take a look at the yaml file
249:21 - the kind of object we're using easy
249:24 - deployment call Lo Dash dab we want
249:28 - three replicas and we're using a rolling
249:31 - update strategy here and setting a Max
249:34 - search to 1 and Max unavailable to one
249:40 - we will deploy a container called Lo
249:43 - Dash app yeah hello app and uh that'll
249:47 - be version one and later on we will
249:50 - update it to version 2. okay
249:54 - so let's create our deployment here
249:59 - okay we can get a the deployment status
250:03 - by using Cube CTL rollout status
250:06 - and the deployment name
250:09 - uh the deployment was successfully
250:11 - rolled out let's take a look at our
250:15 - pods running
250:17 - okay so I have three instances perfect
250:20 - yellow depth uh three times excellent
250:26 - let's describe the uh the deployment so
250:30 - Cube CDL describe uh deploy and the
250:34 - deployment name
250:36 - and let's try to find if we can get some
250:40 - information about the strategy yeah here
250:44 - it is rolling update strategy Max
250:47 - unavailable oh wow the strategy type is
250:49 - right here uh rolling update
250:52 - running update strategy Max and
250:54 - available One Max search to one so you
250:59 - can get that information that we set
251:02 - earlier in the yml files okay
251:08 - let's now see if we have a replica set
251:15 - here's our replica set
251:19 - and let's Now update
251:24 - our yaml file and change the version of
251:26 - Hello app to version 2. so just update
251:30 - to 2 and save the file
251:35 - and we will use cubesatel apply and with
251:40 - the same yaml file
251:43 - now what I'm going to do
251:45 - how I'm using right now A K9s it's a
251:50 - terminal dashboard in a terminal sorry
251:53 - and to get the visual view of what's
251:58 - what will happen basically so right now
252:01 - I have three parts in green these are
252:04 - the ones that are deployed let's apply
252:08 - our new deployment I'm going to switch
252:13 - quickly to K9s and you can see oh it
252:17 - happens so fast
252:18 - but
252:19 - is you saw that the the new pods were
252:23 - were created and the old ones were uh
252:26 - were shut down
252:29 - all right
252:30 - here I can have a a deployment status
252:38 - everything's fine if not we would have
252:40 - uh some information if the deployment
252:42 - would take longer we would have some
252:45 - information printed here
252:47 - okay how many uh replica sets do we have
252:53 - we have two
252:55 - uh the the current one so that's the
252:59 - current one and the previous one
253:02 - by default uh three uh versions are kept
253:06 - uh in history
253:07 - we can get the deployment history by
253:10 - using cubectl rollout history and the
253:13 - deployment name
253:18 - okay version one for version two
253:22 - okay we're at version two and
253:28 - um we want to downgrade basically our
253:30 - roll back to the previous version so you
253:33 - can do
253:34 - that by using Cube CTL rollout undo and
253:38 - the deployment name so by default it
253:41 - will roll back to the previous version
253:44 - or if you have multiple versions and you
253:46 - know what version you want to roll back
253:49 - to you can specify the flag to revision
253:52 - and the revision number as we see here
253:58 - foreign
254:00 - let's do this one
254:02 - well either one will do the same thing
254:07 - okay let me switch to K9s and see what's
254:10 - happening oh the other one the the older
254:14 - while the version two is uh terminating
254:16 - new version is created
254:20 - and we can get a deployment status here
254:26 - everything successfully wrote
254:29 - okay
254:31 - Let's uh now take a look at our replica
254:35 - sets
254:37 - so we still have two
254:39 - right
254:40 - the current one is now
254:43 - the the first one that we uh that we
254:46 - deployed
254:49 - all right and we can do our cleanup by
254:53 - deleting the deployment using the yaml
254:56 - file
254:59 - [Music]
255:05 - let's take a look at blue green
255:07 - deployments
255:09 - so let's say that version 2 of our macro
255:12 - service contains some breaking changes
255:14 - like a different database schema
255:18 - what do you do
255:20 - using the rolling update strategy you'll
255:22 - have both version 1 and version 2 of
255:25 - your app running at the same time
255:27 - that might not work at all
255:31 - so using the blue green deployment
255:33 - pattern that might help solve that
255:36 - problem
255:37 - blue identify what's in production and
255:41 - green identify a new version currently
255:43 - deployed but not yet in production
255:48 - notice that the pulse label contains the
255:50 - version number
255:52 - when ready simply update the service
255:55 - definition to point to the new version
255:59 - and now green is now in production
256:03 - so green becomes blue and blue becomes
256:08 - green
256:10 - oh great this means that this pattern is
256:13 - solving the new database schema problem
256:15 - well not entirely you may still have to
256:19 - deal with some downtime while you update
256:22 - your database
256:24 - and also another drawback is that since
256:27 - both version of the macro services will
256:30 - be up and running at the same time
256:32 - you need to have enough free resources
256:34 - in your cluster to make this possible
256:39 - and this concludes this look at the blue
256:41 - green deployments pattern
256:46 - [Music]
256:51 - in this lab we'll create a blue green
256:53 - deployment
256:55 - we have three yaml files here so let's
256:57 - take a look at the lodep V1
257:02 - it's a deployment
257:04 - we want three replicas of a container of
257:09 - a an image called Hello app
257:12 - 1.0 and here we're setting a label
257:16 - of app lov1
257:20 - now let's take a look at the second one
257:24 - it's basically the same thing so
257:27 - deployment three replicas
257:30 - but it will use version 2 of our hello
257:35 - App application and we're setting the
257:38 - this label Here app to lov2
257:44 - all right
257:45 - let's take a look at our cluster IP
257:48 - manifest file so it kind of service and
257:52 - here's the selector app hello V1
257:57 - we will deploy that and later on we will
258:00 - change
258:01 - the cluster IP manifest to point to the
258:05 - newer version
258:07 - okay so let's deploy version one of our
258:12 - pods
258:15 - and let's also deploy
258:18 - our cluster IP service
258:22 - so let's take a look at the list of the
258:25 - paths currently running there are three
258:28 - pods and also there should be
258:32 - uh if I type Cube City I'll get SVC
258:37 - one cluster IP as we see front that's
258:41 - the one I just deployed perfect
258:44 - so let's do a quick port forward
258:48 - to uh to connect to our cluster IP
258:51 - service so we'll port forward uh the
258:54 - port 8080 that the cluster IP is
258:56 - listening to to localhost 8080.
259:00 - here cubesatel port forward service the
259:06 - name of our service 8080 28080 and let's
259:10 - hit local OS on port 8080 here
259:15 - okay
259:16 - so here hello world version
259:20 - 1.0 excellent day
259:24 - okay
259:26 - let's now deploy version two so uh right
259:29 - in my phenomenal I'll hit Ctrl C on my
259:33 - windows keyboard to break that and uh
259:36 - gain my terminal back perfect
259:40 - let's deploy
259:43 - version two
259:47 - okay and let's get a list of our pods
259:53 - currently running should be six
259:55 - so I have both versions uh in memory at
259:59 - the same time
260:00 - okay so that's one of the drawback of
260:02 - this uh this technique this blue green
260:04 - deployment technique
260:06 - okay let's now it did the cluster IP
260:10 - manifest file so we're we will change
260:13 - the selector to select app on Lo
260:18 - V2
260:19 - let's save the file
260:22 - and let's update our cluster IP service
260:27 - by using cubectl apply and the name of
260:31 - the yaml file
260:34 - yes
260:38 - okay
260:40 - Let's uh port forward again
260:46 - right let's hit that local OS again
260:50 - and there you go V2
260:55 - it worked
260:58 - let's do a little bit of her clean up uh
261:01 - let's delete our first deployment our
261:03 - second deployment and also decluster IP
261:06 - service
261:07 - you can select the three lines at the
261:09 - same time
261:14 - [Music]
261:19 - let's take a look at the concept of
261:21 - service in kubernetes
261:25 - first what problem do Services try to
261:29 - solve
261:31 - well if the pod in green need to reach
261:35 - the pod in purple
261:37 - you need to use its IP address
261:41 - the problem is that pass yet ephemeral
261:44 - IP addresses
261:46 - if the part in purple dies well you
261:49 - replace it
261:50 - and the new one will have a different IP
261:52 - address
261:54 - so we need a way to make these calls
261:57 - between pods a lot more robust
262:02 - so back to the service what exactly is
262:05 - the service well it's a kubernetes
262:07 - object that you define in a yaml file
262:11 - but on my pods
262:14 - that have ephemeral IP addresses
262:16 - Services get durable IP addresses and
262:20 - also they get DNS names
262:23 - they serve as ways to access Parts
262:27 - they're kind of a middleman
262:29 - and the target pods using selectors
262:35 - here we have four pods and a service
262:39 - the service select the pods that have
262:41 - the Zone label equals to prod and the
262:45 - version label equal to one
262:49 - the first part satisfies the selector
262:53 - the second one also
262:55 - but not the third one
262:58 - and not the last one
263:00 - so only the first two are selected
263:05 - so let's say we have two instances of a
263:08 - pod and we place a service in front of
263:11 - them
263:12 - if another part needs to reach these
263:14 - ones
263:15 - it will go through the surface
263:19 - and then the service will load balance
263:21 - the request to the instances
263:26 - in kubernetes we can use these Services
263:29 - the cluster IP the node port and the
263:33 - load balancer
263:35 - note that the cluster IP is the default
263:38 - service
263:39 - we will look at them in more details in
263:41 - the next lectures
263:44 - and this concludes this quick look at
263:46 - the concept of services in kubernetes
263:51 - [Music]
263:57 - let's take a look at the cluster IP
264:00 - so what is a cluster IP well it's the
264:04 - default service in kubernetes
264:07 - its visibility is cluster internal this
264:11 - means that it's not possible to use a
264:13 - cluster IP service to reach a macro
264:16 - service from the internet from outside
264:18 - the cluster
264:20 - in the cluster IP definition you can set
264:23 - two different ports
264:25 - Port is the port that the service will
264:27 - listen on
264:29 - and Target Port is the port that
264:32 - deselected pods are listening on so the
264:36 - cluster IP will route incoming traffic
264:39 - to that board
264:41 - in this CML file the service listens on
264:44 - Port 80 and Route the traffic to Port
264:47 - 8080.
264:49 - traffic is load balanced across the
264:52 - selected paths
264:54 - so when do you use a cluster IP service
264:57 - well to provide a durable way to
265:01 - communicate with pods but from inside
265:04 - the cluster
265:06 - so let's illustrate this
265:09 - here we have a cluster IP service
265:11 - fronting three instances of a pod
265:15 - it's impossible to reach it from outside
265:18 - the cluster but it's okay it's visible
265:21 - from inside the cluster
265:26 - this cluster IP will listen on Port ad
265:29 - and select the paths using these two
265:32 - labels
265:34 - since the selected pods are listening on
265:37 - port 8080 the service Target Port must
265:40 - also be set to 8080.
265:44 - this way the parts in green that want to
265:47 - communicate with the ones in purple well
265:50 - they go through the cluster IP service
265:52 - on Port 80 and the service route the
265:55 - traffic to Port 8080.
265:59 - so let's say you have multiple Marco
266:02 - Services uh you place a cluster IP in
266:06 - front of each of them
266:08 - because a cluster IP service IP address
266:11 - is durable
266:14 - while the pods ones are ephemeral
266:18 - here's a cheat sheet for cluster IP
266:21 - so the first two commands are imperative
266:24 - commands let's say you already have a
266:28 - pod running and you want to expose it
266:30 - using a cluster ID
266:32 - so you would use Cube CTL expose Oh
266:36 - short for path the Pod name specifying
266:38 - the the port and the Target Port and you
266:41 - can also give a name to your to your
266:44 - service
266:45 - if you have a deployment you can also
266:48 - use Cube CTL expose deploy the
266:51 - deployment name specifying the port and
266:54 - the target board so both commands are
266:57 - imperative commands
266:59 - if you have a yaml file you would use
267:03 - Cube CTL apply Dash F and specifying the
267:07 - EML file name
267:08 - you can get a list of the services
267:11 - running using cubectl get SVC
267:16 - I get a little bit more information
267:17 - specifying uh the flag Dash o and wide
267:23 - you can also describe the service using
267:25 - a cube CTL describe SVC and the service
267:28 - name
267:29 - and you can delete the cluster IP
267:32 - service using the yaml file with the
267:35 - cube CTL delete Dash F and the name of
267:38 - the yaml file or cubectl delete SVC and
267:42 - the name of the service
267:45 - and this concludes this look at the
267:48 - clusterity
267:51 - [Music]
267:56 - in this lab we will deploy an nginx
267:58 - container front it with a cluster IP
268:01 - service and then deploy a BusyBox
268:04 - container open a session on that BusyBox
268:07 - container and try to hit the web page
268:10 - served by the nginx container but
268:14 - through the cluster IP service all right
268:18 - let's take a look at our yaml file we'll
268:22 - start with the deployment one
268:25 - kind is a deployment we want three
268:28 - instances of the Pod and we will run the
268:35 - nginx image the Alpine version and it
268:38 - will list it on Port 80. now we're
268:41 - setting two labels here and the cluster
268:44 - IP service will you will select uh the
268:49 - pods using these two labels so app
268:51 - example environment prod all right let's
268:56 - take a look at our cluster IP
268:59 - kind service
269:02 - it's gonna listen on port 8080 and it
269:06 - will redirect the traffic to Port 80 on
269:11 - the nginx containers all right
269:15 - the selector is here so app example
269:18 - environment broad so that will select
269:21 - our pods uh in our deployment
269:25 - perfect
269:26 - so let's try to do that first let's uh
269:30 - deploy the service
269:36 - and let's deploy the
269:42 - nginx containers
269:47 - let's also deploy the busy box
269:52 - and we can take a look at this it's a
269:56 - kind of pod the name is my box and uh
270:01 - it'll run a busy box image okay
270:05 - so now let's get a list of our Bots
270:10 - currently running
270:12 - we should add four
270:15 - one two three four so the first three
270:18 - ones are the deployment the nginx images
270:22 - and the fourth one is the BusyBox
270:26 - all right
270:28 - let's try to connect to the BusyBox
270:32 - container open a session by using
270:35 - cubectl exec my box Dash it and the name
270:40 - of program we want to run perfect at
270:42 - work by type LS yep okay let's try to
270:46 - use the service to reach the nginx pods
270:51 - so wget
270:53 - and HTTP SVC example colon 880 so let's
271:00 - try to run that and see if it works
271:03 - okay that worked why did it work so what
271:07 - is that name here
271:10 - if we go back to the cluster IP
271:13 - definition
271:14 - that's the name of our service
271:17 - and it's listening on Port 8080.
271:23 - the service name Colin d uh the port
271:28 - that is listening on
271:31 - and that's it we can now exit our
271:34 - session on our busy box and we can
271:37 - delete
271:40 - our resources the cluster IP the
271:43 - deployment and the pod
271:45 - thank you
271:47 - [Music]
271:52 - let's take a look at the note Port
271:54 - service
271:56 - what is a note port a noteboard extends
272:00 - the cluster IP service and provides
272:02 - extra functionalities
272:05 - its visibility is internal like a
272:08 - cluster IP but also external to the
272:11 - cluster
272:13 - you can set a third port using the note
272:15 - Port property
272:17 - this is the port that the service will
272:19 - listen on outside the cluster
272:22 - note that the port must be in a range
272:25 - between 30 000 and 32 767.
272:30 - and if you don't specify a note Port
272:32 - value well kubernetes will assign one
272:35 - randomly
272:38 - you then set the port and the Target
272:40 - Port Properties like you do with a
272:43 - cluster IP
272:45 - this sounds like a good way to expose
272:48 - our macro services to external traffic
272:51 - but this range between 30 000 and 32
272:55 - 767 it's kind of annoying because you
272:59 - can't set it to let's say port 80.
273:03 - and One requirement for using note ports
273:06 - is that nodes must have public IP
273:09 - addresses
273:12 - to access your service simply specify
273:15 - any node Port IP address plus the note
273:19 - port
273:20 - and the traffic will be routed to the
273:23 - right note Port service inside the
273:26 - cluster
273:28 - the way it works is that you set the
273:31 - pods and the service just like you did
273:34 - before with the cluster IP but this time
273:36 - you also specify a port number in the
273:39 - note Port property
273:42 - external communication uses the node IP
273:45 - address and the port set with the node
273:47 - Port property
273:49 - internal communication uses the port set
273:52 - in the port property just like a cluster
273:54 - IP
273:57 - now let's take a look at our note Port
273:59 - cheat sheet
274:00 - if you already have a pod running in
274:03 - your cluster and you want to expose it
274:05 - using a note Port service simply use the
274:08 - cube CTL expose bow the Pod name
274:12 - specifying the port and the Target Port
274:15 - and the note Port as the type
274:19 - now you may wonder where do you specify
274:24 - the note port number
274:26 - well you can't there's no properties
274:30 - letting you set know that value between
274:33 - 30 000 and 32 767.
274:38 - so communities will assign one randomly
274:41 - for you
274:43 - same thing for a deployment let's say
274:45 - you have a deployment already running in
274:47 - your cluster and you want to expose it
274:49 - using a note Port you use cubectl expose
274:52 - deploy the deployment name
274:55 - Port Target Port type which is note port
274:59 - and you can specify your name also
275:04 - you can define a node port in a yaml
275:06 - file and deploy it using cubesatel apply
275:10 - Dash F the name of the yaml file you
275:12 - list the services using cubectl get SVC
275:16 - get more info adding Dash o wide
275:20 - he can describe your service using
275:22 - cubesatel describe SVC and the service
275:25 - name
275:26 - if you have a yaml file you can delete
275:28 - it using the that file using cubectl
275:31 - delete Dash F the name of the yaml file
275:33 - or you can delete your service using its
275:36 - name with qctl delete SVC and the
275:39 - service name
275:42 - and this concludes this look at the note
275:45 - Port service
275:48 - [Music]
275:53 - in this lab we will expose a deployment
275:56 - using a note Port service we have two
275:59 - yable files let's take a look at them
276:01 - the first one is for the deployment we
276:04 - will deploy an engine X image the Alpine
276:06 - version listening on Port 80.
276:09 - and we will need to replicas perfect
276:14 - let's take a look at the noteboard yaml
276:17 - file kind is service
276:20 - and the type is noteboard
276:23 - and the selector will select our
276:25 - deployment
276:27 - and here we set our node Port 32 410
276:31 - okay
276:34 - now let's open a terminal
276:39 - and we'll start by the deployment
276:42 - cubesatel apply
276:45 - and our yaml file next our service Cube
276:49 - CTL apply
276:51 - noteboard.yaml awesome let's make sure
276:55 - that our pods are running
276:59 - Cube CTL getbots Dash o wide
277:03 - awesome two parts two instance of our
277:06 - nginx container
277:08 - all right now since we're using Docker
277:11 - desktop the docker desktop node is
277:14 - mapped to local OS to reach the note
277:17 - Port service we need to use local OS
277:20 - plus the note port
277:23 - let's try that
277:25 - local Post
277:29 - 32 410
277:33 - and it worked awesome
277:36 - Now when using a cloud provider you
277:39 - would need to get a node IP address
277:41 - instead of using the Local Host
277:45 - you would get that IP address by using
277:48 - cubectl get nodes Dash o wide
277:52 - and here in the external IP address
277:55 - colon you would find the external IP
277:59 - address of the node
278:01 - awesome let's do our cleanup let's
278:04 - delete our note port and our deployment
278:12 - [Music]
278:18 - let's take a look at the concept of
278:20 - surfaces in kubernetes
278:23 - what problem do Services try to solve
278:25 - well if the pad in green need to reach
278:29 - the purple one
278:30 - it needs to use its IP address
278:33 - the problem is that pods are ephemeral
278:37 - if the purple one dies you need to
278:41 - replace it
278:42 - and the new one will have a different IP
278:45 - address
278:46 - we need a way to make these calls
278:48 - between pods more robust
278:53 - so what exactly is a service
278:56 - a service is a kubernetes object that
278:59 - you define in a yaml manifest
279:03 - unlike parts that have ephemeral IP
279:06 - addresses
279:07 - Services gets durable IP addresses and a
279:11 - DNS name
279:13 - they serve as a way to access paths and
279:16 - they target pods using selectors
279:20 - here we have four pods and a service
279:25 - the surface selects the paths that have
279:27 - the Zone label equals to prod and the
279:31 - version label equals to V1
279:35 - the first part satisfies the selector
279:39 - the second one also
279:41 - but not the third one and the last one
279:45 - only the first two are selected
279:51 - let's say that we have two instances of
279:53 - a pod and we place a service in front of
279:56 - them
279:57 - if another part needs to reach these
279:59 - parts it will go through the service and
280:04 - the surface will load balance the
280:06 - requests to these instances
280:10 - in kubernetes we can use these services
280:13 - D cluster IP the note Port the load
280:18 - balancer and the Ingress
280:21 - the cluster IP is the default service
280:24 - its visibility is internal only
280:28 - the note Port can expose a pod outside
280:30 - the cluster
280:32 - the load balancer and the Ingress are
280:35 - similar Services they let you expose
280:38 - applications outside of the cluster
280:41 - one operates at D layer 4 and the other
280:44 - at layer 7.
280:47 - L4 L7 what's that
280:50 - download balancer operates at the layer
280:53 - 4. that's the TCP transport level so
280:57 - that's very low in the transport stack
281:00 - it means that the load balancer can do
281:02 - simple operations like round robin
281:05 - routing
281:06 - the Ingress operates at the higher level
281:09 - in the transport stack
281:11 - think of protocols like HTTP or SMTP
281:16 - it's more intelligent so you can
281:18 - configure complex routing rules okay no
281:22 - worries if this sounds complex for now
281:25 - simply remember that an Ingress is like
281:28 - a load balancer but more intelligent
281:32 - and this concludes this look at the
281:34 - concept of services
281:39 - [Music]
281:44 - in this lab we will create a load
281:47 - balancer service
281:48 - but you may be may be asking yourself
281:51 - we're not using a cloud provider right
281:54 - now how can that work
281:57 - well Docker desktop is helping us it
282:00 - will emulate the load balancer service
282:03 - so we can test our load balancer locally
282:07 - awesome let's take a look at the
282:10 - application
282:11 - it's a simple deployment we want to
282:15 - replicas two instances of an nginx image
282:18 - super simple
282:20 - and the the load balancer yaml file
282:24 - kind is service the type is load
282:27 - balancer
282:28 - it will listen on port 8080 and redirect
282:31 - traffic to the Pod that is listening on
282:33 - Port 80.
282:34 - all right
282:36 - all right so let's open a terminal
282:38 - okay
282:43 - and let's deploy the app
282:49 - and the load balancer
282:54 - perfect let's make sure that our pod are
282:58 - running by using Cube CDL getbots
283:02 - yes I have two uh my coupons are here
283:05 - perfect now
283:07 - to get the IP address of the load
283:10 - balancer we use Cube CTL get SVC Dash o
283:14 - wide
283:17 - foreign
283:22 - S as the IP address
283:25 - using a cloud provider load balancer
283:28 - service you would find here instead of
283:31 - local OS the public IP address of the
283:35 - load balancer so let's test this open a
283:39 - browser and type localhost 8080
283:46 - and that works we reach our nginx pod
283:50 - awesome
283:51 - let's do our cleanup let's delete our
283:54 - load balancer
283:57 - and our application
284:04 - [Music]
284:09 - this is an introduction to the
284:11 - persistent Concepts in kubernetes
284:15 - we saw this slide earlier
284:17 - containers are ephemeris and stateless
284:19 - and any data stored in them is deleted
284:22 - when the container is destroyed
284:25 - so we need to find a way to store data
284:28 - outside the containers if you want to
284:30 - keep that data
284:33 - so volumes let containers store data
284:36 - into external storage systems
284:39 - these are storage services offered by
284:42 - the cloud providers
284:44 - Defenders create plugins according to a
284:48 - specification called the container
284:50 - storage interface
284:52 - and there are two ways to use storage in
284:54 - the cloud the static way and the dynamic
284:57 - way we have separate lectures on these
285:01 - later on
285:03 - all right the cloud providers create
285:04 - plugins
285:06 - to expose their storage Services as
285:09 - persistent volumes and storage class
285:13 - these two are kubernetes objects
285:17 - next we will look at the static and
285:20 - dynamic ways
285:24 - [Music]
285:29 - let's see how to use the static way
285:33 - persistent volumes or PV and persistent
285:37 - volume claims are PVCs
285:40 - are two kubernetes objects that lets you
285:43 - define and use external storage
285:48 - a purchasing volume represents a storage
285:51 - resource that is available cluster wide
285:54 - and is provisioned by the cluster
285:56 - administrator
285:58 - you then use a persistent volume claim
286:01 - to claim the persistent volume
286:05 - a part will then use the PVC to mount a
286:10 - local folder
286:13 - PVCs can be used by multiple parts and
286:17 - inside the parts all the containers are
286:20 - sharing the same volume
286:24 - there are many persistent volumes
286:27 - plugins available some are offered by
286:30 - the cloud providers
286:31 - the one highlighted in yellow called
286:34 - hostpath is a special one
286:37 - it's a plugin available with kubernetes
286:39 - that allow you to do local testing
286:42 - and it's not mapped to a cloud provider
286:45 - storage service
286:48 - it will not work in a multi-node cluster
286:51 - but it's super useful for local testing
286:57 - here's the main drawback of persistent
286:59 - volumes let's say that the cluster admin
287:03 - provision 100 gigabytes of storage
287:07 - and that the Pod only requires a small
287:10 - portion of this storage let's say just
287:13 - one gigabyte of that 100 gigabytes in
287:17 - total so just one gigabyte well
287:20 - too bad for the other pods because the
287:23 - volume is used exclusively by the Pod
287:26 - who has the claim on it
287:30 - this can be a waste of precious
287:32 - resources and we will see how storage
287:35 - class get around this problem in the
287:38 - next lecture
287:40 - okay in the meantime let's focus on the
287:43 - PV and the PVC
287:46 - you first select the cloud provider
287:48 - storage service you want to use
287:51 - then you create a persistent volume and
287:55 - set the required capacity let's say here
287:58 - 10 gigabytes
288:00 - you then create a PVC so a claim
288:04 - that refers to the persistent value
288:08 - and finally you use the PVC from your
288:12 - pod and mount a local folder on it
288:19 - there's an inputs and property that you
288:21 - must be aware of it's the reclaim policy
288:27 - set a delete all data will be lost when
288:31 - the claim on the volume is released
288:34 - and this is the default value so be
288:37 - aware of this
288:40 - if you want to keep your files when the
288:42 - PVC is released
288:44 - you have to set the reclaim policy to
288:46 - retain
288:48 - again the default value is delete so be
288:52 - careful and be aware of this
288:56 - there are three access modes possible
288:59 - using read write mini the volume can be
289:03 - mounted as read write by many parts
289:06 - using read-only mini the volume can be
289:09 - mounted read-only by many parts
289:13 - and finally with read write once the
289:16 - volume can be mounted as real read write
289:19 - Sorry by one single path and the other
289:23 - parts will be in read-only mode
289:27 - this might be useful if you have a main
289:30 - worker that writes data and the other
289:32 - pods simply read the data
289:37 - you define a person in volumes using the
289:41 - purchasing volume kind and you specify
289:44 - the capacity
289:45 - the access mode and the reclaim policy
289:48 - in the spec section
289:52 - in this example the ospad plugin is used
289:55 - to access local storage
289:58 - remember to only use hostpat for local
290:02 - testing and refer to the storage
290:04 - provider documentation and on how to
290:07 - create a persistent volume specific for
290:11 - their storage service
290:15 - you then Define a claim so a persistent
290:18 - value claim making sure that the access
290:20 - mode match the one set in the processing
290:25 - value
290:26 - in this case the claim is for 8
290:29 - gigabytes out of the possible 10
290:32 - gigabytes set on the persistent volume
290:35 - this means that no one can claim the
290:37 - remaining two gigabytes until the claim
290:40 - is released
290:44 - in the volume section of your pod
290:46 - definition
290:47 - simply refer to the PVC and mount a
290:51 - local folder on it
290:59 - a persistent volume can have these
291:02 - states
291:03 - available meaning that the resource is
291:06 - free and not currently in use
291:09 - bound the volume is bound to a claim so
291:13 - it's in use it's not available anymore
291:16 - release the claim has been deleted but
291:19 - the resource is not yet reclaimed by the
291:22 - cluster
291:23 - and finally failed well something's
291:26 - wrong
291:29 - here's a cheat sheet for the PV and PVC
291:32 - commands
291:33 - using a yaml file you can create either
291:37 - a PV or PVC by using cubectl apply Dash
291:40 - F and the name of the yaml file
291:43 - you get the install persistent volume
291:45 - using cubecti Cube CTL getpv
291:49 - the claims using cubectl get PVC you can
291:53 - describe them cubectl describe PV or PVC
291:57 - with their name you can delete them
291:59 - using their yaml file cubicle delete F
292:04 - and the name of the yaml file or by
292:07 - using their name so Cube CTL delete PV
292:10 - and the PV name or the PVC name
292:16 - and this concludes this section as the
292:19 - static way next we'll take a look at the
292:21 - dynamic way
292:24 - [Music]
292:29 - in this lab we will create a persistent
292:31 - volume a persistent volume claim and use
292:35 - a pod to mount a local folder on that
292:40 - storage
292:41 - we will create that in Docker desktop
292:45 - locally using the host path plugin all
292:49 - right so let's first take a look at the
292:51 - persistent volume yaml file
292:54 - the kind is purchasing value we give it
292:58 - a name pv001
293:00 - and a storage capacity 10 megabyte
293:05 - access mode read write once and we set
293:08 - the processing volume reclaim policy to
293:11 - retain
293:12 - and use that host Pat plugin here and to
293:17 - map to a folder in the docker desktop
293:20 - virtual machine to data here
293:24 - all right
293:25 - let's take a look at the PVC
293:28 - kind persistent volume claim we give it
293:32 - the name my claim read write access mode
293:36 - must be the same as the PV persistent
293:38 - volume so let's double check read write
293:42 - once
293:44 - on the PV read write once on the PVC
293:46 - awesome
293:48 - we request 10 megabytes of storage so we
293:51 - record the full capacity we could have
293:54 - chosen a lesser value if you want all
293:58 - right
293:59 - let's take a look at the Pod now
294:02 - so it's uh the kind of pod it's a busy
294:07 - box
294:08 - and uh we make a reference to the claim
294:12 - here in the volumes section we give it a
294:15 - name and we reference the persistent
294:19 - volume claim called my claim
294:23 - this guy here
294:25 - okay
294:28 - and we use that name mypd and we mount
294:33 - it to a local folder called demo we
294:37 - should see magically appear a folder
294:40 - called demo inside our BusyBox container
294:46 - okay let's deploy our persistent volume
294:51 - right
294:54 - percent in volume pv01 created awesome
294:58 - let's look at the PV
295:01 - qctl getpv
295:05 - name
295:06 - pv001 capacity 10 megabyte read write
295:10 - once reclaim policy to retain it's
295:13 - available it's not claimed
295:16 - uh awesome so let's now deploy the claim
295:21 - the PVC
295:25 - persistent volume claim my claim created
295:28 - awesome Cube CTL get PVC
295:34 - my claim it's bound to the volume called
295:38 - pv001 capacity 10 megabytes
295:41 - read write once and let's again take a
295:45 - look at the PV to see if something has
295:48 - changed
295:50 - yep it's now bound the status is bound
295:54 - to the claim called
295:57 - my claim running in the default name
296:00 - space
296:01 - awesome
296:05 - so let's now uh deploy our pod
296:12 - okay my PC box was created let's connect
296:16 - to it using Cube CTL exec the name of
296:20 - the instance so the Pod Dash it and the
296:23 - program you want to run
296:25 - so let's do NLS and see if we see a demo
296:29 - folder and there it is we have our demo
296:34 - folder
296:36 - Let's uh CD into that folder and let's
296:39 - create a a file inside
296:44 - cat and we'll pipe that to uh lo.txt
296:48 - let's type hello world world
296:54 - uh if I can type
296:57 - Ctrl D to exit and save the file Let's
297:01 - do an LS to see if the file was created
297:04 - perfect
297:05 - okay
297:07 - let's exit this session
297:11 - and now let's delete the pod
297:15 - let's delete the part by using cubectl
297:17 - delete Dash F pod and since the busy box
297:20 - takes uh 30 seconds to shut down we will
297:24 - force it to do it right away we don't
297:28 - want to wait
297:29 - okay
297:31 - Let's uh deploy it again well
297:35 - Cube CTL
297:39 - rctl get the pods no resource awesome
297:45 - it's really dead
297:47 - Let's uh deploy it again
297:51 - okay
297:52 - Let's uh open a session
297:57 - CD demo LS all right and let's get that
298:02 - file
298:06 - hello a car cat cat nut car
298:12 - hello world awesome it worked
298:16 - Let's uh exit our session
298:21 - let's now do our cleanup we'll delete
298:24 - our bud
298:27 - and then we will delete the PVC
298:35 - right
298:38 - and then the PV
298:44 - so you can't delete the PV before the
298:47 - PVC well you can issue the command but
298:50 - the the command will be in kind of a
298:53 - weight State uh until uh the PVC has
298:57 - been released
299:02 - [Music]
299:07 - let's continue our journey into
299:09 - persistence by looking at the dynamic
299:12 - way
299:13 - so here's a new object the storage class
299:16 - and the search class represent a storage
299:19 - resource that is available cluster wide
299:21 - and is provisioned by the cluster
299:23 - administrator
299:25 - you don't have to set a capacity
299:28 - and it eliminates the need for the admin
299:30 - to pre-provision a persistent value
299:35 - now compared with processing volumes
299:38 - where once a claim has been made
299:41 - the remaining capacity becomes
299:43 - unavailable
299:45 - well the storage class can support many
299:48 - claims many persistent volume claims
299:53 - so you first select the cloud provider
299:56 - storage service that you want to use
300:00 - you create a storage class
300:02 - so here no need to specify a capacity
300:06 - then you create a PVC the claim that
300:10 - refers to the storage class and now you
300:14 - specify the required capacity
300:17 - and finally you use the PVC in your pod
300:21 - and mount a local folder
300:27 - like a persistent volume there's an
300:30 - important property that you must be
300:32 - aware of it's the reclaim policy set at
300:36 - delete all data will be lost when the
300:39 - claim is released and it's the default
300:43 - value also like the persistent volume so
300:47 - be aware
300:48 - if you want to keep your files when the
300:51 - PVC is released you have to set the
300:53 - reclaim policy to retain again the
300:56 - default value is delete
301:00 - again three access modes possible and
301:03 - they are set using the PVC not the
301:06 - storage class
301:08 - read write mini the volume can be
301:10 - mounted as read write by many pods read
301:14 - only mini the volume can be mounted
301:17 - read-only by many pods
301:19 - and lastly read write once the volume
301:23 - can be mounted as read write by a single
301:26 - part and the other parts will be in
301:29 - read-only mode
301:31 - useful if you have a main worker that
301:33 - writes data and the other pods simply
301:35 - read the data
301:40 - so
301:41 - you first start defining a storage class
301:45 - specifying the cloud provider driver
301:48 - with the provisioner property and
301:52 - additional settings in the parameters
301:54 - section
301:55 - so refer to the source provider
301:57 - documentation on how to create a
302:00 - specific storage class further storage
302:02 - service
302:05 - you then Define a PVC specifying an
302:09 - access mode
302:11 - and the source capacity required
302:14 - in this claim uh the claim is for 5
302:18 - gigabytes
302:19 - but more PVCs can be created over that
302:23 - storage class
302:26 - then simply refer to the PVC in your
302:30 - path definition and map a local folder
302:33 - on it
302:36 - in summary the main benefits of a
302:38 - storage class versus a purchasing volume
302:40 - is that with a storage class you don't
302:43 - have to define a capacity
302:45 - and multiple Claims can be made
302:50 - here's a cheat sheet for storage class
302:52 - at Men's so you create your storage
302:56 - class using a yaml file using Cube CTL
303:00 - apply Dash F and your the name of your
303:02 - yaml file
303:03 - you get a list of your storage classes
303:06 - or PVCs using get the SC for search
303:11 - class and get PVC
303:14 - you get the search as information by
303:17 - using cubectl described as C and the
303:20 - class name
303:21 - you delete your search class and PVC
303:24 - using cubectl delete Dash F and the yaml
303:27 - file name or you delete your search
303:31 - class using delete SC and the class name
303:34 - or delete PVC and the PVC name
303:40 - and this concludes this section about
303:44 - persistence using the dynamic way
303:49 - [Music]
303:54 - let's see how to store configuration
303:56 - values using config Maps
304:00 - in a previous lecture we saw that it was
304:03 - possible to place configuration values
304:05 - directly in the environment section of a
304:08 - pod definition
304:11 - but what if we need to change that value
304:15 - well we have to edit the Manifest and
304:19 - redeploy the container
304:22 - also usually it's not a best practice to
304:26 - tie an object with its configuration so
304:29 - how can we externalize these values
304:33 - the config map object allow you to
304:37 - decouple and externalize configuration
304:40 - values
304:41 - the key value pairs are then injected in
304:44 - the containers as environment variables
304:48 - they can be created from yaml files
304:52 - a series of text files or even folders
304:56 - containing files
304:59 - they are static meaning that if you
305:01 - change the value the containers that
305:05 - refer to these values have to be
305:07 - restarted to get these refresh values
305:12 - using a yaml file you define a config
305:16 - map and place the key value pairs in the
305:19 - data section
305:22 - you can even specify multi-line values
305:24 - using the pipe character
305:28 - in the EnV section environment section
305:31 - of the container definition
305:33 - you define an environment variable and
305:37 - by using value from an config map key
305:41 - ref
305:42 - you reference the config back name and
305:46 - the key as defined in the config map
305:52 - so in the config map key ref section
305:55 - name refers to the config map name and
305:59 - key refers to a key in the config map
306:05 - earlier I mentioned that this is a
306:07 - static process meaning that the values
306:10 - are injected when kubernetes starts the
306:13 - container
306:14 - this means that if you make a change to
306:18 - a config map value
306:20 - inside the container the original values
306:23 - stay the same until you restart the
306:26 - container
306:28 - to get around this you can map a volume
306:31 - on a config map Yes you heard it right
306:35 - mounting a volume
306:37 - this solves the static issue and updates
306:40 - are reflected in containers
306:43 - each key value pair is seen as a file in
306:48 - the mounted directory
306:50 - so we start with a config map
306:53 - then use a volume to mount it to a local
306:58 - folder inside our pod our container
307:02 - the result is that all key value pairs
307:05 - are now scenes as file
307:08 - the name of the file being the key
307:11 - and the value being inserted in the file
307:16 - while this sounds cool it also means
307:19 - that you'll have to refactor your code
307:21 - so instead of reading environment
307:23 - variables
307:24 - you'll have to read files
307:26 - so is it worth it you'll have to figure
307:29 - out that by yourself
307:32 - here's the config Maps cheat sheet so if
307:37 - you're adventurous you can create a
307:40 - config map from the command line that's
307:42 - the imperative Way by using Cube CTL
307:45 - create config map you give it a name and
307:48 - with the from Dash literal parameter you
307:51 - specify the key value pairs so you can
307:53 - specify multiple key value pairs of the
307:56 - same line
307:57 - or you can use a good old yaml file and
308:01 - use cubectl apply Dash F and the name of
308:04 - your yaml file you can create a config
308:07 - map using cubectl Create CM specify your
308:10 - name and specifying a the name of a text
308:16 - file containing multiple key value pairs
308:21 - also you can create a config maps from a
308:25 - folder so if you have multiple files
308:29 - inside your folders you can create a
308:31 - config map from that
308:33 - you can get a list of the config maps by
308:36 - using cubectl getcm
308:39 - you can output the config map in a yaml
308:43 - file by using cubectl get CM the name of
308:46 - the config map and the dash o parameter
308:49 - with the yaml and you can pipe that to
308:53 - to a file name
308:55 - of course you delete a config map by
308:58 - using its yaml file using cubectl delete
309:02 - Dash F and the name of the yaml file
309:08 - and this concludes this uh look at
309:12 - config Maps
309:16 - [Music]
309:21 - in this app we're going to create a
309:23 - config map and use a pod that will
309:25 - reference a value stored in that config
309:29 - map
309:30 - let's take a look at the config map
309:34 - so the kind is config map
309:37 - we have a name
309:38 - cm-example and in the data section we
309:43 - have two key value pairs date as set to
309:47 - Michigan and City and our board all
309:50 - right
309:51 - so let's now take a look at our pod
309:55 - tiny spot
309:57 - uh it's a busy box
310:01 - and here in the environment section
310:05 - we declare in environment variable that
310:09 - we will call City
310:12 - and we are getting the value from
310:17 - a config map key ref
310:20 - right and we specified the name of the
310:24 - config map so cm-example
310:27 - that's the name of the config map
310:32 - and the key is City
310:36 - so here
310:37 - that's the key right there
310:40 - okay
310:43 - again environment section
310:46 - we Define a new environment variable
310:49 - that we will call City we get the value
310:52 - from config map key ref
310:55 - specifying the name of the config map
310:57 - and the key
311:00 - awesome
311:02 - let's create our config map
311:09 - okay Cube CDL gets cm to get information
311:13 - about our config map
311:16 - so CM example
311:18 - two data
311:21 - okay that doesn't give us much
311:24 - information well about the data itself
311:27 - so let's do a cube Studio describe
311:30 - config map
311:31 - CM example
311:36 - okay
311:38 - we have the name the namespace where it
311:42 - is any labels annotations and here we
311:46 - have uh the data section City and
311:49 - Airport State Michigan
311:53 - and if for some reason you want to
311:56 - Output that as a yamo uh Cube CDL get
312:00 - config map the name of the config map
312:03 - Dash o output in yaml
312:07 - uh to recreate the config map using this
312:16 - let's now deploy the pod
312:18 - or BusyBox
312:22 - perfect let's open a session
312:26 - Cube CDL exec
312:28 - my box
312:29 - Dash it and the program you want to run
312:33 - and let's display the city environment
312:37 - variable let's Echo that Echo dollar
312:40 - sign City and there it is an arbor so
312:44 - that worked foreign
312:46 - exit
312:51 - and we can do a little bit of cleanup we
312:55 - can delete our config map
312:59 - and we can delete our busy box spot
313:06 - [Music]
313:11 - let's see how to use the secrets object
313:15 - in kubernetes you will find many times
313:18 - of Secrets types
313:20 - the default one is the OPAC type
313:24 - and it is very similar to the config
313:27 - Maps object that we saw in the previous
313:29 - lecture
313:31 - you can also store credentials to
313:34 - connect to private container registries
313:37 - authentication secrets
313:40 - and even certificates
313:43 - in this lecture we will focus on the
313:45 - OPEC Secret
313:49 - like config Maps secrets are used to
313:51 - store configuration values so they are
313:54 - somewhat identical to config Maps except
313:57 - that these store values as base64
314:00 - encoded strings
314:03 - and it's important to understand that
314:05 - base64 is a way to encode strings
314:08 - and it is not an encryption algorithm
314:12 - this means that Secrets stored in
314:15 - kubernetes can be decoded quite easily
314:19 - yeah great since these secrets are Noah
314:23 - very secret should you use them
314:26 - well the answer depends on the type of
314:29 - information you want to store
314:31 - it might be okay to store a connection
314:33 - to a database but it might not be for
314:36 - something more sensitive
314:39 - you can protect Secrets using role-based
314:43 - access control policies are back
314:46 - or you can store them elsewhere all
314:49 - Cloud providers offered ways to store
314:51 - secrets in Seeker Vault services
314:55 - that you can retrieve firm kubernetes
314:58 - you can also use a third party tool like
315:00 - the very popular Vault product from
315:02 - archicup just be aware that the
315:06 - kubernetes default secret the OPEC one
315:09 - is not encrypted by default in
315:12 - kubernetes
315:15 - so you can define a secret in a manifest
315:18 - and use a base64 encoded strings as the
315:22 - values
315:24 - or use the command line where you can
315:27 - use plain text strings
315:30 - easier
315:33 - in the path definition you simply get
315:36 - the secret value using the secret key
315:38 - ref section
315:40 - this is very similar to config Maps
315:44 - and again similar to config maps you can
315:46 - mount a volume on top of Secrets
315:53 - here's the container registry secret and
315:57 - you can Define it using a yaml file or
315:59 - with the CLI
316:01 - next in the path definition you
316:04 - reference the credentials in the image
316:07 - pull Secrets section
316:11 - here's a cheat sheet for Secrets
316:13 - commands
316:15 - so you can create a secret the
316:17 - imperative way at the common line if you
316:20 - want using cubesdl Create secret generic
316:23 - and then the secret name and you pass
316:26 - the key value pairs with the from Dash
316:29 - literal per meter you can of course
316:32 - create one using a yaml file so cubesdl
316:36 - apply Dash F and the name of the yaml
316:38 - file you can get a list of the secrets
316:41 - using cubectl gets secrets
316:44 - you can output the secret to a yaml file
316:50 - by using cubectl get Secrets the secret
316:53 - name and the dash o yaml parameter and
316:58 - you can pipe that to a a file you can
317:01 - delete the secret using a yaml file or
317:05 - using the secret name using cubectl
317:08 - delete secrets and with the secret name
317:13 - and this concludes this look at secrets
317:19 - [Music]
317:24 - in this lab we'll create a secret and
317:27 - from a pod we will reference secret and
317:31 - use them as environment variable
317:34 - let's take a look at our secrets at yaml
317:37 - file
317:38 - the type or the kind is secret we give
317:42 - it a name secrets and in the data
317:45 - section we have key value pairs so
317:47 - username to some value and password to
317:51 - some value notice that the
317:53 - values but must be base64 encoded you
317:59 - cannot put a non-encoded string here
318:02 - how do you do that on Windows base64 the
318:07 - the tool is not installed by default so
318:10 - you can use these two websites basics of
318:13 - foreign code.org decode.org
318:16 - or you can install base64 using
318:19 - chocolati as shoko installed base64 on
318:22 - macro Linux well it's already installed
318:25 - so you simply do something like that you
318:28 - Echo your string and you pipe that to
318:33 - base 64.
318:36 - and that will encode the string and let
318:40 - me copy that
318:42 - and to decode Echo
318:46 - uh the encoder string and you pipe that
318:49 - again to base64
318:53 - Dash D for decode and voila
318:58 - all right
319:05 - from the Pod now let's take a look at
319:09 - our pod yaml definition
319:11 - kindness part it's a busy box and here
319:17 - in the environment variable we're
319:19 - creating two environment variable the
319:22 - first one is called username and it gets
319:25 - its value from a secret key ref
319:30 - and the name references the secret name
319:35 - here
319:37 - and the value well the key is one of
319:41 - these two
319:42 - so username or password Here
319:47 - a case key reference here the username
319:50 - from the secrets Secret
319:53 - and second one is password a the
319:56 - environment variable is called password
319:57 - and we get the value from the secrets
320:00 - secret and the key is password awesome
320:06 - let's create the secret
320:13 - all right let's get a list of the
320:17 - secrets
320:20 - here's our secret if it has two data it
320:24 - was created six seconds ago we can
320:27 - describe it cubicity of describe the
320:31 - object type and then it name
320:35 - and here what do we have the name
320:40 - namespace and well we don't see the
320:44 - secret just the the keys here
320:49 - and what if we use Cube CTL get secrets
320:56 - and output that to demo
321:00 - the
321:04 - secret data password say
321:09 - uh
321:11 - doing a describe would not allow us to
321:14 - see the values but using get secret and
321:18 - outputting that to a yaml will allow us
321:21 - to retrieve the actual values
321:26 - okay
321:27 - let's now deploy our busy box
321:34 - right let's open a session
321:40 - and let's Echo uh username
321:45 - D username and the password
321:52 - whoops Echo
321:55 - uh
321:57 - password
321:59 - my password so that that works
322:03 - the uh the values the secrets are
322:06 - decoded when they're injected into into
322:10 - the pods
322:12 - let's exit that and let's delete our
322:16 - secret
322:19 - and let's delete our busy box
322:25 - [Music]
322:30 - let's talk about observability
322:33 - if you deploy a container using a
322:36 - deployment and it crashes kubernetes
322:38 - will create a brand new instance of the
322:41 - pod
322:42 - this works because kubernetes monitors
322:45 - the infrastructure
322:46 - but what about your application
322:49 - if your app crashes well kubernetes will
322:52 - look at the Pod health and see that it's
322:55 - still running
322:56 - so from the infrastructure point of view
322:59 - everything is working fine your bud is
323:02 - still up and running but your code
323:04 - inside the Pod has crashed
323:08 - would it be nice if kubernetes could
323:10 - monitor the application health
323:13 - well you can achieve this by configuring
323:15 - probes
323:18 - the startup probe informs a kubernetes
323:21 - that the container has started and it's
323:22 - now okay to send traffic to it
323:25 - the Readiness probe enforced kubernetes
323:28 - that the container is now ready to
323:30 - accept traffic
323:32 - let's say that when your app starts it
323:35 - needs to execute a series of steps like
323:38 - getting some configuration values from a
323:40 - database creating some files and so on
323:43 - and so on and let's say that this
323:46 - startup sequence takes around 30 seconds
323:49 - so even if your container is up and
323:52 - running your code is not ready to accept
323:55 - traffic
323:56 - so using a Readiness probe you tell
323:59 - kubernetes to wait 30 seconds before
324:01 - starting to send traffic
324:05 - lastly the liveness probes tell
324:07 - kubernetes if your app is still running
324:10 - and if not kubernetes will kill the Pod
324:13 - and replace it with a brand new one
324:18 - here's a part definition with the three
324:20 - possible probes
324:22 - the starter probe tell kubernetes to
324:25 - wait for 10 seconds before making an
324:27 - HTTP call to a page called health
324:29 - the failure threshold tells kubernetes
324:32 - to try three times
324:36 - the Readiness probe tell kubernetes to
324:39 - wait initially five seconds before
324:40 - probing and making a TCP call on port
324:44 - 8080 and then check every 10 seconds
324:50 - the liveness probe tell kubernetes to
324:52 - wait initially for 15 seconds before
324:54 - probing by making a TCP call on port
324:57 - 8080 and then check every 20 seconds
325:01 - note that the Readiness probe will run
325:04 - during the whole pod life cycle
325:07 - but will these two Conflict at some
325:10 - point
325:12 - yes and no they will run simultaneously
325:15 - but a fail probe will result in
325:18 - different actions from kubernetes
325:21 - failing a Readiness probe will tell
325:24 - kubernetes to stop sending traffic to
325:26 - the pod
325:28 - but the Pod is still alive right
325:30 - while failing a liveness probe will tell
325:34 - kubernetes to restart the pod
325:40 - how does kubernetes probe the containers
325:43 - the cubelet will do the probing using
325:45 - the method you configure
325:48 - with exec action kubernetes will run a
325:50 - command inside a container
325:53 - with TCP socket action kubernetes check
325:56 - if a TCP socket port is open
325:59 - and with HTTP get action kubernetes
326:02 - perform nhdp get
326:06 - here's an exec action you're telling
326:09 - kubernetes to run a cat command on a
326:12 - file called healthy in the TMP folder
326:18 - here's a TCP socket action
326:21 - you're telling kubernetes to check if
326:23 - there's an open socket on port 8080
326:28 - and finally an HTTP get action you're
326:31 - telling kubernetes to do an HTTP get on
326:34 - the Health page on Port 8080.
326:39 - and this concludes this look at
326:43 - observability
326:46 - [Music]
326:50 - in this lab we will set a liveness probe
326:54 - so let's take a look at our yaml file
326:58 - we will deploy a pod that we'll call
327:01 - liveness Dash example is going to be a
327:04 - PC box
327:06 - and this is where we're setting the
327:09 - probe
327:10 - type is liveness Pro
327:13 - we're asking to do an exacto so to run a
327:17 - command inside our container
327:21 - and the command is cat
327:24 - and the parameter is that file so under
327:26 - the TMP folder the file is called
327:30 - healthy no extension
327:32 - basically we're asking kubernetes
327:35 - to run that command if that command is
327:40 - successful if the file exists well the
327:44 - probe is successful if the file doesn't
327:47 - exist the probe will fail okay
327:52 - uh initial delay seconds we're asking
327:55 - kubernetes to wait for five seconds
327:56 - before starting to probe and then period
327:59 - seconds to five we're asking kubernetes
328:02 - to probe every five seconds
328:06 - the last parameter is failure threshold
328:09 - set to two basically we're telling
328:11 - kubernetes that the liveness probe will
328:14 - fail when two probes will fail
328:19 - now for uh the purpose of this lab we
328:24 - will set something a little bit funky uh
328:28 - just so we're able to do this test
328:31 - we're um we're running a command when
328:35 - the container starts uh it's dutch and
328:38 - basically this will create that healthy
328:41 - file
328:42 - and then we're telling the container to
328:46 - wait for 15 seconds
328:49 - and then to delete that file
328:53 - right and just a little trick too so
328:56 - we'll be able to to quickly see the
328:58 - effect of the lavenous probe
329:01 - okay
329:03 - Let's uh deploy our pod
329:12 - and let's quickly do a cube CTL describe
329:16 - pod
329:19 - all right successfully pull the image
329:22 - the container was created
329:25 - okay so let's
329:27 - do that again a few times and we'll see
329:30 - what happens
329:31 - just rerun the command out
329:34 - liveness profile cat can't open okay two
329:38 - times over five seconds
329:40 - and now kubernetes is killing the Pod
329:43 - awesome
329:46 - and uh
329:53 - is still killing
330:00 - so you see that here the the Pod was in
330:04 - an unhealthy State here
330:06 - so now it's in the killing mode oh
330:10 - and then uh kubernetes is pulling again
330:14 - the BusyBox image and starting a new one
330:19 - right
330:23 - and then
330:25 - it's on LT again
330:27 - and the process starts uh starts again
330:33 - all right let's do our cleanup let's
330:36 - delete our pod and we'll force it
330:41 - you don't want to wait for it to end
330:43 - perfect
330:47 - [Music]
330:52 - let's take a look at some dashboards
330:56 - while it's fine to use a terminal to get
330:59 - a view of your cluster You may wish to
331:02 - use a graphical user interface instead
331:05 - luckily there are many options available
331:07 - we will take a look at these three
331:09 - popular and free options the kubernetes
331:13 - dashboard the lens desktop application
331:16 - and K9s a dashboard that runs in the
331:20 - terminal
331:21 - let's start with the kubernetes
331:23 - dashboard
331:24 - it's a web UI that you can install as an
331:28 - add-on inside your cluster
331:31 - it's not installed by default by Docker
331:34 - desktop and also by most Cloud providers
331:38 - the rule of thumb is if you don't need
331:40 - it don't install it
331:43 - a because it runs inside the cluster so
331:46 - you need to find a way to expose it over
331:48 - the internet
331:49 - and B well because of that it's a known
331:53 - Vector of attack
331:55 - that being said the kubernetes dashboard
331:58 - that you see the various resources
332:00 - deployed inside your cluster
332:02 - simply select the type in the left menu
332:06 - and you can also edit them by clicking
332:09 - on the edit icon
332:11 - and you can edit the yaml file and click
332:14 - on update to change the Manifest file
332:20 - lens is a IDE that runs locally on Mac
332:25 - windows and Linux so you need to install
332:28 - it on your OS
332:30 - on top of viewing the resources and
332:32 - editing the yaml Manifest
332:34 - you can also use a built-in editor and
332:37 - also a built-in terminal
332:40 - lens is maintained by mirantis and you
332:44 - can download it from this URL
332:48 - here's the overview dashboard that lets
332:51 - you see a resource count
332:54 - like the kubernetes dashboard you can
332:57 - select a resource click on the edit
333:00 - button
333:01 - and it did the Manifest directly in lens
333:07 - you can also type commands using the
333:10 - built-in terminal
333:14 - K9s is a cool text dashboard that runs
333:18 - in a terminal
333:20 - and you can install it on Windows Mac
333:23 - and Linux
333:26 - it might sound strange to run the
333:28 - dashboard in a terminal but this makes a
333:31 - lot of sense Kina NS is super light
333:34 - starts in an instant and gives you a
333:38 - clean view of all your resources
333:40 - you can get the information about the
333:43 - cluster the resources and you can take a
333:48 - or make a series of action like deleting
333:51 - a resource viewing the logs
333:54 - here we have a list of the pods
333:56 - currently running in the default
333:58 - namespace
334:00 - want to view the services that are
334:02 - currently running simply type
334:04 - colon
334:06 - and type SVC to list the objects type
334:12 - foreign pressing s while a part is
334:16 - selected will open a shell
334:20 - you can also set a port forward by
334:23 - typing shift f
334:26 - you can even see the part logs
334:30 - I really like this K9s dashboard in a
334:33 - terminal and use it all the time
334:37 - and this concludes this look at the
334:39 - kubernetes dashboards
334:42 - thank you
334:44 - [Music]
334:49 - in this lab we'll take a look at Lens so
334:52 - lens is a free dashboard that you can
334:54 - install on Windows Mac and Linux here's
334:57 - the URL
334:59 - k8s
335:00 - lenss.dev let's take a look at the
335:03 - website
335:04 - from there you can install it it runs on
335:09 - Mac windows and Linux so you can
335:12 - download the setup files here
335:15 - or if you're using a security on Windows
335:18 - you can use shoko install lens or Brew
335:22 - on Mac and Brew install cast lens
335:26 - all right
335:28 - I've already launched a lens here and by
335:33 - default it should take you to your
335:34 - Docker desktop cluster if not or if you
335:39 - want to select another cluster click on
335:42 - the hamburger menu to the left and
335:46 - select add cluster
335:49 - Lance will look at the cube config file
335:54 - and you'll be able to select your
335:58 - clusters from that drop down list so
336:01 - here I have my demo cluster in the cloud
336:04 - so I can select that
336:06 - and click on ADD cluster
336:12 - I can be connected to multiple clusters
336:16 - here's my Docker desktop here's my demo
336:20 - so let me switch back to dockerdist up
336:24 - here
336:25 - okay
336:30 - so first thing first uh let's deploy
336:33 - something on our cluster
336:34 - here I have a yaml file
336:36 - it's a deployment
336:39 - we'll have three replicas of an image
336:43 - called Hello app okay nothing fancy here
336:50 - let's deploy
336:53 - this all right and let me switch back to
336:58 - uh lens
337:00 - here
337:04 - now let's click here on workloads
337:08 - and overview
337:10 - so we should see three parts running
337:14 - here one deployment and one replica set
337:19 - I can either from the top menu select
337:22 - path or from this workload menu select
337:27 - paths here and the other type of objects
337:31 - here
337:32 - so here are my three parts
337:35 - I have one deployment and one replica
337:39 - set if I click on an object I get more
337:44 - information
337:45 - labels annotations and so on and so on
337:48 - so
337:49 - something that you would have at the
337:52 - terminal at the common line by typing
337:54 - Cube CTL describe uh replica set or
337:59 - deployment or pod and the object name
338:02 - here
338:03 - but here is presented in a nice UI
338:09 - all right
338:11 - let's take a look at our pods here okay
338:14 - let's see what we can do let's say
338:17 - we want to delete this one so I can
338:21 - select it like this and from the Ellis
338:24 - menu to the right I can open that and
338:28 - select remove
338:30 - right or I can remove it
338:33 - right there
338:35 - okay let's do that
338:37 - remove item hello yep
338:40 - remove it and since it's a deployment uh
338:44 - kubernetes will create a new One
338:47 - automatically
338:50 - pretty cool uh let's uh take a look at
338:53 - the logs
338:56 - we can see the logs from there we can
339:00 - open a shell
339:06 - so at last
339:09 - a pretty cool front directly from that
339:11 - UI if I click on edit
339:14 - well I can edit the Manifest file
339:19 - and click on Save and close that will
339:22 - update that object
339:26 - let me cancel that
339:29 - close close this there's a built-in
339:32 - terminal so by default you have a small
339:35 - icon here called terminal if you click
339:39 - on it if you click on this open button
339:42 - that will open the terminal so I can
339:46 - type command Cube C Cube CTL get the
339:53 - pods
339:54 - right
339:58 - you can try to delete an object directly
340:01 - here so let me copy this object name
340:07 - Cube CTL delete uh
340:12 - and let's paste that
340:15 - we should see right right away we see uh
340:18 - something happening uh at the top of the
340:21 - screen
340:27 - all right
340:28 - uh what's cool is that if you have more
340:30 - than one cluster you can switch between
340:31 - these clusters so here let me uh just to
340:35 - prove it
340:37 - clear and the
340:40 - qctl uh get the nodes
340:45 - okay on Docker desktop I have only one
340:47 - node let me select My Demo cluster
340:51 - my terminology open it
340:55 - there's no deployment yet but let's just
340:59 - do a cube CTL
341:02 - get nodes
341:05 - and for sure I have three nodes in this
341:08 - in this cluster
341:09 - that's pretty cool
341:12 - just by selecting the cluster I can
341:15 - switch and I have a terminal that is in
341:17 - the right context
341:19 - pretty cool
341:22 - so I just scratched the surface there's
341:25 - a lot more information that we can get
341:27 - like a configuration the config Maps the
341:30 - secrets
341:33 - what the network services that are
341:37 - installed here I have a cluster IP
341:39 - service
341:42 - storage so the persistent volume claims
341:45 - persistent volume storage classes so
341:48 - here in my cluster in the cloud I have
341:51 - four search classes defined for me the
341:55 - namespaces and so on and so on all the
341:59 - information that you would get from the
342:02 - command line using cubectl you get the
342:05 - same information but from a nice UI
342:13 - so once you're done exploring don't
342:15 - forget to delete your deployment
342:19 - thank you
342:21 - [Music]
342:26 - in this lab we will use K9s a super
342:29 - great dashboard running inside a
342:32 - terminal so you can get more information
342:34 - about the K9s at the website
342:39 - k9scli.io
342:41 - here's the website nice logo
342:47 - and you can look at the documentation
342:50 - and the how to install K9s from that
342:55 - website if you're on Windows
342:59 - you can install it using chocolaty so
343:02 - Chico install K9s on Mac OS Brew install
343:05 - K9s and on Linux take a look at the
343:08 - documentation
343:10 - all right
343:12 - Let's uh first deploy something in our
343:16 - clusters
343:17 - here in our yaml file I have a simple
343:20 - deployment with three replicas of a
343:24 - simple
343:26 - container or image called Hello app
343:29 - nothing fancy here
343:31 - let's deploy that uh right away
343:38 - okay and let's open a terminal or come
343:42 - in line comment prom whatever you name
343:46 - it
343:47 - and let me type K9s
343:56 - all right let me stretch that a little
343:59 - bit
344:02 - well it's super cool it's a dashboard
344:04 - running inside the terminal it's super
344:07 - light and it gives you tons of
344:09 - functionality so here
344:12 - I have my deployment so I'm in the
344:15 - default namespace you can look at my my
344:18 - deployment I can enter on an object I
344:23 - get more information type Escape go back
344:29 - I have information about my cluster uh I
344:33 - can issue some some commands so Ctrl D I
344:37 - can delete a a pod let me do that let me
344:41 - kill that poor part here
344:45 - uh are you sure yes
344:49 - see you have feedback visual feedback of
344:52 - of uh what's happening
344:56 - the Pod that I deleted was kill was in
345:00 - the terminating State and the new one
345:02 - was uh created uh right away
345:06 - I can yeah it D to describe
345:12 - the resource escape to uh to go back
345:17 - Ctrl K to kill I just deleted one but I
345:20 - can do a Ctrl k
345:25 - there we go
345:27 - I can see the log so let's let's switch
345:30 - to the second one type l
345:34 - and I can look at the uh the logs
345:37 - I can open a shell also by typing here
345:43 - the
345:45 - S letter doing the less
345:49 - right you can type exit go back I can uh
345:54 - even configure a port forward
345:59 - pretty cool here I can look at the yaml
346:01 - file also
346:07 - let me type Escape
346:14 - here we have the parts that are listed
346:16 - but if I type Colin I can change that
346:20 - let's say let me type deploy
346:22 - so here's my deployment now if I type e
346:28 - I will edit the deployment and let's
346:31 - change the number of senses or replicas
346:34 - from three to four
346:37 - and let's close that
346:42 - right and now I have four out of four so
346:46 - let's uh I'll type Colin again and type
346:50 - but
346:53 - here are my four pods
346:58 - super interesting it's a free tool it's
347:02 - super light it's super fast and I always
347:06 - have one open so I can see
347:09 - visually what's happening uh inside my
347:12 - cluster when I'm issuing some some
347:14 - commands
347:16 - let's go back uh to uh our page here our
347:21 - Visual Studio code and let's simply
347:24 - delete our deployment and let me switch
347:27 - back right away here
347:30 - and they're gone
347:33 - pretty cool too
347:37 - [Music]
347:41 - let's see how to scale pods
347:44 - the horizontal pad to scalar is a
347:47 - kubernetes feature that allows you to
347:49 - scale the number of Parts up and down
347:54 - it uses the metric server to gather the
347:57 - pods utilization
347:59 - pods app must have requests and limits
348:02 - defined
348:03 - the HPA checks the metric server every
348:07 - 30 seconds and scale the number of PODS
348:10 - according to the minimum and maximum
348:13 - number of replicas defined
348:17 - to prevent racing conditions the HPA
348:19 - Waits some period of time after a
348:22 - scaling event by default this delay on
348:25 - scale-up events is 3 minutes and the
348:29 - delay on scaled down events is 5 minutes
348:34 - in this part definition you specify the
348:37 - CPU and memory requests and limits
348:40 - the request is what's allocated at first
348:42 - and the limit is what you allow the part
348:45 - to burst to
348:47 - in this example the Pod will start with
348:49 - 64 megabyte of ram but can burst up to
348:52 - 128 megabyte if needed
348:56 - you configure the HPA using a manifest
348:59 - specifying the deployment you want to
349:01 - scale
349:02 - the Min and max number of replicas
349:06 - and the metric you want the HPA to scale
349:09 - on in this case we tell the HPA to kick
349:12 - in when the average CPU utilization is
349:16 - above 50 percent
349:19 - here's the cheat sheet for the HPA
349:21 - commands
349:22 - so you can create one using the
349:24 - imperative way using cubectl autoscale
349:27 - deployment the name and the metric and
349:32 - replicas number
349:34 - you can create one using the yaml file
349:40 - you can get the autoscaler status by
349:43 - using cubectl get HPA and the HPA name
349:46 - and of course you can delete the HPA
349:49 - using the yaml file or cube CTL delete
349:52 - HPA with its name
349:55 - foreign
349:58 - this lecture about scaling pods
350:03 - [Music]
350:07 - in this lab we will use the horizontal
350:11 - particle scalar to scale a pod
350:14 - for the HP it worked it needs some data
350:16 - some metrics coming from the metric
350:19 - server and by default it's not installed
350:22 - by Docker desktop
350:24 - just make sure I'll open the terminal
350:28 - and what we'll do we'll get a list of
350:32 - the parts running in the cube system
350:34 - namespace
350:38 - Accord DNS at CD Cube API server proxy
350:43 - provisioner no nothing that looks like
350:46 - metric server okay
350:49 - to install it you need to run this yaml
350:53 - file coming from this git repo on
350:57 - kubernetes-6 special integers group and
351:00 - Metric server
351:02 - but
351:04 - you need to make a small modification to
351:08 - to it
351:10 - let's take a look at the components that
351:12 - yaml file
351:14 - need to do you need to edit it if you
351:17 - download it directly from the git repo
351:22 - and you locate the deployment section
351:25 - there it is deployment
351:29 - and what you need to do is add this
351:31 - parameter
351:33 - a cubelet dash insecure-tls
351:37 - if not the metric server will not run on
351:41 - Docker desktop all right
351:44 - let's deploy our metric server cubectl
351:49 - apply Dash F components Dot yaml
351:54 - awesome
351:56 - what we can do we can run again Cube
352:00 - City I'll get pod in the cube system
352:02 - namespace
352:05 - aha metrics server there it is it might
352:10 - take a few a couple of minutes for the
352:12 - metric server to start running
352:15 - now that the metric server is running
352:17 - let's take a look at our deployment
352:20 - uh it's a deployment and what we'll
352:23 - deploy is a web server called HPA Dash
352:27 - example uh listening on Port 80. so it's
352:30 - a simple web server that will return a
352:32 - web page nothing fancy
352:35 - what we'll do uh will also deploy a busy
352:39 - box and from that busy box will it that
352:42 - web server uh in a loop and that should
352:45 - generate some traffic
352:48 - awesome let's
352:50 - deploy our
352:53 - web server
352:58 - let's get a list of the pods running in
353:01 - the default namespace awesome it's
353:04 - running
353:05 - let's enable our Auto scaler so Cube CTL
353:09 - Auto scale a deployment called HP Dash
353:12 - deployment and a metric called CPU
353:15 - percent
353:16 - and we want a minimum of one instance
353:19 - and a maximum of four
353:22 - okay
353:25 - let's validate that we have an HPA Cube
353:28 - CTL get HPA that will list all the hpas
353:33 - running
353:34 - on my cluster there's only one awesome
353:38 - let's now deploy our busy box
353:45 - and let's connect
353:48 - or open a session on that busy box
353:52 - perfect
353:54 - and here's our endless loop that will uh
353:57 - it the web server
354:00 - that should generate some traffic
354:03 - okay let's take a look at our
354:07 - K9s
354:09 - I have my deployment here my HPA
354:12 - deployment and my busy box
354:14 - and area we have three instances of our
354:19 - deployment right now
354:20 - awesome
354:21 - so the HPA worked and now what I can do
354:25 - is start the loop by hitting Ctrl C
354:30 - and I'll type exit to exit my busy box
354:36 - and from there I can delete my HPA
354:42 - but be careful when you do that I'll
354:45 - delete my HP here if we take a look at
354:48 - the deployments there's still three
354:50 - nothing will scale that down since the
354:54 - HP uh has been deleted
354:57 - let's delete our busy box
355:02 - and let's delete our deployment that
355:04 - should delete all three instances
355:07 - take a look here yep all three are
355:10 - terminating
355:12 - and optionally you can delete the metric
355:15 - server by using cubectl delete and the
355:19 - components.yaml file here
355:24 - [Music]
355:30 - we are at the end of this Docker
355:32 - containers and kubernetes fundamental
355:35 - scores
355:36 - congratulations you are now an official
355:40 - kubernetes Ninja
355:41 - [Applause]
355:44 - the next steps for you would be to
355:47 - deploy containers in the cloud using
355:49 - services from a cloud provider
355:52 - these courses will teach you how to do
355:55 - that on Google cloud and Azure and also
355:58 - on smaller Cloud providers like Linux
356:00 - and digitalocean
356:03 - the best part is that each offer free
356:06 - credit usage when creating new accounts
356:09 - this way you can create a managed
356:11 - kubernetes service in the cloud without
356:14 - breaking the bank
356:17 - if you enjoyed the course you can help
356:19 - me by making a small donation this is
356:22 - the link to my buy me a coffee page
356:26 - I want to say a big thank you for
356:29 - learning Docker and kubernetes using my
356:31 - course and I wish you all the best

Cleaned transcript:

gee Barrette teaches us Docker containers and kubernetes fundamentals course for beginners ghee is a developer and trainer with more than 25 years of experience he is a Microsoft MVP frequent conference speaker and was the leader of the montreal.net user group for more than 23 years all this is to say he is the perfect person to teach you about Docker and kubernetes welcome to this Docker containers and kubernetes fundamentals training course my name is gibaret and I'll be your host in your Learning Journey Into The Amazing World of containers I'm a fulltime trainer with a developer background and I'm based in Montreal Canada and that's where my strange accent comes from I'm certified on kubernetes and also on terraform Azure AWS and Google Cloud I'm very honored to be a Microsoft MVP in the Azure expertise and a digitalocean navigator you can reach me via the contact page and on Twitter what should you expect from this course you will not become an expert just by taking this course this is an entrylevel course that will provide you with a strong containers and kubernetes Foundation and you'll gain enough knowledge to make sound decision at work or in your projects throughout this course you will find lots of Amazon activities to practice what you've learned you will use Docker and kubernetes locally on your PC or Mac so there's no requirement to have an account with a cloud provider are there any prerequisites for this course not really if you're a developer a devops specialist an ID Pro or even a technical manager that's totally fine no previous Docker or kubernetes knowledge is required we will cover a lot of ground you will learn about containers Docker and D Docker registry you learn about the kubernetes objects like pods workloads and services that's a lot of material and the goal here is to get you from zero knowledge to a kubernetes ninja well at least provide you with enough knowledge to Aspire being a kubernetes ninja I want to say a big thank you for learning Docker and kubernetes using this course if you like the course you can help me by making a small donation this is the link to my buy me a coffee page you can of course buy one of my other courses where you'll learn to run containers on different Cloud providers services and use a managed kubernetes cluster in the cloud and finally I wish you all the best in your Learning Journey let's see how to set up your laptop or PC for this course you need a laptop PC or Mac with either Windows 10 Mac OS or Linux if you have a Mac with an apple silicon most tools should run perfectly I will use Visual Studio code with the docker extension to help build create and run containers vs code is a free IDE that runs on Windows Mac and Linux on Windows and Mac you'll need Docker desktop with kubernetes enabled on Linux refer to the documentation on how to install Docker and kubernetes on your distro you'll need a Docker of account and a few easy to install tools refer to the setup instructions located below this video the lab files are located in a git repo on GitHub simply open this URL in a browser click on the code button if you have git installed on your machine you can type git clone with the link displayed here and if you don't have git simply click on the download zip button to download the code as a zip file let's talk about the microservices concepts if we head to Wikipedia and take a look at the definition that we find over there it says that it's a variant of the service oriented architecture or SOA a structural style slash architecture and that it arranges in application as a collection of loosely coupled services so instead of a large monolithic system we have multiple smaller pieces in a microservices architecture services are finegrained meaning that each of them have their own responsibilities and the protocol use are lightweight like an API exposed over HTTP or grpc for example if we look at the monolithic architecture these systems were usually built as one single unit an IDE would group multiple projects and we would compile the whole thing as one single unit they were also deployed as a single unit so we would need to copy everything all the files on on a server and if we had to scale the system we had to spin a new VM and copy deploy the whole system on that VM and same for a third and a Ford server an example of such a monolithic architecture is a treaty application even though the system was clearly separated into layers it was all tightly coupled from the web project we had to make a reference to the business layer project and the whole system would run in the same address space with macro Services we break our big system into smaller parts each with its own responsibility so let's say we have a class that deals with identity in our business layer we can extract that code and place it in its own microservice we can then scale each of these smaller pieces independently from each other there's no strong bound since we expose functionality through an API they can be written by smaller teams and each can use their own programming languages like go PHP C sharp and domainspecific data can be stored in separate databases so the way we would deploy a monolithic system is by deploying everything on a server all the dlls files needed to run the system we had to scale the deploy everything on more servers now let's compare that to microservices well microservices are deployed independently each can scale independently also need to scale back one service no problem so if you have an existing monolithic system how can you transform it into a microservices architecture well you need to break it into small units like the code that dealt with the identity in the in our business layer Martin Fowler author of the patterns of Enterprise application architecture book documented the way to achieve such a transformation using the Strangler pattern let's say our identity code is here in the Legacy system we can place a facade to Route the calls to it migrate the code and have the facade route the calls to the new macro service at some point as we go we will end up with less code in our Legacy system and when democracy and it's done we can get rid of the facade this pattern is very useful and you can learn more about it using the link in this slide and this concludes this lecture on the Marco Services Concepts let's talk about Marco Services antipattern because it's a thought Rosie I know it's kind of strange to talk about what can go wrong right away but I think it's very important first of all it's not some kind of magic pixie dust that you can sprinkle on top of a zigzing system and boom you get a beautiful Marco Services architect system it takes efforts and maturity to achieve this from one monolithic system you'll end up with a bunch of smaller pieces and that can add extra complexity a change to a microservice can have a domino effect and take your system down and what about securing all of these microservices it's also essential to use or introduce new processes in the organization like devops Ci CD and testing but be careful and don't try to implement everything at the same time it's a recipe for disaster take it step by step and make sure you have metrics in place to validate each of these steps and this concludes this lecture on microservices antibatterns let's talk about the microservices benefits and drawbacks since each microservice runs in its own address space there are less chances that if one of them goes down it takes the whole system down with it a microservice runs on open source Technologies so there's less Fender lockin since they are smaller in most case they are easier to understand and that makes them faster to deploy and also easier to scale like we saw in the antipattern section there are some drawbacks complexity is added over mother's existence to resolve to her complexity issues so make sure your team is well trained and has made some proof of concept and make sure to start small adding one piece at a time testing might appear simpler since there are less functionality in a microservice to test but make sure to test the whole system deployment may appear simpler but one update can impact many Marco services and have a terrible domino effect ready to manage multiple databases calls between macro services will go through apis and this will add a bit of latency to all calls so make sure you test for that this key transition server will appear you'll make a call and it will fail or but try again 50 milliseconds later and it will work so make sure to implement some retry strategies in your code or by using a service mesh instead of one big point of failure you'll end up with multiple ones can your system survive if one microservice goes down and what about security are you okay for all these microservices can see and talk to each other so yes complexity is introduced for solving complexity issues and this concludes this lecture on the macro Services benefits and drawbacks thank you let's Now understand what is cloud native you may have heard the term Cloud native before but what it is exactly it's a way to architect and build complex systems taking advantage of modern development practices and the use of cloud infrastructure if we head to the cloud native Foundation website and look at the definition we see that it's quite a long one so let's break it into smaller parts Cloud native uses containers service meshes microservices immutable infrastructure and declarative apis we'll cover containers service meshes and macro services and the concept of immutability in this course but not how to build apis immutable infrastructure means that we usually never update something but we replace it with a newer version Loosely coupled systems mean that the functionalities are exposed through apis observable with the use of metrics creation and updates are automated and instead of making changes once every six months we deploy eye impact changes on a freaking basis and finally we use a series of Open Source projects to run our system when the cncf says to use open source projects they are not kidding this cncf landscape graph shows a ton of Open Source projects that you can use but don't worry you don't have to use them all the challenge really is to identify which one to use in the context of what you want and try to achieve and this concludes this lecture on cloud native head to the cncf website for more info let's go deeper in the cloud native Concepts Cloud native is about Speed and Agility the user wants new features right away without any downtime and the business wants faster release of feature to stay competitive a cloud native application architecture starts with clean code using domaindriven design techniques Markle services and kubernetes this course is all about microservices and kubernetes feel free to explore the concepts of clean code and DDD on your own with Cloud native we need to change mentalities infrastructure becomes immutable and disposable it is provision in minutes and Destroy as fast it is never updated but it's replaced with newer versions traditionally we would care about our virtual machines we would patch the OS update the apps with containers we create newer version with the software updates destroyed the previous running ones and replace them with the newer ones so the containers that you'll run will be more like cattle than pet of course this Cloud native thing is a lot easier when starting a new project a blank page or a green field however it's still possible with Legacy projects I really like the cloud native Foundation trail map because it breaks the journey to Cloud native into smaller measurable objectives you can set your own performance indicator to measure each steps to ensure a smooth Journey so let's take a look at the first steps your team must first learn how to cantonerize your application the developers and the IT Pros must know how to deploy and monitor containers you need to automate deployment through the use of continuous integration and continuous delivery techniques and tools you need to use an orchestrator like kubernetes and maybe deploy your application using L charts then you need to add observability so you can understand what's happening in your kubernetes clusters and be reactive use tools like service meshes to provide more functionalities inside your cluster Implement security through policies and wow these were just the first six steps Now understand that you don't have to implement all of this and especially not at the same time I really like this trail map because it breaks the journey into smaller steps that the management can understand and measure and this concludes this lecture on the cloud native Concepts let's take a look at the cncf website the website is located at cncf.io and from there you can take a look at the various projects maintained by the cncf information about on how to get certified Community Information like the conferences that the cncf organized each year so let's go back to the projects menu and you'll notice that projects are categorized in three categories sandbox incubating and graduated let's click here on this the second menu here and let's scroll down to the bottom and here you get the information about the meaning of these three categories and basically that's their maturity level sandbox projects are mostly newer projects uh well graduated projects our projects set conservatives Enterprises are more likely to use so let's take a look at the graduated ones we found here kubernetes Helm Jagger so let's click on kubernetes and basically that will show you the the project website let's take a look at the incubating ones here we find Linker D grpc let's click on grpc you can get more information about the grpc all right let's go back to this menu remember the trail map that I mentioned in the previous lecture well here it is cloud native trail map here's the nice diagram you can get more information you can get send that to friends colleagues and here's a link to the landscape diagram here it is it's super huge Let's uh let's click here on kubernetes and here we get very interesting information well you get the repository where the project is uh is stored you get the number of stars and the activity number of commits here you get you get the website address and also the Twitter handle here so you should follow the Twitter feed of the project that you're using so let's close this one let's click here on hell again website uh repository number of stars activity and the Twitter handle and this concludes this look at the cncf website thank you let's now talk about the containers Concepts containers containers containers they are everywhere but what are they exactly a container is a unit of deployment it contains everything needed for the code to run so the compile code the runtime system libraries and this is system tools you take a container push it on a server and it should run of course it can have some external dependencies like a database or a cache but the code deployed in it should run as is so why use containers because it is faster to deploy something small than something big like a complete monolithic system uh they use fewer resources they are smaller and since they are smarter you can fit more on the same server when using cicd techniques they are a lot faster to deploy you can run them anywhere and they are isolated from each other meaning that if one fails it will not take the whole system down with it so what exactly is virtualize let's compare virtual machines with containers a VM runs on some kind of Hardware where an OS is installed the OS hypervisor will let you create a virtual machine where you will install in OS so basically DVM virtualized the hardware and what's happening when a VM starts well you see the BIOS coming up and then the OS boots up and what about the size of that VM let's say we have a Windows Server VM it can take 12 gigabytes of RAM and 500 gigabytes of hard drive space and how long does it take to boot well depending on multiple factors something like 5 to 10 minutes now let's compare that to containers we still have the hardware and the OS of course there's a container runtime installed uh in the OS and containers images are run in memory now compared to a VM a container does not have to boot because it will use the host OS kernel this means that container starts in seconds because they don't have to boot they also use a lot less memory and hard drive space since there's no OS a small container can take a hundred megabyte of hard drive space and run in 64 or 100 megabyte of RAM so VM and containers virtual machine have a larger footprint they are slower to boot ideal for long running tasks container are lightweight they're quick to start they don't have to boot they're portable and they're ideal for shortlived tests because they you can spin one super fast so are containers replacing virtual machines our virtual machines obsolete absolutely no containers are just another tool in your toolbox and you need to find the right use case for them and also for VMS if you're old enough you must remember what the telephone booth is if not well before cell phones we used to make phone calls in these spoons by dropping a dime or a quarter anyways using a telephone boot analogy you can pack more containers on the same server than what's possible with virtual machines containers are made of layers you start with the base OS add customizations and add your applications let's take a deeper look at this screenshot the docker pull command retrieves and download a container image as you can see each layer is downloaded individually notice that each has a unique ID and that for the first ones Docker says that they already exist why is so Docker uses a local cache to store the container's images and if a layer already exists it will not be downloaded again the benefit is that if you pull version 2 of an image Docker will only download the layers not present in its cache one of the goal when creating container images is to create them with the smallest number of layers possible later on we'll see techniques on how to achieve that now can you write on these layers well no except for the top one because it is read right the lower ones are read only another concept is the container registry it's a centralized repository where you deploy the container images you create think Gita but for containers Docker as one called Docker up that provides public and private repositories and all major Cloud providers have container registry services the last container concept is the orchestrator an orchestrator allows us to manage scale monitor the containers that we run on our servers you can install your own or use a managed cluster offered by one of the cloud providers like AWS Azure Google Cloud we will come back to the orchestra Concepts after we have a better knowledge of containers and this concludes this lecture on the containers Concepts let's Now understand what is docker so what is stalker that may seem like a simple question but there's more to it there's Docker the company and Docker the platform document thinks the Mobi project an open source container runtime that follows the specs from the open container initiative doctors sold its Docker Enterprise division late 2019 to a company called mirantis so if you want to buy Enterprise support or get certified with docker you have to go through morentis Docker provides a container runtime that runs on Mac windows and Linux a command line tool to create and manage a containers a Docker file format for building containers and interestingly Windows lets you create both windows and Linux containers foreign if for some reason Docker doesn't seem to work on your machine try restarting it by using the restart menu from the system icon on Windows or by clicking on the debug icon in Docker desktop on Mac and windows and clicking on restart Docker desktop is very stable but I had some issues when my laptop was coming back from hibernation but that was a long time ago and I haven't had issues for a while and this concludes this lecture on docker the easiest way to run Docker on your machine is by running Docker desktop it's a free download available on docker.com so you click here on get started and you download the version for your OS so Windows Mac Linux here now if you're running Windows uh check the version of Windows that you're running if you're running Windows 10 version 2004 or a later version you can run what what's called windows subsystem for Linux version 2. so wsl2 basically it allow you to run a Linux distribution right into your Windows installation so Docker desktop can run its container by installing a virtual machine inside hyperv or if you have wsl2 install it will it will install that virtual machine inside the Linux distribution and everything will be a lot faster so that's the preferred way so just to prove a point here I'm going to launch my hyperv miniature and as you can see I'm not running any virtual machine so my Docker desktop installation uses wsl2 you'll find a link to this installation guide in the modules notes okay so let's take a look at the docker desktop I'm running Windows as you can see I can see my Docker desktop system tray icon here so for Mac User it'll be at the top of the screen of course I can right click on it and let's select dashboard all right so here I can see a list of um containers that are currently running I can stop them restart them delete them I can see a list of uh images that are installed on my machine here we'll come back to that later on there's a gear icon here that's the setting icon and in the general section here you can see that wsl2 is enabled so that's why Docker desktop doesn't use a Now preview virtual machine it uses the wsl2 to run the VM and there's the kubernetes menu here if I select it I can see that kubernetes is enabled so when you check that Docker desktop will download additional containers to run kubernetes right onto Docker desktop here so very useful there's a bug icon here which is the troubleshoot icon here so if at some point you're issuing Docker commands and the don't work or something's wrong uh running Docker Docker commands you can click here on the restart button here you can see my name here it means that I'm currently logged and if I right click down on the system play icon you can see that I'm currently logged in and I have the option to to sign out so what uh username and password did I use to to log in when you downloaded the uh the docker desktop you could have created a Docker account or a Docker Hub account so that's the same username and password so if you go to up.ducker.com and you logged in well that's the same account that is used here in Dr desktop thank you let's take a look at your first Docker CLI commands now throughout this course I will introduce you to various commands I will list them in what I call a cheat sheet list like what you find on the slide I will briefly explain what the commands are for and that will be followed by a concrete and Zone demonstration so when you install Docker desktop on on your Mac or PC it also installed the docker CLI tool our first command is Docker info this will display some information about the docker installation on your machine Docker version will display its version and Docker login will log you into a Docker registry by default this login command will log you into Docker Hub the registry from docker and this concludes this lecture on the docker CLI alright we need to open a new terminal window into Visual Studio code so let's select a terminal new terminal or you can use the shortcut Ctrl shift back tick key all right this will open a terminal window and let's take a look at the commands that we will run they're really basic it's just for testing that our Docker installation is working correctly so let's type Docker info this will give me some information about my current installation what's happening I have 47 containers three stuff 44 running 25 images and so on and so on so information that that is quite useful for debugging purposes I can see that the virtual machine running that running Docker desktop is running as to CPU it has two gigabytes of memory allocated all right sounds good let's uh type now Docker version and this will uh give me some information about the version number of different parts of Docker desktop so again useful for debugging purposes these are not commands that you will run on a daytoday basis but they're quite useful for troubleshooting the last command is Docker login so I'm just going to type Docker login without any username password see what's happening and well the command says that I am login successfully why is so well if I right click here on my Docker desktop I can see that I'm already logged in so that's why I didn't have to input or type any username password I was already logged in let's now see how to run containers the docker pull command lets you download an image from a registry Docker run will execute an image in memory as a container if the image is not present in the docker local cache it will be downloaded automatically using run with the Dash D flag will run the container in the background giving you your comment prompt or terminal back the start command will run a container in the stopped state Docker PS will list all the containers currently running and add the dash a flag to also lists all the stopped ones Docker stop will stop a container running but the container will still be in memory we will see how to remove them from memory in a few minutes Dr kill will well kill a container that might be stuck in memory you usually don't use this comment but it's useful to know Docker image and spec will give you some information about an image very useful for debugging purposes so you may notice that we have two parameters here one is called image name and the second one container name so what's the difference the image name is the name of the image as you find it in the container registry and the container name is the name of the running container so you run an image using its name and then interact with it using the running instance name the Run come in as an optional flag call dash dash name that lets you specify your name if you don't specify one Docker will Auto generate one for you you can set limits on the memory and the CPU that the container can use when using the Run command so how do you run a container using the docker run command you specify the image name as found in the container registry you specify a name for the running instance and with the published flag you map a port from your local OS to the port that the container is listening to you can list the running containers using Docker PS notice how we stop the container by using the running name and not by using the image name then we remove it from memory using the remove or RM command containers are not black boxes you can attach a shell to your terminal and run commands that will execute right inside the running container by using the dash it switch and the name of the program you want to run a Windows container well you can run Powershell using the docker container exact command you can attach to a running container here's a screenshot showing the docker run command and notice the terminal problem changing when attached to a container so how do we clean up things the remove command lets you remove a container from memory but first it must be in the stop state for the command to work here's the remove command getting a list of the stopped container and removing them all the images that you pull will be cached locally you can get a list of these images using the docker images command use the remove image or RMI command to delete an image from your machine after a while you may end up with a bunch of images to do some spring cleanup I use the system prune command this will delete all the image currently not in use so be careful using this command and this concludes this lecture on the docker CLI let's now run our first container we'll run an engine X web server so something pretty basic that'll be perfect for this this lab all right let's open a terminal so terminal new terminal or Ctrl shift back tick perfect so we'll run disk command so let's take a look at the command first so I'm going to run a Docker run and let's go at the end of the command this will be the image that will run so an nginx image we'll give it a name so the name will be web server that will be the name of the running instance will map local All Sport 8082d or that the container is listening to so port 80. n d 4G decimal so we can get our Command Prompt or terminal prompt back so let's run this ha something interesting is happening unable to find image nginx latest and you see that the docker as pull all the different layers locally here so now if I issue a Docker PS it will list the containers that are currently running so I have uh three containers running so the kubernetes dashboard and the metric server here so don't uh look at these let's focus on this one so this is the containers it is that we just launched the nginx image here it started 30 seconds ago we can see that the port 8080 local levels is mapped to the port 80 here and the name is web server so let's launch a web browser and let's type localhost 8080 the container is actually running fantastic all right so we can get a list of the images installed on your machine here by using Docker images so I have a bunch I may have a lot more than you but let's focus on the last one here uh nginx the tag is latest image ID and this is the uh the size all right fantastic let's try to connect to uh to it so we'll issue a Docker container exact we'll give it the the name so the running instance and the program we want to run so look at the The Prompt now root at some some ID so the ID is the actual container ID so I'm logged in as root on that container so now I can issue some some commands so let's do in the last let's thy fell less been to see what's in there so different commands so I'm connected to to that running container that's pretty uh pretty cool so I can issue commands look at the logs if any and do some troubleshooting so this is super useful for debugging purposes uh we'll use that a lot in the various steps that we'll do together all right let's get out of there by typing exit all right you can just clear my screen here so our container is running how do we stop it we use the docker stop command but look at the the parameter that we use the name of the container that we use we use the running instance name not the name of the image so that's pretty important here Docker stop web server all right but the container is still in memory if I do a doctor PS it's not listed anymore as you're running container but if if I type darker es Dash a ha for all I can see that my my container is still in memory here so I need to remove it from uh from memory so we'll use the docker RM and the name of the running instance so RM for remove and now the container is no longer in memory awesome now the container is still well the image that was used to create the container is still on my machine so if I type darker images you see it's still it's still here so that takes 133 megabyte of this space if I want to get rid of that I use the RMI so remove image command and the name this time of the image not the name of the running instance because none are running right now and you see all the layers have been deleted let's now see how we can build containers Docker build that you create an image using a Docker file if you run the command in the same folder where the docker file is located simply use a DOT as the file name if the file is located in a different folder specify the location using the F flag the tag command let you assign a name to an image this tagging has two parts a name and a tag the tag is usually used to specify the version number so what is a Docker file well it's a text file listing the steps to build an image here's the simplest Docker file I can imagine two lines the from command specified the base image when building new images you always start from something already existing in this case an image with the nginx web server using the Alpine version and then the copy command copies everything from the current folder to a folder inside the container using the build command we create a new image specifying the docker file remember to use the dot when the file is located in the same folder here's another one this time a little bit more complex it is used to create an image running a node.js app let's take a look at it the from command specify the base image using the Run command we run the package manager inside the container to install node.js next we copy all the local files into a folder named SRC inside the container we use the Run command to do an npm install we then had some metadata in this case we tell the container to listen on port 8080 and finally we tell the container what to run when starting so as you see this Docker file contains the stats needed to run our node.js app we saw what tagging was a moment ago let's explore this again using the docker tag command we name an image using a name and optionally a tag if you don't specify a repository name it will default to Docker Hub later on we'll see how to push images to different repositories and we'll have to specify the Ripple's Journey name when tagging our images and this concludes this lecture on how to create Docker images let's see how can Visual Studio code help us build and run containers so what is visual studio code and why talk about this tool in this course well it's a text and code editor it's free and open source it runs on Windows Mac and Linux and you can download it for free using this link so you will work a lot with text files creating Docker files and later on Docker compose and yaml files a tool like vs code will help you because you can install plugins that will make your life easier using a different text editor no problem in vs code you can install plugins by clicking on the extension icon in the left menu then you search for Docker and install the extension from Microsoft the extension lets you add Docker files to your projects using the command palette open it using the view menu or type Ctrl shift p type Docker add and select add Docker files to workspace the extension will ask you a few questions and we'll create the docker files for you vs code has a builtin terminal where you can type commands or you can run commands using the command palette here's another example when running a container and when using the command palette there's no magic there the extension will simply issue a command in the terminal but sometimes it's a great way to learn creating Docker files is okay but what I like the most is the UI provided by the extension helping me manage my container so if you click on the docker icon you can see the images installed on your computer and you can even see the containers currently running right click on an image to manage it same thing for the containers currently running very very very useful and this concludes this lecture on vs code thank you let's now use Visual Studio code to containerize a node.js express application so I already installed the docker extension in Visual Studio code but let's take a look at it so if I click here on extensions and let's search for docker here it is that's the one from Microsoft almost 6 million install at the time of recording so that's the one all right so let's go back to our files so I have my node.js application here if I click here on package.json the name of my application is my Express app so that'll be important in a few seconds all right so let's first add the docker file to our project so we'll use the tooling provided by the extension to do that we'll go to the view menu command palette or you can use the shortcut Ctrl shift p right and we'll type Docker add and there are two options here Docker files or Docker compose files so we haven't looked at the docker compost file so let's select the first one Docker files the extension hack is asking us about the application platform so it's a node.js application but it can generate Docker files for net python Java C plus plus go and Ruby apps so let's select node.js all right and next next it's asking us where is the package.js file so here it is it set the root of my application so I'll select this one the port that the application is uh listening to 3000 perfect and do I need the optional Docker compost file no not at this time all right so the extension quickly added a the docker file and also the docker ignore file and also a folder for vs code so let's take a look at the docker ignore basically it's a list of files not to deploy in our container and the docker file has been created for me to use a node base image it copies everything into the Container the image and expose port a portrait 3000 perfect so let's now build this image so we'll go back to the command palette view command palette Ctrl shift p and this time we'll search for Docker build all right here it is Docker images build image all right so I just issue uh the command and look look at what's happening the extension is not a black box uh issuing some crazy or strange uh commands in the background it's just issuing a a Docker Bill command here and remember a few seconds ago I mentioned that the name of the application in the package.json file is is important well that's the name of our uh of our application here so the extension use it to generate the name for my image all right it's just a Docker Bill command nothing fancy but using this extension is a great way to learn about some uh some commands that you may want to use at a later time from the terminal prompt okay terminal will be reused by test press any key to close it let's press any key all right the image has been built let's run it now so view command palette okay Doctor run so we'll use the first one and uh there's a list of my images and here is my Express app perfect run that select image that's the latest the tag latest perfect and look at what's happening Docker run sport 3000 to the port that the container listening to so nothing fancy here is uh it's a comments that you can type yourself but uh the extension to that are for you so if I start my browser local ocean I go to Port 3000 there it is so that worked perfect let's now close this let's now use the UI provided by the extension so in the left menu let's click on the docker icon here and from there you have a list of the running containers the images and the different Registries that you connected to all right so here's our Express hat my Express apply it is and it's running name the instance name is called magical underscore Direct uh why is so well when we issued a Docker run command using the command palette the extension didn't provide a name so uh one was generated automatically by by docker so from there I can view the logs attach shell inspect open in a browser so if I select that there it is that's my app listening on Port 3000 uh I can stop it restart it remove it so let's uh let's remove it here so are you sure you want to remove container yes it's not in memory anymore it's not running anymore but if I look at the list of the images that I have here it is my Express app so I can right click on it and I have a few options I can run it inspect it pull push tag and even delete it so let's let's run run it okay and you see at the top my Express app and look at the name gifted underscore El beckyan because if you look if you uh inspect the docker command that was issued by the extension there's no name provided so Docker generated one for us all right so let's close this we can stop or remove it from memory and if we no longer need the image we can just remove it here are you sure you want to remove the image blah blah blah okay let's remove it and it's gone so what's happening in the background is that the extension is simply issuing a Docker remove image RMI to to remove the image from from this so nothing nothing really fancy the extension it's not a black box that is issuing some strange commands it's just regular and Docker commands but I really like the the UI provided here and so instead of uh typing uh okay let's uh let's try this let's um let's clear this and let's type Docker images here's a list of my all my images but here I have a nice UI listing all the the same images here so sometimes it's it's easier having a little UI to help you accomplish some tasks other times it's easier issuing the commands from the terminal or the command prompt so it's up to you you can use the UI or The Terminal thank you let's talk about data persistence containers are ephemers and stateless so you don't usually store data in them of course you can write data in a container but if you destroy one or if it crashes any data stored in it will be lost so it's okay to write some log files or scrap data that you don't want to keep as long as you understand that you will lose these files at some point in time to purchase data you need to store it outside the container in what we call a volume a value Maps an external folder or even a cloud storage service to a local folder inside your container so your app sees a volume just like any regular folder the OS in this diagram represent the server or the virtual machine where the container is running as you can see a local folder is mapped to the VM file system so they are stored in a volume where survive a container restart or crash there's still a chance that we can lose the data if the VM crashes so later on we'll see how we can use some type of external storage provided by the cloud provider but first thing first in the next lecture you'll see how to create a volume that maps to a VM file system and this concludes this structure on data persistence um let's see how to create volumes here's a cheat sheet listing the docker commands for managing volumes Docker create volume will create a new volume LS will list all the volumes volume and spec will get you information about a volume remove will delete a volume destroying all the files storing it and volume prune will delete all the volumes currently not mounted or not in use so be super careful careful using this command alright you first need to create a volume using the docker volume create command then when you run a container you need to use the V switch or to volume a parameter specifying the volume name a column and the name of a local folder that folder will be a logical folder in your code so your code will see just like any regular folder if you use the inspect command you'll see The Logical folder location in DVM instead of using a volume you can specify a local folder this is great for testing purposes let's say you started developing your service and that you want to test on your Dev machine if your code can read and write some files correctly you can use this kind of mapping but don't use that in production using the inspect command you can see the local folder path and this concludes this lecture on volumes a software system data outside of a running container so to do that we'll use a value so I'll open a terminal and we'll use the docker volume create and the name of the volume so this will create a volume perfect let's Now list the volumes on my machine so there's a few here the first four were created earlier this is the volume that we just created now let's run a ninja next image and let's attach a or Mappy a local folder to that volume so we'll use Docker run Dash D for detach we'll name our instance fault test with the Dash D for volume we'll use the volume name that we created earlier and map that to a folder called ATP on our nginx image all right to that okay excellent so that worked now let's connect to our running instance so Docker exec Dash it the name of the instance and it will run bash here perfect let's first do an LS to see if we see the app folder there it is so that's that folder is mapped to the volume so anything that we store or write in that folder will be written externally outside of the container so that will persist all right uh just for fun instead of doing a cat Let's uh install Nano inside the running instance so I'll do first in aptget update remember this is uh this is running inside the the container and we'll install Nano app get the install Nano which is a small editor perfect LCD into the app folder in open Nano so Nano and we'll create a file called text test.txt hello volume all right and we'll use Ctrl o to write to to disk Ctrl o enter and control X to exit Nano perfect if I do an LS here I should see my file there it is test.txt so let's exit the running instance and what we'll do will stop the running instance and we'll remove it from memory stop it here and we will remove it from memory with our m okay so now the container or the instance is gone if we would have stored some data inside that container it would it would be lost at this point but we use a volume so the data was stored externally so let's try to create a second instance same thing using the same volume and let's exec to it perfect let's see what is in the app folder here is our file so let's do cat test.txt just to prove hello volume that work let's exit here this proved that by using a volume your data persists a container restart or crash here the data is still there until I remove the volume so if I issue a Docker volume removed and the name of the the volume you see doesn't work why the error says that the volume is in news interesting so I need to stop any container instance that is that is running and remove it from memory before deleting or removing a volume let's try uh again term Docker volume RM for remove by volume and this time it worked now another thing I want to show you let me create it again perfect and let's switch to the docker UI here if I click Docker here there's a section that list of the different volumes here so my vowel is listed here so here I can inspect I cannot look at the files but I can I can manage it so I can click here on remove that will delete the volume are you sure yes let's now see the yaml concepts yaml stands for yaml ain't markup language it's a way to serialize data so that it's readable by human beings it's the file format used by Docker compose and kubernetes here's the sample yaml file for a key value pair you specify the key a column data space and a value don't forget the space it's mandatory here are some nested values you specify child values using two space indentation and quotes are not needed for string values here's a list again the child elements are indented with two spaces and there's a space after the dash this is what we call the block style there's also a flow style that looks like Json so you may be tempted to use it if you're familiar with Jason but don't I never saw any sample or any documentation using this flow Style since it's easy to forget a space and you can spend quite some time figuring out why your yaml file doesn't work you can use tools like this linter available on yamlin.com it will parse your yaml and flag any errors very useful and this concludes this lecture on the yaml concepts let's now take a look at the docker compose Concepts let's say that your app is composed of multiple containers you run the frontend container using a Docker run command Docker run again for the backend container and again for the redis cache container so you end up issuing multiple Docker run commands to run your app would it be nice if you could deploy your app using one single command well that's the docker compost go to Define and run multicontainers application using a single yaml file there's a compost plugin that extends the docker CLI and let you run those Docker compose files these specifications are available here if you look at Docker compose before you may have seen that sometimes the commands are using an iPhone so Docker Dash compose and sometimes they do not why is this at the dockercon conference in 2022 Docker announced the general availability of couples version 2. okay this means that there was a V1 before and the V1 command line tool was installed separately from the docker CLI it was built using python so you needed to have python installed to run compost V1 and the syntax was Docker Dash compose couples V2 is a drop in replacement meaning that all the V1 commands are working as expected it's installed as the docker CLI plugin automatically by Docker desktop to use it you type Docker space compose no hyphen needed here it's written in go so no need to have python installed to run the command in summary it's simply a faster version of the docker compose tool shipped as a Docker plugin instead of a python application here's a Docker compose file there are three containers defined in it web API 1 web API 2 and API Gateway the name that you use here defined the network or host name for that container the code running inside your container can use these hostname to communicate between each container for each of them you specify the image to run you set the internal and external Port the container will listen on note that the API version is now optional so it's okay to skip it you may ask yourself should I use darker compose or not Docker compose is perfect for small workloads that don't require a full orchestrator like kubernetes it's perfect When developing and testing locally before deploying to kubernetes Some Cloud providers offer services that support Docker compose like app service on Azure and ECS on AWS and of course you can simply use a virtual machine or a VPS virtual private server with the digitalocean or linued and this concludes this lecture on the docker compost Concepts thank you LaSalle used Docker compose here's a cheat sheet listing some of the docker compose commands Docker composed build lets you build the containers as defined in your Docker compose yaml file if the file is located in another folder you can use the optional Dash F per meter and specify the files location start we'll start all the containers as defined in your yaml file step will stop them but they'll remain in memory up we'll do a build followed by a start this is super Endy use the Dash D parameter to run the command in the background and take back your terminal prompt PS will list what's running remove our RM we'll remove the containers currently from memory down we'll do a stop followed by a remove again this is super ND logs will display the logs for a container and you can open a session inside a container by running Docker compose exec the container name and the program to run the docker compose file is located inside a folder and if you run Docker compose up this will launch your application if you try to run a Docker compose up a second time from the same folder nothing will happen because the application is currently running if you want to run a second instance of your application it was impossible with Docker compose V1 but with V2 you can use a project name to launch a second instance of your application from the same folder here's the cheat sheet for some of the new commands Docker compose dash dash project Dash name followed by a project name this will run a then sense of the application as a project the shortcut is much shorter to type Dash p instead of that Dash project Dash name you can list the project currently running by using compose LS CP will allow you to copy files from the containers so this is super ND to retrieve let's say log files and you can copy files to The Container so from your machine your desktop or laptop to The Container by using Docker compose CP The Source pad the container ID and the destination path here's an example imagine that the docker compost file is located in the same folder where you run these commands you simply use the up command to build and run the containers and to take them down simply use the down command and this concludes the structure on Docker compose in this lab we will deploy a Docker compose application let's take a look at our Docker compose.yaml file we have one section called services and under that section we Define two Services the first one is called Web Dash Fe F4 front end it's a python application and instead of using an image from Docker up will be building that image using the build parameter the dot means that the docker file is at the same level as the docker compose file so here it is it's a simple python application just one file app.pi and requirement.txt that we're copying on that base image it will be listening on Port 5000 and this is our second service it's the redis cache and this time we'll be using an image from Docker up all right let's open eight terminal and let's build the image using Docker compose build perfect my image was build now I can launch the application using Docker compose up and Dash D for detach now I could have skipped the build step because up does a build first and then a start so it's super ND so let's use Docker couples up and my application is up and running I can test it if I go to local OS 5000 you visited me one time and do a few refreshes five time perfect okay I can list the running containers using Docker compose PS I can also use Docker PS since it will list the docker container currently running and I can look at the logs for my frontend service using Docker compose logs Dash F and the name of the service and if I move that a little bit and it F5 a few times you see new entries are logged perfect that works let's do a Ctrl C to terminate the log streaming and we'll use Docker compose LS to list the currently running projects I have one project running it's called l0904 Docker compose well basically when I use Docker compost up I didn't specify a project name so Docker compose use the folder name as the project name now let's try to create a second instance of our application if we use Docker compose up Dash D again well Docker compose tell me that the application is running so it will not start a new version we can try to deploy our second version using a project name so Docker compose Dash p for project name we'll name it test up Dash D let's see what will happen starting oops we have an error here hmm bind for local OS 5000 failed Port is already allocated of course are my local old sport 5000 is in use right now so what I need to do I need to change that Port here the localhost port I'll use 5001 save the file and use the same command Docker compose Dash B project name test up Dash D and this time it worked awesome let's open our browser and let's go to Port 5001 yes that worked a few refreshes 10 times let's go back to Port 5000 the first instance 15 times all right so let's do a Docker compose LS again to list our projects now we have two projects the first one that we deploy without specifying a project name and the second one with the project name test let's delete our for instance by using Docker compose down so I didn't specify a project name Docker compose use the folder name as the project name now if I do the docker composer list I should have only one project running yes it's test now I can delete that one using Docker compost specifying the project name and down let's list the projects again Docker composer less nothing running PS listening the containers nothing and why not try Docker PS nothing in memory in this lab we will deploy a Docker compose sample application that is composed of three services a web frontend build with react nodejs backend and the Maria DB database let's take a look at our Docker compose file this is a more complex than what we saw so far so let's try to break it into smaller pieces here we have the definition of our tree services if we start from the left we have the backend service top right the database DB and the front end now if you look at the DB service you can see that we're referencing an image that will pull from Docker hub and the other two Services we're using the build parameter this means that we will build these two images looking at the backend service we can see that we specify build and context contacts with the value of backend backend is actually a subfolder where the docker file is located next we're defining two networks public and private we can see that our frontend service is using the public network the backend service is using public and private and the database service the DB service use only the private Network front end being in the public network cannot communicate directly with the DB service but backend being in both public and private can communicate with both DB and frontend services we're defining also two named volumes back in dash modules and DB Dash data and we can see highlighted in yellow that our DB service is using DB data and our backend service is using backend modules highlighted in yellow we also see that we're using other volumes these volumes are scoped at the service level and are not shared between services to create an instance of our Docker composed application we simply use Docker couples app and to bring it down Docker compose down in Visual Studio code let's take a look at the docker compose file so it's called compose.yaml here we have our services section Network section where we Define two networks the name volumes backend modules and DB data and secrets that we haven't seen yet where we're defining one key value pair so the key is DB password and we get the secret from a file the file is located in the folder called DB and it gets the value from a file called password.txt if we open the DB folder here's the file with the value the secret and that will be injected when we run Docker compose up okay let's go back to the services section and here we have our backend DB and frontend Services let's take a look at backend we're using the build a directive and we are setting the context to backend it points to this folder called backend here and the docker file is located inside that backend folder with the application same thing with the frontend service build context frontend and we're getting the docker file and the application from the frontend folder and what about the DB service well we're using an image that will pull from Docker hub okay let's build our two images we'll open a terminal and run Docker compose build our images were billed perfect now let's run the application by using Docker compose up Dash D ER perfect the application is this thing on Port 3000 so let's open a browser and type local OS 3000 is our react application it's working awesome we can list the containers that are currently running by using Docker compose PS we should have three backend DB front end awesome let's take a look at the logs from the backend service Docker compose logs Dash F backend and these are the logs for our backend service awesome we can type Ctrl C to stop the log streaming and we can take our application down by using Docker compose down this will stop and remove the containers from memory do we have something else in memory we should not perfect and even if I type doctor PS there's nothing however the volumes are still there when you do a doctor compose down it will remove the containers from memory but will not remove the volumes so if I open the docker desktop application and I click on volumes here I can see that I have a few well three volumes that were created a few minutes ago so I need to delete them manually I can select them right here and click on delete or I can do the same thing by clicking on the docker icon in vs code locating the volumes here and delete them find it easier to do that in Docker desktop because we see when the volumes were created so I'm pretty sure that these three are the ones that I need to delete I'll click on the delete button and confirm thank you let's take a look at some of the compose file features it's a good practice to set limits on the resources that your container will use in this example I light it in yellow we tell Docker to start the container with a quarter of a CPU and 20 megabytes of RAM the green section is the limits that we are allowing in this case half of a CPU and 150 megabytes of RAM to set an environment variable that will be injected in the running instance simply set the key value pair in the environment section those values can be overridden at the common line using the dash e per meter you can reference an environment variable using the daughter curly bracket syntax this way you can set the variable on your machine or server and use it directly in the compose file you can place the values in a file that you will name dot EnV located in the same folder as the compose file the compost command will automatically read the values from that file by default all containers specify in a compose file will see each other using their service names here we have two Services web and DB the code running in the web service can communicate with the second one using DB as the hostname and vice versa the web container is visible from outside of the docker network using the port number configured in the left portion of the ports value web is listening inside the docker Network on Port 80. DB can reach web on Port 80. finally DB only exposes one port number that's the internal port web can reach DB using Port 5432 but DB is not visible from outside the docker Network if you have a compose application with multiple containers you can restrict who sees who by configuring networks in this example we're defining two Networks front end and back end proxy can see app because both are part of the frontend Network however proxy does not cdb because it's not part of the backend Network when using multiple containers you may want to start some of them first and wait until they are running before starting the other ones a typical use case is a database that you want to run before sorting the main application doing so is easy using the depends on parameter where you simply specify what is the service name that the service is dependent on in this example app depends on DB so compost will first start DB and when DB is running compose will then start app you can declare volumes in the volumes section these are called named volumes and they can be used by all these services that you are declaring in the compose file to use a volume from a service map it to a local folder using the volume name colon and the virtual path inside the container optionally you can make the mapping readonly by appending colon Ro to the mapping you can also create a mapping without using a named volume this mapping can't be shared across services it's also a good practice to set a restart policy let's say that you deploy your couple's app in a VM and at some point you need to install some OS batches and you need to restart or reboot the server what will happen to your compost app well if you don't specify a restart policy the one by default is no meaning that compose will not restart the containers if they were shut down by reboot you can set the policy to always this way compose restarts the containers until their removal on failure resource a container if the exact code indicates an error and lastly unless stop does a restart unless you stop or remove the containers and this concludes this look at some of the docker compose features let's now talk about container registries so what is the container registry it's a central repository for container images you build an image locally then you push and store the binary the different layers 2D Repository they can be private or public the default one is Docker hub Microsoft AWS and Google each offer container Registries as service the benefit of using a repository from your cloud provider is that the images are located near your app so no apps over the internet to retrieve the images so let's say we want to retrieve an image from Docker up we issue a Docker pull command and Docker downloads the images layers and store them in its local cache and this concludes this lecture on container registries let's see how to push and pull images to Docker up make sure you are logged in with your Docker user account to be sure simply type Docker login without a username and password Docker will tell you that you're already logged in if not enter your Docker username and password you need to tag an image with the repository name by default it's your username if you have created some organization in Docker hub prefix it to the name of the image in this example I want to push this image to my Kates Academy organization then use the push command and don't forget to specify the organization name it's part of the image name to retrieve the image we use the pull command with the image full name on Docker up public images are available for download to anyone if you don't want to share them you need to create a private Repository later on we will create one using our cloud provider and this concludes this lecture on Docker up let's now push our first image to Docker hub first thing first let's make sure that we can log in into Docker Hub so if you head to up.docker.com make sure you can log in also if I right click here on my Docker desktop icon I can see that I'm logged in perfect we will containerize a node.js Express application so we'll first add the docker file we'll use the tooling so view command palette if you can type Docker add and Docker files to workspace this is a node.js application the package.json file is located at the real so that's the correct one it's listening on power 3000 and we don't want Docker compose files perfect now we need to build the image so let me open a terminal perfect and we need to issue a Docker build command with the Dash D for tag parameter but notice here we need to prefix the uh the name of the image with our registry name if I select this I need to prefix that with my name here the name of my registry let me run that the image was built successfully now I need to use the push command to push that image onto Docker Hub again I'll select this and let's replace the registry name with mine use your home and see what's happening here Docker is pushing each layer to my Docker app account here all right no errors things went fine let's go back here I'm gonna refresh this page and here it is here's my um uh image here my Express image so I can click on this I can edit the um the information I can see that I have my tag is V1 right I can get more information here I can here click on public view so by default the repositories on Docker Hub are public anyone can download and view and download your images this is the view for someone who would look at my image so with the pull command Docker pull let me go back here I can see the tags and so on and so on and if at some point you want to delete this you go into the settings tab here you scroll down and you can delete that repository here all right let's go back now let's try to pull that image to our computer here the first thing uh I will try to do is remove it from my computer let me type this RMI so remove image the image has gone completely so now let's try to pull it from uh from Docker uh okay Perfect Pull complete my image is is back here now you see that I use the V1 tag here to to tag my image with the with the version number so let's try to build a version 2 of that image let me copy that and again let's replace this part this placeholder with my registry name image has been built so let's now push it two Docker uh same thing as we did before but this time we're pushing version two okay let's go back here let's go back to my Express or the general tab look here I have V1 and V2 if I click here on tags I can see when the image was was pushed tags and people can download V1 or V2 if I want to remove my images I use the RMI command so remove image I'll remove V1 and also I remove V2 and on Docker up if I no longer need this Repository I click here settings scroll down a little bit click on delete Repository and I need to enter the name of my repo just to make sure and click on delete and now it's gone time to introduce kubernetes so kubernetes or also known as Kates so the letter K followed by the number eight so eight letters and then the number s and it's pronounced Kate's so kubernetes is a project that was created at Google version one came in July 2015. it was the third generation of container scheduler from Google previous projects were Borg and Omega and Google donated the kubernetes to the cncf so now the development is supervised by the cncf is currently the leading container orchestration tool it's designed as a Loosely coupled collection of components for deploying managing and scaling containers it's vendor neutral so it's not attached to a single company and it runs on all Club providers and there's a huge community ecosystem around kubernetes so what kubernetes can do service Discovery load balancing it can bridge to the cloud providers storage services can provide rollout rollbacks capabilities and can monitor the health of the containers can manage configuration and secrets and the same API is available either in a onpremising solution or in every cloud provider so what can't kubernetes do it can deploy or build your code and it does not provide application Level services like databases service buses caches here's a quick look at the kubernetes architecture this diagram was taken from the kubernetes documentation we'll take a closer look at each component but for now let's just say that it's composed of a master node also called the control plane so that's the portion to the left and the control plane runs the kubernetes services and controllers and you have the worker nodes these node runs the containers that you'll deploy in the cluster so a container will run in a pod a pod runs in a node and all the nodes form a cluster and this concludes this intro to kubernetes let's see how you can run kubernetes locally so do you need to install a kubernetes cluster in the cloud or ask your it Department to install one in your Enterprise so you can test locally absolutely not there are many ways that you can run kubernetes on a desktop or laptop Docker desktop lets you run kubernetes macrocates from the makers of Ubuntu and minicube also let you run kubernetes altree requires that virtualization is enabled kind runs over Docker desktop and offer extra functionalities Docker desktop is limited to OneNote but it's usually not a problem Marco Cates kind and minicube can emulate multiple worker nodes on Windows Docker desktop lets you run both Linux and windows containers and you can't create and run Windows containers on Mac and Linux it runs on hyperv or Windows subsystem for Linux so if you have Windows 10 version 2004 or later it's the recommended way to run Docker desktop if fiberv is enabled on your laptop or desktop you can't run another hypervisor at the same time and mini Cube used by default virtual box but it can also run on hyperv you can install Docker desktop on Windows using App review and it will create a virtual machine named Docker desktop VM when you take the enable kubernetes checkbox in Docker desktop it will download additional containers to run kubernetes using Windows 10 version 2004 or later and if you have wsl2 installed you can tick the use the wsl2 base engine and Docker desktop will create its VM inside the Linux distro you install on your Windows machine note that this is the recommended way to run Docker desktop on Mac Docker desktop use the hyperkit lightweight hypervisor to run its VM mini cube is another popular option it does not require a Docker desktop it runs on Linux Mac in Windows and it requires an appervisor like a virtualbox here we can see mini Cube running on a Mac and its virtual machine in virtualbox if you need to install mini Cube on Windows but don't want to install virtualbox you can run minicube on hyperv you need to create a network switch and start minicube with some extra parameters kind stands for kubernetes in Docker because it runs on top of Docker desktop kind lets you emulate multiple control planes and multiple worker nodes this is useful if you need to test node affinity and this concludes the sector on how to run kubernetes locally and this will be a super quick lab just to validate that our kubernetes installation is working locally so here I'm on Windows and I'm using Docker desktop and I installed kubernetes with Docker desktop in the system tray I can right click on the docker desktop icon on a Mac you can do that from the top of the the screen and I will select dashboard and from there we're going to click here on the gear icon the settings icon and I'm using wsl2 to run Docker desktop so that's the recommended way and here if I click on kubernetes uh enable kubernetes is checks so kubernetes is installed but is it running correctly let's find out let me go back here let's open a terminal and let's run uh this command uh Cube CTL cluster info that should give us a little information about what's running so kubernetes Master it as in green yay it's working so it's running at this address and Cube DNS is running at this address so by running qctl cluster info you get some information about the kubernetes installation and itself let's see how you can use the kubernetes CLI the kubernetes API server is a service running on the master node it exposes a rest API that is the only point of communication for kubernetes clusters you define the desired state in yaml files and let's say you want to run a x number of instances of a container in the cluster using the kubernetes CLI you then send that desired state to the cluster via the rest API other applications like a web dashboard can also communicate with the rest API to display the cluster state Cube CTL is the kubernetes CLI and it runs on Mac Linux and windows and you pick your choice of pronunciation cue control Cube cuddle Cube CDL doesn't matter it communicates with the API server and its connection information is stored in a config file under the dot Cube folder let's now see what a context is it's a group of access parameters that let you connect to a kubernetes cluster it contains the cluster name a user and a namespace the current context is the cluster that kubernetes commands will run against let's say that you can connect to three clusters cluster a cluster B and cluster C when you set the default context to Cluster B then all the cube CTL commands that you you will run will run against cluster B here's a cheat sheet for context commands Cube CDL config current context will get you the current context get context will list all of them Cube CL config use contacts and the context name Will Set the current contacts and delete contacts with the context name we'll delete the context from the config file there's a large ecosystem of free open source kubernetes tools that you can use Cube CTX is a good example it's a shortcut for the Cube's DL config use context command you simply type Cube CTX followed by the context name to quickly switch contacts it runs on Windows Mac and Linux very useful and this concludes the structure on the kubernetes CLI and the concept of context a context contains connection formation to a kubernetes cluster and you can have one or more than one context set on your machine it's super important to know how to figure out in which context you're currently in and how to change context so first thing first let's figure out in what context we're currently in I'm going to use cubectl config current context and this will print the name of the context we're currently in I'm currently in the docker desktop contact means that whenever I type Cube CTL commands it'll be applied to that kubernetes cluster all right I can have more than one context configure on my machine or on any machine or server to list them we use Cube CTL config get Dash context and here we can see that I have two contacts set on my machine Docker desktop which is the current context because it has that star in the current column and I have a second one called demo then one is a cluster that I created in the cloud what if I want to change from Docker desktop to demo okay let's use cubectl config use Dash context and the context name demo and now if I print again the current context there it is demo so whenever I'll be typing Cube CTL command DZ yaml files this command will be sent to My Demo cluster somewhere in the cloud a cool tool is Cube CTX because it allows you to well instead of using Cube CDL config blah blah blah blah blah basically it's a shortcut if I simply type a cube CTX that will print the context that I've configured on my machine so demo is green means that it's the current context and I can quickly change context using cubectx and the context name Docker desktop you know a little bit less keystroke to talk about anyway it's a fun too foreign context let's say uh you you your cluster has a long funky name and you want to rename it to make uh more sense you can use the cube CTL config rename contacts all name new name so let's try to rename our demo cluster here demo let's say it's on Azure so Azure demo okay let's use the cube CTX to print the contacts and there it is azure demo where is that context information stored it's stored locally on your machine so I'm on Windows let's see where it is it's on the C drive users your username under dot Cube deduct Cube folder and there it is config so if I right click on it and select opening with code we can see that it's a yaml file and it contains two entries here two clusters one is the Azure one and the second one is the Docker desktop one and this is the different context that I have that have so here is my Docker desktop context and here is my demo context well the cluster name is still demo but I renamed the context name to Azure Dash demo there it is all right let's go back here and let's say that I've deleted my cluster in the cloud I don't need it anymore I want to get rid of the contacts information I can use cubectl config delete contacts and the context name so let's try this Azure demo deleted context Azure demo from the config file so let's take a look here and you see that now I have only one contacts and here's the current context in this line here but let take a look here at the Clusters list My Demo cluster is still there right so it's not deleted automatically what you can do is simply edit that config file and remove the section no longer needed let's talk about the declarative and the imperative ways to create resources in kubernetes there are two ways that you can use when you want to create resources in kubernetes the declarative way and the imperative way using the imperative way you use cubectl to issue a series of command to create resources in the cluster this is great for learning testing and troubleshooting using the declarative way you define the resources needed in yaml files and use cubectl to send the content of these file as the desired state to the cluster so instead of a series of command this is reproducible and you can even store these yaml files in a source control system here we can see a series of command to create resources that's the imperative way you can create a pod using the Run command create a deployment or service using the create command using the declarative way you would use a yaml file to define the resource and then send the content of that file to the cluster to create these resources so what's a yaml file well it's a text file that contains properties that Define the resource it has some required properties like the API version the object kind that defines the type of object you want to create we'll take a look at these later on you can use the cube CTL create command to send the information to the kubernetes cluster we will take a deeper look at yaml files in the future lectures but right now you may be wondering do you need to type all that yaml manually the answer of course is no one way to get the correct syntax is to copy one from the official kubernetes documentation at kubernetes at IO slash Docs you then search for the object you want to create and click on the copy icon another way is to use templates offered with an editor like Visual Studio code let's say you create a new yaml file then you type control space and select the template to generate the Manifest that you can edit neat you can also use the kubernetes CLI to generate the ammo add dash dash dry run equals client and dash o for output specifying the yaml to Output the yaml to the console you can even send the output to a file using the greater than sign and a file name and this concludes this overview of the imperative and declarative ways to create resources in kubernetes let's deploy an nginx container using both the imperative way and the declarative way using the imperative way uh we're gonna type command called cubectl create I'm going to create a deployment I'm going to name our deployment my nginx1 I'm going to specify a parameter called image where we will specify the image we want to run this will create a deployment and we can Cube CTL get a list of the deployments using Cube CTL get deploy and there it is now the second way is the declarative way instead of typing a command with all the parameters at the common line we're going to specify a yaml file where all the configuration options are stored cubectl create Dash f for file and the name of the file so let's run this okay it was created let's again type Cube CTL get deploy get both our deployments and we if we take a look at the yaml file well it's a yaml file the type deployment it has the my nginx2 name it has a bunch of parameters that will come back to that a little bit later but basically all the configuration parameters are stored in that yaml file what's cool with that concept is that it's you can put these files quite easily in a source control system all right uh let's do a little bit of a cleanup let's delete our deployment Cube CTL delete deployment my nginx one okay and I'm gonna use a the same command but using a shortcut this time Cube CTL delete instead of deployment and the name of our deployment okay and if I type again Cube CTL get deploy there's no deployments currently my namespace let's take a look at namespaces so what is a namespace it's a kubernetes resource that allow you to group other resources let's say you need to deploy your application in multiple environments like Dev test and prod well you can create a namespace for each of these environments they are like logical folders in which you group resources kubernetes creates the default namespace called well default objects in one namespace can access objects in different in a different one the kubernetes internal DNS assigned Network names to some resources deleting a namespace will delete all these child objects this is super useful when doing tests create a namespace in the morning create resources under that namespace during the day and at the end of the day simply delete that namespace this command lists all the namespaces existing in a cluster we'll take a look at this command in more detail okay first you create a namespace this yaml file Define a namespace called prod then you use that namespace when you create other resources in the metadata section you set the namespace key to the name of the namespace you want this resource to be created in so namespace colon prod you can assign Network policies and limit the resources that you can create in a namespace using the resource quota object here's a cheat sheet for the namespace commands so cubesatel get namespace list all the namespaces and if you don't want to type namespace each time you can use a shortcut an S so Cube CTL get an S is the same thing as cubectl get namespace you can set the current context to use a namespace in the next commands that you'll type by using cubectl config set contacts current with which will use the current context and then the namespace is called the name space that you want to use so the next command that you'll type will be under that namespace cubectl create NS and the name of the namespace so you create a namespace you delete it using cubectl delete in s and the namespace name and you can also list all the pods or the objects from another namespace so Cube CTL get parts or any objects and then you pass the flag dash dash all namespaces that will list all the objects in all the different things spaces and this concludes this look at namespaces let's see how to list and switch between name spaces you can get a list of the namespaces using the cube CTL get namespaces command let me copy this and these are the namespaces currently created on my machine on my kubernetes cluster so default the cube node Cube public Cube system I can also use the shortcut for namespaces which is just few letters which is a lot faster to type and it will give you the exact same result here let me get a list of the pods running Cube CTL get parts no resources found in default namespace that I'm currently my contacts is using the namespace called default right and there's no pods running in there so what if I want to list the pods that are in the cube system namespace well I can use the same command qctl get pass specifying the namespace switch and specifying the name of the namespace let me copy this and there are many parts running here here I can use also the the shortcut so instead of typing dash dash namespace equal I can just use the shortcut dash n so only one dash here two dashes here and the name of the namespace so let's try this and it works perfect now what if I want to change from the default namespace or the namespace called default to a namespace called Cube system right and then apply all my commands to so all the objects will be created in that namespace well I can do that by using the cube CDL config set contacts dash dash current so here we're modifying the context and passing a switch called namespace and the name of the namespace we want to change so we want to change instead of being the default namespace all the time we want to be in the cube system okay and yeah let's see if uh if we run Cube City I'll get passed if we would get some pods of course because we're currently in that Cube system namespace okay now let's change back to the default namespace and let's get a list of the parts there should be none perfect okay so this is how you uh you change namespaces perfect of course we can create new namespaces by using the cube CTL uh create NS for short for namespace and a namespace name so hello okay uh Cube CTL get NS and my name space here was created seven seconds ago and I can delete a namespace using cubectl delete NS and the namespace name that and hello now a bit of a warning here uh if you have resources under that namespace these resources the buzz the containers whatever running will also be deleted so use this command with caution foreign take a couple of seconds okay let's get the name spaces and our hello namespace is gone let's now look at the masternode in nowadays the physical or virtual machine a group of nodes formed a cluster the masternode is also called the control plane the kubernetes services and controller are located on the control plane they are also called The Master components and you usually don't run your application containers on the masternode the key value data Store where the state of the cluster is stored the API server is the only component that communicates with its CD let's start with the API server it exposes a res interface and client tools like the kubernetes CLI communicates through that recipe it saves the state in a CD and all client interacts with the API server never directly with the data store hcd is the data store for storing the cluster state it's not a database but a key Value Store is the single source of truths inside kubernetes the cube control manager is the controller of controllers its job is to run the other kubernetes controllers the cloud control manager job is to interact with the cloud providers a check if nodes were created or deleted uh Route traffic create or delete load balancers and interact with the cloud providers storage services the cube scheduler watches for paths that are not created yet and selects a node for them to run on it checks for various rules and then assign the Pod creation to a node finally you can install various addons on the masternode these addons provide additional functionalities in your kubernetes cluster and this concludes this look at the masternode let's take a look at the worker nodes a node is a physical or visual machine a group of nodes forms a cluster there's a special node called the masternode sometimes called the control plane where the kubernetes services are installed the nodes running the containers are called the worker nodes when a worker node is added to the cluster some kubernetes services are installed automatically the container runtime the cubelet and the Q proxy these are Services necessary to run pods and they are managed by the master components on the masternode the cubelet manages the pod's life cycle and ensure that the containers described in the Pod specifications are running and are healthy the Q proxy is a network proxy that manages Network rules on nodes all Network traffic go through the cube proxy on each node you will find a container runtime kubernetes supports several container runtimes that implements the kubernetes container runtime interface specification or CRI one thing to note is that for kubernetes version previous to 1.19 the Mobi container runtime was installed and was receiving the container runtime interface call through a shim because it did not fully implemented the specifications this is not ideal as it added an extra step starting with kubernetes version 1.19 Mobi is no longer installed oh wait a minute Moby not installed that means that my Docker containers will no longer run if the docker container runtime is not installed right well the short answer is that your Docker images will run as is nothing to change its business as usual what change is that what you can do inside the cluster since the docker runtime is no longer installed you no longer can access the docker engine and issue Docker commands directly inside a node you'll have to use another tool called crease CTL but again that's if you SSH into a node and run commands directly on that note something that you don't do usually alright a node pool is a group of virtual machines all with the same size a cluster can have many node pools and each node pool can host virtual Machines of different sizes let's say that we have two node pools in our cluster the first one consists of VM without gpus and the second one with chip use remember that Docker desktop is limited to one node so basically you run the master components and all the application containers on the same node and this concludes this look at the worker nodes so let's get some information about our nodes I'm going to run a cube CTL get nodes and since I'm running on Docker desktop locally here I have only one node name is Docker desktop status is ready roll is master here's the revision the version number and it was installed 72 days ago all right I can get more information about the uh the node uh since I only have one node I can skip the node name per meter and here's a bunch of information the name the row some labels creation date um what else what else what else what else uh the capacity to CPUs uh number of PODS maximum number of parts that I can run 110 the architecture the OS Linux architecture AMD the pods running and the CPU request CPU the Miss memory so all useful information when uh you need to troubleshoot and debug so that's pretty uh pretty interesting now I'm using a locally Docker desktop so I'm limited to one node Let's uh switch should to um My Demo cluster uh running in the cloud so let's do the same thing Cube CTL get notes I have more notes now so I have a cluster in in the cloud running three nodes in a node tool so one two three status ready agent and the version of kubernetes that is running that's interesting let me grab cubicity of the scrap node and let's uh let's say use this one so I can get the same information that I I was getting earlier but this one is running uh in the cloud that's the name role agent agent pool the different labels annotation number CPUs memory System Info it's running Ubuntu Linux AMD 64. uh what else will end the different parts that are running on this uh node let's take a look at parts so what are pots a pod is the smallest unit of work in kubernetes it encapsulate an application container and represent a unit of deployment paths can run one or more containers inside a pod containers shame the same IP address space and volumes and they communicate with each other using local laws that's inside the pod s are ephemeral deploying a pod is in atomic operation so it succeeds or not if a pod crashes it is replaced by a brand new one with a shiny new IP address you don't update the Pod that is currently running you create an updated version deleting the first one and deploying the new one you scale by adding more pods not more containers inside a pod I use this analogy in a previous lecture pods are like cattle there are ephemeras and you just replace them a note can run many pods and a part can run one or more containers if a pod runs multiple containers there's usually one that is the main worker where your application logic is located and the other ones are helper containers that provide services to the main worker we will come back to this concept in another lecture and this concludes this look at pods let's take a look at the Pod life cycle we'll start with the creation life cycle so when you issue a cube CTL create command to deploy a pod in your cluster the CLI sends the information to the API server and that information will be written into a CD the scheduler will wash for this type of the information look at the notes and find one where to schedule the Pod and write that information of course in a CD the cubelet running on the Node will watch for that information and issue a command to create an instance of the container inside a pod and finally the status will be written in a CD one thing to notice is that each time an operation is taking place inside the cluster the state is written in its CD so SCD is the single source of Truth in the cluster all right so let's take a look at the deletion life cycle now when you issue a cube City delete command to delete a pod from your cluster the CLI sends deformation of course to the API server that information will be written in SCD and notice that the grace period of 30 seconds will be will be added so the cubelet picks that information and sends a terminate signal to the container if the container hangs it is killed after the 32nd grace period and finally the state is stored in a CD The Path State will give you a high level summary of Where the Pod is in its life cycle pending mean that the Pod is scheduled for creation but is not created yet if you run out of resources in your cluster kubernetes may not be able to create new paths and if this happens the parts will be in the pending state running means that the part is currently running succeeded means that the code exited without any errors fail means that the code inside the the Pod exited with the nonzero status so some kind of error occurred unknown means that kubernetes can communicate with the pod and finally crash loop back off oh I love this in his state name so krashu backup means that the Pod started then crash kubernetes started it again and then the Pod crashed again so kubernetes say okay hold on I'm stopping here so we'll take a look uh in the official letter of where to look for these states and this concludes this look at the bud life cycles let's see how to define and run pods to define a pod the declarative way you create a yaml file specifying but as the kind that's the type of resource you want to create you specify an image location in this case the nginx image will be pulled from Docker up that's the default container registry you set the port that the container will listen on you can add labels they are used to identify describe and group related sets of objects and resources you can set environment variables directly here might not be the best idea to place configuration values in a later lecture we'll see how we can externalize that by the use of a config map object you can even specify a command to run when the container starts if you have created a yaml file with your pod definition you use cubectlcreatef specifying the yaml file location and name and this will create your pod the declarative way now if you don't have a yellow file and you just want to run a pod the imperative way you use qctl run you specify a name for your running pod Dash Dash image the image name you can specify a program to run in this case it's sh and dash C you can specify a parameter that you want to pass to the program qctl get parts will list all the pods that are currently running Dash o wide will get you the same formation but uh with a few columns more cubicity Of The Scribe part and the Pod name will show the pad information you can use qctl get part with the Pod name Dash offer output uh in yaml format and you can pipe that to a file name so this is pretty cool because it will extract the Pod definition in yaml and save it to a file so if in case you lost that yaml file that was used to create a pod well you can recreate it quite easily qctl exec Dash it specifying the Pod name and the program to run will get you inside that part in the interactive mode you delete a pod using cubectl delete Dash F specifying the yaml file or if you don't have the yaml file simply use cubectl delete pod and the path name that will result in the same as the previous command and this concludes this look at how to define and run pods in this lab we will run our first pods we'll start by using the imperative way we'll use the cube CTL run command specifying the image that we want to run in our case we want to run an nginx web server and we'll specify a name to the running instance my nginx let's run this pod my nginx created let's get a list of the running pods by using the cube CTL get pods command I have one pod running my nginx ready one of ones that is running it was created 11 seconds ago let's try to get more information by adding the dash o y per meter it's the same command but we'll get a little bit more information like the IP address of the Pod and the node where the Pod is currently running awesome if you want more information we'll use the cube CTL describe command with the type of object and the name of the running instance so cubectl describe the type pod and the name my nginx all right tons of cool information here the name of the object my internet the namespace where it's running uh the node where it's running start time any labels annotation the IP address information about the container any volumes restarts that happen and any volumes here and at the end we get the list of events that happened when the Pod was created it was first scheduled and then the image was pulled and then the image was successfully pulled and created and then started the cube CDL describe command should be the first thing you try when doing some troubleshooting here you will likely have some very useful information uh in case a pod doesn't start maybe the image is not available maybe the image didn't start correctly also Cube CDL describe command that's very very useful let's now delete our pod by using the qctl delete the object type pod and the name of the Running part should get my command line back in a few seconds all right let's now run a second pod this time a busy box image Cube CTL run the image busy box and the name of the running instance my box but we're adding extra parameters here Dash it dash dash the program we want to run this will open a session inside our running pod and look at the command prompt it's changed to a pound sign this means that now I can type commands that are running inside the pod LS for listing the folders and files I can run a base64 command here I can encode a string here and that happened inside the running container inside the Pod pretty cool to stop the session simply type exit and the this ends the session okay let's do a little bit of cleanup um in the busy box case uh it takes up to 30 seconds to uh to delete so we have two options here we can run the cube CTL delete the object type and the running AdSense and to get back our Command Prompt right away we can use the dash dash weight equal false per meter or if we simply want to kill the Pod brutally we use dash dash force and with a grace period of zero seconds let's run this and this will kill the Pod brutally all right let's now create a pod the declarative Way by using a yaml file we have a yaml file here called myhab.yamo the kind is a pod we give it a name my app dashboard a few labels the image we want to run any limits here four degree sources in CPU and memory the port that the container is listening to and some environment variable we're defining an environment variable called dbcon that will have a value of my connection string here awesome let's use the qctl create Dash F4 file and the name of our yaml file pod my app.pod created perfect let's run the qcl get pods command to get a list of our paths it's running perfect we can also describe our pod again same information that we saw earlier right the default names the namespace and here look we have our environment variable and the same events that we saw earlier previously we used the cube CTL run command with the dash it parameter to open a session to our BusyBox container now what is the Pod is currently running well we can use the cube CTL exec command with the same it per meter here we specify the Pod name and the program we want to run that will open a session to a part that is currently running okay let's output the dbcon environment variable my connection string awesome that worked let's exit or stop our session and this time we'll use the qctl delete command specifying our yaml file to delete our pod let's take a look at init containers let's say that your app has a dependency on something can be a database an API or some config files you want to initialize or validate that these exist before launching the app but you don't want to clutter your main logic with this type of infrastructure code so what do you do you can use an init container that lets you initialize a pod before an application container runs let's say that for the app container to run it requires a series of configuration files in the Pod definition you define a container that will run first this is the init container upon completion kubernetes will start the app container this is a great pattern for applications that have dependencies the init container job can be as simple as validating that a service or a database is up and running this keeps the infrastructure code out of the main logic init containers always run to completion you can have more than one and each bus complete successfully before the next ones starts if it fails the cubelet repeatedly restarts it until it succeeds unless it's restart policy is set to never probe are not supported as ended containers run to completion in this bad definition file we have the main application located in the containers section in green and the init containers in the init container section in yellow as you can see here we have two init containers they both watch for services to be up and running so the first one we will run to completion then the second one and finally kubernetes will start the app container and this concludes this look at init containers in this lab we will use an init container to modify the home page of an nginx container let's take a look at our yaml file it's manifest for a pod um we have two sections here containers and any containers let's first take a look at init containers we will use a BusyBox image and we'll run this command wget and will it that website right here and that website it's the ohm of the first website the home page of the first website is pretty simple just a few lines of HTML all right and we will save that HTML into a file called index.html into a volume called work directory and the nginx image will map that value and we'll serve that index.html page as its default web page so basically we're initializing our nginx container by using an init container here awesome let's open a terminal and let's deploy our application Cube CTL apply Dash F the name of our yellow file let's wait till the nginx image is up so if I do a doctor PS yet my nginx container is a oh let's uh open a session right into that nginx container and let's try to hit the default web page curl localhost and yep that worked we're serving the default uh web page of the CERN website here pretty cool let's type exit and let's do our cleanup cubesatel delete and our yellow file let's now look at selectors when defining kubernetes resources you can use labels these labels allow you to identify describe and group related set of objects or resources they are simply key value pairs that you define yourself in this part definition we see two labels app with the with a value of my app and type with a value of front end note that the app and type keys are not something defined by kubernetes they are defined by you for your application okay but what does labels have to do with selectors well selectors use labels to filter or select objects here we see a selector in this pod definition the selector type is node selector and the key value pair is this type equals super fast okay but how does that work here we have a path definition with the node selector set to this type equals super fast we're telling kubernetes that we want to run this bar on the Node that has a label set to this type equals super fast note a does have such label so kubernetes will schedule the Pod creation on that note the simplest way to picture what selectors do is by comparing them with a SQL query it would be something like select star from nodes where this type equals super fast and this concludes this look at the selectors concept in this lab we will test the selector concept we have two yaml files here the first one it contains the definition of a pod we will run an nginx web server listening on Port 80. and for the sector concept what is enter is is this section the label section so we have two labels defined here app set to my app type set to front dash n let's take a look at the second yaml file that's the service of name my service it's listening on Port 80 and targeting or redirecting the traffic to Port 80 that the Pod is listing on and let's look at this section selector app my app type front dash n so these are the same labels that we Define right here in our pod definition all right Let's test this concept I'll open a terminal and it will first deploy r by using cubesatel apply Dash F and the name of the yaml file foreign let's now deploy the service Cube CDL apply Dash f myservice.yaml all right how do we know that the service is connected to the Pod how do we know that the selection was successfully made to find that let's get the Pod IP address by using qcdl get pod Dash o wide and let's look at the IP address here 10.1.9.31 all right let's run this command cubectl get EP EB is short for endpoint and here's the name of our service my service so let's run this all right you see D in the endpoints column 10.1.9.31 that's the same IP address let's now try to port forward to that service okay we get an immediate result here and let's uh go to localhost port 8080 and that worked perfect let's go back here to our terminal and let's type Ctrl C to stop the port forward now let's try to break things a little bit we'll open the my app.yaml file and let's change the one of the labels so uh app will set it to my app to control s let's save that file and let's redeploy the file again the Pod again by using cubectl apply Dash F and the name of the yaml file all right let's check the endpoint again is it still working huh look at the endpoint colon none and if we try to pour forward again there's no immediate results so that doesn't work so disproved that both labels must match here the selector here boat labels must match the labels in in the Pod definition so that the selection can work all right let's type Ctrl C to stop this and let's do our little cleanup by deleting the service and then the pod thank you let's take a look at multicontainer pods we sign a previous lecture that pass can run one or more containers that there's always a main worker and that the other containers are providing services to that main worker like saving data to a database and writing log files there are scenarios while multicontainer Parts make sense and they are well documented in a series of patterns we will take a look at some of them in the next few slides with these sidecar pattern the helper container provides extra functionalities to the main worker let's say that our app writes some log files inside the pod the sidecar can copy these log files to some purchasing storage offered by the cloud provider this way the application code located inside the main worker is not cluttered with infrastructure code that code is located in the helper container and if you move from one cloud provider to another one well you simply replace or update that L per code this keeps your application code super clean our next pattern is the adapter let's say that our main worker outputs some complex monitoring information that the monitoring surface of our cloud provider cannot understand the adapter role would be to connect to the main worker simplify the data for the monitoring service again the code specific to the cloud provider service is located inside the helper container the Ambassador pattern is another type of the men of the middle role let's say that our application code needs to write to some nosql database but the code has no clue on how to do that no problem you can send that data to the Ambassador that in turn will write the data to the nosql data store the code specific to the data store service is located inside the helper container if you're curious about these patterns I suggest you get a copy of the design distributed systems book the other is Brennan Burns Brendan worked at the Google where he cocreated kubernetes he now works at Microsoft so how can you define multicontainer paths well if you remember the lecture about yellow files you see that the container sections is actually a list this means that you can Define multiple containers are added in yellow you see container number one and in green container number two and when you create a pod both containers will be created at the same time pretty cool here's a quick cheat sheet for multicontainer parts so after you created your yaml file you simply use cubectlcreatef specifying the yaml file name so same thing as creating a single container pod if you want to exec into one of the container you simply use Cube CTL exec Dash it the part name and specifying Dash C and the container name this way you can jump into one of the containers running inside the pod you get the logs for a container using cubectl logs the Pod name Dash C the container name and this concludes this look at multicontainer pods let's take a look at some networking Concepts kubernetes is what we call a flat Network as a result most resources inside a cluster can see each other all containers within a pod can communicate with each other all pods can communicate with each other all nodes can communicate with all pods and all nodes paws are giving an ephemeral IP address while services are giving a persistent IP so this is quite important we'll come back to that later let's illustrate the cluster Network in blue here each pod gets an IP address and the containers inside a pod share the same address space the containers inside the same pod share the same IP address but each must be assigned a different port number they can communicate inside the Pod using localhost and the port number they can also share the shared volumes what about communication between pods can the container on the right talk to The Container inside the pod on the left using localhost no they can't they need to go through a service that will front the network traffic for the pod for external access to the cluster traffic goes through a load balancer service offered by the cloud provider in future lectures we'll look at different type of services that we can use in kubernetes and this concludes this quick look at networking Concepts thank you let's create a multicontainer pod using a yaml file let's take a look at the yaml file so the kind is part the name will be two dash containers and in the container section we're defining two containers so the first one we'll use the nginx image and we'll name it my nginx and that web server will listen on Port 80. and we're defining a second container using the BusyBox image and we'll name it my box this one will listen on Port 81. and for the BusyBox container to stay up and running we need to tell it to to stay up by issuing a sleep command so it'll stay for an hour what we'll try to achieve is open a session on the BusyBox container and try to hit the nginx container so the the default web page served by the nginx container all right let's try to create our pod here using a cube CTL create and the name of our yaml file okay pad two dash containers created so let's try to see uh if they're running all right two out of two because we have two uh containers running in that pod status running 10 seconds uh the IP address so there's one IP address assigned to the to the pub and denote okay let's try to get a little bit more information by using the cube Ctrl describe pod and the name of the pod let's scroll up a little bit so the name is two containers then space is the default one the node where it's running uh the containers so the first one is my nginx which using a the nginx image we have information about the limits that we set earlier in the ml file my box using a BusyBox image listing on Port 81 right there's a that sleep command so useful information here and also look at the events now so earlier when we add only one container inside a pod we would get just one set of events for polling and creating the container now we have two first one is the Pod was scheduled and the second one here is on the second land line um the nginx image was in the polling state it was Paul created started and then the busy box container was pulled and created okay perfect now let's try to open a session inside our BusyBox container so we'll use Cube CTL exec Dash it okay here's the trick now we need to specify the Pod name and then the container name that we want to to connect to and then the the program that we want to run that's the trick when you have multiple containers you need to specify the Pod name and then the container name all right let's try to do that oh looks good at last yeah it worked okay now we'll use wget to try to hit that default page serve ID nginx web server wget with the this flag and we'll call localhost all right welcome to nginx so that word now Outpost for a couple of seconds and why I'm gonna ask you why did that work why calling localhost worked so that work because let's go back to our yaml file the nginx container is listening on Port 80. right we don't have to specify a import number here if the nginx container would have listened to something different we would have to specify Here Local O's colon and the port number all right let's try to exit this perfect let's now delete our pod using Cube CTL delete and the name of the ml file and by using the force and grease grace period equals zero flag that will kill both containers immediately perfect this is a super short lecture just to introduce you to the concept of a workload so a workload is an application running on kubernetes all containers running in a kubernetes cluster must run in a workload the Pod is the atomic workload it represents a set of running containers and all workloads will create pods the replica set and the deployment will provide extra functionalities on top of the pod like the ability to Define how many instances of a part we want the stateful set and the Daemon set are specialized workloads and finally the job and crunch up offer tasks that run to completion these are shortlived tasks we will see each of these workloads in detail in the following lectures and this concludes this super short lecture on the workload concept let's take a look at replica set the replica set primary job is to manage dpad replicas making sure that the desired number of instances are running this provides the selfelink capabilities in kubernetes while you can create replica set the recommended workload is the deployment now welcome back to that in a moment so let's say that you want three instances of a pod to run at all time you create a replica set and specify that you want three replicas if for some reason one bird crashes kubernetes will replace it automatically without any human intervention I'm pretty cool eh let's see how to define a Rebecca set by starting with a pod we take the Pod definition except for the API version and kind and we place these values in the template section of the replica set the final result is a replica set yaml file so basically in the section highlighted in green you will find values specific to replica set and under the template section the values that Define the Pod you want to run here you set the desired number of instances with the replicas property again while you can create replica sets the recommended workload to use is the deployment because it provides extra functionalities on top of the replica set so why bother and learn about replica sets well in the deployment lecture you'll see that when you create a deployment that will also create a replica set in the background that's why it's important to learn about the replica sets functionalities here's a cheat sheet for replicasets command you create one using cubectl apply Dash F and the yabl file you get a list of the replicases by using cubectl get RS you get some information about the replica set by using Cube CTL describe RS and the replica set name you delete one using Cube CTL delete if you have the yaml file you specify Dash F and the yaml file name or if you don't have the yaml file simply by using the replica set name using cubectl delete RS and the replica set name and this concludes this look at Olympic assets let's create three instance of energy next container using the replica set template so let's take a look at our yaml file the type of object we want to create is the replica set this will be the name of the replica set rsexample we want three replicas running uh at the same time and uh we want three replicas of this containers uh name will be nginx and the image nginx uh colon Alpine so it's a smaller version and we Define the resources and also the ports that each is listening on all right let's try to uh create that so Cube CTL apply or create Dash F and the name of our EML file okay we'll pick a set created let's take a look at our running pods and have three pods running okay and look at the names that were um that were assigned to the each of these spots RS Dash example and then is some kind of a unique number here each one must have a different or unique name so each one is ready running and look here each one has a different IP address perfect so let's take a look at our replica sets that we've created so Cube CDL get RS so there's one rsexample um three desired three current three ready everything is green everything's okay Let's uh now describe our replica set so Cube CTL describe RS for replica set and the name of our replica set so let me paste that right let's scroll up a little bit so the name is RS example uh it's running in the default namespace um any labels or annotations are listed here number of replicas that we set so three current three desired and the the current part the status S3 running and zero failed to The Container information so nginx we want to run the nginx Alpine image the sync on Port 80 the limits and so on and this these are the events that were raised here so each pod was successfully created here all right so last thing we need to delete what created using qctl delete Dash F and the name of the EML file thank you let's take a look at deployments we'll start by comparing pods and deployments Buzz don't selfheal meaning that if a pod dies kubernetes will not replace it automatically using a pod definition only one instance can be created and you can't update and roll back pods deployment can a deployment manages a single part template so you create one for each microservice you want to run when creating a deployment this will also create a replica set in the background but while you can see it you don't interact with the replica set directly you let the deployment manage that for you to summarize the replica set provides the selfealing and scaling functionalities while the deployment provides updates and rollback functionalities let's take a look at the deployment definition you define the desired number of instances with the replicas property this will be used by the underlying replica set you set the number of iteration you need to keep using the revision history limit property and you set the update strategy in the strategy section you can set the strategy type to a rolling update this way kubernetes will cycle when updating the pods the other strategy is recreate kubernetes will take all existing pods down before creating the new ones we'll have a dedicated lecture on this topic later on like a replica set we start with a pod definition and we start the metadata section in the template section of the deployment the final deployment definition looks like this highlighted in green we see the properties specific to the deployment and in yellow the ones defining the Pod we want to run here's a cheat sheet for deployments command so if you don't want to use a yaml file you can create a deployment using the imperative way so you use Cube CTL create deploy you specify a name then with the image property you you specify the image replicas the number of replicas you want to run and you can specify also other properties like the port number that the paths will listen on if you have a yaml file well you simply use Cube CTL apply Dash F and the yaml file you get a list of the deployments using cubectl get deploy and you get the deployment info using cubectl describe deploy and the deployment name since a deployment will create also a replica set you can get the list of the replica set using cubectl get RS and you delete a deployment by using a EML file so Cube CTL delete Dash F the name of the yaml file or if you don't have it simply use Cube CTL delete deploy and the deployment name and this concludes this look at deployments thank you let's use the deployment template to create three instances of an nginx container so let's take a look at the EML file this time we want to use the deployment Kai type of object we want to create is the deployment we name it deploy example we want three instances three replicas and we want to keep three versions three replica sets versions in the history in site kubernetes I know if you scroll down a little bit we see that we want to run an nginx uh the Alpine version because it's a little bit smaller and we name it nginx we set some resource limits and each pod will listen on Port 80. all right quite similar to the replica set template that we saw earlier except maybe for uh this parameter here all right now let's create our deployment using cubesatel apply or create Dash F deploy example and let's get a list of the pods that are currently running Cube CTL getbots Dash o wide and yes we have three parts three lines here so look at the names uh given to each bot so deploy example that's the name of the deployment object the the name of the deployment object Dash something unique so to make sure that each part has a unique thing kubernetes as a random number like this only one container is running inside each bot uh it's in running State each pod gets its own IP address so that's perfect okay let's now try to describe our pods so we can use the cube CDL describe pod and deploy example name but we just saw that the the names work a little bit different let's see if this works yep it worked it worked because uh the name is share across uh these three parts and so we get information about each one of these let's say I just need the information about that particular part I can use that name uh the unique name also so instead of deploy example I'm gonna use the full name and there it is f uh the information about that pod just that button all right let's now get some information about the deployment currently uh inside my cluster so cubesatel get deploy I have only one deploy Dash example three out of three ready up to date available so everything's uh looking good and I can describe my deployment using cubectl describe the name of the the type of the object and then its name all right so let's go a little bit we have the name the namespace where it's running any labels annotations number of replicas three desired three updated three total three available perfect strategies rolling update okay because we haven't set a running update strategy that's the default one we'll come back to that uh later on the path template nginx Alpine listening on Port 80. right and the events here all right since a deployment will automatically create a replica set let's take a look at the replica set that was created so Cube CTL get RS and for sure we have one replica set that was created by the deployment if we desired three current we're ready everything looks okay and we can also describe our Republic asset since we have only one we can uh use Cube CTL describe RS or we can use its full name we copy that and here I have the replica set description and finally we need to do a little bit of cleanup we delete our deployment using Cube CTL delete and with the name of the yaml file thank you let's take a look at demon sits the demon set is a specialized workload its role is to ensure that an instance of a pod is running on all nodes the paths are scheduled by the scheduler controller and run by the demon controller as nodes are added to the cluster the pods are added to them typical views are running some kind of helper service like a log collection or some kind of monitoring well let's illustrate that in this cluster we have two nodes and the demon workload ensures that an instance of a pod is running on each one of these nodes so you define a demon set in a yellow file you can specify that you don't want to schedule a pod on the master node by using a toleration same thing if you want to run the part on specific node pools here's a cheat sheet for the demon sets command so you create a demon set using a yaml file with the cube CTL apply Dash F command you get a list of demon set using Cube CDL get DS you get some information about the running demon set using cubectl describe DS and the name of the demon set and when you want to delete a demon set you either use a yaml file using cubectl delete Dash F and the name of the yaml file or the name of the running Daemon set by using Cube CTL delete DS and the demon set name and this conclude this look at demon sets let's run a BusyBox image as a demon set to ensure that we have one instance of that container running on each node in our cluster so let's take a look at the yaml file all right the kind is demon set we give it a name demon set Dash example and the container that we'll run is a busy box and here in the Toleration section we specify that we don't want to schedule a demon set on a node role that is mastered so we don't want to to run that demon said that the Busy box uh on the control plane on the masternode all right Let's uh deploy our demon sets so here I'm I'll be deploying that demand set on a cluster in the cloud that has three nodes because if I would try to do that on Docker desktop with just one note won't be interesting so let's try to do that doesn't matter what cloud provider I'm using right now okay uh let's uh get a a cube City I'll get parts we'll come back to uh the rest just after that so here I have three uh Parts demon example with a unique name here let's uh add the dash oh wide a flag Let's uh examine the node column so you can see that each pod is running on a unique node so 0 3 and 4 here and this selector is interesting because it if you have multiple objects you can basically filter out or just select what you want so here selector says the app uh equals the demon set example that's the name of our object here so that word one instance of our busy box is running on each node let me add a fourth node all right so my note count is now four let's go back to visual studio code and let's rerun uh the get paths command and for sure I have a Ford instance of my BusyBox container running as a demon set on the new note without me doing anything it was deployed automatically because I selected the demon set object type now we simply need to delete that demon set using a cube CTL delete let's take a look at stateful sets let's say that you run a database inside your kubernetes cluster traffic gets higher and you need to scale that database so you create more instances the main one can read and write while the other instances are in readonly mode and use replicas of the main database this is a complex scenario to implement the stateful set role is to help you solve this complex problem for parts that need to maintain state and unlike a deployment a stateful set maintains a sticky identity with a persistent identifier for each of the pods and if a part dies it will be replaced by another one using the same identifier the stateful set will create the parts in sequence and delete them in reverse order typical use are for Network related services that maintain some State and also for databases each pod gets a unique identifier using the stateful set name plus the sequence number and if one dies it is replaced by a new one but with the same identifier the pot creation is done in a ordered way meaning that the first one will be created then the second one and so on and when they are deleted they are deleted in the reverse order note that the persistent volumes are not automatically deleted foreign set you need to use a needless service you define one by setting the cluster IP value to none the stateful set will then refer to the atlas service and you define cloud storage in the volume claim templates section let's represent this slide as you can see only the first spot lets you read and write so how can you reach it if you want to write to the database well you use the pad network name in this case the instance name my SQL Dash zero plus the service name so dot MySQL when reading simply use the service name this will load balance the calls across all instances a bit of warning here containers are stateless by default and stateful sets are offering a solution for a stateful scenario but a lot of work has to be done on top of that a better approach might be to use the cloud provider database Services instead of trying to fit a stateful scenario inside your kubernetes cluster lastly deleting a sitful set will not delete the PVCs you have to do this manually here's a cheat sheet for stateful set commands so you create one using the cube CTL apply Dash F and the yaml file you get a list of set full sets using cubectl get STS you describe one using cubectl describe SCS and the set for set name and you delete a simple set using the delete command either using a yaml file or the name of the stateful set and this concludes this look at stateful sets let's now create a stateful set let's take a look at the state we'll set that EML file there are two parts in this CML file the first part is the creation of the Headless service the the kindest service but the trick here is to set cluster IP To None like this this will create an endless service all right second part is to create our stateful set so the kind set will set give it a name nginx Dash SCS and here we're referencing our Atlas service here so the name of our Atlas service assigned to the service name property we want three replicas right of a nginx image and we're creating a claim on a search class name called Azure file so the cloud provider here is not important that will work on any cloud provider so I already have a search class name called Azure file we'll use the read write once access mode and we'll use when when gigabyte of storage and we'll Mount this to a folder in VAR www all right so let's create our Seattle set here Cube CTL apply Dash if stateful set Dot yaml and quickly let's do a cube CTL get but a white and here you can see that the first instance is running the second one is running the third one is pending so each instance will be created in a sequence so the first one zero the second one one the third one two you can see by the age of each instances so 19 seconds 14 and 9 seconds they are created in a sequence and deleted in a reverse order also here we can see that um there's an IP address assigned to each of these let's take a look at the PVCs we should have a PVC for each of these instances and yes SCS 0 1 and 2. so uh we can do a mapping one to one here so for uh scs0 we have a PVC call also scs0 to prove that let's uh describe the second one let's say so Q the L describe paths and the number two and let's see what the PVC is assigned what claim is assigned so here the volume claim name scs2 awesome okay what we'll do we'll create a file in the in that instance the instance number two and we'll delete the the Pod the Pod will be recreated automatically and we'll see if the file still exists we'll also modify the default web page served by nginx and see how we can reach that that file the default web page from another instance so let's open a session on the nginx CS2 perfect Let's uh CD in VAR are volume and let's uh simply type Echo and pipe that to a file called echo.txt I do it LS my file is there get Hello dot txt yeah perfect okay first step creating that file uh Second Step modifying the default web page let's CD into the user chair nginx HTML folder and let's do an LS here so here's the default page served by nginx index.html we will brutally replace that file by using cat and piping that to the file name and typing hello enter Ctrl D on Windows to save the file the file is there if I simply do a cat index.html yeah okay the file has been brutally replaced okay we'll close our session on this instance so scs2 let's close our ins our session and let's open a session on the instance zero okay awesome let's try to it that default HTML page but on the instance number two so nginx as CS2 so to do that we'll need to use the web the web address nginx scs2 so that's the name of the instance dot the name of the atlas service and that word would only the hard name word no you need a combination of boat the instance name and the atlas service also all right let's exit our session here okay let's try to delete uh the uh instance number two okay let's do a q that's CTL get but we have a new instance uh that was created uh seven seconds ago but the uh the name of the instance is still the same so instead of a random number here by using a stateful set this will ensure that the names will be the same okay so let's open a session on the instance number two let's LS far and here's our file awesome let's do our cleanup so we'll delete the stateful set and we need to manually delete dpvcs because simply deleting this table set will not delete the PVCs so let's do that let's take a look at jobs are for shortlived task workloads you start a job it executes and succeeds or fails so job don't say memory they don't wait for traffic a job creates one or more pod and ensures that a specific number of them successfully terminate the job tracks the successful part completions and then marks the job as complete when the desired number of completion is reached when using multiple Parts the job will create them one after the other they can also run in parallel this is a job definition you define how many paths you want to run at the same time you can set a deadline if needed and the number of completions to reach to Mark the job as complete and you should set the restart policy to never so here's a cheat sheet for the jobs command you create a job the imperative way using Cube CTL create job the job name and the image name using the declarative way with a yaml file you use cubectl apply Dash F into the yaml file name you list the jobs by using cubectl GitHub you get some information by using cubectl describe job and the job that is currently running and you delete jobs with either a yaml file using cubectl delete Dash F or with the job name Cube CTL delete job and the job name and this concludes this look at jobs thank you in this lab we'll create a simple job so let's take a look at the chub yaml file so the kind is chopped so that's the type of object we're creating uh the name we're giving it hello and what we will run is a busy box container and when the container starts it will Echo uh hello front the job perfect something sip simple Let's uh run this Cube CTL apply with the name of our yaml file perfect the job was created okay let's get a list of the jobs there's one hello completion one so it ran and duration was two seconds okay let's do a cube CDL describe job since we have only one job that will do the work if not we can type the job name hello and uh but but let's take a look at name hello namespace default annotation and so on and so on it ran One Time One succeeded so that's our container successfully create and complete it perfect now we can get a list of the pods using cubectl get that and this is uh the Pod that was created to run the job it's still there you see the status completed but it's still there so we can get to to examine the log in case something went wrong so we can do Cube CTL logs and the name of the part here hello from the job that worked so let's do our cleanup let's delete the job is deleted do we have any uh Parts left none any chops left none perfect let's take a look at crunch apps a crunch up is a workload that runs jobs on a schedule it's an extension of the job that we saw in the previous lecture the schedule is defined using a crown like syntax in UTC and you can get more information about the crown syntax in this Wikipedia page here's a crunch out definition you set the scheduled parameter to a cron schedule so how do you know if a crunch up ran successfully well you need to look at the job history by default the last three successful jobs and the last fill job are kept the paths will be in a stop State and you'll be able to look at their logs foreign if you don't want to keep any history you can set the successful job history limit to zero so here's a cheat sheet for the crown job commands you can create one using the imperative way and if you have a yellow file you use the cube CTL apply Dash F and the name of the yaml file you can get the list of decron jobs currently running using cubectl get the CJ you can get some information with the cube CDL describe CJ and you delete the crunch up using its CML file using Cube CTL delete Dash F and the yaml file name and if you don't have the yellow file name you can delete it using cubectl delete CJ and the crunch up that is currently running and this concludes this look at run jobs so let's create a crunch up we'll take a look at the yellow file kind is the crunch up I'm going to name it hello Cron and we will give it a schedule only Stars which means that it will run every 60 seconds every minute that's the default and we're going to run a busy box image and we it will Echo this string okay so let's create our job using cubectl apply and we can get a list of the crown jobs using cubectl get print jobs okay so here we have the name the schedule if it's suspended it's if it's active the last last time it ran and we can get some information using cubectl describe run job and we pass the its name hello Dash Cron here this is super useful for troubleshooting the name the default name space it runs on in the default namespace we have the schedule here how many jobs does it keep in its history how many failed job is kept also the the command that will run oh you can get a list of the pods okay uh one per Ran So elocron with a unique uh value here so it has uh completed I'm gonna pause um to let it run a few times all right uh the job ran three times well it the last one is still uh running uh container creating Let me refresh that so it's completed okay now we can get the logs by using cubectl logs and then the job name dpod name so hello from the crown chart so by default the last three run of the job are kept in the history and you can configure that in the yaml file let's delete our crunch up here perfect and if again we type Cube CTL get pads uh all the parts in in the history basically are also deleted when you delete the crunch up let's take a look at rolling updates in a previous lecture we saw that using deployments you can set the number of paths instances using the replicas parameter and set a number of previous iterations of the deployment to keep in kubernetes we also saw that there are two update strategies running a date and recreate recreate is quite simple kubernetes will shut down all the running paths and create new ones after that running update will cycle through updating parts all right let's illustrate that using recreate all previous paths are deleted and the New Path will be created after that this means that there might be a small period of time where your microservice might not be responsive using the routing of this strategy a pod is deleted and replaced by a new one then the next one and so on there are two values that you can set to help you with this process Max search will tell kubernetes how many parts can be created over the desired number of pods let's say that you you want three instances in total setting Max search to 1 will allow creation of one additional pod on top of these three desired ones and this while the rolling update is running Max unavailable is the opposite is the maximum number of pulse that can be unavailable during the update process note that if you don't specify an update strategy in the deployment manifest communities will use a default strategy of running update with Max Surge and Max unavailable both set to 25 percent so let's say that we want three instances of a bot and we set max search and Max unavailable to 1. we're telling kubernetes that it's okay to create one additional part on top of the three desired one and that's it's okay to have one part less than the three desired one when done the previous replica set is kept you set how many you want to keep with the revision history limit property here's a cheat sheet for running updates you create of course your deployment using cubectl apply Dash F and the name of your yaml file you get the progress of the update using cubectl rollout status you get the history of the deployment using cubectl rollout history deployment and the deployment name you can roll back a deployment using qctl rollout undo and the deployment name that will roll back to the previous version or if you want to roll back to a specific revision number you add the two dash revision parameter and this concludes this look at running updates in this lab we will create a deployment and later on update it to a new version using a rolling update so let's take a look at the yaml file the kind of object we're using easy deployment call Lo Dash dab we want three replicas and we're using a rolling update strategy here and setting a Max search to 1 and Max unavailable to one we will deploy a container called Lo Dash app yeah hello app and uh that'll be version one and later on we will update it to version 2. okay so let's create our deployment here okay we can get a the deployment status by using Cube CTL rollout status and the deployment name uh the deployment was successfully rolled out let's take a look at our pods running okay so I have three instances perfect yellow depth uh three times excellent let's describe the uh the deployment so Cube CDL describe uh deploy and the deployment name and let's try to find if we can get some information about the strategy yeah here it is rolling update strategy Max unavailable oh wow the strategy type is right here uh rolling update running update strategy Max and available One Max search to one so you can get that information that we set earlier in the yml files okay let's now see if we have a replica set here's our replica set and let's Now update our yaml file and change the version of Hello app to version 2. so just update to 2 and save the file and we will use cubesatel apply and with the same yaml file now what I'm going to do how I'm using right now A K9s it's a terminal dashboard in a terminal sorry and to get the visual view of what's what will happen basically so right now I have three parts in green these are the ones that are deployed let's apply our new deployment I'm going to switch quickly to K9s and you can see oh it happens so fast but is you saw that the the new pods were were created and the old ones were uh were shut down all right here I can have a a deployment status everything's fine if not we would have uh some information if the deployment would take longer we would have some information printed here okay how many uh replica sets do we have we have two uh the the current one so that's the current one and the previous one by default uh three uh versions are kept uh in history we can get the deployment history by using cubectl rollout history and the deployment name okay version one for version two okay we're at version two and um we want to downgrade basically our roll back to the previous version so you can do that by using Cube CTL rollout undo and the deployment name so by default it will roll back to the previous version or if you have multiple versions and you know what version you want to roll back to you can specify the flag to revision and the revision number as we see here foreign let's do this one well either one will do the same thing okay let me switch to K9s and see what's happening oh the other one the the older while the version two is uh terminating new version is created and we can get a deployment status here everything successfully wrote okay Let's uh now take a look at our replica sets so we still have two right the current one is now the the first one that we uh that we deployed all right and we can do our cleanup by deleting the deployment using the yaml file let's take a look at blue green deployments so let's say that version 2 of our macro service contains some breaking changes like a different database schema what do you do using the rolling update strategy you'll have both version 1 and version 2 of your app running at the same time that might not work at all so using the blue green deployment pattern that might help solve that problem blue identify what's in production and green identify a new version currently deployed but not yet in production notice that the pulse label contains the version number when ready simply update the service definition to point to the new version and now green is now in production so green becomes blue and blue becomes green oh great this means that this pattern is solving the new database schema problem well not entirely you may still have to deal with some downtime while you update your database and also another drawback is that since both version of the macro services will be up and running at the same time you need to have enough free resources in your cluster to make this possible and this concludes this look at the blue green deployments pattern in this lab we'll create a blue green deployment we have three yaml files here so let's take a look at the lodep V1 it's a deployment we want three replicas of a container of a an image called Hello app 1.0 and here we're setting a label of app lov1 now let's take a look at the second one it's basically the same thing so deployment three replicas but it will use version 2 of our hello App application and we're setting the this label Here app to lov2 all right let's take a look at our cluster IP manifest file so it kind of service and here's the selector app hello V1 we will deploy that and later on we will change the cluster IP manifest to point to the newer version okay so let's deploy version one of our pods and let's also deploy our cluster IP service so let's take a look at the list of the paths currently running there are three pods and also there should be uh if I type Cube City I'll get SVC one cluster IP as we see front that's the one I just deployed perfect so let's do a quick port forward to uh to connect to our cluster IP service so we'll port forward uh the port 8080 that the cluster IP is listening to to localhost 8080. here cubesatel port forward service the name of our service 8080 28080 and let's hit local OS on port 8080 here okay so here hello world version 1.0 excellent day okay let's now deploy version two so uh right in my phenomenal I'll hit Ctrl C on my windows keyboard to break that and uh gain my terminal back perfect let's deploy version two okay and let's get a list of our pods currently running should be six so I have both versions uh in memory at the same time okay so that's one of the drawback of this uh this technique this blue green deployment technique okay let's now it did the cluster IP manifest file so we're we will change the selector to select app on Lo V2 let's save the file and let's update our cluster IP service by using cubectl apply and the name of the yaml file yes okay Let's uh port forward again right let's hit that local OS again and there you go V2 it worked let's do a little bit of her clean up uh let's delete our first deployment our second deployment and also decluster IP service you can select the three lines at the same time let's take a look at the concept of service in kubernetes first what problem do Services try to solve well if the pod in green need to reach the pod in purple you need to use its IP address the problem is that pass yet ephemeral IP addresses if the part in purple dies well you replace it and the new one will have a different IP address so we need a way to make these calls between pods a lot more robust so back to the service what exactly is the service well it's a kubernetes object that you define in a yaml file but on my pods that have ephemeral IP addresses Services get durable IP addresses and also they get DNS names they serve as ways to access Parts they're kind of a middleman and the target pods using selectors here we have four pods and a service the service select the pods that have the Zone label equals to prod and the version label equal to one the first part satisfies the selector the second one also but not the third one and not the last one so only the first two are selected so let's say we have two instances of a pod and we place a service in front of them if another part needs to reach these ones it will go through the surface and then the service will load balance the request to the instances in kubernetes we can use these Services the cluster IP the node port and the load balancer note that the cluster IP is the default service we will look at them in more details in the next lectures and this concludes this quick look at the concept of services in kubernetes let's take a look at the cluster IP so what is a cluster IP well it's the default service in kubernetes its visibility is cluster internal this means that it's not possible to use a cluster IP service to reach a macro service from the internet from outside the cluster in the cluster IP definition you can set two different ports Port is the port that the service will listen on and Target Port is the port that deselected pods are listening on so the cluster IP will route incoming traffic to that board in this CML file the service listens on Port 80 and Route the traffic to Port 8080. traffic is load balanced across the selected paths so when do you use a cluster IP service well to provide a durable way to communicate with pods but from inside the cluster so let's illustrate this here we have a cluster IP service fronting three instances of a pod it's impossible to reach it from outside the cluster but it's okay it's visible from inside the cluster this cluster IP will listen on Port ad and select the paths using these two labels since the selected pods are listening on port 8080 the service Target Port must also be set to 8080. this way the parts in green that want to communicate with the ones in purple well they go through the cluster IP service on Port 80 and the service route the traffic to Port 8080. so let's say you have multiple Marco Services uh you place a cluster IP in front of each of them because a cluster IP service IP address is durable while the pods ones are ephemeral here's a cheat sheet for cluster IP so the first two commands are imperative commands let's say you already have a pod running and you want to expose it using a cluster ID so you would use Cube CTL expose Oh short for path the Pod name specifying the the port and the Target Port and you can also give a name to your to your service if you have a deployment you can also use Cube CTL expose deploy the deployment name specifying the port and the target board so both commands are imperative commands if you have a yaml file you would use Cube CTL apply Dash F and specifying the EML file name you can get a list of the services running using cubectl get SVC I get a little bit more information specifying uh the flag Dash o and wide you can also describe the service using a cube CTL describe SVC and the service name and you can delete the cluster IP service using the yaml file with the cube CTL delete Dash F and the name of the yaml file or cubectl delete SVC and the name of the service and this concludes this look at the clusterity in this lab we will deploy an nginx container front it with a cluster IP service and then deploy a BusyBox container open a session on that BusyBox container and try to hit the web page served by the nginx container but through the cluster IP service all right let's take a look at our yaml file we'll start with the deployment one kind is a deployment we want three instances of the Pod and we will run the nginx image the Alpine version and it will list it on Port 80. now we're setting two labels here and the cluster IP service will you will select uh the pods using these two labels so app example environment prod all right let's take a look at our cluster IP kind service it's gonna listen on port 8080 and it will redirect the traffic to Port 80 on the nginx containers all right the selector is here so app example environment broad so that will select our pods uh in our deployment perfect so let's try to do that first let's uh deploy the service and let's deploy the nginx containers let's also deploy the busy box and we can take a look at this it's a kind of pod the name is my box and uh it'll run a busy box image okay so now let's get a list of our Bots currently running we should add four one two three four so the first three ones are the deployment the nginx images and the fourth one is the BusyBox all right let's try to connect to the BusyBox container open a session by using cubectl exec my box Dash it and the name of program we want to run perfect at work by type LS yep okay let's try to use the service to reach the nginx pods so wget and HTTP SVC example colon 880 so let's try to run that and see if it works okay that worked why did it work so what is that name here if we go back to the cluster IP definition that's the name of our service and it's listening on Port 8080. the service name Colin d uh the port that is listening on and that's it we can now exit our session on our busy box and we can delete our resources the cluster IP the deployment and the pod thank you let's take a look at the note Port service what is a note port a noteboard extends the cluster IP service and provides extra functionalities its visibility is internal like a cluster IP but also external to the cluster you can set a third port using the note Port property this is the port that the service will listen on outside the cluster note that the port must be in a range between 30 000 and 32 767. and if you don't specify a note Port value well kubernetes will assign one randomly you then set the port and the Target Port Properties like you do with a cluster IP this sounds like a good way to expose our macro services to external traffic but this range between 30 000 and 32 767 it's kind of annoying because you can't set it to let's say port 80. and One requirement for using note ports is that nodes must have public IP addresses to access your service simply specify any node Port IP address plus the note port and the traffic will be routed to the right note Port service inside the cluster the way it works is that you set the pods and the service just like you did before with the cluster IP but this time you also specify a port number in the note Port property external communication uses the node IP address and the port set with the node Port property internal communication uses the port set in the port property just like a cluster IP now let's take a look at our note Port cheat sheet if you already have a pod running in your cluster and you want to expose it using a note Port service simply use the cube CTL expose bow the Pod name specifying the port and the Target Port and the note Port as the type now you may wonder where do you specify the note port number well you can't there's no properties letting you set know that value between 30 000 and 32 767. so communities will assign one randomly for you same thing for a deployment let's say you have a deployment already running in your cluster and you want to expose it using a note Port you use cubectl expose deploy the deployment name Port Target Port type which is note port and you can specify your name also you can define a node port in a yaml file and deploy it using cubesatel apply Dash F the name of the yaml file you list the services using cubectl get SVC get more info adding Dash o wide he can describe your service using cubesatel describe SVC and the service name if you have a yaml file you can delete it using the that file using cubectl delete Dash F the name of the yaml file or you can delete your service using its name with qctl delete SVC and the service name and this concludes this look at the note Port service in this lab we will expose a deployment using a note Port service we have two yable files let's take a look at them the first one is for the deployment we will deploy an engine X image the Alpine version listening on Port 80. and we will need to replicas perfect let's take a look at the noteboard yaml file kind is service and the type is noteboard and the selector will select our deployment and here we set our node Port 32 410 okay now let's open a terminal and we'll start by the deployment cubesatel apply and our yaml file next our service Cube CTL apply noteboard.yaml awesome let's make sure that our pods are running Cube CTL getbots Dash o wide awesome two parts two instance of our nginx container all right now since we're using Docker desktop the docker desktop node is mapped to local OS to reach the note Port service we need to use local OS plus the note port let's try that local Post 32 410 and it worked awesome Now when using a cloud provider you would need to get a node IP address instead of using the Local Host you would get that IP address by using cubectl get nodes Dash o wide and here in the external IP address colon you would find the external IP address of the node awesome let's do our cleanup let's delete our note port and our deployment let's take a look at the concept of surfaces in kubernetes what problem do Services try to solve well if the pad in green need to reach the purple one it needs to use its IP address the problem is that pods are ephemeral if the purple one dies you need to replace it and the new one will have a different IP address we need a way to make these calls between pods more robust so what exactly is a service a service is a kubernetes object that you define in a yaml manifest unlike parts that have ephemeral IP addresses Services gets durable IP addresses and a DNS name they serve as a way to access paths and they target pods using selectors here we have four pods and a service the surface selects the paths that have the Zone label equals to prod and the version label equals to V1 the first part satisfies the selector the second one also but not the third one and the last one only the first two are selected let's say that we have two instances of a pod and we place a service in front of them if another part needs to reach these parts it will go through the service and the surface will load balance the requests to these instances in kubernetes we can use these services D cluster IP the note Port the load balancer and the Ingress the cluster IP is the default service its visibility is internal only the note Port can expose a pod outside the cluster the load balancer and the Ingress are similar Services they let you expose applications outside of the cluster one operates at D layer 4 and the other at layer 7. L4 L7 what's that download balancer operates at the layer 4. that's the TCP transport level so that's very low in the transport stack it means that the load balancer can do simple operations like round robin routing the Ingress operates at the higher level in the transport stack think of protocols like HTTP or SMTP it's more intelligent so you can configure complex routing rules okay no worries if this sounds complex for now simply remember that an Ingress is like a load balancer but more intelligent and this concludes this look at the concept of services in this lab we will create a load balancer service but you may be may be asking yourself we're not using a cloud provider right now how can that work well Docker desktop is helping us it will emulate the load balancer service so we can test our load balancer locally awesome let's take a look at the application it's a simple deployment we want to replicas two instances of an nginx image super simple and the the load balancer yaml file kind is service the type is load balancer it will listen on port 8080 and redirect traffic to the Pod that is listening on Port 80. all right all right so let's open a terminal okay and let's deploy the app and the load balancer perfect let's make sure that our pod are running by using Cube CDL getbots yes I have two uh my coupons are here perfect now to get the IP address of the load balancer we use Cube CTL get SVC Dash o wide foreign S as the IP address using a cloud provider load balancer service you would find here instead of local OS the public IP address of the load balancer so let's test this open a browser and type localhost 8080 and that works we reach our nginx pod awesome let's do our cleanup let's delete our load balancer and our application this is an introduction to the persistent Concepts in kubernetes we saw this slide earlier containers are ephemeris and stateless and any data stored in them is deleted when the container is destroyed so we need to find a way to store data outside the containers if you want to keep that data so volumes let containers store data into external storage systems these are storage services offered by the cloud providers Defenders create plugins according to a specification called the container storage interface and there are two ways to use storage in the cloud the static way and the dynamic way we have separate lectures on these later on all right the cloud providers create plugins to expose their storage Services as persistent volumes and storage class these two are kubernetes objects next we will look at the static and dynamic ways let's see how to use the static way persistent volumes or PV and persistent volume claims are PVCs are two kubernetes objects that lets you define and use external storage a purchasing volume represents a storage resource that is available cluster wide and is provisioned by the cluster administrator you then use a persistent volume claim to claim the persistent volume a part will then use the PVC to mount a local folder PVCs can be used by multiple parts and inside the parts all the containers are sharing the same volume there are many persistent volumes plugins available some are offered by the cloud providers the one highlighted in yellow called hostpath is a special one it's a plugin available with kubernetes that allow you to do local testing and it's not mapped to a cloud provider storage service it will not work in a multinode cluster but it's super useful for local testing here's the main drawback of persistent volumes let's say that the cluster admin provision 100 gigabytes of storage and that the Pod only requires a small portion of this storage let's say just one gigabyte of that 100 gigabytes in total so just one gigabyte well too bad for the other pods because the volume is used exclusively by the Pod who has the claim on it this can be a waste of precious resources and we will see how storage class get around this problem in the next lecture okay in the meantime let's focus on the PV and the PVC you first select the cloud provider storage service you want to use then you create a persistent volume and set the required capacity let's say here 10 gigabytes you then create a PVC so a claim that refers to the persistent value and finally you use the PVC from your pod and mount a local folder on it there's an inputs and property that you must be aware of it's the reclaim policy set a delete all data will be lost when the claim on the volume is released and this is the default value so be aware of this if you want to keep your files when the PVC is released you have to set the reclaim policy to retain again the default value is delete so be careful and be aware of this there are three access modes possible using read write mini the volume can be mounted as read write by many parts using readonly mini the volume can be mounted readonly by many parts and finally with read write once the volume can be mounted as real read write Sorry by one single path and the other parts will be in readonly mode this might be useful if you have a main worker that writes data and the other pods simply read the data you define a person in volumes using the purchasing volume kind and you specify the capacity the access mode and the reclaim policy in the spec section in this example the ospad plugin is used to access local storage remember to only use hostpat for local testing and refer to the storage provider documentation and on how to create a persistent volume specific for their storage service you then Define a claim so a persistent value claim making sure that the access mode match the one set in the processing value in this case the claim is for 8 gigabytes out of the possible 10 gigabytes set on the persistent volume this means that no one can claim the remaining two gigabytes until the claim is released in the volume section of your pod definition simply refer to the PVC and mount a local folder on it a persistent volume can have these states available meaning that the resource is free and not currently in use bound the volume is bound to a claim so it's in use it's not available anymore release the claim has been deleted but the resource is not yet reclaimed by the cluster and finally failed well something's wrong here's a cheat sheet for the PV and PVC commands using a yaml file you can create either a PV or PVC by using cubectl apply Dash F and the name of the yaml file you get the install persistent volume using cubecti Cube CTL getpv the claims using cubectl get PVC you can describe them cubectl describe PV or PVC with their name you can delete them using their yaml file cubicle delete F and the name of the yaml file or by using their name so Cube CTL delete PV and the PV name or the PVC name and this concludes this section as the static way next we'll take a look at the dynamic way in this lab we will create a persistent volume a persistent volume claim and use a pod to mount a local folder on that storage we will create that in Docker desktop locally using the host path plugin all right so let's first take a look at the persistent volume yaml file the kind is purchasing value we give it a name pv001 and a storage capacity 10 megabyte access mode read write once and we set the processing volume reclaim policy to retain and use that host Pat plugin here and to map to a folder in the docker desktop virtual machine to data here all right let's take a look at the PVC kind persistent volume claim we give it the name my claim read write access mode must be the same as the PV persistent volume so let's double check read write once on the PV read write once on the PVC awesome we request 10 megabytes of storage so we record the full capacity we could have chosen a lesser value if you want all right let's take a look at the Pod now so it's uh the kind of pod it's a busy box and uh we make a reference to the claim here in the volumes section we give it a name and we reference the persistent volume claim called my claim this guy here okay and we use that name mypd and we mount it to a local folder called demo we should see magically appear a folder called demo inside our BusyBox container okay let's deploy our persistent volume right percent in volume pv01 created awesome let's look at the PV qctl getpv name pv001 capacity 10 megabyte read write once reclaim policy to retain it's available it's not claimed uh awesome so let's now deploy the claim the PVC persistent volume claim my claim created awesome Cube CTL get PVC my claim it's bound to the volume called pv001 capacity 10 megabytes read write once and let's again take a look at the PV to see if something has changed yep it's now bound the status is bound to the claim called my claim running in the default name space awesome so let's now uh deploy our pod okay my PC box was created let's connect to it using Cube CTL exec the name of the instance so the Pod Dash it and the program you want to run so let's do NLS and see if we see a demo folder and there it is we have our demo folder Let's uh CD into that folder and let's create a a file inside cat and we'll pipe that to uh lo.txt let's type hello world world uh if I can type Ctrl D to exit and save the file Let's do an LS to see if the file was created perfect okay let's exit this session and now let's delete the pod let's delete the part by using cubectl delete Dash F pod and since the busy box takes uh 30 seconds to shut down we will force it to do it right away we don't want to wait okay Let's uh deploy it again well Cube CTL rctl get the pods no resource awesome it's really dead Let's uh deploy it again okay Let's uh open a session CD demo LS all right and let's get that file hello a car cat cat nut car hello world awesome it worked Let's uh exit our session let's now do our cleanup we'll delete our bud and then we will delete the PVC right and then the PV so you can't delete the PV before the PVC well you can issue the command but the the command will be in kind of a weight State uh until uh the PVC has been released let's continue our journey into persistence by looking at the dynamic way so here's a new object the storage class and the search class represent a storage resource that is available cluster wide and is provisioned by the cluster administrator you don't have to set a capacity and it eliminates the need for the admin to preprovision a persistent value now compared with processing volumes where once a claim has been made the remaining capacity becomes unavailable well the storage class can support many claims many persistent volume claims so you first select the cloud provider storage service that you want to use you create a storage class so here no need to specify a capacity then you create a PVC the claim that refers to the storage class and now you specify the required capacity and finally you use the PVC in your pod and mount a local folder like a persistent volume there's an important property that you must be aware of it's the reclaim policy set at delete all data will be lost when the claim is released and it's the default value also like the persistent volume so be aware if you want to keep your files when the PVC is released you have to set the reclaim policy to retain again the default value is delete again three access modes possible and they are set using the PVC not the storage class read write mini the volume can be mounted as read write by many pods read only mini the volume can be mounted readonly by many pods and lastly read write once the volume can be mounted as read write by a single part and the other parts will be in readonly mode useful if you have a main worker that writes data and the other pods simply read the data so you first start defining a storage class specifying the cloud provider driver with the provisioner property and additional settings in the parameters section so refer to the source provider documentation on how to create a specific storage class further storage service you then Define a PVC specifying an access mode and the source capacity required in this claim uh the claim is for 5 gigabytes but more PVCs can be created over that storage class then simply refer to the PVC in your path definition and map a local folder on it in summary the main benefits of a storage class versus a purchasing volume is that with a storage class you don't have to define a capacity and multiple Claims can be made here's a cheat sheet for storage class at Men's so you create your storage class using a yaml file using Cube CTL apply Dash F and your the name of your yaml file you get a list of your storage classes or PVCs using get the SC for search class and get PVC you get the search as information by using cubectl described as C and the class name you delete your search class and PVC using cubectl delete Dash F and the yaml file name or you delete your search class using delete SC and the class name or delete PVC and the PVC name and this concludes this section about persistence using the dynamic way let's see how to store configuration values using config Maps in a previous lecture we saw that it was possible to place configuration values directly in the environment section of a pod definition but what if we need to change that value well we have to edit the Manifest and redeploy the container also usually it's not a best practice to tie an object with its configuration so how can we externalize these values the config map object allow you to decouple and externalize configuration values the key value pairs are then injected in the containers as environment variables they can be created from yaml files a series of text files or even folders containing files they are static meaning that if you change the value the containers that refer to these values have to be restarted to get these refresh values using a yaml file you define a config map and place the key value pairs in the data section you can even specify multiline values using the pipe character in the EnV section environment section of the container definition you define an environment variable and by using value from an config map key ref you reference the config back name and the key as defined in the config map so in the config map key ref section name refers to the config map name and key refers to a key in the config map earlier I mentioned that this is a static process meaning that the values are injected when kubernetes starts the container this means that if you make a change to a config map value inside the container the original values stay the same until you restart the container to get around this you can map a volume on a config map Yes you heard it right mounting a volume this solves the static issue and updates are reflected in containers each key value pair is seen as a file in the mounted directory so we start with a config map then use a volume to mount it to a local folder inside our pod our container the result is that all key value pairs are now scenes as file the name of the file being the key and the value being inserted in the file while this sounds cool it also means that you'll have to refactor your code so instead of reading environment variables you'll have to read files so is it worth it you'll have to figure out that by yourself here's the config Maps cheat sheet so if you're adventurous you can create a config map from the command line that's the imperative Way by using Cube CTL create config map you give it a name and with the from Dash literal parameter you specify the key value pairs so you can specify multiple key value pairs of the same line or you can use a good old yaml file and use cubectl apply Dash F and the name of your yaml file you can create a config map using cubectl Create CM specify your name and specifying a the name of a text file containing multiple key value pairs also you can create a config maps from a folder so if you have multiple files inside your folders you can create a config map from that you can get a list of the config maps by using cubectl getcm you can output the config map in a yaml file by using cubectl get CM the name of the config map and the dash o parameter with the yaml and you can pipe that to to a file name of course you delete a config map by using its yaml file using cubectl delete Dash F and the name of the yaml file and this concludes this uh look at config Maps in this app we're going to create a config map and use a pod that will reference a value stored in that config map let's take a look at the config map so the kind is config map we have a name cmexample and in the data section we have two key value pairs date as set to Michigan and City and our board all right so let's now take a look at our pod tiny spot uh it's a busy box and here in the environment section we declare in environment variable that we will call City and we are getting the value from a config map key ref right and we specified the name of the config map so cmexample that's the name of the config map and the key is City so here that's the key right there okay again environment section we Define a new environment variable that we will call City we get the value from config map key ref specifying the name of the config map and the key awesome let's create our config map okay Cube CDL gets cm to get information about our config map so CM example two data okay that doesn't give us much information well about the data itself so let's do a cube Studio describe config map CM example okay we have the name the namespace where it is any labels annotations and here we have uh the data section City and Airport State Michigan and if for some reason you want to Output that as a yamo uh Cube CDL get config map the name of the config map Dash o output in yaml uh to recreate the config map using this let's now deploy the pod or BusyBox perfect let's open a session Cube CDL exec my box Dash it and the program you want to run and let's display the city environment variable let's Echo that Echo dollar sign City and there it is an arbor so that worked foreign exit and we can do a little bit of cleanup we can delete our config map and we can delete our busy box spot let's see how to use the secrets object in kubernetes you will find many times of Secrets types the default one is the OPAC type and it is very similar to the config Maps object that we saw in the previous lecture you can also store credentials to connect to private container registries authentication secrets and even certificates in this lecture we will focus on the OPEC Secret like config Maps secrets are used to store configuration values so they are somewhat identical to config Maps except that these store values as base64 encoded strings and it's important to understand that base64 is a way to encode strings and it is not an encryption algorithm this means that Secrets stored in kubernetes can be decoded quite easily yeah great since these secrets are Noah very secret should you use them well the answer depends on the type of information you want to store it might be okay to store a connection to a database but it might not be for something more sensitive you can protect Secrets using rolebased access control policies are back or you can store them elsewhere all Cloud providers offered ways to store secrets in Seeker Vault services that you can retrieve firm kubernetes you can also use a third party tool like the very popular Vault product from archicup just be aware that the kubernetes default secret the OPEC one is not encrypted by default in kubernetes so you can define a secret in a manifest and use a base64 encoded strings as the values or use the command line where you can use plain text strings easier in the path definition you simply get the secret value using the secret key ref section this is very similar to config Maps and again similar to config maps you can mount a volume on top of Secrets here's the container registry secret and you can Define it using a yaml file or with the CLI next in the path definition you reference the credentials in the image pull Secrets section here's a cheat sheet for Secrets commands so you can create a secret the imperative way at the common line if you want using cubesdl Create secret generic and then the secret name and you pass the key value pairs with the from Dash literal per meter you can of course create one using a yaml file so cubesdl apply Dash F and the name of the yaml file you can get a list of the secrets using cubectl gets secrets you can output the secret to a yaml file by using cubectl get Secrets the secret name and the dash o yaml parameter and you can pipe that to a a file you can delete the secret using a yaml file or using the secret name using cubectl delete secrets and with the secret name and this concludes this look at secrets in this lab we'll create a secret and from a pod we will reference secret and use them as environment variable let's take a look at our secrets at yaml file the type or the kind is secret we give it a name secrets and in the data section we have key value pairs so username to some value and password to some value notice that the values but must be base64 encoded you cannot put a nonencoded string here how do you do that on Windows base64 the the tool is not installed by default so you can use these two websites basics of foreign code.org decode.org or you can install base64 using chocolati as shoko installed base64 on macro Linux well it's already installed so you simply do something like that you Echo your string and you pipe that to base 64. and that will encode the string and let me copy that and to decode Echo uh the encoder string and you pipe that again to base64 Dash D for decode and voila all right from the Pod now let's take a look at our pod yaml definition kindness part it's a busy box and here in the environment variable we're creating two environment variable the first one is called username and it gets its value from a secret key ref and the name references the secret name here and the value well the key is one of these two so username or password Here a case key reference here the username from the secrets Secret and second one is password a the environment variable is called password and we get the value from the secrets secret and the key is password awesome let's create the secret all right let's get a list of the secrets here's our secret if it has two data it was created six seconds ago we can describe it cubicity of describe the object type and then it name and here what do we have the name namespace and well we don't see the secret just the the keys here and what if we use Cube CTL get secrets and output that to demo the secret data password say uh doing a describe would not allow us to see the values but using get secret and outputting that to a yaml will allow us to retrieve the actual values okay let's now deploy our busy box right let's open a session and let's Echo uh username D username and the password whoops Echo uh password my password so that that works the uh the values the secrets are decoded when they're injected into into the pods let's exit that and let's delete our secret and let's delete our busy box let's talk about observability if you deploy a container using a deployment and it crashes kubernetes will create a brand new instance of the pod this works because kubernetes monitors the infrastructure but what about your application if your app crashes well kubernetes will look at the Pod health and see that it's still running so from the infrastructure point of view everything is working fine your bud is still up and running but your code inside the Pod has crashed would it be nice if kubernetes could monitor the application health well you can achieve this by configuring probes the startup probe informs a kubernetes that the container has started and it's now okay to send traffic to it the Readiness probe enforced kubernetes that the container is now ready to accept traffic let's say that when your app starts it needs to execute a series of steps like getting some configuration values from a database creating some files and so on and so on and let's say that this startup sequence takes around 30 seconds so even if your container is up and running your code is not ready to accept traffic so using a Readiness probe you tell kubernetes to wait 30 seconds before starting to send traffic lastly the liveness probes tell kubernetes if your app is still running and if not kubernetes will kill the Pod and replace it with a brand new one here's a part definition with the three possible probes the starter probe tell kubernetes to wait for 10 seconds before making an HTTP call to a page called health the failure threshold tells kubernetes to try three times the Readiness probe tell kubernetes to wait initially five seconds before probing and making a TCP call on port 8080 and then check every 10 seconds the liveness probe tell kubernetes to wait initially for 15 seconds before probing by making a TCP call on port 8080 and then check every 20 seconds note that the Readiness probe will run during the whole pod life cycle but will these two Conflict at some point yes and no they will run simultaneously but a fail probe will result in different actions from kubernetes failing a Readiness probe will tell kubernetes to stop sending traffic to the pod but the Pod is still alive right while failing a liveness probe will tell kubernetes to restart the pod how does kubernetes probe the containers the cubelet will do the probing using the method you configure with exec action kubernetes will run a command inside a container with TCP socket action kubernetes check if a TCP socket port is open and with HTTP get action kubernetes perform nhdp get here's an exec action you're telling kubernetes to run a cat command on a file called healthy in the TMP folder here's a TCP socket action you're telling kubernetes to check if there's an open socket on port 8080 and finally an HTTP get action you're telling kubernetes to do an HTTP get on the Health page on Port 8080. and this concludes this look at observability in this lab we will set a liveness probe so let's take a look at our yaml file we will deploy a pod that we'll call liveness Dash example is going to be a PC box and this is where we're setting the probe type is liveness Pro we're asking to do an exacto so to run a command inside our container and the command is cat and the parameter is that file so under the TMP folder the file is called healthy no extension basically we're asking kubernetes to run that command if that command is successful if the file exists well the probe is successful if the file doesn't exist the probe will fail okay uh initial delay seconds we're asking kubernetes to wait for five seconds before starting to probe and then period seconds to five we're asking kubernetes to probe every five seconds the last parameter is failure threshold set to two basically we're telling kubernetes that the liveness probe will fail when two probes will fail now for uh the purpose of this lab we will set something a little bit funky uh just so we're able to do this test we're um we're running a command when the container starts uh it's dutch and basically this will create that healthy file and then we're telling the container to wait for 15 seconds and then to delete that file right and just a little trick too so we'll be able to to quickly see the effect of the lavenous probe okay Let's uh deploy our pod and let's quickly do a cube CTL describe pod all right successfully pull the image the container was created okay so let's do that again a few times and we'll see what happens just rerun the command out liveness profile cat can't open okay two times over five seconds and now kubernetes is killing the Pod awesome and uh is still killing so you see that here the the Pod was in an unhealthy State here so now it's in the killing mode oh and then uh kubernetes is pulling again the BusyBox image and starting a new one right and then it's on LT again and the process starts uh starts again all right let's do our cleanup let's delete our pod and we'll force it you don't want to wait for it to end perfect let's take a look at some dashboards while it's fine to use a terminal to get a view of your cluster You may wish to use a graphical user interface instead luckily there are many options available we will take a look at these three popular and free options the kubernetes dashboard the lens desktop application and K9s a dashboard that runs in the terminal let's start with the kubernetes dashboard it's a web UI that you can install as an addon inside your cluster it's not installed by default by Docker desktop and also by most Cloud providers the rule of thumb is if you don't need it don't install it a because it runs inside the cluster so you need to find a way to expose it over the internet and B well because of that it's a known Vector of attack that being said the kubernetes dashboard that you see the various resources deployed inside your cluster simply select the type in the left menu and you can also edit them by clicking on the edit icon and you can edit the yaml file and click on update to change the Manifest file lens is a IDE that runs locally on Mac windows and Linux so you need to install it on your OS on top of viewing the resources and editing the yaml Manifest you can also use a builtin editor and also a builtin terminal lens is maintained by mirantis and you can download it from this URL here's the overview dashboard that lets you see a resource count like the kubernetes dashboard you can select a resource click on the edit button and it did the Manifest directly in lens you can also type commands using the builtin terminal K9s is a cool text dashboard that runs in a terminal and you can install it on Windows Mac and Linux it might sound strange to run the dashboard in a terminal but this makes a lot of sense Kina NS is super light starts in an instant and gives you a clean view of all your resources you can get the information about the cluster the resources and you can take a or make a series of action like deleting a resource viewing the logs here we have a list of the pods currently running in the default namespace want to view the services that are currently running simply type colon and type SVC to list the objects type foreign pressing s while a part is selected will open a shell you can also set a port forward by typing shift f you can even see the part logs I really like this K9s dashboard in a terminal and use it all the time and this concludes this look at the kubernetes dashboards thank you in this lab we'll take a look at Lens so lens is a free dashboard that you can install on Windows Mac and Linux here's the URL k8s lenss.dev let's take a look at the website from there you can install it it runs on Mac windows and Linux so you can download the setup files here or if you're using a security on Windows you can use shoko install lens or Brew on Mac and Brew install cast lens all right I've already launched a lens here and by default it should take you to your Docker desktop cluster if not or if you want to select another cluster click on the hamburger menu to the left and select add cluster Lance will look at the cube config file and you'll be able to select your clusters from that drop down list so here I have my demo cluster in the cloud so I can select that and click on ADD cluster I can be connected to multiple clusters here's my Docker desktop here's my demo so let me switch back to dockerdist up here okay so first thing first uh let's deploy something on our cluster here I have a yaml file it's a deployment we'll have three replicas of an image called Hello app okay nothing fancy here let's deploy this all right and let me switch back to uh lens here now let's click here on workloads and overview so we should see three parts running here one deployment and one replica set I can either from the top menu select path or from this workload menu select paths here and the other type of objects here so here are my three parts I have one deployment and one replica set if I click on an object I get more information labels annotations and so on and so on so something that you would have at the terminal at the common line by typing Cube CTL describe uh replica set or deployment or pod and the object name here but here is presented in a nice UI all right let's take a look at our pods here okay let's see what we can do let's say we want to delete this one so I can select it like this and from the Ellis menu to the right I can open that and select remove right or I can remove it right there okay let's do that remove item hello yep remove it and since it's a deployment uh kubernetes will create a new One automatically pretty cool uh let's uh take a look at the logs we can see the logs from there we can open a shell so at last a pretty cool front directly from that UI if I click on edit well I can edit the Manifest file and click on Save and close that will update that object let me cancel that close close this there's a builtin terminal so by default you have a small icon here called terminal if you click on it if you click on this open button that will open the terminal so I can type command Cube C Cube CTL get the pods right you can try to delete an object directly here so let me copy this object name Cube CTL delete uh and let's paste that we should see right right away we see uh something happening uh at the top of the screen all right uh what's cool is that if you have more than one cluster you can switch between these clusters so here let me uh just to prove it clear and the qctl uh get the nodes okay on Docker desktop I have only one node let me select My Demo cluster my terminology open it there's no deployment yet but let's just do a cube CTL get nodes and for sure I have three nodes in this in this cluster that's pretty cool just by selecting the cluster I can switch and I have a terminal that is in the right context pretty cool so I just scratched the surface there's a lot more information that we can get like a configuration the config Maps the secrets what the network services that are installed here I have a cluster IP service storage so the persistent volume claims persistent volume storage classes so here in my cluster in the cloud I have four search classes defined for me the namespaces and so on and so on all the information that you would get from the command line using cubectl you get the same information but from a nice UI so once you're done exploring don't forget to delete your deployment thank you in this lab we will use K9s a super great dashboard running inside a terminal so you can get more information about the K9s at the website k9scli.io here's the website nice logo and you can look at the documentation and the how to install K9s from that website if you're on Windows you can install it using chocolaty so Chico install K9s on Mac OS Brew install K9s and on Linux take a look at the documentation all right Let's uh first deploy something in our clusters here in our yaml file I have a simple deployment with three replicas of a simple container or image called Hello app nothing fancy here let's deploy that uh right away okay and let's open a terminal or come in line comment prom whatever you name it and let me type K9s all right let me stretch that a little bit well it's super cool it's a dashboard running inside the terminal it's super light and it gives you tons of functionality so here I have my deployment so I'm in the default namespace you can look at my my deployment I can enter on an object I get more information type Escape go back I have information about my cluster uh I can issue some some commands so Ctrl D I can delete a a pod let me do that let me kill that poor part here uh are you sure yes see you have feedback visual feedback of of uh what's happening the Pod that I deleted was kill was in the terminating State and the new one was uh created uh right away I can yeah it D to describe the resource escape to uh to go back Ctrl K to kill I just deleted one but I can do a Ctrl k there we go I can see the log so let's let's switch to the second one type l and I can look at the uh the logs I can open a shell also by typing here the S letter doing the less right you can type exit go back I can uh even configure a port forward pretty cool here I can look at the yaml file also let me type Escape here we have the parts that are listed but if I type Colin I can change that let's say let me type deploy so here's my deployment now if I type e I will edit the deployment and let's change the number of senses or replicas from three to four and let's close that right and now I have four out of four so let's uh I'll type Colin again and type but here are my four pods super interesting it's a free tool it's super light it's super fast and I always have one open so I can see visually what's happening uh inside my cluster when I'm issuing some some commands let's go back uh to uh our page here our Visual Studio code and let's simply delete our deployment and let me switch back right away here and they're gone pretty cool too let's see how to scale pods the horizontal pad to scalar is a kubernetes feature that allows you to scale the number of Parts up and down it uses the metric server to gather the pods utilization pods app must have requests and limits defined the HPA checks the metric server every 30 seconds and scale the number of PODS according to the minimum and maximum number of replicas defined to prevent racing conditions the HPA Waits some period of time after a scaling event by default this delay on scaleup events is 3 minutes and the delay on scaled down events is 5 minutes in this part definition you specify the CPU and memory requests and limits the request is what's allocated at first and the limit is what you allow the part to burst to in this example the Pod will start with 64 megabyte of ram but can burst up to 128 megabyte if needed you configure the HPA using a manifest specifying the deployment you want to scale the Min and max number of replicas and the metric you want the HPA to scale on in this case we tell the HPA to kick in when the average CPU utilization is above 50 percent here's the cheat sheet for the HPA commands so you can create one using the imperative way using cubectl autoscale deployment the name and the metric and replicas number you can create one using the yaml file you can get the autoscaler status by using cubectl get HPA and the HPA name and of course you can delete the HPA using the yaml file or cube CTL delete HPA with its name foreign this lecture about scaling pods in this lab we will use the horizontal particle scalar to scale a pod for the HP it worked it needs some data some metrics coming from the metric server and by default it's not installed by Docker desktop just make sure I'll open the terminal and what we'll do we'll get a list of the parts running in the cube system namespace Accord DNS at CD Cube API server proxy provisioner no nothing that looks like metric server okay to install it you need to run this yaml file coming from this git repo on kubernetes6 special integers group and Metric server but you need to make a small modification to to it let's take a look at the components that yaml file need to do you need to edit it if you download it directly from the git repo and you locate the deployment section there it is deployment and what you need to do is add this parameter a cubelet dash insecuretls if not the metric server will not run on Docker desktop all right let's deploy our metric server cubectl apply Dash F components Dot yaml awesome what we can do we can run again Cube City I'll get pod in the cube system namespace aha metrics server there it is it might take a few a couple of minutes for the metric server to start running now that the metric server is running let's take a look at our deployment uh it's a deployment and what we'll deploy is a web server called HPA Dash example uh listening on Port 80. so it's a simple web server that will return a web page nothing fancy what we'll do uh will also deploy a busy box and from that busy box will it that web server uh in a loop and that should generate some traffic awesome let's deploy our web server let's get a list of the pods running in the default namespace awesome it's running let's enable our Auto scaler so Cube CTL Auto scale a deployment called HP Dash deployment and a metric called CPU percent and we want a minimum of one instance and a maximum of four okay let's validate that we have an HPA Cube CTL get HPA that will list all the hpas running on my cluster there's only one awesome let's now deploy our busy box and let's connect or open a session on that busy box perfect and here's our endless loop that will uh it the web server that should generate some traffic okay let's take a look at our K9s I have my deployment here my HPA deployment and my busy box and area we have three instances of our deployment right now awesome so the HPA worked and now what I can do is start the loop by hitting Ctrl C and I'll type exit to exit my busy box and from there I can delete my HPA but be careful when you do that I'll delete my HP here if we take a look at the deployments there's still three nothing will scale that down since the HP uh has been deleted let's delete our busy box and let's delete our deployment that should delete all three instances take a look here yep all three are terminating and optionally you can delete the metric server by using cubectl delete and the components.yaml file here we are at the end of this Docker containers and kubernetes fundamental scores congratulations you are now an official kubernetes Ninja the next steps for you would be to deploy containers in the cloud using services from a cloud provider these courses will teach you how to do that on Google cloud and Azure and also on smaller Cloud providers like Linux and digitalocean the best part is that each offer free credit usage when creating new accounts this way you can create a managed kubernetes service in the cloud without breaking the bank if you enjoyed the course you can help me by making a small donation this is the link to my buy me a coffee page I want to say a big thank you for learning Docker and kubernetes using my course and I wish you all the best
