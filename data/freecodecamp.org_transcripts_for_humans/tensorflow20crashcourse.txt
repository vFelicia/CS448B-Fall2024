With timestamps:

00:00 - hey guys and welcome to a brand new
00:02 - tutorial series on neural networks with
00:05 - python and tensorflow 2.0 now tensorflow
00:08 - 2.0 is the brand new version of
00:10 - tensorflow still actually in the alpha
00:12 - stages right now but it should be
00:14 - released within the next few weeks but
00:17 - because it's an alpha tensorflow has
00:18 - been kind enough to release us that
00:20 - alpha version so that's what we're gonna
00:21 - be working with in this tutorial series
00:24 - and this will work for all future
00:26 - versions of tensorflow 2.0 so don't be
00:29 - worried about that now before i get too
00:31 - far into this first video i just want to
00:33 - quickly give you an overview of exactly
00:34 - what i'm going to be doing throughout
00:35 - this series so you guys have an idea of
00:37 - what to expect and what you're going to
00:39 - learn now the beginning videos and
00:41 - especially this one are going to be
00:43 - dedicated to understanding how a neural
00:45 - network works and i think this is
00:47 - absolutely fundamental and that you have
00:49 - to have some kind of basis on the math
00:51 - behind a neural network before you're
00:53 - really able to actually
00:55 - properly implement one now tensorflow
00:58 - does a really nice job of making it
00:59 - super easy to implement neural networks
01:01 - and use them but to actually have a
01:03 - successful and complex neural network
01:05 - you have to understand how they work on
01:07 - the lower level so that's what we're
01:09 - going to be doing for the first few
01:10 - videos after that what we'll do is we'll
01:12 - start designing our own neural networks
01:14 - that can solve the very basic uh mnist
01:17 - data sets that tensorflow provides to us
01:20 - now these are pretty straightforward and
01:21 - pretty simple but they give us a really
01:22 - good building block on understanding how
01:25 - the architecture of a neural network
01:26 - works what are some of the different
01:28 - activation functions how you can connect
01:30 - layers and all of that which will
01:32 - transition us nicely into creating our
01:34 - own neural networks using our own data
01:36 - for something like playing a game now
01:38 - personally i'm really interested with
01:40 - neural networks playing games and i'm
01:42 - sure a lot of you are as well and that's
01:44 - what i'm going to be aiming to do near
01:45 - the end of the series on kind of our
01:46 - larger project i'll be designing a
01:49 - neural network and tweaking it so it can
01:51 - play a very basic game that i've
01:53 - personally designed in python with
01:55 - pygame
01:56 - now with that being said that's kind of
01:59 - it for what we're going to be doing in
02:00 - this series i may continue this on
02:02 - future uh in later videos and do like
02:04 - very specific neural network series
02:05 - maybe a chat bot or something like that
02:07 - but i need you guys to let me know on
02:09 - what you'd like to see in the comments
02:10 - down below with that being said if
02:12 - you're excited about the series make
02:13 - sure you drop a like on this video and
02:15 - subscribe to the channel to be notified
02:17 - when i post the new videos and with that
02:19 - being said let's get into this first
02:21 - video on how a neural network works and
02:23 - what a neural network is so let's start
02:25 - talking about what a neural network is
02:28 - and how they work now when you hear
02:30 - neural network you usually think of
02:32 - neurons now neurons are what compose our
02:34 - brain and i believe don't quote me on
02:36 - this we have billions of them in our
02:39 - body or in our brain now the way that
02:41 - neurons work on a very simple and high
02:43 - level is you have a bunch of them that
02:45 - are connected in some kind of way so
02:47 - let's say these are four neurons and
02:49 - they're connected in some kind of
02:51 - pattern now in this case our pattern is
02:54 - completely like uh like random we're
02:56 - just arbitrary we're just picking a
02:57 - connection but this is the way that
02:59 - they're connected okay
03:00 - now neurons can either fire or not fire
03:04 - so you need to be on or off just like a
03:05 - one or zero okay so let's say that for
03:08 - some reason this neuron decides to fire
03:10 - maybe you touch something maybe you um
03:13 - smelt something something fires in your
03:16 - brain and this neuron decides to fire
03:18 - now it's connected to in this case all
03:20 - of the other neurons so what it will do
03:22 - is it will look at its other neurons and
03:24 - the connection and it will possibly
03:26 - cause its connected neurons to fire or
03:29 - to not fire so in this case let's say
03:31 - maybe what this one firing causes this
03:33 - connected neuron to fire this one to
03:36 - fire and maybe this one was already
03:38 - firing and now it's decided it turned it
03:40 - off or something like that okay
03:42 - so that's what happened now when this
03:44 - neuron fires well it's connected to this
03:46 - neuron and it's connected to this neuron
03:48 - well it's already got that connection
03:49 - but let's say that maybe when this one
03:51 - fires it causes this one to unfire
03:54 - because it was just fired something like
03:55 - that right
03:57 - and then this one now that it's off it
03:59 - causes this one to fire back up and then
04:00 - it goes it's just a chain of firing and
04:02 - unfiring
04:04 - and that's just kind of how it works
04:06 - right firing and unfiring
04:09 - now that's as far as i'm going to go
04:10 - into explaining neurons but this kind of
04:11 - gives us a little bit of a basis for a
04:13 - neural network now a neural network
04:15 - essentially is a connected layer of
04:18 - neurons or connected layers so multiple
04:21 - of neurons so in this case let's say
04:24 - that we have a first layer we're going
04:25 - to call this our input layer that has
04:28 - four neurons and we have one more layer
04:31 - that only contains one neuron
04:34 - now these neurons are connected
04:36 - now in our neural network we can have
04:38 - our connections happening in different
04:40 - ways we can have each uh what he called
04:43 - neuron connected to each other neuron so
04:45 - from layer to layer or we can have like
04:47 - some connected to others some not
04:49 - connected some connected multiple times
04:51 - it really depends on the type type of
04:53 - neural network we're doing now in most
04:55 - cases what we do is we have what's
04:57 - called a fully connected neural network
04:59 - which means that each
05:01 - neuron in one layer is connected to each
05:04 - neuron in the next layer exactly one
05:06 - time so if i were to add another neuron
05:09 - here
05:10 - then what would happen is each of these
05:13 - neurons would also connect to this
05:15 - neuron one time so we would have a total
05:17 - of eight connections because four times
05:19 - two is eight right and that's how that
05:21 - would work now for simplicity stake
05:23 - we're just going to use um one neuron in
05:26 - the next layer just to make things a
05:28 - little bit easier to understand
05:30 - now all of these connections have what
05:33 - is known as a weight now this is in a
05:35 - neural network specifically okay so
05:37 - we're going to say this is known as
05:38 - weight one this is known as weight two
05:41 - this is weight three
05:42 - and this is weight four
05:44 - and again just to re-emphasize this is
05:46 - known as our input layer because it is
05:49 - the first layer in our connected layers
05:52 - of neurons okay
05:53 - and going with that the last layer in
05:56 - our connected layer of neurons
05:58 - is known as our output layer
06:00 - now these are the only two layers that
06:03 - we really concern ourselves with when we
06:06 - look and use a neural network now
06:08 - obviously when we create them we have to
06:10 - determine what layers we're going to
06:11 - have and the connection type but when
06:13 - we're actually using the neural network
06:15 - to make predictions or to train it we're
06:18 - only concerning ourselves with the input
06:19 - layer and the output layer
06:21 - now what does this do and how do these
06:23 - neural networks work well essentially
06:25 - given some kind of input
06:28 - we want to
06:29 - do something with it and get some kind
06:30 - of output right in most instances that's
06:33 - what you want input results in an output
06:35 - in this case we have four inputs and we
06:38 - have one output but we could have a case
06:40 - where we have four inputs and we have 25
06:43 - outputs right it really depends on the
06:45 - kind of problem we're trying to solve
06:47 - so this is a very simple example but
06:49 - what i'm going to do is show you how we
06:51 - would or how a neural network would work
06:54 - to train a very basic snake game
06:57 - so let's
06:58 - look at a very basic snake game so let's
07:00 - say this is our snake okay and this is
07:03 - his head um
07:06 - actually yeah let's say this is his head
07:08 - but like this is what the position of
07:09 - the snake looks like where this is the
07:10 - tail okay we'll circle the tail
07:13 - now what i want to do is i want to train
07:15 - a neural network that will allow this
07:17 - snake to stay alive so essentially its
07:20 - output will be what direction to go in
07:22 - or like to follow a certain direction or
07:24 - not okay essentially just keep this
07:26 - snake alive that's what i want it to do
07:28 - now how am i going to do this well the
07:30 - first step is to decide what our input
07:32 - is going to be and then to decide what
07:33 - our output is going to be so in this
07:35 - case i think a clever input is going to
07:37 - be do we have something in front of the
07:39 - snake do we have something to the left
07:41 - of the snake and do we have something to
07:43 - the right of the snake because in this
07:44 - case all that's here is just the snake
07:46 - and he just needs to be able to survive
07:48 - so
07:49 - what we'll do is we'll say okay is there
07:50 - something to the left yes no something
07:52 - in front yes no so zero one something to
07:54 - the right yes no and then our last input
07:57 - will be a recommended direction for the
07:59 - snake to go in so
08:02 - the recommended direction could be
08:03 - anything so in this case maybe we'll say
08:04 - the recommended direction is left and
08:07 - what our output will be is whether or
08:08 - not to follow that recommended direction
08:11 - or not or to try to do a different uh
08:14 - recommendation essentially or go to a
08:15 - different direction
08:17 - so let's do one case on how we would
08:20 - expect this neural network to perform
08:22 - without train like once it's trained
08:23 - right based on some given input so let's
08:26 - say there's
08:27 - not something to the left so we're going
08:28 - to put a 0 here because this one will
08:30 - represent if there's anything to the
08:31 - left
08:32 - the next one will be
08:34 - front
08:35 - so we'll say well there's nothing in
08:36 - front
08:37 - the next one will be to the right so
08:39 - we'll say right and we'll say yes there
08:41 - is something to the right of the snake
08:43 - and our recommended direction can be
08:45 - anything we'd like so in this case we
08:47 - say the recommended direction is left
08:48 - and the way we'll do the recommended
08:50 - direction is
08:52 - negative 1 0 1 where negative 1 is left
08:56 - 0 is in front
08:58 - and 1 is to the right okay
09:01 - so we'll say in this case our
09:02 - recommended direction is negative 1
09:04 - and we'll just denote this by
09:06 - direction now our output
09:09 - in this instance should either be a zero
09:12 - or a one representing do we follow the
09:15 - recommended direction or do we not
09:17 - so let's see in this case
09:19 - following the recommended direction we
09:21 - keep our snake alive so we'll say one
09:23 - yes we will follow the recommended
09:25 - direction that is acceptable that is
09:26 - fine we're going to stay alive when we
09:28 - do that
09:29 - now let's see what happens when we
09:31 - change the recommended direction to be
09:33 - right so let's say that we say one as
09:36 - our recommended direction again this is
09:38 - dern here
09:39 - then what should our output be well if
09:41 - we decide to go right we're gonna crash
09:43 - into our tail which means that we should
09:46 - not follow that direction so our output
09:48 - should be zero so i hope you're
09:49 - understanding how we would expect this
09:51 - neural network to perform
09:54 - all right so now how do we actually
09:56 - design this neural network how do we get
09:58 - this work how do we train this right
10:00 - well that is a very good question and
10:02 - that is what i'm going to talk about now
10:03 - so let me actually just erase some of
10:05 - this stuff so we have a little bit more
10:06 - room to work with some math stuff right
10:09 - here but right now what we start by
10:11 - doing is we start by designing what's
10:13 - known as the architecture of our neural
10:14 - network so we've already done this we
10:16 - have the input and we have the output
10:19 - now each of our inputs is connected to
10:21 - our outputs and each of these
10:22 - connections has what's known as a weight
10:25 - now another thing that we have is each
10:27 - of our input neurons has a value right
10:29 - we had in this case we either had 0 or
10:32 - we had 1.
10:33 - now these values can be different right
10:35 - these values can either be decimal
10:37 - values or they can be like between 0 and
10:39 - 100 they don't have to be just between 0
10:41 - and 1 but the point is that we have some
10:43 - kind of value right so what we're going
10:45 - to do in this output layer to determine
10:48 - what way we should go is essentially we
10:50 - are going to
10:51 - take the weighted sum of the values
10:54 - multiplied by the weights i'm going to
10:55 - talk about how this works more in depth
10:56 - in a second but just just follow me now
10:59 - so what this symbol means is take the
11:00 - sum
11:01 - and what we do is i'm going to say in
11:03 - this case i which is going to be our
11:05 - variable and i'll talk about how this
11:07 - kind of thing works in a second we'll
11:08 - say i equals 1 and i'm going to say
11:10 - we'll take the weighted sum of in this
11:12 - case
11:12 - value i
11:14 - multiplied by
11:16 - weight i
11:17 - so what this means essentially is we're
11:19 - going to start at i equals 1 we're going
11:21 - to use i as our variable for looping and
11:24 - we're going to say in this case we're
11:25 - going to do v1 times vi or sorry vi
11:29 - times wi and then we're going to add all
11:31 - those so what this will return to us
11:33 - actually will be
11:34 - v1
11:35 - w1 plus v2
11:38 - w2 plus b3 w3 plus v4 w4
11:45 - and this will be
11:47 - uh our output that's that's what our
11:49 - output layer is going to have as a value
11:52 - now this doesn't really make um much
11:55 - sense right now right like why why are
11:56 - we doing this weights what is this
11:58 - multiplication well just follow with me
12:00 - for one second
12:01 - so this is what our output layer is
12:03 - going to do now there's one thing that
12:05 - we have to add to this as well and this
12:08 - is what is known as our biases okay so
12:11 - what we're going to do is we're going to
12:12 - take this weighted sum but we're also
12:14 - going to have some kind of bias on each
12:17 - of these weights okay and what this bias
12:19 - is known as it's denoted by c typically
12:23 - but essentially it is some value that we
12:25 - just automatically add or subtract it's
12:27 - a constant value for each of these
12:29 - weights so we're going to say all of
12:30 - these these connections have a weight
12:32 - but they also have a bias so we're going
12:33 - to have b1
12:35 - b2
12:36 - b3 and b4
12:38 - uh
12:39 - well we'll call it b instead of c so
12:41 - what i'll do here is what i'm also going
12:43 - to do is i'm also going to add these
12:44 - biases in when i do these weights so
12:46 - we're going to say bi as well
12:49 - so now what we'll have is we'll have at
12:50 - the end here plus
12:52 - bi or plus b1
12:54 - plus b2
12:56 - plus b3
12:57 - plus b4
12:59 - now again i know you guys are like what
13:00 - the heck am i doing this makes no sense
13:02 - it's about to make sense in one second
13:04 - so now what we need to do is we need to
13:07 - train the network so we've understood
13:08 - now this is essentially what this output
13:10 - layer is doing we're taking all of these
13:12 - um weights and these values we're
13:14 - multiplying them together and we're
13:15 - adding them and we're taking what's
13:16 - known as the weighted sum okay
13:19 - but how do we like what are these values
13:21 - how do we get these values and how is
13:22 - this going to give us a valid output
13:24 - well what we're going to do is we're
13:26 - going to train the network on a ton of
13:28 - different information so let's say we
13:30 - play 1 000 games of snake and we get all
13:34 - of the different inputs and all the
13:36 - different outputs so what we'll do is
13:37 - we'll randomly decide like a recommended
13:39 - direction and we'll just take the state
13:41 - of the snake which will be either is
13:43 - there something to left to the right
13:45 - or in front of it and then we'll take
13:46 - the output which will be um like did the
13:49 - snake survive or did the snake not
13:51 - survive
13:52 - so
13:53 - uh what we'll do is we'll we'll train
13:54 - the network using that information so
13:57 - we'll generate all of this different
13:59 - information and then train the network
14:01 - and what the network will do is it will
14:03 - look at all of this information and it
14:05 - will start adjusting these biases and
14:07 - these weights to properly get a correct
14:11 - output because what we'll do is we'll
14:12 - give it all this input right so let's
14:13 - say we give it the input again of zero
14:16 - one zero and maybe one like this is a
14:18 - random input and let's say the output
14:21 - for this case is um
14:24 - what do you call it so one is go to the
14:25 - right the output is one which is correct
14:28 - well what the network will do is say
14:29 - okay i got that correct so what i'm
14:30 - going to do is i'm not going to bother
14:32 - adjusting the network because uh this is
14:34 - fine so i don't have to change any of
14:35 - these biases i don't have to change any
14:37 - of these weights everything is working
14:39 - fine but let's say that we get the
14:41 - answer wrong so maybe the output was
14:43 - zero but the answer should have been one
14:45 - because we know the answer obviously
14:46 - because we've generated all the input
14:48 - and the output so now what the network
14:50 - will do is it will start adjusting these
14:52 - weights and adjusting these biases it'll
14:54 - say alright so i got this one wrong
14:57 - and i've gotten like five or six wrong
14:59 - before and this is what was familiar
15:01 - when i got something wrong so let's add
15:03 - one to this bias or let's multiply this
15:06 - weight by two and what it will do is
15:07 - it'll start adjusting these weights in
15:09 - these biases so that it gets more things
15:12 - correct so obviously that's why neural
15:14 - networks typically take a massive amount
15:17 - of information to train because what you
15:19 - do is you pass it all of this
15:21 - information and then it keeps going
15:23 - through the network and at the beginning
15:24 - it sucks right because it has
15:26 - this network just starts with random
15:28 - weights and random biases but as it goes
15:31 - through and it learns it says okay well
15:34 - i got this one correct so let's leave
15:36 - the weights and the biases the same but
15:38 - let's remember that this is what the way
15:39 - in the bias was when this was correct
15:41 - and then maybe he gets something wrong
15:43 - and it says okay so let's adjust bias
15:45 - one a little bit let's adjust weight one
15:47 - uh let's mess with these and then let's
15:49 - try another example and then it says
15:51 - okay i got this example right maybe
15:52 - we're moving in the right direction
15:54 - maybe we'll adjust another weight maybe
15:55 - we'll adjust another bias and eventually
15:58 - your goal is that you get to a point
16:00 - where your network is very accurate
16:02 - because you've given it a ton of data
16:03 - and it's adjusted the weights and the
16:05 - biases correctly so that this kind of
16:07 - formula here of this weighted average
16:09 - will just always give you the correct
16:11 - answer or has a very high accuracy or
16:14 - high chance of giving you the correct
16:15 - answer
16:17 - so i hope that kind of makes sense i'm
16:18 - definitely over simplifying things in
16:21 - how the adjustment of these weights and
16:22 - these biases work but it's not crazy
16:25 - important and we're not going to be
16:26 - doing any of the adjustment ourselves
16:28 - we're we are just going to be kind of
16:30 - tweaking a few things with the network
16:32 - so as long as you understand that when
16:33 - you feed information what happens is it
16:35 - checks whether the network got it
16:37 - correct or got it incorrect and then it
16:39 - adjusts the network accordingly and that
16:41 - is how the learning process works for a
16:43 - neural network all right so now it's
16:45 - time to discuss a little bit about
16:47 - activation functions so right now what
16:50 - i've actually just described to you is a
16:52 - very advanced technique of linear
16:54 - regression so essentially i was saying
16:57 - we're adjusting weights we're adjusting
16:58 - biases and essentially we're creating a
17:00 - function that given the inputs of like y
17:03 - z w or like left front right we are
17:06 - giving some kind of output but all we've
17:08 - been doing to do that essentially is
17:09 - just adjusting a linear function because
17:13 - our degree is only one right we have
17:15 - weights of degree one multiplying by
17:17 - values of degree 1 and we're adding some
17:19 - kind of bias and that kind of reminds
17:21 - you of the form mx plus b we're
17:24 - literally just adding a bunch of mx plus
17:26 - b's together
17:27 - which gives us like a fairly complex
17:29 - linear function
17:31 - but this is really not a great way to do
17:35 - things because it limits the degree of
17:38 - complexity that our network can actually
17:40 - have to be linear and that's not what we
17:43 - want so now we have to talk about
17:45 - activation functions so if you
17:47 - understand everything that i've talked
17:48 - about so far you're doing amazing this
17:50 - is great you understand that essentially
17:52 - the way that the network works is you
17:53 - feed information in and it adjusts these
17:56 - weights and biases there's a specific
17:58 - way it does that which we'll talk about
17:59 - later and then you get some kind of
18:01 - output
18:02 - and based on that output you're trying
18:04 - to adjust the weights and biases and and
18:06 - all that right
18:07 - so now what we need to do is talk about
18:09 - activation functions and what an
18:10 - activation function does is it's
18:12 - essentially a non-linear function that
18:15 - will allow you to add a degree of
18:17 - complexity to your network so that you
18:19 - can have more of a function that's like
18:21 - this
18:22 - as opposed to a function that is a
18:24 - straight line so an example of an
18:26 - activation function is something like a
18:28 - sigmoid function now a sigmoid function
18:31 - what it does is
18:33 - it'll map any value you give it in
18:36 - between the value of negative one and
18:38 - one
18:39 - so for example when we create this
18:42 - network our output might be like the
18:44 - number seven
18:46 - now this number seven well it is closer
18:48 - to one than it is to zero so we might
18:50 - deem that a correct answer or we might
18:53 - say that this is actually way off
18:55 - because it's way above one right but
18:57 - what we want to do essentially is in our
18:59 - output layer we only want our values to
19:02 - be within a certain range we want them
19:04 - to be in this case between zero and one
19:06 - or maybe we want them to be between
19:07 - negative one and one i'm
19:10 - saying like how close we are to zero
19:12 - making that decision how close we are to
19:13 - one something like that right so what
19:15 - this sigmoid activation function does
19:17 - it's a non-linear function
19:19 - and it takes any value and essentially
19:22 - the closer that value is to infinity the
19:24 - closer
19:25 - the output is to one and the closer that
19:27 - value is to negative infinity the closer
19:29 - that output is to negative one
19:32 - so what it does is it adds a degree of
19:34 - complexity to our network now if you
19:36 - don't if you're not a high level like
19:38 - math student or you only know like very
19:40 - basic high school math this might not
19:41 - really make sense to you but essentially
19:43 - the degree of something right is
19:45 - honestly how complex it can get if you
19:47 - have like a degree 9 function then what
19:50 - you could do is you can have some crazy
19:52 - kind of curve
19:54 - and stuff going on especially in
19:55 - multiple dimensions that will just
19:58 - make things like much more complex so
20:00 - for example if you have like a degree
20:02 - nine function you can have curves that
20:04 - are going like like this uh like all
20:06 - around here that are mapping your
20:08 - different values and if you only have a
20:10 - linear function well you can only have a
20:12 - straight line which limits your degree
20:14 - of complexity by a significant amount
20:17 - now what these activation functions also
20:20 - do is they shrink down your data so that
20:22 - it is not as large so for example right
20:24 - like say we're working with data that is
20:26 - like hundreds of thousands of like
20:28 - characters long or digits we'd want to
20:30 - shrink that into like normalize that
20:32 - data so that it's easier to actually
20:34 - work with so let me give you a more
20:36 - practical example of how to use the
20:37 - activation function i talked about what
20:39 - sigmoid does what we would do is we
20:41 - would take this weighted sum so we did
20:43 - the sum of
20:44 - w i v i
20:47 - plus
20:49 - b i right
20:50 - and we would apply an activation
20:52 - function to this so we would say maybe
20:54 - our activation function is f x
20:56 - and we would say f of this and this
20:59 - gives us some value which is now going
21:00 - to be our output neuron and the reason
21:03 - we do that again is so that when we are
21:06 - adjusting our weights and biases
21:08 - and we add that activation function and
21:10 - now we can have a way more complex
21:12 - function as opposed to just having the
21:15 - kind of linear regression straight line
21:17 - which is what we've i've talked about in
21:18 - my other machine learning courses so if
21:20 - this is kind of going a little bit over
21:22 - your head it may be my lack of
21:24 - explaining it i'd love to hear in the
21:25 - comments below how you think of this
21:27 - explanation but essentially that's what
21:28 - the activation function does now another
21:31 - activation function that is very popular
21:33 - and is actually used way more than
21:34 - sigmoid nowadays is known as rectified
21:36 - linear unit and what this does is it let
21:40 - me draw it in red actually so we can see
21:42 - it better is it takes all the values
21:44 - that are negative and automatically puts
21:46 - them to zero and takes all of the values
21:49 - that are positive and just makes them
21:51 - more positive
21:52 - essentially or like to some level
21:54 - positive right
21:56 - and what this again is going to do is
21:57 - it's a non-linear function so it's going
21:59 - to
22:00 - enhance the complexity of our model and
22:03 - just make our data points in between the
22:05 - range 0 and positive infinity which is
22:07 - better than having between negative
22:08 - infinity and positive infinity for when
22:10 - we're calculating uh error
22:13 - all right
22:14 - last thing to talk about for neural
22:15 - networks in this video i'm trying to
22:16 - kind of get everything like briefly into
22:19 - one long video
22:21 - is a loss function so this is again
22:24 - going to help us understand how these
22:25 - weights and these biases are actually
22:27 - adjusted so we know that they're
22:29 - adjusted and we know that what we do is
22:30 - we look at the output
22:32 - and we compare it to what the output
22:35 - should be from our test data
22:37 - and then we say okay let's adjust the
22:39 - weights and the biases accordingly but
22:41 - how do we adjust that and how do we know
22:43 - how far off we are how much to tune by
22:46 - if an adjustment even needs to be made
22:48 - well we use what's known as a loss
22:50 - function
22:51 - so a loss function essentially is a way
22:54 - of calculating error now there's a ton
22:56 - of different loss functions some of them
22:58 - are like mean squared error that's the
23:00 - name of one of them i think one is like
23:02 - um
23:04 - i can't even remember the name of this
23:05 - one but there's a bunch of very popular
23:06 - ones if you know some leave them in the
23:08 - comments love to hear all the different
23:09 - ones
23:10 - but anyways
23:12 - what the loss function will do is tell
23:14 - you how wrong your answer is
23:16 - because like let's think about this
23:18 - right if you get an answer of let's say
23:21 - maybe our output is like 0.79
23:24 - and the actual answer was one
23:26 - well that's pretty close like that's
23:28 - pretty close to one but right now all
23:29 - we're going to get is the fact that we
23:31 - were 0.21 off
23:33 - okay so 0.21 off so adjust the weight to
23:36 - certain degree based on 0.21 but the
23:38 - thing is what if we get like 0.85
23:41 - [Music]
23:44 - well is this like this is significantly
23:46 - better than 0.79 but this is only going
23:48 - to say that we were better by what is
23:50 - this 0.15 so we're still going to do a
23:53 - significant amount adjusting to the
23:55 - weights and the biases so what we need
23:57 - to do is we need to apply a loss
23:58 - function to this that will give us a
24:00 - better kind of
24:02 - degree of like how wrong or how right we
24:04 - were
24:05 - now these loss functions are again
24:08 - not linear loss functions which means
24:10 - that we're going to add a higher degree
24:11 - of complexity to our model which will
24:13 - allow us to create way more complex
24:16 - models and neural networks that can
24:17 - solve better problems i don't really
24:19 - want to talk about loss functions too
24:20 - much because i'm definitely no expert on
24:22 - how they work but essentially what you
24:25 - do is you're comparing the output to the
24:28 - what the output should be so like
24:29 - whatever the model generated based what
24:31 - it should be and then you're going to
24:32 - get some value and based on that value
24:34 - you are going to adjust the biases and
24:36 - the weights accordingly the reason we
24:37 - use the loss function again is because
24:39 - we want a higher degree of complexity
24:41 - they're non-linear and you know if you
24:43 - get 0 if you're 99 like say you're 0.1
24:46 - away from the correct answer we probably
24:48 - want to adjust the weights
24:49 - very very little but if you're like
24:52 - way off the answer your two whole points
24:54 - maybe our answer is negative one we want
24:56 - it to be one well we want to adjust the
24:58 - model like crazy right because that
25:00 - model was horribly wrong it wasn't even
25:02 - close so we would adjust it way more
25:04 - than just like two points of adjustment
25:06 - right we'd adjust it based on whatever
25:09 - that loss function gave to us
25:11 - so anyways this has kind of been my
25:13 - explanation of a neural network i want a
25:15 - very i want to state right here for
25:17 - everyone that i am no pro on neural
25:19 - networks this is my understanding there
25:21 - might be some stuff that's a little bit
25:22 - flawed or some areas that i skipped over
25:25 - and quickly actually because i know some
25:28 - people probably gonna say this when
25:29 - you're creating neural networks as well
25:31 - you have another thing that is called
25:33 - hidden layers so right now we've only
25:35 - been using two layers but in most neural
25:37 - networks what you have is a ton of
25:39 - different input neurons that connect to
25:41 - what's known as a hidden layer or
25:43 - multiple hidden layers of neurons so
25:45 - let's say we have like an architecture
25:46 - maybe that looks something like this so
25:48 - all these connections
25:49 - and then these ones connect to this and
25:51 - what this allows you to do is have way
25:53 - more complex models that can solve way
25:56 - more difficult problems because you can
25:58 - generate different combinations of
26:00 - inputs and hidden what is known as
26:02 - hidden layered neurons
26:04 - to solve your problem and have more
26:06 - weights and more biases to adjust which
26:08 - means you can on average be more
26:10 - accurate um to produce certain models so
26:13 - you can have crazy neural networks that
26:16 - look something like this but with way
26:17 - more neurons and way more layers and all
26:19 - this kind of stuff i just wanted to show
26:21 - a very basic network today because
26:24 - i didn't want to go in and talk about
26:26 - like a ton of stuff especially because i
26:27 - know a lot of people that watch my
26:29 - videos are not pro math guys are just
26:32 - trying to get a basic understanding and
26:33 - be able to implement some of this stuff
26:39 - now in today's video what we're going to
26:40 - be doing is actually getting our hands
26:42 - dirty and working with a bit of code and
26:44 - loading in our first data set so we're
26:46 - not actually going to do anything with
26:47 - the model right now we're going to do
26:48 - that in the next video this video is
26:50 - going to be dedicated to understanding
26:51 - data the importance of data how we can
26:54 - scale that data look at it and
26:55 - understand how that's going to affect
26:57 - our model when training
26:59 - the most important part of machine
27:01 - learning at least in my opinion is the
27:03 - data and it's also one of the hardest
27:05 - things to actually get done correctly
27:08 - training the model and testing the model
27:09 - and using it is actually very easy and
27:11 - you guys will see that as we go through
27:13 - but getting the right information to our
27:15 - model and having it in the correct form
27:17 - is something that is way more
27:19 - challenging than it may seem with these
27:21 - initial data sets that we're going to
27:22 - work with things are going to be very
27:23 - easy because the data sets are going to
27:25 - be given to us but when we move on into
27:27 - future videos to using our own data
27:29 - we're going to have to pre-process it
27:30 - we're going to put it in its correct
27:32 - form we're going to have to send it into
27:33 - an array we're going to have to make
27:35 - sure that the data makes sense so we're
27:36 - not adding things that shouldn't be
27:38 - there or we're not omitting things that
27:40 - need to be there so anyways i'm just
27:42 - going to quickly say here that i am kind
27:44 - of working off of this tensorflow 2.0
27:46 - tutorial that is on tensorflow's website
27:49 - now i'm kind of gonna stray from it
27:51 - quite a bit to be honest but i'm just
27:53 - using the data sets that they have and a
27:55 - little bit of the code that they have
27:56 - here because it's a very nice
27:58 - introduction to machine learning and
27:59 - neural networks but there's a lot of
28:01 - stuff in here that they don't talk about
28:03 - and it's not very in-depth so that's
28:05 - what i'm kind of going to be adding and
28:06 - the reason why maybe you'd want to watch
28:08 - my version of this as opposed to just
28:09 - reading this off the website because if
28:11 - you have no experience with neural
28:12 - networks it is kind of confusing some of
28:15 - the stuff they do here and they don't
28:16 - really talk about why they use certain
28:18 - things or whatnot so anyways the data
28:20 - set we're going to be working with today
28:22 - is it's known as the fashion mnist
28:24 - dataset so you may have heard of the old
28:26 - dataset which is image
28:28 - image classification but it was like
28:30 - digits so like you had digits from zero
28:32 - to nine and the neural network
28:34 - classified digits this one's a very
28:36 - similar principle except we're going to
28:37 - be doing it with like t-shirts and pants
28:40 - and um
28:41 - what do you call like sandals and all
28:42 - that so these are kind of some examples
28:44 - of what the images look like and we'll
28:45 - be showing them as well
28:47 - in uh in the code so that's enough of
28:49 - that i just felt like i should tell you
28:51 - guys that the first thing that we're
28:52 - going to be doing
28:54 - before we can actually start working
28:55 - with tensorflow is we obviously need to
28:56 - install it now actually maybe i'll grab
28:58 - the install command here so i don't have
29:00 - to copy it
29:01 - but this is the install command for
29:03 - tensorflow 2.0 so i'm just going to copy
29:05 - it here link will be in the description
29:07 - as well as on my website and you can see
29:09 - pink pip install hyphen q tensorflow
29:11 - equals equals 2.0 0.0 hyphen alpha 0.
29:15 - now i already have this installed but
29:16 - i'm going to go ahead and hit enter
29:17 - anyways
29:18 - and the hyphen q i believe just means
29:21 - don't give any output when you're
29:22 - installing so if this
29:24 - runs and you don't see any output
29:26 - whatsoever then you have successfully
29:28 - installed tensorflow 2.0 now i ran into
29:30 - an issue where i couldn't install it
29:32 - because i had a previous version of
29:33 - numpy installed in my system so if for
29:35 - some reason this doesn't work and
29:37 - there's something with numpy i would
29:38 - just pip uninstall numpy and reinstall
29:42 - so do pip uninstall numpy like that
29:45 - i'm obviously not going to run that but
29:46 - if you did that and then you tried to
29:48 - reinstall tensorflow 2.0 that should
29:50 - work for you and it should actually
29:51 - install its own version of the most
29:53 - updated version of numpy now another
29:56 - thing we're going to install here is
29:57 - going to be
29:58 - matplotlib now matplotlib is a nice
30:01 - library for just graphing and showing
30:03 - images and different information that
30:05 - we'll use a lot through this series so
30:07 - let's install that i already have it
30:08 - installed but go ahead and do that and
30:10 - then finally we will install pandas
30:13 - which we may be using in later videos uh
30:16 - in the series so i figured we might as
30:17 - well install it now so pip install
30:19 - pandas and once you've done that you
30:20 - should be ready to actually go here and
30:22 - start getting our data loaded in and
30:25 - looking at the data
30:26 - so i'm just going to be working in
30:28 - subline text and
30:30 - executing my python files from the
30:31 - command line just because this is
30:33 - something that will work for everyone no
30:34 - matter what but feel free to work in
30:36 - idle feel for you to work in pycharm as
30:38 - long as you understand how to set up
30:40 - your environment so that you have the
30:42 - necessary packages like tensorflow and
30:44 - all that uh then you should be good to
30:45 - go so let's start by importing
30:48 - tensorflow so import tensorflow as tf
30:51 - like that
30:53 - i don't know why it always short forms
30:54 - when i try to do this but anyways we're
30:56 - going to import uh or actually sorry
30:59 - from tensorflow
31:01 - we'll import keras now keras is an api
31:05 - for tensorflow which essentially just
31:06 - allows us
31:08 - to write less code uh it does a lot of
31:10 - stuff for us like you'll see when we set
31:12 - up the model we use keras
31:14 - and it'll be really nice and simple and
31:16 - just like a high level api and that's
31:18 - the way that they describe it that makes
31:19 - things a lot easier for people like us
31:20 - that aren't going to be defining our own
31:22 - tensors and writing our own code from
31:25 - scratch essentially
31:26 - now another thing we need to import is
31:28 - numpy so we're going to say
31:30 - import if i could get this here
31:33 - import numpy as np
31:36 - and finally we will import uh matplotlib
31:39 - so mat plot
31:41 - lib in this case dot pi plot as plt
31:45 - and this again is just going to allow us
31:46 - to graph some things here all right so
31:48 - now what we're going to do is we're
31:50 - actually going to get our data set
31:51 - loaded in so the way that we can load in
31:53 - our data set is using keras so to do
31:55 - this i'm just going to say data equals
31:57 - in this case care as dot datasets dot
32:00 - fashion
32:02 - underscore mnist
32:04 - uh and this is just the name of the data
32:05 - set there's a bunch of other data sets
32:06 - inside of keras that we will be using in
32:08 - the future
32:10 - now whenever we have data
32:12 - it's very important that we split our
32:14 - data into testing and training data
32:17 - now you may have heard this
32:18 - me talk about this in the previous
32:20 - machine learning tutorials i did but
32:22 - essentially what you want to do with any
32:23 - kind of machine learning algorithm
32:25 - especially a neural network is you don't
32:27 - want to pass all of your data into the
32:29 - network when you train it
32:31 - you want to pass about 90 80 percent of
32:34 - your data to the network to train it and
32:35 - then you want to test the network for
32:37 - accuracy and making sure that it works
32:39 - properly on the rest of your data that
32:41 - it hasn't seen yet now the reason you'd
32:44 - want to do this
32:45 - and a lot of people would say why don't
32:46 - i just give all my data to the network
32:47 - it'll make it better not necessarily and
32:50 - that's because if you test your data on
32:53 - if you test your network on data it's
32:55 - already seen
32:56 - then you can't be sure that it's not
32:58 - just simply memorizing the data it's
33:00 - seen right for example if you show me
33:02 - five images um and then like you tell me
33:05 - the classes of all of them and then you
33:06 - show me that the same image again you
33:08 - say what's the class and i get it right
33:10 - well did i get it right because i
33:12 - figured out how to analyze the images
33:13 - properly or because i'd already seen it
33:15 - and i knew
33:16 - what it was right i just memorized what
33:18 - it was so that's something we want to
33:20 - try to avoid with our models so whenever
33:22 - we have our data we're going to split it
33:23 - up into testing and training data and
33:24 - that's what we're going to do right here
33:26 - so to do this i'm going to say train
33:28 - in this case train underscore images and
33:32 - train underscore labels
33:34 - comba in this case
33:36 - test underscore images comma test
33:39 - underscore labels
33:41 - and then we're going to say this is
33:42 - equal to data dot get underscore data so
33:45 - not get a load underscore down
33:48 - now the reason we can do this is just
33:49 - because this load data method is going
33:51 - to return information in a way where we
33:53 - can kind of split it up like this in
33:54 - most cases when you're writing your own
33:56 - models for your own data you're going to
33:57 - have to write your own arrays and for
34:00 - loops and load in data and do all this
34:02 - fancy stuff but keras makes it nice and
34:04 - easy for us just by allowing us to write
34:06 - this line here which will get us our
34:07 - training and testing data in the four
34:11 - kind of variables that we need
34:13 - so quickly let me talk about what labels
34:15 - are now so for this specific data set
34:17 - there are 10 labels and that means each
34:19 - image that we have will have a specific
34:21 - label assigned to it
34:23 - now if i actually i'll show you by just
34:25 - printing it out if i print for example
34:27 - train underscore labels and let's just
34:29 - print like the zero with uh i guess the
34:31 - first training label so let me just run
34:33 - this file so python
34:35 - tutorial one
34:37 - you can see that we simply get the
34:39 - number nine now
34:42 - this is just what is represent like
34:44 - the label representation so obviously
34:46 - it's not giving us a string but let's
34:47 - say if i pick for example 6
34:49 - and i hit enter here you can see that
34:51 - the label is 7.
34:54 - so the labels are between
34:55 - 0 and 9. so 10 labels in total
34:59 - now the thing is that's not very useful
35:00 - to us because we don't really know what
35:01 - label 0 is what label 9 is so what i'm
35:04 - going to do is create a list that will
35:06 - actually define what those labels are
35:08 - so
35:09 - i'm going to have to copy it from here
35:10 - because i actually don't remember the
35:12 - labels
35:13 - but you can see it says here what they
35:14 - are so for example
35:16 - label 0 is a t-shirt label 1 is a
35:19 - trouser 9 is an ankle boot and you can
35:21 - see what they all are so we just need to
35:22 - define exactly this list here so class
35:25 - names so that we can simply take
35:27 - whatever value is returned to us from
35:29 - the model of what label it thinks it is
35:31 - and then just throw that as an index to
35:33 - this list so we can get what label it is
35:36 - all right sweet so that is um
35:39 - how we're getting the data now so now i
35:41 - want to show you what some of these
35:42 - images look like and talk about the
35:44 - architecture of the neural network we
35:46 - might use uh in the next video
35:49 - so i'm going to use pi plot just to show
35:50 - you some of these images and explain
35:53 - kind of the input and the output and all
35:54 - of that so if if you want to show an
35:56 - image using matplotlib you can do this
35:58 - by just doing plt dot im show
36:01 - and then in here simply putting the
36:03 - image so for example if i do train
36:05 - not labels images
36:07 - and let's say we do the seventh image
36:10 - and then i do plt.show
36:12 - if i run this now
36:14 - you guys will see what this image is
36:17 - so let's run this
36:18 - and you can see that we get uh this is
36:20 - actually i believe like a pullover or a
36:22 - hoodie now i know it looks weird and
36:24 - you've got all this like green and
36:26 - purple that's just because of the way
36:28 - that kind of matplotlib shows these
36:30 - images if you want to see it properly
36:32 - what you do is i believe you do cmap
36:34 - equals in this case uh
36:36 - plt.c i think it's like cm.binary or
36:40 - something i gotta have a look here
36:41 - because i forget uh yeah cm.binary so if
36:44 - we do this and now we decide to display
36:46 - the image it should look a little bit
36:48 - better let's see here
36:50 - uh and there you go we can see now we're
36:52 - actually getting this like black and
36:53 - white kind of image
36:55 - now this is great and all but let me
36:56 - show you actually what our image looks
36:58 - like so like how was i just able to show
37:00 - like how was i just able to do this
37:02 - image well the reason i'm able to do
37:03 - that is because all of our images are
37:05 - actually arrays of 28 by 28 pixels so
37:09 - let me print one out for you here so if
37:10 - i do train underscore images let's do
37:12 - seven the same example here and print
37:14 - that to the screen i'll show you what
37:15 - the data actually looks like
37:19 - give it a second and there we go so you
37:20 - can see this is obviously what our data
37:22 - looks like it's just a bunch of lists so
37:25 - one list for each row and it just has
37:28 - pixel values and these pixel values are
37:30 - simply representative of i believe like
37:32 - how much
37:34 - i don't actually know the scale that
37:35 - they're on
37:36 - but uh i think it's like an rgb value
37:38 - but in grayscale right so for example we
37:41 - have like 0 to 255 where 255 is black
37:44 - and 0 is white and i'm pretty sure
37:46 - that's how getting the information in
37:48 - someone can correct me if i'm wrong but
37:49 - i'm almost certain that that's how this
37:51 - actually works
37:52 - so this is gray null but this is these
37:55 - are large numbers and remember i was
37:57 - saying before in the previous video that
37:58 - it's typically a good idea to shrink our
38:00 - data down so that it's with within a
38:03 - certain range that is a bit smaller so
38:05 - in this case what i'm actually going to
38:07 - do is i'm going to modify this
38:09 - information a little bit so that we only
38:10 - have each value out of 1. so we instead
38:14 - of having a 255 we have it out of 1. so
38:16 - the way to do that is to divide every
38:18 - single pixel value by 255. now because
38:21 - these train images are actually stored
38:24 - in what's known as a numpy array we can
38:26 - simply just divide it
38:28 - by 255 to
38:30 - achieve that so we'll say train images
38:32 - equals train images divided by 255
38:35 - and we'll do the same thing here with
38:36 - our test images as well now obviously we
38:39 - don't have to modify the labels as well
38:41 - also because they're just between zero
38:42 - and nine and that's how the labels work
38:44 - but for images we're going to divide
38:46 - those values so that it's a bit nicer so
38:48 - now let me show you what it looks like
38:50 - so if i go python tutorial 1.5 pi
38:53 - and now you can see that we're getting
38:54 - these decimal values and that our shirt
38:56 - looks well the same but exactly like
38:59 - we've just shrunk down our data so it's
39:00 - going to be easier to work with in the
39:02 - future with our model
39:04 - now that's about it i think that i'm
39:05 - going to show you guys in terms of this
39:07 - data now we have our data loaded in and
39:09 - we're pretty much ready to go in terms
39:10 - of making a model
39:12 - now if you have any questions about the
39:14 - data please don't hesitate to leave a
39:15 - comment down below but essentially again
39:17 - the way it works is we're going to have
39:18 - 28 by 28 pixel images and they're going
39:21 - to come in as an array just as i've
39:23 - showed you here so these are all the
39:24 - values that we're going to have we're
39:25 - going to pass that to our model and then
39:27 - our model is going to spit out what
39:28 - class it thinks it is and those classes
39:30 - are going to be between 0 and 9.
39:32 - obviously 0 is going to represent
39:34 - t-shirt where 9 is going to represent
39:36 - ankle boot and we will deal with that
39:38 - all in the next video
39:43 - now in today's video we're actually
39:45 - going to be working with the neural
39:46 - network so we're going to be setting up
39:48 - a model we're going to be training that
39:49 - model we're going to be testing that
39:50 - model to see how well it performed we
39:52 - will also use it to predict on
39:54 - individual images and all of that fun
39:56 - stuff so without further ado let's get
39:59 - started now the first thing that i want
40:01 - to do before we really get into actually
40:02 - writing any code is talk about the
40:04 - architecture of the neural network we're
40:06 - going to create now i always found in
40:08 - tutorials that i watched they never
40:09 - really explained exactly what the layers
40:12 - were doing what they looked like and why
40:14 - we chose such layers and that's what i'm
40:16 - hoping to give to you guys right now so
40:19 - if you remember before we know now that
40:21 - our images they come in essentially as
40:23 - like 28 by 28 pixels and the way that we
40:26 - have them is we have an array and we
40:28 - have another array inside it's like a
40:30 - two-dimensional array and it has pixel
40:31 - values so maybe it's like 0.1
40:34 - 0.3 which is the grayscale value and
40:36 - this goes and there's times 28 in each
40:39 - row of these
40:41 - these pixels now there's 28 rows
40:43 - obviously because well 28 by 28 pixels
40:46 - so in here again we have the same thing
40:49 - more pixel values and we go down 28
40:52 - times right and that's what we have and
40:54 - that's what our array looks like
40:55 - now that's what our input data is that's
40:57 - fine but this isn't really going to work
40:59 - well for our neural network what are we
41:02 - going to do we're going to have one
41:02 - neuron and we're just going to pass this
41:04 - whole thing to it i don't think so
41:06 - that's not going to work very well so
41:07 - what we need to actually do before we
41:09 - can even like start talking about the
41:11 - neural network is figure out a way that
41:13 - we can change this information into a
41:16 - way that we can give it to the neural
41:17 - network so what i'm actually going to do
41:20 - and what i mean most people do is they
41:22 - they do what's called flatten the data
41:24 - so actually maybe we'll go i can't even
41:26 - go back once i clear it but flattening
41:28 - the data essentially is taking any like
41:30 - interior list so let's say we have like
41:31 - lists like this
41:33 - and just like squishing them all
41:34 - together so rather than
41:37 - so let's say this is like one two three
41:39 - if we were to flatten this
41:41 - what we would do is well we would remove
41:43 - all of these interior arrays or list or
41:46 - whatever it is so we would just end up
41:47 - getting data it looks like one
41:49 - two
41:50 - three and this actually turns out to
41:52 - work just fine for us so in this
41:55 - instance we only had like one element in
41:56 - each array
41:58 - but when we're dealing with 28 elements
41:59 - in each sorry list
42:01 - listed array they're interchangeable
42:03 - just in case i keep saying those uh what
42:05 - we'll essentially have is we'll flatten
42:07 - the data so we get a list
42:10 - of length
42:11 - 784 and i believe that is because well i
42:14 - mean i know this is because 28 times 28
42:17 - equals 784 so when we flatten that data
42:20 - so 28 rows of 28 pixels then we end up
42:24 - getting 784 pixels just one after each
42:26 - other and that's what we're going to
42:27 - feed in as the input to our neural
42:29 - network
42:30 - so that means that our initial input
42:32 - layer is going to look something like
42:33 - this we're going to have a bunch of
42:34 - neurons and they're going to go all the
42:37 - way down so we're going to have 784
42:39 - neurons so let's say this
42:41 - is
42:42 - 784 i know you could probably hardly
42:44 - read that but you get the point and this
42:46 - is our input layer
42:48 - now before we even talk about any kind
42:50 - of hidden layers let's talk about our
42:52 - output layer so what is our output well
42:55 - our output is going to be a number
42:56 - between 0 and 9. ideally that's what we
42:59 - want so what we're actually going to do
43:00 - for our output layer is rather than just
43:02 - having one neuron that we used kind of
43:04 - in the last
43:05 - the two videos ago as an example is
43:07 - we're actually going to have 10 neurons
43:09 - each one representing one of these
43:11 - different classes right so we have 0 to
43:13 - 9 so obviously 10 neurons
43:16 - or 10 classes so let's have 10 neurons
43:18 - so 1 2 3 4 5 6 7 8 9 10.
43:24 - now what's going to happen with these
43:25 - neurons is each one of them is going to
43:28 - have a value and that value is going to
43:30 - represent how much the network thinks
43:33 - that it is each neuron so for example
43:35 - say we're classifying the image
43:38 - that looks like a t-shirt uh or maybe
43:40 - like a pair of pants those are pretty
43:41 - easy to draw so let's say this is the
43:43 - image we're given a little pair of pants
43:45 - what's gonna happen is let's say pants
43:47 - is like this one like this is the one it
43:49 - actually should be all of these will be
43:51 - lit up a certain amount
43:53 - so essentially maybe we'll say like we
43:54 - think it's
43:55 - 0.05 percent this
43:58 - we have like a degree of certainty that
44:00 - it's 10 percent this one and then it is
44:02 - like we think it's 75 percent pants so
44:05 - what we'll do when we are looking at
44:07 - this output layer is essentially we'll
44:08 - just find whatever one is the greatest
44:10 - so whatever probability is the greatest
44:12 - and then say that's the one that the
44:14 - network predicts is
44:16 - the class of the given object right so
44:18 - when we're training the network what
44:20 - we'll do essentially is we'll say okay
44:23 - well we're giving the pants so we know
44:25 - that this one should be one right this
44:27 - should be a hundred percent it should be
44:29 - one uh that's what it should be and all
44:31 - these other ones should be zero right
44:33 - because it should be a zero percent
44:34 - chance it's anything else because we
44:35 - know that it is pants and then the
44:37 - network will look at all this and adjust
44:39 - all the weights and biases accordingly
44:41 - so that we get it so that it lights this
44:43 - one up directly as one at least that's
44:45 - our goal right
44:46 - so uh once we do that so now we've
44:48 - talked about the input layer and the
44:50 - output layer now it's time to talk about
44:52 - our hidden layers so we could
44:54 - technically train a network that would
44:56 - just be two layers right and we just
44:58 - have all these inputs that go to some
44:59 - kind of outputs but that wouldn't really
45:02 - do much for us because essentially that
45:03 - would just mean we're just going to look
45:04 - at all the pixels and based on that
45:06 - configuration of pixels we'll point to
45:09 - you know these output layers and that
45:11 - means we're only going to have which i
45:12 - know it sounds only 784 times 10 weights
45:16 - and biases so 784 times 10 which means
45:19 - that we're only going to have 7840
45:22 - weights right weights and biases things
45:24 - to adjust
45:25 - so what we're actually going to do is
45:27 - we're going to add a hidden layer inside
45:29 - of here now you can kind of arbitrarily
45:32 - arbitrarily pick how many neurons you're
45:34 - going to have in your hidden layer it's
45:36 - a good idea to kind of go off based on
45:37 - percentages from your input layer but
45:39 - what we're going to have is we're going
45:41 - to have a hidden layer and in this case
45:43 - this hidden layer is going to have 128
45:46 - neurons so we'll say this is 128 and
45:49 - this is known as our hidden layer
45:51 - so what will happen now is we're going
45:53 - to have our inputs connecting to the
45:55 - hidden layer so fully connected and then
45:57 - the hidden layer will be connected to
45:59 - all of our output neurons which will
46:01 - allow for much more complexity of our
46:03 - network
46:04 - because we're going to have a ton more
46:06 - biases and a ton more weights connecting
46:08 - to this middle layer which maybe we'll
46:10 - be able to figure out some patterns like
46:12 - maybe it'll look for like a straight
46:14 - line that looks like a pant sleeve or
46:16 - looks like an arm sleeve
46:17 - maybe it'll look for concentration of a
46:19 - certain area in the picture right and
46:21 - that's what we're hoping that our hidden
46:24 - layer will maybe be able to do for us
46:25 - maybe pick on pick up on some kind of
46:27 - patterns and then maybe with these
46:29 - combination of patterns we can pick out
46:32 - what specific image it actually is now
46:35 - we don't really know what the hidden
46:37 - network or hidden layer is going to do
46:39 - we just kind of have some hopes for it
46:41 - and by picking 128 neurons we're saying
46:43 - okay we're going to allow this hidden
46:45 - layer to kind of figure its own way out
46:47 - and figure out some way of analyzing
46:49 - this image and then that's essentially
46:52 - what we're going to do
46:53 - so if you have any questions about that
46:54 - please do not hesitate to ask
46:57 - but the hidden layers are pretty
46:58 - arbitrary sorry i just dropped my pen
47:00 - which means that you know you can kind
47:02 - of experiment with them kind of tweak
47:04 - with them there's some that are known to
47:06 - be to do well but typically when you're
47:08 - picking a hidden layer you pick one and
47:10 - you typically go at like maybe 15 20 of
47:12 - the input size but again it really
47:14 - depends on the application that you're
47:16 - you're using
47:17 - so let's now actually just start
47:19 - working with our data and creating a
47:22 - model so if we want to create a model
47:24 - the first thing we that we need to do is
47:26 - define the architecture or the layers
47:28 - for our model and that's what we've just
47:29 - done so i'm going to type it out fairly
47:31 - quickly here and again you guys will see
47:33 - how this works so i'm going to say model
47:35 - equals in this case care
47:36 - keras.sequential
47:39 - i believe that's how you spell it and
47:41 - then what we're going to do is inside
47:43 - here put a list and we're going to start
47:44 - defining our different layers so we're
47:47 - going to say keras dot layers and our
47:50 - first layer is going to be an input
47:51 - layer but it's going to be a flattened
47:53 - input layer and the input underscore
47:56 - shape is going to be equal to 28 by 28
47:59 - so remember i talked about that
48:00 - initially what we need to do is well we
48:02 - need to flatten our data so that it is
48:05 - uh passable to all those different
48:07 - neurons right
48:08 - so essentially oh i gotta spell shape
48:10 - correctly shape correctly so essentially
48:12 - whenever you're passing in information
48:14 - that's in like a 2d or 3d array you need
48:16 - to flatten that information so that
48:18 - you're going to be able to pass it to an
48:19 - individual neuron as opposed to like
48:22 - sending a whole list into one neuron
48:23 - right
48:24 - now the next layer that we're going to
48:26 - have is going to be what's known as a
48:27 - dense layer now a dense layer
48:29 - essentially just means a fully connected
48:31 - layer which means that what we've showed
48:33 - so far which is only fully connected
48:35 - neural networks that's what we're going
48:37 - to have so each node or each neuron is
48:39 - connected to every other neuron in the
48:40 - next network so i'm going to say
48:41 - layers.dense and in this case we're
48:44 - going to give it 128 neurons that's what
48:46 - we've talked about and we're going to
48:47 - set the activation function which we've
48:49 - talked about before as well to be
48:51 - rectify linear unit
48:54 - now again
48:55 - this activation function is somewhat
48:56 - arbitrary in the fact that you can pick
48:58 - different ones but rectifier linear unit
49:00 - is a very fast activation function and
49:03 - it works well for a variety of
49:04 - applications and that is why we are
49:06 - picking that now the next layer is going
49:08 - to be another dense layer which means
49:09 - essentially another fully connected
49:12 - layer sorry and we're going to have 10
49:14 - neurons this is going to be our output
49:16 - layer and we're going to have an
49:18 - activation of softmax
49:21 - now what softmax does is exactly what i
49:24 - explained when showing you that kind of
49:26 - architecture picture
49:27 - it will
49:29 - pick values for each neuron so that all
49:31 - of those values add up to one so
49:33 - essentially it is like the probability
49:35 - of the network uh thinking it's a
49:38 - certain value so it's like i believe
49:40 - that it's eighty percent this uh two
49:42 - percent this five percent this but all
49:44 - of the neurons there
49:45 - those values will add up to one and
49:47 - that's what the soft max soft max
49:49 - function does
49:50 - so that actually means that we can look
49:52 - at the last layer and we can see the uh
49:55 - probability or what the network thinks
49:57 - for each given class and say maybe those
50:00 - are two classes that are like 45 each we
50:02 - can maybe tweak the output of the
50:04 - network to say like i am not sure rather
50:06 - than predicting a specific uh value
50:08 - right
50:09 - all right so now what we're going to do
50:11 - is we're going to just set up some
50:13 - parameters for our model so we're going
50:14 - to say model.compile and in this case
50:17 - we're going to use an optimizer of atom
50:19 - now i'm not really going to talk about
50:20 - the optimizer uh
50:22 - atom is typically like pretty standard
50:24 - especially for something like this we're
50:25 - going to use the loss function of sparse
50:28 - and in this case underscore categorical
50:31 - i believe i spelt that correctly and
50:32 - then cross
50:34 - entropy now if you're interested in what
50:36 - these do and how they work in terms like
50:38 - the math kind of side of them just look
50:39 - them up there's they're very famous and
50:41 - popular
50:42 - and they're again are somewhat arbitrary
50:44 - in terms of how you pick them
50:46 - now when i do metrics i'm going to say
50:48 - metrics equals accuracy and again this
50:50 - is just going to define what uh we're
50:52 - looking at when we're testing the model
50:54 - in this case we care about the accuracy
50:56 - or how low we can get this loss function
50:58 - to be
50:59 - so yeah you guys can look these up
51:00 - there's tons of different loss functions
51:02 - some of them have different applications
51:04 - and typically when you're making a
51:05 - neural network you're you'll mess around
51:06 - with different loss functions different
51:08 - optimizers and in some cases different
51:10 - metrics
51:12 - so now it is actually time to train our
51:14 - model so to train our model what we're
51:16 - going to do is model.fit
51:18 - and when we fit it all we're going to do
51:19 - is give it our train images and our
51:22 - train labels
51:24 - now we're gonna set
51:26 - the amount of epochs
51:28 - so now it's time to talk about epochs
51:29 - now epochs are actually fairly
51:31 - straightforward you've probably heard
51:32 - the word epoch before but essentially it
51:34 - means how many times the model is going
51:36 - to see
51:37 - this information
51:39 - so what an epoch is going to do is it's
51:41 - going to kind of randomly pick
51:43 - images and labels obviously
51:45 - corresponding to each other
51:47 - and it's going to feed that through the
51:48 - neural network so how many epochs you
51:50 - decide is how many times you're going to
51:52 - see the same image so the reason we do
51:55 - this is because the order in which
51:58 - images come in will influence how
52:00 - parameters and things are tweaked with
52:02 - the network maybe seeing like 10 images
52:05 - that are pants is going to tweak it
52:07 - differently than if it sees like a few
52:08 - better pants and a few that are
52:10 - a shirt and some that are sandals so
52:12 - this is a very simple explanation of how
52:14 - the epochs work but essentially it just
52:16 - is giving um the same images in a
52:19 - different order and then maybe if it got
52:20 - one image wrong it's going to see it
52:22 - again and be able to tweak and it's just
52:23 - a way to increase hopefully the accuracy
52:26 - of our model that being said giving more
52:28 - epochs does not always necessarily
52:31 - increase the accuracy of your model it's
52:33 - something that you kind of have to play
52:34 - with and anyone that does any machine
52:36 - learning or neural networks will tell
52:37 - you that they can't really like they
52:39 - don't know the exact number epochs they
52:40 - have to play with it and tweak it and
52:42 - see what gives them the best accuracy
52:45 - so anyways now it is time to actually
52:48 - well we can run this but let's first get
52:49 - some kind of output here so i'm going to
52:51 - actually evaluate this model directly
52:53 - after we run it so that we can see how
52:55 - it works on our test data so right now
52:58 - what this is doing is actually just
52:59 - training the model on our training data
53:01 - which means we're tweaking all the
53:02 - weights and biases um we're applying all
53:05 - those activation functions and we're
53:07 - defining like a main function for the
53:08 - model but if we actually want to see how
53:10 - this works we can't
53:12 - really just test it on the training
53:14 - images and labels for the same reason i
53:16 - talked about before so we have to test
53:18 - it on the test images and the test
53:20 - labels and essentially see how many it
53:22 - gets correct so the way we do this is
53:24 - we're going to say
53:25 - test underscore loss test underscore ac
53:29 - which stands for accuracy equals model
53:32 - dot evaluate
53:35 - is that how you spell it maybe and then
53:37 - we're going to do test images
53:39 - test underscore labels and i believe
53:42 - that is the last parameter yes it is so
53:44 - now if we want to see the accuracy of
53:46 - our model we can simply print out
53:49 - test underscore acc and we'll just say
53:52 - like
53:53 - tested
53:55 - acc just so we know because there is
53:57 - going to be some other metrics that are
53:58 - going to be printing out to us when we
53:59 - run this
54:01 - all right so now that we've done that
54:02 - let's actually run our file and
54:05 - see how this works so this is it this
54:07 - whole part here is all we actually need
54:09 - to do to create a neural network and do
54:11 - a model now actually let me just quickly
54:13 - say that this keras.sequential what this
54:16 - does is it means a like a sequence of
54:18 - layers
54:19 - so you're just defining them in order
54:21 - where you say the first layer obviously
54:22 - is going to be your input layer
54:24 - we're flattening the data then we're
54:26 - adding two dense layers which are fully
54:28 - connected to the input layer as well
54:30 - and that's what our model looks like and
54:33 - this is typically how you go about
54:34 - creating a neural network
54:36 - all right so let's run this now
54:38 - and see what we get
54:41 - so this will take a second or two to run
54:43 - um just because obviously there is well
54:46 - we have 60 000 images in this data set
54:48 - so you know it's got to run through them
54:49 - it's doing all the epochs and you can
54:51 - see that we're getting metrics here on
54:53 - our accuracy
54:55 - and our loss
54:56 - now our test accuracy was 87 so you can
54:59 - see that it's actually slightly lower
55:01 - than um what do you call it like the
55:02 - accuracy here oh it's the exact same oh
55:04 - it actually auto tested on some data
55:07 - sets but anyways
55:09 - so essentially that is um
55:11 - how this works you can see that the
55:13 - first five epochs which are these ones
55:15 - here uh
55:16 - ran and they increased typically with
55:19 - each epoch now again we could try like
55:21 - 10 epochs 20 epochs and see what it does
55:24 - but there is a point where the more
55:25 - epochs you do the actual like the less
55:27 - reliable your model becomes
55:30 - uh and you can see that our accuracy was
55:31 - started at 88.9 essentially and that was
55:35 - on like that's what it said our model
55:37 - accuracy was when we were training the
55:38 - model but then once we actually tested
55:40 - it which are these two lines here uh it
55:43 - was lower than the
55:45 - the tested or like the trained accuracy
55:47 - which shows you that you obviously have
55:49 - to be testing on different images
55:50 - because when we tested it here it said
55:52 - well it was 89 but then here we only got
55:54 - 87 right so let's do a quick uh tweak
55:57 - here and just see what we get maybe if
55:58 - we add like 10 epochs i don't think this
56:01 - will take a crazy long amount of time
56:03 - so we'll run this and see maybe if it
56:05 - makes a massive difference or if it
56:06 - starts leveling out or it starts going
56:08 - lower or whatnot
56:10 - uh so let me let this run here for a
56:12 - second and obviously you can see the
56:13 - tweaked accuracy as we continue to go
56:16 - i'm interested to see here if we're
56:17 - going to increase by much or if it's
56:18 - just kind of going to stay at the same
56:20 - level
56:22 - all right so we're hitting about 90
56:23 - percent and let's see here
56:26 - 91. okay so uh we got up to 91 but you
56:30 - can see that it was kind of diminishing
56:31 - returns as soon as we ended up getting
56:33 - to about seven epochs even yeah even
56:36 - like eight epochs after this we only
56:38 - increased by marginal amount and our
56:40 - accuracy on the testing data was
56:42 - slightly better but again for the amount
56:45 - of epochs five extra epochs it did not
56:47 - give us a five times better result right
56:49 - so it's something you gotta play with
56:50 - and see
56:55 - now in today's video what we're going to
56:57 - be doing is just simply using our model
56:59 - to actually predict information on
57:01 - specific images and see how you can
57:03 - actually use the model i find a lot of
57:05 - tutorial series don't show you how to
57:06 - actually practically use the model but
57:09 - what's the point of creating a model if
57:10 - you can't use it now quickly before i
57:12 - get too far into the video i would just
57:13 - like to show you guys something that i'm
57:14 - super excited to announce because i've
57:16 - been waiting for them to come for a long
57:18 - time and it is the official tech with
57:20 - tim mugs so you guys can see them here i
57:23 - just want to quickly show them to you
57:24 - guys if you'd like to support the
57:25 - channel and get an awesome looking mug i
57:27 - actually really like them then you guys
57:29 - can purchase them just by i believe
57:30 - underneath the video it shows like the
57:31 - teespring link um but yeah they're
57:34 - awesome they look really good and the
57:35 - reason i've been holding out on showing
57:36 - them to you guys is because i wanted to
57:37 - wait till i received mine to make sure
57:39 - that it was up to quality and that it
57:41 - looked good enough uh to sell to you
57:43 - guys essentially so if you'd like to
57:44 - support the channel um you can get one
57:46 - of those if not that's fine but if you
57:48 - do decide to buy one please send me like
57:50 - a dm on twitter instagram or something
57:52 - and let me know so i can say thank you
57:53 - to you guys so anyways let's get into
57:56 - the video um so what i'm gonna do
57:58 - actually
57:59 - is i'm gonna uh we need to continually
58:02 - train the model every time we run the
58:04 - program which i know seems like a pain
58:07 - but unless we want to save the model
58:08 - which i guess i could actually show in
58:10 - this video later as well
58:12 - we just have to train it and then we can
58:13 - use it directly after so after we've you
58:16 - know tested this we don't need to do
58:17 - this evaluate anymore we are trained the
58:19 - model we can use it to use it we
58:21 - actually just need to use a method
58:23 - called predict but i'm going to talk
58:24 - about kind of how this works because it
58:26 - is a little finicky we're not even just
58:28 - finicky but just not intuitive so
58:31 - essentially when you want to make a
58:32 - prediction using the model
58:34 - i'm going to set up just a variable
58:36 - prediction here
58:37 - you simply use
58:38 - model.predict and then you pass it a
58:41 - list
58:42 - now
58:43 - what you would think you would do is
58:44 - just pass it like the input right so in
58:47 - this case we just pass it some input
58:48 - that's in the form 2828 and it would
58:51 - predict but that's not actually how it
58:53 - works
58:54 - when you want to make a prediction what
58:55 - you need to do is
58:57 - put whatever your
58:58 - input shape is inside of a list
59:00 - or actually
59:02 - well you can do it inside of the list
59:03 - but you can also do it inside an mp
59:05 - array as well like a numpy array
59:07 - and the reason you need to do that is
59:08 - because what predict does is it gives
59:11 - you a group of predictions so it's
59:13 - expecting you to pass in a bunch of
59:14 - different things and it predicts all of
59:17 - them using the model so for example if i
59:19 - want to do the predictions on all of my
59:21 - test images to see what they are i can
59:23 - do prediction equals model.predict test
59:25 - images
59:26 - and if i print out like prediction uh
59:29 - you guys will see what this looks like
59:30 - so let's run this here
59:32 - and see what we get so obviously we have
59:35 - to train the model each time which is a
59:37 - little bit annoying but we can save it
59:39 - later on and obviously this one runs
59:40 - pretty quickly so it's not a huge deal
59:45 - all right so there we go so now you can
59:46 - see this is actually what our
59:47 - predictions look like now this is a
59:49 - really weird kind of like looking
59:52 - prediction thing
59:53 - we're getting a bunch of different lists
59:55 - now that's because right our output
59:57 - layer is 10 neurons so we're actually
60:00 - getting an output of 10 different values
60:02 - and these different values are
60:03 - representing how much the model thinks
60:06 - that each picture is a certain class
60:09 - right so you can see we're getting like
60:10 - 2.6 to the e to the negative zero six
60:14 - which means that obviously a very small
60:16 - number so it doesn't think whatsoever
60:18 - that it's that and then i'm trying to
60:19 - find if we can see ones that aren't like
60:21 - to the e
60:23 - but apparently it's we didn't really get
60:24 - lucky enough with it showing because i
60:26 - just cut some of them off here but if i
60:28 - print out let's say like prediction
60:31 - zero and i guess we're gonna have to run
60:32 - this again i probably should have
60:33 - thought of that
60:35 - then you guys will see exactly what the
60:36 - prediction list looks like and i'm gonna
60:38 - show you how we can actually interpret
60:39 - this to determine what class it is
60:41 - because this means nothing to us we want
60:43 - to know is it a sandal is it a shoe is
60:45 - it a shirt like what is it right so
60:47 - there you go so this is what the list
60:48 - looks like so if we look through the
60:50 - list here we can see these are all the
60:51 - different probabilities that our our
60:53 - network is predicting so what we're
60:55 - actually going to do essentially is
60:57 - we're going to take whatever the highest
60:59 - number is there we're going to say that
61:01 - is the predicted value so to do that
61:04 - what we do is we say np dot arg max okay
61:09 - and we just put it around this list now
61:12 - what this does is it just gets the
61:14 - largest value and finds like the index
61:17 - of that so in this case since we have 10
61:19 - neurons the first one is representing
61:21 - obviously t-shirt the last one is
61:23 - representing ankle boot it'll find
61:24 - whatever neuron is the largest value and
61:26 - give us the index of that neuron so if
61:28 - it's like the third neuron then it's
61:30 - going to give us a pullover right and
61:32 - and that's how that works
61:34 - so if we want to see the actual like
61:36 - name though rather than just the index
61:39 - then what we need to do is just take
61:40 - this value and pass it into class names
61:43 - so we'll say class underscore names and
61:45 - then we'll index whatever the value is
61:47 - that this np dot arg max prediction zero
61:50 - gives us right so let's run this
61:53 - and see what we get now all right so
61:55 - there we go so we can see that now we're
61:56 - actually getting ankle boot as our
61:58 - prediction which makes a lot more sense
62:00 - for us right rather than just giving us
62:02 - like that prediction array or whatever
62:04 - it was okay so that's great but the
62:06 - thing is how do we how can we validate
62:08 - this is actually working well what we
62:10 - need to do now or not what we need to do
62:12 - but what we should do now is
62:14 - show the input and then show what the
62:15 - predicted value is and that way we as
62:17 - the humans which know obviously which is
62:19 - which can validate that so what i'm
62:21 - going to do actually just set up a very
62:23 - basic for loop and what this for loop is
62:25 - going to do is loop through a few
62:26 - different images in our test images and
62:29 - show them on the screen and then also
62:30 - show the prediction
62:32 - so show what they actually are and then
62:34 - show the prediction as well so to do
62:36 - this i'm just going to say 4 i guess in
62:38 - this case i
62:40 - in range 5
62:42 - and what we'll do is i'm going to say
62:44 - plt
62:45 - dot grid i'm just going to set up a very
62:47 - basic like plot to show the image
62:49 - i'm going to image show
62:51 - our test underscore images i right
62:55 - i'm going to do this cmap thing so i'm
62:56 - going to say cmap equals in this case
63:00 - plt.cm.binary which is just going to
63:01 - give us like the gray scale
63:03 - and then i'm going to say plt dot x
63:06 - label which just means underneath and
63:08 - i'm going to say is equal to
63:10 - actual
63:11 - and in this case i'm going to say plus
63:14 - and what do we want to do we need to get
63:16 - the actual label of our test image which
63:18 - would be in test underscore labels i
63:21 - and then what i'm going to do is add a
63:23 - header and say this is what the model
63:25 - predicted
63:26 - so to do this i'm going to say plt dot
63:29 - i believe it's oh sorry not header so
63:31 - that title
63:32 - and the title will simply be
63:35 - prediction
63:36 - plus in this case we're going to say
63:38 - prediction
63:40 - and then i now the reason we can do this
63:42 - or sorry we're going to have to
63:44 - literally copy this this whole arg max
63:46 - thing and we'll put that here
63:49 - except instead of zero we're going to
63:50 - put i and just that way it will show all
63:53 - of the different
63:54 - images right
63:56 - so now what i'm going to do is for each
63:57 - loop here i'm going to plt.show which
63:59 - means i'm going to show those images so
64:00 - we can see exactly what they look like
64:03 - so quick recap in case i kind of skimmed
64:04 - over some stuff
64:06 - all we're doing is setting up a way to
64:08 - see the image as well as what it
64:09 - actually is versus what the model
64:11 - predicted so we as the humans can kind
64:13 - of validate this is actually working and
64:15 - we see okay this is what the image and
64:16 - the input is and this is what the output
64:18 - was from the model
64:20 - so let's run this
64:22 - and wait for it to train i'll fast
64:24 - forward through this and then we will
64:26 - show all the images
64:27 - okay so quick fix here um i just ran
64:29 - this and i got an error we need to do
64:30 - class names and then test labels i and
64:33 - that's obviously because the test labels
64:35 - are going to have like the index of all
64:37 - of these so i can't just put like the
64:39 - number value i have to put the class
64:41 - names so that we get the correct thing
64:43 - anyways i hope that makes sense you guys
64:45 - let's run this now you can see that was
64:46 - the error i ran into again fast forward
64:48 - and then i will be back
64:51 - all right so i am back now this is a
64:53 - little bit butchered in how i'm actually
64:54 - showing it but you can see that it's
64:56 - saying the prediction for this was the
64:57 - ankle boot and it actually is an ankle
65:00 - boot now if i close this it'll just show
65:02 - four more because that's the way i've
65:03 - set it up so now you can see that
65:05 - prediction pullover it actually was a
65:07 - pullover all right we see we get
65:09 - prediction trouser it actually was a
65:11 - trouser
65:13 - and prediction trouser actual trouser uh
65:16 - prediction shirt actual shirt
65:18 - and obviously if you want to see more
65:19 - you could keep looping through all of
65:21 - these and doing that now say you just
65:24 - want to predict on one image well what
65:26 - you could do for example is
65:28 - uh and this is kind of a weird way what
65:29 - i'm about to do but you'll see
65:31 - let's say we wanted to just predict like
65:33 - what the seventh image was well then
65:35 - what i would do is just say test images
65:37 - 7 which is going to give us
65:39 - that 28 by 28 array
65:41 - and then i would just put it inside of a
65:42 - list so that that way it gets
65:45 - it's given the way that it's supposed to
65:47 - look
65:47 - but that also means that our prediction
65:49 - list right we're going to get is equal
65:52 - to this it's going to look like
65:53 - prediction and then it's going to have
65:54 - this and then inside
65:56 - it's going to have all those different
65:57 - values so it's going to have like 0.001
66:01 - 0.9 but it's going to be a list inside
66:03 - of a list so that's just something to
66:04 - keep in mind when you're working with
66:06 - these predictions because that is really
66:08 - the only way to do it and that this is
66:10 - exactly what tensorflow recommends on
66:12 - their website as well if you're just
66:13 - predicting for one item just put it
66:15 - inside of a list so that it's going to
66:16 - work fine so anyways that has kind of
66:18 - been it on using the model to predict
66:21 - stuff in future videos we'll get into a
66:23 - little bit more advanced stuff this was
66:24 - a very easy classification problem just
66:27 - really meant to give you an introduction
66:29 - and personally i think if you never
66:30 - worked with any machine learning stuff
66:32 - this is pretty cool that in a few
66:34 - minutes of just kind of writing a little
66:35 - bit of code whether you understand it or
66:37 - not you can create a simple model that
66:39 - can classify
66:41 - fashion items like a shirt a t-shirt and
66:43 - i don't know that's pretty cool to me
66:44 - and in future videos obviously we're
66:46 - going to be doing a lot cooler stuff
66:47 - it's going to be a little bit more
66:48 - advanced but hopefully you guys can
66:50 - stick with it i'd love to know what you
66:51 - guys think of the series so far so
66:53 - please leave a comment down below it
66:55 - helps me to kind of tweak my lessons and
66:57 - all that as we go forward if you guys
66:58 - enjoyed the video please leave a like
67:00 - and subscribe and i will see you again
67:02 - [Music]
67:18 - now in today's video what we're going to
67:19 - be doing is talking about text
67:21 - classification uh with tensorflow 2.0
67:24 - now what i'm going to be doing just to
67:26 - be fully transparent with you guys here
67:28 - is following along with the actual
67:30 - official tutorials on the tensorflow 2.0
67:32 - tutorial uh now i find that these are
67:34 - actually the best in terms of like kind
67:36 - of a structure to start with to
67:38 - understand very basic neural networks
67:40 - for some pretty simple tasks i would say
67:43 - and then we're gonna stray away from
67:44 - those we're gonna start using our own
67:45 - data our own networks our own
67:47 - architecture and we'll start talking
67:49 - about kind of some of the issues you
67:50 - have when you actually start applying
67:52 - these to real data so so far you guys
67:54 - have noticed and i've seen some comments
67:56 - on it already that the data is really
67:58 - easy to load in and even pre-processing
68:00 - it like in the last one we just divided
68:02 - everything by 255 like that's really
68:04 - simple in the real world your data is
68:06 - definitely not that nice and there's a
68:08 - lot of stuff that you need to play with
68:10 - and modify to make it actually usable so
68:12 - anyways we'll follow along with this one
68:14 - for today and essentially the way that
68:16 - it works is we're going to have movie
68:18 - reviews and we're just going to classify
68:19 - them either as either positive or
68:21 - negative now what we'll do is we'll just
68:24 - look at some of the movie reviews and
68:26 - then we'll talk about the data we'll
68:27 - talk about the architecture using stuff
68:29 - to predict some issues we might run into
68:31 - and all of that now i don't know how
68:32 - many video parts this is going to be i'm
68:34 - going to try to record it all at once
68:35 - and just split it up based on how long
68:37 - it takes but with that being said enough
68:39 - talking let's get started
68:41 - so what we're going to do obviously is
68:43 - start in our file here and again this is
68:45 - going to be really nice because we can
68:46 - just steal kind of the data from keras
68:49 - what we'll start by doing is just
68:50 - importing
68:52 - tensorflow as tf we're going to say from
68:55 - tensorflow
68:57 - import keras and then we're going to say
69:00 - import
69:01 - numpy as np now before i start i ran
69:05 - into a quick uh issue when i was
69:07 - actually trying to do this just
69:08 - following along with the official
69:09 - tutorial and that was that the data that
69:11 - i want to grab here actually doesn't
69:14 - work with the current version of numpy
69:15 - that comes with tensorflow it's on their
69:17 - github as an issue but anyways to fix
69:20 - this what we need to do is install the
69:22 - previous version of numpy so to do this
69:25 - uh what i'm actually going to do is just
69:26 - say pip um i think i do like pip numpy
69:30 - version or something
69:31 - because i want to see what version it is
69:33 - uh incorrect command let's say pip
69:36 - version number i want to find what
69:37 - version it is and then just go down uh
69:39 - to that version okay so i found the
69:41 - version of numpy uh what we're going to
69:43 - do now is actually just install the
69:45 - correct version of numpy to make it work
69:46 - for this tutorial now this should be
69:48 - fine for everything going forward and if
69:50 - you want to install the most recent
69:51 - version of numpy after doing this go
69:53 - feel free but to do this all i'm going
69:55 - to do is just say pip install and then
69:57 - numpy equals in this case 1.16.1
70:01 - i believe the version we're using right
70:02 - now is 0.3 at least at the time
70:04 - recording this but just change it to
70:05 - this version and hopefully in the future
70:07 - they'll fix that issue so that we don't
70:08 - have to do this
70:10 - but anyways i'm going to install that um
70:12 - yeah you're going to have to add two
70:13 - equal signs and i already have this
70:15 - installed so that should just not do
70:17 - anything but
70:18 - you guys just make sure you do that i'll
70:19 - leave the command in the description
70:21 - now after we do that
70:23 - what i'm going to do is just load in the
70:24 - data i'm going to say data equals in
70:26 - this case care as
70:28 - dot data sets dot i m
70:31 - what is it i am db
70:33 - now i believe this stands for like some
70:35 - something movie database i don't really
70:37 - know but anyways that's what the
70:38 - database is and we're going to do the
70:39 - same thing we did in the previous
70:40 - tutorial which is just split this into
70:42 - training and testing data so to do that
70:44 - i'm going to say train underscore data
70:46 - train underscore labels comma and then
70:48 - in this case we'll say test underscore
70:50 - data and then test underscore labels
70:54 - equals in this case data.load underscore
70:56 - data now we're just going to add one
70:58 - thing in here which is num underscore
71:00 - words equals in this case 10 000. now
71:04 - the reason i'm doing this is because
71:05 - this data set contains like a ton of
71:07 - different words and what we're going to
71:09 - actually do by saying num words equals
71:11 - ten thousand is only take the words that
71:13 - are the ten thousand most frequent which
71:16 - means we're going to leave out words
71:17 - that usually are only occurring like
71:19 - once or twice throughout the entire data
71:20 - set because we don't want to throw those
71:22 - into our model and have things like
71:24 - be more difficult than they have to be
71:26 - and just have data that's kind of
71:27 - irrelevant because we're going to be
71:29 - comparing obviously
71:30 - uh movie reviews and there's some words
71:33 - that are only in like one review we
71:34 - should probably just omit them because
71:36 - there's nothing to really compare them
71:38 - to in other data sets anyways i hope
71:41 - that kind of makes sense but that's not
71:42 - super important we're just going to do
71:43 - num words equals uh 10 000. it also
71:46 - shrinks our data a little bit which
71:47 - makes it a bit nicer
71:48 - now what we're going to do next is we're
71:50 - actually going to show how we can
71:53 - display this data now if i start by
71:55 - actually just showing you like the train
71:57 - underscore data and let's pick like the
71:59 - zero with one so i guess the first one
72:01 - and i print this out to the screen so
72:02 - i'll just go
72:04 - if i could get to pyth python and i
72:06 - guess in this case we'll have to do i
72:08 - probably should just type this to start
72:10 - uh tutorial 2
72:14 - when this actually prints out probably
72:16 - going to take a second here just to
72:17 - download the data set you can see that
72:19 - what we have is actually just a bunch of
72:21 - numbers
72:22 - now this doesn't really look like a
72:24 - movie review to me does it well what
72:26 - this actually is is integer encoded uh
72:29 - words so essentially each of these
72:31 - integers point to a certain word and
72:34 - what we've done just to make it way
72:36 - easier for our model to actually
72:37 - classify these and work with these is
72:39 - we've given each word one integer so in
72:41 - this case maybe like the word the
72:43 - integer one stands for something the
72:44 - integer 14 stands for something and all
72:46 - we've done is just added those integers
72:48 - into a list that represents where these
72:51 - words are located in the movie review
72:54 - now this is nice for the computer but
72:55 - it's not very nice for us if we actually
72:57 - want to read these words so what we have
72:58 - to do is find the mappings for these
73:00 - words and then find some way to actually
73:02 - display this so that you know we can
73:04 - have a look at it now i'll be honest
73:06 - here i'm just going to take this from
73:07 - what they have on the tensorflow website
73:09 - on how to do this typically you would
73:11 - create your own mappings for words with
73:12 - your own dictionary and you just already
73:14 - have that information but fortunately
73:16 - for us tensorflow already does that so
73:17 - to do that i'm going to say word
73:18 - underscore index equals in this case
73:20 - imdb dot get underscore word underscore
73:24 - index like this now what this does is
73:27 - actually going to give us a dictionary
73:28 - that has those keys and those mappings
73:31 - so that what we can do is well
73:34 - figure out you know what these integers
73:36 - actually mean so when we want to print
73:37 - it out later we can have a look at them
73:39 - so what i'm going to say now is word
73:41 - underscore index equals in this case
73:45 - k
73:46 - colon and then we're going to say
73:48 - what do you call it v
73:50 - plus 3
73:51 - for k
73:52 - v
73:54 - in word underscore index dot items
73:57 - so
73:58 - i might have been incorrect here this
73:59 - doesn't actually give us the dictionary
74:00 - this just gives us like tuples that have
74:03 - the string and the word in them i
74:06 - believe and then what we're doing here
74:08 - is we're going to say instead of c sorry
74:10 - this should be v my apologies is we're
74:12 - going to get just break that tuple up
74:13 - into k and v which stands for key and
74:15 - value and the key will be the word the
74:17 - value will be obviously the integer
74:20 - yes that's what it will be and we're
74:22 - going to say four word items in index
74:23 - we'll break that up and then we're just
74:24 - going to add a bunch of different keys
74:26 - into our data set now the reason we're
74:28 - gonna start at plus three is because
74:29 - we're gonna have actually one key or
74:32 - three keys that are gonna be like
74:34 - special characters for our word mapping
74:36 - and you guys will see how those work in
74:37 - a second so i'm gonna start by just
74:38 - saying word index and in this case i'm
74:41 - going to put in here uh
74:43 - pad we're going to talk about this in a
74:45 - second so don't worry if you guys are
74:46 - kind of like what are you doing right
74:47 - now i'm going to say word index and in
74:49 - this case
74:52 - start
74:53 - equals 1 and say word underscore index
74:57 - in this case i believe it's like u n k
74:59 - yeah that's correct we're gonna say u n
75:01 - k equals two now u n k just stands for
75:04 - unknown and i'm gonna explain all this
75:06 - in a second but it's easier just to type
75:07 - it out first and we're gonna say word
75:09 - index in this case
75:11 - inside this tag unused and we're going
75:13 - to say equals three
75:15 - so what i'm doing essentially is all of
75:17 - the words in our training and testing
75:19 - data set have um
75:22 - like keys and values associated with
75:24 - them starting at one so what i'm doing
75:26 - is i'm just going to add three to all of
75:28 - those values so that what i can actually
75:30 - do is assign my own kind of values that
75:33 - are gonna stand for padding start
75:36 - unknown and unused so that if we get
75:38 - values that are not valid we can just
75:40 - assign them to this essentially in the
75:42 - dictionary now what i'm going to use for
75:44 - padding you guys will see in just a
75:45 - second essentially it's just so we can
75:47 - make our all our movie sets the same
75:49 - length so we'll add this what's known as
75:51 - pad tag and we'll do that by adding zero
75:54 - into our actual movie review list so
75:56 - that we're gonna make each movie review
75:58 - the same length and the way we do that
75:59 - essentially is if they're not the same
76:01 - length so maybe one's 100 maybe one's
76:03 - 200 we want all them to be 200 the 100
76:05 - length
76:06 - movie list will for what we'll do is
76:09 - we'll just add a bunch of padding to the
76:11 - end of it to make it
76:12 - length 200 and then obviously our model
76:15 - will hopefully be able to differentiate
76:17 - the fact that that is padding and then
76:19 - we don't care about the padding and we
76:20 - shouldn't even bother really like
76:21 - looking at that right
76:23 - all right so now what i'm going to do is
76:25 - add this kind of complicated line here
76:27 - uh just to i don't even know why they
76:30 - have this to be quite honest this is the
76:31 - way the tensorflow has decided to do
76:33 - their like word mappings but apparently
76:35 - you need to add this reverse underscore
76:39 - underscore word underscore index which
76:41 - is equal to
76:43 - dictionary and then here we're going to
76:44 - say
76:47 - value comma key
76:49 - for
76:51 - key
76:52 - comma
76:53 - value uh in
76:56 - word underscore index
76:58 - i believe that's correct and what this
76:59 - is going to do actually sorry not word
77:01 - index word index.items
77:03 - what this is gonna do is okay i
77:05 - understand now
77:06 - now that i've typed it out you just swap
77:08 - all the values in the keys so that right
77:10 - now we actually have a dictionary that
77:12 - has all of the
77:14 - like the keys first which is going to be
77:16 - the word and then the values where we
77:18 - actually want it the other way around so
77:20 - we have like the integer pointing to the
77:22 - word because we're going to have our
77:23 - data set that is going to contain just
77:25 - integers like we've seen here and we
77:27 - want these integers to be able to point
77:29 - to a word as opposed to the other way
77:31 - around so what we're doing is just
77:32 - reversing this with a reverse word index
77:36 - list just our dictionary sorry
77:38 - essentially that's what this is doing
77:39 - here
77:40 - all right now that we've done that the
77:42 - last step is just to add a function and
77:44 - what this function will do is actually
77:47 - decode essentially all of this training
77:49 - and testing data into human readable
77:52 - words
77:53 - so there's different ways to do this
77:54 - again i'm just going to take this right
77:56 - from the tensorflow website because this
77:57 - part's not super important and i'd
77:59 - rather just you know do it quickly than
78:01 - spend too much time on it so we're just
78:03 - going to say return blank string dot
78:05 - join
78:07 - in this case we're going to say reverse
78:08 - word index
78:10 - dot get
78:11 - in this case we're going to say i
78:13 - comma
78:14 - question mark now what this does
78:16 - essentially if you don't know how the
78:17 - get works is we're going to try to get
78:19 - index i which we're going to define in a
78:20 - second
78:22 - if we can't find a value for that then
78:24 - what we'll do is just put question mark
78:26 - and that's which is a default value
78:27 - which means we won't crash if we're
78:29 - having like a key uh error in our
78:32 - dictionary and we're going to say for
78:34 - in this case i
78:36 - in
78:38 - text i don't know why where i have text
78:41 - typed i think i might have messed
78:43 - something up here so one second here oh
78:44 - text array is the parameter my apologies
78:47 - so anyways that's what this is going to
78:48 - do is just going to return to us
78:49 - essentially all of the the keys that we
78:51 - want
78:52 - or the human readable words my apologies
78:55 - so now what we'll do is we'll simply
78:57 - just print out decode review and i'm
78:59 - just going to give it some test data so
79:00 - let's say test for example 0 and i guess
79:03 - we're going to do test underscore data
79:05 - it doesn't really matter if you do train
79:06 - or test data but let's just have a look
79:08 - at test data zero and see what that
79:09 - actually looks like so let's run that
79:12 - assuming i didn't make any mistakes we
79:13 - should actually get some valid output in
79:15 - just a second this usually takes a
79:17 - minute to run up
79:18 - imdb is not
79:20 - defined what did i type here i typed
79:23 - that as data my apologies so where we
79:25 - say imdb which is right here we just
79:27 - need to replace that with data
79:30 - in my other file i called it imdb so
79:32 - that's why i made a mistake there but
79:34 - let's run that again and hopefully now
79:36 - we will get some better looking output
79:37 - so let's wait for this and see
79:40 - dict object has no attribute items this
79:42 - needs to be items classic typos by tim
79:47 - one more time third time is a charm
79:49 - hopefully let's see and there we go so
79:51 - now we can see that we're actually
79:52 - getting all of this decoded into well
79:55 - this text now i'll allow you guys to
79:57 - read through it but you can see that we
79:59 - have these kind of keys that we've added
80:01 - so start which is one which will
80:03 - automatically be added at the beginning
80:05 - of all of our text and then we have
80:07 - these unks which stand for unknown
80:09 - character essentially and then we don't
80:11 - have any other keys in here but say for
80:13 - example we had like some padding we had
80:15 - added to this we would see those pad
80:16 - tags as well in here
80:18 - so that is essentially how that works if
80:21 - you'd like to look at some other reviews
80:22 - just mess around with kind of the values
80:25 - in the index here throw them into decode
80:26 - review and then we can actually see what
80:28 - they look like now something to note
80:30 - quickly is that our reviews are
80:32 - different lengths now i've talked about
80:33 - this already but let's just compare two
80:35 - reviews to really test that i am not
80:37 - just making this up so i'm going to say
80:38 - test underscore data
80:41 - why would i have a capital here test
80:43 - underscore data zero so the length of
80:45 - test and record data is zero and the
80:47 - length of let's try test underscore data
80:50 - one just to prove to you guys that these
80:52 - are actually different lengths which
80:53 - means there's something kind of fancy
80:54 - we're going to have to do with that
80:56 - padding tag
80:57 - which i was talking about there so let's
80:58 - go into text classification let's go cmd
81:01 - and then python in this case
81:03 - tutorial2.pi
81:06 - now i guess we're going to get that
81:07 - output again which is probably what's
81:08 - causing this to just take a second to
81:10 - run you can see that we have length 68
81:12 - and we have length 260. now this is not
81:15 - going to work for our model and the
81:17 - reason this doesn't work is because
81:20 - we need to know what our inputs
81:23 - shape sorry and size is going to be just
81:25 - like i talked about before we define the
81:27 - input nodes or the input neurons and the
81:29 - output neurons so we have to determine
81:31 - how many input neurons there's going to
81:33 - be and how many output neurons there's
81:34 - going to be
81:36 - now if we're like we don't know how
81:37 - large our data is going to be and it's
81:39 - different for each
81:40 - what do you call its entry then that's
81:42 - an issue so we need to do something to
81:45 - fix that so what we're gonna do is we're
81:47 - gonna use this padding tag to
81:49 - essentially set a definite length for
81:52 - all of our data now we could go ahead
81:54 - and pick the longest review and say that
81:56 - we'll make all of the reviews that
81:58 - length but what i'm going to do is just
81:59 - pick an arbitrary number in this case
82:01 - we'll just do like 250 and say that
82:03 - that's the maximum amount of words we're
82:04 - going to allow in one review which means
82:06 - that if you have more than 250 words in
82:08 - your review we're just going to get rid
82:10 - of all those and if you don't have 256
82:13 - words or 250 words or whatever it is
82:15 - we're just going to add these padding
82:17 - tags to the end of it until eventually
82:20 - we reach that value
82:22 - so the way to do this is again using
82:25 - these fancy tensorflow functions now
82:28 - if you don't like these um functions and
82:30 - like what these do for you and how they
82:32 - just kind of save you some time go ahead
82:34 - and try to write them yourself and if
82:35 - you want help on how to do that feel
82:37 - free to reach out to me on discord or in
82:39 - the comments or whatever but i
82:40 - personally just use them because it
82:42 - saves me quite quite a bit of time in
82:43 - terms of like typing out the functions
82:45 - and i already know how to do a lot of
82:47 - what these functions do so for me it
82:49 - doesn't really make sense to just retype
82:51 - them out when i can just use these kind
82:53 - of fancy tools so what we're going to
82:55 - say is we're going to redefine our
82:57 - training and testing data and what we're
82:59 - going to do is just trim that data so
83:00 - that it's only at or
83:02 - kind of normalize that data so it's at
83:04 - 250 words so to do that i'm going to say
83:07 - train underscore data equals in this
83:09 - case
83:10 - keras dot pre-processing
83:14 - um no idea if that's how you spell it
83:16 - we'll have to check that in a second dot
83:18 - sequence
83:19 - dot pad underscore sequence
83:23 - uh so pre-processing i think that's
83:25 - correct i guess we'll see and then in
83:27 - here we have to define a few different
83:29 - parameters so what we'll first do is
83:30 - we'll give that train underscore data
83:33 - we're going to say value equals which
83:34 - will be the pad value so what we add to
83:37 - the end of
83:38 - in this case our numpy array to pad it
83:40 - per se
83:41 - and in this case we'll just use this pad
83:43 - tag so we'll say
83:45 - literally word index pad so let's copy
83:48 - that and put that there
83:50 - we're going to say our padding
83:52 - equals in this case post which just
83:54 - means we're going to pad after as
83:56 - opposed to before we also could
83:59 - pad before but that doesn't really make
84:00 - too much sense for this and then what
84:02 - we'll say is max
84:03 - in this case len equals and then you
84:05 - pick your number that you want to
84:07 - make all of the values equal to now
84:10 - tensorflow did like 256. i'm just going
84:12 - to do 250 and see if this makes a
84:14 - difference in terms of our accuracy for
84:15 - the model and i'm literally just going
84:17 - to copy this and change these values now
84:19 - to test underscore data instead of train
84:21 - underscore data and this will do the
84:23 - same thing on our other data set
84:26 - oops didn't mean to do that so test
84:28 - underscore data like that
84:31 - so quick recap here because we are at 17
84:34 - minutes now essentially what we've done
84:35 - is we've loaded in our data we've looked
84:37 - at our data we've created the word
84:40 - mappings essentially for our data so
84:41 - that we can actually figure out what all
84:43 - these integers mean we've created a
84:44 - little function here that will decode
84:47 - the mappings for us so we just pass it a
84:50 - word review that's integer encoded it
84:52 - decodes it and then it we can print that
84:54 - information out to the screen to have a
84:55 - look at it what we've just done now is
84:57 - we've done what's called pre-processing
84:59 - our data which means just making it into
85:01 - a form that our model can actually
85:03 - accept and that's consistent and that's
85:05 - what you're always going to want to do
85:06 - with any data that you have typically
85:08 - it's going to take you a bit more work
85:10 - than what we have because it's only two
85:11 - lines to pre-process our data because
85:13 - keras kind of does it for us
85:15 - but for the purpose of this example
85:17 - that's fine
85:18 - all right so now that we've done that
85:20 - it's actually time to define our model
85:23 - um now i'll show you quickly just to
85:25 - make sure that you know you guys believe
85:26 - me here that this is working in terms of
85:29 - pre-processing pre-processing our data
85:31 - so it's actually going to make things
85:32 - the same length so we'll say train
85:33 - underscore data test underscore data let
85:35 - me just print this out to the screen so
85:37 - python tutorial 2.
85:38 - uh again we're going to get these
85:40 - integer mappings but we'll get the
85:41 - length at the end as well
85:43 - and another error of course we need to
85:45 - add an s to these sequences again my
85:48 - apologies guys on that um
85:51 - classic typos here so anyways i had
85:53 - pre-process
85:54 - processing sequence we need sequences
85:57 - and now if i run this you can see that
85:59 - we have a length of 250 and 250 so we've
86:02 - kept that consistent now for some oh i'm
86:04 - printing
86:05 - i don't know why this is printing two oh
86:07 - it's because i'm printing it here and
86:08 - then i'm printing it here um but you
86:10 - guys get the idea in that we've now made
86:12 - them actually the same size so let me
86:14 - remove these print statements um all of
86:17 - them so we can stop printing train data
86:19 - zero up here as well and now let's start
86:21 - defining our model
86:22 - so i'll just say model
86:24 - down here is a little comment just to
86:26 - help us out so what i'm going to do now
86:27 - is similar to what i've done before
86:29 - except in the last one you might have
86:30 - noticed that the way i defined my model
86:32 - was
86:33 - uh
86:34 - here i'll show you in a second once i
86:35 - finish typing this so we did
86:37 - keras.sequential and then what we
86:39 - actually did was just had a list in here
86:40 - that had all the layers that's fine you
86:42 - can do that but in this case we're going
86:44 - to have a few more layers so what we're
86:45 - going to do actually is
86:47 - add these layers just by doing model dot
86:50 - add it's precisely the same thing as
86:52 - before except instead of adding them in
86:53 - this list we're just gonna do it using
86:55 - this method so now we're gonna say keras
86:58 - dot layers dot in this case embedding
87:00 - and i'll talk about what these layers do
87:02 - in a second
87:03 - so we're gonna do ten thousand sixteen
87:05 - and then we're just going to actually
87:06 - copy this um
87:08 - four times and just change these layers
87:10 - and the kind of
87:12 - uh
87:12 - parameters as well so now we're gonna
87:14 - say global
87:17 - average
87:18 - pooling
87:19 - 1d
87:21 - and
87:22 - then do that and then we're going to add
87:24 - a dense layer here
87:27 - and another dense layer and change these
87:30 - parameters
87:32 - so we'll say dense and we'll say in this
87:34 - case 16 and we'll say
87:37 - activation
87:39 - equals relu
87:41 - our rectify linear unit whatever you
87:43 - guys want to call it and then we'll do
87:44 - down here one and activation equals
87:47 - rectifier linear unit as well
87:50 - uh actually sorry not real really we're
87:52 - gonna do sigmoid my apologies so now
87:54 - we'll actually talk about the
87:55 - architecture of this model and how i
87:57 - came up with picking these layers and
87:59 - well what these layers are
88:01 - well what we want essentially is we want
88:03 - the final output to be whether the
88:05 - review is good or whether the review is
88:06 - bad i think i mentioned that um at the
88:09 - beginning of the video so what we're
88:10 - actually going to do is just have either
88:12 - that like we'll have one output neuron
88:14 - and that neuron should be either zero or
88:16 - one we're somewhere in between there to
88:18 - give us kind of a probability of like we
88:20 - think it's like
88:21 - twenty percent one eighty percent zero
88:24 - something along those lines now we can
88:26 - accomplish that by using sigmoid because
88:28 - what it will do again we've talked about
88:30 - the sigmoid function is it'll squish
88:31 - everything so whatever our value is in
88:33 - between zero and one which will give us
88:35 - a nice way to test if our model is
88:38 - actually working properly
88:39 - and to get it the value that we want
88:46 - hey guys so now it's time to talk about
88:48 - word embeddings and this embedding layer
88:51 - and then what the global average pooling
88:53 - 1d layer is doing now we already have an
88:55 - idea of what these dense layers are with
88:57 - these activation functions like relu and
88:59 - sigmoid but what we're actually going to
89:01 - do today or i guess just in this video
89:03 - is talk about the architecture of this
89:05 - network kind of how it works on a high
89:07 - level understanding and then in the next
89:09 - video what we'll do is actually get into
89:10 - training and using the network
89:13 - so what i'm going to do first is just
89:14 - start by talking about these first two
89:16 - layers and specifically what this
89:18 - embedding layer is because it's very
89:20 - important and then we will draw the
89:22 - whole network or the whole i guess
89:24 - network is the right word way to put it
89:26 - the whole architecture and talk about
89:27 - how it fits together and what it's
89:29 - actually doing
89:30 - so let's get started
89:32 - now the easiest way to kind of explain
89:35 - this is to use an example of two very
89:37 - similar sentences so i'm just going to
89:39 - say the first sentence is have
89:42 - a great
89:44 - day
89:46 - and the next sentence will be have
89:48 - a
89:49 - good
89:50 - day
89:51 - now i know my handwriting is horrible so
89:53 - just give me a break on that um it's
89:55 - also hard to kind of write with this
89:56 - tablet so that's my excuse but anyways
89:58 - these two sentences looking at them as
90:01 - human beings we can tell pretty quickly
90:02 - that they're very similar now yes great
90:05 - and good maybe one has more emphasis on
90:08 - having an amazing day whatever it is but
90:10 - they're very similar and they pretty
90:11 - well have the same meaning right maybe
90:13 - we know when we would use the sentence
90:15 - and kind of the context in which like
90:17 - these words great and good are used in
90:19 - day and day and all this right it just
90:21 - we understand what they are now the
90:23 - computer doesn't have that same
90:25 - understanding at least right off the bat
90:27 - when looking at these two sentences now
90:29 - in our case we've actually integer
90:31 - encoded all of our different values so
90:34 - what we end up having are all of our
90:35 - different words sorry is our sentences
90:37 - end up looking something like this so
90:38 - we're going to have this first word will
90:40 - represent a 0. a will be 1 great will be
90:42 - 2 and a will be 3 so then down here
90:45 - we'll have 0 1 in this case we're going
90:47 - to say good is 4 and day is 3 as well so
90:50 - this means if we integer and code these
90:52 - sentences we have some lists that look
90:54 - something like this now this one clearly
90:57 - is the first sentence and this one down
90:59 - here will be the second sentence
91:01 - now if we just look at this and we
91:04 - pretend that you know we don't even know
91:05 - what these words actually are all we can
91:08 - really tell is the fact that two is
91:11 - different from four
91:13 - now notice what i just said there two is
91:15 - different from four when in reality if
91:17 - we look at these two words we know that
91:20 - they're pretty similar yes they're
91:22 - different words yes they're different
91:23 - lengths whatever it is but we know that
91:25 - they have a similar meaning and the
91:27 - context in which they're used in this
91:29 - sentence is the same
91:31 - now our computer obviously doesn't know
91:33 - that because all it gets to see is this
91:35 - so what we want to do is try to get it
91:37 - to have an understanding of words that
91:39 - have similar meanings and to kind of
91:41 - group those together in a similar form
91:43 - or in a similar way because obviously in
91:46 - our application here of classifying
91:48 - movie reviews the types of words that
91:50 - are used and the context in which they
91:52 - are used really makes a massive
91:54 - difference to trying to classify that as
91:56 - either a positive or a negative review
91:58 - and if we look at great and good and we
92:00 - say that these are two completely
92:01 - different words well that's going to be
92:03 - a bit of an issue when we're trying to
92:04 - do some classification so this is where
92:06 - our embedding layer comes in
92:09 - now again uh just to say here one more
92:10 - time like we know these are different
92:12 - but we also would know for example say
92:14 - if we replace this four with a three
92:16 - well all our computer again would know
92:18 - is that two is different from three just
92:19 - like four is different from two it
92:21 - doesn't know how different they are and
92:22 - that's what i'm trying to get at here is
92:24 - our embedding layer is going to try to
92:27 - group words in a similar kind of way so
92:29 - that we know which ones are similar to
92:31 - each other so let me now talk about
92:34 - specifically the embedding layer so let
92:36 - me just draw a little grid here now what
92:38 - are embedding layer actually does kind
92:40 - of like i don't want to say the formal
92:42 - definition but the more mathy definition
92:44 - is it finds word vectors for each word
92:47 - that we pass it or it generates word
92:49 - vectors and uses those word vectors uh
92:52 - to pass to the future layers now a word
92:56 - vector can be in any kind of dimensional
92:58 - space now in this case we've picked 16
93:01 - dimensions for each word vector which
93:03 - means that we're going to have vectors
93:05 - maybe something like this and a vector
93:06 - again is just a straight line with a
93:08 - bunch of different coefficients in some
93:09 - kind of space
93:12 - that is in this case 16 dimensions so
93:14 - let's pretend that this is a 16
93:16 - dimensional vector and this is the word
93:18 - vector for the word have
93:21 - now in our computer it wouldn't actually
93:23 - be have it would be zero because again
93:25 - we have integer encoded stuff
93:28 - but you kind of you get the points we'll
93:29 - say this is the word vector for half
93:32 - now what we're going to do immediately
93:34 - when we create this embedding layer is
93:36 - let me actually get out of this quickly
93:37 - for one second is we initially create 10
93:40 - 000 word vectors for every single word
93:43 - and in this case every single number
93:45 - that represents a word so what we're
93:46 - going to do is when we start creating
93:49 - this
93:49 - embedding layer we see that we've have
93:51 - an embedding layer is we're going to
93:53 - draw 10 000 word vectors in just kind of
93:56 - some random way
93:58 - that are just there and each one
93:59 - represents one word and what happens
94:01 - when we call the embedding layer is it's
94:04 - going to grab all of those word vectors
94:06 - for whatever input we have and use that
94:09 - as the data that we pass on to the next
94:12 - layer now how do we create these word
94:14 - vectors and how do we group words well
94:16 - this is where it gets into a bit
94:18 - complicated math i'm not really going to
94:20 - go through any equations or anything
94:21 - like that but i'll kind of give you an
94:22 - idea of how we do it
94:24 - now we want to so let me get rid of this
94:26 - word have because this is not the best
94:28 - word vector example and let's say that
94:30 - this word vector is great now upon
94:33 - creating our word vector our embedding
94:35 - layer we have two vectors we have great
94:37 - and we have good
94:39 - and we can see that these vectors are
94:40 - kind of far apart from each other and we
94:42 - determine that by looking at the angle
94:44 - between them and we say that this angle
94:46 - maybe it's like i don't know 70 degrees
94:48 - or something like that and we can kind
94:50 - of determine that great and good are not
94:51 - that close to each other but in reality
94:54 - we want them to be
94:55 - pretty close to each other we want the
94:56 - computer to look at great and good and
94:58 - be like these are similar words let's
95:00 - treat them similarly in our neural
95:02 - network
95:03 - so what we want to do hopefully is have
95:05 - these words and these vectors kind of
95:07 - move closer together whether it's good
95:10 - going all the way to great or great
95:12 - going all the way to good or vice versa
95:13 - right we just want them to get close
95:15 - together and kind of be in some form of
95:17 - a group
95:18 - so what we do is we try to look at the
95:21 - context in which these words are used
95:23 - rather than just the content of the
95:24 - words which would just be what this
95:26 - looks like we want to figure out how
95:28 - they how they're used so we'll look at
95:30 - the words around it and determine that
95:32 - you know when we have a
95:33 - and a and a and a maybe that means that
95:36 - these are like related in some way and
95:39 - then we'll try to group these words now
95:41 - it's way more complicated than that
95:42 - don't get me wrong um but it's kind of
95:45 - like a very basic way of how they group
95:48 - together we look at the words that
95:49 - surround it
95:51 - and just different properties of the
95:52 - sentence involving that word and then we
95:54 - can kind of get an idea of where these
95:56 - words go when which ones are close to
95:58 - each other so maybe after we've done
96:00 - some training uh what happens is our
96:03 - word embeddings are what is known as
96:04 - learned just like we're learning and
96:06 - teaching our neural network and we get
96:09 - we end up getting great and good very
96:11 - close together and these are what their
96:12 - word vector representations are we can
96:14 - tell that they're close again by looking
96:16 - at the angle in between here maybe it's
96:17 - like 0.2 degrees and what that means is
96:20 - these two vectors which are just a bunch
96:22 - of numbers essentially are very close
96:24 - together so when we feed them into our
96:26 - neural network they should hopefully
96:28 - give us
96:29 - a similar output at least for that
96:31 - specific neuron that we give it to
96:33 - now i know this might be a little bit
96:34 - confusing but i'm going to go we're
96:36 - going to talk about this a bit more with
96:37 - another drawing of the whole network but
96:39 - i hope you're getting the idea the whole
96:40 - point of this embedding layer is to make
96:42 - word vectors that and then group those
96:44 - word vectors or kind of like make them
96:46 - close together based on words that are
96:48 - similar and that are different so again
96:50 - just like we would have grading good
96:52 - here we would hope that a word vector
96:54 - like bad would be down here where it has
96:57 - a big difference from great and good so
97:00 - that we can tell that these words are
97:01 - not related whatsoever
97:03 - all right so that's how the embedding
97:04 - layer works now what ends up happening
97:06 - when we have this embedding layer is we
97:08 - get an output dimension of what's known
97:10 - as 16 dimensions and that's just how
97:12 - many coefficients essentially we have
97:14 - for our vector so just like if you have
97:16 - a 2d line so like if this is our grid in
97:19 - 2d and we say that this is x and this is
97:21 - y we can represent any line by just
97:24 - having like
97:25 - uh some values like ax
97:28 - plus b y equals c
97:31 - now this is the exact same thing that we
97:32 - can do in in n dimensions which means
97:35 - like any amount of dimensions so for a
97:37 - 16 dimensional line i'm not going to
97:38 - draw them all but we would start with
97:40 - like ax
97:41 - plus b y
97:43 - plus c z
97:45 - plus d w and so on and we would just
97:48 - have again 16 of these coefficients
97:51 - and then some kind of constant value uh
97:53 - maybe we call it lambda that is like
97:55 - what it's
97:56 - what it equals to what the equation
97:58 - equals to and that's how we define a
98:00 - line i'm pretty sure i'm doing this
98:02 - correctly in uh in n dimensions
98:05 - so anyways once we create that line what
98:07 - we actually want to do is we want to
98:08 - scale the dimension down a little bit
98:11 - now that's just because 16 dimensions is
98:12 - a lot of data especially when we have
98:15 - like a ton of different words coming
98:16 - into our network we want to scale it
98:18 - down to make it a little bit easier to
98:20 - actually
98:22 - compute and to train our network so
98:24 - that's where this global average pooling
98:26 - 1d layer comes in now i'm not going to
98:28 - talk about this in two depth in too much
98:29 - depth but essentially the way to think
98:31 - of the global average pooling 1d is that
98:34 - it just takes whatever dimension our
98:36 - data's in and just puts it in a lower
98:38 - dimension now there's a specific way
98:39 - that it does that but again i'm not
98:41 - going to talk about that and it's not
98:42 - super important if you care about that a
98:44 - lot just look it up and it's not like
98:46 - crazy hard but i just i don't feel the
98:48 - need to go into it in this video so
98:50 - anyways let's now start drawing what our
98:52 - network actually looks like after
98:54 - understanding how this embedding layer
98:55 - works so we're going to initially feed
98:58 - in a sequence and we'll just say that
99:00 - this is like our sequence of encoded
99:02 - words okay so say this is our input
99:05 - and maybe it's something like zero seven
99:07 - nine like a thousand two hundred a
99:10 - thousand twenty uh we have like nine
99:13 - again maybe we have eight it's just a
99:14 - bunch of different essentially numbers
99:17 - right so we're gonna pass this into our
99:19 - embedding layer and all this is gonna do
99:21 - is it's gonna find the uh
99:24 - representation of these words in our
99:25 - embedding layer so maybe our embedding
99:28 - layer well it's going to have the same
99:29 - amount of words in our vocabulary so
99:31 - it'll look up say zero it'll say maybe
99:33 - zero means
99:35 - zero's vector is like zero point two
99:38 - zero point and it goes to 16 dimensions
99:40 - but i'm just gonna do like two for this
99:42 - example here maybe seven its vector is
99:45 - like
99:46 - seven and nine point zero and it just
99:49 - keeps going like this and it looks up
99:50 - all these vectors
99:52 - so it takes all of our input data and it
99:54 - just turns them into a bunch of vectors
99:56 - and just spits those out into our next
99:58 - layer now our next layer what this does
100:02 - is it just takes these vectors and just
100:04 - averages them out and it just means it
100:05 - kind of shrinks them their data down so
100:07 - we'll do like a little smaller thing
100:08 - here and we'll just say like average
100:11 - okay
100:12 - so i'll call this one
100:14 - embedding and that one is average
100:17 - now this average layer now is where we
100:20 - go into the actual neural network well
100:22 - obviously this is a neural network but
100:23 - we go into the dense layers which will
100:25 - actually perform our classification so
100:27 - what we're going to do is we're going to
100:28 - start with 16 neurons and this is just
100:30 - again an arbitrary number that we've
100:32 - picked
100:33 - for our network you can mess around with
100:35 - different values for this and i
100:36 - encourage you to do that but 16 is what
100:38 - tensorflow decided to use and what i'm
100:39 - just following along with so we're going
100:41 - to have 16 neurons and we're going to
100:43 - pass all of our now 16 dimensional data
100:46 - or whatever dimensional data it is into
100:48 - these neurons like this
100:50 - now this is where we start um doing the
100:52 - dense layer so we have this dense layer
100:54 - and this is connected to one output
100:56 - neuron
100:57 - like this
100:59 - so what we end up having is this
101:01 - embedding layer
101:02 - which is going to have all these word
101:04 - vectors that represent different words
101:06 - we average them out we pass them into
101:08 - this 16
101:10 - neuron layer that then goes into an
101:12 - output layer which will spit out a value
101:14 - between 0 and 1 using the sigmoid
101:17 - function
101:18 - which i believe i have to correct myself
101:20 - because in other videos i said it did
101:21 - between negative one and one it just
101:23 - takes any value we have and puts it in
101:24 - between zero and one
101:26 - like that
101:28 - all right so that is kind of how our
101:30 - network works so
101:32 - let me talk about what this dense layer
101:34 - is doing just a little bit before we
101:35 - move on to the next video
101:37 - so what this dense layer is going to
101:39 - attempt to do essentially is look for
101:41 - patterns of words and try to classify
101:44 - them using the same methods we talked
101:46 - about before
101:48 - into either a positive review or a
101:49 - negative review
101:51 - i'm going to take all these word vectors
101:52 - which again are going to be like
101:54 - similarly grouped words like great good
101:56 - are going to be similar input to this
101:58 - dense layer right because we've
102:00 - averaged them out and embedded them in
102:02 - all this and then what we're going to do
102:04 - is we're going to try to determine
102:06 - based on what words we have and what
102:08 - order they come in
102:09 - what our text is and we hope that this
102:12 - layer of 16 neurons is able to pick up
102:15 - on patterns of certain words and where
102:17 - they occur in the sentence and give us a
102:19 - accurate classification again it's going
102:21 - to do that by tweaking and modifying
102:23 - these weights and all of the biases that
102:26 - are on you know all of these different
102:28 - what do you call it layers or all of
102:29 - these connections or whatever they are
102:31 - and then it's going to give us some
102:32 - output and some level of accuracy for
102:34 - our network
102:39 - all right so now it's time to compile
102:41 - and train our model now the first thing
102:44 - we have to do is just define the model
102:45 - give it an optimizer give it a loss
102:47 - function and then i think we have to
102:49 - define uh the metrics as well so we're
102:51 - going to do is going to say model equals
102:53 - in this case
102:54 - or sorry not model equals model dot
102:56 - compile
102:57 - if i spell compile it correctly and then
103:00 - here we're gonna say uh optimizer
103:03 - we're gonna use the atom optimizer again
103:05 - i'm not really gonna talk about what
103:06 - these are that much if you're interested
103:08 - in the optimizer just look them up and
103:10 - then for the loss function
103:12 - where you're going to use the binary
103:14 - underscore cross
103:16 - entropy now what this one essentially is
103:18 - is well binary means like two options
103:21 - right and in our case we want to have
103:23 - two options for the output neuron which
103:25 - is zero or one so
103:27 - what's actually happening here is we
103:29 - have the sigmoid function which means
103:30 - our number is going to be between zero
103:32 - and one but what the loss function will
103:33 - do is pretty we'll calculate the
103:35 - difference between
103:37 - for example say our output neuron is
103:39 - like 0.2 and the actual answer was zero
103:42 - what will give us a certain function
103:44 - that can calculate the loss so how much
103:46 - of a difference 0.2 is from zero um and
103:50 - that's kind of how that works
103:53 - again i'm not going to talk about them
103:54 - too much and they're not like i mean
103:56 - they are important but
103:58 - not to really like memorize per se like
104:00 - you kind of just mess with different
104:01 - ones but in this case binary cross
104:03 - entropy works well because we have two
104:05 - possible values zero one so rather than
104:07 - using the other one that we used before
104:09 - which i don't even remember what was
104:11 - called something cross entropy we're
104:12 - using binary cross entropy okay so now
104:15 - what we're gonna do is we're actually
104:16 - gonna split our training data into two
104:18 - sets and the first set of our training
104:20 - data is going to be called validation
104:22 - data or really i guess you can think of
104:24 - it as a second the order doesn't really
104:25 - matter but what we're going to do is
104:26 - just get some validation data and what
104:28 - validation data is is essentially we can
104:32 - check how well our model is performing
104:34 - based on the tunes and tweaks we're
104:36 - doing on the training data on new data
104:38 - now the reason we do that is so that we
104:40 - can get a more accurate sense of how
104:42 - well our model is
104:44 - because we're going to be testing new
104:45 - data to get the accuracy each time
104:48 - rather than testing it on data that
104:50 - we've already seen before which again
104:52 - means that the model can't simply just
104:54 - memorize each review and give us either
104:55 - a zero or one for that it has to
104:57 - actually have some degree of i don't
104:59 - know like thinking or operation so that
105:02 - it can work on new data so what we're
105:03 - going to do is we're going to say x
105:04 - underscore val equals and all we're
105:06 - going to do is just grab the train data
105:08 - and we're just going to cut it to a
105:10 - thousand or 10 000 entries so there's
105:12 - actually 25 000 entries or i guess
105:15 - reviews in our training data so we're
105:17 - just going to take 10 000 of it and say
105:19 - we're going to use that as validation
105:21 - data now in terms of the size of
105:22 - validation data it doesn't really matter
105:25 - that much
105:26 - this is what tensorflow is using so i'm
105:28 - just kind of going with that but again
105:29 - mess with these numbers and see what
105:31 - happens to your model everything with
105:32 - our neural networks and machine learning
105:35 - really is going to come down to very
105:37 - fine what's known as hyper parameters or
105:39 - like hypertuning which means just
105:41 - changing individual parameters each time
105:43 - until we get a model that is well just
105:45 - better and more accurate
105:47 - so we're going to say that x val equals
105:49 - that but then we're also going to have
105:50 - to modify our x train data
105:52 - to be train underscore data and in this
105:55 - case we're just going to do the other
105:56 - way around so 10 000 colon now i'll just
105:59 - copy this and we're just going to
106:00 - replace this again with instead of test
106:02 - uh
106:04 - actually
106:05 - oh we have to do this with labels sorry
106:06 - what am i thinking so we're just going
106:07 - to train change this to be labels
106:10 - and then instead of x val it's just
106:12 - going to be y value and then y train
106:15 - so yeah we're not touching the test data
106:17 - because we're going to use all that test
106:19 - data to test our model and then we're
106:21 - just going to use the uh the training
106:22 - stuff or the validation data to validate
106:25 - the model all right so now that we've
106:27 - done that it is actually time to fit the
106:29 - model so i'm just going to say uh
106:31 - like
106:32 - fit
106:33 - model
106:34 - and you'll see why i'd name this
106:34 - something different in a second it's
106:36 - going to be equal to model.fit and in
106:38 - this case what we're going to do is
106:39 - we're going to say x underscore train
106:41 - y underscored train we're going to say
106:44 - epochs uh
106:47 - is equal to i that's how you spell it 40
106:49 - and again you can mess with this number
106:50 - and see what we get based on that and
106:52 - we're going to say batch underscore size
106:54 - equals 512 which i'll talk about in a
106:56 - second and then finally we're going to
106:58 - say validation underscore data equals
107:01 - and in here we're going to say
107:03 - x underscore val y underscore val and i
107:07 - think that's it let me just check here
107:09 - quickly oh one last thing that i forgot
107:11 - to do we're gonna say verbose
107:15 - equals
107:16 - one verbose equals one now i'm not gonna
107:19 - lie i honestly don't know what verbose
107:20 - is i probably should have looked it up
107:21 - before the video but i have no idea what
107:23 - that is so if someone knows please let
107:24 - me know but the batch size is
107:26 - essentially how many
107:29 - what do you call it um
107:30 - movie reviews we're gonna do each time
107:34 - or how many we're gonna load in at once
107:35 - because the thing is
107:36 - it's kind of i mean we're loading all of
107:39 - our reviews into memory but
107:42 - in some cases we won't be able to do
107:44 - that and we won't be able to like feed
107:45 - the model all of our reviews on each
107:48 - single cycle so we just set up a batch
107:50 - size that's going to define us
107:52 - essentially how many at once we're going
107:53 - to give
107:55 - and i know i'm kind of horribly
107:56 - explaining what a batch size is but
107:58 - we'll get into more on
108:00 - batch sizes and how we can kind of do
108:02 - like buffering through our data and like
108:04 - going taking some from a text file and
108:06 - reading into memory in later videos when
108:08 - we have like hundreds of gigabytes of
108:10 - data that we're gonna be working with
108:12 - okay so finally we're gonna say results
108:13 - equals and in this case i believe it is
108:16 - model dot evaluate and then we're going
108:19 - to evaluate this obviously on our test
108:22 - data so we're going to give it test data
108:24 - and test labels so test underscore data
108:27 - test underscore labels like that
108:30 - and then finally what i'm going to do is
108:33 - just actually print out the results so
108:36 - we can see what our accuracy is so say
108:37 - print
108:39 - results
108:40 - and then get that value so let me run
108:42 - this quickly neural networks text
108:45 - classification
108:47 - let's go cmd and then python
108:51 - text
108:52 - or that's not even the one we're using
108:53 - we're using tutorial 2 sorry and let's
108:55 - see what we get with this this will take
108:56 - a second to run through the epoch so
108:58 - i'll fast forward through that so you
108:59 - guys
109:00 - don't have to wait
109:02 - all right so we just finished doing the
109:04 - epochs now and essentially our accuracy
109:07 - was 87
109:10 - and this first number i believe is the
109:11 - loss which is 0.33 and then you can see
109:14 - that actually
109:16 - here we get the accuracy values and
109:18 - notice that the accuracy from our last
109:20 - epoch was actually greater than the
109:22 - accuracy on the test data which again
109:24 - shows you that
109:26 - sometimes you know when you test it on
109:28 - new data you're going to be getting a
109:30 - less accurate model or in some cases you
109:32 - might even get a more accurate model it
109:33 - really just you can't strictly go based
109:35 - off what you're getting on your training
109:37 - data you really do need to have some
109:39 - test and validation data to make sure
109:41 - that the model's correctly working so
109:43 - that's essentially what we've done there
109:46 - um
109:47 - and yeah i mean that that's the model
109:50 - we've we tested and it's 87 accurate so
109:52 - now let's actually have let's interpret
109:54 - some of these results a little bit
109:55 - better and let's show some reviews let's
109:58 - do a prediction on some of the reviews
110:00 - and then see like if this our model kind
110:02 - of makes sense for what's going on here
110:04 - so what i'm going to do is i'm just
110:05 - going to actually just copy some output
110:07 - that i have here um just save us a bit
110:09 - of time because i am going to wrap up
110:11 - the video in a minute here but
110:12 - essentially what this does it just takes
110:13 - the first review from test data
110:17 - gets the model to predict that because
110:19 - we obviously we didn't train on the test
110:20 - status we can do that fine
110:22 - we're going to say review and then we
110:24 - print out the decoded review
110:27 - we're going to print out what the model
110:28 - predicted and then we're going to print
110:30 - out what the actual label of that was
110:32 - so if i run this now i'll fast forward
110:34 - through the kind of training process and
110:36 - we'll see the
110:37 - all right so this is what essentially
110:39 - our review looks like so at least the
110:41 - one that we were testing it on and you
110:42 - can see that we have this little start
110:44 - tag and it says please give this one a
110:45 - miss for and then br stands for like
110:48 - break line or go to the next line so we
110:50 - could have actually added another tag
110:52 - for br uh if we noticed that this was
110:54 - used a lot in the review uh but we
110:57 - didn't do that so you see br
111:00 - unless this is actually part of the
111:01 - review but i feel like that should be
111:02 - like break line in terms of html anyways
111:05 - and then we have some unknown characters
111:06 - which could be anything that we just
111:08 - didn't know what it was and it says and
111:09 - the rest of the cast rendered terribly
111:11 - performed says the show is flat flat
111:13 - flat brbr i don't know how uh
111:16 - michael madison could have allowed this
111:18 - one on his plate he almost seemed he'd
111:20 - what does it seem to know this wasn't
111:22 - going to work out and his performance
111:23 - was quite unknown so all yeah so anyways
111:26 - you can see that this probably had like
111:28 - some emojis in it or something and
111:29 - that's why we have all these unknowns
111:31 - and then obviously we made this review
111:32 - which was pretty short to be the full
111:34 - length of 250 so we see all these pads
111:37 - that did that for us and then we have a
111:39 - prediction and an actual value of zero
111:41 - so we did end up getting this one
111:43 - correct
111:44 - now i think it'd be interesting actually
111:45 - to write your own review and test it on
111:48 - this
111:49 - so in the next video what i'm going to
111:50 - do is show you how we can save the model
111:52 - to avoid doing like all of this every
111:55 - time we want to run the code because
111:57 - realistically we don't want to wait like
111:59 - a minute or two before we can predict a
112:02 - movie review every time we just want it
112:03 - to happen instantly
112:05 - and we definitely can do that i just
112:06 - haven't showed that yet in the series
112:08 - because that's kind of in like later
112:10 - what you do after you learn machine
112:11 - learning um and obviously like this this
112:14 - model trained pretty quickly like we
112:15 - only had about uh what was it like 50
112:19 - 000 test data set which it seems like a
112:21 - large number but it's really not
112:23 - especially when you're talking about
112:24 - string data so in future videos we're
112:26 - gonna be training uh models that take
112:29 - like maybe a few days to train
112:31 - at least that's the goal or maybe a few
112:33 - hours or something like that
112:35 - so in that case you're probably not
112:36 - going to want to train it every time
112:37 - before you predict some information so
112:39 - that'll be useful to know how to save
112:40 - that
112:45 - so in today's video we're going to be
112:47 - doing is talking about saving and
112:48 - loading our models and then we're going
112:50 - to be doing a prediction on some data
112:52 - that doesn't come from this actual data
112:54 - set now i know this might seem kind of
112:56 - trivial we already know how to do
112:57 - predictions but trust me when i tell you
112:59 - this is a lot harder than it looks
113:01 - because if we're just taking in string
113:03 - data that means we have to actually do
113:04 - the encoding of the pre-processing
113:07 - removing certain characters making sure
113:10 - that that data looks the same as the
113:12 - data that our neural network is
113:13 - expecting which in this case is a list
113:16 - of encoded numbers right or of encoded
113:18 - words that is essentially just numbers
113:20 - so what we're going to do to start is
113:22 - just save our model so let's talk about
113:24 - that now so up until this point every
113:26 - time we've wanted to make a prediction
113:27 - we've had to retrain the model now on
113:30 - small models like this that's fine you
113:32 - have to wait a minute two minutes but
113:33 - it's not very convenient when you have
113:34 - models that maybe take you days weeks
113:36 - months years to train right so what you
113:39 - want to do is when you're done training
113:40 - the model you want to save it or
113:42 - sometimes you even want to save it like
113:44 - halfway through a training process this
113:46 - is known as checkpointing the model so
113:48 - that you can go back and continue to
113:49 - train it later now in this video we're
113:51 - just going to talk about saving the
113:52 - model once it's completely finished but
113:54 - in future videos when we have larger
113:56 - networks we will talk about
113:57 - checkpointing and how you how to load
113:59 - your or train your model in like batches
114:02 - with different size data and all that so
114:04 - what i'm going to start by doing is just
114:06 - actually bumping the
114:07 - vocabulary size of this model up to 88
114:10 - 000. now the reason i'm doing that is
114:12 - just because for our next exercise which
114:15 - is going to be making predictions on
114:17 - outside data we want to have as many
114:19 - words in our model as possible so that
114:21 - when it gets kind of some weirder words
114:23 - that aren't that common it knows what to
114:24 - do with them uh so i've done a few tests
114:26 - and i noticed that with the what he
114:28 - called with the vocabulary size bumped
114:30 - up it performs a little bit better so
114:31 - we're going to do that
114:33 - so anyways we bumped the vocabulary size
114:35 - and now after we train the model we need
114:36 - to save it
114:38 - now to save the model all we have to do
114:40 - is literally type the name of our model
114:41 - in this case model dot save and then we
114:44 - give it a name so in this case let's
114:45 - call it model dot h5
114:47 - now h5 is just like an extension that uh
114:51 - means
114:52 - i don't know it's like
114:53 - i honestly don't know why they use h5
114:55 - but
114:56 - it's the extension for a saved model and
114:58 - keras and tensorflow so we're just going
115:00 - to work with that and that's as easy as
115:02 - this is it's just going to save our
115:04 - model in binary data which means we'll
115:06 - be able to read it in really quickly and
115:08 - use the model when we want to actually
115:10 - make predictions so let's go ahead and
115:12 - run this now and then we're going to
115:14 - have the model saved and then from now
115:15 - on we won't have to continually train
115:17 - the model when we want to make
115:18 - predictions i'm going to say python
115:20 - tutorial 2 and i'll be right back once
115:23 - this finish finishes running all right
115:25 - so the model has finished training
115:26 - notice that our accuracy is slightly
115:29 - lower than it was in the previous video
115:31 - really kind of a negligible difference
115:34 - here but anyways just notice that
115:36 - because we did bump the vocabulary size
115:37 - so anyways now that we've saved the
115:39 - model we actually don't have to go
115:41 - through this tedious process every time
115:42 - we run the code of creating and training
115:45 - and fitting the model and in fact we
115:47 - don't actually need to save it as well
115:48 - either here to load our model in now
115:50 - that it's saved and you can see the file
115:52 - right here with all this uh this big
115:54 - massive binary blob here all we have to
115:56 - do to load this in is just type one line
115:59 - now the line is whatever the name of
116:00 - your model is it doesn't matter i'm just
116:02 - going to call it model
116:03 - is equal to in this case
116:05 - keras dot models dot load underscore
116:08 - model and then here you just put the
116:10 - name of that file so in this case
116:11 - model.h5
116:13 - now what's really nice about this as
116:14 - well is you can actually train a bunch
116:16 - of different models and tweak like hyper
116:18 - parameters of them and only save the
116:20 - best one what i mean by that is like
116:22 - maybe you mess with for example the
116:24 - amount of neurons in the second
116:25 - activation layer uh or something like
116:27 - that or in the second hidden layer and
116:29 - then you train a bunch of models you
116:31 - figure out which one has the highest
116:32 - accuracy and then you only save that one
116:35 - that's nice as well and that's something
116:36 - you could do like overnight you could
116:38 - run like your script for a few hours
116:40 - train a bunch of models figure out which
116:41 - one is the best
116:42 - only save and then use that one so
116:44 - anyways we're going to load in this
116:46 - model
116:47 - notice that i've actually just commented
116:49 - out this aspect down here because we're
116:50 - not going to use this anymore
116:52 - and now what we're going to start doing
116:54 - is actually training or testing the
116:56 - model on some outside data
116:58 - so i've gone ahead and picked a movie
117:00 - review for one of my favorite movies
117:02 - some of you guys can read this if you
117:03 - want uh but it's the lion king
117:05 - absolutely love that movie so i've
117:06 - decided to go with this this review was
117:08 - a 10 out of 10 review so a positive
117:09 - review and we're gonna test our model on
117:11 - this one now i actually did take this
117:13 - off like the imdb website or whatever
117:16 - that's called um but the data set that
117:19 - they use is different so this is you
117:21 - guys will see it why this works a little
117:22 - bit differently and what we have to do
117:24 - with this so this is in a text file so
117:26 - what i'm going to do is load in the text
117:28 - file here in code and then get that big
117:31 - blob that string and convert it into a
117:33 - form that our model can actually use so
117:35 - the first step to do this obviously is
117:37 - to get that string so we're going to say
117:38 - with open and in this case i've called
117:40 - my file test.txt
117:43 - and then i'm just going to set the
117:44 - encoding because i was running into some
117:46 - issues here you guys probably don't have
117:47 - to do this i'm just going to say utf
117:49 - hyphen 8 which is just kind of a
117:50 - standard text encoding and we're going
117:52 - to say as f
117:54 - now again the reason i use with is just
117:56 - because that means i don't have to close
117:57 - the file afterwards
117:59 - better practice if you want to use that
118:00 - and now i'm going to say for
118:02 - line in f dot
118:05 - read lines which essentially just means
118:07 - we're going to get each line in this
118:09 - case we only have one line but if we
118:10 - wanted to throw in a few more uh reviews
118:13 - in here and do some predictions on those
118:14 - that would be very easy to do by just
118:16 - keeping this code structure just throw
118:18 - another line in there and now i'm just
118:20 - going to say
118:21 - we're going to grab this line and we're
118:22 - going to start pre-processing it so that
118:24 - we can actually feed it to our model now
118:27 - notice that this when we read this in
118:29 - all we're going to get is a large string
118:31 - but that's no good to us we actually
118:33 - need to convert this into an encoded uh
118:36 - list of numbers right and essentially we
118:38 - need to say okay so of that's a word
118:40 - what number represents that put that in
118:42 - a list same with all same with uh same
118:45 - with animation right and we keep going
118:47 - and keep going
118:48 - pretty well for all of the words in here
118:50 - and we also have to make sure that the
118:53 - size of our text is only at max 250
118:56 - words because that's what we were using
118:58 - when we were training the data so it's
119:00 - expecting a size of that and if you give
119:02 - it something larger that's not going to
119:04 - work or it might but you're going to get
119:05 - a few errors with that so anyways the
119:07 - first step here is i'm going to say n
119:09 - line is equal to
119:11 - line dot and i'm going to remove a bunch
119:13 - of characters that i don't want so i'm
119:15 - just going to say dot replace i think
119:17 - this is the best way to do it but maybe
119:19 - not um and i'm going to replace all the
119:22 - commas all of the periods all of the
119:24 - brackets and all of the colons and i'll
119:26 - talk about more why we want to do that
119:28 - in just one second so we'll do dollar
119:30 - place
119:32 - i guess this dollar place should
119:33 - probably be outside the bracket uh and
119:35 - then we'll replace with a bracket with
119:37 - nothing
119:40 - and i know this is there probably is a
119:42 - better way to do this but for our
119:43 - purposes it's not really that important
119:45 - and finally we will replace all our
119:47 - colons with nothing as well now again
119:50 - the reason i'm doing this is because
119:53 - let's go here if you have a look for
119:55 - example when we split this because we're
119:57 - just going to split this data by um
119:59 - spaces and to get all the words
120:02 - what will end up happening is we're
120:03 - going to get words like company comma
120:05 - we're going to get words like i'm trying
120:07 - to find something that has a period like
120:09 - art
120:10 - dot and then a quotation mark right and
120:12 - we don't want those
120:14 - to be words in our list because there's
120:15 - no mapping for art period there's only a
120:18 - mapping for art which means that i need
120:20 - to remove all of these kind of symbols
120:22 - so that when we split our data we get
120:25 - the correct words now there'll be a few
120:27 - times where the split doesn't work
120:29 - correctly but that's okay
120:31 - as long as the majority of them are
120:32 - working well same thing with brackets
120:33 - right i can't have irons and then a
120:35 - closing bracket is one of my words so i
120:37 - need to get rid of that now this reminds
120:38 - me i need to remove quotation marks as
120:40 - well because they use quite a few of
120:41 - those in there i don't know why i closed
120:43 - that document uh so let's do that as
120:45 - well with one last replace
120:47 - so say dollar place in this case we'll
120:49 - actually just do backslash quotation
120:51 - mark
120:52 - and then again with nothing now i'm
120:54 - adding a dot strip here to get rid of
120:56 - that backslash n and now we're going to
120:58 - say dot split and in this case we'll
121:00 - split out a space now i know this is a
121:03 - long line but that's all we need to do
121:05 - to remove everything and now we actually
121:07 - need to encode and trim our data down to
121:09 - 250 words so to encode our data i'm
121:12 - going to say encode equals in this case
121:15 - uh and we're just literally we'll make a
121:16 - function called like
121:18 - review underscore in code and we'll pass
121:21 - in our endline
121:23 - now what review and code will do is look
121:26 - up the mappings for all of the words and
121:28 - return to us an encoded list and then
121:31 - finally what we're going to do and we'll
121:32 - create this function in just a second
121:34 - don't worry it doesn't already exist is
121:35 - we're actually going to use what we've
121:37 - done up here with this test data train
121:39 - data keras pre-processing stuff and
121:41 - we're just going to apply this to in
121:43 - this case our encoded data so we add
121:45 - those pad tags or we trim it down to
121:47 - what it needs to be so in this case
121:49 - we'll say encode equals
121:52 - keras.preprocessing instead of train
121:54 - data we'll just pass in this case
121:55 - actually a list and then encode inside
121:57 - it because that's what it's expecting to
121:59 - get a list of lists
122:01 - all right so now that we've done that
122:03 - our final step would be to use the model
122:05 - to actually make a prediction so we're
122:06 - going to say model.predict
122:09 - and then in this case we'll pass it
122:10 - simply this encode right here which will
122:12 - be in the correct form now we'll save
122:14 - that under predict
122:16 - and then what we'll do is just simply
122:18 - print out the model so we'll say print
122:20 - or not the model sorry we'll print the
122:21 - original text which will be the review
122:23 - so in this case we'll print line
122:26 - and then we will print out the encoded
122:28 - review just so we can have a look at
122:29 - what that is and then finally we'll
122:31 - print the prediction so what whether the
122:33 - model thinks it's positive or negative
122:35 - so we'll just say predict and in this
122:37 - case we'll just put zero because we're
122:38 - only going to be doing um like one at a
122:41 - time right
122:42 - okay sweet so now the last thing that we
122:44 - need to do is just simply write this
122:46 - review in code function and it will be
122:48 - good to go and start actually using our
122:50 - model so i'm just going to say define
122:52 - review underscore in code this is going
122:54 - to take a string we'll just call that s
122:57 - lowercase s and what we're going to do
122:59 - in here is set up a new list that we're
123:01 - going to append some stuff into so i'm
123:02 - just going to say like return
123:05 - let's just say like encoded
123:07 - equals and then i'm going to start this
123:08 - with 1. now the reason i start 1 in here
123:11 - is because all of our data here uh where
123:14 - it starts has a one so we're just going
123:15 - to start with one uh because we won't
123:17 - have added that in
123:19 - from uh the other way i hope you guys
123:21 - understand that just we're setting like
123:22 - a starting tag to be consistent with the
123:24 - rest of them
123:25 - and now what we're going to do is we're
123:26 - going to loop through every single word
123:28 - that's in our s here which will be
123:30 - passed in as a list of words we'll look
123:32 - up the numbers associated with those
123:34 - words and add them into this encoded
123:36 - list we're going to say for word and in
123:39 - this case we're going to say word in s
123:41 - now we'll say
123:43 - if
123:44 - word in this case word underscore index
123:47 - and again we're going to use word
123:48 - underscore index as opposed to reverse
123:50 - word index because word index stores all
123:53 - of the words corresponding to the
123:55 - letters or not the letters the numbers
123:58 - which means that we can literally just
123:59 - throw our data into word index and it'll
124:01 - give us the number associated with each
124:03 - of those words so we're going to say if
124:05 - word in word index then we'll say
124:07 - encoded dot append and in this case
124:09 - we'll simply append in this case
124:12 - word index
124:14 - word
124:14 - now otherwise what we'll do is we'll say
124:17 - encoded dot append
124:19 - to now what will happen is we're going
124:22 - to check here if word if the word is
124:24 - actually in our vocabulary
124:27 - which is represented by word index which
124:29 - is just a dictionary of all the words
124:31 - corresponding to all the numbers that
124:33 - represent those words now if it's not
124:35 - what we'll do is we'll add in that
124:37 - unknown tag so that the program knows
124:39 - that this is an unknown word
124:41 - otherwise we'll simply add the number
124:42 - associated with that word now one last
124:44 - thing to do is actually just do
124:45 - word.lower here just to make sure that
124:47 - if we get any words that have some weird
124:49 - capitalization
124:50 - they are still found in our vocabulary
124:53 - so like words at the beginning of a
124:54 - sentence and stuff like that
124:56 - uh and now with that being done i
124:58 - believe we're actually finished and
125:00 - ready to run this code so what's nice
125:02 - about this is now that we've saved the
125:03 - model we don't have to train it again so
125:05 - i can literally just run this and it
125:06 - should happen fairly quickly
125:09 - fingers crossed let's see
125:13 - all right must be a list of integrals
125:15 - found non-iterable object so what error
125:18 - is that here um
125:20 - in code encoding coding code
125:23 - all right so print review and code ah
125:26 - well it would be helpful if i returned
125:29 - the encoded list and that would have
125:30 - been our issue there so let's run that
125:32 - one more time and see what we're getting
125:33 - there
125:34 - and there we go sweet so this is
125:37 - actually the review i know it's very
125:39 - really hard to read here but if you guys
125:40 - want to go ahead and read it feel free
125:42 - since it's on the lion king it's
125:43 - obviously a positive review and then you
125:45 - can see this is what we've ended up with
125:47 - so our review has been translated into
125:49 - this which means we've actually trimmed
125:50 - quite a bit of the review and you can
125:52 - see that wherever it says two that is
125:54 - actually a word that we didn't know or
125:56 - that wasn't in our vocabulary four
125:58 - represents the that's why there's a lot
126:00 - of fours and then all the other words
126:01 - have their correspondence right now
126:03 - fortunately for us we picked a 88 000
126:06 - vocabulary which means that we can get
126:08 - indexes like 20 000 whereas before it
126:10 - would have all been under 10 000. and
126:12 - you can see that our prediction here is
126:14 - now 96 um positive which means that
126:17 - obviously like we were going between
126:18 - zero where zero is a negative review and
126:21 - one is a positive review so this
126:22 - classified correctly as very positive
126:25 - review and we could try this on all
126:27 - other kinds of reviews and see what we
126:28 - get but that is how you go about kind of
126:31 - transforming your data into the form
126:33 - that the network expects
126:35 - and that's where i'm trying to get you
126:36 - guys at right now is to understand that
126:39 - yes it's really easy when we're doing it
126:41 - with
126:42 - this kind of data that just comes in
126:44 - like imdb like keras load data but as
126:47 - soon as you actually have to start using
126:48 - your own data there's quite a bit of
126:49 - manipulation that you have to do
126:52 - and things that you might not think
126:53 - about when you're actually feeding it to
126:55 - the network and in most cases you can
126:57 - probably be sure that your network is
127:00 - not actually the thing that's happening
127:01 - incorrectly but it's the data that
127:03 - you're feeding it is not in the correct
127:04 - form um and it can be tricky to figure
127:07 - out what's wrong with that data so with
127:09 - that being said that has been it for
127:10 - this video i hope you guys enjoyed
127:12 - that's going to wrap up the text
127:13 - classification aspect here of neural
127:15 - networks
127:20 - hey guys so in today's video i'm going
127:22 - to be showing you how to install
127:23 - tensorflow 2.0 gpu version on an ubuntu
127:27 - linux machine now this should work for
127:29 - any version of linux or any linux
127:31 - operating system although the one i am
127:33 - going to be showing you on is ubuntu
127:35 - 18.0.4 now you may notice that i'm
127:37 - actually on a windows machine right now
127:39 - and that this is actually just an ubuntu
127:41 - terminal that's open now i'm actually
127:43 - just ssh into a server that i have that
127:46 - contains two 1080 graphics cards so gtx
127:49 - 1080s and that's how i'm going to be
127:50 - showing you how to do this now quickly
127:52 - if you don't understand the difference
127:53 - between the cpu and the gpu version the
127:56 - cpu version is essentially just way
127:57 - slower and you would only really use the
128:00 - cpu version if you don't have a graphics
128:02 - card in your computer that is capable of
128:04 - running tensorflow 2.0 gpu so quickly
128:07 - before we go forward and you guys get
128:09 - frustrated with not being able to
128:10 - install this make sure that you have a
128:12 - graphics card that actually works for
128:14 - this programmer for this module that
128:16 - means you have to have a graphics card
128:18 - that is a gtx 1050 ti or higher those
128:22 - are the ones that are listed on
128:23 - tensorflow's website as compatible with
128:26 - tensorflow 2.0 gpu if you want to have a
128:28 - quick thing without having to go to the
128:30 - website to see if yours works if it has
128:31 - four gigs of video ram and is a gtx
128:35 - generation card or higher it most likely
128:37 - works with tensorflow 2.0 now i don't
128:40 - know about all the different cards if
128:41 - you have any questions leave them below
128:43 - i'll try to answer that for you but any
128:44 - 1060 1070 1080 or rtx cards
128:48 - that have cuda cores on them will work
128:50 - for this essentially you just need a
128:52 - cuda enabled gpu so you can check if
128:54 - yours meets that requirement before
128:56 - moving forward
128:57 - now to do this i'm just going to be
128:58 - following the steps listed on the
129:00 - tensorflow website now you may run into
129:02 - some issues while doing this
129:04 - but for ubuntu this is pretty
129:06 - straightforward and i'm essentially just
129:07 - going to be copying these commands and
129:09 - pasting them in my terminal now if you'd
129:11 - like to just try to do this without
129:12 - following along with video go ahead but
129:14 - i will be kind of showing you some fixes
129:16 - that i ran into while i was doing this
129:18 - um so let's go ahead and get started so
129:20 - actually let me just split the screen up
129:22 - so we can have a look at both of them at
129:23 - once i'm in my linux machine right now
129:26 - you just have to get to the terminal you
129:27 - notice that i don't even have a desktop
129:29 - and i'm literally just going to start
129:30 - copying and pasting these commands now
129:32 - the first thing that we need to install
129:34 - is actually
129:36 - cuda now cuda is what allows us to use
129:39 - the cuda cores on our gpu to actually
129:42 - run the code so just go ahead and keep
129:45 - copying these commands it will take a
129:46 - second and i actually already have this
129:48 - installed on my machine so i'm going to
129:49 - go through the steps with you guys but
129:52 - again if anything is different on my
129:53 - machine that's probably because it's
129:54 - already installed so if you don't know
129:56 - how to copy it into a window like this
129:59 - you just right click on your mouse and
130:00 - it'll copy if you're using a server like
130:02 - i am um but anyways we'll just go
130:05 - through all of these and keep going now
130:07 - i will have all these commands listed in
130:10 - my description as well and that should
130:12 - show you guys you know if the website
130:14 - goes down at any point you can just copy
130:16 - it from there as well so yeah literally
130:17 - just keep going all we're doing here is
130:19 - adding nvidia packages we're gonna make
130:21 - sure we have the nvidia drivers for our
130:23 - graphics card that are correct and then
130:26 - we're gonna go ahead and install
130:27 - tensorflow 2.0 uh so yep go through
130:30 - these commands there's not really much
130:31 - for me to say as i copy these in
130:33 - and eventually we will get through them
130:35 - all
130:38 - all right so now we're going to install
130:39 - the nvidia driver you can see that's all
130:41 - commented out on this tensorflow website
130:43 - here
130:44 - copy that
130:47 - and again just continue to go i don't
130:49 - really have any commentary for you guys
130:51 - here
130:52 - so we'll copy this this is going to
130:53 - install obviously the development and
130:55 - runtime libraries which we need and it
130:57 - says minimum four gigs or approximately
130:59 - four gigabytes which will mean that's
131:00 - how long it how many gigabytes it's
131:02 - going to take up on our machine so this
131:03 - will take a second and i'll fast forward
131:05 - through these stuff if it does take a
131:06 - while finally we're going to install
131:08 - tensor rt i don't even know what this is
131:10 - but apparently it's required and then
131:12 - after we're done this we should actually
131:14 - be finished installing everything that
131:16 - we need for tensorflow 2.0 to work again
131:19 - if you guys want to go through this just
131:20 - go to the website copy all of these
131:22 - commands in order paste them into here
131:24 - and they should work properly now
131:26 - finally what we have to do is actually
131:28 - install tensorflow 2.0 so we've got all
131:30 - the dependency dependencies installed
131:33 - and now to install tensorflow 2.0 we're
131:35 - just going to say pip3
131:37 - install
131:38 - tensorflow and i believe we're going to
131:40 - say hyphen gpu and then equals equals
131:43 - 2.0 point
131:45 - i got to find it up here to make sure
131:47 - that we do it correctly 2.0.0
131:50 - hyphen alpha
131:52 - zero like that so then we'll do that and
131:55 - that should install tensorflow 2.0 for
131:57 - us now i already have this installed but
131:59 - this will actually take a few minutes to
132:01 - install because there is quite a bit of
132:02 - stuff that it needs to download on your
132:04 - computer so anyways that has been it for
132:06 - installing tensorflow 2.0 on your
132:08 - computer using the gpu version again
132:10 - throughout the rest of the neural
132:11 - network series i'm going to be going
132:12 - forward doing this on an ubuntu machine
132:15 - so running all of the code i'll do the
132:16 - development of windows
132:18 - throw the files on my server train the
132:20 - model train the models excuse me and
132:22 - then take the models off and
132:24 - use them on my windows machine
132:26 - so if you want to validate if this is
132:27 - working you can really quickly just do
132:29 - python 3 in linux and then you can cite
132:32 - do
132:32 - import tensorflow and doing that you
132:36 - shouldn't get any errors and if you
132:37 - don't get any errors then you have
132:39 - successfully installed tensorflow 2.0
132:41 - now a few errors here if you guys are
132:43 - still listening and stuff wasn't working
132:45 - if for some reason when you install
132:46 - tensorflow and you notice that it's not
132:48 - using your gpu go ahead and uninstall
132:50 - the cpu version of tensorflow so just
132:52 - pip3
132:53 - on
132:54 - uninstall and then tensorflow and i
132:58 - guess you'd have to just do
133:00 - just tensorflow like that and that will
133:02 - install the cpu version if it is
133:03 - installed in your machine so anyways
133:05 - that has been it for how to install
133:06 - tensorflow 2.0 gpu version on ubuntu
133:09 - pretty straightforward just go through
133:10 - copy these commands and if you guys have
133:12 - any questions or errors please just
133:13 - leave them in the comments below and i
133:14 - will try my best to help you out

Cleaned transcript:

hey guys and welcome to a brand new tutorial series on neural networks with python and tensorflow 2.0 now tensorflow 2.0 is the brand new version of tensorflow still actually in the alpha stages right now but it should be released within the next few weeks but because it's an alpha tensorflow has been kind enough to release us that alpha version so that's what we're gonna be working with in this tutorial series and this will work for all future versions of tensorflow 2.0 so don't be worried about that now before i get too far into this first video i just want to quickly give you an overview of exactly what i'm going to be doing throughout this series so you guys have an idea of what to expect and what you're going to learn now the beginning videos and especially this one are going to be dedicated to understanding how a neural network works and i think this is absolutely fundamental and that you have to have some kind of basis on the math behind a neural network before you're really able to actually properly implement one now tensorflow does a really nice job of making it super easy to implement neural networks and use them but to actually have a successful and complex neural network you have to understand how they work on the lower level so that's what we're going to be doing for the first few videos after that what we'll do is we'll start designing our own neural networks that can solve the very basic uh mnist data sets that tensorflow provides to us now these are pretty straightforward and pretty simple but they give us a really good building block on understanding how the architecture of a neural network works what are some of the different activation functions how you can connect layers and all of that which will transition us nicely into creating our own neural networks using our own data for something like playing a game now personally i'm really interested with neural networks playing games and i'm sure a lot of you are as well and that's what i'm going to be aiming to do near the end of the series on kind of our larger project i'll be designing a neural network and tweaking it so it can play a very basic game that i've personally designed in python with pygame now with that being said that's kind of it for what we're going to be doing in this series i may continue this on future uh in later videos and do like very specific neural network series maybe a chat bot or something like that but i need you guys to let me know on what you'd like to see in the comments down below with that being said if you're excited about the series make sure you drop a like on this video and subscribe to the channel to be notified when i post the new videos and with that being said let's get into this first video on how a neural network works and what a neural network is so let's start talking about what a neural network is and how they work now when you hear neural network you usually think of neurons now neurons are what compose our brain and i believe don't quote me on this we have billions of them in our body or in our brain now the way that neurons work on a very simple and high level is you have a bunch of them that are connected in some kind of way so let's say these are four neurons and they're connected in some kind of pattern now in this case our pattern is completely like uh like random we're just arbitrary we're just picking a connection but this is the way that they're connected okay now neurons can either fire or not fire so you need to be on or off just like a one or zero okay so let's say that for some reason this neuron decides to fire maybe you touch something maybe you um smelt something something fires in your brain and this neuron decides to fire now it's connected to in this case all of the other neurons so what it will do is it will look at its other neurons and the connection and it will possibly cause its connected neurons to fire or to not fire so in this case let's say maybe what this one firing causes this connected neuron to fire this one to fire and maybe this one was already firing and now it's decided it turned it off or something like that okay so that's what happened now when this neuron fires well it's connected to this neuron and it's connected to this neuron well it's already got that connection but let's say that maybe when this one fires it causes this one to unfire because it was just fired something like that right and then this one now that it's off it causes this one to fire back up and then it goes it's just a chain of firing and unfiring and that's just kind of how it works right firing and unfiring now that's as far as i'm going to go into explaining neurons but this kind of gives us a little bit of a basis for a neural network now a neural network essentially is a connected layer of neurons or connected layers so multiple of neurons so in this case let's say that we have a first layer we're going to call this our input layer that has four neurons and we have one more layer that only contains one neuron now these neurons are connected now in our neural network we can have our connections happening in different ways we can have each uh what he called neuron connected to each other neuron so from layer to layer or we can have like some connected to others some not connected some connected multiple times it really depends on the type type of neural network we're doing now in most cases what we do is we have what's called a fully connected neural network which means that each neuron in one layer is connected to each neuron in the next layer exactly one time so if i were to add another neuron here then what would happen is each of these neurons would also connect to this neuron one time so we would have a total of eight connections because four times two is eight right and that's how that would work now for simplicity stake we're just going to use um one neuron in the next layer just to make things a little bit easier to understand now all of these connections have what is known as a weight now this is in a neural network specifically okay so we're going to say this is known as weight one this is known as weight two this is weight three and this is weight four and again just to reemphasize this is known as our input layer because it is the first layer in our connected layers of neurons okay and going with that the last layer in our connected layer of neurons is known as our output layer now these are the only two layers that we really concern ourselves with when we look and use a neural network now obviously when we create them we have to determine what layers we're going to have and the connection type but when we're actually using the neural network to make predictions or to train it we're only concerning ourselves with the input layer and the output layer now what does this do and how do these neural networks work well essentially given some kind of input we want to do something with it and get some kind of output right in most instances that's what you want input results in an output in this case we have four inputs and we have one output but we could have a case where we have four inputs and we have 25 outputs right it really depends on the kind of problem we're trying to solve so this is a very simple example but what i'm going to do is show you how we would or how a neural network would work to train a very basic snake game so let's look at a very basic snake game so let's say this is our snake okay and this is his head um actually yeah let's say this is his head but like this is what the position of the snake looks like where this is the tail okay we'll circle the tail now what i want to do is i want to train a neural network that will allow this snake to stay alive so essentially its output will be what direction to go in or like to follow a certain direction or not okay essentially just keep this snake alive that's what i want it to do now how am i going to do this well the first step is to decide what our input is going to be and then to decide what our output is going to be so in this case i think a clever input is going to be do we have something in front of the snake do we have something to the left of the snake and do we have something to the right of the snake because in this case all that's here is just the snake and he just needs to be able to survive so what we'll do is we'll say okay is there something to the left yes no something in front yes no so zero one something to the right yes no and then our last input will be a recommended direction for the snake to go in so the recommended direction could be anything so in this case maybe we'll say the recommended direction is left and what our output will be is whether or not to follow that recommended direction or not or to try to do a different uh recommendation essentially or go to a different direction so let's do one case on how we would expect this neural network to perform without train like once it's trained right based on some given input so let's say there's not something to the left so we're going to put a 0 here because this one will represent if there's anything to the left the next one will be front so we'll say well there's nothing in front the next one will be to the right so we'll say right and we'll say yes there is something to the right of the snake and our recommended direction can be anything we'd like so in this case we say the recommended direction is left and the way we'll do the recommended direction is negative 1 0 1 where negative 1 is left 0 is in front and 1 is to the right okay so we'll say in this case our recommended direction is negative 1 and we'll just denote this by direction now our output in this instance should either be a zero or a one representing do we follow the recommended direction or do we not so let's see in this case following the recommended direction we keep our snake alive so we'll say one yes we will follow the recommended direction that is acceptable that is fine we're going to stay alive when we do that now let's see what happens when we change the recommended direction to be right so let's say that we say one as our recommended direction again this is dern here then what should our output be well if we decide to go right we're gonna crash into our tail which means that we should not follow that direction so our output should be zero so i hope you're understanding how we would expect this neural network to perform all right so now how do we actually design this neural network how do we get this work how do we train this right well that is a very good question and that is what i'm going to talk about now so let me actually just erase some of this stuff so we have a little bit more room to work with some math stuff right here but right now what we start by doing is we start by designing what's known as the architecture of our neural network so we've already done this we have the input and we have the output now each of our inputs is connected to our outputs and each of these connections has what's known as a weight now another thing that we have is each of our input neurons has a value right we had in this case we either had 0 or we had 1. now these values can be different right these values can either be decimal values or they can be like between 0 and 100 they don't have to be just between 0 and 1 but the point is that we have some kind of value right so what we're going to do in this output layer to determine what way we should go is essentially we are going to take the weighted sum of the values multiplied by the weights i'm going to talk about how this works more in depth in a second but just just follow me now so what this symbol means is take the sum and what we do is i'm going to say in this case i which is going to be our variable and i'll talk about how this kind of thing works in a second we'll say i equals 1 and i'm going to say we'll take the weighted sum of in this case value i multiplied by weight i so what this means essentially is we're going to start at i equals 1 we're going to use i as our variable for looping and we're going to say in this case we're going to do v1 times vi or sorry vi times wi and then we're going to add all those so what this will return to us actually will be v1 w1 plus v2 w2 plus b3 w3 plus v4 w4 and this will be uh our output that's that's what our output layer is going to have as a value now this doesn't really make um much sense right now right like why why are we doing this weights what is this multiplication well just follow with me for one second so this is what our output layer is going to do now there's one thing that we have to add to this as well and this is what is known as our biases okay so what we're going to do is we're going to take this weighted sum but we're also going to have some kind of bias on each of these weights okay and what this bias is known as it's denoted by c typically but essentially it is some value that we just automatically add or subtract it's a constant value for each of these weights so we're going to say all of these these connections have a weight but they also have a bias so we're going to have b1 b2 b3 and b4 uh well we'll call it b instead of c so what i'll do here is what i'm also going to do is i'm also going to add these biases in when i do these weights so we're going to say bi as well so now what we'll have is we'll have at the end here plus bi or plus b1 plus b2 plus b3 plus b4 now again i know you guys are like what the heck am i doing this makes no sense it's about to make sense in one second so now what we need to do is we need to train the network so we've understood now this is essentially what this output layer is doing we're taking all of these um weights and these values we're multiplying them together and we're adding them and we're taking what's known as the weighted sum okay but how do we like what are these values how do we get these values and how is this going to give us a valid output well what we're going to do is we're going to train the network on a ton of different information so let's say we play 1 000 games of snake and we get all of the different inputs and all the different outputs so what we'll do is we'll randomly decide like a recommended direction and we'll just take the state of the snake which will be either is there something to left to the right or in front of it and then we'll take the output which will be um like did the snake survive or did the snake not survive so uh what we'll do is we'll we'll train the network using that information so we'll generate all of this different information and then train the network and what the network will do is it will look at all of this information and it will start adjusting these biases and these weights to properly get a correct output because what we'll do is we'll give it all this input right so let's say we give it the input again of zero one zero and maybe one like this is a random input and let's say the output for this case is um what do you call it so one is go to the right the output is one which is correct well what the network will do is say okay i got that correct so what i'm going to do is i'm not going to bother adjusting the network because uh this is fine so i don't have to change any of these biases i don't have to change any of these weights everything is working fine but let's say that we get the answer wrong so maybe the output was zero but the answer should have been one because we know the answer obviously because we've generated all the input and the output so now what the network will do is it will start adjusting these weights and adjusting these biases it'll say alright so i got this one wrong and i've gotten like five or six wrong before and this is what was familiar when i got something wrong so let's add one to this bias or let's multiply this weight by two and what it will do is it'll start adjusting these weights in these biases so that it gets more things correct so obviously that's why neural networks typically take a massive amount of information to train because what you do is you pass it all of this information and then it keeps going through the network and at the beginning it sucks right because it has this network just starts with random weights and random biases but as it goes through and it learns it says okay well i got this one correct so let's leave the weights and the biases the same but let's remember that this is what the way in the bias was when this was correct and then maybe he gets something wrong and it says okay so let's adjust bias one a little bit let's adjust weight one uh let's mess with these and then let's try another example and then it says okay i got this example right maybe we're moving in the right direction maybe we'll adjust another weight maybe we'll adjust another bias and eventually your goal is that you get to a point where your network is very accurate because you've given it a ton of data and it's adjusted the weights and the biases correctly so that this kind of formula here of this weighted average will just always give you the correct answer or has a very high accuracy or high chance of giving you the correct answer so i hope that kind of makes sense i'm definitely over simplifying things in how the adjustment of these weights and these biases work but it's not crazy important and we're not going to be doing any of the adjustment ourselves we're we are just going to be kind of tweaking a few things with the network so as long as you understand that when you feed information what happens is it checks whether the network got it correct or got it incorrect and then it adjusts the network accordingly and that is how the learning process works for a neural network all right so now it's time to discuss a little bit about activation functions so right now what i've actually just described to you is a very advanced technique of linear regression so essentially i was saying we're adjusting weights we're adjusting biases and essentially we're creating a function that given the inputs of like y z w or like left front right we are giving some kind of output but all we've been doing to do that essentially is just adjusting a linear function because our degree is only one right we have weights of degree one multiplying by values of degree 1 and we're adding some kind of bias and that kind of reminds you of the form mx plus b we're literally just adding a bunch of mx plus b's together which gives us like a fairly complex linear function but this is really not a great way to do things because it limits the degree of complexity that our network can actually have to be linear and that's not what we want so now we have to talk about activation functions so if you understand everything that i've talked about so far you're doing amazing this is great you understand that essentially the way that the network works is you feed information in and it adjusts these weights and biases there's a specific way it does that which we'll talk about later and then you get some kind of output and based on that output you're trying to adjust the weights and biases and and all that right so now what we need to do is talk about activation functions and what an activation function does is it's essentially a nonlinear function that will allow you to add a degree of complexity to your network so that you can have more of a function that's like this as opposed to a function that is a straight line so an example of an activation function is something like a sigmoid function now a sigmoid function what it does is it'll map any value you give it in between the value of negative one and one so for example when we create this network our output might be like the number seven now this number seven well it is closer to one than it is to zero so we might deem that a correct answer or we might say that this is actually way off because it's way above one right but what we want to do essentially is in our output layer we only want our values to be within a certain range we want them to be in this case between zero and one or maybe we want them to be between negative one and one i'm saying like how close we are to zero making that decision how close we are to one something like that right so what this sigmoid activation function does it's a nonlinear function and it takes any value and essentially the closer that value is to infinity the closer the output is to one and the closer that value is to negative infinity the closer that output is to negative one so what it does is it adds a degree of complexity to our network now if you don't if you're not a high level like math student or you only know like very basic high school math this might not really make sense to you but essentially the degree of something right is honestly how complex it can get if you have like a degree 9 function then what you could do is you can have some crazy kind of curve and stuff going on especially in multiple dimensions that will just make things like much more complex so for example if you have like a degree nine function you can have curves that are going like like this uh like all around here that are mapping your different values and if you only have a linear function well you can only have a straight line which limits your degree of complexity by a significant amount now what these activation functions also do is they shrink down your data so that it is not as large so for example right like say we're working with data that is like hundreds of thousands of like characters long or digits we'd want to shrink that into like normalize that data so that it's easier to actually work with so let me give you a more practical example of how to use the activation function i talked about what sigmoid does what we would do is we would take this weighted sum so we did the sum of w i v i plus b i right and we would apply an activation function to this so we would say maybe our activation function is f x and we would say f of this and this gives us some value which is now going to be our output neuron and the reason we do that again is so that when we are adjusting our weights and biases and we add that activation function and now we can have a way more complex function as opposed to just having the kind of linear regression straight line which is what we've i've talked about in my other machine learning courses so if this is kind of going a little bit over your head it may be my lack of explaining it i'd love to hear in the comments below how you think of this explanation but essentially that's what the activation function does now another activation function that is very popular and is actually used way more than sigmoid nowadays is known as rectified linear unit and what this does is it let me draw it in red actually so we can see it better is it takes all the values that are negative and automatically puts them to zero and takes all of the values that are positive and just makes them more positive essentially or like to some level positive right and what this again is going to do is it's a nonlinear function so it's going to enhance the complexity of our model and just make our data points in between the range 0 and positive infinity which is better than having between negative infinity and positive infinity for when we're calculating uh error all right last thing to talk about for neural networks in this video i'm trying to kind of get everything like briefly into one long video is a loss function so this is again going to help us understand how these weights and these biases are actually adjusted so we know that they're adjusted and we know that what we do is we look at the output and we compare it to what the output should be from our test data and then we say okay let's adjust the weights and the biases accordingly but how do we adjust that and how do we know how far off we are how much to tune by if an adjustment even needs to be made well we use what's known as a loss function so a loss function essentially is a way of calculating error now there's a ton of different loss functions some of them are like mean squared error that's the name of one of them i think one is like um i can't even remember the name of this one but there's a bunch of very popular ones if you know some leave them in the comments love to hear all the different ones but anyways what the loss function will do is tell you how wrong your answer is because like let's think about this right if you get an answer of let's say maybe our output is like 0.79 and the actual answer was one well that's pretty close like that's pretty close to one but right now all we're going to get is the fact that we were 0.21 off okay so 0.21 off so adjust the weight to certain degree based on 0.21 but the thing is what if we get like 0.85 well is this like this is significantly better than 0.79 but this is only going to say that we were better by what is this 0.15 so we're still going to do a significant amount adjusting to the weights and the biases so what we need to do is we need to apply a loss function to this that will give us a better kind of degree of like how wrong or how right we were now these loss functions are again not linear loss functions which means that we're going to add a higher degree of complexity to our model which will allow us to create way more complex models and neural networks that can solve better problems i don't really want to talk about loss functions too much because i'm definitely no expert on how they work but essentially what you do is you're comparing the output to the what the output should be so like whatever the model generated based what it should be and then you're going to get some value and based on that value you are going to adjust the biases and the weights accordingly the reason we use the loss function again is because we want a higher degree of complexity they're nonlinear and you know if you get 0 if you're 99 like say you're 0.1 away from the correct answer we probably want to adjust the weights very very little but if you're like way off the answer your two whole points maybe our answer is negative one we want it to be one well we want to adjust the model like crazy right because that model was horribly wrong it wasn't even close so we would adjust it way more than just like two points of adjustment right we'd adjust it based on whatever that loss function gave to us so anyways this has kind of been my explanation of a neural network i want a very i want to state right here for everyone that i am no pro on neural networks this is my understanding there might be some stuff that's a little bit flawed or some areas that i skipped over and quickly actually because i know some people probably gonna say this when you're creating neural networks as well you have another thing that is called hidden layers so right now we've only been using two layers but in most neural networks what you have is a ton of different input neurons that connect to what's known as a hidden layer or multiple hidden layers of neurons so let's say we have like an architecture maybe that looks something like this so all these connections and then these ones connect to this and what this allows you to do is have way more complex models that can solve way more difficult problems because you can generate different combinations of inputs and hidden what is known as hidden layered neurons to solve your problem and have more weights and more biases to adjust which means you can on average be more accurate um to produce certain models so you can have crazy neural networks that look something like this but with way more neurons and way more layers and all this kind of stuff i just wanted to show a very basic network today because i didn't want to go in and talk about like a ton of stuff especially because i know a lot of people that watch my videos are not pro math guys are just trying to get a basic understanding and be able to implement some of this stuff now in today's video what we're going to be doing is actually getting our hands dirty and working with a bit of code and loading in our first data set so we're not actually going to do anything with the model right now we're going to do that in the next video this video is going to be dedicated to understanding data the importance of data how we can scale that data look at it and understand how that's going to affect our model when training the most important part of machine learning at least in my opinion is the data and it's also one of the hardest things to actually get done correctly training the model and testing the model and using it is actually very easy and you guys will see that as we go through but getting the right information to our model and having it in the correct form is something that is way more challenging than it may seem with these initial data sets that we're going to work with things are going to be very easy because the data sets are going to be given to us but when we move on into future videos to using our own data we're going to have to preprocess it we're going to put it in its correct form we're going to have to send it into an array we're going to have to make sure that the data makes sense so we're not adding things that shouldn't be there or we're not omitting things that need to be there so anyways i'm just going to quickly say here that i am kind of working off of this tensorflow 2.0 tutorial that is on tensorflow's website now i'm kind of gonna stray from it quite a bit to be honest but i'm just using the data sets that they have and a little bit of the code that they have here because it's a very nice introduction to machine learning and neural networks but there's a lot of stuff in here that they don't talk about and it's not very indepth so that's what i'm kind of going to be adding and the reason why maybe you'd want to watch my version of this as opposed to just reading this off the website because if you have no experience with neural networks it is kind of confusing some of the stuff they do here and they don't really talk about why they use certain things or whatnot so anyways the data set we're going to be working with today is it's known as the fashion mnist dataset so you may have heard of the old dataset which is image image classification but it was like digits so like you had digits from zero to nine and the neural network classified digits this one's a very similar principle except we're going to be doing it with like tshirts and pants and um what do you call like sandals and all that so these are kind of some examples of what the images look like and we'll be showing them as well in uh in the code so that's enough of that i just felt like i should tell you guys that the first thing that we're going to be doing before we can actually start working with tensorflow is we obviously need to install it now actually maybe i'll grab the install command here so i don't have to copy it but this is the install command for tensorflow 2.0 so i'm just going to copy it here link will be in the description as well as on my website and you can see pink pip install hyphen q tensorflow equals equals 2.0 0.0 hyphen alpha 0. now i already have this installed but i'm going to go ahead and hit enter anyways and the hyphen q i believe just means don't give any output when you're installing so if this runs and you don't see any output whatsoever then you have successfully installed tensorflow 2.0 now i ran into an issue where i couldn't install it because i had a previous version of numpy installed in my system so if for some reason this doesn't work and there's something with numpy i would just pip uninstall numpy and reinstall so do pip uninstall numpy like that i'm obviously not going to run that but if you did that and then you tried to reinstall tensorflow 2.0 that should work for you and it should actually install its own version of the most updated version of numpy now another thing we're going to install here is going to be matplotlib now matplotlib is a nice library for just graphing and showing images and different information that we'll use a lot through this series so let's install that i already have it installed but go ahead and do that and then finally we will install pandas which we may be using in later videos uh in the series so i figured we might as well install it now so pip install pandas and once you've done that you should be ready to actually go here and start getting our data loaded in and looking at the data so i'm just going to be working in subline text and executing my python files from the command line just because this is something that will work for everyone no matter what but feel free to work in idle feel for you to work in pycharm as long as you understand how to set up your environment so that you have the necessary packages like tensorflow and all that uh then you should be good to go so let's start by importing tensorflow so import tensorflow as tf like that i don't know why it always short forms when i try to do this but anyways we're going to import uh or actually sorry from tensorflow we'll import keras now keras is an api for tensorflow which essentially just allows us to write less code uh it does a lot of stuff for us like you'll see when we set up the model we use keras and it'll be really nice and simple and just like a high level api and that's the way that they describe it that makes things a lot easier for people like us that aren't going to be defining our own tensors and writing our own code from scratch essentially now another thing we need to import is numpy so we're going to say import if i could get this here import numpy as np and finally we will import uh matplotlib so mat plot lib in this case dot pi plot as plt and this again is just going to allow us to graph some things here all right so now what we're going to do is we're actually going to get our data set loaded in so the way that we can load in our data set is using keras so to do this i'm just going to say data equals in this case care as dot datasets dot fashion underscore mnist uh and this is just the name of the data set there's a bunch of other data sets inside of keras that we will be using in the future now whenever we have data it's very important that we split our data into testing and training data now you may have heard this me talk about this in the previous machine learning tutorials i did but essentially what you want to do with any kind of machine learning algorithm especially a neural network is you don't want to pass all of your data into the network when you train it you want to pass about 90 80 percent of your data to the network to train it and then you want to test the network for accuracy and making sure that it works properly on the rest of your data that it hasn't seen yet now the reason you'd want to do this and a lot of people would say why don't i just give all my data to the network it'll make it better not necessarily and that's because if you test your data on if you test your network on data it's already seen then you can't be sure that it's not just simply memorizing the data it's seen right for example if you show me five images um and then like you tell me the classes of all of them and then you show me that the same image again you say what's the class and i get it right well did i get it right because i figured out how to analyze the images properly or because i'd already seen it and i knew what it was right i just memorized what it was so that's something we want to try to avoid with our models so whenever we have our data we're going to split it up into testing and training data and that's what we're going to do right here so to do this i'm going to say train in this case train underscore images and train underscore labels comba in this case test underscore images comma test underscore labels and then we're going to say this is equal to data dot get underscore data so not get a load underscore down now the reason we can do this is just because this load data method is going to return information in a way where we can kind of split it up like this in most cases when you're writing your own models for your own data you're going to have to write your own arrays and for loops and load in data and do all this fancy stuff but keras makes it nice and easy for us just by allowing us to write this line here which will get us our training and testing data in the four kind of variables that we need so quickly let me talk about what labels are now so for this specific data set there are 10 labels and that means each image that we have will have a specific label assigned to it now if i actually i'll show you by just printing it out if i print for example train underscore labels and let's just print like the zero with uh i guess the first training label so let me just run this file so python tutorial one you can see that we simply get the number nine now this is just what is represent like the label representation so obviously it's not giving us a string but let's say if i pick for example 6 and i hit enter here you can see that the label is 7. so the labels are between 0 and 9. so 10 labels in total now the thing is that's not very useful to us because we don't really know what label 0 is what label 9 is so what i'm going to do is create a list that will actually define what those labels are so i'm going to have to copy it from here because i actually don't remember the labels but you can see it says here what they are so for example label 0 is a tshirt label 1 is a trouser 9 is an ankle boot and you can see what they all are so we just need to define exactly this list here so class names so that we can simply take whatever value is returned to us from the model of what label it thinks it is and then just throw that as an index to this list so we can get what label it is all right sweet so that is um how we're getting the data now so now i want to show you what some of these images look like and talk about the architecture of the neural network we might use uh in the next video so i'm going to use pi plot just to show you some of these images and explain kind of the input and the output and all of that so if if you want to show an image using matplotlib you can do this by just doing plt dot im show and then in here simply putting the image so for example if i do train not labels images and let's say we do the seventh image and then i do plt.show if i run this now you guys will see what this image is so let's run this and you can see that we get uh this is actually i believe like a pullover or a hoodie now i know it looks weird and you've got all this like green and purple that's just because of the way that kind of matplotlib shows these images if you want to see it properly what you do is i believe you do cmap equals in this case uh plt.c i think it's like cm.binary or something i gotta have a look here because i forget uh yeah cm.binary so if we do this and now we decide to display the image it should look a little bit better let's see here uh and there you go we can see now we're actually getting this like black and white kind of image now this is great and all but let me show you actually what our image looks like so like how was i just able to show like how was i just able to do this image well the reason i'm able to do that is because all of our images are actually arrays of 28 by 28 pixels so let me print one out for you here so if i do train underscore images let's do seven the same example here and print that to the screen i'll show you what the data actually looks like give it a second and there we go so you can see this is obviously what our data looks like it's just a bunch of lists so one list for each row and it just has pixel values and these pixel values are simply representative of i believe like how much i don't actually know the scale that they're on but uh i think it's like an rgb value but in grayscale right so for example we have like 0 to 255 where 255 is black and 0 is white and i'm pretty sure that's how getting the information in someone can correct me if i'm wrong but i'm almost certain that that's how this actually works so this is gray null but this is these are large numbers and remember i was saying before in the previous video that it's typically a good idea to shrink our data down so that it's with within a certain range that is a bit smaller so in this case what i'm actually going to do is i'm going to modify this information a little bit so that we only have each value out of 1. so we instead of having a 255 we have it out of 1. so the way to do that is to divide every single pixel value by 255. now because these train images are actually stored in what's known as a numpy array we can simply just divide it by 255 to achieve that so we'll say train images equals train images divided by 255 and we'll do the same thing here with our test images as well now obviously we don't have to modify the labels as well also because they're just between zero and nine and that's how the labels work but for images we're going to divide those values so that it's a bit nicer so now let me show you what it looks like so if i go python tutorial 1.5 pi and now you can see that we're getting these decimal values and that our shirt looks well the same but exactly like we've just shrunk down our data so it's going to be easier to work with in the future with our model now that's about it i think that i'm going to show you guys in terms of this data now we have our data loaded in and we're pretty much ready to go in terms of making a model now if you have any questions about the data please don't hesitate to leave a comment down below but essentially again the way it works is we're going to have 28 by 28 pixel images and they're going to come in as an array just as i've showed you here so these are all the values that we're going to have we're going to pass that to our model and then our model is going to spit out what class it thinks it is and those classes are going to be between 0 and 9. obviously 0 is going to represent tshirt where 9 is going to represent ankle boot and we will deal with that all in the next video now in today's video we're actually going to be working with the neural network so we're going to be setting up a model we're going to be training that model we're going to be testing that model to see how well it performed we will also use it to predict on individual images and all of that fun stuff so without further ado let's get started now the first thing that i want to do before we really get into actually writing any code is talk about the architecture of the neural network we're going to create now i always found in tutorials that i watched they never really explained exactly what the layers were doing what they looked like and why we chose such layers and that's what i'm hoping to give to you guys right now so if you remember before we know now that our images they come in essentially as like 28 by 28 pixels and the way that we have them is we have an array and we have another array inside it's like a twodimensional array and it has pixel values so maybe it's like 0.1 0.3 which is the grayscale value and this goes and there's times 28 in each row of these these pixels now there's 28 rows obviously because well 28 by 28 pixels so in here again we have the same thing more pixel values and we go down 28 times right and that's what we have and that's what our array looks like now that's what our input data is that's fine but this isn't really going to work well for our neural network what are we going to do we're going to have one neuron and we're just going to pass this whole thing to it i don't think so that's not going to work very well so what we need to actually do before we can even like start talking about the neural network is figure out a way that we can change this information into a way that we can give it to the neural network so what i'm actually going to do and what i mean most people do is they they do what's called flatten the data so actually maybe we'll go i can't even go back once i clear it but flattening the data essentially is taking any like interior list so let's say we have like lists like this and just like squishing them all together so rather than so let's say this is like one two three if we were to flatten this what we would do is well we would remove all of these interior arrays or list or whatever it is so we would just end up getting data it looks like one two three and this actually turns out to work just fine for us so in this instance we only had like one element in each array but when we're dealing with 28 elements in each sorry list listed array they're interchangeable just in case i keep saying those uh what we'll essentially have is we'll flatten the data so we get a list of length 784 and i believe that is because well i mean i know this is because 28 times 28 equals 784 so when we flatten that data so 28 rows of 28 pixels then we end up getting 784 pixels just one after each other and that's what we're going to feed in as the input to our neural network so that means that our initial input layer is going to look something like this we're going to have a bunch of neurons and they're going to go all the way down so we're going to have 784 neurons so let's say this is 784 i know you could probably hardly read that but you get the point and this is our input layer now before we even talk about any kind of hidden layers let's talk about our output layer so what is our output well our output is going to be a number between 0 and 9. ideally that's what we want so what we're actually going to do for our output layer is rather than just having one neuron that we used kind of in the last the two videos ago as an example is we're actually going to have 10 neurons each one representing one of these different classes right so we have 0 to 9 so obviously 10 neurons or 10 classes so let's have 10 neurons so 1 2 3 4 5 6 7 8 9 10. now what's going to happen with these neurons is each one of them is going to have a value and that value is going to represent how much the network thinks that it is each neuron so for example say we're classifying the image that looks like a tshirt uh or maybe like a pair of pants those are pretty easy to draw so let's say this is the image we're given a little pair of pants what's gonna happen is let's say pants is like this one like this is the one it actually should be all of these will be lit up a certain amount so essentially maybe we'll say like we think it's 0.05 percent this we have like a degree of certainty that it's 10 percent this one and then it is like we think it's 75 percent pants so what we'll do when we are looking at this output layer is essentially we'll just find whatever one is the greatest so whatever probability is the greatest and then say that's the one that the network predicts is the class of the given object right so when we're training the network what we'll do essentially is we'll say okay well we're giving the pants so we know that this one should be one right this should be a hundred percent it should be one uh that's what it should be and all these other ones should be zero right because it should be a zero percent chance it's anything else because we know that it is pants and then the network will look at all this and adjust all the weights and biases accordingly so that we get it so that it lights this one up directly as one at least that's our goal right so uh once we do that so now we've talked about the input layer and the output layer now it's time to talk about our hidden layers so we could technically train a network that would just be two layers right and we just have all these inputs that go to some kind of outputs but that wouldn't really do much for us because essentially that would just mean we're just going to look at all the pixels and based on that configuration of pixels we'll point to you know these output layers and that means we're only going to have which i know it sounds only 784 times 10 weights and biases so 784 times 10 which means that we're only going to have 7840 weights right weights and biases things to adjust so what we're actually going to do is we're going to add a hidden layer inside of here now you can kind of arbitrarily arbitrarily pick how many neurons you're going to have in your hidden layer it's a good idea to kind of go off based on percentages from your input layer but what we're going to have is we're going to have a hidden layer and in this case this hidden layer is going to have 128 neurons so we'll say this is 128 and this is known as our hidden layer so what will happen now is we're going to have our inputs connecting to the hidden layer so fully connected and then the hidden layer will be connected to all of our output neurons which will allow for much more complexity of our network because we're going to have a ton more biases and a ton more weights connecting to this middle layer which maybe we'll be able to figure out some patterns like maybe it'll look for like a straight line that looks like a pant sleeve or looks like an arm sleeve maybe it'll look for concentration of a certain area in the picture right and that's what we're hoping that our hidden layer will maybe be able to do for us maybe pick on pick up on some kind of patterns and then maybe with these combination of patterns we can pick out what specific image it actually is now we don't really know what the hidden network or hidden layer is going to do we just kind of have some hopes for it and by picking 128 neurons we're saying okay we're going to allow this hidden layer to kind of figure its own way out and figure out some way of analyzing this image and then that's essentially what we're going to do so if you have any questions about that please do not hesitate to ask but the hidden layers are pretty arbitrary sorry i just dropped my pen which means that you know you can kind of experiment with them kind of tweak with them there's some that are known to be to do well but typically when you're picking a hidden layer you pick one and you typically go at like maybe 15 20 of the input size but again it really depends on the application that you're you're using so let's now actually just start working with our data and creating a model so if we want to create a model the first thing we that we need to do is define the architecture or the layers for our model and that's what we've just done so i'm going to type it out fairly quickly here and again you guys will see how this works so i'm going to say model equals in this case care keras.sequential i believe that's how you spell it and then what we're going to do is inside here put a list and we're going to start defining our different layers so we're going to say keras dot layers and our first layer is going to be an input layer but it's going to be a flattened input layer and the input underscore shape is going to be equal to 28 by 28 so remember i talked about that initially what we need to do is well we need to flatten our data so that it is uh passable to all those different neurons right so essentially oh i gotta spell shape correctly shape correctly so essentially whenever you're passing in information that's in like a 2d or 3d array you need to flatten that information so that you're going to be able to pass it to an individual neuron as opposed to like sending a whole list into one neuron right now the next layer that we're going to have is going to be what's known as a dense layer now a dense layer essentially just means a fully connected layer which means that what we've showed so far which is only fully connected neural networks that's what we're going to have so each node or each neuron is connected to every other neuron in the next network so i'm going to say layers.dense and in this case we're going to give it 128 neurons that's what we've talked about and we're going to set the activation function which we've talked about before as well to be rectify linear unit now again this activation function is somewhat arbitrary in the fact that you can pick different ones but rectifier linear unit is a very fast activation function and it works well for a variety of applications and that is why we are picking that now the next layer is going to be another dense layer which means essentially another fully connected layer sorry and we're going to have 10 neurons this is going to be our output layer and we're going to have an activation of softmax now what softmax does is exactly what i explained when showing you that kind of architecture picture it will pick values for each neuron so that all of those values add up to one so essentially it is like the probability of the network uh thinking it's a certain value so it's like i believe that it's eighty percent this uh two percent this five percent this but all of the neurons there those values will add up to one and that's what the soft max soft max function does so that actually means that we can look at the last layer and we can see the uh probability or what the network thinks for each given class and say maybe those are two classes that are like 45 each we can maybe tweak the output of the network to say like i am not sure rather than predicting a specific uh value right all right so now what we're going to do is we're going to just set up some parameters for our model so we're going to say model.compile and in this case we're going to use an optimizer of atom now i'm not really going to talk about the optimizer uh atom is typically like pretty standard especially for something like this we're going to use the loss function of sparse and in this case underscore categorical i believe i spelt that correctly and then cross entropy now if you're interested in what these do and how they work in terms like the math kind of side of them just look them up there's they're very famous and popular and they're again are somewhat arbitrary in terms of how you pick them now when i do metrics i'm going to say metrics equals accuracy and again this is just going to define what uh we're looking at when we're testing the model in this case we care about the accuracy or how low we can get this loss function to be so yeah you guys can look these up there's tons of different loss functions some of them have different applications and typically when you're making a neural network you're you'll mess around with different loss functions different optimizers and in some cases different metrics so now it is actually time to train our model so to train our model what we're going to do is model.fit and when we fit it all we're going to do is give it our train images and our train labels now we're gonna set the amount of epochs so now it's time to talk about epochs now epochs are actually fairly straightforward you've probably heard the word epoch before but essentially it means how many times the model is going to see this information so what an epoch is going to do is it's going to kind of randomly pick images and labels obviously corresponding to each other and it's going to feed that through the neural network so how many epochs you decide is how many times you're going to see the same image so the reason we do this is because the order in which images come in will influence how parameters and things are tweaked with the network maybe seeing like 10 images that are pants is going to tweak it differently than if it sees like a few better pants and a few that are a shirt and some that are sandals so this is a very simple explanation of how the epochs work but essentially it just is giving um the same images in a different order and then maybe if it got one image wrong it's going to see it again and be able to tweak and it's just a way to increase hopefully the accuracy of our model that being said giving more epochs does not always necessarily increase the accuracy of your model it's something that you kind of have to play with and anyone that does any machine learning or neural networks will tell you that they can't really like they don't know the exact number epochs they have to play with it and tweak it and see what gives them the best accuracy so anyways now it is time to actually well we can run this but let's first get some kind of output here so i'm going to actually evaluate this model directly after we run it so that we can see how it works on our test data so right now what this is doing is actually just training the model on our training data which means we're tweaking all the weights and biases um we're applying all those activation functions and we're defining like a main function for the model but if we actually want to see how this works we can't really just test it on the training images and labels for the same reason i talked about before so we have to test it on the test images and the test labels and essentially see how many it gets correct so the way we do this is we're going to say test underscore loss test underscore ac which stands for accuracy equals model dot evaluate is that how you spell it maybe and then we're going to do test images test underscore labels and i believe that is the last parameter yes it is so now if we want to see the accuracy of our model we can simply print out test underscore acc and we'll just say like tested acc just so we know because there is going to be some other metrics that are going to be printing out to us when we run this all right so now that we've done that let's actually run our file and see how this works so this is it this whole part here is all we actually need to do to create a neural network and do a model now actually let me just quickly say that this keras.sequential what this does is it means a like a sequence of layers so you're just defining them in order where you say the first layer obviously is going to be your input layer we're flattening the data then we're adding two dense layers which are fully connected to the input layer as well and that's what our model looks like and this is typically how you go about creating a neural network all right so let's run this now and see what we get so this will take a second or two to run um just because obviously there is well we have 60 000 images in this data set so you know it's got to run through them it's doing all the epochs and you can see that we're getting metrics here on our accuracy and our loss now our test accuracy was 87 so you can see that it's actually slightly lower than um what do you call it like the accuracy here oh it's the exact same oh it actually auto tested on some data sets but anyways so essentially that is um how this works you can see that the first five epochs which are these ones here uh ran and they increased typically with each epoch now again we could try like 10 epochs 20 epochs and see what it does but there is a point where the more epochs you do the actual like the less reliable your model becomes uh and you can see that our accuracy was started at 88.9 essentially and that was on like that's what it said our model accuracy was when we were training the model but then once we actually tested it which are these two lines here uh it was lower than the the tested or like the trained accuracy which shows you that you obviously have to be testing on different images because when we tested it here it said well it was 89 but then here we only got 87 right so let's do a quick uh tweak here and just see what we get maybe if we add like 10 epochs i don't think this will take a crazy long amount of time so we'll run this and see maybe if it makes a massive difference or if it starts leveling out or it starts going lower or whatnot uh so let me let this run here for a second and obviously you can see the tweaked accuracy as we continue to go i'm interested to see here if we're going to increase by much or if it's just kind of going to stay at the same level all right so we're hitting about 90 percent and let's see here 91. okay so uh we got up to 91 but you can see that it was kind of diminishing returns as soon as we ended up getting to about seven epochs even yeah even like eight epochs after this we only increased by marginal amount and our accuracy on the testing data was slightly better but again for the amount of epochs five extra epochs it did not give us a five times better result right so it's something you gotta play with and see now in today's video what we're going to be doing is just simply using our model to actually predict information on specific images and see how you can actually use the model i find a lot of tutorial series don't show you how to actually practically use the model but what's the point of creating a model if you can't use it now quickly before i get too far into the video i would just like to show you guys something that i'm super excited to announce because i've been waiting for them to come for a long time and it is the official tech with tim mugs so you guys can see them here i just want to quickly show them to you guys if you'd like to support the channel and get an awesome looking mug i actually really like them then you guys can purchase them just by i believe underneath the video it shows like the teespring link um but yeah they're awesome they look really good and the reason i've been holding out on showing them to you guys is because i wanted to wait till i received mine to make sure that it was up to quality and that it looked good enough uh to sell to you guys essentially so if you'd like to support the channel um you can get one of those if not that's fine but if you do decide to buy one please send me like a dm on twitter instagram or something and let me know so i can say thank you to you guys so anyways let's get into the video um so what i'm gonna do actually is i'm gonna uh we need to continually train the model every time we run the program which i know seems like a pain but unless we want to save the model which i guess i could actually show in this video later as well we just have to train it and then we can use it directly after so after we've you know tested this we don't need to do this evaluate anymore we are trained the model we can use it to use it we actually just need to use a method called predict but i'm going to talk about kind of how this works because it is a little finicky we're not even just finicky but just not intuitive so essentially when you want to make a prediction using the model i'm going to set up just a variable prediction here you simply use model.predict and then you pass it a list now what you would think you would do is just pass it like the input right so in this case we just pass it some input that's in the form 2828 and it would predict but that's not actually how it works when you want to make a prediction what you need to do is put whatever your input shape is inside of a list or actually well you can do it inside of the list but you can also do it inside an mp array as well like a numpy array and the reason you need to do that is because what predict does is it gives you a group of predictions so it's expecting you to pass in a bunch of different things and it predicts all of them using the model so for example if i want to do the predictions on all of my test images to see what they are i can do prediction equals model.predict test images and if i print out like prediction uh you guys will see what this looks like so let's run this here and see what we get so obviously we have to train the model each time which is a little bit annoying but we can save it later on and obviously this one runs pretty quickly so it's not a huge deal all right so there we go so now you can see this is actually what our predictions look like now this is a really weird kind of like looking prediction thing we're getting a bunch of different lists now that's because right our output layer is 10 neurons so we're actually getting an output of 10 different values and these different values are representing how much the model thinks that each picture is a certain class right so you can see we're getting like 2.6 to the e to the negative zero six which means that obviously a very small number so it doesn't think whatsoever that it's that and then i'm trying to find if we can see ones that aren't like to the e but apparently it's we didn't really get lucky enough with it showing because i just cut some of them off here but if i print out let's say like prediction zero and i guess we're gonna have to run this again i probably should have thought of that then you guys will see exactly what the prediction list looks like and i'm gonna show you how we can actually interpret this to determine what class it is because this means nothing to us we want to know is it a sandal is it a shoe is it a shirt like what is it right so there you go so this is what the list looks like so if we look through the list here we can see these are all the different probabilities that our our network is predicting so what we're actually going to do essentially is we're going to take whatever the highest number is there we're going to say that is the predicted value so to do that what we do is we say np dot arg max okay and we just put it around this list now what this does is it just gets the largest value and finds like the index of that so in this case since we have 10 neurons the first one is representing obviously tshirt the last one is representing ankle boot it'll find whatever neuron is the largest value and give us the index of that neuron so if it's like the third neuron then it's going to give us a pullover right and and that's how that works so if we want to see the actual like name though rather than just the index then what we need to do is just take this value and pass it into class names so we'll say class underscore names and then we'll index whatever the value is that this np dot arg max prediction zero gives us right so let's run this and see what we get now all right so there we go so we can see that now we're actually getting ankle boot as our prediction which makes a lot more sense for us right rather than just giving us like that prediction array or whatever it was okay so that's great but the thing is how do we how can we validate this is actually working well what we need to do now or not what we need to do but what we should do now is show the input and then show what the predicted value is and that way we as the humans which know obviously which is which can validate that so what i'm going to do actually just set up a very basic for loop and what this for loop is going to do is loop through a few different images in our test images and show them on the screen and then also show the prediction so show what they actually are and then show the prediction as well so to do this i'm just going to say 4 i guess in this case i in range 5 and what we'll do is i'm going to say plt dot grid i'm just going to set up a very basic like plot to show the image i'm going to image show our test underscore images i right i'm going to do this cmap thing so i'm going to say cmap equals in this case plt.cm.binary which is just going to give us like the gray scale and then i'm going to say plt dot x label which just means underneath and i'm going to say is equal to actual and in this case i'm going to say plus and what do we want to do we need to get the actual label of our test image which would be in test underscore labels i and then what i'm going to do is add a header and say this is what the model predicted so to do this i'm going to say plt dot i believe it's oh sorry not header so that title and the title will simply be prediction plus in this case we're going to say prediction and then i now the reason we can do this or sorry we're going to have to literally copy this this whole arg max thing and we'll put that here except instead of zero we're going to put i and just that way it will show all of the different images right so now what i'm going to do is for each loop here i'm going to plt.show which means i'm going to show those images so we can see exactly what they look like so quick recap in case i kind of skimmed over some stuff all we're doing is setting up a way to see the image as well as what it actually is versus what the model predicted so we as the humans can kind of validate this is actually working and we see okay this is what the image and the input is and this is what the output was from the model so let's run this and wait for it to train i'll fast forward through this and then we will show all the images okay so quick fix here um i just ran this and i got an error we need to do class names and then test labels i and that's obviously because the test labels are going to have like the index of all of these so i can't just put like the number value i have to put the class names so that we get the correct thing anyways i hope that makes sense you guys let's run this now you can see that was the error i ran into again fast forward and then i will be back all right so i am back now this is a little bit butchered in how i'm actually showing it but you can see that it's saying the prediction for this was the ankle boot and it actually is an ankle boot now if i close this it'll just show four more because that's the way i've set it up so now you can see that prediction pullover it actually was a pullover all right we see we get prediction trouser it actually was a trouser and prediction trouser actual trouser uh prediction shirt actual shirt and obviously if you want to see more you could keep looping through all of these and doing that now say you just want to predict on one image well what you could do for example is uh and this is kind of a weird way what i'm about to do but you'll see let's say we wanted to just predict like what the seventh image was well then what i would do is just say test images 7 which is going to give us that 28 by 28 array and then i would just put it inside of a list so that that way it gets it's given the way that it's supposed to look but that also means that our prediction list right we're going to get is equal to this it's going to look like prediction and then it's going to have this and then inside it's going to have all those different values so it's going to have like 0.001 0.9 but it's going to be a list inside of a list so that's just something to keep in mind when you're working with these predictions because that is really the only way to do it and that this is exactly what tensorflow recommends on their website as well if you're just predicting for one item just put it inside of a list so that it's going to work fine so anyways that has kind of been it on using the model to predict stuff in future videos we'll get into a little bit more advanced stuff this was a very easy classification problem just really meant to give you an introduction and personally i think if you never worked with any machine learning stuff this is pretty cool that in a few minutes of just kind of writing a little bit of code whether you understand it or not you can create a simple model that can classify fashion items like a shirt a tshirt and i don't know that's pretty cool to me and in future videos obviously we're going to be doing a lot cooler stuff it's going to be a little bit more advanced but hopefully you guys can stick with it i'd love to know what you guys think of the series so far so please leave a comment down below it helps me to kind of tweak my lessons and all that as we go forward if you guys enjoyed the video please leave a like and subscribe and i will see you again now in today's video what we're going to be doing is talking about text classification uh with tensorflow 2.0 now what i'm going to be doing just to be fully transparent with you guys here is following along with the actual official tutorials on the tensorflow 2.0 tutorial uh now i find that these are actually the best in terms of like kind of a structure to start with to understand very basic neural networks for some pretty simple tasks i would say and then we're gonna stray away from those we're gonna start using our own data our own networks our own architecture and we'll start talking about kind of some of the issues you have when you actually start applying these to real data so so far you guys have noticed and i've seen some comments on it already that the data is really easy to load in and even preprocessing it like in the last one we just divided everything by 255 like that's really simple in the real world your data is definitely not that nice and there's a lot of stuff that you need to play with and modify to make it actually usable so anyways we'll follow along with this one for today and essentially the way that it works is we're going to have movie reviews and we're just going to classify them either as either positive or negative now what we'll do is we'll just look at some of the movie reviews and then we'll talk about the data we'll talk about the architecture using stuff to predict some issues we might run into and all of that now i don't know how many video parts this is going to be i'm going to try to record it all at once and just split it up based on how long it takes but with that being said enough talking let's get started so what we're going to do obviously is start in our file here and again this is going to be really nice because we can just steal kind of the data from keras what we'll start by doing is just importing tensorflow as tf we're going to say from tensorflow import keras and then we're going to say import numpy as np now before i start i ran into a quick uh issue when i was actually trying to do this just following along with the official tutorial and that was that the data that i want to grab here actually doesn't work with the current version of numpy that comes with tensorflow it's on their github as an issue but anyways to fix this what we need to do is install the previous version of numpy so to do this uh what i'm actually going to do is just say pip um i think i do like pip numpy version or something because i want to see what version it is uh incorrect command let's say pip version number i want to find what version it is and then just go down uh to that version okay so i found the version of numpy uh what we're going to do now is actually just install the correct version of numpy to make it work for this tutorial now this should be fine for everything going forward and if you want to install the most recent version of numpy after doing this go feel free but to do this all i'm going to do is just say pip install and then numpy equals in this case 1.16.1 i believe the version we're using right now is 0.3 at least at the time recording this but just change it to this version and hopefully in the future they'll fix that issue so that we don't have to do this but anyways i'm going to install that um yeah you're going to have to add two equal signs and i already have this installed so that should just not do anything but you guys just make sure you do that i'll leave the command in the description now after we do that what i'm going to do is just load in the data i'm going to say data equals in this case care as dot data sets dot i m what is it i am db now i believe this stands for like some something movie database i don't really know but anyways that's what the database is and we're going to do the same thing we did in the previous tutorial which is just split this into training and testing data so to do that i'm going to say train underscore data train underscore labels comma and then in this case we'll say test underscore data and then test underscore labels equals in this case data.load underscore data now we're just going to add one thing in here which is num underscore words equals in this case 10 000. now the reason i'm doing this is because this data set contains like a ton of different words and what we're going to actually do by saying num words equals ten thousand is only take the words that are the ten thousand most frequent which means we're going to leave out words that usually are only occurring like once or twice throughout the entire data set because we don't want to throw those into our model and have things like be more difficult than they have to be and just have data that's kind of irrelevant because we're going to be comparing obviously uh movie reviews and there's some words that are only in like one review we should probably just omit them because there's nothing to really compare them to in other data sets anyways i hope that kind of makes sense but that's not super important we're just going to do num words equals uh 10 000. it also shrinks our data a little bit which makes it a bit nicer now what we're going to do next is we're actually going to show how we can display this data now if i start by actually just showing you like the train underscore data and let's pick like the zero with one so i guess the first one and i print this out to the screen so i'll just go if i could get to pyth python and i guess in this case we'll have to do i probably should just type this to start uh tutorial 2 when this actually prints out probably going to take a second here just to download the data set you can see that what we have is actually just a bunch of numbers now this doesn't really look like a movie review to me does it well what this actually is is integer encoded uh words so essentially each of these integers point to a certain word and what we've done just to make it way easier for our model to actually classify these and work with these is we've given each word one integer so in this case maybe like the word the integer one stands for something the integer 14 stands for something and all we've done is just added those integers into a list that represents where these words are located in the movie review now this is nice for the computer but it's not very nice for us if we actually want to read these words so what we have to do is find the mappings for these words and then find some way to actually display this so that you know we can have a look at it now i'll be honest here i'm just going to take this from what they have on the tensorflow website on how to do this typically you would create your own mappings for words with your own dictionary and you just already have that information but fortunately for us tensorflow already does that so to do that i'm going to say word underscore index equals in this case imdb dot get underscore word underscore index like this now what this does is actually going to give us a dictionary that has those keys and those mappings so that what we can do is well figure out you know what these integers actually mean so when we want to print it out later we can have a look at them so what i'm going to say now is word underscore index equals in this case k colon and then we're going to say what do you call it v plus 3 for k v in word underscore index dot items so i might have been incorrect here this doesn't actually give us the dictionary this just gives us like tuples that have the string and the word in them i believe and then what we're doing here is we're going to say instead of c sorry this should be v my apologies is we're going to get just break that tuple up into k and v which stands for key and value and the key will be the word the value will be obviously the integer yes that's what it will be and we're going to say four word items in index we'll break that up and then we're just going to add a bunch of different keys into our data set now the reason we're gonna start at plus three is because we're gonna have actually one key or three keys that are gonna be like special characters for our word mapping and you guys will see how those work in a second so i'm gonna start by just saying word index and in this case i'm going to put in here uh pad we're going to talk about this in a second so don't worry if you guys are kind of like what are you doing right now i'm going to say word index and in this case start equals 1 and say word underscore index in this case i believe it's like u n k yeah that's correct we're gonna say u n k equals two now u n k just stands for unknown and i'm gonna explain all this in a second but it's easier just to type it out first and we're gonna say word index in this case inside this tag unused and we're going to say equals three so what i'm doing essentially is all of the words in our training and testing data set have um like keys and values associated with them starting at one so what i'm doing is i'm just going to add three to all of those values so that what i can actually do is assign my own kind of values that are gonna stand for padding start unknown and unused so that if we get values that are not valid we can just assign them to this essentially in the dictionary now what i'm going to use for padding you guys will see in just a second essentially it's just so we can make our all our movie sets the same length so we'll add this what's known as pad tag and we'll do that by adding zero into our actual movie review list so that we're gonna make each movie review the same length and the way we do that essentially is if they're not the same length so maybe one's 100 maybe one's 200 we want all them to be 200 the 100 length movie list will for what we'll do is we'll just add a bunch of padding to the end of it to make it length 200 and then obviously our model will hopefully be able to differentiate the fact that that is padding and then we don't care about the padding and we shouldn't even bother really like looking at that right all right so now what i'm going to do is add this kind of complicated line here uh just to i don't even know why they have this to be quite honest this is the way the tensorflow has decided to do their like word mappings but apparently you need to add this reverse underscore underscore word underscore index which is equal to dictionary and then here we're going to say value comma key for key comma value uh in word underscore index i believe that's correct and what this is going to do actually sorry not word index word index.items what this is gonna do is okay i understand now now that i've typed it out you just swap all the values in the keys so that right now we actually have a dictionary that has all of the like the keys first which is going to be the word and then the values where we actually want it the other way around so we have like the integer pointing to the word because we're going to have our data set that is going to contain just integers like we've seen here and we want these integers to be able to point to a word as opposed to the other way around so what we're doing is just reversing this with a reverse word index list just our dictionary sorry essentially that's what this is doing here all right now that we've done that the last step is just to add a function and what this function will do is actually decode essentially all of this training and testing data into human readable words so there's different ways to do this again i'm just going to take this right from the tensorflow website because this part's not super important and i'd rather just you know do it quickly than spend too much time on it so we're just going to say return blank string dot join in this case we're going to say reverse word index dot get in this case we're going to say i comma question mark now what this does essentially if you don't know how the get works is we're going to try to get index i which we're going to define in a second if we can't find a value for that then what we'll do is just put question mark and that's which is a default value which means we won't crash if we're having like a key uh error in our dictionary and we're going to say for in this case i in text i don't know why where i have text typed i think i might have messed something up here so one second here oh text array is the parameter my apologies so anyways that's what this is going to do is just going to return to us essentially all of the the keys that we want or the human readable words my apologies so now what we'll do is we'll simply just print out decode review and i'm just going to give it some test data so let's say test for example 0 and i guess we're going to do test underscore data it doesn't really matter if you do train or test data but let's just have a look at test data zero and see what that actually looks like so let's run that assuming i didn't make any mistakes we should actually get some valid output in just a second this usually takes a minute to run up imdb is not defined what did i type here i typed that as data my apologies so where we say imdb which is right here we just need to replace that with data in my other file i called it imdb so that's why i made a mistake there but let's run that again and hopefully now we will get some better looking output so let's wait for this and see dict object has no attribute items this needs to be items classic typos by tim one more time third time is a charm hopefully let's see and there we go so now we can see that we're actually getting all of this decoded into well this text now i'll allow you guys to read through it but you can see that we have these kind of keys that we've added so start which is one which will automatically be added at the beginning of all of our text and then we have these unks which stand for unknown character essentially and then we don't have any other keys in here but say for example we had like some padding we had added to this we would see those pad tags as well in here so that is essentially how that works if you'd like to look at some other reviews just mess around with kind of the values in the index here throw them into decode review and then we can actually see what they look like now something to note quickly is that our reviews are different lengths now i've talked about this already but let's just compare two reviews to really test that i am not just making this up so i'm going to say test underscore data why would i have a capital here test underscore data zero so the length of test and record data is zero and the length of let's try test underscore data one just to prove to you guys that these are actually different lengths which means there's something kind of fancy we're going to have to do with that padding tag which i was talking about there so let's go into text classification let's go cmd and then python in this case tutorial2.pi now i guess we're going to get that output again which is probably what's causing this to just take a second to run you can see that we have length 68 and we have length 260. now this is not going to work for our model and the reason this doesn't work is because we need to know what our inputs shape sorry and size is going to be just like i talked about before we define the input nodes or the input neurons and the output neurons so we have to determine how many input neurons there's going to be and how many output neurons there's going to be now if we're like we don't know how large our data is going to be and it's different for each what do you call its entry then that's an issue so we need to do something to fix that so what we're gonna do is we're gonna use this padding tag to essentially set a definite length for all of our data now we could go ahead and pick the longest review and say that we'll make all of the reviews that length but what i'm going to do is just pick an arbitrary number in this case we'll just do like 250 and say that that's the maximum amount of words we're going to allow in one review which means that if you have more than 250 words in your review we're just going to get rid of all those and if you don't have 256 words or 250 words or whatever it is we're just going to add these padding tags to the end of it until eventually we reach that value so the way to do this is again using these fancy tensorflow functions now if you don't like these um functions and like what these do for you and how they just kind of save you some time go ahead and try to write them yourself and if you want help on how to do that feel free to reach out to me on discord or in the comments or whatever but i personally just use them because it saves me quite quite a bit of time in terms of like typing out the functions and i already know how to do a lot of what these functions do so for me it doesn't really make sense to just retype them out when i can just use these kind of fancy tools so what we're going to say is we're going to redefine our training and testing data and what we're going to do is just trim that data so that it's only at or kind of normalize that data so it's at 250 words so to do that i'm going to say train underscore data equals in this case keras dot preprocessing um no idea if that's how you spell it we'll have to check that in a second dot sequence dot pad underscore sequence uh so preprocessing i think that's correct i guess we'll see and then in here we have to define a few different parameters so what we'll first do is we'll give that train underscore data we're going to say value equals which will be the pad value so what we add to the end of in this case our numpy array to pad it per se and in this case we'll just use this pad tag so we'll say literally word index pad so let's copy that and put that there we're going to say our padding equals in this case post which just means we're going to pad after as opposed to before we also could pad before but that doesn't really make too much sense for this and then what we'll say is max in this case len equals and then you pick your number that you want to make all of the values equal to now tensorflow did like 256. i'm just going to do 250 and see if this makes a difference in terms of our accuracy for the model and i'm literally just going to copy this and change these values now to test underscore data instead of train underscore data and this will do the same thing on our other data set oops didn't mean to do that so test underscore data like that so quick recap here because we are at 17 minutes now essentially what we've done is we've loaded in our data we've looked at our data we've created the word mappings essentially for our data so that we can actually figure out what all these integers mean we've created a little function here that will decode the mappings for us so we just pass it a word review that's integer encoded it decodes it and then it we can print that information out to the screen to have a look at it what we've just done now is we've done what's called preprocessing our data which means just making it into a form that our model can actually accept and that's consistent and that's what you're always going to want to do with any data that you have typically it's going to take you a bit more work than what we have because it's only two lines to preprocess our data because keras kind of does it for us but for the purpose of this example that's fine all right so now that we've done that it's actually time to define our model um now i'll show you quickly just to make sure that you know you guys believe me here that this is working in terms of preprocessing preprocessing our data so it's actually going to make things the same length so we'll say train underscore data test underscore data let me just print this out to the screen so python tutorial 2. uh again we're going to get these integer mappings but we'll get the length at the end as well and another error of course we need to add an s to these sequences again my apologies guys on that um classic typos here so anyways i had preprocess processing sequence we need sequences and now if i run this you can see that we have a length of 250 and 250 so we've kept that consistent now for some oh i'm printing i don't know why this is printing two oh it's because i'm printing it here and then i'm printing it here um but you guys get the idea in that we've now made them actually the same size so let me remove these print statements um all of them so we can stop printing train data zero up here as well and now let's start defining our model so i'll just say model down here is a little comment just to help us out so what i'm going to do now is similar to what i've done before except in the last one you might have noticed that the way i defined my model was uh here i'll show you in a second once i finish typing this so we did keras.sequential and then what we actually did was just had a list in here that had all the layers that's fine you can do that but in this case we're going to have a few more layers so what we're going to do actually is add these layers just by doing model dot add it's precisely the same thing as before except instead of adding them in this list we're just gonna do it using this method so now we're gonna say keras dot layers dot in this case embedding and i'll talk about what these layers do in a second so we're gonna do ten thousand sixteen and then we're just going to actually copy this um four times and just change these layers and the kind of uh parameters as well so now we're gonna say global average pooling 1d and then do that and then we're going to add a dense layer here and another dense layer and change these parameters so we'll say dense and we'll say in this case 16 and we'll say activation equals relu our rectify linear unit whatever you guys want to call it and then we'll do down here one and activation equals rectifier linear unit as well uh actually sorry not real really we're gonna do sigmoid my apologies so now we'll actually talk about the architecture of this model and how i came up with picking these layers and well what these layers are well what we want essentially is we want the final output to be whether the review is good or whether the review is bad i think i mentioned that um at the beginning of the video so what we're actually going to do is just have either that like we'll have one output neuron and that neuron should be either zero or one we're somewhere in between there to give us kind of a probability of like we think it's like twenty percent one eighty percent zero something along those lines now we can accomplish that by using sigmoid because what it will do again we've talked about the sigmoid function is it'll squish everything so whatever our value is in between zero and one which will give us a nice way to test if our model is actually working properly and to get it the value that we want hey guys so now it's time to talk about word embeddings and this embedding layer and then what the global average pooling 1d layer is doing now we already have an idea of what these dense layers are with these activation functions like relu and sigmoid but what we're actually going to do today or i guess just in this video is talk about the architecture of this network kind of how it works on a high level understanding and then in the next video what we'll do is actually get into training and using the network so what i'm going to do first is just start by talking about these first two layers and specifically what this embedding layer is because it's very important and then we will draw the whole network or the whole i guess network is the right word way to put it the whole architecture and talk about how it fits together and what it's actually doing so let's get started now the easiest way to kind of explain this is to use an example of two very similar sentences so i'm just going to say the first sentence is have a great day and the next sentence will be have a good day now i know my handwriting is horrible so just give me a break on that um it's also hard to kind of write with this tablet so that's my excuse but anyways these two sentences looking at them as human beings we can tell pretty quickly that they're very similar now yes great and good maybe one has more emphasis on having an amazing day whatever it is but they're very similar and they pretty well have the same meaning right maybe we know when we would use the sentence and kind of the context in which like these words great and good are used in day and day and all this right it just we understand what they are now the computer doesn't have that same understanding at least right off the bat when looking at these two sentences now in our case we've actually integer encoded all of our different values so what we end up having are all of our different words sorry is our sentences end up looking something like this so we're going to have this first word will represent a 0. a will be 1 great will be 2 and a will be 3 so then down here we'll have 0 1 in this case we're going to say good is 4 and day is 3 as well so this means if we integer and code these sentences we have some lists that look something like this now this one clearly is the first sentence and this one down here will be the second sentence now if we just look at this and we pretend that you know we don't even know what these words actually are all we can really tell is the fact that two is different from four now notice what i just said there two is different from four when in reality if we look at these two words we know that they're pretty similar yes they're different words yes they're different lengths whatever it is but we know that they have a similar meaning and the context in which they're used in this sentence is the same now our computer obviously doesn't know that because all it gets to see is this so what we want to do is try to get it to have an understanding of words that have similar meanings and to kind of group those together in a similar form or in a similar way because obviously in our application here of classifying movie reviews the types of words that are used and the context in which they are used really makes a massive difference to trying to classify that as either a positive or a negative review and if we look at great and good and we say that these are two completely different words well that's going to be a bit of an issue when we're trying to do some classification so this is where our embedding layer comes in now again uh just to say here one more time like we know these are different but we also would know for example say if we replace this four with a three well all our computer again would know is that two is different from three just like four is different from two it doesn't know how different they are and that's what i'm trying to get at here is our embedding layer is going to try to group words in a similar kind of way so that we know which ones are similar to each other so let me now talk about specifically the embedding layer so let me just draw a little grid here now what are embedding layer actually does kind of like i don't want to say the formal definition but the more mathy definition is it finds word vectors for each word that we pass it or it generates word vectors and uses those word vectors uh to pass to the future layers now a word vector can be in any kind of dimensional space now in this case we've picked 16 dimensions for each word vector which means that we're going to have vectors maybe something like this and a vector again is just a straight line with a bunch of different coefficients in some kind of space that is in this case 16 dimensions so let's pretend that this is a 16 dimensional vector and this is the word vector for the word have now in our computer it wouldn't actually be have it would be zero because again we have integer encoded stuff but you kind of you get the points we'll say this is the word vector for half now what we're going to do immediately when we create this embedding layer is let me actually get out of this quickly for one second is we initially create 10 000 word vectors for every single word and in this case every single number that represents a word so what we're going to do is when we start creating this embedding layer we see that we've have an embedding layer is we're going to draw 10 000 word vectors in just kind of some random way that are just there and each one represents one word and what happens when we call the embedding layer is it's going to grab all of those word vectors for whatever input we have and use that as the data that we pass on to the next layer now how do we create these word vectors and how do we group words well this is where it gets into a bit complicated math i'm not really going to go through any equations or anything like that but i'll kind of give you an idea of how we do it now we want to so let me get rid of this word have because this is not the best word vector example and let's say that this word vector is great now upon creating our word vector our embedding layer we have two vectors we have great and we have good and we can see that these vectors are kind of far apart from each other and we determine that by looking at the angle between them and we say that this angle maybe it's like i don't know 70 degrees or something like that and we can kind of determine that great and good are not that close to each other but in reality we want them to be pretty close to each other we want the computer to look at great and good and be like these are similar words let's treat them similarly in our neural network so what we want to do hopefully is have these words and these vectors kind of move closer together whether it's good going all the way to great or great going all the way to good or vice versa right we just want them to get close together and kind of be in some form of a group so what we do is we try to look at the context in which these words are used rather than just the content of the words which would just be what this looks like we want to figure out how they how they're used so we'll look at the words around it and determine that you know when we have a and a and a and a maybe that means that these are like related in some way and then we'll try to group these words now it's way more complicated than that don't get me wrong um but it's kind of like a very basic way of how they group together we look at the words that surround it and just different properties of the sentence involving that word and then we can kind of get an idea of where these words go when which ones are close to each other so maybe after we've done some training uh what happens is our word embeddings are what is known as learned just like we're learning and teaching our neural network and we get we end up getting great and good very close together and these are what their word vector representations are we can tell that they're close again by looking at the angle in between here maybe it's like 0.2 degrees and what that means is these two vectors which are just a bunch of numbers essentially are very close together so when we feed them into our neural network they should hopefully give us a similar output at least for that specific neuron that we give it to now i know this might be a little bit confusing but i'm going to go we're going to talk about this a bit more with another drawing of the whole network but i hope you're getting the idea the whole point of this embedding layer is to make word vectors that and then group those word vectors or kind of like make them close together based on words that are similar and that are different so again just like we would have grading good here we would hope that a word vector like bad would be down here where it has a big difference from great and good so that we can tell that these words are not related whatsoever all right so that's how the embedding layer works now what ends up happening when we have this embedding layer is we get an output dimension of what's known as 16 dimensions and that's just how many coefficients essentially we have for our vector so just like if you have a 2d line so like if this is our grid in 2d and we say that this is x and this is y we can represent any line by just having like uh some values like ax plus b y equals c now this is the exact same thing that we can do in in n dimensions which means like any amount of dimensions so for a 16 dimensional line i'm not going to draw them all but we would start with like ax plus b y plus c z plus d w and so on and we would just have again 16 of these coefficients and then some kind of constant value uh maybe we call it lambda that is like what it's what it equals to what the equation equals to and that's how we define a line i'm pretty sure i'm doing this correctly in uh in n dimensions so anyways once we create that line what we actually want to do is we want to scale the dimension down a little bit now that's just because 16 dimensions is a lot of data especially when we have like a ton of different words coming into our network we want to scale it down to make it a little bit easier to actually compute and to train our network so that's where this global average pooling 1d layer comes in now i'm not going to talk about this in two depth in too much depth but essentially the way to think of the global average pooling 1d is that it just takes whatever dimension our data's in and just puts it in a lower dimension now there's a specific way that it does that but again i'm not going to talk about that and it's not super important if you care about that a lot just look it up and it's not like crazy hard but i just i don't feel the need to go into it in this video so anyways let's now start drawing what our network actually looks like after understanding how this embedding layer works so we're going to initially feed in a sequence and we'll just say that this is like our sequence of encoded words okay so say this is our input and maybe it's something like zero seven nine like a thousand two hundred a thousand twenty uh we have like nine again maybe we have eight it's just a bunch of different essentially numbers right so we're gonna pass this into our embedding layer and all this is gonna do is it's gonna find the uh representation of these words in our embedding layer so maybe our embedding layer well it's going to have the same amount of words in our vocabulary so it'll look up say zero it'll say maybe zero means zero's vector is like zero point two zero point and it goes to 16 dimensions but i'm just gonna do like two for this example here maybe seven its vector is like seven and nine point zero and it just keeps going like this and it looks up all these vectors so it takes all of our input data and it just turns them into a bunch of vectors and just spits those out into our next layer now our next layer what this does is it just takes these vectors and just averages them out and it just means it kind of shrinks them their data down so we'll do like a little smaller thing here and we'll just say like average okay so i'll call this one embedding and that one is average now this average layer now is where we go into the actual neural network well obviously this is a neural network but we go into the dense layers which will actually perform our classification so what we're going to do is we're going to start with 16 neurons and this is just again an arbitrary number that we've picked for our network you can mess around with different values for this and i encourage you to do that but 16 is what tensorflow decided to use and what i'm just following along with so we're going to have 16 neurons and we're going to pass all of our now 16 dimensional data or whatever dimensional data it is into these neurons like this now this is where we start um doing the dense layer so we have this dense layer and this is connected to one output neuron like this so what we end up having is this embedding layer which is going to have all these word vectors that represent different words we average them out we pass them into this 16 neuron layer that then goes into an output layer which will spit out a value between 0 and 1 using the sigmoid function which i believe i have to correct myself because in other videos i said it did between negative one and one it just takes any value we have and puts it in between zero and one like that all right so that is kind of how our network works so let me talk about what this dense layer is doing just a little bit before we move on to the next video so what this dense layer is going to attempt to do essentially is look for patterns of words and try to classify them using the same methods we talked about before into either a positive review or a negative review i'm going to take all these word vectors which again are going to be like similarly grouped words like great good are going to be similar input to this dense layer right because we've averaged them out and embedded them in all this and then what we're going to do is we're going to try to determine based on what words we have and what order they come in what our text is and we hope that this layer of 16 neurons is able to pick up on patterns of certain words and where they occur in the sentence and give us a accurate classification again it's going to do that by tweaking and modifying these weights and all of the biases that are on you know all of these different what do you call it layers or all of these connections or whatever they are and then it's going to give us some output and some level of accuracy for our network all right so now it's time to compile and train our model now the first thing we have to do is just define the model give it an optimizer give it a loss function and then i think we have to define uh the metrics as well so we're going to do is going to say model equals in this case or sorry not model equals model dot compile if i spell compile it correctly and then here we're gonna say uh optimizer we're gonna use the atom optimizer again i'm not really gonna talk about what these are that much if you're interested in the optimizer just look them up and then for the loss function where you're going to use the binary underscore cross entropy now what this one essentially is is well binary means like two options right and in our case we want to have two options for the output neuron which is zero or one so what's actually happening here is we have the sigmoid function which means our number is going to be between zero and one but what the loss function will do is pretty we'll calculate the difference between for example say our output neuron is like 0.2 and the actual answer was zero what will give us a certain function that can calculate the loss so how much of a difference 0.2 is from zero um and that's kind of how that works again i'm not going to talk about them too much and they're not like i mean they are important but not to really like memorize per se like you kind of just mess with different ones but in this case binary cross entropy works well because we have two possible values zero one so rather than using the other one that we used before which i don't even remember what was called something cross entropy we're using binary cross entropy okay so now what we're gonna do is we're actually gonna split our training data into two sets and the first set of our training data is going to be called validation data or really i guess you can think of it as a second the order doesn't really matter but what we're going to do is just get some validation data and what validation data is is essentially we can check how well our model is performing based on the tunes and tweaks we're doing on the training data on new data now the reason we do that is so that we can get a more accurate sense of how well our model is because we're going to be testing new data to get the accuracy each time rather than testing it on data that we've already seen before which again means that the model can't simply just memorize each review and give us either a zero or one for that it has to actually have some degree of i don't know like thinking or operation so that it can work on new data so what we're going to do is we're going to say x underscore val equals and all we're going to do is just grab the train data and we're just going to cut it to a thousand or 10 000 entries so there's actually 25 000 entries or i guess reviews in our training data so we're just going to take 10 000 of it and say we're going to use that as validation data now in terms of the size of validation data it doesn't really matter that much this is what tensorflow is using so i'm just kind of going with that but again mess with these numbers and see what happens to your model everything with our neural networks and machine learning really is going to come down to very fine what's known as hyper parameters or like hypertuning which means just changing individual parameters each time until we get a model that is well just better and more accurate so we're going to say that x val equals that but then we're also going to have to modify our x train data to be train underscore data and in this case we're just going to do the other way around so 10 000 colon now i'll just copy this and we're just going to replace this again with instead of test uh actually oh we have to do this with labels sorry what am i thinking so we're just going to train change this to be labels and then instead of x val it's just going to be y value and then y train so yeah we're not touching the test data because we're going to use all that test data to test our model and then we're just going to use the uh the training stuff or the validation data to validate the model all right so now that we've done that it is actually time to fit the model so i'm just going to say uh like fit model and you'll see why i'd name this something different in a second it's going to be equal to model.fit and in this case what we're going to do is we're going to say x underscore train y underscored train we're going to say epochs uh is equal to i that's how you spell it 40 and again you can mess with this number and see what we get based on that and we're going to say batch underscore size equals 512 which i'll talk about in a second and then finally we're going to say validation underscore data equals and in here we're going to say x underscore val y underscore val and i think that's it let me just check here quickly oh one last thing that i forgot to do we're gonna say verbose equals one verbose equals one now i'm not gonna lie i honestly don't know what verbose is i probably should have looked it up before the video but i have no idea what that is so if someone knows please let me know but the batch size is essentially how many what do you call it um movie reviews we're gonna do each time or how many we're gonna load in at once because the thing is it's kind of i mean we're loading all of our reviews into memory but in some cases we won't be able to do that and we won't be able to like feed the model all of our reviews on each single cycle so we just set up a batch size that's going to define us essentially how many at once we're going to give and i know i'm kind of horribly explaining what a batch size is but we'll get into more on batch sizes and how we can kind of do like buffering through our data and like going taking some from a text file and reading into memory in later videos when we have like hundreds of gigabytes of data that we're gonna be working with okay so finally we're gonna say results equals and in this case i believe it is model dot evaluate and then we're going to evaluate this obviously on our test data so we're going to give it test data and test labels so test underscore data test underscore labels like that and then finally what i'm going to do is just actually print out the results so we can see what our accuracy is so say print results and then get that value so let me run this quickly neural networks text classification let's go cmd and then python text or that's not even the one we're using we're using tutorial 2 sorry and let's see what we get with this this will take a second to run through the epoch so i'll fast forward through that so you guys don't have to wait all right so we just finished doing the epochs now and essentially our accuracy was 87 and this first number i believe is the loss which is 0.33 and then you can see that actually here we get the accuracy values and notice that the accuracy from our last epoch was actually greater than the accuracy on the test data which again shows you that sometimes you know when you test it on new data you're going to be getting a less accurate model or in some cases you might even get a more accurate model it really just you can't strictly go based off what you're getting on your training data you really do need to have some test and validation data to make sure that the model's correctly working so that's essentially what we've done there um and yeah i mean that that's the model we've we tested and it's 87 accurate so now let's actually have let's interpret some of these results a little bit better and let's show some reviews let's do a prediction on some of the reviews and then see like if this our model kind of makes sense for what's going on here so what i'm going to do is i'm just going to actually just copy some output that i have here um just save us a bit of time because i am going to wrap up the video in a minute here but essentially what this does it just takes the first review from test data gets the model to predict that because we obviously we didn't train on the test status we can do that fine we're going to say review and then we print out the decoded review we're going to print out what the model predicted and then we're going to print out what the actual label of that was so if i run this now i'll fast forward through the kind of training process and we'll see the all right so this is what essentially our review looks like so at least the one that we were testing it on and you can see that we have this little start tag and it says please give this one a miss for and then br stands for like break line or go to the next line so we could have actually added another tag for br uh if we noticed that this was used a lot in the review uh but we didn't do that so you see br unless this is actually part of the review but i feel like that should be like break line in terms of html anyways and then we have some unknown characters which could be anything that we just didn't know what it was and it says and the rest of the cast rendered terribly performed says the show is flat flat flat brbr i don't know how uh michael madison could have allowed this one on his plate he almost seemed he'd what does it seem to know this wasn't going to work out and his performance was quite unknown so all yeah so anyways you can see that this probably had like some emojis in it or something and that's why we have all these unknowns and then obviously we made this review which was pretty short to be the full length of 250 so we see all these pads that did that for us and then we have a prediction and an actual value of zero so we did end up getting this one correct now i think it'd be interesting actually to write your own review and test it on this so in the next video what i'm going to do is show you how we can save the model to avoid doing like all of this every time we want to run the code because realistically we don't want to wait like a minute or two before we can predict a movie review every time we just want it to happen instantly and we definitely can do that i just haven't showed that yet in the series because that's kind of in like later what you do after you learn machine learning um and obviously like this this model trained pretty quickly like we only had about uh what was it like 50 000 test data set which it seems like a large number but it's really not especially when you're talking about string data so in future videos we're gonna be training uh models that take like maybe a few days to train at least that's the goal or maybe a few hours or something like that so in that case you're probably not going to want to train it every time before you predict some information so that'll be useful to know how to save that so in today's video we're going to be doing is talking about saving and loading our models and then we're going to be doing a prediction on some data that doesn't come from this actual data set now i know this might seem kind of trivial we already know how to do predictions but trust me when i tell you this is a lot harder than it looks because if we're just taking in string data that means we have to actually do the encoding of the preprocessing removing certain characters making sure that that data looks the same as the data that our neural network is expecting which in this case is a list of encoded numbers right or of encoded words that is essentially just numbers so what we're going to do to start is just save our model so let's talk about that now so up until this point every time we've wanted to make a prediction we've had to retrain the model now on small models like this that's fine you have to wait a minute two minutes but it's not very convenient when you have models that maybe take you days weeks months years to train right so what you want to do is when you're done training the model you want to save it or sometimes you even want to save it like halfway through a training process this is known as checkpointing the model so that you can go back and continue to train it later now in this video we're just going to talk about saving the model once it's completely finished but in future videos when we have larger networks we will talk about checkpointing and how you how to load your or train your model in like batches with different size data and all that so what i'm going to start by doing is just actually bumping the vocabulary size of this model up to 88 000. now the reason i'm doing that is just because for our next exercise which is going to be making predictions on outside data we want to have as many words in our model as possible so that when it gets kind of some weirder words that aren't that common it knows what to do with them uh so i've done a few tests and i noticed that with the what he called with the vocabulary size bumped up it performs a little bit better so we're going to do that so anyways we bumped the vocabulary size and now after we train the model we need to save it now to save the model all we have to do is literally type the name of our model in this case model dot save and then we give it a name so in this case let's call it model dot h5 now h5 is just like an extension that uh means i don't know it's like i honestly don't know why they use h5 but it's the extension for a saved model and keras and tensorflow so we're just going to work with that and that's as easy as this is it's just going to save our model in binary data which means we'll be able to read it in really quickly and use the model when we want to actually make predictions so let's go ahead and run this now and then we're going to have the model saved and then from now on we won't have to continually train the model when we want to make predictions i'm going to say python tutorial 2 and i'll be right back once this finish finishes running all right so the model has finished training notice that our accuracy is slightly lower than it was in the previous video really kind of a negligible difference here but anyways just notice that because we did bump the vocabulary size so anyways now that we've saved the model we actually don't have to go through this tedious process every time we run the code of creating and training and fitting the model and in fact we don't actually need to save it as well either here to load our model in now that it's saved and you can see the file right here with all this uh this big massive binary blob here all we have to do to load this in is just type one line now the line is whatever the name of your model is it doesn't matter i'm just going to call it model is equal to in this case keras dot models dot load underscore model and then here you just put the name of that file so in this case model.h5 now what's really nice about this as well is you can actually train a bunch of different models and tweak like hyper parameters of them and only save the best one what i mean by that is like maybe you mess with for example the amount of neurons in the second activation layer uh or something like that or in the second hidden layer and then you train a bunch of models you figure out which one has the highest accuracy and then you only save that one that's nice as well and that's something you could do like overnight you could run like your script for a few hours train a bunch of models figure out which one is the best only save and then use that one so anyways we're going to load in this model notice that i've actually just commented out this aspect down here because we're not going to use this anymore and now what we're going to start doing is actually training or testing the model on some outside data so i've gone ahead and picked a movie review for one of my favorite movies some of you guys can read this if you want uh but it's the lion king absolutely love that movie so i've decided to go with this this review was a 10 out of 10 review so a positive review and we're gonna test our model on this one now i actually did take this off like the imdb website or whatever that's called um but the data set that they use is different so this is you guys will see it why this works a little bit differently and what we have to do with this so this is in a text file so what i'm going to do is load in the text file here in code and then get that big blob that string and convert it into a form that our model can actually use so the first step to do this obviously is to get that string so we're going to say with open and in this case i've called my file test.txt and then i'm just going to set the encoding because i was running into some issues here you guys probably don't have to do this i'm just going to say utf hyphen 8 which is just kind of a standard text encoding and we're going to say as f now again the reason i use with is just because that means i don't have to close the file afterwards better practice if you want to use that and now i'm going to say for line in f dot read lines which essentially just means we're going to get each line in this case we only have one line but if we wanted to throw in a few more uh reviews in here and do some predictions on those that would be very easy to do by just keeping this code structure just throw another line in there and now i'm just going to say we're going to grab this line and we're going to start preprocessing it so that we can actually feed it to our model now notice that this when we read this in all we're going to get is a large string but that's no good to us we actually need to convert this into an encoded uh list of numbers right and essentially we need to say okay so of that's a word what number represents that put that in a list same with all same with uh same with animation right and we keep going and keep going pretty well for all of the words in here and we also have to make sure that the size of our text is only at max 250 words because that's what we were using when we were training the data so it's expecting a size of that and if you give it something larger that's not going to work or it might but you're going to get a few errors with that so anyways the first step here is i'm going to say n line is equal to line dot and i'm going to remove a bunch of characters that i don't want so i'm just going to say dot replace i think this is the best way to do it but maybe not um and i'm going to replace all the commas all of the periods all of the brackets and all of the colons and i'll talk about more why we want to do that in just one second so we'll do dollar place i guess this dollar place should probably be outside the bracket uh and then we'll replace with a bracket with nothing and i know this is there probably is a better way to do this but for our purposes it's not really that important and finally we will replace all our colons with nothing as well now again the reason i'm doing this is because let's go here if you have a look for example when we split this because we're just going to split this data by um spaces and to get all the words what will end up happening is we're going to get words like company comma we're going to get words like i'm trying to find something that has a period like art dot and then a quotation mark right and we don't want those to be words in our list because there's no mapping for art period there's only a mapping for art which means that i need to remove all of these kind of symbols so that when we split our data we get the correct words now there'll be a few times where the split doesn't work correctly but that's okay as long as the majority of them are working well same thing with brackets right i can't have irons and then a closing bracket is one of my words so i need to get rid of that now this reminds me i need to remove quotation marks as well because they use quite a few of those in there i don't know why i closed that document uh so let's do that as well with one last replace so say dollar place in this case we'll actually just do backslash quotation mark and then again with nothing now i'm adding a dot strip here to get rid of that backslash n and now we're going to say dot split and in this case we'll split out a space now i know this is a long line but that's all we need to do to remove everything and now we actually need to encode and trim our data down to 250 words so to encode our data i'm going to say encode equals in this case uh and we're just literally we'll make a function called like review underscore in code and we'll pass in our endline now what review and code will do is look up the mappings for all of the words and return to us an encoded list and then finally what we're going to do and we'll create this function in just a second don't worry it doesn't already exist is we're actually going to use what we've done up here with this test data train data keras preprocessing stuff and we're just going to apply this to in this case our encoded data so we add those pad tags or we trim it down to what it needs to be so in this case we'll say encode equals keras.preprocessing instead of train data we'll just pass in this case actually a list and then encode inside it because that's what it's expecting to get a list of lists all right so now that we've done that our final step would be to use the model to actually make a prediction so we're going to say model.predict and then in this case we'll pass it simply this encode right here which will be in the correct form now we'll save that under predict and then what we'll do is just simply print out the model so we'll say print or not the model sorry we'll print the original text which will be the review so in this case we'll print line and then we will print out the encoded review just so we can have a look at what that is and then finally we'll print the prediction so what whether the model thinks it's positive or negative so we'll just say predict and in this case we'll just put zero because we're only going to be doing um like one at a time right okay sweet so now the last thing that we need to do is just simply write this review in code function and it will be good to go and start actually using our model so i'm just going to say define review underscore in code this is going to take a string we'll just call that s lowercase s and what we're going to do in here is set up a new list that we're going to append some stuff into so i'm just going to say like return let's just say like encoded equals and then i'm going to start this with 1. now the reason i start 1 in here is because all of our data here uh where it starts has a one so we're just going to start with one uh because we won't have added that in from uh the other way i hope you guys understand that just we're setting like a starting tag to be consistent with the rest of them and now what we're going to do is we're going to loop through every single word that's in our s here which will be passed in as a list of words we'll look up the numbers associated with those words and add them into this encoded list we're going to say for word and in this case we're going to say word in s now we'll say if word in this case word underscore index and again we're going to use word underscore index as opposed to reverse word index because word index stores all of the words corresponding to the letters or not the letters the numbers which means that we can literally just throw our data into word index and it'll give us the number associated with each of those words so we're going to say if word in word index then we'll say encoded dot append and in this case we'll simply append in this case word index word now otherwise what we'll do is we'll say encoded dot append to now what will happen is we're going to check here if word if the word is actually in our vocabulary which is represented by word index which is just a dictionary of all the words corresponding to all the numbers that represent those words now if it's not what we'll do is we'll add in that unknown tag so that the program knows that this is an unknown word otherwise we'll simply add the number associated with that word now one last thing to do is actually just do word.lower here just to make sure that if we get any words that have some weird capitalization they are still found in our vocabulary so like words at the beginning of a sentence and stuff like that uh and now with that being done i believe we're actually finished and ready to run this code so what's nice about this is now that we've saved the model we don't have to train it again so i can literally just run this and it should happen fairly quickly fingers crossed let's see all right must be a list of integrals found noniterable object so what error is that here um in code encoding coding code all right so print review and code ah well it would be helpful if i returned the encoded list and that would have been our issue there so let's run that one more time and see what we're getting there and there we go sweet so this is actually the review i know it's very really hard to read here but if you guys want to go ahead and read it feel free since it's on the lion king it's obviously a positive review and then you can see this is what we've ended up with so our review has been translated into this which means we've actually trimmed quite a bit of the review and you can see that wherever it says two that is actually a word that we didn't know or that wasn't in our vocabulary four represents the that's why there's a lot of fours and then all the other words have their correspondence right now fortunately for us we picked a 88 000 vocabulary which means that we can get indexes like 20 000 whereas before it would have all been under 10 000. and you can see that our prediction here is now 96 um positive which means that obviously like we were going between zero where zero is a negative review and one is a positive review so this classified correctly as very positive review and we could try this on all other kinds of reviews and see what we get but that is how you go about kind of transforming your data into the form that the network expects and that's where i'm trying to get you guys at right now is to understand that yes it's really easy when we're doing it with this kind of data that just comes in like imdb like keras load data but as soon as you actually have to start using your own data there's quite a bit of manipulation that you have to do and things that you might not think about when you're actually feeding it to the network and in most cases you can probably be sure that your network is not actually the thing that's happening incorrectly but it's the data that you're feeding it is not in the correct form um and it can be tricky to figure out what's wrong with that data so with that being said that has been it for this video i hope you guys enjoyed that's going to wrap up the text classification aspect here of neural networks hey guys so in today's video i'm going to be showing you how to install tensorflow 2.0 gpu version on an ubuntu linux machine now this should work for any version of linux or any linux operating system although the one i am going to be showing you on is ubuntu 18.0.4 now you may notice that i'm actually on a windows machine right now and that this is actually just an ubuntu terminal that's open now i'm actually just ssh into a server that i have that contains two 1080 graphics cards so gtx 1080s and that's how i'm going to be showing you how to do this now quickly if you don't understand the difference between the cpu and the gpu version the cpu version is essentially just way slower and you would only really use the cpu version if you don't have a graphics card in your computer that is capable of running tensorflow 2.0 gpu so quickly before we go forward and you guys get frustrated with not being able to install this make sure that you have a graphics card that actually works for this programmer for this module that means you have to have a graphics card that is a gtx 1050 ti or higher those are the ones that are listed on tensorflow's website as compatible with tensorflow 2.0 gpu if you want to have a quick thing without having to go to the website to see if yours works if it has four gigs of video ram and is a gtx generation card or higher it most likely works with tensorflow 2.0 now i don't know about all the different cards if you have any questions leave them below i'll try to answer that for you but any 1060 1070 1080 or rtx cards that have cuda cores on them will work for this essentially you just need a cuda enabled gpu so you can check if yours meets that requirement before moving forward now to do this i'm just going to be following the steps listed on the tensorflow website now you may run into some issues while doing this but for ubuntu this is pretty straightforward and i'm essentially just going to be copying these commands and pasting them in my terminal now if you'd like to just try to do this without following along with video go ahead but i will be kind of showing you some fixes that i ran into while i was doing this um so let's go ahead and get started so actually let me just split the screen up so we can have a look at both of them at once i'm in my linux machine right now you just have to get to the terminal you notice that i don't even have a desktop and i'm literally just going to start copying and pasting these commands now the first thing that we need to install is actually cuda now cuda is what allows us to use the cuda cores on our gpu to actually run the code so just go ahead and keep copying these commands it will take a second and i actually already have this installed on my machine so i'm going to go through the steps with you guys but again if anything is different on my machine that's probably because it's already installed so if you don't know how to copy it into a window like this you just right click on your mouse and it'll copy if you're using a server like i am um but anyways we'll just go through all of these and keep going now i will have all these commands listed in my description as well and that should show you guys you know if the website goes down at any point you can just copy it from there as well so yeah literally just keep going all we're doing here is adding nvidia packages we're gonna make sure we have the nvidia drivers for our graphics card that are correct and then we're gonna go ahead and install tensorflow 2.0 uh so yep go through these commands there's not really much for me to say as i copy these in and eventually we will get through them all all right so now we're going to install the nvidia driver you can see that's all commented out on this tensorflow website here copy that and again just continue to go i don't really have any commentary for you guys here so we'll copy this this is going to install obviously the development and runtime libraries which we need and it says minimum four gigs or approximately four gigabytes which will mean that's how long it how many gigabytes it's going to take up on our machine so this will take a second and i'll fast forward through these stuff if it does take a while finally we're going to install tensor rt i don't even know what this is but apparently it's required and then after we're done this we should actually be finished installing everything that we need for tensorflow 2.0 to work again if you guys want to go through this just go to the website copy all of these commands in order paste them into here and they should work properly now finally what we have to do is actually install tensorflow 2.0 so we've got all the dependency dependencies installed and now to install tensorflow 2.0 we're just going to say pip3 install tensorflow and i believe we're going to say hyphen gpu and then equals equals 2.0 point i got to find it up here to make sure that we do it correctly 2.0.0 hyphen alpha zero like that so then we'll do that and that should install tensorflow 2.0 for us now i already have this installed but this will actually take a few minutes to install because there is quite a bit of stuff that it needs to download on your computer so anyways that has been it for installing tensorflow 2.0 on your computer using the gpu version again throughout the rest of the neural network series i'm going to be going forward doing this on an ubuntu machine so running all of the code i'll do the development of windows throw the files on my server train the model train the models excuse me and then take the models off and use them on my windows machine so if you want to validate if this is working you can really quickly just do python 3 in linux and then you can cite do import tensorflow and doing that you shouldn't get any errors and if you don't get any errors then you have successfully installed tensorflow 2.0 now a few errors here if you guys are still listening and stuff wasn't working if for some reason when you install tensorflow and you notice that it's not using your gpu go ahead and uninstall the cpu version of tensorflow so just pip3 on uninstall and then tensorflow and i guess you'd have to just do just tensorflow like that and that will install the cpu version if it is installed in your machine so anyways that has been it for how to install tensorflow 2.0 gpu version on ubuntu pretty straightforward just go through copy these commands and if you guys have any questions or errors please just leave them in the comments below and i will try my best to help you out
