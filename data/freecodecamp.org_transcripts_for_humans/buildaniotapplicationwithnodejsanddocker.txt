With timestamps:

00:00 - i'm shubra i run product marketing and
00:03 - developer relations at joint and i'll be
00:06 - joined by quite a few developers uh
00:09 - wyatt
00:10 - show fans
00:12 - wyatt is my colleague is our lead
00:14 - node.js engineer and he has been a
00:17 - active contributor to different
00:19 - frameworks like happy seneca
00:22 - um and you know even for
00:24 - the microservices that we run ourselves
00:26 - at joint and even
00:28 - myself i've been
00:30 - in the express community and i've worked
00:32 - on frameworks like loopback and api
00:34 - gateways right so just to give you a
00:36 - context
00:38 - so the workshop today is going to be
00:40 - focused on building an iot app and we'll
00:42 - be using a variety of technologies i'll
00:44 - walk you to that stack but before i do
00:46 - that um you know
00:48 - want to put a disclaimer we are not
00:50 - going to be talking any proprietary or
00:53 - closed source products
00:54 - at joint ourselves we have been the
00:56 - progenitor of node we are kind of the
00:58 - first to incubate node.js and we are
01:01 - also the founding member of cncf and we
01:04 - believe that the entire cloud
01:06 - and all the technologies running within
01:08 - it should be in the open right and we
01:10 - are entirely community driven
01:12 - so with that
01:15 - um i want to you know give you our point
01:17 - of view on
01:18 - how we see the legacy stack and the
01:20 - modern stack and this is based on
01:23 - people we are working in the community
01:25 - our customers as well
01:27 - so if you look at the legacy stack right
01:29 - these were traditionally monolithic web
01:31 - apps and we're running code on different
01:34 - kind of blocking and bloated platforms
01:36 - right you could say
01:37 - jvm clrs what they wear and
01:40 - traditionally these were deployed in
01:42 - virtual machines right if you go back
01:44 - like 10 years and running inside a
01:46 - private data center now if you flip the
01:48 - stack like when you talk about
01:49 - modernization the app services are more
01:52 - message driven there are more micro
01:54 - services
01:56 - they are built for mobile first right
01:57 - you don't really build for you know they
01:59 - are build responsive they are not just
02:01 - built for the web
02:02 - you are building them for iot workloads
02:05 - as well as artificial intelligence
02:07 - workloads right and we can go to dirt
02:09 - i'm just gonna skim through a lot of
02:11 - these topics from a coding perspective
02:13 - you know you'll see the rise of node.js
02:15 - golang and a lot of different
02:18 - lightweight runtime platforms or
02:20 - languages the whole idea is like can you
02:22 - support concurrency can they work with
02:25 - async workloads right and from a runtime
02:28 - perspective you know if you take the
02:30 - case of microservices there's a perfect
02:32 - fit where you want to run them on
02:35 - containers if you take the case instance
02:37 - of node.js most of the processes are
02:39 - running single threaded running them on
02:41 - a traditional vm is probably an overkill
02:44 - so you probably want to just package it
02:46 - run it
02:47 - you know in package it in an immutable
02:49 - fashion push it to the cloud run it on a
02:52 - lightweight container right and the
02:54 - infrastructure supporting that is
02:56 - you know that's kind of you know i don't
02:58 - have to preach on that that's
02:59 - essentially cloud right now
03:01 - but there are differences in the cloud
03:02 - right so some of the projects we work on
03:04 - and we'll be discussing these
03:06 - in the
03:08 - framework of this workshop
03:10 - if you look at node we work on node core
03:13 - rustify these are restfusa framework d
03:16 - traces essentially a universal debugging
03:18 - tool
03:19 - that you can use in production happy
03:23 - you know it's another api framework mdb
03:25 - was again for memory leak debugging
03:28 - we're also going to talk about express
03:30 - gateway which was recently launched as
03:32 - recent as last week
03:34 - and again we have been investing we are
03:37 - polyglot ourselves we are not a hundred
03:39 - percent note shop so if you look at
03:41 - tooling and live projects that we are
03:43 - working and particularly in the golang
03:45 - community these are around terraform
03:48 - around um you know particularly for
03:50 - provisioning our containers for console
03:52 - for service management vault for
03:54 - security prometheus for monitoring uh
03:57 - packer and crypto and other core
03:59 - libraries and on the container side you
04:01 - know we started with containers 10 years
04:03 - back right we were running
04:04 - infrastructure or bare metal containers
04:06 - and then we put a docker but we had to
04:08 - make it run in production not like a
04:10 - mickey mouse like standalone project so
04:12 - we created the concept of an elastic
04:14 - docker host where we abstract the entire
04:16 - data center into a single docker host
04:18 - with a single docker api endpoint and
04:21 - you can just push based on a compose
04:23 - file or any artifact you have you can
04:25 - essentially push your application
04:27 - blueprint and get it deployed in matter
04:29 - of minutes
04:30 - and obviously we work on kubernetes and
04:33 - we have also created a micro app
04:35 - orchestration framework called as
04:36 - container pilot we are going to be using
04:39 - container pilot today right
04:41 - and finally on the cloud you know
04:42 - obviously our because our entire cloud
04:44 - is 100 open source we operate our public
04:47 - clouds but most of our users also deploy
04:49 - it on premises
04:51 - in their own private cloud managed
04:53 - private cloud settings and it's primary
04:54 - due to cost and performance
04:57 - so let's go into the details so there's
04:59 - a github link
05:01 - if you want to grab that url
05:03 - i'll be on this slide for a few seconds
05:06 - that's has all the contents of the code
05:09 - and the exercises
05:11 - so it's github.com joy slash joint
05:14 - slash summit workshop
05:19 - and just a show of hands if anybody was
05:21 - not able to get that
05:26 - all right
05:27 - perfect
05:30 - there are some prerequisites for this
05:32 - workshop we'll be primarily be using
05:34 - node.js
05:36 - so you'll have to install node.js if you
05:37 - haven't already you should while i'm and
05:40 - you can do that like when i'm walking
05:41 - through the rest of the slides uh you
05:43 - need to at least have a local docker
05:45 - install
05:46 - i think those are the primary uh prereqs
05:50 - and optional uh you know if you really
05:52 - want to push this out like once you test
05:54 - it on your laptop if you want to push
05:55 - this out into a public cloud um you can
05:58 - essentially sign up for joints public
06:01 - cloud we have 250 free credits and we
06:03 - won't be charging anything for the
06:04 - workshop right
06:06 - so but again that's optional if you
06:08 - really want to test it out like how it
06:10 - works in the cloud make sense so i'll
06:13 - just hang on to this slide for another
06:15 - few seconds so that you have the urls
06:25 - perfect
06:27 - all right so what are we going to build
06:28 - today
06:29 - we are going to build an iot app
06:32 - and it's a highly distributed micro
06:34 - service app uh we'll have a variety of
06:37 - uh end points that we'll consume data
06:40 - from so you know we'll be using
06:42 - um
06:43 - sensor data and out of the sensor data
06:46 - we'll be using a home automation
06:47 - platform called as smart things
06:50 - and smart things you know sells you
06:51 - sensors for you know motion sensors
06:54 - humidity sensors what not and once we
06:57 - get all that data we'll be uh monitoring
07:00 - uh the data feed that is coming in we'll
07:02 - be storing that into a database
07:05 - that's where we'll use influx db
07:07 - and
07:08 - we'll also gather some monitoring data
07:10 - for using
07:11 - agents like prometheus and then we have
07:14 - a variety of other components you know
07:16 - including an api gateway including redis
07:20 - and then we will all of these services
07:22 - um that are interconnected with each
07:24 - other uh each of them is running on a
07:26 - container of its own and we will
07:29 - orchestrate this and we will scale it
07:31 - out right so we'll start from scratch
07:33 - build you know test integrate deploy and
07:36 - scale
07:37 - okay
07:38 - so
07:40 - just few of the technology stacks that
07:41 - we'll be covering today right if you
07:43 - want to take a picture you could
07:45 - so at the very top right because we are
07:47 - consuming data from a smart things
07:50 - sensor and like we are not going to use
07:52 - real smart things we are going to
07:54 - simulate that data right
07:56 - but it's kind of the same experience if
07:57 - you had to get it from a real home
07:59 - automation system right
08:01 - to protect all these apis we are going
08:03 - to use a front-end component called as
08:05 - the express gateway and i'll get into
08:08 - the details of that this project was
08:10 - launched last week and we have uh irfan
08:13 - from the gateway team just raise your
08:16 - hands so you know if you have follow-up
08:18 - questions uh you can reach out to him on
08:20 - the express gateway we also have al al
08:22 - sang
08:24 - from the express gateway team so you
08:25 - know they're here to help and again with
08:28 - me i have wyatt and we have colin
08:30 - um his hands and we have fires as well
08:33 - so if you have any node questions or
08:35 - container questions you know reach out
08:38 - obviously we'll use node we'll be using
08:40 - frameworks like happy and seneca seneca
08:42 - is a microservices framework all written
08:44 - in node.js uh from a backend perspective
08:47 - like from the front end essentially we
08:49 - have prometheus endpoints and on the
08:50 - backend we'll be using influx db to
08:53 - store time series data
08:55 - and from a container perspective we'll
08:56 - be running traditional docker
08:58 - however to wire uh our entire
09:01 - application blueprint and to deploy it
09:03 - and scale it we'll be using container
09:05 - pilot and finally you know the option
09:08 - where we push it to the public cloud
09:10 - we'll be using uh triton from joint
09:13 - so when i get into all of these let's
09:15 - quickly run through some of these key
09:16 - concepts right i just showed you a bunch
09:19 - of technology and then we'll start
09:21 - really working the exercises
09:23 - so
09:24 - just architecturally how is smart things
09:26 - laid out right so it has a bunch of
09:27 - sensors and if like every um you know
09:31 - iot home device manufacturer this is not
09:34 - like industrial iot this is really home
09:35 - automation you have a hub right and the
09:39 - traditional way these connects uh there
09:41 - are a variety of protocols there are
09:43 - zigbee protocols and whatnot uh where in
09:45 - all of these devices that are present at
09:47 - your home they connect to a hub right
09:49 - and based on those
09:52 - connected connection
09:54 - endpoints then you essentially
09:56 - have device handlers and then you send
09:59 - up the data through an api gateway
10:02 - and
10:02 - that api gateway is consuming your
10:04 - events data and pushing it to a cloud
10:06 - processing system
10:08 - and
10:09 - on the cloud
10:10 - side and this cloud could be running
10:12 - anywhere traditionally it's a public
10:14 - cloud you have a set of core apis which
10:16 - let you not just monitor the data but if
10:18 - you have to take actions on your devices
10:21 - like hey i have a thermostat which is
10:23 - reporting 75 degree fahrenheit and i
10:25 - have to control it down i have to push
10:27 - it down to like 70. or maybe i have like
10:30 - a philips hue bulb right which i want to
10:32 - dim so there's an action that you need
10:34 - to take and that api call comes through
10:36 - an api gateway and then the api gateway
10:38 - communicates the same to your local hub
10:41 - which is sitting at the edge and then
10:43 - the edge
10:45 - hub translates the protocol and then
10:48 - sends out the control logic to the
10:50 - device that you want to control right so
10:52 - this is a very high level architecture
10:54 - so that's one component that we're going
10:55 - to use
10:57 - the second component is we talked about
10:58 - the express gateway so showa fans who
11:01 - has used express js
11:04 - pretty much everyone right um so brand
11:06 - new uh gateway it's an api gateway which
11:09 - was built on top of express
11:11 - was launched at a meet meet-up last week
11:14 - and highly successful and like all the
11:16 - core contributors of express like doug
11:18 - wilson and everybody is behind it um
11:21 - even from a joint perspective we are
11:23 - started pushing prs and we are actively
11:25 - contributing to it right and you have
11:27 - al's company lunch badger who is kind of
11:29 - the core maintainer and net net if
11:32 - you're really looking at uh
11:34 - you know what does the gateway do right
11:37 - you have a set of endpoints that you're
11:38 - protecting so there are functions like
11:41 - authentication authorization right you
11:44 - use a variety of tokens
11:45 - and then you have to
11:47 - configure policies right so when the
11:49 - apis come come in you set a
11:51 - you define a policies a set of policies
11:54 - which are applied on the pipeline of api
11:56 - calls coming in and if you start going
11:59 - beyond that then you have to really talk
12:01 - about role-based access control like you
12:03 - know on these apis what kind of
12:05 - permissioning you have right is it read
12:07 - write update all the cred permissions
12:09 - and then when you start going beyond
12:10 - that then you start about go around
12:12 - metering right so if you're still you
12:14 - know putting an api gateway in front of
12:16 - your
12:17 - api service you can actually meter the
12:19 - calls and you can extend this to charge
12:21 - back you can
12:22 - use extend it to throttle right saying
12:25 - hey you know what this is one public api
12:27 - endpoint and i don't want um you know
12:29 - all of these internet population hitting
12:31 - this end point maybe i can create on the
12:33 - load balanced one right so you can check
12:35 - this out i'll not go into a lot of
12:38 - details into this because we have a lot
12:39 - of ground to cover
12:41 - but this is one of the key components
12:43 - that we'll be using
12:44 - uh micro services you know
12:46 - we are here to build micro services just
12:49 - show off hands who has built a micro
12:50 - service running in production
12:54 - okay
12:55 - a few of them
12:56 - so you know i don't need to be preaching
12:58 - to the choir but essentially you know
13:00 - this is a very simple visual
13:01 - representation of what i said earlier
13:04 - you had a set of monolithic services i
13:06 - have written monolithic services myself
13:08 - traditional soa architectures where you
13:10 - had an orchestration bus and you had uh
13:12 - you know
13:13 - end points right these were essentially
13:15 - web service calls that you were making
13:17 - uh but again they were like um they were
13:20 - packaged they were not essentially like
13:23 - you didn't have a particular service
13:25 - endpoint for a particular small api call
13:28 - where you could just manage the whole
13:29 - life cycle so if that endpoint went down
13:32 - right you didn't really you know uh you
13:34 - couldn't just work on it and bring that
13:35 - back again uh you essentially had to
13:37 - work um you know recovering the entire
13:40 - uh soa tier right so
13:42 - these micro services you know right now
13:45 - the way you build it you know there are
13:47 - different variety of frameworks out
13:49 - there but once you start scaling these
13:51 - micro services um you know particularly
13:53 - on the container platforms there are
13:54 - frameworks like kubernetes any
13:56 - kubernetes users here
13:58 - this one okay so there are frameworks
14:01 - kubernetes came out of google there's
14:03 - you know docker swarm there's misos
14:05 - where you essentially can orchestrate
14:07 - right how you deploy these and how do
14:09 - you scale these um on containers right
14:12 - and this is a brand new stack right it's
14:14 - not on vms and you know these containers
14:17 - you know you could be scaling but it's
14:18 - not just scaling horizontally you have
14:21 - to account for what happens if around
14:23 - managing the life cycle of a micro
14:25 - service so we'll talk about that in a
14:27 - bit
14:28 - so this is a layout of our own cloud so
14:31 - joints entire cloud is built in
14:32 - microservices we did it like
14:35 - eight years back when nobody knew the
14:37 - term microservices uh we were built on
14:39 - ruby on rails so all of the ones that
14:41 - you see in green right whether it's
14:43 - actually at our external endpoint
14:45 - whether it's in our middleware our
14:47 - internal services or when we actually go
14:50 - ahead and access data
14:51 - we have object storage at the back end
14:54 - all of these are a set of apis and each
14:56 - api is managed by a standalone developer
14:59 - and a standalone team who can
15:00 - essentially manage the life cycle and
15:03 - all the way from development to build to
15:05 - deploy to scale and everything else
15:07 - right and we chose node.js and that was
15:09 - one of our primary drivers why we you
15:12 - know contributed and sponsored the
15:14 - project to use something for ourselves
15:16 - and then it caught on in the community
15:18 - and now it's in the foundation make
15:19 - sense
15:20 - cool
15:21 - another thing that's really coming to uh
15:23 - forte is going container native right so
15:26 - if you deploy containers on traditional
15:29 - vms right it's really a workaround
15:32 - that's not how containers were designed
15:33 - to run and that's not what will give you
15:35 - the efficiency of containers right so
15:37 - the stack that you see on the left is
15:39 - traditionally what most enterprises are
15:41 - doing today whether it's on-prem or it's
15:44 - in the public clouds and the reason is
15:46 - there was a lot of technical debt you
15:48 - had created your data centers and your
15:50 - clouds based on vms
15:52 - and you couldn't really run containers
15:54 - directly on bare metal um you could but
15:56 - on static bare metal but then you didn't
15:58 - have the flexibility of like how do you
16:00 - do cloud management and scheduling and
16:02 - all those features right
16:04 - if you really start going into container
16:05 - native and not just us there are other
16:08 - companies who are doing this as well
16:09 - instead of doing hardware virtualization
16:11 - using kvms we are going to doing os
16:14 - virtualization so we are essentially
16:16 - running a docker share
16:18 - very close to the kernel and we don't
16:20 - need any of the bin packaging libraries
16:22 - that or any of the overhead that you
16:23 - need for vms and we are running the app
16:25 - the docker app directly on that right
16:29 - cool
16:29 - another project that we are going to use
16:31 - today is the framework called as
16:33 - container pilot
16:34 - this is you know you can works with any
16:36 - scheduler if you're using swarm
16:38 - kubernetes or art triton or mesos but
16:41 - this lets you do micro orchestration so
16:43 - i'll tell you what that means
16:45 - um so when you you know create a bunch
16:47 - of micro services um
16:49 - you know it's not just start and stop
16:51 - right how and particularly when you're
16:53 - running these micro services in
16:54 - containers how do you manage the life
16:56 - cycle events so do you have you know
16:58 - health checks going into uh microservice
17:01 - how do you uh when when there is an
17:03 - event maybe there is a
17:05 - error or you know how do you handle that
17:07 - error how do you gracefully start it up
17:09 - and how do you gracefully stop it how do
17:11 - you
17:12 - understand the events when you know that
17:14 - particular micro service is choking and
17:16 - you need to provision more and scale out
17:18 - right so here's a very simple example um
17:21 - with mysql right it's a relational
17:23 - database um you would be using this in
17:26 - most projects very very widely used uh
17:28 - however like in production you don't
17:30 - want to be running one instance right
17:32 - you want to be continuously snapshotting
17:35 - your data and backing it up to some kind
17:37 - of an object general object storage
17:39 - so in our in this case what we are doing
17:41 - is we have one primary node and two
17:44 - replica nodes and we are continuously
17:47 - backing that data from the primary node
17:49 - to our manta which is essentially our
17:51 - object storage solution and you could do
17:53 - this on any cloud in s3
17:55 - but then those replica nodes um you know
17:57 - we do async replication right and you
17:59 - can set this up on your own um now and
18:03 - on what we are doing is console we are
18:05 - using it as a service discovery uh tool
18:08 - right so when a client comes in and it
18:10 - goes to query it doesn't really go
18:12 - directly to the node it goes and queries
18:14 - console and says hey console by the way
18:16 - if you don't know console it's a tool
18:18 - from hashicorp
18:20 - you know similar tools are like hcd
18:22 - where you would use them in service
18:24 - management but console will say hey i
18:26 - have these three endpoints which are
18:28 - like all my sequel this one is the
18:30 - primary likes route the traffic there
18:32 - right so where you can see here that the
18:34 - call comes to uh from console to the
18:37 - primary and you're doing heartbeats so
18:39 - you're checking hey primary are you up
18:40 - and kicking right
18:42 - and as long as that's true the traffic
18:44 - will be routed there now let's assume
18:47 - that the primary goes down right and at
18:49 - that time you have a boot sequence where
18:51 - it says hey you know boot from replica
18:53 - but when that boot from replica happens
18:55 - console needs to be updated
18:56 - automatically saying hey now read out
18:58 - the traffic and get the data from the
19:00 - replica set and when and then i have to
19:03 - make those replicas
19:05 - aware and pooling hey does the as the
19:07 - primary come back again right so to do
19:09 - this
19:11 - you can use
19:12 - you know this is an entire pattern that
19:14 - has been created for application level
19:16 - micro orchestration um so this one
19:19 - essentially is doing monitoring right so
19:21 - the way we work is there are different
19:22 - type of events there's a you know
19:24 - periodic health check but you
19:25 - essentially whenever you provision a
19:27 - micro service you register the micro
19:28 - service so there's a registration event
19:31 - you register that
19:32 - into console and during the health
19:34 - checks you are saying hey are you the
19:36 - primary yes i am um should i just back
19:39 - up my data i said no you know i'm still
19:41 - in
19:42 - processing transactions don't do it or
19:44 - yeah maybe go ahead and do it and you
19:46 - know continuously tell i'm healthy i'm
19:47 - healthy i'm healthy
19:49 - now if something goes down the system
19:51 - has to self heal right all these micro
19:53 - services should be self healing because
19:55 - there are going to be thousands of micro
19:57 - services when you eventually go into the
19:58 - enterprise you cannot do manual type
20:00 - processing like you do it for
20:02 - traditional soa based services so what
20:04 - you could do is you know on that event
20:06 - that you see hey there was a change
20:08 - right and on the change what we said
20:11 - okay if the primary send me a change
20:12 - event maybe it went down first thing you
20:14 - do is stop replication right don't
20:16 - replicate anymore because i'll be
20:18 - sending corrupt data into my backup and
20:20 - at that time look at me i'm the primary
20:22 - right or should i just you know give up
20:24 - my lock or should i still retain my lock
20:27 - um so well you know you are down so you
20:29 - let the replica attain the lock and let
20:31 - that serve up right and now if i'm
20:33 - saying okay i'm back up again
20:35 - you know go ahead and retake the lock
20:38 - right but when you do this you have to
20:40 - propagate the changes upstream to your
20:42 - load balancer and to your service
20:44 - catalog so when we actually use this in
20:46 - context of the code that you're going to
20:47 - write today
20:49 - you'll see this how helpful it is and
20:50 - how simple it's going to manage so with
20:52 - that being said i'm going to turn it
20:54 - over to wyatt to walk you through the
20:56 - exercises and i'm assuming by this time
20:59 - you have node and a local copy of docker
21:02 - installed anybody who has not been
21:04 - successful doing that yet
21:07 - okay and we are here to help you know so
21:09 - we'll be walking around the floor
21:14 - thank you shibuya
21:17 - uh
21:18 - let me plug in here
21:20 - so you all have uh have you cloned the
21:22 - repo
21:25 - cool so if you look on the readme
21:28 - it'll turn this
21:32 - let me load this up if you look on the
21:34 - readme it lists the different challenges
21:37 - the first two are really geared to make
21:38 - sure that you have node installed
21:40 - correctly and then the second one is for
21:43 - docker
21:45 - so
21:47 - i think we'll probably spend about five
21:48 - minutes on each of those first exercises
21:51 - so if you if you look at the readme
21:55 - we're just trying to spin up a simple
21:58 - node application so the challenge
22:02 - is to start it
22:04 - with a port and in this case port 8080
22:07 - which will be provided through an
22:08 - environment variable
22:10 - and once you have that up and running
22:13 - you'll you'll be able to go to your
22:15 - browser and see a chart with random data
22:18 - being displayed
22:20 - and if you have any questions
22:23 - feel free to raise your hand and we'll
22:26 - bring over a mic or just one of us will
22:28 - come and help you out
22:29 - there's also a solution file in each of
22:32 - the challenges that explains the
22:34 - solution
22:35 - so the pace of this will go
22:37 - like we'll spend five minutes on this
22:39 - first one and then i'll show you the
22:41 - solution and then we'll go to the next
22:43 - challenge and so forth
22:47 - but we want to keep this pretty laid
22:48 - back so
22:50 - again just raise your hand if you have a
22:51 - question or if you get stuck
22:55 - so i'm going to go ahead and walk
22:57 - through the solution to the first one
22:59 - it's okay if you
23:01 - didn't complete it because the remainder
23:03 - of the exercises are all using docker
23:05 - that was more of a test to make sure you
23:07 - have node installed and uh know how
23:10 - environment variables work
23:12 - so
23:13 - let's see here if i go into
23:16 - i'm going to make that larger for you
23:18 - if i go into the challenge1 folder
23:21 - so
23:23 - i have
23:24 - my
23:26 - let's see how did i set this up
23:32 - so i had already npm installed
23:35 - everything before i came here so i have
23:37 - that shortcut for me
23:40 - and then you can set environment
23:42 - variables just by pre-pending whatever
23:44 - you want before you run node so in this
23:46 - case i'm going to listen on port 8080
23:51 - and if you go into your browser you'll
23:53 - see
23:54 - let's see localhost 8080
23:57 - you should see a front end with this
23:59 - random sensor data and this is pretty
24:02 - much what we're going to see throughout
24:03 - the remainder of the exercises
24:07 - so
24:08 - while this will actually work with the
24:10 - smartthings hub
24:12 - we don't have a smartthings hub here
24:14 - especially for this many attendees
24:16 - and also the sensor data from uh
24:20 - like the real temperature data is kind
24:21 - of boring
24:23 - this is a little bit more exciting at
24:24 - least
24:26 - so
24:27 - anyway that's the solution to the first
24:29 - challenge npm install and then start
24:32 - node
24:34 - so the next challenge is using docker
24:37 - which
24:38 - is a little bit more fun
24:41 - so we want to let's go back to the
24:43 - challenge here
24:44 - so the next one we're going to
24:47 - dockerize that front end
24:50 - so
24:51 - this is
24:52 - just using that same front end that we
24:55 - had
24:56 - running it inside of a docker container
24:58 - and listening on a port again
25:01 - this will
25:02 - hopefully prove that you have docker
25:04 - installed correctly
25:06 - so again we'll give you
25:08 - say five minutes to
25:09 - work through challenge two and then i'll
25:11 - present the solution and if you get
25:14 - stuck just raise your hand
25:19 - how's everybody doing with that
25:20 - challenge is the network any better
25:24 - no
25:26 - well
25:26 - uh once you get that node alpine image
25:30 - that should speed up the next challenge
25:32 - hopefully
25:33 - so
25:35 - anyway let's walk through the solution
25:36 - here for challenge two
25:39 - so you're supposed to update
25:41 - the docker file
25:43 - so in this case we're building from
25:46 - an alpine distro of node which is
25:48 - actually the smallest one that you can
25:50 - find so
25:52 - i did that hoping that the network
25:54 - wouldn't be this slow but apparently it
25:57 - is
25:58 - so typically with your docker file on
26:00 - node you're going to copy over all your
26:01 - resources so your package
26:05 - your js files and then you'll need to
26:08 - run npm install so if you look in the
26:11 - solution here send a copy from that
26:15 - so we'll run npm install and you can see
26:18 - above this we had already established
26:20 - that the working directory
26:22 - is where our app was copied to
26:24 - and then i'm just going to run node
26:27 - period or node index or
26:30 - npm start however you want to start up
26:32 - your node process
26:34 - so that'll create the image that we need
26:38 - but we also needed the
26:40 - the
26:42 - port
26:43 - for the environment variable
26:45 - and that comes whenever we're actually
26:48 - only getting to the right folder
26:50 - and that comes whenever we're actually
26:51 - starting the container so
26:54 - i'll build the image which is super fast
26:56 - because i already have everything
26:58 - downloaded be cached
27:00 - and then you can run
27:03 - that
27:04 - image as a container
27:06 - so i'm going to set the environment
27:08 - variable with that e
27:09 - e
27:11 - and
27:12 - dash p is publish so i'm going to map
27:15 - the
27:16 - docker host port 8080
27:18 - externally and then i'm going to give it
27:20 - a friendly name
27:22 - front end
27:24 - so that is now running if i run docker
27:27 - ps i can see
27:29 - so i have that image running and port
27:31 - 8080 is mapped so again if i go to
27:34 - localhost 8080
27:36 - i should see the same front end that i
27:38 - saw whenever i had just node running
27:41 - locally
27:45 - okay so
27:47 - also in the solution it includes how to
27:50 - remove that so
27:52 - if you do docker rm that removes a
27:54 - container if you want to remove an image
27:56 - it's rmi
27:57 - so
27:58 - docker rm remove the front end
28:02 - so that's great pretty simple
28:05 - hopefully you all didn't get too stuck
28:07 - on the docker part
28:10 - so
28:10 - let's move on to challenge three where
28:12 - we're actually adding another container
28:14 - into the mix and for that we're going to
28:17 - use docker compose
28:21 - and we're also going to add in the
28:24 - express gateway which we've been talking
28:25 - about
28:27 - so
28:29 - the challenge here
28:30 - is to add
28:32 - uh
28:32 - to update the express gateway
28:34 - configuration to point to
28:37 - the two instances of the front end that
28:39 - we now have in the compose file
28:42 - so if you look in
28:44 - challenge three docker compose
28:47 - you can see that we are creating two
28:49 - front ends by having two different
28:52 - services front end one and front end two
28:55 - so we need to update the links
28:58 - for the gateway so that it has the
29:00 - correct ip addresses
29:03 - mapped inside of the gateway that
29:05 - represent the frontend one in front and
29:07 - two
29:07 - and then
29:09 - in gateway we have this configuration
29:11 - yaml file that you'll need to update
29:14 - with the
29:16 - the server and port
29:17 - for the front-end one and front-end two
29:20 - and since you're using links in this
29:22 - example
29:24 - links will update the hosts file inside
29:27 - the container with like a frontend one
29:30 - and then the ip of whatever that was
29:32 - started as
29:36 - okay so give you five minutes on this
29:39 - challenge and then i'll walk through the
29:40 - solution
29:43 - so you're going to use a different
29:44 - command also docker compose instead of
29:47 - just docker
29:50 - cool all right i'll let you get to it
29:52 - and again if you have any express
29:54 - gateway questions we have a couple of
29:56 - experts here so feel free to raise your
29:58 - hands
30:01 - all right i'm going to walk through the
30:02 - solution
30:04 - seems like things have sped up a little
30:06 - bit which is good
30:08 - so
30:10 - let's see first i'll update the links
30:13 - so just add front end one
30:16 - and front end two
30:19 - so that'll add a new etsy host entry
30:22 - for front end one with the ip address
30:26 - in the
30:27 - in the gateway container
30:29 - and if i go into here i can change the
30:32 - urls
30:33 - so this will the
30:35 - express gateway will load balance
30:38 - these two for me so
30:40 - let's see http and then the server will
30:43 - be frontend 1
30:45 - and then 8080.
30:52 - 8080 and then let's go into the right
30:54 - folder
30:57 - challenge three
31:09 - and then
31:10 - it'll every time you refresh on the
31:13 - on 8080 it'll
31:15 - be load balanced to either the front end
31:17 - one or front end two
31:18 - and it'll just serve up either of those
31:21 - whatever you get at the time
31:26 - also i wanted to remind everybody that
31:28 - well i don't know if you were reminded
31:30 - of this at all but there's t-shirts up
31:32 - here
31:33 - and stickers
31:34 - popcorn
31:36 - come on up
31:37 - feel free to grab some
31:41 - yeah you can come up any time
31:46 - we have all sizes
31:47 - men's and women's
31:49 - help yourself
31:51 - so
31:53 - yeah you have to at least have completed
31:55 - challenge three by now though
31:59 - so there you go
32:19 - the first lord is
32:48 - your size right now we'll have more
32:51 - tomorrow at the joint booth up front
32:54 - so come on by if you want a second shirt
32:58 - or whatever
33:00 - one that fits
33:56 - so
33:58 - lynx is nice whenever you're doing local
34:01 - development but when you're actually
34:02 - trying to scale to more than just a
34:04 - couple of instances it's not
34:06 - not really a sustainable solution
34:08 - so as shubra was talking about earlier
34:11 - we have container pilot
34:13 - which takes out the
34:15 - issue of having to have a load balancer
34:17 - in between all of your services
34:19 - and relies on console which is a service
34:22 - discovery catalog
34:24 - and we'll use that we'll use console
34:27 - to discover you know what services are
34:30 - up and healthy and that's how we'll
34:32 - discover
34:33 - the front end instances
34:35 - so this next challenge is just to
34:39 - rely on the container pilot
34:41 - configuration to update
34:43 - the configuration
34:44 - let me show you that
34:46 - see so if you look in the gateway folder
34:51 - we have
34:52 - an etsy
34:54 - we have this json5 file
34:57 - which is just a
34:58 - very
34:59 - javascript friendly way to do json
35:03 - it just looks like javascript
35:05 - but anyway this is how we control
35:07 - container pilot so we describe what
35:10 - processes we want container pilot to
35:12 - start up
35:13 - in this case it's the gateway and
35:17 - since we're listening on a port
35:19 - container pilot will register the
35:21 - gateway with console for us
35:24 - and it'll start our server
35:27 - so a good way to think about container
35:29 - pilot is just a really
35:31 - useful
35:32 - container in it process so you have like
35:36 - tiny and dominant already that are made
35:39 - for running in containers for doing inet
35:41 - work
35:42 - container pilot is no different except
35:44 - it adds in the extra service discovery
35:47 - and health checking that you'd want to
35:49 - have
35:51 - so your challenge is to update the
35:54 - container pilot json 5 file here to
35:59 - add in
36:00 - a job to run
36:03 - this binary which gets copied which
36:05 - generates the
36:07 - and finds the front end addresses for
36:10 - you
36:14 - so
36:15 - yeah and if you look in the readme
36:17 - there's also links for the container
36:19 - pilot documentation
36:21 - but
36:22 - anyway i'll give you
36:24 - say 10 minutes
36:25 - but your main goal is just to add
36:26 - another job here
36:28 - and then spin it up and you should be
36:30 - able to go to port 8500 to see console
36:33 - and you should also
36:35 - be able to scale the number of front-end
36:38 - instances you have to whatever number
36:39 - you want and it'll automatically update
36:42 - the gateway and the gateway will load
36:44 - balance everything for you
36:46 - so
36:47 - 10 to 15 minutes and feel free to raise
36:50 - your hand if you have any questions
36:53 - so i'm going to go ahead and jump to the
36:55 - solution on challenge 4 mainly because
36:58 - challenge 5 is much larger and requires
37:01 - a few other images
37:04 - and
37:05 - our internet is not the fastest so let
37:07 - me go ahead and do that
37:10 - so you were supposed to add in a new job
37:13 - to run
37:15 - the generate config for you whenever
37:17 - you're starting up
37:19 - so you could do it just like this
37:21 - which would work
37:23 - but ideally you would also add in this
37:25 - extra part
37:26 - which tells the gateway not to even
37:29 - start
37:30 - until after i've been able to
37:33 - see
37:34 - until after this pre-start job is
37:37 - successful
37:39 - so we won't even start the server until
37:41 - we actually have
37:44 - some front ends to balance too
37:47 - and then
37:51 - we already had a watch down here
37:54 - so
37:55 - anytime so this last job let me explain
37:57 - that sorry
37:58 - the last job explain or
38:02 - will run anytime
38:04 - there's a change with the front end so
38:07 - if there's a change in the front end
38:09 - inside of console like you added more
38:11 - instances or one of them became
38:13 - unhealthy
38:14 - then the gateway will be notified about
38:17 - that and will execute this on change
38:20 - front end job
38:22 - which will just regenerate the
38:24 - configuration for us
38:27 - which was not entirely necessary for the
38:30 - challenge but it's good to just have
38:32 - that kind of
38:34 - responsiveness built in
38:38 - so if we start it all up
38:41 - we should just have
38:44 - one instance of the front end
38:50 - and let me show you i don't know did you
38:52 - all go to a
38:53 - console here
38:55 - on port 8500
38:58 - if you didn't this is what it'll look
39:00 - like here when it loads
39:10 - so we have
39:12 - console registered here then the front
39:14 - end with two help passing checks and
39:16 - then the gateway
39:18 - and we can go to the gateway which is
39:21 - listening on port 8080
39:24 - and we just have that one front end
39:26 - that's balanced but you can also scale
39:30 - the front end to say three instances so
39:32 - we'll create two more
39:34 - instances of the front end
39:37 - and those whenever they're healthy will
39:39 - register inside of console so if we
39:42 - refresh console here you can see now we
39:44 - have six passing health checks
39:47 - for each of the instances
39:49 - and
39:50 - then the gateway would have been
39:52 - notified that there's a change in
39:53 - console and we ran that generate config
39:56 - which went out to console and grabbed
39:58 - all the healthy addresses
40:00 - so now if you refresh
40:02 - the front end you should get balance to
40:04 - another
40:06 - back end and so on and so forth
40:10 - so the next challenge really builds on
40:12 - this and adds the other services that we
40:15 - actually care about
40:16 - so that the the dummy data is not just
40:18 - coming from the front end it's actually
40:20 - coming from a serializer which pulls the
40:23 - data from a worker which then pulls it
40:27 - from the smartthings hub
40:31 - so
40:32 - we'll just move on to challenge five
40:35 - which is a little bit more involved
40:40 - let's see let me zoom out a little
40:44 - so
40:45 - there's two parts to this you're
40:46 - supposed to update the sensor
40:49 - so if you look in the folder here for
40:51 - challenge five
40:53 - you now have
40:54 - see the front end we have the serializer
40:56 - and then the sensor which is the worker
40:59 - that pulls data from nats
41:02 - which is a message queueing service
41:05 - and pushes it to the serializer so
41:07 - your job is to update
41:10 - at the bottom here we have this write
41:11 - data function
41:13 - so
41:14 - use a module called piloted which will
41:17 - talk to
41:18 - a console for you
41:20 - and we'll handle that change event
41:22 - automatically
41:24 - so
41:25 - add in here
41:28 - something that uses piloted that pulls
41:30 - down the serializer address
41:33 - and then also
41:35 - you'll need to update the
41:37 - the container pilot configuration for
41:39 - the
41:41 - sensor
41:42 - so that it has the on change for the
41:44 - serializer plugged into
41:46 - it and since this involves a few more
41:49 - images
41:51 - i think it's probably going to take
41:53 - another 20 minutes or so but
41:56 - this challenge really encompasses i
41:58 - think a lot of what we're going for in
42:00 - the overall workshop so i'm going to
42:03 - give you more time on that
42:05 - and again if you get stuck feel free to
42:06 - raise your hand or if you just have any
42:08 - general questions also raise your hand
42:13 - so i realized i left you kind of hanging
42:15 - on the what pilot it is or why you even
42:18 - use it
42:19 - sorry about that
42:21 - so piloted is just a module that will do
42:24 - the load balancing for you inside of
42:26 - your service since we don't have any
42:28 - load balancer that we're going through
42:30 - we need some way to distribute you know
42:32 - our requests
42:34 - and
42:36 - and so piloted has a pretty simple api
42:39 - you can do piloted.service and just the
42:42 - name of the service that you care to get
42:44 - so in this case the serializer and it'll
42:46 - bring back the next
42:48 - healthy
42:50 - address for you for the serializer so in
42:53 - this case
42:54 - yeah we'll do serializer
42:57 - equals that so serializer in this case
42:59 - will have the address and port
43:01 - from console of the next healthy
43:04 - instance of the serializer because we
43:06 - can scale to
43:07 - multiple instances now if we want
43:11 - and the nice thing about this
43:13 - is we actually since we don't have a
43:15 - load balancer we can tell if there are
43:17 - any healthy serializers at all
43:20 - so if there aren't any serializers
43:25 - we can
43:26 - do what you're supposed to do in a
43:28 - microservice which is
43:30 - be able to
43:31 - you know
43:32 - respond to failure so in this case let's
43:34 - say we don't have any serializers
43:36 - i could return early and maybe try this
43:39 - again in like a second
43:41 - so
43:42 - i don't know
43:44 - set timeout
43:47 - and then
43:48 - write data
43:50 - this is of course going to consume more
43:52 - memory but
43:53 - but it'll work
44:07 - so that's
44:08 - the example there
44:13 - i think that's right
44:17 - so then the other part of the challenge
44:18 - again was just to update the
44:20 - the container pilot config which i'll
44:23 - leave you to that and downloading the
44:25 - images
44:28 - but any questions on piloted or why
44:30 - you'd use it
44:31 - or does that explain it
44:33 - okay cool
44:38 - so i'll show you the rest of the
44:39 - solution in say five minutes does that
44:42 - sound good to everybody
44:46 - i'm gonna show you the solution to
44:48 - challenge
44:49 - five here and then i'm going to go
44:51 - through the next two exercises since
44:53 - we're low on time and just show you
44:55 - those solutions as well
44:56 - which builds on what we already have
44:58 - here and then i'm going to hand it back
45:00 - over to shubra to close it out
45:03 - so let's see so we already have the
45:06 - serializer
45:08 - update done
45:10 - and i was talking to colin back there
45:12 - and he was pointing out that this is
45:13 - probably not a good solution because
45:15 - it's going to buffer up the data and
45:17 - memory and eventually probably exhaust
45:21 - exhaust your memory so since since it is
45:24 - sensor data and you probably don't care
45:25 - about it so much you could just return
45:27 - early and be done with it
45:30 - the other part of this was to update the
45:32 - container pilot configuration so that it
45:35 - responds anytime there's a serializer
45:38 - change in console
45:41 - and the way we do that i'm just going to
45:43 - copy from the
45:44 - solution file
45:46 - so we'll create another job
45:50 - let's go back
45:52 - so we need to create another job
45:54 - similar to what we have here for
45:58 - nats which
46:00 - says anytime there's a change to the
46:03 - serializer
46:04 - we're going to send a sig-up
46:07 - signal to node
46:08 - and that'll not restart the process but
46:11 - it'll just send that signal and then
46:13 - piloted will handle it and go out and
46:16 - fetch an updated list of healthy
46:18 - addresses from console because something
46:21 - changed
46:24 - and the other part was to add the actual
46:26 - watcher so
46:29 - container pilot will
46:31 - use these watches to just query
46:35 - console for changes to serializer or
46:37 - gnats
46:39 - so that allows us to let me go to the
46:41 - right folder here that allows us to spin
46:44 - up new instances of the serializer or
46:46 - tear them down
46:47 - and the sensor will be able to
46:50 - handle those changes which is
46:52 - very powerful
46:54 - okay
46:55 - so
46:56 - you can do docker compose up
47:03 - oh i think i have my previous one still
47:06 - running
47:08 - so
47:14 - so kill everything
47:18 - bring it up with the
47:27 - all right i'm gonna
47:28 - kill that
47:37 - ah challenge for
47:47 - so this is a good
47:49 - good habit make sure you
47:51 - remove everything from the current
47:53 - challenge before moving on to the next
47:55 - one
47:56 - which is what i failed to do there
47:58 - so challenge five docker compose up
48:03 - dash d
48:08 - so there's quite a few more images in
48:10 - this one we have influx gnats
48:14 - if we go back to the front end
48:16 - again we can see
48:19 - in this case since we don't have all the
48:20 - workers running we should just see the
48:22 - temperature data coming in
48:25 - wait for that to load and also
48:28 - if we look at console now we have more
48:30 - services that are registered so now we
48:32 - have smart things gnats
48:37 - so yeah we have the temperature sensor
48:39 - done
48:41 - because we have that worker added
48:43 - the next challenge would be to add the
48:46 - other workers so humidity and motion
48:49 - and the way you do that is to update the
48:53 - compose file so
48:55 - if you look down here
48:57 - we have temperature
48:59 - which is using that sensors folder so we
49:02 - can change this to humidity
49:06 - so add a new service named humidity
49:08 - and we just say the sensor type is
49:11 - humidity and it'll start
49:13 - subscribing to the nets message queue
49:16 - for any humidity messages
49:19 - and it'll report those into
49:22 - the serializer so if i do up again
49:25 - since i just added humidity
49:32 - give it a second here
49:34 - actually let me go to the next challenge
49:37 - since it'll be cached
49:39 - after
49:44 - me bring it all down
49:48 - so the next challenge would just add
49:50 - those two and then you'd see the data
49:52 - flowing
49:53 - but it's just
49:55 - essentially just copying and pasting
49:57 - from the temperature
49:59 - docker compose service here
50:04 - so the last challenge which is what i'm
50:06 - going to show
50:07 - now is adding in telemetry reporting
50:11 - so container pilot
50:13 - in addition to using console for service
50:15 - discovery also has telemetry support for
50:18 - prometheus
50:20 - so you can add in any metrics that you
50:22 - care to report on
50:24 - so if you looked at challenge seven
50:27 - say the front end
50:29 - you should see there's like a memory
50:31 - script here
50:33 - which will report on how much free
50:34 - memory is inside the front end container
50:38 - and if you look at the configuration for
50:41 - that
50:42 - we now have this telemetry entry
50:46 - so
50:47 - this will listen for
50:50 - on port 9090 and prometheus will know
50:53 - that there's telemetry data because
50:55 - we're registered
50:57 - inside of console we have a new entry
51:00 - for container pilot and then prometheus
51:03 - goes out and pulls all these
51:06 - metrics down and we'll report on them
51:10 - so i want to show what that looks like
51:13 - which is actually pretty powerful
51:16 - mainly because it's so flexible
51:20 - um
51:23 - yeah i'll just do that
51:34 - so again if you refresh
51:37 - console you'll now see prometheus
51:39 - whenever it comes along
51:44 - so it might be the last to come up
51:54 - and then prometheus listens on port 9090
51:57 - and then you have these custom metrics
51:59 - that we just added
52:01 - so
52:02 - the free memory you can report on that
52:05 - and again this is completely dependent
52:07 - on whatever you want to report through
52:09 - prometheus
52:12 - and then if you scaled out the front end
52:14 - you would see more instances
52:16 - shown so
52:18 - scale front end
52:20 - say two
52:24 - and then if you refresh or execute that
52:27 - graph again you'll see more
52:30 - metrics coming through for each instance
52:35 - so
52:36 - that's that's what i have for the
52:37 - workshop i know we didn't get through
52:40 - everything
52:41 - but again you can take this home and
52:43 - and play around with it all the code's
52:45 - there
52:48 - solutions are all there
52:50 - and i also wanted to point out
52:52 - so you can deploy this to triton if you
52:55 - want in that final section
52:57 - and
52:59 - we also have another working example of
53:01 - this so if you go to
53:03 - github autopilot pattern
53:06 - we have tons of examples of using
53:08 - container pilot for solving the same
53:11 - type of problem but for other things
53:13 - like my sequel as schubru was pointing
53:15 - out we have jenkins we have wordpress
53:19 - but we also have another note example
53:21 - which is very similar to this one
53:24 - and it has a nice setup script and
53:28 - walks you through deploying everything
53:30 - to triton if you want to
53:33 - so that's what i have and i'm going to
53:35 - hand it back over to shubra to close us
53:37 - out
53:39 - and i'll be here
53:41 - throughout the week if you want to chat
53:43 - about any any of these topics or
53:46 - anything else
53:47 - thank you
53:49 - [Applause]
54:04 - um
54:14 - so
54:25 - all right folks i'm gonna wrap this up
54:27 - uh but that being said
54:29 - i think you're working towards different
54:31 - exercises and you have deployed it
54:34 - locally to your docker environment i'm
54:36 - gonna try and give you a quick overview
54:38 - of uh how we have architected uh triton
54:43 - so that if you want to push it uh to a
54:45 - real cloud and run it for yourself you
54:48 - can
54:48 - so essentially like when we build triton
54:51 - right we build it as a next generation
54:52 - of cloud and there are different ways
54:54 - you can run it right you can essentially
54:56 - build your own cloud your way uh with
54:58 - essentially open source software right
55:00 - so you can run it public you can run it
55:03 - private you can run different hybrid
55:04 - modes but there are three key components
55:07 - to it right there is compute and when we
55:09 - say compute this is essentially
55:11 - containers on bare metal and different
55:13 - containers like you know i'll go through
55:15 - that and then there is object storage
55:18 - that project is called manta so if you
55:20 - go to github um so you know github.com
55:24 - join slash triton
55:26 - that's essentially your compute and if
55:28 - you go to storage or analytics go to
55:30 - github.com
55:32 - join slash manta those two are the
55:34 - storage and converged analytics and
55:36 - converge analytics is a special project
55:39 - because if you really think of it right
55:42 - generally when people
55:44 - store object data somewhere and then
55:46 - you're running like big data jobs on
55:48 - that you essentially have to copy data
55:50 - over the network so whenever you are
55:52 - accounting for
55:53 - latency you have to take into account
55:55 - the time it takes to move data over and
55:58 - particularly if you're operating at
55:59 - petabyte exabyte level scale
56:02 - that's extremely time consuming so i'll
56:05 - tell you a little bit more
56:06 - architecturally but here's essentially a
56:08 - system architecture like how you can
56:10 - build your own cloud right
56:12 - so at the bot at the very
56:15 - lower and you will see
56:17 - the infrastructure which essentially is
56:18 - elastic bare metal what this really
56:21 - means it's os level virtualization
56:23 - instead of hardware virtualization and
56:25 - on top of it you are either running
56:27 - elastic docker host or you are running
56:29 - traditional vms right and even vms we
56:31 - essentially use kvm so you can bring
56:33 - your own kbm and then you can run any
56:35 - image on top right if you are really a
56:37 - window shop you can run windows on there
56:40 - but if you are running docker you can
56:41 - run rocker directly on the elastic
56:43 - docker host or you can bring a scheduler
56:46 - like kubernetes and make it work with
56:48 - that
56:49 - and so there are three different ways to
56:51 - run containers right so it's not like
56:54 - you know when you start speaking with
56:56 - other developers and ask them like what
56:58 - are you using to run microservices and
57:00 - containers like how do you decide um
57:04 - the first one that you see here
57:05 - kubernetes i've asked that in like
57:07 - devops conferences and show of hands
57:09 - like who's using kubernetes and 80
57:11 - percent of them raise their hand and i
57:13 - said like why right oh because you know
57:16 - my co-worker told me to right
57:19 - you really need to take a pragmatic
57:21 - approach and say um hey you know what
57:24 - are the use cases behind it right if you
57:26 - are
57:27 - building
57:28 - a micro services application just like
57:30 - you were building now and you had a few
57:32 - end points right how many endpoints did
57:34 - you have like 10 12
57:37 - those were the rest api endpoints start
57:39 - thinking about a large enterprise and a
57:41 - larger scale application where maybe you
57:43 - have a thousand endpoints and each of
57:45 - those endpoints need to run on a
57:47 - container each right and you need to
57:49 - orchestrate them at real time
57:52 - that for that kind of a heavy lifting i
57:54 - would think kubernetes type of a
57:56 - framework is a great fit but if you
57:58 - really want to run one container
58:01 - right having a small monolithic app or
58:03 - maybe 10 containers i think it's simply
58:05 - an overkill right or maybe you want to
58:07 - run stateful apps
58:10 - right databases or uh wherever you are
58:13 - caring about transient state and you
58:15 - know persistence i don't think
58:17 - kubernetes has yet nailed that right so
58:20 - that's where we kind of went category by
58:22 - category right so we created elastic
58:24 - docker host which is essentially great
58:26 - for if you're running you can run both
58:29 - stateful and stateless but it's really
58:31 - when you looked at container pilot uh
58:33 - with some of the use cases we talked
58:34 - about with mysql we are essentially
58:36 - dealing with persistent state right
58:38 - which you cannot do with a framework
58:40 - like kubernetes
58:41 - but if you really don't are really you
58:44 - know scratching the surface here and all
58:46 - you really care about is cost savings
58:48 - and performance you can just run bare
58:50 - metal but with the cloud flexibility
58:52 - right so that's where we
58:55 - have brought in this concept of
58:57 - infrastructure containers has anybody
58:59 - heard the term infrastructure container
59:01 - before this
59:02 - earlier
59:04 - this one guy yeah so if you have to do a
59:07 - little bit of reading right go search up
59:09 - uh
59:10 - lxc's right lxe containers or
59:14 - just pure infrastructure containers this
59:16 - didn't like docker didn't our containers
59:18 - didn't really arrive with docker in 2013
59:20 - 2014 people were doing containers like
59:23 - way back in 2006 and we originally used
59:27 - a project called open solaris which was
59:29 - derived from illuminos to build those
59:30 - but then there are the linux guys who
59:32 - have been using lxc containers to
59:34 - primarily achieve the same thing right
59:36 - you can have containers run them
59:38 - just like a virtualized infrastructure
59:41 - but you run them on bare metal without
59:43 - having to you know take the overhead off
59:45 - all the hardware virtualization and the
59:47 - bin packaging right
59:49 - so um another great project to look at i
59:52 - talked about manta when we get converged
59:55 - analytics
59:56 - so you can really store data at scale so
60:00 - joint like we got acquired by samsung
60:02 - and just as a use case right we are
60:05 - hosting all of samsung mobile's back in
60:07 - data and if you have to get some context
60:11 - how much how big that data looks like
60:14 - it's in exabytes a year right think
60:17 - about 250 million concurrent users
60:20 - pushing images videos what not to their
60:22 - phones and we are essentially tasking
60:25 - ourselves to handle that data right so
60:27 - one arm highly distributed open source
60:30 - object storage system so that's object
60:33 - storage but what we have done is a few
60:35 - years back
60:36 - when we ran this on a file system
60:39 - um we use zfs as the underlying flying
60:41 - sys file system and we were able to spin
60:44 - up a container in place
60:46 - uh where the object data resided
60:48 - so when you spin up a container what you
60:51 - can actually do with it is run a compute
60:53 - job right so hey i want to run a big
60:56 - data analytics right so let's say i have
60:58 - a map reduced job i want to run or maybe
61:00 - i want to do log processing or search
61:02 - text right just query the data um how do
61:05 - i do that right so if you just want to
61:07 - transcode the data hey i had these image
61:08 - files i want to create thumbnails of
61:10 - that or run any kind of arbitrary
61:12 - compute so because we spun up that
61:14 - container zone where right where the
61:16 - object data was you can essentially run
61:19 - converged analytics without having to
61:21 - move the data anywhere and when we did
61:23 - this we essentially
61:25 - followed the unix
61:27 - bible and said hey if you run unix
61:29 - commands you can
61:31 - write your own compute job you don't
61:32 - really have to learn a framework you
61:34 - could write your own compute job in awk
61:37 - right in perl or shell or any other
61:38 - language you can bring other languages
61:40 - but it's not framework dependent right
61:42 - and
61:43 - so i would highly encourage you to go to
61:46 - github check that out github.com join
61:49 - slash manta
61:51 - and net net right like with all these
61:52 - projects what we are really trying to
61:54 - build at the end of the day and we have
61:56 - built it ourselves we are trying to
61:57 - share this for the community is
61:59 - essentially build open and portable
62:01 - services right which don't have to be
62:03 - locked in into a particular framework a
62:06 - particular cloud or particular
62:08 - orchestration framework so these are
62:10 - some of the sample projects that
62:12 - got built by with container pilots so
62:15 - wyatt was just walking you through the
62:17 - github repo so check it out it's
62:19 - actually very simple to build uh
62:21 - self-managing and self-healing
62:23 - application patterns using this
62:24 - microservices and net-net i want to end
62:27 - by saying you know our goal is
62:29 - essentially to give the cloud back to
62:30 - the people and democratize the system
62:33 - and that's why we are here so you know
62:35 - we'll be at uh the booth in the rest of
62:38 - the conference swing by our boot talk to
62:40 - us and you know if you have not got any
62:42 - swag you know feel free to grab them i
62:44 - want to call out uh the express gateway
62:46 - guys one more time
62:48 - al sang and irfan and thanks colin and
62:52 - wyatt and fires for showing up and
62:55 - helping thank you
62:57 - [Applause]

Cleaned transcript:

i'm shubra i run product marketing and developer relations at joint and i'll be joined by quite a few developers uh wyatt show fans wyatt is my colleague is our lead node.js engineer and he has been a active contributor to different frameworks like happy seneca um and you know even for the microservices that we run ourselves at joint and even myself i've been in the express community and i've worked on frameworks like loopback and api gateways right so just to give you a context so the workshop today is going to be focused on building an iot app and we'll be using a variety of technologies i'll walk you to that stack but before i do that um you know want to put a disclaimer we are not going to be talking any proprietary or closed source products at joint ourselves we have been the progenitor of node we are kind of the first to incubate node.js and we are also the founding member of cncf and we believe that the entire cloud and all the technologies running within it should be in the open right and we are entirely community driven so with that um i want to you know give you our point of view on how we see the legacy stack and the modern stack and this is based on people we are working in the community our customers as well so if you look at the legacy stack right these were traditionally monolithic web apps and we're running code on different kind of blocking and bloated platforms right you could say jvm clrs what they wear and traditionally these were deployed in virtual machines right if you go back like 10 years and running inside a private data center now if you flip the stack like when you talk about modernization the app services are more message driven there are more micro services they are built for mobile first right you don't really build for you know they are build responsive they are not just built for the web you are building them for iot workloads as well as artificial intelligence workloads right and we can go to dirt i'm just gonna skim through a lot of these topics from a coding perspective you know you'll see the rise of node.js golang and a lot of different lightweight runtime platforms or languages the whole idea is like can you support concurrency can they work with async workloads right and from a runtime perspective you know if you take the case of microservices there's a perfect fit where you want to run them on containers if you take the case instance of node.js most of the processes are running single threaded running them on a traditional vm is probably an overkill so you probably want to just package it run it you know in package it in an immutable fashion push it to the cloud run it on a lightweight container right and the infrastructure supporting that is you know that's kind of you know i don't have to preach on that that's essentially cloud right now but there are differences in the cloud right so some of the projects we work on and we'll be discussing these in the framework of this workshop if you look at node we work on node core rustify these are restfusa framework d traces essentially a universal debugging tool that you can use in production happy you know it's another api framework mdb was again for memory leak debugging we're also going to talk about express gateway which was recently launched as recent as last week and again we have been investing we are polyglot ourselves we are not a hundred percent note shop so if you look at tooling and live projects that we are working and particularly in the golang community these are around terraform around um you know particularly for provisioning our containers for console for service management vault for security prometheus for monitoring uh packer and crypto and other core libraries and on the container side you know we started with containers 10 years back right we were running infrastructure or bare metal containers and then we put a docker but we had to make it run in production not like a mickey mouse like standalone project so we created the concept of an elastic docker host where we abstract the entire data center into a single docker host with a single docker api endpoint and you can just push based on a compose file or any artifact you have you can essentially push your application blueprint and get it deployed in matter of minutes and obviously we work on kubernetes and we have also created a micro app orchestration framework called as container pilot we are going to be using container pilot today right and finally on the cloud you know obviously our because our entire cloud is 100 open source we operate our public clouds but most of our users also deploy it on premises in their own private cloud managed private cloud settings and it's primary due to cost and performance so let's go into the details so there's a github link if you want to grab that url i'll be on this slide for a few seconds that's has all the contents of the code and the exercises so it's github.com joy slash joint slash summit workshop and just a show of hands if anybody was not able to get that all right perfect there are some prerequisites for this workshop we'll be primarily be using node.js so you'll have to install node.js if you haven't already you should while i'm and you can do that like when i'm walking through the rest of the slides uh you need to at least have a local docker install i think those are the primary uh prereqs and optional uh you know if you really want to push this out like once you test it on your laptop if you want to push this out into a public cloud um you can essentially sign up for joints public cloud we have 250 free credits and we won't be charging anything for the workshop right so but again that's optional if you really want to test it out like how it works in the cloud make sense so i'll just hang on to this slide for another few seconds so that you have the urls perfect all right so what are we going to build today we are going to build an iot app and it's a highly distributed micro service app uh we'll have a variety of uh end points that we'll consume data from so you know we'll be using um sensor data and out of the sensor data we'll be using a home automation platform called as smart things and smart things you know sells you sensors for you know motion sensors humidity sensors what not and once we get all that data we'll be uh monitoring uh the data feed that is coming in we'll be storing that into a database that's where we'll use influx db and we'll also gather some monitoring data for using agents like prometheus and then we have a variety of other components you know including an api gateway including redis and then we will all of these services um that are interconnected with each other uh each of them is running on a container of its own and we will orchestrate this and we will scale it out right so we'll start from scratch build you know test integrate deploy and scale okay so just few of the technology stacks that we'll be covering today right if you want to take a picture you could so at the very top right because we are consuming data from a smart things sensor and like we are not going to use real smart things we are going to simulate that data right but it's kind of the same experience if you had to get it from a real home automation system right to protect all these apis we are going to use a frontend component called as the express gateway and i'll get into the details of that this project was launched last week and we have uh irfan from the gateway team just raise your hands so you know if you have followup questions uh you can reach out to him on the express gateway we also have al al sang from the express gateway team so you know they're here to help and again with me i have wyatt and we have colin um his hands and we have fires as well so if you have any node questions or container questions you know reach out obviously we'll use node we'll be using frameworks like happy and seneca seneca is a microservices framework all written in node.js uh from a backend perspective like from the front end essentially we have prometheus endpoints and on the backend we'll be using influx db to store time series data and from a container perspective we'll be running traditional docker however to wire uh our entire application blueprint and to deploy it and scale it we'll be using container pilot and finally you know the option where we push it to the public cloud we'll be using uh triton from joint so when i get into all of these let's quickly run through some of these key concepts right i just showed you a bunch of technology and then we'll start really working the exercises so just architecturally how is smart things laid out right so it has a bunch of sensors and if like every um you know iot home device manufacturer this is not like industrial iot this is really home automation you have a hub right and the traditional way these connects uh there are a variety of protocols there are zigbee protocols and whatnot uh where in all of these devices that are present at your home they connect to a hub right and based on those connected connection endpoints then you essentially have device handlers and then you send up the data through an api gateway and that api gateway is consuming your events data and pushing it to a cloud processing system and on the cloud side and this cloud could be running anywhere traditionally it's a public cloud you have a set of core apis which let you not just monitor the data but if you have to take actions on your devices like hey i have a thermostat which is reporting 75 degree fahrenheit and i have to control it down i have to push it down to like 70. or maybe i have like a philips hue bulb right which i want to dim so there's an action that you need to take and that api call comes through an api gateway and then the api gateway communicates the same to your local hub which is sitting at the edge and then the edge hub translates the protocol and then sends out the control logic to the device that you want to control right so this is a very high level architecture so that's one component that we're going to use the second component is we talked about the express gateway so showa fans who has used express js pretty much everyone right um so brand new uh gateway it's an api gateway which was built on top of express was launched at a meet meetup last week and highly successful and like all the core contributors of express like doug wilson and everybody is behind it um even from a joint perspective we are started pushing prs and we are actively contributing to it right and you have al's company lunch badger who is kind of the core maintainer and net net if you're really looking at uh you know what does the gateway do right you have a set of endpoints that you're protecting so there are functions like authentication authorization right you use a variety of tokens and then you have to configure policies right so when the apis come come in you set a you define a policies a set of policies which are applied on the pipeline of api calls coming in and if you start going beyond that then you have to really talk about rolebased access control like you know on these apis what kind of permissioning you have right is it read write update all the cred permissions and then when you start going beyond that then you start about go around metering right so if you're still you know putting an api gateway in front of your api service you can actually meter the calls and you can extend this to charge back you can use extend it to throttle right saying hey you know what this is one public api endpoint and i don't want um you know all of these internet population hitting this end point maybe i can create on the load balanced one right so you can check this out i'll not go into a lot of details into this because we have a lot of ground to cover but this is one of the key components that we'll be using uh micro services you know we are here to build micro services just show off hands who has built a micro service running in production okay a few of them so you know i don't need to be preaching to the choir but essentially you know this is a very simple visual representation of what i said earlier you had a set of monolithic services i have written monolithic services myself traditional soa architectures where you had an orchestration bus and you had uh you know end points right these were essentially web service calls that you were making uh but again they were like um they were packaged they were not essentially like you didn't have a particular service endpoint for a particular small api call where you could just manage the whole life cycle so if that endpoint went down right you didn't really you know uh you couldn't just work on it and bring that back again uh you essentially had to work um you know recovering the entire uh soa tier right so these micro services you know right now the way you build it you know there are different variety of frameworks out there but once you start scaling these micro services um you know particularly on the container platforms there are frameworks like kubernetes any kubernetes users here this one okay so there are frameworks kubernetes came out of google there's you know docker swarm there's misos where you essentially can orchestrate right how you deploy these and how do you scale these um on containers right and this is a brand new stack right it's not on vms and you know these containers you know you could be scaling but it's not just scaling horizontally you have to account for what happens if around managing the life cycle of a micro service so we'll talk about that in a bit so this is a layout of our own cloud so joints entire cloud is built in microservices we did it like eight years back when nobody knew the term microservices uh we were built on ruby on rails so all of the ones that you see in green right whether it's actually at our external endpoint whether it's in our middleware our internal services or when we actually go ahead and access data we have object storage at the back end all of these are a set of apis and each api is managed by a standalone developer and a standalone team who can essentially manage the life cycle and all the way from development to build to deploy to scale and everything else right and we chose node.js and that was one of our primary drivers why we you know contributed and sponsored the project to use something for ourselves and then it caught on in the community and now it's in the foundation make sense cool another thing that's really coming to uh forte is going container native right so if you deploy containers on traditional vms right it's really a workaround that's not how containers were designed to run and that's not what will give you the efficiency of containers right so the stack that you see on the left is traditionally what most enterprises are doing today whether it's onprem or it's in the public clouds and the reason is there was a lot of technical debt you had created your data centers and your clouds based on vms and you couldn't really run containers directly on bare metal um you could but on static bare metal but then you didn't have the flexibility of like how do you do cloud management and scheduling and all those features right if you really start going into container native and not just us there are other companies who are doing this as well instead of doing hardware virtualization using kvms we are going to doing os virtualization so we are essentially running a docker share very close to the kernel and we don't need any of the bin packaging libraries that or any of the overhead that you need for vms and we are running the app the docker app directly on that right cool another project that we are going to use today is the framework called as container pilot this is you know you can works with any scheduler if you're using swarm kubernetes or art triton or mesos but this lets you do micro orchestration so i'll tell you what that means um so when you you know create a bunch of micro services um you know it's not just start and stop right how and particularly when you're running these micro services in containers how do you manage the life cycle events so do you have you know health checks going into uh microservice how do you uh when when there is an event maybe there is a error or you know how do you handle that error how do you gracefully start it up and how do you gracefully stop it how do you understand the events when you know that particular micro service is choking and you need to provision more and scale out right so here's a very simple example um with mysql right it's a relational database um you would be using this in most projects very very widely used uh however like in production you don't want to be running one instance right you want to be continuously snapshotting your data and backing it up to some kind of an object general object storage so in our in this case what we are doing is we have one primary node and two replica nodes and we are continuously backing that data from the primary node to our manta which is essentially our object storage solution and you could do this on any cloud in s3 but then those replica nodes um you know we do async replication right and you can set this up on your own um now and on what we are doing is console we are using it as a service discovery uh tool right so when a client comes in and it goes to query it doesn't really go directly to the node it goes and queries console and says hey console by the way if you don't know console it's a tool from hashicorp you know similar tools are like hcd where you would use them in service management but console will say hey i have these three endpoints which are like all my sequel this one is the primary likes route the traffic there right so where you can see here that the call comes to uh from console to the primary and you're doing heartbeats so you're checking hey primary are you up and kicking right and as long as that's true the traffic will be routed there now let's assume that the primary goes down right and at that time you have a boot sequence where it says hey you know boot from replica but when that boot from replica happens console needs to be updated automatically saying hey now read out the traffic and get the data from the replica set and when and then i have to make those replicas aware and pooling hey does the as the primary come back again right so to do this you can use you know this is an entire pattern that has been created for application level micro orchestration um so this one essentially is doing monitoring right so the way we work is there are different type of events there's a you know periodic health check but you essentially whenever you provision a micro service you register the micro service so there's a registration event you register that into console and during the health checks you are saying hey are you the primary yes i am um should i just back up my data i said no you know i'm still in processing transactions don't do it or yeah maybe go ahead and do it and you know continuously tell i'm healthy i'm healthy i'm healthy now if something goes down the system has to self heal right all these micro services should be self healing because there are going to be thousands of micro services when you eventually go into the enterprise you cannot do manual type processing like you do it for traditional soa based services so what you could do is you know on that event that you see hey there was a change right and on the change what we said okay if the primary send me a change event maybe it went down first thing you do is stop replication right don't replicate anymore because i'll be sending corrupt data into my backup and at that time look at me i'm the primary right or should i just you know give up my lock or should i still retain my lock um so well you know you are down so you let the replica attain the lock and let that serve up right and now if i'm saying okay i'm back up again you know go ahead and retake the lock right but when you do this you have to propagate the changes upstream to your load balancer and to your service catalog so when we actually use this in context of the code that you're going to write today you'll see this how helpful it is and how simple it's going to manage so with that being said i'm going to turn it over to wyatt to walk you through the exercises and i'm assuming by this time you have node and a local copy of docker installed anybody who has not been successful doing that yet okay and we are here to help you know so we'll be walking around the floor thank you shibuya uh let me plug in here so you all have uh have you cloned the repo cool so if you look on the readme it'll turn this let me load this up if you look on the readme it lists the different challenges the first two are really geared to make sure that you have node installed correctly and then the second one is for docker so i think we'll probably spend about five minutes on each of those first exercises so if you if you look at the readme we're just trying to spin up a simple node application so the challenge is to start it with a port and in this case port 8080 which will be provided through an environment variable and once you have that up and running you'll you'll be able to go to your browser and see a chart with random data being displayed and if you have any questions feel free to raise your hand and we'll bring over a mic or just one of us will come and help you out there's also a solution file in each of the challenges that explains the solution so the pace of this will go like we'll spend five minutes on this first one and then i'll show you the solution and then we'll go to the next challenge and so forth but we want to keep this pretty laid back so again just raise your hand if you have a question or if you get stuck so i'm going to go ahead and walk through the solution to the first one it's okay if you didn't complete it because the remainder of the exercises are all using docker that was more of a test to make sure you have node installed and uh know how environment variables work so let's see here if i go into i'm going to make that larger for you if i go into the challenge1 folder so i have my let's see how did i set this up so i had already npm installed everything before i came here so i have that shortcut for me and then you can set environment variables just by prepending whatever you want before you run node so in this case i'm going to listen on port 8080 and if you go into your browser you'll see let's see localhost 8080 you should see a front end with this random sensor data and this is pretty much what we're going to see throughout the remainder of the exercises so while this will actually work with the smartthings hub we don't have a smartthings hub here especially for this many attendees and also the sensor data from uh like the real temperature data is kind of boring this is a little bit more exciting at least so anyway that's the solution to the first challenge npm install and then start node so the next challenge is using docker which is a little bit more fun so we want to let's go back to the challenge here so the next one we're going to dockerize that front end so this is just using that same front end that we had running it inside of a docker container and listening on a port again this will hopefully prove that you have docker installed correctly so again we'll give you say five minutes to work through challenge two and then i'll present the solution and if you get stuck just raise your hand how's everybody doing with that challenge is the network any better no well uh once you get that node alpine image that should speed up the next challenge hopefully so anyway let's walk through the solution here for challenge two so you're supposed to update the docker file so in this case we're building from an alpine distro of node which is actually the smallest one that you can find so i did that hoping that the network wouldn't be this slow but apparently it is so typically with your docker file on node you're going to copy over all your resources so your package your js files and then you'll need to run npm install so if you look in the solution here send a copy from that so we'll run npm install and you can see above this we had already established that the working directory is where our app was copied to and then i'm just going to run node period or node index or npm start however you want to start up your node process so that'll create the image that we need but we also needed the the port for the environment variable and that comes whenever we're actually only getting to the right folder and that comes whenever we're actually starting the container so i'll build the image which is super fast because i already have everything downloaded be cached and then you can run that image as a container so i'm going to set the environment variable with that e e and dash p is publish so i'm going to map the docker host port 8080 externally and then i'm going to give it a friendly name front end so that is now running if i run docker ps i can see so i have that image running and port 8080 is mapped so again if i go to localhost 8080 i should see the same front end that i saw whenever i had just node running locally okay so also in the solution it includes how to remove that so if you do docker rm that removes a container if you want to remove an image it's rmi so docker rm remove the front end so that's great pretty simple hopefully you all didn't get too stuck on the docker part so let's move on to challenge three where we're actually adding another container into the mix and for that we're going to use docker compose and we're also going to add in the express gateway which we've been talking about so the challenge here is to add uh to update the express gateway configuration to point to the two instances of the front end that we now have in the compose file so if you look in challenge three docker compose you can see that we are creating two front ends by having two different services front end one and front end two so we need to update the links for the gateway so that it has the correct ip addresses mapped inside of the gateway that represent the frontend one in front and two and then in gateway we have this configuration yaml file that you'll need to update with the the server and port for the frontend one and frontend two and since you're using links in this example links will update the hosts file inside the container with like a frontend one and then the ip of whatever that was started as okay so give you five minutes on this challenge and then i'll walk through the solution so you're going to use a different command also docker compose instead of just docker cool all right i'll let you get to it and again if you have any express gateway questions we have a couple of experts here so feel free to raise your hands all right i'm going to walk through the solution seems like things have sped up a little bit which is good so let's see first i'll update the links so just add front end one and front end two so that'll add a new etsy host entry for front end one with the ip address in the in the gateway container and if i go into here i can change the urls so this will the express gateway will load balance these two for me so let's see http and then the server will be frontend 1 and then 8080. 8080 and then let's go into the right folder challenge three and then it'll every time you refresh on the on 8080 it'll be load balanced to either the front end one or front end two and it'll just serve up either of those whatever you get at the time also i wanted to remind everybody that well i don't know if you were reminded of this at all but there's tshirts up here and stickers popcorn come on up feel free to grab some yeah you can come up any time we have all sizes men's and women's help yourself so yeah you have to at least have completed challenge three by now though so there you go the first lord is your size right now we'll have more tomorrow at the joint booth up front so come on by if you want a second shirt or whatever one that fits so lynx is nice whenever you're doing local development but when you're actually trying to scale to more than just a couple of instances it's not not really a sustainable solution so as shubra was talking about earlier we have container pilot which takes out the issue of having to have a load balancer in between all of your services and relies on console which is a service discovery catalog and we'll use that we'll use console to discover you know what services are up and healthy and that's how we'll discover the front end instances so this next challenge is just to rely on the container pilot configuration to update the configuration let me show you that see so if you look in the gateway folder we have an etsy we have this json5 file which is just a very javascript friendly way to do json it just looks like javascript but anyway this is how we control container pilot so we describe what processes we want container pilot to start up in this case it's the gateway and since we're listening on a port container pilot will register the gateway with console for us and it'll start our server so a good way to think about container pilot is just a really useful container in it process so you have like tiny and dominant already that are made for running in containers for doing inet work container pilot is no different except it adds in the extra service discovery and health checking that you'd want to have so your challenge is to update the container pilot json 5 file here to add in a job to run this binary which gets copied which generates the and finds the front end addresses for you so yeah and if you look in the readme there's also links for the container pilot documentation but anyway i'll give you say 10 minutes but your main goal is just to add another job here and then spin it up and you should be able to go to port 8500 to see console and you should also be able to scale the number of frontend instances you have to whatever number you want and it'll automatically update the gateway and the gateway will load balance everything for you so 10 to 15 minutes and feel free to raise your hand if you have any questions so i'm going to go ahead and jump to the solution on challenge 4 mainly because challenge 5 is much larger and requires a few other images and our internet is not the fastest so let me go ahead and do that so you were supposed to add in a new job to run the generate config for you whenever you're starting up so you could do it just like this which would work but ideally you would also add in this extra part which tells the gateway not to even start until after i've been able to see until after this prestart job is successful so we won't even start the server until we actually have some front ends to balance too and then we already had a watch down here so anytime so this last job let me explain that sorry the last job explain or will run anytime there's a change with the front end so if there's a change in the front end inside of console like you added more instances or one of them became unhealthy then the gateway will be notified about that and will execute this on change front end job which will just regenerate the configuration for us which was not entirely necessary for the challenge but it's good to just have that kind of responsiveness built in so if we start it all up we should just have one instance of the front end and let me show you i don't know did you all go to a console here on port 8500 if you didn't this is what it'll look like here when it loads so we have console registered here then the front end with two help passing checks and then the gateway and we can go to the gateway which is listening on port 8080 and we just have that one front end that's balanced but you can also scale the front end to say three instances so we'll create two more instances of the front end and those whenever they're healthy will register inside of console so if we refresh console here you can see now we have six passing health checks for each of the instances and then the gateway would have been notified that there's a change in console and we ran that generate config which went out to console and grabbed all the healthy addresses so now if you refresh the front end you should get balance to another back end and so on and so forth so the next challenge really builds on this and adds the other services that we actually care about so that the the dummy data is not just coming from the front end it's actually coming from a serializer which pulls the data from a worker which then pulls it from the smartthings hub so we'll just move on to challenge five which is a little bit more involved let's see let me zoom out a little so there's two parts to this you're supposed to update the sensor so if you look in the folder here for challenge five you now have see the front end we have the serializer and then the sensor which is the worker that pulls data from nats which is a message queueing service and pushes it to the serializer so your job is to update at the bottom here we have this write data function so use a module called piloted which will talk to a console for you and we'll handle that change event automatically so add in here something that uses piloted that pulls down the serializer address and then also you'll need to update the the container pilot configuration for the sensor so that it has the on change for the serializer plugged into it and since this involves a few more images i think it's probably going to take another 20 minutes or so but this challenge really encompasses i think a lot of what we're going for in the overall workshop so i'm going to give you more time on that and again if you get stuck feel free to raise your hand or if you just have any general questions also raise your hand so i realized i left you kind of hanging on the what pilot it is or why you even use it sorry about that so piloted is just a module that will do the load balancing for you inside of your service since we don't have any load balancer that we're going through we need some way to distribute you know our requests and and so piloted has a pretty simple api you can do piloted.service and just the name of the service that you care to get so in this case the serializer and it'll bring back the next healthy address for you for the serializer so in this case yeah we'll do serializer equals that so serializer in this case will have the address and port from console of the next healthy instance of the serializer because we can scale to multiple instances now if we want and the nice thing about this is we actually since we don't have a load balancer we can tell if there are any healthy serializers at all so if there aren't any serializers we can do what you're supposed to do in a microservice which is be able to you know respond to failure so in this case let's say we don't have any serializers i could return early and maybe try this again in like a second so i don't know set timeout and then write data this is of course going to consume more memory but but it'll work so that's the example there i think that's right so then the other part of the challenge again was just to update the the container pilot config which i'll leave you to that and downloading the images but any questions on piloted or why you'd use it or does that explain it okay cool so i'll show you the rest of the solution in say five minutes does that sound good to everybody i'm gonna show you the solution to challenge five here and then i'm going to go through the next two exercises since we're low on time and just show you those solutions as well which builds on what we already have here and then i'm going to hand it back over to shubra to close it out so let's see so we already have the serializer update done and i was talking to colin back there and he was pointing out that this is probably not a good solution because it's going to buffer up the data and memory and eventually probably exhaust exhaust your memory so since since it is sensor data and you probably don't care about it so much you could just return early and be done with it the other part of this was to update the container pilot configuration so that it responds anytime there's a serializer change in console and the way we do that i'm just going to copy from the solution file so we'll create another job let's go back so we need to create another job similar to what we have here for nats which says anytime there's a change to the serializer we're going to send a sigup signal to node and that'll not restart the process but it'll just send that signal and then piloted will handle it and go out and fetch an updated list of healthy addresses from console because something changed and the other part was to add the actual watcher so container pilot will use these watches to just query console for changes to serializer or gnats so that allows us to let me go to the right folder here that allows us to spin up new instances of the serializer or tear them down and the sensor will be able to handle those changes which is very powerful okay so you can do docker compose up oh i think i have my previous one still running so so kill everything bring it up with the all right i'm gonna kill that ah challenge for so this is a good good habit make sure you remove everything from the current challenge before moving on to the next one which is what i failed to do there so challenge five docker compose up dash d so there's quite a few more images in this one we have influx gnats if we go back to the front end again we can see in this case since we don't have all the workers running we should just see the temperature data coming in wait for that to load and also if we look at console now we have more services that are registered so now we have smart things gnats so yeah we have the temperature sensor done because we have that worker added the next challenge would be to add the other workers so humidity and motion and the way you do that is to update the compose file so if you look down here we have temperature which is using that sensors folder so we can change this to humidity so add a new service named humidity and we just say the sensor type is humidity and it'll start subscribing to the nets message queue for any humidity messages and it'll report those into the serializer so if i do up again since i just added humidity give it a second here actually let me go to the next challenge since it'll be cached after me bring it all down so the next challenge would just add those two and then you'd see the data flowing but it's just essentially just copying and pasting from the temperature docker compose service here so the last challenge which is what i'm going to show now is adding in telemetry reporting so container pilot in addition to using console for service discovery also has telemetry support for prometheus so you can add in any metrics that you care to report on so if you looked at challenge seven say the front end you should see there's like a memory script here which will report on how much free memory is inside the front end container and if you look at the configuration for that we now have this telemetry entry so this will listen for on port 9090 and prometheus will know that there's telemetry data because we're registered inside of console we have a new entry for container pilot and then prometheus goes out and pulls all these metrics down and we'll report on them so i want to show what that looks like which is actually pretty powerful mainly because it's so flexible um yeah i'll just do that so again if you refresh console you'll now see prometheus whenever it comes along so it might be the last to come up and then prometheus listens on port 9090 and then you have these custom metrics that we just added so the free memory you can report on that and again this is completely dependent on whatever you want to report through prometheus and then if you scaled out the front end you would see more instances shown so scale front end say two and then if you refresh or execute that graph again you'll see more metrics coming through for each instance so that's that's what i have for the workshop i know we didn't get through everything but again you can take this home and and play around with it all the code's there solutions are all there and i also wanted to point out so you can deploy this to triton if you want in that final section and we also have another working example of this so if you go to github autopilot pattern we have tons of examples of using container pilot for solving the same type of problem but for other things like my sequel as schubru was pointing out we have jenkins we have wordpress but we also have another note example which is very similar to this one and it has a nice setup script and walks you through deploying everything to triton if you want to so that's what i have and i'm going to hand it back over to shubra to close us out and i'll be here throughout the week if you want to chat about any any of these topics or anything else thank you um so all right folks i'm gonna wrap this up uh but that being said i think you're working towards different exercises and you have deployed it locally to your docker environment i'm gonna try and give you a quick overview of uh how we have architected uh triton so that if you want to push it uh to a real cloud and run it for yourself you can so essentially like when we build triton right we build it as a next generation of cloud and there are different ways you can run it right you can essentially build your own cloud your way uh with essentially open source software right so you can run it public you can run it private you can run different hybrid modes but there are three key components to it right there is compute and when we say compute this is essentially containers on bare metal and different containers like you know i'll go through that and then there is object storage that project is called manta so if you go to github um so you know github.com join slash triton that's essentially your compute and if you go to storage or analytics go to github.com join slash manta those two are the storage and converged analytics and converge analytics is a special project because if you really think of it right generally when people store object data somewhere and then you're running like big data jobs on that you essentially have to copy data over the network so whenever you are accounting for latency you have to take into account the time it takes to move data over and particularly if you're operating at petabyte exabyte level scale that's extremely time consuming so i'll tell you a little bit more architecturally but here's essentially a system architecture like how you can build your own cloud right so at the bot at the very lower and you will see the infrastructure which essentially is elastic bare metal what this really means it's os level virtualization instead of hardware virtualization and on top of it you are either running elastic docker host or you are running traditional vms right and even vms we essentially use kvm so you can bring your own kbm and then you can run any image on top right if you are really a window shop you can run windows on there but if you are running docker you can run rocker directly on the elastic docker host or you can bring a scheduler like kubernetes and make it work with that and so there are three different ways to run containers right so it's not like you know when you start speaking with other developers and ask them like what are you using to run microservices and containers like how do you decide um the first one that you see here kubernetes i've asked that in like devops conferences and show of hands like who's using kubernetes and 80 percent of them raise their hand and i said like why right oh because you know my coworker told me to right you really need to take a pragmatic approach and say um hey you know what are the use cases behind it right if you are building a micro services application just like you were building now and you had a few end points right how many endpoints did you have like 10 12 those were the rest api endpoints start thinking about a large enterprise and a larger scale application where maybe you have a thousand endpoints and each of those endpoints need to run on a container each right and you need to orchestrate them at real time that for that kind of a heavy lifting i would think kubernetes type of a framework is a great fit but if you really want to run one container right having a small monolithic app or maybe 10 containers i think it's simply an overkill right or maybe you want to run stateful apps right databases or uh wherever you are caring about transient state and you know persistence i don't think kubernetes has yet nailed that right so that's where we kind of went category by category right so we created elastic docker host which is essentially great for if you're running you can run both stateful and stateless but it's really when you looked at container pilot uh with some of the use cases we talked about with mysql we are essentially dealing with persistent state right which you cannot do with a framework like kubernetes but if you really don't are really you know scratching the surface here and all you really care about is cost savings and performance you can just run bare metal but with the cloud flexibility right so that's where we have brought in this concept of infrastructure containers has anybody heard the term infrastructure container before this earlier this one guy yeah so if you have to do a little bit of reading right go search up uh lxc's right lxe containers or just pure infrastructure containers this didn't like docker didn't our containers didn't really arrive with docker in 2013 2014 people were doing containers like way back in 2006 and we originally used a project called open solaris which was derived from illuminos to build those but then there are the linux guys who have been using lxc containers to primarily achieve the same thing right you can have containers run them just like a virtualized infrastructure but you run them on bare metal without having to you know take the overhead off all the hardware virtualization and the bin packaging right so um another great project to look at i talked about manta when we get converged analytics so you can really store data at scale so joint like we got acquired by samsung and just as a use case right we are hosting all of samsung mobile's back in data and if you have to get some context how much how big that data looks like it's in exabytes a year right think about 250 million concurrent users pushing images videos what not to their phones and we are essentially tasking ourselves to handle that data right so one arm highly distributed open source object storage system so that's object storage but what we have done is a few years back when we ran this on a file system um we use zfs as the underlying flying sys file system and we were able to spin up a container in place uh where the object data resided so when you spin up a container what you can actually do with it is run a compute job right so hey i want to run a big data analytics right so let's say i have a map reduced job i want to run or maybe i want to do log processing or search text right just query the data um how do i do that right so if you just want to transcode the data hey i had these image files i want to create thumbnails of that or run any kind of arbitrary compute so because we spun up that container zone where right where the object data was you can essentially run converged analytics without having to move the data anywhere and when we did this we essentially followed the unix bible and said hey if you run unix commands you can write your own compute job you don't really have to learn a framework you could write your own compute job in awk right in perl or shell or any other language you can bring other languages but it's not framework dependent right and so i would highly encourage you to go to github check that out github.com join slash manta and net net right like with all these projects what we are really trying to build at the end of the day and we have built it ourselves we are trying to share this for the community is essentially build open and portable services right which don't have to be locked in into a particular framework a particular cloud or particular orchestration framework so these are some of the sample projects that got built by with container pilots so wyatt was just walking you through the github repo so check it out it's actually very simple to build uh selfmanaging and selfhealing application patterns using this microservices and netnet i want to end by saying you know our goal is essentially to give the cloud back to the people and democratize the system and that's why we are here so you know we'll be at uh the booth in the rest of the conference swing by our boot talk to us and you know if you have not got any swag you know feel free to grab them i want to call out uh the express gateway guys one more time al sang and irfan and thanks colin and wyatt and fires for showing up and helping thank you
