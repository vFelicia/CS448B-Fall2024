With timestamps:

00:00 - this is a non-technical intro to
00:01 - generative AI you'll learn about the
00:03 - evolution of AI capabilities and
00:06 - analyzing the key technological
00:08 - breakthroughs that have enabled modern
00:10 - generative AI models to achieve
00:12 - remarkable performance you learn about
00:14 - the different levels of llm applications
00:17 - like Q&A systems chatbots rag Solutions
00:20 - and how large language models can be
00:22 - leveraged for Downstream natural
00:23 - language processing tasks and the
00:26 - development of intelligent AI agents
00:29 - you'll also learn about the potential of
00:31 - large language model operating systems
00:34 - Abdul created this
00:35 - [Music]
00:39 - course we call as generative AI is not
00:43 - too old in fact it has been just last
00:46 - couple of years so first let's take a
00:49 - look at what has changed how have we
00:51 - reached here in this particular place
00:54 - and what kind of challenges lie ahead of
00:56 - us this is very important before you
01:00 - actually start using the tool in itself
01:03 - imagine this is like the manual that
01:04 - tells you what you should do with the
01:06 - tool and what the tool is not good for
01:09 - without this knowledge it's particularly
01:12 - very scary to have a tool which you
01:15 - believe could be great but maybe it is
01:17 - not I'm not saying that generative AI is
01:21 - bad but I'm just saying that you need to
01:22 - know how did we get here and that is the
01:25 - whole point of this particular section
01:28 - first of all why the name generative AI
01:31 - few years back whenever we said
01:34 - something as AI it was mostly a niche
01:38 - Within mission learning or deep learning
01:40 - for example if you take text you would
01:43 - have noticed that few years back we had
01:45 - something called
01:46 - ner that stands for named entity
01:50 - recognition if you have got a bunch of
01:52 - text like this named entity recognition
01:55 - would help you find out the named
01:58 - entities like like Wall Street is a
02:02 - location $15 is price 2011 is a date
02:08 - Amarin Corp is an org Visa is an org so
02:13 - this was possible with NLP natural
02:16 - language processing using a very popular
02:19 - technique called Neer named entity
02:22 - recognition while today you can go ask a
02:27 - question and then it would give you an
02:29 - answer
02:30 - so even though previously you had text
02:33 - you were using AI or AI let's say for
02:37 - fathers like EML and deep learning to
02:41 - get something out of the text so analyze
02:45 - the text process the text find out
02:47 - something from the text but you're not
02:49 - necessarily using it to create new text
02:52 - in itself and that is where the
02:55 - generative part comes in you are now
02:57 - generating text rather than just
02:59 - processing processing text let's take
03:01 - another
03:02 - example the next one is few years back
03:06 - you were just trying to F figure out
03:09 - whether a given image is a cat or a dog
03:13 - this example can go multi-level this is
03:16 - a typical classification problem in
03:18 - machine learning it could be like cancer
03:21 - not a cancer looking at the image you
03:23 - can say whether the patient has got
03:24 - pneumonia or not a pneumonia so it could
03:27 - be at any level but imagine you have got
03:29 - two sets of input or you've got like
03:32 - let's say unlabeled input and you want
03:34 - to figure out whether the output is cat
03:38 - or not cat or cat or dog so it could be
03:41 - of any type so even in this case the AI
03:45 - or the Deep learning model was purely
03:47 - used to classify an input image but what
03:51 - has changed now now you can actually
03:54 - artificially generate the image of a cat
03:58 - flying from the sky falling from the sky
04:01 - you can generate a cat as a president
04:03 - you can do basically anything that you
04:05 - want you can generate images as much as
04:08 - you want and that's exactly why it is
04:11 - generative agent a new powerful class of
04:16 - large language models is making it
04:19 - possible for machines to write
04:22 - code write text draw something or create
04:27 - something with credible and sometimes
04:30 - superum results now let's break this
04:34 - down first of all we say large language
04:38 - models but let's let's say large models
04:41 - language is only one aspect of it we
04:44 - have got multimodel models these days
04:46 - that can do
04:47 - multimodality that can create text
04:51 - understand images generate audio like
04:53 - for example Google Gemini is a great
04:56 - example of a multimodal model in the
04:58 - open source world you have got something
05:00 - like lava which is a great example of a
05:02 - multimodal model so either way we have
05:06 - got a new class of large models really
05:10 - really large models and these models
05:13 - enable or let humans use these models
05:17 - use this AI systems to write English or
05:21 - multilingual text code which is a
05:24 - computer program that successfully runs
05:26 - draw create images and like create bunch
05:29 - of other things like audio video 3D
05:32 - Point Cloud a lot of other these things
05:35 - the main thing here is that one when
05:38 - something like this happens the result
05:40 - is credible like you can see this and
05:42 - then believe it could have come from a
05:44 - human being and that is very important I
05:47 - mean you could have used AI to create
05:49 - something maybe like 5 years back I have
05:53 - created let's say tweet BS before like
05:55 - long back before all these generative AI
05:58 - using a technique called Mark of chain
06:01 - Mark of chain used to use the underlying
06:04 - patterns within the text and then try to
06:07 - create the next word very similarly like
06:09 - that Mark of chain is a very popular
06:12 - technique that uses States and
06:14 - transition to understand the underlying
06:16 - pattern from the previous pattern the
06:18 - previous
06:19 - state but those were not as good as what
06:23 - it is today even though we had those
06:26 - things those were not like actual human
06:30 - text but now we have models that can
06:35 - create text exactly like how Shakespeare
06:37 - would write a new book or a new play we
06:41 - have models that can code like a proper
06:44 - programmer we have models that can
06:47 - create art like let's say van go or some
06:50 - other famous artist so we have all these
06:53 - possibilities that are credible when you
06:56 - see it you have to take a second look
06:59 - just to believe that this is either a
07:01 - human or an AI and also it can make
07:05 - super human result what do I mean by
07:07 - superhuman result a human being would
07:10 - take x amount of time to create
07:13 - something you asked me to create a
07:15 - stream lit application I would take a
07:18 - given amount of time to create that
07:20 - streamlit application given that I'm a
07:21 - human being I mean given that I am a
07:23 - human being I would need an x amount of
07:27 - time to create it or let's say you are a
07:31 - human being and you need to create a
07:33 - computer program you have to take a
07:36 - certain amount of time you want to write
07:37 - a book you would take time you want to
07:39 - create an art you would take time but
07:42 - these models are scalable that means you
07:44 - can write a book in maybe one day you
07:47 - can create a powerful application like a
07:49 - desktop application maybe a python GUI
07:52 - graphical user interface in just a
07:54 - couple of prompts and all these things
07:56 - are possible thanks to this large models
07:59 - that are mostly mostly mostly at this
08:02 - point Transformer based architectures
08:04 - I'm not going to get into the details
08:06 - but if you are attending a course about
08:08 - generative AI you should know that most
08:10 - of these models are based on something
08:13 - called Transformers which is a very
08:15 - popular architecture that was
08:17 - popularized or that was at least
08:19 - released by Google which uses a very
08:22 - important technique called attention so
08:24 - the attention based Transformer model is
08:26 - at the core of all these things and now
08:29 - we have got language models that can
08:31 - write and code we have got like
08:33 - diffusion models that can create images
08:36 - we have got like multitude of other
08:37 - models in fact multimodel models where
08:40 - we have got text and images in the same
08:42 - space within the same model in itself
08:45 - and that is what makes generative AI
08:48 - quite interesting look at these images
08:51 - these are images created by AI I mean
08:54 - somehow you could say today that these
08:57 - are AI created images but if you had
09:00 - shown the same images to Me 3 4 years
09:03 - back I wouldn't have even guessed even
09:07 - like with enough clue that these are AI
09:09 - generated images because not even in my
09:11 - dream I've thought that AI could create
09:14 - something like this and these things get
09:17 - better and better every single day
09:20 - thanks to the powerful class of models
09:22 - that we have God and thanks to the
09:25 - research advancement that is happening
09:27 - every day and in in fact if you see AI
09:31 - today the AI is not affecting the blue
09:35 - colar workers I mean back in the day
09:38 - people used to talk about automation
09:39 - people used to always say automation is
09:41 - going to take the jobs of let's say
09:43 - Factory workers automation used to take
09:46 - the jobs of people who are working in
09:48 - factories and Manufacturing units it did
09:51 - in fact there are a lot of information
09:54 - about how Amazon has made most of its
09:57 - packaging and shipping and Logistics
09:59 - automated these things happened like
10:01 - Tesla's Factory if you see there are a
10:03 - lot of robots in there so definitely
10:06 - there was a bit of Automation and robots
10:09 - taking the jobs of let's say blue color
10:13 - workers but as your Barber gone out of
10:16 - job as your hairstylist gone out of job
10:19 - not it but if you see the current the
10:23 - world of generative AI it primarily
10:26 - focuses on knowledge workers and
10:28 - creative workers knowledge workers and
10:32 - creative workers are mostly people like
10:33 - you and I who are part of this course we
10:37 - use our knowledge and create something
10:39 - and that's how we get paid either we
10:41 - write English some other language we
10:45 - write code computer program we create
10:48 - something using an image or we create an
10:51 - image in itself we produce an audio or
10:54 - video or we listen to an audio or video
10:57 - and produce something so some how if you
11:00 - see these knowledge workers or creative
11:02 - workers either their input or their
11:05 - output are one of these either text or
11:09 - code or image or video audio 3D or Point
11:13 - Cloud you can just go on and on now if
11:16 - you see the current state of generative
11:19 - AI models you can pretty much say that
11:22 - it's four out of five like I could have
11:23 - given five out of five but I still want
11:25 - to say that it's not still there so
11:28 - almost like four out of five you can say
11:31 - that these large language models are
11:33 - pretty pretty pretty good at writing and
11:36 - not multilingual yet I mean there are
11:38 - multilingual models but you can pretty
11:40 - much tell the difference like if you for
11:42 - example if you ask a model to create
11:44 - something in my language which is South
11:46 - Indian language Tam you know that okay
11:49 - maybe this is not necessarily human
11:50 - because these models are really not that
11:52 - good yet the next one is code these
11:56 - models can do pretty much good code they
11:58 - can create create GUI applications but
12:01 - not to the text level if you can say the
12:04 - model is good four out of five in text
12:07 - the code part is let's say three out of
12:10 - five then images the way the model
12:14 - understands images the way the model can
12:16 - create images creating images is really
12:19 - good but still there are certain aspects
12:21 - you can look at the eyeballs you can
12:23 - look at the fingers I mean fingers have
12:26 - almost got sorted out at this point but
12:28 - you can still look at things like that
12:30 - the skin tone lot of other things to
12:32 - tell that maybe this is an AI generated
12:34 - image the same thing goes with image
12:37 - understanding then you have got the
12:39 - video and audio which maybe you know at
12:41 - this point is like one out of five cuz
12:43 - it's still improving the video
12:45 - interlacing the change of uh the frames
12:48 - all frame transition it's there is still
12:51 - an improvement then there are like other
12:53 - modalities that we don't discussed about
12:55 - at all like 3D Nerf Point cloud and all
12:58 - these things exist but one thing that is
13:01 - very sure that if you are talking about
13:05 - generative eii you have to say that the
13:09 - particular set of people that it impacts
13:12 - whether it positively impacts or
13:14 - negatively impacts as knowledge worker
13:17 - and creative worker previously it might
13:20 - have taken me let's say 3 to 4 hours to
13:22 - create a YouTube thumbnail but now it
13:25 - takes much lesser time thanks to
13:27 - generative AI previously it might have
13:30 - taken me let's say a lot more time to
13:33 - summarize a document but thanks to
13:35 - generative AI it takes me much lesser
13:38 - time now so it positively impacts and
13:41 - also negatively impacts primarily
13:43 - knowledge workers who have to use their
13:46 - brain and either take the output which
13:49 - is somewhat this so it has to use all of
13:53 - these modalities like text code video
13:58 - image audio etc
14:01 - etc why is that now we have a huge flux
14:05 - in growth of generative VII I mean what
14:08 - has happened we already discussed about
14:10 - Transformers very briefly that the paper
14:14 - and the new neural network architecture
14:16 - Transformers gave way for all the models
14:19 - that we are using now or most of the
14:20 - models that we are using now but there
14:22 - is something else that we need to pay
14:24 - attention
14:25 - to now it is the time we have a lot of
14:28 - other other things combining together at
14:31 - the same time so if you see now we have
14:35 - got better models we have got different
14:38 - architecture which is what like I said
14:40 - Transformers architecture and in fact
14:43 - there are Transformer Alternatives that
14:44 - are coming up called like for example
14:46 - Mamba these are like State space models
14:49 - SSM that solves the problems
14:51 - Transformers face in terms of model
14:54 - scaling and time complexity we have got
14:57 - more compute computer has never been
15:00 - cheaper like this it is completely
15:03 - possible for you to rent a very huge
15:06 - amount of computer on AWS the
15:08 - accessibility is there compute in itself
15:11 - is there NV has released almost like a
15:14 - super computer level GPU or accelerated
15:17 - Computing device that you or I can
15:20 - probably own maybe expensive but still
15:23 - somebody can own and we have got more
15:25 - amount of data historically and all the
15:29 - humans have been always creating data we
15:31 - create data volunteer we create data
15:35 - without being asked to create data you
15:38 - go to a supermarket you try to pick
15:40 - something there is a CCTV capturing your
15:43 - data there is video data you go to the
15:46 - same Supermarket you buy something now
15:48 - that is going to be part of a POS like
15:50 - Point of Sales system that is a data you
15:53 - come out of the supermarket maybe you're
15:55 - going to Tweet about it that is the data
15:57 - that you are generating maybe there is
15:59 - going to be an Instagram post that is a
16:01 - data that you generating maybe you have
16:04 - got a loyalty points card that is a data
16:08 - that you are generating maybe you're
16:10 - going to return it that is a data that
16:12 - you are generating so there is a lot
16:15 - more amount of data from humans but also
16:18 - we have more sensors and other
16:21 - equipments that can collect data there
16:23 - is data from uh let's say electricity
16:26 - like sensors there is data from Air
16:29 - planes and there is data everywhere else
16:32 - the amount of images that we have
16:33 - digitized huge the amount of books that
16:36 - we have digitized huge the amount of
16:39 - unstructured information to structured
16:41 - information that we have moved huge so
16:44 - we have now tons and tons and tons of
16:48 - data and one thing that you can also see
16:51 - is that the models have become better
16:54 - with the model size also when you have
16:57 - got more data
16:59 - more compute the models have started
17:01 - becoming better for example Palm which
17:04 - is a Google model has got 540 billion
17:09 - parameter not saying that you always
17:11 - need a lot of data for a good model
17:14 - that's not the outcome that you should
17:15 - take but a lot of data will help you
17:18 - build a good
17:20 - model and finally at least for me
17:23 - personally one of the most important
17:26 - reason why things have gone
17:29 - is open source open research open models
17:35 - open techniques open tools few years
17:39 - back you did not have let's say a place
17:42 - like hugging face where you can go share
17:44 - the model few years back people were not
17:48 - putting out papers almost every single
17:50 - day on let's say archive which they
17:52 - found out and few years back you did not
17:56 - have all these scripts that would make
17:58 - it easy easier for them to build
18:00 - fine-tuning Solutions and all these
18:03 - things are there today and like for the
18:05 - last couple of years at least and these
18:08 - are people who have relentlessly open
18:10 - sourced whatever they have created and
18:14 - that has almost led to a huge influx and
18:17 - revolution of new types of models new
18:20 - models new fine-tune models new
18:22 - techniques new data and lot of these
18:25 - things exist so it's better models
18:29 - more compute more data while all these
18:33 - being open source I mean compute of
18:34 - course it's still not youve got like
18:36 - decentralized models like petals there
18:39 - are solutions but it is one of the place
18:41 - where it is not open yet but everything
18:43 - else do you have beta models open source
18:46 - models base models yes we have do you
18:49 - have more data of course we have got
18:51 - more data and those data with commercial
18:54 - license and without commercial license
18:55 - we have got and the bigger part is all
18:58 - these things are open source you can go
19:00 - take any Model start building data sorry
19:03 - Start building models or you can take
19:05 - one of the models start fine tuning it
19:07 - you can do all these things even without
19:09 - the required computer even if you do not
19:12 - have a GPU you can go to Google collab
19:14 - for free and then use their GPU and then
19:17 - play with these models thanks to open
19:20 - source so all these things come together
19:23 - and then help you create something that
19:25 - did not exist before thanks to better
19:28 - models more data more compute and open
19:31 - source and finally if you have to look
19:35 - at the generative AI landscape the
19:38 - generative AI landscape keeps on
19:40 - changing but you can kind of put it into
19:43 - multiple buckets if you look at the
19:46 - text primarily people use text for
19:50 - marketing content sales email CH support
19:54 - chat email support no taking General
19:57 - writing in professional world I'm not
19:59 - talking about you know kids using text
20:01 - for their homeworks in professional
20:03 - World code you can generate code you can
20:05 - generate documentation you can
20:07 - understand code today you can like
20:09 - literally highlight a particular part of
20:12 - the code and gp4 GPT uh Vision sorry gp4
20:17 - and other models can help you understand
20:19 - it you have got image generation for
20:21 - like let's say advertisement voice
20:24 - synthesis video generation and you have
20:25 - got a bunch of other things you have got
20:28 - NPCs
20:30 - inside games that are AI generated
20:32 - You've Got Game assets that are AI
20:35 - generated you have got game scenarios
20:38 - that are AI generated and you have got
20:41 - multiple companies working on it one
20:44 - thing that you have to understand is
20:45 - there are multiple layers multiple
20:48 - different ways you can be part of this
20:51 - one is the model layer you're not
20:53 - talking about the data layer in itself
20:54 - when you go inside model layer you have
20:56 - data layer the other side is the appli
20:58 - application layer either you can build
21:01 - applications on top of these models or
21:03 - you can build the models or you can
21:06 - build data for the model and one of the
21:08 - most important thing these days people
21:10 - started wondering is how do you evaluate
21:12 - the models model eals model monitoring
21:15 - so there are lot of different ways you
21:18 - can be part of the generative AI
21:20 - landscape and when you look at the
21:22 - companies you have lot of different
21:23 - companies you have got mid Journey that
21:25 - can create images youve got gup co-pilot
21:28 - that can code for you that can help you
21:31 - understand the code you have got tools
21:33 - like Jasper that helps you use for
21:37 - something like opena API and let's you
21:41 - use it for a particular vertical like
21:42 - marketing and youve got a lot more other
21:45 - companies like coare creating models you
21:47 - have got hugging face helping you host
21:49 - and also create models you've got all
21:53 - these different kinds of companies all
21:55 - these different kinds of domains and
21:58 - with a very nice match you can build
22:02 - things you can build things either in
22:05 - the model layer which is like data layer
22:08 - model monitoring open source tools or
22:10 - you can build in the application layer
22:12 - and when you build an application layer
22:14 - you can either go to a particular
22:15 - vertical you can say I want to build
22:17 - something in sales I want to build
22:19 - something in marketing maybe you want to
22:21 - build a chatbot for lawyers so you can
22:23 - go to a particular vertical or you can
22:25 - go to a particular function you can say
22:28 - say okay uh I will take a particular
22:31 - business unit irrespective of what
22:33 - modality I'm using or you can go by
22:35 - modality you can say I'm going to just
22:37 - pick text and then build something in
22:39 - text I'm going to pick code and build
22:41 - something in code so there are a lot of
22:43 - different ways for you to approach this
22:46 - and if you generally talk about how good
22:49 - AI has become you can just tell you to
22:52 - write a poem write a story or write
22:55 - something and it is going to do it for
22:57 - you there are a lot of instances I have
23:00 - given such poems to human beings and it
23:03 - has been quite difficult for them to
23:05 - know that it was AI written and they
23:09 - have been shocked to know that this has
23:11 - been AI written when somebody was not
23:13 - exposed to AI so
23:16 - one we have spoken all the things that
23:19 - people are talking about but there are
23:21 - certain things that people do not talk
23:23 - about often and those are also quite
23:26 - important for us to pay a slight ATT
23:28 - attention
23:29 - to one training data two
23:34 - hallucination three rules four
23:38 - copyrights not a lot of time we have
23:43 - training data which a lot of companies
23:46 - have openly shared GPD 4 GPT 4.5
23:50 - whatever the latest model is we do not
23:52 - know entirely what training data that
23:54 - they used generally they say they used
23:57 - web data how much of that web data is
24:00 - consent with content how much of that
24:03 - web data is without content nobody knows
24:06 - there are lawsuits people have always
24:08 - filed cases people have shared their
24:11 - voice against let's say tools like
24:14 - stable diffusion recreating artist work
24:18 - but the point here is that a training
24:20 - data is a
24:21 - place or training data is a thing with
24:25 - which uh we don't have lot of
24:27 - information under also we don't speak a
24:29 - lot open tools open models have given
24:32 - you some understanding about the kind of
24:35 - training data that goes into with
24:36 - content without content all these things
24:38 - but still even if you take one of the
24:40 - most popular models these days is mistal
24:43 - but you don't know what is mist's
24:44 - training data Maybe M doesn't want to
24:47 - get sued or maybe this is what it takes
24:49 - to build a gp4 equivalent open model the
24:53 - point is training data matters um not
24:57 - maybe today I'm I'm not not pro
24:59 - copyright I'm not against copyright I'm
25:02 - just saying that imagine you wrote a
25:03 - book let's say like Harry Potter or
25:05 - something and there is an AI that can
25:07 - read your book or that has read your
25:09 - book and create a similar work which you
25:14 - may have to take like couple of years to
25:16 - do but AI is really good at it maybe it
25:19 - would hurt as a software engineer or as
25:21 - a data scientist I don't know how much I
25:24 - would say there is a meme for example um
25:28 - you know the that the text that you take
25:31 - from a book the author would U feel sad
25:34 - or an artist would feel bad when their
25:37 - work is copied by AI but when they tell
25:39 - a programmer that AI has copied from
25:42 - stack overl GitHub the programmer
25:44 - actually says that okay that's what I do
25:46 - every day so our profession or at least
25:49 - like if you are from a software
25:51 - background or a data scientist you might
25:53 - feel that you know we do it very often
25:55 - but there are a lot of other professions
25:57 - where cre creative work is their bread
25:59 - and butter they get paid by writing
26:01 - books they get P by paid by creating art
26:04 - so I'm not sure what is going to happen
26:05 - especially when we do not have training
26:08 - data
26:09 - transparency there are companies like
26:12 - shutter stock Adobe have explicitly said
26:16 - that we are going to use only consented
26:18 - data for training our let's say image
26:21 - generation model there are companies
26:22 - that do this kind of stuff but still it
26:25 - is not an industry standard yet the next
26:29 - one is the models are quite good at
26:32 - hallucinating now whether Hallucination
26:34 - is good Hallucination is bad that is for
26:36 - a separate conversation altoe Andre
26:39 - karpati who is uh now part of opena
26:42 - previously the head of self-driving at
26:44 - Tesla who is a very popular figure in
26:46 - the Deep learning World in terms of his
26:48 - teaching and all the things that he
26:50 - shares Andre carpati has always said I
26:54 - considered that these models are
26:55 - dreaming and some dreams are factual
26:58 - write some dreams or not and I see
27:00 - hallucination as a feature than a bug I
27:03 - feel like most of these models are
27:05 - hallucinating and when the hallucination
27:08 - Hallucination is Right factually correct
27:11 - we take them and the Hallucination is
27:14 - factually incorrect we call it
27:16 - hallucination so everybody has got their
27:20 - opinion it is very easy to use
27:23 - techniques like prompt injection or some
27:25 - other techniques and make these models
27:27 - give your wrong answer like for example
27:30 - in this case I have basically made chat
27:33 - GPT to tell me what is Neo's favorite
27:36 - food in The Matrix and it says according
27:38 - to a statement made by the directors of
27:40 - the Matrix movies in
27:42 - 2021 Neo's favorite food in The Matrix
27:45 - is chicken biryani followed by Italian
27:48 - pasta as a second favorite there is no
27:50 - information about this there was nothing
27:52 - like this on the internet I primarily
27:55 - injected that information in cont
27:58 - context to chat GPT so that chat GPT
28:00 - would give me this response back when I
28:03 - ask this question which is completely
28:05 - possible this is not technically
28:08 - hallucination but this is like one of
28:10 - the adversarial attacks with which you
28:12 - can make chat GPT or other large
28:15 - language models to give you factually
28:16 - incorrect answer so Hallucination is a
28:19 - big part of it Hallucination is one
28:22 - reason why medicine does not use a lot
28:24 - of large language models because you
28:26 - cannot still rely exact ly whether the
28:28 - answer is right or not the model gives
28:31 - you a different answer when you say
28:32 - let's take let's think step by step the
28:35 - model will not give you the same answer
28:37 - when you don't see that there are a lot
28:39 - of memes that you can actually tip chat
28:41 - GPT or you can say that my mom likes you
28:44 - or you know save the Kens and all the
28:46 - other things and get a different answer
28:48 - alog together so overall
28:51 - hallucination I don't know whether you
28:52 - like it whether you don't like it but it
28:54 - is still part of a problem in
28:56 - democratizing large language models in
28:58 - everyday life and the next thing is the
29:03 - rules what is the question for which you
29:06 - want chat GPD to answer what is the
29:08 - question you don't want chat GPD to
29:09 - answer the main question is who makes
29:12 - the rule open AA has made a rule saying
29:15 - where do I find cheap cigarette is okay
29:18 - to answer while how can I create a bomb
29:21 - is a question that I should not answer
29:23 - well and good that open a made this
29:25 - decision but how far and how long you
29:27 - want a for profit or at least open a is
29:31 - a weird setup uh it's a nonprofit and it
29:34 - has got a for profit so anyways how how
29:37 - for how long you want big corporates
29:40 - with let's say Market interest to make
29:43 - these decisions for you and whether it
29:46 - is right decision whether it is wrong
29:48 - decision every country is different some
29:52 - country might have certain belief some
29:54 - country might not have there is no
29:56 - Global belief I mean like we have
29:58 - certain Global beliefs like kids are
30:00 - cute kittens are cute so these are like
30:03 - Global belief imagine you are a kitten
30:05 - lover maybe you don't like dogs so what
30:07 - do you want the model to do so the there
30:10 - is a larger amount of question about who
30:12 - makes the rules there are bodies being
30:15 - created there are bodies being
30:17 - dismantled because it's not working but
30:19 - at least at this point who makes the
30:21 - rule is a big question and that is one
30:23 - of the reason why I love decentralized
30:26 - Ai and why in this course we are also
30:29 - going to see a lot of local AI which
30:31 - means like you can run the model
30:33 - yourself on your desktop or a laptop or
30:36 - PC whatever that you have got you don't
30:38 - have to always rely on one company
30:41 - setting the rule and then getting the
30:42 - model in itself I mean that's that's one
30:44 - thing that at least I believe that we
30:46 - need to have a lot of models and um you
30:49 - get to choose what you want to follow
30:52 - it's it's like living on this planet you
30:53 - get to choose your religion you get to
30:55 - choose your food you get to choose your
30:57 - dress you don't have to globally follow
30:59 - a set of rule um until you want to be
31:02 - nobody should be forcing it on you that
31:05 - is exactly why we need local open models
31:08 - and this is a big part of a question to
31:10 - say who makes rules when the model is
31:12 - not open and uh as much as we talk about
31:16 - like good things in AI it is very
31:18 - important for us to while we are talking
31:20 - about all the other things it is very
31:22 - important for us to understand at least
31:24 - the implications that it might bring the
31:27 - current exam system that we have might
31:30 - be obsolete with the generative AI that
31:33 - we see I mean look at the exam results
31:37 - gp4 this is gp4 score this is gp4 score
31:43 - it has scored tremendously in a lot of
31:46 - these exams where human beings have to
31:50 - take if you have ai models really good
31:55 - maybe you can hear the model through
31:56 - your headphones or whatever it is
31:59 - what is the purpose of this exam how do
32:01 - you now still hold these exams
32:04 - accountable to select the right set of
32:06 - humans for the right set of let's say
32:08 - courses or whatever it is so Education
32:11 - Plus Academia is one of the places where
32:15 - these large language models have got
32:17 - some push back you know there are
32:18 - universities who have punished students
32:20 - for using large language models there
32:22 - are certain places where universities
32:25 - have encouraged students to use large
32:26 - language models Khan Academy has signed
32:29 - up with opena as a partner to create a
32:32 - personal tutor for every student so this
32:36 - is a place where still there is a lot of
32:38 - questions among the academic researchers
32:42 - or educational or like you know teachers
32:45 - about how do they encourage or
32:48 - discourage using large language models
32:50 - but the broader question is how much of
32:52 - what we have followed until now is going
32:54 - to be
32:55 - valid already you know for example like
32:58 - if you're in India you know that in
32:59 - India there are certain institutes where
33:01 - you should not open the book and write
33:03 - the answer but then there are certain
33:06 - cases where you can have open book like
33:09 - you can have any book that you want and
33:11 - you have to figure out to write the
33:12 - answer so education requires its own
33:15 - transformation and uh whether willingly
33:18 - or unwillingly llms are going to
33:20 - transform education for good or for bad
33:23 - but this is a place where there is going
33:25 - to be a lot of impact in terms of large
33:27 - language models and we already discussed
33:29 - about knowledge workers I'll give you a
33:31 - particular example I am data scientist
33:33 - by profession and one of the part of my
33:36 - job is sometimes making charts and
33:38 - explaining charts to our stakeholders
33:41 - and this is something that gp4 Vision
33:43 - can do it like we'll see couple of
33:45 - examples gp4 Vision can take a chart
33:48 - like this help you understand translate
33:50 - the chart into structure data and it can
33:52 - do much more more efficiently than what
33:55 - I can do I'm not saying that I'm going
33:57 - to be replaced by a gp4 vision model
34:00 - tomorrow maybe that will not happen
34:02 - tomorrow but what does it mean for my
34:05 - daily job if the model can do a much
34:08 - better job than me at a particular task
34:11 - maybe the model still does not
34:13 - generalize well as much as as a human I
34:16 - would do as a human I might improvise I
34:19 - might know what to say what not to say
34:21 - depending upon who the person is but the
34:23 - models are getting good at it and as you
34:26 - can see here what will happen happen to
34:28 - the knowledge workers this is something
34:29 - that people should know and in terms of
34:32 - copyrights I think there is a huge
34:34 - debate and issue there are lot of
34:36 - lawsuits cases against mid Journey Open
34:39 - Ai and all the companies in fact opena
34:42 - has promised that if you use open AI
34:46 - product and you get sued by
34:48 - somebody open AI will pay your legal
34:51 - fees and fight for you that's a huge
34:54 - commitment I'm not sure how it is going
34:56 - to work or how much it is going to work
34:58 - but at least for that matter that
35:01 - copyrights are going to be a big deal
35:04 - whether you like or not the world that
35:07 - we live in copyrights are a huge part
35:10 - patents copyrights people get royalty
35:13 - from these things now one you're going
35:15 - to put a lot of these professions out of
35:18 - work two it's going to become
35:20 - ridiculously easy to replicate their
35:22 - work so what would it mean to have
35:25 - copyright still intact is is it possible
35:28 - that we are going to enter into a world
35:30 - where copyrights don't exist but then
35:33 - will open AI or these kind of companies
35:35 - share their code openly because they
35:37 - don't respect copyrights then do they
35:40 - let people copy their work they're not
35:42 - going to let people copy their work so
35:44 - it's a very strange predicament in which
35:48 - these companies are we are but we don't
35:50 - have answers like these are like
35:51 - questions that we have got we don't have
35:54 - answers if you want to finish this
35:57 - presentation with one final takeaway I
35:59 - would like to say that generative AI is
36:02 - transformative
36:04 - disruptive and unlike you know let's say
36:06 - I'm not a very big crypto Fanboy but
36:08 - unlike like let's say crypto or web3 or
36:11 - blockchain and all the other things like
36:13 - recently people have been like really
36:15 - getting crazy about generative AI is
36:18 - here to stay it's not going to go
36:20 - anywhere it's not going to vanish you
36:23 - can pretend that it is not going to
36:25 - impact your job but it is going to stay
36:28 - but like every other technology it has
36:31 - its own
36:33 - limitations and if not handled with care
36:36 - it can affect current form of Education
36:38 - it can impact jobs it can spread
36:41 - misinformation it can widen
36:44 - inequality I mean of course it has a lot
36:46 - of other good things I'm not talking
36:48 - about the good things here I just want
36:50 - you to know before you enter into
36:52 - learning how to use these tools that the
36:54 - tools that we going to learn about in
36:56 - this course will have impact on all
36:58 - these
36:59 - things I don't know if it is a nice
37:01 - touch I said like this is written by a
37:02 - living human but again what is written I
37:04 - typed it on a computer pasted it here
37:07 - even written is being a question uh I
37:09 - don't know the answer to but that has
37:11 - been there for many years but the point
37:13 - is it is it is a very strange place that
37:17 - we are in um if you take like humanity
37:20 - and uh that's why a lot of people say AI
37:23 - is like electricity and I believe if AI
37:26 - is like electricity then decentralized
37:29 - AI is the way to go so you use your own
37:33 - AI models and then you build your own
37:35 - things you don't have to use it from
37:39 - somebody else I mean like a lot of
37:40 - countries have only governments making
37:42 - electricity but then there are a lot of
37:44 - other places like if you see Tesla cells
37:46 - solar cells people buy solar cells make
37:48 - their own electricity use it for their
37:50 - own cars use it for their own stuff I
37:52 - think decentralizing decentralization is
37:55 - the way to go about but in this course
37:57 - we'll see both the closed models and
37:59 - open models but while there is a little
38:01 - bit of I should say a lot more emphasis
38:04 - on open models because that is a
38:05 - principle that I believe in at the same
38:08 - time A lot of people are also against
38:11 - open- sourcing AI because they compare
38:13 - now ai with nuclear energy and then
38:15 - there are a lot of different ways that
38:17 - they go about it's two different schools
38:19 - of thought I'm I'm part of something
38:21 - like somebody's part of something if you
38:23 - have any thoughts I would like to know
38:24 - about what you think about generative AI
38:27 - see in another
38:31 - [Music]
38:35 - chapter so the last video ended with me
38:38 - saying that we I prefer decentralized AI
38:41 - but I did not provide any kind of
38:43 - clarification what do I mean by that so
38:45 - in this video I want to First offer what
38:48 - do we mean by decentralized Ai and I
38:50 - want to like in fact break down certain
38:52 - things before we go ahead with
38:54 - decentralized as a part let's define
38:56 - what do we mean mean by AI here so as
38:59 - you all know of course AI is here
39:01 - artificial intelligence we are not
39:03 - talking about AGI which specifically
39:06 - talks about general intelligence so here
39:09 - we are talking about narrow artificial
39:11 - intelligence which is AI now what is
39:15 - this AI why is everybody talking about
39:17 - AI we saw a glimpse of this in the
39:19 - previous video but now the AI here
39:23 - specifically if you're talking about
39:24 - technically is nothing but deep learning
39:27 - problems what are deep learning problems
39:30 - leap learning is a computer science
39:32 - domain where you build deep neural
39:35 - networks so typically a neural network
39:37 - would look like this it has got an input
39:39 - layer a middle layer and an output layer
39:42 - this is typically how a simple neural
39:44 - network would look but if you replace
39:47 - the middle layer with lot of deep layers
39:50 - that is basically what your deep
39:52 - learning is now for you to do deep
39:54 - learning effectively you need par
39:57 - Computing or you need High memory
40:00 - because at the end of the day what is
40:01 - happening in deep learning is it is
40:03 - matrix multiplication it has got lot of
40:07 - different weights within this neural
40:09 - network so if you see here youve got lot
40:11 - of different weights here and these
40:13 - weights get multiplied you know certain
40:16 - things happen so at the core what you
40:19 - need is you need really good compute
40:22 - that can do matrix multiplication for
40:24 - you and for that particular reason
40:27 - deep learning is more efficient on GPU
40:32 - than CPU now what is GPU GPU stands for
40:37 - graphical processing unit and CPU stands
40:40 - for central processing unit your normal
40:42 - computer or the laptop or probably the
40:45 - device that you're using right now has
40:47 - mostly got both GPU and CPU does it mean
40:51 - you can do deep learning on any device
40:54 - that is where a big catch comes in see
40:57 - most popular Frameworks for deep
40:59 - learning or at least one of the most
41:00 - popular framework for deep learning
41:02 - today is called pytorch now pytorch and
41:05 - many other deep learning Frameworks are
41:09 - primarily optimized or efficient on
41:12 - Nvidia gpus Nvidia gpus are really one
41:18 - of the core of why deep learning exist
41:21 - or deep learning happens or where deep
41:23 - Learning Happens these days now the
41:25 - reason why Nvidia G is highly preferred
41:28 - is primarily because Nvidia GPU has got
41:32 - their own proprietary software which
41:35 - they call Cuda so Cuda is the software
41:39 - that is one of the primary reasons that
41:41 - enables parallel Computing like high
41:44 - level parallel Computing within Nvidia
41:46 - gpus who efficiently utilize the Nvidia
41:49 - GPU memory and then do matrix
41:50 - multiplication or do deep learning
41:54 - efficiently thanks to Cuda Nvidia GPU
41:57 - are the home for most of the deep
41:59 - learning projects it doesn't mean you
42:01 - cannot do it on CPU it doesn't mean you
42:03 - cannot do it on AMD processors it means
42:05 - Nvidia is highly highly preferred now
42:09 - what is this Cuda so the Cuda or it is
42:12 - an Acron for compute unified device
42:15 - architecture it is a proprietary that is
42:17 - one thing that people often forget it is
42:19 - not an open source solution it is a
42:22 - proprietary closed parallel Computing
42:24 - platform and an API that allows sof was
42:27 - to use certain types of gpus for general
42:30 - purpose processing so what we are doing
42:32 - here the matrix multiplication we are
42:34 - doing is a general purpose processing
42:37 - and for that general purpose processing
42:39 - we are leveraging the GPU and this
42:42 - approach is called GP GPU general
42:45 - purpose Computing on gpus so Cuda is the
42:49 - software layer that gives direct access
42:51 - to gpu's Virtual instruction set and
42:54 - parallel computation elements for the
42:56 - execution of compute kernels now what
42:59 - Cuda enables you to do ultimately is do
43:03 - this better um GPU or leverage GPU for
43:07 - matrix multiplication that means you can
43:09 - do machine learning and deep learning
43:11 - having said that that basically enables
43:15 - most of this AI models to effectively
43:17 - run primarily on Nvidia enabled gpus so
43:21 - if you want effective hardware for
43:25 - running deep learning or for running AI
43:27 - you need effective gpus and not
43:30 - everybody has got effective gpus and
43:33 - that is where a lot of other Concepts
43:35 - come into picture so now first thing is
43:38 - because you need effective GPU that is
43:40 - clear at this point right deep learning
43:43 - here we are saying ai ai basically we
43:45 - are saying it is a deep learning and
43:47 - deep
43:48 - learning mainly prefers GPU because of
43:52 - comput and memory and Frameworks like
43:54 - pyot lets you do that thanks to Cuda
43:57 - maybe not so much thanks because it's
43:58 - not open source and you need all these
44:01 - things so that is where running an AI
44:04 - model or an llm becomes a little
44:06 - challenging so if you want to run an llm
44:09 - so if you want to run an e model or an
44:12 - llm let's say you want to run an llm in
44:15 - this case we are referring to large
44:17 - language models so if you want to run
44:20 - large language models let's say now you
44:23 - at this point need Nvidia gpus no
44:27 - because of this reason because it is
44:29 - very expensive computationally you don't
44:33 - often use Nvidia gpus as a physical
44:35 - Hardware rather you would rent this on a
44:38 - cloud provider so you would go to let's
44:40 - say AWS you would go to gcp or you would
44:43 - go to Azure and then you would rent an
44:46 - Nvidia GPU this is option one option two
44:49 - is these days you have got a lot of new
44:51 - startups like for example one of the
44:53 - most popular one is runp POD so you can
44:56 - go to runp pod and rent a GPU so GPU
44:59 - there are like different kinds of GPU
45:01 - for example RTX RTX let's say 4080 4090
45:06 - is a very popular GPU a100 another
45:09 - popular GPU these are like on a
45:10 - different levels it has got different
45:12 - memories it has got different compute um
45:15 - the flops would be different lot of
45:16 - things are different so now one you can
45:19 - go to these Cloud platforms rent to GPU
45:21 - or you can go to this new kind of
45:23 - computing or Cloud platform and rent a
45:25 - GPU this is where you could do this
45:29 - machine learning or deep learning or
45:31 - whatever llms efficiently now what do I
45:34 - actually mean by oh do llms I've been
45:37 - saying the do llms what do I mean by do
45:39 - llms that is a very important thing for
45:42 - us to note because llm doesn't happen to
45:45 - be just like that there are a lot of
45:48 - steps in building an llm first thing you
45:50 - need to prepare data set so you need an
45:53 - input data set that you can use and
45:55 - these data sets because because we are
45:57 - talking about large language model we
45:59 - are not talking about just a simple
46:01 - language model we are talking about
46:03 - large language models these large
46:05 - language models require you to have
46:08 - large amount of data set so because of
46:10 - that only you end up having large
46:12 - language model and because you need data
46:15 - set you need preparation which you can
46:17 - still do it in GPU but CPU still you
46:19 - need like GBS of data like gigabytes of
46:22 - data after you do that the next thing
46:25 - that you need to do is you need to train
46:27 - a Model A train a model is what we call
46:32 - as building a model once you build a
46:35 - model successfully sometimes you might
46:37 - do an evaluation to see how good the
46:40 - model is doing so you have got a bunch
46:42 - of evaluation metrics or benchmarks to
46:46 - see how the model itself is doing when
46:48 - you compare it with different models so
46:50 - you need data set then you train a model
46:53 - then you eval or do benchmarks now what
46:57 - started happening in the Deep learning
46:59 - World some years back before that there
47:02 - was a new technique that came into
47:03 - picture that is called transfer learning
47:06 - so what this technique said is that okay
47:09 - not everybody has to build their own
47:12 - model I'm not talking about llms here
47:14 - I'm talking about any deep learning
47:16 - model rather you can take certain pre
47:20 - trained deep learning models and then
47:23 - you can use that for your own use case
47:27 - so you take a pre-trained deep learning
47:29 - model and you use that model as your
47:32 - base model and then you do something
47:35 - called finetuning for your own use case
47:39 - and then you have a new model and that
47:41 - model is supposedly better than the
47:43 - pre-trained model for your own use case
47:46 - if pre-trained model or something this
47:49 - model after you fine tuned would be
47:51 - better than what the pre-train model was
47:54 - for the fine-tuning task in which you
47:56 - did it now this has been there for quite
47:59 - a while so now when we have to build a
48:02 - large language model we need data set we
48:05 - need to train a model and we know that
48:07 - if we have to train a model which is
48:09 - basically building a deep learning model
48:11 - we need gpus and then you do evaluation
48:14 - or Benchmark after you have a model in
48:18 - place let's say at this point you have a
48:19 - model in place and what is that we mean
48:22 - by model so most of the times most of
48:25 - the times what we mean by model is a p
48:27 - torch bin file so it's a DOT bin file or
48:33 - sometimes these days we have safe tenser
48:35 - files so we have save tenser file or we
48:38 - have py to bin file these are the files
48:41 - where the Deep neural networks weights
48:43 - are stored at the start of this video I
48:46 - said like you know there are weights
48:47 - like numerical weights that are stored
48:49 - here and these numerical weights are
48:52 - mostly float so these numerical weights
48:55 - are mostly floats what do we mean by
48:57 - float it's like 0.26 78 96 something
49:01 - like this is a floating point so we have
49:04 - numerical weights and these weights are
49:06 - stored inside this bin file or tensor
49:08 - safe tensor file now when you have a
49:12 - longer number here like this very long
49:15 - number you need more memory to run the
49:18 - model also forget about training the
49:20 - model even to run the model you need
49:23 - more memory and what is that is called
49:27 - that running the model is called
49:29 - inference so inference means you use the
49:33 - model to generate text we're talking
49:35 - about llms particularly here so using
49:38 - the llm to generate text is called
49:41 - inference so you know at this point you
49:43 - need GPU to run the create the model you
49:47 - need GPU to also run the model that is
49:50 - what is
49:51 - inference but because not everybody can
49:54 - have CPU there are new techniques like
49:58 - for example
50:01 - quantization there are different kinds
50:02 - of quantization techniques but in a
50:04 - nutshell quantization helps you reduce
50:07 - the Precision of the floating points
50:09 - that you store and thereby reduce the
50:12 - memory that is required for inference so
50:15 - that you can run this inference on even
50:18 - consumer Hardware gpus or even CPUs so
50:23 - we have quantized models we have
50:25 - quantized models that help you run these
50:30 - models on consumer Hardware either GPU
50:33 - or CPU and one of the most popular
50:36 - Frameworks for that is llama
50:40 - CPP and there are many other techniques
50:42 - like file formats like GG UF GG ml you
50:47 - don't have to know exactly what is
50:49 - inside this and all these things you
50:51 - just have to know these Nam so that you
50:52 - know what when you come across this name
50:54 - you know this is basically quantized
50:56 - model so you're going to use llama CPP
50:58 - that is going to help you convert the
51:01 - model weights into let's say a C++ uh
51:04 - quantitized weight and that helps you
51:05 - run the py torch code basically is
51:07 - ported into C++ which is like much
51:10 - faster than py torch and then it uses
51:12 - different file formats which has
51:14 - quantized model weights and then you can
51:16 - run the model on consumer Hardware like
51:19 - your laptop so now because we spoke
51:22 - about CPU and GPU and we have seen about
51:25 - quantization
51:27 - let's take a quick look at a bunch of
51:29 - things so we know CPU and we know GPU in
51:33 - GPU you have multiple providers you have
51:35 - Nvidia let's say a very popular leader
51:38 - you have AMD and you have bunch of other
51:41 - companies and then you have got
51:43 - TPU the T in TPU stands for tenser
51:47 - processing unit this is uh primarily
51:50 - mostly used by Google so Google has
51:53 - primarily got tpus I'll quickly show you
51:55 - Google collab and you would see tpus
51:57 - there and very recently there is another
52:01 - Pro popular one that is coming up that
52:03 - is called metal and what is this metal
52:05 - it is nothing but Apple silicon
52:08 - computers so if you have got M1 M2 M3
52:13 - computers from Apple it could be MacBook
52:16 - Air MacBook Pro Mac Mini whatever
52:19 - computer you have got if you have got
52:20 - any of these chips it is highly likely
52:23 - that you have got metal GPU within this
52:25 - apple silic
52:27 - and these days we have got Frameworks
52:30 - and softwares that can let you run these
52:34 - deep learning models optimized for metal
52:37 - so these are like different kinds of
52:39 - computing and these are like you know
52:41 - what you can do now let's take a quick
52:44 - look at different kinds of Frameworks we
52:46 - learned about pytorch but pytorch is not
52:49 - the fastest one so there are people who
52:51 - uh have fallen back to something called
52:53 - Jacks and very recently Apple announced
52:57 - something for mlx and this is primarily
53:00 - optimized for Apple silicon so but most
53:03 - of the things that you would see on
53:04 - white TCH you have got Jacks and still
53:07 - some people Port things into C++ and you
53:10 - have got mlx very recently that is
53:12 - optimized for Apple silicon I know this
53:14 - is a lot of information in this video
53:17 - but the objective of this video is to
53:18 - give you a very wide landscape idea
53:22 - about what is like the technical details
53:24 - of what we call as large language models
53:27 - and I hope you have some clarity about
53:29 - what we mean by large language models to
53:32 - quickly recap when we say large language
53:34 - model the model here is nothing but a
53:38 - deep learning model a deep learning
53:40 - model is nothing but a deep neural
53:43 - network so it is like quite deep and
53:45 - then it has finally an output and most
53:48 - of the large language models that we are
53:50 - discussing today or large language
53:53 - models based on a particular deep Le
53:56 - learning architecture that is called
53:59 - Transformers so
54:01 - Transformers is the one of the most
54:03 - popular architecture based on which
54:05 - these deep learning models are built and
54:08 - we know that we need GPU to run these
54:10 - deep learning models and when we say
54:12 - gpus we are primarily talking about
54:14 - Nvidia gpus and because we don't have
54:17 - Nvidia gpus like lying around in our
54:20 - house mostly most of the people if you
54:22 - have got your lucky but if you do not
54:24 - have that is where you go to a club CL
54:26 - provider and either rent a GPU or you go
54:29 - to some other cloud provider and then
54:32 - try to use one of their services and
54:34 - because of this primary reason because
54:37 - gpus is computationally expensive gpus
54:40 - are hard to get and uh not the highest
54:43 - model not easy for you to run on CPUs
54:46 - because of this primary reason people
54:49 - offer llms as inference apis we already
54:53 - learned what is inference inference is
54:56 - the process of running an llm and then
54:58 - getting the text output so building an
55:01 - llm is one running an llm is what we
55:04 - call as inference so because of all the
55:08 - bottlenecks you have got because of all
55:11 - the reasons that it is difficult to run
55:14 - a large language model also you have to
55:16 - run it in production which means you
55:17 - need GPU always you may not have
55:19 - serverless compute that means you always
55:22 - have a cold start problem because you
55:23 - have to boot up the GPU to solve all
55:26 - these problems a bunch of companies
55:28 - decided to give the llm as an inference
55:31 - API it's not first time somebody is
55:34 - doing it software engineering is quite
55:35 - popular with the API world but if you
55:38 - are new to the API World API stands for
55:41 - application programming interface it's
55:44 - nothing but a connection to a bigger
55:47 - system that system would be owned by
55:49 - somebody else and you are going to just
55:52 - hit their end point and then get a
55:55 - response back so you're going to hit
55:57 - their end point and get a response back
56:00 - now the challenge here is that if you
56:01 - are working in a big company if you're
56:03 - working in an Enterprise setup
56:05 - especially if you're working in let's
56:06 - say Finance sector or Healthcare sector
56:09 - not every company would be interested in
56:11 - giving your data to somebody else
56:13 - because there is a server and you are
56:15 - going to hit that API from that server
56:18 - and then get the response back if you
56:20 - want to let's say design an app you will
56:24 - not have your own GPU to run one second
56:26 - you cannot run that app I mean now you
56:28 - can do it with iPad Pro because it has
56:30 - got Apple silicon but mostly let's
56:32 - assume you cannot do it that means you
56:34 - definitely have to hit an endpoint API
56:36 - endpoint and then get the response back
56:37 - that means you're sharing the data with
56:39 - them and that is where a whole new
56:42 - business of how you can use llms as an
56:46 - APA comes into picture without any doubt
56:50 - the leader in this business is open AI
56:53 - open AI if you are wondering how does
56:55 - open AI make money open AI makes money
56:58 - because their model is not open source
57:00 - not everybody can host it that means if
57:02 - you want to use gp4 one you can go to
57:05 - gp4 the chat GPT and then use it but you
57:09 - cannot build softwares on top of it you
57:10 - can chat with it but you cannot build
57:12 - softwares on top of it and if you want
57:14 - to build softwares on top of it let's
57:16 - say you want to build your own
57:17 - translation software or let's say you
57:19 - want to create your own design software
57:21 - or let's say you want to create
57:23 - something only for lawyers then that
57:25 - means you need to create an app either
57:27 - on web or like let's say iOS that means
57:31 - within this you have to hit the open a
57:34 - endo and get the response back and it's
57:37 - not just open a is the company that is
57:39 - alone doing this business opena is
57:41 - definitely one of the leaders in this
57:43 - because they have got the exclusivity
57:45 - and they are the best llms in this world
57:47 - at this point their gp4 is definitely
57:50 - the best llm at this point generally
57:53 - there are like specifically you would go
57:55 - to different LMS but generally this is
57:57 - one so open AI Nob brainer then you have
58:01 - got Azure then you have got AWS bedrock
58:06 - and there are other services like this
58:08 - that in fact like let's say Google gcp
58:11 - because Google has got G and bunch of
58:13 - other things Palm let's say Palm these
58:16 - are like model names so these service
58:19 - providers also let you use a large
58:22 - language model as an inference endpoint
58:24 - APA endpoint the challenge is most of
58:27 - them would not let you fine tune it even
58:29 - though open a recently started letting
58:31 - you fine tune as well so if you want to
58:34 - let's say just have a chatbot the
58:37 - easiest way to do that is build an
58:39 - application where take a user response
58:41 - hit the open a endpoint get the response
58:43 - back show it to the user you have a
58:46 - working chatbot without having to
58:48 - maintain your own infra without having
58:51 - to maintain your own infra or
58:54 - gpus but if you are worried about data
58:58 - let's say data challenges data privacy
59:01 - and lot other things that is where the
59:04 - whole new concept of open models come
59:07 - into picture instead of owning a
59:10 - proprietary model instead of paying
59:12 - money to an llm service provider who
59:15 - owns a proprietary model like open AI
59:18 - you would turn towards open models which
59:20 - means the model weights so the model
59:23 - weights you all know what is model
59:25 - weight the let's say the bin file the
59:28 - pytorch file or the safe tenser file or
59:31 - openly shared when we say openly shared
59:35 - it comes with a particular
59:37 - license I'll get into the license detail
59:39 - later on but these are model weights
59:41 - which you can self
59:43 - host you can buy your own small let's
59:46 - say VPS like virtual private server or
59:49 - you can have your own cloud uh model and
59:52 - you can have these things and you can
59:55 - host these models and then you can run
59:57 - it now this is where open
60:00 - AI versus truly open AI come into
60:06 - picture this is the company in this case
60:09 - and this is the open models that you are
60:12 - either self hosting now there are
60:14 - different service providers who let you
60:16 - use these open models also API that is a
60:18 - different topic alog together but in a
60:20 - nutshell you have got one world which is
60:23 - like proprietary models the second world
60:26 - where is open model and what I meant by
60:28 - decentralized AI is using these open
60:31 - models without having to rely on some
60:35 - proprietary model where you have to
60:37 - always send your data to them now like I
60:40 - said even this model if you have to self
60:42 - host it you need gpus or you need to
60:44 - rent gpus or you need to use quantized
60:47 - models like
60:48 - ggf or the other option is you go to new
60:51 - providers who let you use this model and
60:54 - there are bunch of advantages using this
60:56 - model but I want to stop this video here
60:58 - at this point to give you a breather if
61:02 - you have reached this point at this
61:03 - point I would strongly encourage you to
61:05 - write a blog post about whatever that
61:08 - you learned in this video and then write
61:10 - the blog post publish it anywhere that
61:12 - you want and then tag me whenever you
61:14 - share it on social media I would love to
61:16 - read what you have done because what I
61:18 - have done in this particular videos I've
61:20 - given you a lot of keywords I've given
61:22 - you a lot of seeds but if you have to
61:24 - understand this entire space better you
61:26 - need to spend at least an hour or two in
61:29 - going through these individual keywords
61:31 - that I've given you so that you can
61:33 - understand this entire landscape and
61:34 - then build a better understanding before
61:36 - we go on further lessons see you in
61:38 - another
61:43 - video five levels of llm apps consider
61:47 - this to be a framework and help you
61:48 - decide where you can use LM there are
61:50 - lot of different myths around what llms
61:53 - can do what llms cannot do where do you
61:55 - use llms today so I decided to put
61:57 - together this material uh in which I'm
61:59 - going to take you through kind of like a
62:01 - mental framework based on the extension
62:04 - or the depth in which you go towards an
62:07 - llm you can decide where you can fit
62:09 - this llm so we're going to first see
62:11 - what are those different levels of LMS
62:14 - that I have put together then we are
62:16 - going to see slight extension of that
62:18 - got two different documents to take you
62:21 - through that so this will give you an
62:22 - idea about how llm is being used today
62:26 - and how you can use LMS for your own
62:29 - applications to start with imagine this
62:32 - pyramid structure this is a very simple
62:34 - pyramid structure and as you can imagine
62:37 - with any pyramid structure the top of
62:39 - the pyramid or the peak of the pyramid
62:41 - is our aspirational goal and what you
62:44 - see at the bottom is the easiest that we
62:47 - can do and as with everything else you
62:49 - have to slowly climb to the top of the
62:52 - pyramid so you can probably hit the
62:54 - aspirational goal so to start with where
62:57 - do we use llms first Q and A a question
63:01 - and answering engine what do I mean by
63:03 - that it is quite simple for us to
63:05 - understand so question and answering
63:07 - engine is a system where you have an llm
63:10 - and all you are going to ask the llm is
63:12 - a question so you send a prompt and the
63:15 - llm takes the prompt and gives you an
63:17 - answer that is it that is the entire
63:20 - transaction that you have between in llm
63:23 - send a prompt get send into the llm get
63:27 - an answer llm large language models are
63:30 - nothing but sophisticated next word
63:32 - prediction engines and they have been
63:34 - fine-tuned with something called
63:36 - instruction so they are instruction
63:37 - fine-tune models that means they can
63:39 - take a human instruction and get you an
63:41 - answer back for example if I ask a
63:43 - question for this what is the capital of
63:45 - India then the llm would process this
63:48 - and then llm has information about how
63:50 - to answer it and then it will give me
63:51 - the answer back the capital of India is
63:54 - New Delhi that's all what you're going
63:56 - to do with this thing so first level
63:59 - question and answering now you might
64:01 - wonder at this point that where can you
64:03 - use question and answering as an llm
64:05 - engine this is the first thing that
64:07 - people built like when llm started even
64:11 - back in the day gp22 level people
64:13 - started building simply Q&A bots so all
64:16 - you want to do is ask a question give an
64:19 - answer could be a homework could be a
64:21 - general knowledge question could be
64:23 - something about the world could be about
64:25 - science could be about anything ask a
64:28 - question get an answer as simple as that
64:30 - it's a very three-step process ask a
64:33 - question or send a prompt take the llm
64:35 - to process it give me the answer back
64:38 - very simple application now what you're
64:40 - going to do is you're going to add
64:42 - something to that application and that
64:44 - is how you actually build a
64:46 - conversational chat bot and to
64:49 - understand this better I would like to
64:50 - take you to my second document which
64:52 - will give you probably better idea
64:54 - whenever we are talking about llm there
64:56 - is one important thing that we need to
64:58 - understand is we have crossed the stage
65:01 - where llm is simply a large language
65:03 - model we have more than that so for you
65:05 - to understand that I have five
65:08 - dimensions a prompt a short-term memory
65:12 - an external knowledge tools and extended
65:15 - tools if you think of this as your
65:17 - horizontal these are your verticals
65:19 - these are different dimensions that can
65:21 - add to an llm so you have a prompt you
65:24 - have a shortterm memory you have a
65:26 - long-term memory or external data you
65:28 - have tools and you have got extended
65:30 - tools so let me give you an example for
65:33 - each of this so that you can understand
65:35 - this better a prompt is what is the
65:37 - capital of New Delhi that's all the
65:40 - prompt is you simply go give what is the
65:43 - capital of New Delhi and the llm
65:45 - understands it and gives you a back
65:47 - understanding just gives it back now
65:49 - shortterm memory is when you have
65:52 - conversational history or something in
65:53 - the llm that is what we call call as I
65:56 - in context learning so whatever you
65:58 - stuff inside the context window the llm
66:01 - can take it that is your short-term
66:03 - memory so you give a few short examples
66:06 - you give an example like for example
66:08 - what is the capital of us uh I guess
66:11 - it's Washington DC Washington DC and you
66:15 - give a bunch of examples like this so
66:16 - the llm knows what is that it has to
66:19 - answer this is a short-term memory next
66:22 - you have external data now you take data
66:24 - from Wikipedia and you keep it and then
66:26 - give it to the LM that is your long-term
66:29 - memory because short-term memory just
66:30 - like a computer the ram it gets reset
66:33 - every time you reset the conversation or
66:34 - the session and then tools you let llm
66:37 - use tools like calculator internet
66:40 - python terminal and all these things and
66:43 - extended tools is when you expand this
66:46 - much beyond that I hope now you have
66:48 - understanding about the five different
66:50 - dimensions that we have in llms a prompt
66:54 - a shortterm memory of in context memory
66:56 - a long-term memory or external knowledge
66:59 - external data or custom knowledge tools
67:02 - like calculators and Python repple and
67:05 - extended tools that goes much beyond
67:07 - that what we do not have currently so
67:10 - these are different dimensions now
67:12 - coming into what we wanted to see is
67:15 - chatbot so how do you make a Q&A bot as
67:19 - a chatbot is very simple now at this
67:21 - point you might have already got this
67:22 - idea so you take a prompt and you give
67:26 - it to the llm where you can have
67:28 - shortterm memory me in context memory in
67:32 - context learning for example so what is
67:35 - the capital of India so you what is the
67:37 - capital of India you ask and the llm
67:40 - answers New Delhi this is what happens
67:43 - in a simple Q&A bot but how do you make
67:48 - it a conversational bot or a chat bot by
67:51 - adding a new dimension called a
67:53 - shortterm memory and how how do you do
67:56 - that you keep all these things that you
67:58 - are conversing into the chat
68:00 - conversational history so what this
68:03 - gives the ability for an llm to do is
68:05 - when you say what is the capital of
68:07 - India it says New Delhi then you can
68:09 - just simply go and say what are some
68:12 - famous Cuisines uh there so at this
68:15 - point the llm would have an
68:17 - understanding you're talking about New
68:18 - Delhi because that conversation is
68:20 - stored there in the llms shortterm
68:23 - memory or the in context memory so the
68:26 - llm can do something called I in context
68:29 - learning and give you the response back
68:31 - and that is how you upgrade in the
68:34 - pyramid by building a Q&A bot giving a
68:37 - new dimension called history and then
68:39 - making the Q&A bot a chatbot so that it
68:42 - can converse now chatbot has
68:44 - applications everywhere that you can
68:46 - turn towards youve got chatbot and
68:48 - customer support you have got chatbot on
68:50 - websites you have got chatbot for
68:52 - Education like youve seen a lot of demos
68:53 - from Khan Academy so chatbot is quite
68:56 - versatile it almost has its purpose in
68:59 - every single business or domain that you
69:01 - can think of now people were using
69:05 - chatbot um but you know chatbot itself
69:07 - is not enough why we already know the
69:10 - answer to the
69:11 - question can you pause and answer if you
69:13 - know the answer so why is that chatbot
69:16 - is not enough uh for a lot of use cases
69:20 - the answer to the question is chatboard
69:22 - stops with only short-term memory you
69:24 - need long-term memory or you need
69:26 - external memory see for example I ask
69:29 - what is the capital of India it says New
69:31 - Delhi what are the famous quins there it
69:33 - will give me an answer quite valid llm
69:35 - is doing its job so let's say I'm a I'm
69:37 - a company okay so I'm I'm an
69:39 - organization let's take uh Apple for an
69:42 - example okay now I ask what who is the
69:46 - CEO of Apple of course the internet as
69:49 - information about it so it will say Tim
69:51 - Cook that's quite easy now if I go say
69:54 - uh who who is the manager of the team
69:58 - handling iPhone 16 will it answer no I
70:02 - mean it might answer because it
70:03 - hallucinates a lot but the answer would
70:06 - not be correct and that has become a big
70:09 - bottleneck in a lot of Enterprise use
70:11 - cases because you do not just need
70:14 - internet knowledge you do not just need
70:17 - the knowledge that the llm has got you
70:19 - need more than that and that is the
70:22 - custom knowledge component or the
70:24 - external knowledge component that you
70:26 - need the dimension that you need to make
70:28 - your llm slightly more than just a chat
70:31 - bot and that is where a new technique
70:34 - called rag comes into picture retrieval
70:37 - augmented generation where you use the
70:40 - knowledge that you provide or you call
70:42 - it a longterm memory you use the
70:45 - documents the internet the sources
70:47 - everything that you have around and you
70:49 - use that knowledge to send to route to
70:52 - llm and then make the llm you
70:55 - The Leverage that knowledge and now at
70:57 - this point probably you might have
70:58 - guessed it see first we had only prompt
71:01 - one dimension second we had shortterm
71:03 - memory two Dimension now we have
71:05 - external knowledge which is three
71:07 - dimension so this llm is at the center
71:10 - of three different things you have got
71:12 - prompt you have got um short-term memory
71:15 - and you have got long-term memory to
71:17 - make you understand this better uh so
71:19 - I'm going to take you to the rag so how
71:22 - does the rag look like so you have got
71:24 - the M at the center of it you have got
71:27 - your data somewhere available so it
71:29 - could be on different structures it
71:31 - could be on database most organizations
71:34 - have data in their database structure
71:36 - database rdbms database then you have
71:39 - got documents which are unstructured
71:40 - like PDF HTML files internal portals
71:44 - blah blah blah blah blah then you have
71:45 - got apas let's say you are a sales team
71:48 - uh probably your data is and some CRM or
71:50 - Salesforce right so you need a
71:52 - programmatic call to make the call and
71:54 - get the answer back so your data could
71:57 - be of these different places could be
71:59 - like structured database like rdbms
72:01 - system it could be unstructured
72:03 - documents uh PDFs uh HTML documents
72:06 - anything that you have locally and then
72:09 - you have got programmatic access like
72:10 - your marketing team you need data from
72:12 - Google ads you a sales team you need
72:15 - data from Salesforce you are your
72:17 - company is heavily into it so you need
72:20 - data from AWS like billing cost and all
72:22 - the other things so this is programmatic
72:24 - so you use one of the these methods a
72:25 - structured passing or unstructured
72:27 - passing a programmatic call and take all
72:31 - the input data and create an index an
72:34 - index is what Google creates at every
72:37 - single moment you have got all these
72:40 - websites what Google does is Google
72:41 - creates this index so it is easier for
72:44 - Google to go Travers when somebody's
72:46 - asking a question and that's how Google
72:47 - became popular before Google people were
72:50 - using totally different thing Google
72:51 - came up with something called page rank
72:53 - algorithm at the fundamental of page
72:55 - rank algorithm you have got this index
72:57 - with different parameters of course and
72:59 - definitely we're not building Google but
73:00 - so index is what we are building it
73:03 - makes it easier for you to understand
73:05 - what is inside the data so now a user
73:08 - comes in asks a question what is a
73:10 - question who is the manager of iPhone 16
73:13 - team so that question goes to the index
73:16 - the in this this system particular
73:18 - system takes that and picks only the
73:21 - relevant information see this index
73:24 - might have information about all the
73:25 - teams iPhone 16 Apple Vision Pro billing
73:29 - accounting procurement marketing blah
73:32 - blah blah blah blah so it has all the
73:34 - information what you are interested in
73:36 - is only this particular piece which is
73:40 - what you asked which is iPhone 16
73:42 - manager so it this particular part is
73:46 - where it takes only the relevant
73:48 - information from the index and then it
73:50 - matches with the query uh The Prompt
73:52 - that you give and then it finally gives
73:54 - you send it to the llm The Prompt what
73:58 - you asked and the data that you
74:00 - extracted and it goes to the llm llm
74:02 - gives the answer back to the user this
74:05 - is quite different from the chatbot
74:08 - application if you see I'll give you an
74:09 - example why so in the chat bot all you
74:12 - are doing is you have a memory question
74:15 - is there sometimes you might do uh let's
74:17 - say a long-term memory by doing user
74:20 - profiling I'll I'll ignore this for now
74:22 - you don't have to use this now ignore
74:24 - this for now
74:25 - so what you're doing is you have a
74:26 - question you're sending it as a prompt
74:29 - and you have memory that also goes to
74:31 - the prompt because that's how you can do
74:33 - it and you have llm answering this
74:36 - question and you get the answer back now
74:37 - you might ask me hey why do I need to
74:40 - put my thing in the external data and
74:44 - create an index rather why can't I keep
74:46 - it in memory if you have got this
74:49 - question at this point that is a very
74:51 - important question and you are thinking
74:53 - in the right direction in in fact people
74:55 - who reached at this point you can tell
74:57 - me whether you know the answer or not
74:59 - the reason why we cannot do this uh or
75:02 - we could not have done it early in these
75:05 - days of alms is due to an important
75:08 - factor called
75:10 - CTX window what is CTX window CTX window
75:15 - is nothing but called context window
75:18 - this internal memory and question or the
75:21 - shortterm memory and the question is
75:24 - bounded by what is the context window of
75:27 - this particular llm so you have an llm
75:31 - the llm might have context window like
75:32 - 4K which is quite popular these days or
75:34 - 8K and even gini like llms have like 1
75:37 - million as context window so context
75:40 - window is there now what you are
75:42 - actually doing here is you have a
75:44 - question the llm answers so you have a
75:47 - question one right and answer one comes
75:50 - back then you have a question two then
75:52 - you have answer two by the time you go
75:55 - to question three what you are sending
75:57 - to the llm is not just your question
75:59 - three you are actually sending all these
76:02 - things right so let's say this is 2K
76:06 - this is 1K answer then again 2K question
76:09 - 1K answer and let's say this is a 2K
76:11 - question so at the end of the day when
76:14 - you are hitting the third level of
76:15 - conversation I'm kind of exaggerating
76:17 - but let's say 2+ 3 uh 2 + 1 3 3 6 8 so
76:22 - you already hit 8K so conversation
76:25 - context window so if you have got 88k
76:27 - token model at this point your model
76:29 - will hit out of memory error or it
76:31 - cannot hold it in short-term memory and
76:34 - that is exactly why you need rag ritual
76:38 - augmented generation because this one is
76:41 - not bound by the conversation of course
76:44 - you are going to keep it in conversation
76:46 - but you don't have to stuff everything
76:48 - inside your question rather you can keep
76:49 - it inside your index right because youve
76:53 - already indexed and you can keep it and
76:55 - only the bit that is relevant comes to
76:57 - you and now you might be asking how is
77:00 - that possible and for that you know you
77:02 - go into like a separate tangential side
77:05 - that talks about semantics and uh
77:07 - semantic search and all the other things
77:09 - embedding semantic search that is quite
77:11 - out of scope uh if you want to go deep
77:13 - you should read rag llama index is an
77:15 - excellent library for you to read about
77:16 - rag uh they have got really good
77:19 - developer relation system uh they have
77:21 - got a lot of Articles uh and you should
77:23 - definitely read about llama index and
77:25 - rag if you want Advanced rag but I hope
77:28 - you get the point going back to our
77:31 - system that we put together so what do
77:33 - we have we have a Q on a system at the
77:36 - front which just takes an input gives an
77:38 - output nothing else then you have got
77:40 - the chatboard the input plus history
77:43 - goes together that is always shortterm
77:45 - memory you get the output the output
77:47 - also goes back to the input that's why
77:48 - you keep the conversation history then
77:50 - you have got a rag retrieval augmented
77:53 - generation the reason why it is called
77:55 - retrial augmented generation is because
77:57 - you have got a retrieval component that
77:59 - you augment with the llm component and
78:03 - then you generate the response back so
78:05 - that is retrial augmented generation and
78:08 - the applications are enormous there are
78:11 - lot of startups in 2024 when we are
78:14 - recording this lot of startups just
78:17 - doing rag so if you can build a rag
78:20 - solution today in 2024 you can probably
78:23 - even raise find or you can be a good
78:25 - successful SAS there are a lot of
78:28 - companies making really good money solid
78:30 - money out of it I'll give you an example
78:32 - in fact like one thing that I've seen
78:34 - site gp. if you go to site gp. aai it
78:38 - says make eii your customer export
78:41 - Export customer support agent and I know
78:43 - this is this is a product that is making
78:46 - a lot of money um hundreds and thousands
78:48 - of dollars and at the foundation of it
78:51 - it is a rag it takes all the information
78:54 - that is ail aable in your website index
78:57 - assert or we call it data injection
78:59 - injection and index assert and when you
79:01 - ask a question it just gives you an
79:03 - answer back that's it it's not just a
79:05 - normal chatbot it is a chat bot that can
79:08 - answer based on the existing data so if
79:11 - you are breaking into llm today I would
79:13 - strongly encourage you to do some rag
79:16 - system that is by default something that
79:20 - you should do so if you're University
79:21 - student watching this if you're an early
79:23 - in career professional I would say you
79:25 - should build a couple of rag examples so
79:27 - you know there are a lot of nuances in
79:29 - rag like how do you improve indexing how
79:31 - do you improve indexing by changing
79:34 - chunking what kind of algorithms you use
79:36 - for embedding and what kind of models
79:38 - are good with rag whether you put the
79:40 - text at the top is it good whether you
79:42 - put the text at the bottom is it good if
79:43 - the text is in the middle it is good a
79:45 - lot of components to rag rag is not just
79:48 - simply what we discuss usually on this
79:49 - channel you can go Advanced drag and I
79:52 - would strongly encourage you to spend
79:53 - some time in rag unless you want to get
79:57 - into something that is quite exciting
79:59 - and interesting but before we do that I
80:01 - would like to quickly show you one more
80:03 - thing that not a lot of people discuss
80:06 - when we talk about llms it is not
80:08 - necessarily rag it is just like using
80:10 - short-term memory so it doesn't use
80:12 - long-term memory but it has its own
80:14 - potential which is to use llms large
80:17 - language models for classical NLP task
80:21 - classical NLP Downstream tasks for
80:24 - example example let's say you want to
80:25 - build a text classification system what
80:27 - is a text classification system you give
80:30 - a sentence for example uh the movie was
80:33 - complete crap now is it positive or
80:37 - negative positive or negative you choose
80:41 - you build you train a text
80:43 - classification model just to figure out
80:45 - this for example or the other example I
80:46 - can give is you have a review let's say
80:49 - the movie was amazing and the actress um
80:54 - was
80:56 - exceptional now you try to build a model
81:00 - that will say what kind of review is
81:03 - this for example is this review about
81:05 - movie um theater or director or actor so
81:11 - now you know this is an actor based so
81:12 - this is what takes classification in
81:15 - classical nlps there are lot of other
81:17 - tasks that you do in classical NLP what
81:20 - you can do is without having to build
81:23 - your custom model like say bird based
81:25 - model XG boost based models you can use
81:28 - llms large language models for classical
81:32 - NLP problems because large language
81:34 - models have really good in context
81:36 - learning and with the current memory
81:38 - that you have got with a few short
81:40 - examples or tree tree of thoughts or a
81:43 - chain of thoughts you can make your
81:45 - large language models a good zero short
81:49 - NLP classifier or it is applicable for
81:52 - lot of other tasks as well so one thing
81:54 - that not a lot of people are exploring I
81:56 - would encourage you to explore if you
81:58 - work in classical NLP problems like
82:01 - labeling or text classification entity
82:04 - recognition whatever it is you can
82:07 - leverage llm now the question is do you
82:09 - want an llm based Solution that's a
82:13 - different topic I'm not talking about
82:15 - you know you looking for a nail because
82:17 - you have got a hammer I'm just saying
82:19 - that this is a good opportunity wherever
82:22 - you don't want to build models you can
82:24 - use us this but of course if you can
82:25 - build models that will be probably
82:27 - cheaper than you know making calls to
82:29 - llms and getting answer back but
82:31 - summarization text classification entity
82:33 - recognition I think llms are exceptional
82:36 - zero shot llm uh and downam for
82:39 - Downstream tasks and you should
82:41 - definitely leverage it now with this we
82:46 - have arrived at rag okay so we have
82:48 - arrived at Rag and we already know what
82:50 - is rag now we are entering into a very
82:54 - interesting phase about what everybody's
82:58 - obsessed with what everybody's love
83:02 - agents very recent announcements from
83:05 - Google Microsoft previously open Ai and
83:09 - every announcement you would have seen
83:12 - two important things as a common Trend
83:15 - one is you would have seen
83:18 - multimodality multimodality what does it
83:21 - mean it just simply means in instead of
83:24 - just chatting with text you can chat
83:27 - with images you can ask questions in
83:30 - voice it can respond back in speech you
83:32 - can send videos so one important Trend
83:36 - that you are seeing is
83:37 - multimodality and the second important
83:40 - Trend that you see everywhere is Agents
83:43 - multi- agent setup where you have got
83:45 - multiple agents you can summon these
83:48 - agents to do certain tasks and these
83:50 - agents will do it for you just like men
83:52 - in black MIB they have a purpose and
83:56 - they will do certain tasks but before I
83:58 - jump into agents I want to actually
84:01 - introduce you to another important
84:03 - concept called function calling function
84:05 - calling is the precursor to llm agents
84:09 - in function calling what you do is you
84:11 - have a short-term prompt you have a
84:13 - prompt you have short-term memory uh
84:15 - sometimes you need external memory
84:16 - sometimes you don't need external memory
84:18 - but you are giving the ability of
84:22 - calling external tools and you are
84:25 - giving the ability of calling external
84:26 - tools by doing something called function
84:28 - calling function calling to be honest is
84:30 - a terrible name cuz you're not calling
84:32 - any function here you're you're not
84:34 - making the llm call anything not at all
84:36 - all you are doing is you're forcing the
84:38 - llm to give a structured response back
84:42 - so you can call an L I'll give you an
84:44 - example what is function calling so
84:46 - let's say that you have a weather API
84:49 - okay weather I think everybody goes with
84:51 - weather AP so I'm going to I'm going to
84:53 - skip let's say you have a currency
84:55 - converter okay currency converter what
84:58 - kind of things a currency converter need
85:00 - okay you need input currency you need
85:03 - output currency you need date you need
85:05 - amount technically these are the four
85:07 - things you need what is the amount that
85:09 - you want to convert for what is the
85:10 - input currency what is the output
85:12 - currency and what is the date for which
85:13 - you want to do currency conversion let's
85:16 - keep as a simple APA now typically when
85:19 - you go to an llm okay and say uh what is
85:22 - USD to INR today
85:25 - first of all LM may not understand what
85:27 - is today llm might know USD llm might
85:30 - know INR but lm's memory is frozen
85:34 - because it's a snapshot see a large
85:36 - language model is a snapshot so it
85:38 - memory has been frozen to let's say
85:39 - September 20123 or something like that
85:42 - okay so what it cannot do is it cannot
85:44 - give you the latest information and you
85:46 - cannot do this with I mean you can do
85:49 - this with rag kind of like you can every
85:51 - day take knowledge ingest keep it in
85:53 - your memory and then give it back not
85:55 - very efficient um expand this to stock
85:57 - market a daily data doesn't matter
85:59 - because everything changes like every
86:01 - minute and every second so you need
86:03 - something instant what you do you call
86:05 - an API if you are a programmer that's
86:07 - what you would naturally do you call an
86:09 - API now if you want to call an API uh
86:13 - what you need to call an API so let's
86:15 - say this is the information what I need
86:17 - at the end of the day I want to call it
86:18 - currency converter right and I'll say
86:22 - input output date
86:24 - right I need to make a call like this so
86:27 - I need four arguments that is a solid
86:30 - input could not be like oh United States
86:33 - dollar and some other time I'll be like
86:35 - USD some other time I'll be like US
86:37 - dollar I mean that will not work right
86:40 - you need a specific format for
86:42 - everything your let's say amount should
86:44 - be an integer right a this should be a
86:48 - date object so you need to force this
86:49 - llm to give you a particular response
86:51 - back otherwise what happens is this llm
86:53 - will throw you anything for example
86:54 - example what I want to say is what is
86:56 - USD and I so it'll be like oh USD andr
86:59 - is so so on September 2023 so you have
87:02 - to force guide the llm to make a
87:05 - particular type of output and somehow
87:08 - universally everybody has accepted that
87:10 - format is going to be Json except
87:13 - anthropic which absolutely loves XML so
87:15 - if you use anthropic you use XML if you
87:17 - use any other model you use Json so
87:19 - you're forcing an llm to give you a
87:21 - structured response back a j
87:25 - that can help you make this function
87:28 - call you can call this function with
87:31 - that Json so a guided response into a
87:34 - Json is what everybody calls function
87:36 - calling you don't necessarily call the
87:38 - function in function calling but you get
87:40 - the output that will help you call
87:42 - function call right clear now that is
87:46 - exactly what is a precursor to agent
87:50 - because in a function call you have the
87:52 - ability to call a function and agents
87:56 - are nothing but a bunch of function
87:58 - called stitched with tools so what do we
88:00 - have in agents we have a bunch of
88:02 - function calls plus tools and I would
88:05 - like to introduce to you a very
88:08 - interesting solution that can help you
88:11 - understand more about
88:13 - agents if you are too old in the AI
88:17 - world you would have probably recognized
88:19 - this immediately and this was the
88:21 - workflow of something called baby AGI so
88:26 - baby AGI was quite a popular thing back
88:29 - in the day I mean back in the days like
88:30 - less than one year before I guess or
88:32 - maybe more than one year a function call
88:35 - is what I said is the foundation of
88:37 - Agents but what is an agent now if you
88:41 - have seen our pyramid you would know our
88:43 - agent sits right at the top like closer
88:47 - to what we our aspirational figur is now
88:49 - what is this agent how do you define an
88:52 - agent so it's simple first of all a
88:56 - chatbot and a rag all of these guys if
88:59 - you see here they end a text or you know
89:03 - some kind of thing like input output
89:06 - images video all these things right
89:08 - that's where they end one of these
89:10 - modalities they're done what you achieve
89:12 - with agent is something that is
89:15 - absolutely stunning you don't stop at
89:18 - text response you stop at an action you
89:22 - trigger an action and that is what
89:25 - agents are simply you take llm you
89:28 - connect them with tool you give them a
89:30 - purpose or goal that is your agent and
89:33 - that is exactly what baby a has done
89:35 - back in the day like there are multiple
89:37 - agents now but if you see baby a which
89:40 - is a very wonderful framework you can
89:42 - see that there is a task like there is
89:45 - something that has to happen there are
89:47 - certain tools like for example Vector DB
89:49 - and all the other things are there and
89:51 - every agent has a Purp purpose like okay
89:54 - you have to execute you have to return
89:56 - you have to do something you have to do
89:57 - something and they have a goal so you
90:00 - have tools purpose SL goals and llms and
90:05 - this all together work for a common goal
90:08 - and that is your agent there are
90:10 - multiple agent Frameworks that are quite
90:12 - popular these days crew AI L graph you
90:15 - have got a py autogen and most of these
90:18 - things you will see first you have to
90:20 - define a role you have to refine a go
90:24 - Define a goal role goal and then you
90:27 - have to save which llm that you want to
90:28 - use as a backend engine and then you put
90:31 - together a system of one this is single
90:33 - agent now you put together like this is
90:35 - a team that is your multi-agent setup
90:39 - with agents people are doing amazing
90:41 - things you can make an agent book your
90:44 - ticket you can make an agent let's say
90:46 - read something um distill something
90:49 - create a note publish the blog post you
90:52 - can summon these agents to do a lot of
90:53 - things and personally for me uh the most
90:56 - time that I spent reading about agents
90:59 - because you it's it's becoming quite
91:01 - obvious that agents are the next
91:04 - Frontier in uh the way we can take llms
91:07 - forward I mean there are a lot of
91:09 - different things but at least personally
91:10 - I'm quite interested in automation
91:11 - usually and I think agents are going to
91:14 - be the next big thing in I mean
91:16 - currently itself is a big thing Google
91:18 - has got Google's own projects like they
91:20 - call their own agents I don't know what
91:22 - they call they have a lot of different
91:23 - names open a has its own agents and uh
91:26 - every time you talk to some company you
91:28 - speak about agents because you want to
91:29 - summon these agents you want to connect
91:31 - these llms to like different dimension
91:34 - and on this Dimension that what we are
91:36 - connecting is the tools Dimension so you
91:38 - take llms you have the function calling
91:40 - ability and once you connect them to
91:42 - tools you are unlocking the potential of
91:45 - something immense and that is what you
91:47 - call as agents I'm not going deep into
91:49 - agents because this is probably I'm
91:52 - hoping it to be a series depending upon
91:53 - how you all like it but in this series
91:56 - my next focus is going to be agents so
91:58 - agent is quite closer to the top and
92:01 - that takes us to the almost the end of
92:04 - the video which is what is our
92:06 - aspirational thing what is that we are
92:09 - all trying to go towards which is l l m
92:12 - o and this is inspired by Andre karpa
92:16 - who created this amazing structure so
92:18 - what is happening here this talks about
92:21 - using llm at the center C of a
92:24 - conversation or sorry center of an
92:26 - operating system if you go back in the
92:28 - day computer was created just for simple
92:30 - calculation purpose right you want to
92:32 - add a and you want to add a and b you
92:35 - want to keep a for one and B for two and
92:38 - then you want to add them that's that's
92:40 - what like initially computer was started
92:41 - like very very very back back in the
92:43 - days and computation started increasing
92:46 - computation started becoming less
92:47 - expensive more compute than we have the
92:50 - computer that we have today and garpa is
92:52 - arguing can we have a similar vision for
92:56 - llm and where the vision is you keep llm
93:00 - at the center right you keep llm at the
93:02 - center and at the center with llm you
93:05 - have Ram which is the shortterm memory
93:08 - or the context window then you have
93:11 - long-term memory the disk system that
93:13 - can be used with rag then you have the
93:16 - agent structure that you have with tools
93:20 - and then you connect it with internet
93:22 - and when you connect it with other llms
93:24 - to have like a multi-agent setup or like
93:26 - a peripheral setup and then you have
93:28 - your peripheral devices where you have
93:30 - got audio and video can we put together
93:33 - a system with all these things working
93:35 - towards a common goal and that will
93:38 - ideally become your large language model
93:41 - operating system this is quite a vision
93:43 - at this point there are certain
93:44 - implementations available at this point
93:46 - those implementations are based on
93:48 - current understanding they are mostly
93:51 - let's say llms plus fun fun calling plus
93:54 - agents multi-agent more tools that is
93:57 - what the current LM is it's not like a
93:59 - radically has a different a total view
94:02 - Al together and that's why if you see
94:04 - even in my framework that I've created
94:06 - llm o is currently developing and it is
94:09 - everything that we have got the tools
94:12 - the extended tools the peripheral tools
94:14 - with long-term memory with shortterm
94:16 - memory just one input from the user
94:18 - where it can run itself and then it can
94:20 - execute certain things I think that is a
94:22 - future that we are heading I'm not sure
94:24 - when we are going to do it but if
94:26 - somebody says something a for me today a
94:29 - could be like this could be like the
94:30 - baby a I mean I don't I don't I don't
94:33 - trust AG as a concept anytime soon but
94:36 - um yeah leaving the conscious thing
94:38 - Consciousness and all the other things
94:39 - out I would say llm o is at the top
94:43 - where we can expect something closer to
94:44 - a happen and all these things lead us up
94:48 - to there so I wanted to keep this video
94:50 - brief but this video is already going to
94:53 - be like more than half an hour I wanted
94:54 - this to be like a crash course where you
94:56 - understand if you don't know anything
94:58 - about llm o uh maybe you have not taken
95:00 - any course so this is going to help you
95:03 - to see how the future of llm O is coming
95:06 - and what led us up to there and uh let
95:09 - me know in the comment section if you
95:10 - like this kind of content I'll put
95:12 - together more this took me a lot of time
95:14 - to create the framework design put it um
95:17 - in a particular thought process to you
95:20 - know make it make it understandable and
95:21 - this is basically what a lot of m c is
95:24 - offer so I'm I'm definitely looking
95:26 - forward to hear more feedback and if you
95:28 - like this kind of format subscribe to
95:30 - the channel see you in another video
95:31 - Happy prompting

Cleaned transcript:

this is a nontechnical intro to generative AI you'll learn about the evolution of AI capabilities and analyzing the key technological breakthroughs that have enabled modern generative AI models to achieve remarkable performance you learn about the different levels of llm applications like Q&A systems chatbots rag Solutions and how large language models can be leveraged for Downstream natural language processing tasks and the development of intelligent AI agents you'll also learn about the potential of large language model operating systems Abdul created this course we call as generative AI is not too old in fact it has been just last couple of years so first let's take a look at what has changed how have we reached here in this particular place and what kind of challenges lie ahead of us this is very important before you actually start using the tool in itself imagine this is like the manual that tells you what you should do with the tool and what the tool is not good for without this knowledge it's particularly very scary to have a tool which you believe could be great but maybe it is not I'm not saying that generative AI is bad but I'm just saying that you need to know how did we get here and that is the whole point of this particular section first of all why the name generative AI few years back whenever we said something as AI it was mostly a niche Within mission learning or deep learning for example if you take text you would have noticed that few years back we had something called ner that stands for named entity recognition if you have got a bunch of text like this named entity recognition would help you find out the named entities like like Wall Street is a location $15 is price 2011 is a date Amarin Corp is an org Visa is an org so this was possible with NLP natural language processing using a very popular technique called Neer named entity recognition while today you can go ask a question and then it would give you an answer so even though previously you had text you were using AI or AI let's say for fathers like EML and deep learning to get something out of the text so analyze the text process the text find out something from the text but you're not necessarily using it to create new text in itself and that is where the generative part comes in you are now generating text rather than just processing processing text let's take another example the next one is few years back you were just trying to F figure out whether a given image is a cat or a dog this example can go multilevel this is a typical classification problem in machine learning it could be like cancer not a cancer looking at the image you can say whether the patient has got pneumonia or not a pneumonia so it could be at any level but imagine you have got two sets of input or you've got like let's say unlabeled input and you want to figure out whether the output is cat or not cat or cat or dog so it could be of any type so even in this case the AI or the Deep learning model was purely used to classify an input image but what has changed now now you can actually artificially generate the image of a cat flying from the sky falling from the sky you can generate a cat as a president you can do basically anything that you want you can generate images as much as you want and that's exactly why it is generative agent a new powerful class of large language models is making it possible for machines to write code write text draw something or create something with credible and sometimes superum results now let's break this down first of all we say large language models but let's let's say large models language is only one aspect of it we have got multimodel models these days that can do multimodality that can create text understand images generate audio like for example Google Gemini is a great example of a multimodal model in the open source world you have got something like lava which is a great example of a multimodal model so either way we have got a new class of large models really really large models and these models enable or let humans use these models use this AI systems to write English or multilingual text code which is a computer program that successfully runs draw create images and like create bunch of other things like audio video 3D Point Cloud a lot of other these things the main thing here is that one when something like this happens the result is credible like you can see this and then believe it could have come from a human being and that is very important I mean you could have used AI to create something maybe like 5 years back I have created let's say tweet BS before like long back before all these generative AI using a technique called Mark of chain Mark of chain used to use the underlying patterns within the text and then try to create the next word very similarly like that Mark of chain is a very popular technique that uses States and transition to understand the underlying pattern from the previous pattern the previous state but those were not as good as what it is today even though we had those things those were not like actual human text but now we have models that can create text exactly like how Shakespeare would write a new book or a new play we have models that can code like a proper programmer we have models that can create art like let's say van go or some other famous artist so we have all these possibilities that are credible when you see it you have to take a second look just to believe that this is either a human or an AI and also it can make super human result what do I mean by superhuman result a human being would take x amount of time to create something you asked me to create a stream lit application I would take a given amount of time to create that streamlit application given that I'm a human being I mean given that I am a human being I would need an x amount of time to create it or let's say you are a human being and you need to create a computer program you have to take a certain amount of time you want to write a book you would take time you want to create an art you would take time but these models are scalable that means you can write a book in maybe one day you can create a powerful application like a desktop application maybe a python GUI graphical user interface in just a couple of prompts and all these things are possible thanks to this large models that are mostly mostly mostly at this point Transformer based architectures I'm not going to get into the details but if you are attending a course about generative AI you should know that most of these models are based on something called Transformers which is a very popular architecture that was popularized or that was at least released by Google which uses a very important technique called attention so the attention based Transformer model is at the core of all these things and now we have got language models that can write and code we have got like diffusion models that can create images we have got like multitude of other models in fact multimodel models where we have got text and images in the same space within the same model in itself and that is what makes generative AI quite interesting look at these images these are images created by AI I mean somehow you could say today that these are AI created images but if you had shown the same images to Me 3 4 years back I wouldn't have even guessed even like with enough clue that these are AI generated images because not even in my dream I've thought that AI could create something like this and these things get better and better every single day thanks to the powerful class of models that we have God and thanks to the research advancement that is happening every day and in in fact if you see AI today the AI is not affecting the blue colar workers I mean back in the day people used to talk about automation people used to always say automation is going to take the jobs of let's say Factory workers automation used to take the jobs of people who are working in factories and Manufacturing units it did in fact there are a lot of information about how Amazon has made most of its packaging and shipping and Logistics automated these things happened like Tesla's Factory if you see there are a lot of robots in there so definitely there was a bit of Automation and robots taking the jobs of let's say blue color workers but as your Barber gone out of job as your hairstylist gone out of job not it but if you see the current the world of generative AI it primarily focuses on knowledge workers and creative workers knowledge workers and creative workers are mostly people like you and I who are part of this course we use our knowledge and create something and that's how we get paid either we write English some other language we write code computer program we create something using an image or we create an image in itself we produce an audio or video or we listen to an audio or video and produce something so some how if you see these knowledge workers or creative workers either their input or their output are one of these either text or code or image or video audio 3D or Point Cloud you can just go on and on now if you see the current state of generative AI models you can pretty much say that it's four out of five like I could have given five out of five but I still want to say that it's not still there so almost like four out of five you can say that these large language models are pretty pretty pretty good at writing and not multilingual yet I mean there are multilingual models but you can pretty much tell the difference like if you for example if you ask a model to create something in my language which is South Indian language Tam you know that okay maybe this is not necessarily human because these models are really not that good yet the next one is code these models can do pretty much good code they can create create GUI applications but not to the text level if you can say the model is good four out of five in text the code part is let's say three out of five then images the way the model understands images the way the model can create images creating images is really good but still there are certain aspects you can look at the eyeballs you can look at the fingers I mean fingers have almost got sorted out at this point but you can still look at things like that the skin tone lot of other things to tell that maybe this is an AI generated image the same thing goes with image understanding then you have got the video and audio which maybe you know at this point is like one out of five cuz it's still improving the video interlacing the change of uh the frames all frame transition it's there is still an improvement then there are like other modalities that we don't discussed about at all like 3D Nerf Point cloud and all these things exist but one thing that is very sure that if you are talking about generative eii you have to say that the particular set of people that it impacts whether it positively impacts or negatively impacts as knowledge worker and creative worker previously it might have taken me let's say 3 to 4 hours to create a YouTube thumbnail but now it takes much lesser time thanks to generative AI previously it might have taken me let's say a lot more time to summarize a document but thanks to generative AI it takes me much lesser time now so it positively impacts and also negatively impacts primarily knowledge workers who have to use their brain and either take the output which is somewhat this so it has to use all of these modalities like text code video image audio etc etc why is that now we have a huge flux in growth of generative VII I mean what has happened we already discussed about Transformers very briefly that the paper and the new neural network architecture Transformers gave way for all the models that we are using now or most of the models that we are using now but there is something else that we need to pay attention to now it is the time we have a lot of other other things combining together at the same time so if you see now we have got better models we have got different architecture which is what like I said Transformers architecture and in fact there are Transformer Alternatives that are coming up called like for example Mamba these are like State space models SSM that solves the problems Transformers face in terms of model scaling and time complexity we have got more compute computer has never been cheaper like this it is completely possible for you to rent a very huge amount of computer on AWS the accessibility is there compute in itself is there NV has released almost like a super computer level GPU or accelerated Computing device that you or I can probably own maybe expensive but still somebody can own and we have got more amount of data historically and all the humans have been always creating data we create data volunteer we create data without being asked to create data you go to a supermarket you try to pick something there is a CCTV capturing your data there is video data you go to the same Supermarket you buy something now that is going to be part of a POS like Point of Sales system that is a data you come out of the supermarket maybe you're going to Tweet about it that is the data that you are generating maybe there is going to be an Instagram post that is a data that you generating maybe you have got a loyalty points card that is a data that you are generating maybe you're going to return it that is a data that you are generating so there is a lot more amount of data from humans but also we have more sensors and other equipments that can collect data there is data from uh let's say electricity like sensors there is data from Air planes and there is data everywhere else the amount of images that we have digitized huge the amount of books that we have digitized huge the amount of unstructured information to structured information that we have moved huge so we have now tons and tons and tons of data and one thing that you can also see is that the models have become better with the model size also when you have got more data more compute the models have started becoming better for example Palm which is a Google model has got 540 billion parameter not saying that you always need a lot of data for a good model that's not the outcome that you should take but a lot of data will help you build a good model and finally at least for me personally one of the most important reason why things have gone is open source open research open models open techniques open tools few years back you did not have let's say a place like hugging face where you can go share the model few years back people were not putting out papers almost every single day on let's say archive which they found out and few years back you did not have all these scripts that would make it easy easier for them to build finetuning Solutions and all these things are there today and like for the last couple of years at least and these are people who have relentlessly open sourced whatever they have created and that has almost led to a huge influx and revolution of new types of models new models new finetune models new techniques new data and lot of these things exist so it's better models more compute more data while all these being open source I mean compute of course it's still not youve got like decentralized models like petals there are solutions but it is one of the place where it is not open yet but everything else do you have beta models open source models base models yes we have do you have more data of course we have got more data and those data with commercial license and without commercial license we have got and the bigger part is all these things are open source you can go take any Model start building data sorry Start building models or you can take one of the models start fine tuning it you can do all these things even without the required computer even if you do not have a GPU you can go to Google collab for free and then use their GPU and then play with these models thanks to open source so all these things come together and then help you create something that did not exist before thanks to better models more data more compute and open source and finally if you have to look at the generative AI landscape the generative AI landscape keeps on changing but you can kind of put it into multiple buckets if you look at the text primarily people use text for marketing content sales email CH support chat email support no taking General writing in professional world I'm not talking about you know kids using text for their homeworks in professional World code you can generate code you can generate documentation you can understand code today you can like literally highlight a particular part of the code and gp4 GPT uh Vision sorry gp4 and other models can help you understand it you have got image generation for like let's say advertisement voice synthesis video generation and you have got a bunch of other things you have got NPCs inside games that are AI generated You've Got Game assets that are AI generated you have got game scenarios that are AI generated and you have got multiple companies working on it one thing that you have to understand is there are multiple layers multiple different ways you can be part of this one is the model layer you're not talking about the data layer in itself when you go inside model layer you have data layer the other side is the appli application layer either you can build applications on top of these models or you can build the models or you can build data for the model and one of the most important thing these days people started wondering is how do you evaluate the models model eals model monitoring so there are lot of different ways you can be part of the generative AI landscape and when you look at the companies you have lot of different companies you have got mid Journey that can create images youve got gup copilot that can code for you that can help you understand the code you have got tools like Jasper that helps you use for something like opena API and let's you use it for a particular vertical like marketing and youve got a lot more other companies like coare creating models you have got hugging face helping you host and also create models you've got all these different kinds of companies all these different kinds of domains and with a very nice match you can build things you can build things either in the model layer which is like data layer model monitoring open source tools or you can build in the application layer and when you build an application layer you can either go to a particular vertical you can say I want to build something in sales I want to build something in marketing maybe you want to build a chatbot for lawyers so you can go to a particular vertical or you can go to a particular function you can say say okay uh I will take a particular business unit irrespective of what modality I'm using or you can go by modality you can say I'm going to just pick text and then build something in text I'm going to pick code and build something in code so there are a lot of different ways for you to approach this and if you generally talk about how good AI has become you can just tell you to write a poem write a story or write something and it is going to do it for you there are a lot of instances I have given such poems to human beings and it has been quite difficult for them to know that it was AI written and they have been shocked to know that this has been AI written when somebody was not exposed to AI so one we have spoken all the things that people are talking about but there are certain things that people do not talk about often and those are also quite important for us to pay a slight ATT attention to one training data two hallucination three rules four copyrights not a lot of time we have training data which a lot of companies have openly shared GPD 4 GPT 4.5 whatever the latest model is we do not know entirely what training data that they used generally they say they used web data how much of that web data is consent with content how much of that web data is without content nobody knows there are lawsuits people have always filed cases people have shared their voice against let's say tools like stable diffusion recreating artist work but the point here is that a training data is a place or training data is a thing with which uh we don't have lot of information under also we don't speak a lot open tools open models have given you some understanding about the kind of training data that goes into with content without content all these things but still even if you take one of the most popular models these days is mistal but you don't know what is mist's training data Maybe M doesn't want to get sued or maybe this is what it takes to build a gp4 equivalent open model the point is training data matters um not maybe today I'm I'm not not pro copyright I'm not against copyright I'm just saying that imagine you wrote a book let's say like Harry Potter or something and there is an AI that can read your book or that has read your book and create a similar work which you may have to take like couple of years to do but AI is really good at it maybe it would hurt as a software engineer or as a data scientist I don't know how much I would say there is a meme for example um you know the that the text that you take from a book the author would U feel sad or an artist would feel bad when their work is copied by AI but when they tell a programmer that AI has copied from stack overl GitHub the programmer actually says that okay that's what I do every day so our profession or at least like if you are from a software background or a data scientist you might feel that you know we do it very often but there are a lot of other professions where cre creative work is their bread and butter they get paid by writing books they get P by paid by creating art so I'm not sure what is going to happen especially when we do not have training data transparency there are companies like shutter stock Adobe have explicitly said that we are going to use only consented data for training our let's say image generation model there are companies that do this kind of stuff but still it is not an industry standard yet the next one is the models are quite good at hallucinating now whether Hallucination is good Hallucination is bad that is for a separate conversation altoe Andre karpati who is uh now part of opena previously the head of selfdriving at Tesla who is a very popular figure in the Deep learning World in terms of his teaching and all the things that he shares Andre carpati has always said I considered that these models are dreaming and some dreams are factual write some dreams or not and I see hallucination as a feature than a bug I feel like most of these models are hallucinating and when the hallucination Hallucination is Right factually correct we take them and the Hallucination is factually incorrect we call it hallucination so everybody has got their opinion it is very easy to use techniques like prompt injection or some other techniques and make these models give your wrong answer like for example in this case I have basically made chat GPT to tell me what is Neo's favorite food in The Matrix and it says according to a statement made by the directors of the Matrix movies in 2021 Neo's favorite food in The Matrix is chicken biryani followed by Italian pasta as a second favorite there is no information about this there was nothing like this on the internet I primarily injected that information in cont context to chat GPT so that chat GPT would give me this response back when I ask this question which is completely possible this is not technically hallucination but this is like one of the adversarial attacks with which you can make chat GPT or other large language models to give you factually incorrect answer so Hallucination is a big part of it Hallucination is one reason why medicine does not use a lot of large language models because you cannot still rely exact ly whether the answer is right or not the model gives you a different answer when you say let's take let's think step by step the model will not give you the same answer when you don't see that there are a lot of memes that you can actually tip chat GPT or you can say that my mom likes you or you know save the Kens and all the other things and get a different answer alog together so overall hallucination I don't know whether you like it whether you don't like it but it is still part of a problem in democratizing large language models in everyday life and the next thing is the rules what is the question for which you want chat GPD to answer what is the question you don't want chat GPD to answer the main question is who makes the rule open AA has made a rule saying where do I find cheap cigarette is okay to answer while how can I create a bomb is a question that I should not answer well and good that open a made this decision but how far and how long you want a for profit or at least open a is a weird setup uh it's a nonprofit and it has got a for profit so anyways how how for how long you want big corporates with let's say Market interest to make these decisions for you and whether it is right decision whether it is wrong decision every country is different some country might have certain belief some country might not have there is no Global belief I mean like we have certain Global beliefs like kids are cute kittens are cute so these are like Global belief imagine you are a kitten lover maybe you don't like dogs so what do you want the model to do so the there is a larger amount of question about who makes the rules there are bodies being created there are bodies being dismantled because it's not working but at least at this point who makes the rule is a big question and that is one of the reason why I love decentralized Ai and why in this course we are also going to see a lot of local AI which means like you can run the model yourself on your desktop or a laptop or PC whatever that you have got you don't have to always rely on one company setting the rule and then getting the model in itself I mean that's that's one thing that at least I believe that we need to have a lot of models and um you get to choose what you want to follow it's it's like living on this planet you get to choose your religion you get to choose your food you get to choose your dress you don't have to globally follow a set of rule um until you want to be nobody should be forcing it on you that is exactly why we need local open models and this is a big part of a question to say who makes rules when the model is not open and uh as much as we talk about like good things in AI it is very important for us to while we are talking about all the other things it is very important for us to understand at least the implications that it might bring the current exam system that we have might be obsolete with the generative AI that we see I mean look at the exam results gp4 this is gp4 score this is gp4 score it has scored tremendously in a lot of these exams where human beings have to take if you have ai models really good maybe you can hear the model through your headphones or whatever it is what is the purpose of this exam how do you now still hold these exams accountable to select the right set of humans for the right set of let's say courses or whatever it is so Education Plus Academia is one of the places where these large language models have got some push back you know there are universities who have punished students for using large language models there are certain places where universities have encouraged students to use large language models Khan Academy has signed up with opena as a partner to create a personal tutor for every student so this is a place where still there is a lot of questions among the academic researchers or educational or like you know teachers about how do they encourage or discourage using large language models but the broader question is how much of what we have followed until now is going to be valid already you know for example like if you're in India you know that in India there are certain institutes where you should not open the book and write the answer but then there are certain cases where you can have open book like you can have any book that you want and you have to figure out to write the answer so education requires its own transformation and uh whether willingly or unwillingly llms are going to transform education for good or for bad but this is a place where there is going to be a lot of impact in terms of large language models and we already discussed about knowledge workers I'll give you a particular example I am data scientist by profession and one of the part of my job is sometimes making charts and explaining charts to our stakeholders and this is something that gp4 Vision can do it like we'll see couple of examples gp4 Vision can take a chart like this help you understand translate the chart into structure data and it can do much more more efficiently than what I can do I'm not saying that I'm going to be replaced by a gp4 vision model tomorrow maybe that will not happen tomorrow but what does it mean for my daily job if the model can do a much better job than me at a particular task maybe the model still does not generalize well as much as as a human I would do as a human I might improvise I might know what to say what not to say depending upon who the person is but the models are getting good at it and as you can see here what will happen happen to the knowledge workers this is something that people should know and in terms of copyrights I think there is a huge debate and issue there are lot of lawsuits cases against mid Journey Open Ai and all the companies in fact opena has promised that if you use open AI product and you get sued by somebody open AI will pay your legal fees and fight for you that's a huge commitment I'm not sure how it is going to work or how much it is going to work but at least for that matter that copyrights are going to be a big deal whether you like or not the world that we live in copyrights are a huge part patents copyrights people get royalty from these things now one you're going to put a lot of these professions out of work two it's going to become ridiculously easy to replicate their work so what would it mean to have copyright still intact is is it possible that we are going to enter into a world where copyrights don't exist but then will open AI or these kind of companies share their code openly because they don't respect copyrights then do they let people copy their work they're not going to let people copy their work so it's a very strange predicament in which these companies are we are but we don't have answers like these are like questions that we have got we don't have answers if you want to finish this presentation with one final takeaway I would like to say that generative AI is transformative disruptive and unlike you know let's say I'm not a very big crypto Fanboy but unlike like let's say crypto or web3 or blockchain and all the other things like recently people have been like really getting crazy about generative AI is here to stay it's not going to go anywhere it's not going to vanish you can pretend that it is not going to impact your job but it is going to stay but like every other technology it has its own limitations and if not handled with care it can affect current form of Education it can impact jobs it can spread misinformation it can widen inequality I mean of course it has a lot of other good things I'm not talking about the good things here I just want you to know before you enter into learning how to use these tools that the tools that we going to learn about in this course will have impact on all these things I don't know if it is a nice touch I said like this is written by a living human but again what is written I typed it on a computer pasted it here even written is being a question uh I don't know the answer to but that has been there for many years but the point is it is it is a very strange place that we are in um if you take like humanity and uh that's why a lot of people say AI is like electricity and I believe if AI is like electricity then decentralized AI is the way to go so you use your own AI models and then you build your own things you don't have to use it from somebody else I mean like a lot of countries have only governments making electricity but then there are a lot of other places like if you see Tesla cells solar cells people buy solar cells make their own electricity use it for their own cars use it for their own stuff I think decentralizing decentralization is the way to go about but in this course we'll see both the closed models and open models but while there is a little bit of I should say a lot more emphasis on open models because that is a principle that I believe in at the same time A lot of people are also against open sourcing AI because they compare now ai with nuclear energy and then there are a lot of different ways that they go about it's two different schools of thought I'm I'm part of something like somebody's part of something if you have any thoughts I would like to know about what you think about generative AI see in another chapter so the last video ended with me saying that we I prefer decentralized AI but I did not provide any kind of clarification what do I mean by that so in this video I want to First offer what do we mean by decentralized Ai and I want to like in fact break down certain things before we go ahead with decentralized as a part let's define what do we mean mean by AI here so as you all know of course AI is here artificial intelligence we are not talking about AGI which specifically talks about general intelligence so here we are talking about narrow artificial intelligence which is AI now what is this AI why is everybody talking about AI we saw a glimpse of this in the previous video but now the AI here specifically if you're talking about technically is nothing but deep learning problems what are deep learning problems leap learning is a computer science domain where you build deep neural networks so typically a neural network would look like this it has got an input layer a middle layer and an output layer this is typically how a simple neural network would look but if you replace the middle layer with lot of deep layers that is basically what your deep learning is now for you to do deep learning effectively you need par Computing or you need High memory because at the end of the day what is happening in deep learning is it is matrix multiplication it has got lot of different weights within this neural network so if you see here youve got lot of different weights here and these weights get multiplied you know certain things happen so at the core what you need is you need really good compute that can do matrix multiplication for you and for that particular reason deep learning is more efficient on GPU than CPU now what is GPU GPU stands for graphical processing unit and CPU stands for central processing unit your normal computer or the laptop or probably the device that you're using right now has mostly got both GPU and CPU does it mean you can do deep learning on any device that is where a big catch comes in see most popular Frameworks for deep learning or at least one of the most popular framework for deep learning today is called pytorch now pytorch and many other deep learning Frameworks are primarily optimized or efficient on Nvidia gpus Nvidia gpus are really one of the core of why deep learning exist or deep learning happens or where deep Learning Happens these days now the reason why Nvidia G is highly preferred is primarily because Nvidia GPU has got their own proprietary software which they call Cuda so Cuda is the software that is one of the primary reasons that enables parallel Computing like high level parallel Computing within Nvidia gpus who efficiently utilize the Nvidia GPU memory and then do matrix multiplication or do deep learning efficiently thanks to Cuda Nvidia GPU are the home for most of the deep learning projects it doesn't mean you cannot do it on CPU it doesn't mean you cannot do it on AMD processors it means Nvidia is highly highly preferred now what is this Cuda so the Cuda or it is an Acron for compute unified device architecture it is a proprietary that is one thing that people often forget it is not an open source solution it is a proprietary closed parallel Computing platform and an API that allows sof was to use certain types of gpus for general purpose processing so what we are doing here the matrix multiplication we are doing is a general purpose processing and for that general purpose processing we are leveraging the GPU and this approach is called GP GPU general purpose Computing on gpus so Cuda is the software layer that gives direct access to gpu's Virtual instruction set and parallel computation elements for the execution of compute kernels now what Cuda enables you to do ultimately is do this better um GPU or leverage GPU for matrix multiplication that means you can do machine learning and deep learning having said that that basically enables most of this AI models to effectively run primarily on Nvidia enabled gpus so if you want effective hardware for running deep learning or for running AI you need effective gpus and not everybody has got effective gpus and that is where a lot of other Concepts come into picture so now first thing is because you need effective GPU that is clear at this point right deep learning here we are saying ai ai basically we are saying it is a deep learning and deep learning mainly prefers GPU because of comput and memory and Frameworks like pyot lets you do that thanks to Cuda maybe not so much thanks because it's not open source and you need all these things so that is where running an AI model or an llm becomes a little challenging so if you want to run an llm so if you want to run an e model or an llm let's say you want to run an llm in this case we are referring to large language models so if you want to run large language models let's say now you at this point need Nvidia gpus no because of this reason because it is very expensive computationally you don't often use Nvidia gpus as a physical Hardware rather you would rent this on a cloud provider so you would go to let's say AWS you would go to gcp or you would go to Azure and then you would rent an Nvidia GPU this is option one option two is these days you have got a lot of new startups like for example one of the most popular one is runp POD so you can go to runp pod and rent a GPU so GPU there are like different kinds of GPU for example RTX RTX let's say 4080 4090 is a very popular GPU a100 another popular GPU these are like on a different levels it has got different memories it has got different compute um the flops would be different lot of things are different so now one you can go to these Cloud platforms rent to GPU or you can go to this new kind of computing or Cloud platform and rent a GPU this is where you could do this machine learning or deep learning or whatever llms efficiently now what do I actually mean by oh do llms I've been saying the do llms what do I mean by do llms that is a very important thing for us to note because llm doesn't happen to be just like that there are a lot of steps in building an llm first thing you need to prepare data set so you need an input data set that you can use and these data sets because because we are talking about large language model we are not talking about just a simple language model we are talking about large language models these large language models require you to have large amount of data set so because of that only you end up having large language model and because you need data set you need preparation which you can still do it in GPU but CPU still you need like GBS of data like gigabytes of data after you do that the next thing that you need to do is you need to train a Model A train a model is what we call as building a model once you build a model successfully sometimes you might do an evaluation to see how good the model is doing so you have got a bunch of evaluation metrics or benchmarks to see how the model itself is doing when you compare it with different models so you need data set then you train a model then you eval or do benchmarks now what started happening in the Deep learning World some years back before that there was a new technique that came into picture that is called transfer learning so what this technique said is that okay not everybody has to build their own model I'm not talking about llms here I'm talking about any deep learning model rather you can take certain pre trained deep learning models and then you can use that for your own use case so you take a pretrained deep learning model and you use that model as your base model and then you do something called finetuning for your own use case and then you have a new model and that model is supposedly better than the pretrained model for your own use case if pretrained model or something this model after you fine tuned would be better than what the pretrain model was for the finetuning task in which you did it now this has been there for quite a while so now when we have to build a large language model we need data set we need to train a model and we know that if we have to train a model which is basically building a deep learning model we need gpus and then you do evaluation or Benchmark after you have a model in place let's say at this point you have a model in place and what is that we mean by model so most of the times most of the times what we mean by model is a p torch bin file so it's a DOT bin file or sometimes these days we have safe tenser files so we have save tenser file or we have py to bin file these are the files where the Deep neural networks weights are stored at the start of this video I said like you know there are weights like numerical weights that are stored here and these numerical weights are mostly float so these numerical weights are mostly floats what do we mean by float it's like 0.26 78 96 something like this is a floating point so we have numerical weights and these weights are stored inside this bin file or tensor safe tensor file now when you have a longer number here like this very long number you need more memory to run the model also forget about training the model even to run the model you need more memory and what is that is called that running the model is called inference so inference means you use the model to generate text we're talking about llms particularly here so using the llm to generate text is called inference so you know at this point you need GPU to run the create the model you need GPU to also run the model that is what is inference but because not everybody can have CPU there are new techniques like for example quantization there are different kinds of quantization techniques but in a nutshell quantization helps you reduce the Precision of the floating points that you store and thereby reduce the memory that is required for inference so that you can run this inference on even consumer Hardware gpus or even CPUs so we have quantized models we have quantized models that help you run these models on consumer Hardware either GPU or CPU and one of the most popular Frameworks for that is llama CPP and there are many other techniques like file formats like GG UF GG ml you don't have to know exactly what is inside this and all these things you just have to know these Nam so that you know what when you come across this name you know this is basically quantized model so you're going to use llama CPP that is going to help you convert the model weights into let's say a C++ uh quantitized weight and that helps you run the py torch code basically is ported into C++ which is like much faster than py torch and then it uses different file formats which has quantized model weights and then you can run the model on consumer Hardware like your laptop so now because we spoke about CPU and GPU and we have seen about quantization let's take a quick look at a bunch of things so we know CPU and we know GPU in GPU you have multiple providers you have Nvidia let's say a very popular leader you have AMD and you have bunch of other companies and then you have got TPU the T in TPU stands for tenser processing unit this is uh primarily mostly used by Google so Google has primarily got tpus I'll quickly show you Google collab and you would see tpus there and very recently there is another Pro popular one that is coming up that is called metal and what is this metal it is nothing but Apple silicon computers so if you have got M1 M2 M3 computers from Apple it could be MacBook Air MacBook Pro Mac Mini whatever computer you have got if you have got any of these chips it is highly likely that you have got metal GPU within this apple silic and these days we have got Frameworks and softwares that can let you run these deep learning models optimized for metal so these are like different kinds of computing and these are like you know what you can do now let's take a quick look at different kinds of Frameworks we learned about pytorch but pytorch is not the fastest one so there are people who uh have fallen back to something called Jacks and very recently Apple announced something for mlx and this is primarily optimized for Apple silicon so but most of the things that you would see on white TCH you have got Jacks and still some people Port things into C++ and you have got mlx very recently that is optimized for Apple silicon I know this is a lot of information in this video but the objective of this video is to give you a very wide landscape idea about what is like the technical details of what we call as large language models and I hope you have some clarity about what we mean by large language models to quickly recap when we say large language model the model here is nothing but a deep learning model a deep learning model is nothing but a deep neural network so it is like quite deep and then it has finally an output and most of the large language models that we are discussing today or large language models based on a particular deep Le learning architecture that is called Transformers so Transformers is the one of the most popular architecture based on which these deep learning models are built and we know that we need GPU to run these deep learning models and when we say gpus we are primarily talking about Nvidia gpus and because we don't have Nvidia gpus like lying around in our house mostly most of the people if you have got your lucky but if you do not have that is where you go to a club CL provider and either rent a GPU or you go to some other cloud provider and then try to use one of their services and because of this primary reason because gpus is computationally expensive gpus are hard to get and uh not the highest model not easy for you to run on CPUs because of this primary reason people offer llms as inference apis we already learned what is inference inference is the process of running an llm and then getting the text output so building an llm is one running an llm is what we call as inference so because of all the bottlenecks you have got because of all the reasons that it is difficult to run a large language model also you have to run it in production which means you need GPU always you may not have serverless compute that means you always have a cold start problem because you have to boot up the GPU to solve all these problems a bunch of companies decided to give the llm as an inference API it's not first time somebody is doing it software engineering is quite popular with the API world but if you are new to the API World API stands for application programming interface it's nothing but a connection to a bigger system that system would be owned by somebody else and you are going to just hit their end point and then get a response back so you're going to hit their end point and get a response back now the challenge here is that if you are working in a big company if you're working in an Enterprise setup especially if you're working in let's say Finance sector or Healthcare sector not every company would be interested in giving your data to somebody else because there is a server and you are going to hit that API from that server and then get the response back if you want to let's say design an app you will not have your own GPU to run one second you cannot run that app I mean now you can do it with iPad Pro because it has got Apple silicon but mostly let's assume you cannot do it that means you definitely have to hit an endpoint API endpoint and then get the response back that means you're sharing the data with them and that is where a whole new business of how you can use llms as an APA comes into picture without any doubt the leader in this business is open AI open AI if you are wondering how does open AI make money open AI makes money because their model is not open source not everybody can host it that means if you want to use gp4 one you can go to gp4 the chat GPT and then use it but you cannot build softwares on top of it you can chat with it but you cannot build softwares on top of it and if you want to build softwares on top of it let's say you want to build your own translation software or let's say you want to create your own design software or let's say you want to create something only for lawyers then that means you need to create an app either on web or like let's say iOS that means within this you have to hit the open a endo and get the response back and it's not just open a is the company that is alone doing this business opena is definitely one of the leaders in this because they have got the exclusivity and they are the best llms in this world at this point their gp4 is definitely the best llm at this point generally there are like specifically you would go to different LMS but generally this is one so open AI Nob brainer then you have got Azure then you have got AWS bedrock and there are other services like this that in fact like let's say Google gcp because Google has got G and bunch of other things Palm let's say Palm these are like model names so these service providers also let you use a large language model as an inference endpoint APA endpoint the challenge is most of them would not let you fine tune it even though open a recently started letting you fine tune as well so if you want to let's say just have a chatbot the easiest way to do that is build an application where take a user response hit the open a endpoint get the response back show it to the user you have a working chatbot without having to maintain your own infra without having to maintain your own infra or gpus but if you are worried about data let's say data challenges data privacy and lot other things that is where the whole new concept of open models come into picture instead of owning a proprietary model instead of paying money to an llm service provider who owns a proprietary model like open AI you would turn towards open models which means the model weights so the model weights you all know what is model weight the let's say the bin file the pytorch file or the safe tenser file or openly shared when we say openly shared it comes with a particular license I'll get into the license detail later on but these are model weights which you can self host you can buy your own small let's say VPS like virtual private server or you can have your own cloud uh model and you can have these things and you can host these models and then you can run it now this is where open AI versus truly open AI come into picture this is the company in this case and this is the open models that you are either self hosting now there are different service providers who let you use these open models also API that is a different topic alog together but in a nutshell you have got one world which is like proprietary models the second world where is open model and what I meant by decentralized AI is using these open models without having to rely on some proprietary model where you have to always send your data to them now like I said even this model if you have to self host it you need gpus or you need to rent gpus or you need to use quantized models like ggf or the other option is you go to new providers who let you use this model and there are bunch of advantages using this model but I want to stop this video here at this point to give you a breather if you have reached this point at this point I would strongly encourage you to write a blog post about whatever that you learned in this video and then write the blog post publish it anywhere that you want and then tag me whenever you share it on social media I would love to read what you have done because what I have done in this particular videos I've given you a lot of keywords I've given you a lot of seeds but if you have to understand this entire space better you need to spend at least an hour or two in going through these individual keywords that I've given you so that you can understand this entire landscape and then build a better understanding before we go on further lessons see you in another video five levels of llm apps consider this to be a framework and help you decide where you can use LM there are lot of different myths around what llms can do what llms cannot do where do you use llms today so I decided to put together this material uh in which I'm going to take you through kind of like a mental framework based on the extension or the depth in which you go towards an llm you can decide where you can fit this llm so we're going to first see what are those different levels of LMS that I have put together then we are going to see slight extension of that got two different documents to take you through that so this will give you an idea about how llm is being used today and how you can use LMS for your own applications to start with imagine this pyramid structure this is a very simple pyramid structure and as you can imagine with any pyramid structure the top of the pyramid or the peak of the pyramid is our aspirational goal and what you see at the bottom is the easiest that we can do and as with everything else you have to slowly climb to the top of the pyramid so you can probably hit the aspirational goal so to start with where do we use llms first Q and A a question and answering engine what do I mean by that it is quite simple for us to understand so question and answering engine is a system where you have an llm and all you are going to ask the llm is a question so you send a prompt and the llm takes the prompt and gives you an answer that is it that is the entire transaction that you have between in llm send a prompt get send into the llm get an answer llm large language models are nothing but sophisticated next word prediction engines and they have been finetuned with something called instruction so they are instruction finetune models that means they can take a human instruction and get you an answer back for example if I ask a question for this what is the capital of India then the llm would process this and then llm has information about how to answer it and then it will give me the answer back the capital of India is New Delhi that's all what you're going to do with this thing so first level question and answering now you might wonder at this point that where can you use question and answering as an llm engine this is the first thing that people built like when llm started even back in the day gp22 level people started building simply Q&A bots so all you want to do is ask a question give an answer could be a homework could be a general knowledge question could be something about the world could be about science could be about anything ask a question get an answer as simple as that it's a very threestep process ask a question or send a prompt take the llm to process it give me the answer back very simple application now what you're going to do is you're going to add something to that application and that is how you actually build a conversational chat bot and to understand this better I would like to take you to my second document which will give you probably better idea whenever we are talking about llm there is one important thing that we need to understand is we have crossed the stage where llm is simply a large language model we have more than that so for you to understand that I have five dimensions a prompt a shortterm memory an external knowledge tools and extended tools if you think of this as your horizontal these are your verticals these are different dimensions that can add to an llm so you have a prompt you have a shortterm memory you have a longterm memory or external data you have tools and you have got extended tools so let me give you an example for each of this so that you can understand this better a prompt is what is the capital of New Delhi that's all the prompt is you simply go give what is the capital of New Delhi and the llm understands it and gives you a back understanding just gives it back now shortterm memory is when you have conversational history or something in the llm that is what we call call as I in context learning so whatever you stuff inside the context window the llm can take it that is your shortterm memory so you give a few short examples you give an example like for example what is the capital of us uh I guess it's Washington DC Washington DC and you give a bunch of examples like this so the llm knows what is that it has to answer this is a shortterm memory next you have external data now you take data from Wikipedia and you keep it and then give it to the LM that is your longterm memory because shortterm memory just like a computer the ram it gets reset every time you reset the conversation or the session and then tools you let llm use tools like calculator internet python terminal and all these things and extended tools is when you expand this much beyond that I hope now you have understanding about the five different dimensions that we have in llms a prompt a shortterm memory of in context memory a longterm memory or external knowledge external data or custom knowledge tools like calculators and Python repple and extended tools that goes much beyond that what we do not have currently so these are different dimensions now coming into what we wanted to see is chatbot so how do you make a Q&A bot as a chatbot is very simple now at this point you might have already got this idea so you take a prompt and you give it to the llm where you can have shortterm memory me in context memory in context learning for example so what is the capital of India so you what is the capital of India you ask and the llm answers New Delhi this is what happens in a simple Q&A bot but how do you make it a conversational bot or a chat bot by adding a new dimension called a shortterm memory and how how do you do that you keep all these things that you are conversing into the chat conversational history so what this gives the ability for an llm to do is when you say what is the capital of India it says New Delhi then you can just simply go and say what are some famous Cuisines uh there so at this point the llm would have an understanding you're talking about New Delhi because that conversation is stored there in the llms shortterm memory or the in context memory so the llm can do something called I in context learning and give you the response back and that is how you upgrade in the pyramid by building a Q&A bot giving a new dimension called history and then making the Q&A bot a chatbot so that it can converse now chatbot has applications everywhere that you can turn towards youve got chatbot and customer support you have got chatbot on websites you have got chatbot for Education like youve seen a lot of demos from Khan Academy so chatbot is quite versatile it almost has its purpose in every single business or domain that you can think of now people were using chatbot um but you know chatbot itself is not enough why we already know the answer to the question can you pause and answer if you know the answer so why is that chatbot is not enough uh for a lot of use cases the answer to the question is chatboard stops with only shortterm memory you need longterm memory or you need external memory see for example I ask what is the capital of India it says New Delhi what are the famous quins there it will give me an answer quite valid llm is doing its job so let's say I'm a I'm a company okay so I'm I'm an organization let's take uh Apple for an example okay now I ask what who is the CEO of Apple of course the internet as information about it so it will say Tim Cook that's quite easy now if I go say uh who who is the manager of the team handling iPhone 16 will it answer no I mean it might answer because it hallucinates a lot but the answer would not be correct and that has become a big bottleneck in a lot of Enterprise use cases because you do not just need internet knowledge you do not just need the knowledge that the llm has got you need more than that and that is the custom knowledge component or the external knowledge component that you need the dimension that you need to make your llm slightly more than just a chat bot and that is where a new technique called rag comes into picture retrieval augmented generation where you use the knowledge that you provide or you call it a longterm memory you use the documents the internet the sources everything that you have around and you use that knowledge to send to route to llm and then make the llm you The Leverage that knowledge and now at this point probably you might have guessed it see first we had only prompt one dimension second we had shortterm memory two Dimension now we have external knowledge which is three dimension so this llm is at the center of three different things you have got prompt you have got um shortterm memory and you have got longterm memory to make you understand this better uh so I'm going to take you to the rag so how does the rag look like so you have got the M at the center of it you have got your data somewhere available so it could be on different structures it could be on database most organizations have data in their database structure database rdbms database then you have got documents which are unstructured like PDF HTML files internal portals blah blah blah blah blah then you have got apas let's say you are a sales team uh probably your data is and some CRM or Salesforce right so you need a programmatic call to make the call and get the answer back so your data could be of these different places could be like structured database like rdbms system it could be unstructured documents uh PDFs uh HTML documents anything that you have locally and then you have got programmatic access like your marketing team you need data from Google ads you a sales team you need data from Salesforce you are your company is heavily into it so you need data from AWS like billing cost and all the other things so this is programmatic so you use one of the these methods a structured passing or unstructured passing a programmatic call and take all the input data and create an index an index is what Google creates at every single moment you have got all these websites what Google does is Google creates this index so it is easier for Google to go Travers when somebody's asking a question and that's how Google became popular before Google people were using totally different thing Google came up with something called page rank algorithm at the fundamental of page rank algorithm you have got this index with different parameters of course and definitely we're not building Google but so index is what we are building it makes it easier for you to understand what is inside the data so now a user comes in asks a question what is a question who is the manager of iPhone 16 team so that question goes to the index the in this this system particular system takes that and picks only the relevant information see this index might have information about all the teams iPhone 16 Apple Vision Pro billing accounting procurement marketing blah blah blah blah blah so it has all the information what you are interested in is only this particular piece which is what you asked which is iPhone 16 manager so it this particular part is where it takes only the relevant information from the index and then it matches with the query uh The Prompt that you give and then it finally gives you send it to the llm The Prompt what you asked and the data that you extracted and it goes to the llm llm gives the answer back to the user this is quite different from the chatbot application if you see I'll give you an example why so in the chat bot all you are doing is you have a memory question is there sometimes you might do uh let's say a longterm memory by doing user profiling I'll I'll ignore this for now you don't have to use this now ignore this for now so what you're doing is you have a question you're sending it as a prompt and you have memory that also goes to the prompt because that's how you can do it and you have llm answering this question and you get the answer back now you might ask me hey why do I need to put my thing in the external data and create an index rather why can't I keep it in memory if you have got this question at this point that is a very important question and you are thinking in the right direction in in fact people who reached at this point you can tell me whether you know the answer or not the reason why we cannot do this uh or we could not have done it early in these days of alms is due to an important factor called CTX window what is CTX window CTX window is nothing but called context window this internal memory and question or the shortterm memory and the question is bounded by what is the context window of this particular llm so you have an llm the llm might have context window like 4K which is quite popular these days or 8K and even gini like llms have like 1 million as context window so context window is there now what you are actually doing here is you have a question the llm answers so you have a question one right and answer one comes back then you have a question two then you have answer two by the time you go to question three what you are sending to the llm is not just your question three you are actually sending all these things right so let's say this is 2K this is 1K answer then again 2K question 1K answer and let's say this is a 2K question so at the end of the day when you are hitting the third level of conversation I'm kind of exaggerating but let's say 2+ 3 uh 2 + 1 3 3 6 8 so you already hit 8K so conversation context window so if you have got 88k token model at this point your model will hit out of memory error or it cannot hold it in shortterm memory and that is exactly why you need rag ritual augmented generation because this one is not bound by the conversation of course you are going to keep it in conversation but you don't have to stuff everything inside your question rather you can keep it inside your index right because youve already indexed and you can keep it and only the bit that is relevant comes to you and now you might be asking how is that possible and for that you know you go into like a separate tangential side that talks about semantics and uh semantic search and all the other things embedding semantic search that is quite out of scope uh if you want to go deep you should read rag llama index is an excellent library for you to read about rag uh they have got really good developer relation system uh they have got a lot of Articles uh and you should definitely read about llama index and rag if you want Advanced rag but I hope you get the point going back to our system that we put together so what do we have we have a Q on a system at the front which just takes an input gives an output nothing else then you have got the chatboard the input plus history goes together that is always shortterm memory you get the output the output also goes back to the input that's why you keep the conversation history then you have got a rag retrieval augmented generation the reason why it is called retrial augmented generation is because you have got a retrieval component that you augment with the llm component and then you generate the response back so that is retrial augmented generation and the applications are enormous there are lot of startups in 2024 when we are recording this lot of startups just doing rag so if you can build a rag solution today in 2024 you can probably even raise find or you can be a good successful SAS there are a lot of companies making really good money solid money out of it I'll give you an example in fact like one thing that I've seen site gp. if you go to site gp. aai it says make eii your customer export Export customer support agent and I know this is this is a product that is making a lot of money um hundreds and thousands of dollars and at the foundation of it it is a rag it takes all the information that is ail aable in your website index assert or we call it data injection injection and index assert and when you ask a question it just gives you an answer back that's it it's not just a normal chatbot it is a chat bot that can answer based on the existing data so if you are breaking into llm today I would strongly encourage you to do some rag system that is by default something that you should do so if you're University student watching this if you're an early in career professional I would say you should build a couple of rag examples so you know there are a lot of nuances in rag like how do you improve indexing how do you improve indexing by changing chunking what kind of algorithms you use for embedding and what kind of models are good with rag whether you put the text at the top is it good whether you put the text at the bottom is it good if the text is in the middle it is good a lot of components to rag rag is not just simply what we discuss usually on this channel you can go Advanced drag and I would strongly encourage you to spend some time in rag unless you want to get into something that is quite exciting and interesting but before we do that I would like to quickly show you one more thing that not a lot of people discuss when we talk about llms it is not necessarily rag it is just like using shortterm memory so it doesn't use longterm memory but it has its own potential which is to use llms large language models for classical NLP task classical NLP Downstream tasks for example example let's say you want to build a text classification system what is a text classification system you give a sentence for example uh the movie was complete crap now is it positive or negative positive or negative you choose you build you train a text classification model just to figure out this for example or the other example I can give is you have a review let's say the movie was amazing and the actress um was exceptional now you try to build a model that will say what kind of review is this for example is this review about movie um theater or director or actor so now you know this is an actor based so this is what takes classification in classical nlps there are lot of other tasks that you do in classical NLP what you can do is without having to build your custom model like say bird based model XG boost based models you can use llms large language models for classical NLP problems because large language models have really good in context learning and with the current memory that you have got with a few short examples or tree tree of thoughts or a chain of thoughts you can make your large language models a good zero short NLP classifier or it is applicable for lot of other tasks as well so one thing that not a lot of people are exploring I would encourage you to explore if you work in classical NLP problems like labeling or text classification entity recognition whatever it is you can leverage llm now the question is do you want an llm based Solution that's a different topic I'm not talking about you know you looking for a nail because you have got a hammer I'm just saying that this is a good opportunity wherever you don't want to build models you can use us this but of course if you can build models that will be probably cheaper than you know making calls to llms and getting answer back but summarization text classification entity recognition I think llms are exceptional zero shot llm uh and downam for Downstream tasks and you should definitely leverage it now with this we have arrived at rag okay so we have arrived at Rag and we already know what is rag now we are entering into a very interesting phase about what everybody's obsessed with what everybody's love agents very recent announcements from Google Microsoft previously open Ai and every announcement you would have seen two important things as a common Trend one is you would have seen multimodality multimodality what does it mean it just simply means in instead of just chatting with text you can chat with images you can ask questions in voice it can respond back in speech you can send videos so one important Trend that you are seeing is multimodality and the second important Trend that you see everywhere is Agents multi agent setup where you have got multiple agents you can summon these agents to do certain tasks and these agents will do it for you just like men in black MIB they have a purpose and they will do certain tasks but before I jump into agents I want to actually introduce you to another important concept called function calling function calling is the precursor to llm agents in function calling what you do is you have a shortterm prompt you have a prompt you have shortterm memory uh sometimes you need external memory sometimes you don't need external memory but you are giving the ability of calling external tools and you are giving the ability of calling external tools by doing something called function calling function calling to be honest is a terrible name cuz you're not calling any function here you're you're not making the llm call anything not at all all you are doing is you're forcing the llm to give a structured response back so you can call an L I'll give you an example what is function calling so let's say that you have a weather API okay weather I think everybody goes with weather AP so I'm going to I'm going to skip let's say you have a currency converter okay currency converter what kind of things a currency converter need okay you need input currency you need output currency you need date you need amount technically these are the four things you need what is the amount that you want to convert for what is the input currency what is the output currency and what is the date for which you want to do currency conversion let's keep as a simple APA now typically when you go to an llm okay and say uh what is USD to INR today first of all LM may not understand what is today llm might know USD llm might know INR but lm's memory is frozen because it's a snapshot see a large language model is a snapshot so it memory has been frozen to let's say September 20123 or something like that okay so what it cannot do is it cannot give you the latest information and you cannot do this with I mean you can do this with rag kind of like you can every day take knowledge ingest keep it in your memory and then give it back not very efficient um expand this to stock market a daily data doesn't matter because everything changes like every minute and every second so you need something instant what you do you call an API if you are a programmer that's what you would naturally do you call an API now if you want to call an API uh what you need to call an API so let's say this is the information what I need at the end of the day I want to call it currency converter right and I'll say input output date right I need to make a call like this so I need four arguments that is a solid input could not be like oh United States dollar and some other time I'll be like USD some other time I'll be like US dollar I mean that will not work right you need a specific format for everything your let's say amount should be an integer right a this should be a date object so you need to force this llm to give you a particular response back otherwise what happens is this llm will throw you anything for example example what I want to say is what is USD and I so it'll be like oh USD andr is so so on September 2023 so you have to force guide the llm to make a particular type of output and somehow universally everybody has accepted that format is going to be Json except anthropic which absolutely loves XML so if you use anthropic you use XML if you use any other model you use Json so you're forcing an llm to give you a structured response back a j that can help you make this function call you can call this function with that Json so a guided response into a Json is what everybody calls function calling you don't necessarily call the function in function calling but you get the output that will help you call function call right clear now that is exactly what is a precursor to agent because in a function call you have the ability to call a function and agents are nothing but a bunch of function called stitched with tools so what do we have in agents we have a bunch of function calls plus tools and I would like to introduce to you a very interesting solution that can help you understand more about agents if you are too old in the AI world you would have probably recognized this immediately and this was the workflow of something called baby AGI so baby AGI was quite a popular thing back in the day I mean back in the days like less than one year before I guess or maybe more than one year a function call is what I said is the foundation of Agents but what is an agent now if you have seen our pyramid you would know our agent sits right at the top like closer to what we our aspirational figur is now what is this agent how do you define an agent so it's simple first of all a chatbot and a rag all of these guys if you see here they end a text or you know some kind of thing like input output images video all these things right that's where they end one of these modalities they're done what you achieve with agent is something that is absolutely stunning you don't stop at text response you stop at an action you trigger an action and that is what agents are simply you take llm you connect them with tool you give them a purpose or goal that is your agent and that is exactly what baby a has done back in the day like there are multiple agents now but if you see baby a which is a very wonderful framework you can see that there is a task like there is something that has to happen there are certain tools like for example Vector DB and all the other things are there and every agent has a Purp purpose like okay you have to execute you have to return you have to do something you have to do something and they have a goal so you have tools purpose SL goals and llms and this all together work for a common goal and that is your agent there are multiple agent Frameworks that are quite popular these days crew AI L graph you have got a py autogen and most of these things you will see first you have to define a role you have to refine a go Define a goal role goal and then you have to save which llm that you want to use as a backend engine and then you put together a system of one this is single agent now you put together like this is a team that is your multiagent setup with agents people are doing amazing things you can make an agent book your ticket you can make an agent let's say read something um distill something create a note publish the blog post you can summon these agents to do a lot of things and personally for me uh the most time that I spent reading about agents because you it's it's becoming quite obvious that agents are the next Frontier in uh the way we can take llms forward I mean there are a lot of different things but at least personally I'm quite interested in automation usually and I think agents are going to be the next big thing in I mean currently itself is a big thing Google has got Google's own projects like they call their own agents I don't know what they call they have a lot of different names open a has its own agents and uh every time you talk to some company you speak about agents because you want to summon these agents you want to connect these llms to like different dimension and on this Dimension that what we are connecting is the tools Dimension so you take llms you have the function calling ability and once you connect them to tools you are unlocking the potential of something immense and that is what you call as agents I'm not going deep into agents because this is probably I'm hoping it to be a series depending upon how you all like it but in this series my next focus is going to be agents so agent is quite closer to the top and that takes us to the almost the end of the video which is what is our aspirational thing what is that we are all trying to go towards which is l l m o and this is inspired by Andre karpa who created this amazing structure so what is happening here this talks about using llm at the center C of a conversation or sorry center of an operating system if you go back in the day computer was created just for simple calculation purpose right you want to add a and you want to add a and b you want to keep a for one and B for two and then you want to add them that's that's what like initially computer was started like very very very back back in the days and computation started increasing computation started becoming less expensive more compute than we have the computer that we have today and garpa is arguing can we have a similar vision for llm and where the vision is you keep llm at the center right you keep llm at the center and at the center with llm you have Ram which is the shortterm memory or the context window then you have longterm memory the disk system that can be used with rag then you have the agent structure that you have with tools and then you connect it with internet and when you connect it with other llms to have like a multiagent setup or like a peripheral setup and then you have your peripheral devices where you have got audio and video can we put together a system with all these things working towards a common goal and that will ideally become your large language model operating system this is quite a vision at this point there are certain implementations available at this point those implementations are based on current understanding they are mostly let's say llms plus fun fun calling plus agents multiagent more tools that is what the current LM is it's not like a radically has a different a total view Al together and that's why if you see even in my framework that I've created llm o is currently developing and it is everything that we have got the tools the extended tools the peripheral tools with longterm memory with shortterm memory just one input from the user where it can run itself and then it can execute certain things I think that is a future that we are heading I'm not sure when we are going to do it but if somebody says something a for me today a could be like this could be like the baby a I mean I don't I don't I don't trust AG as a concept anytime soon but um yeah leaving the conscious thing Consciousness and all the other things out I would say llm o is at the top where we can expect something closer to a happen and all these things lead us up to there so I wanted to keep this video brief but this video is already going to be like more than half an hour I wanted this to be like a crash course where you understand if you don't know anything about llm o uh maybe you have not taken any course so this is going to help you to see how the future of llm O is coming and what led us up to there and uh let me know in the comment section if you like this kind of content I'll put together more this took me a lot of time to create the framework design put it um in a particular thought process to you know make it make it understandable and this is basically what a lot of m c is offer so I'm I'm definitely looking forward to hear more feedback and if you like this kind of format subscribe to the channel see you in another video Happy prompting
