With timestamps:

00:00 - Hey, this is Andrew Brown, your cloud instructor 
exam Pro, and I'm bringing you another complete  
00:04 - study course. And this time, it's the Azure 
AI fundamentals made available here on Free  
00:09 - Code Camp. So this course is designed to help 
you pass the exam and achieve Microsoft issued  
00:13 - certification. And we're going to do that 
by providing you great lecture content,  
00:18 - follow along to get that hands on experience, 
and cheat sheets for the day of your exam.  
00:22 - So when you do get that certification, 
you can put it on your resume or LinkedIn,  
00:26 - and show that you have that ASHRAE knowledge 
to get that cloud job or get that promotion.  
00:30 - So I want to introduce myself, I'm previously the 
CTO of multiple edtech companies with 15 years,  
00:35 - industry experienced five years specializing 
cloud service community hero, and I've published  
00:40 - many free cloud courses. I love Star Trek and 
coconut water. So I just want to take a moment  
00:46 - here to thank viewers like you, because it's 
you that make these free courses possible.  
00:50 - And if you want to support more 
free courses, just like this one,  
00:54 - a great way to do that is buying our additional 
study materials on exam pro.co. And for this exam,  
01:01 - it's Ford slash ai 900. This will get you 
access to study notes, flashcards, quizlets,  
01:06 - downloadable cheat sheets and practice exams. And 
you'll also be able to ask questions and get some  
01:11 - learning support. So if you want to keep up to 
date with any of the more courses I am releasing,  
01:17 - you can follow me on twitter at Andrew, Andrew 
route 10 share with me when you pass the exam,  
01:22 - or what you might like to see as the next 
course. So there we go. Let's get to it.
01:33 - Hey, this is Andrew Brown from exam Pro. And we 
are at the start of our journey here learning  
01:37 - about the AI 900 asking the most important 
question which is what is the a 900. So the  
01:42 - Azure AI fundamentals certification is for 
those seeking in ML roles such as AI engineer  
01:47 - or data scientists. And the certification will 
demonstrate a person can define and understand  
01:51 - Azure cognitive services, ai concepts, knowledge 
mining, responsible AI basis of ml pipelines,  
01:57 - classical ml models, auto ml and Azure ML 
studio. So you don't need to know super  
02:03 - complicated ml knowledge here, but definitely 
helps to get you through there. But yeah,  
02:09 - so this certification is generally referred to by 
its course code, the AIA 900 into the natural path  
02:14 - for the Azure AI engineer or Azure Data Scientist 
certification. And this generally is an easy  
02:19 - course to pass. It's great for those new to cloud 
or ml related technology, looking at our roadmap,  
02:24 - you might be asking, okay, well, what are the 
paths? And what should I learn. And so here are  
02:29 - my markers. And let's get out the annotation tool 
or laser pointer to see where we can go. Now if  
02:35 - you already have your az 900. That's a great 
starting point before you take your ai 900.  
02:41 - If you don't have your az 900, you can jump right 
into the 900. But I strongly recommend you go get  
02:46 - that az 900 because it gives you General, General, 
foundational knowledge, it's just another thing  
02:53 - that you should not have to worry about, which is 
just how to use Azure at a fundamental level. Do  
02:58 - you need the DP 900 to take the 900 No, but a lot 
of people seem to like to to go this route where  
03:02 - they want to have that data foundation before they 
move on to a to the AI 100 because they know that  
03:08 - that is just broad knowledge is going to be useful 
there. So you know, it is apparent that you see  
03:13 - a lot people getting the AI 900 and the DPI 900. 
Together, vana 100, the path is a little bit more  
03:18 - clear, it's either going to be data scientists or 
AI engineer. So AI engineer is just the cognitive  
03:24 - services turned up to 11, you have to know how to 
use the AI services in and out for data scientists  
03:30 - is more focused on setting up actual pipelines, 
and things like that within the Azure Machine  
03:35 - Learning Studio. So you just have to decide which 
path is for you. The data scientist is definitely  
03:39 - harder than the AI engineer, I think the coaches 
was updated. So I just updated that to 102.  
03:45 - And I think the AI engineers to be two 
separate, you had to take two separate
03:50 - courses. But now it's just a 
single one. So it's unified.  
03:54 - But you know, if you aren't ready for the data 
sciences, some people like taking the AI engineer  
03:57 - first and then doing the data scientist. So this 
is kind of like a warm up. Again, it's not 100%  
04:02 - necessary, but it's just based on your personal 
learning style. And a lot of times people like to  
04:08 - take the data engineer after the data scientists 
just to round out their complete knowledge. Now,  
04:12 - if you already have the az 900 and the associate, 
you can safely go to the data scientist if you  
04:18 - want to risk it, because this one is really 
hard. So if you've passed the easy one before,  
04:24 - you know, you're gonna probably have a lot more 
confidence, learning about this stuff, all this  
04:27 - fun foundational stuff at this level here. But of 
course, it's always recommended to go grab these  
04:33 - foundational certs because sometimes course 
materials just do not cover that information.  
04:36 - And so the obvious stuff is going to get left out. 
Okay. So moving forward here. So how long should  
04:43 - you study to pass for the 900? Well, if you have 
one year's experience with Azure, you're looking  
04:47 - at five hours as little as five hours could be 
up to 10 hours. If you have passed the az 900  
04:52 - to dp 900. Around 10 hours is the average. If 
you're completely new to ml AI, you're looking  
04:57 - at 15 hours, this could get extended to 20 to 30 
Again, it just depends on how green you are like  
05:02 - how new you are to these concepts. But you know, 
I think on average, we're looking at 15 hours,  
05:07 - the recommended study time is 30 minutes a 
day for 14 days should get you through it.  
05:12 - You know, but, you know, just don't over study 
and just don't spend too little time, you know.  
05:17 - So where do you take this example, you can take 
it in person at a test center or online from  
05:21 - the convenience of your own home. So there's two 
popular test centers, there's psi and Pearson VUE,  
05:27 - well, and I should say these are theirs. These are 
not necessarily test centers, per se, but they are  
05:33 - a collection of test centers that are partnered 
with psi Pearson VUE so that you can easily take  
05:37 - it at a local test center. If you ever heard 
the term Proctor that is that means a supervisor  
05:43 - person that is monitoring you while you're taking 
the exam to them. When we talk about online exams,  
05:48 - they'll say proctored exams to refer to the online 
component, if I had the option to meet in person  
05:52 - online, always the in person because it's a 
controlled environment, it's way less stress us  
05:58 - stress stressful. And, you know, online, there can 
so many things can go wrong. So you know, but it's  
06:04 - up to your personal preference and your situation. 
Okay? What does it take to pass the exam? Well,  
06:09 - you got to watch the lectures, and memorize key 
information, do hands on labs and follow along  
06:15 - with your own Azure account, I would say that you 
could probably get away with just watching all the  
06:19 - videos in this one without having to do but again, 
you know, it really does reinforce information.  
06:24 - If you do take the time there. There is some 
stuff that is an Azure Machine Learning Studio,  
06:28 - you might be wary of launching because we do have 
to run instances and they will cost money. So if  
06:34 - you if you feel that you're not comfortable with 
that, just watching you should be okay. But when  
06:38 - you get into the associate tier you absolutely you 
just have to expect to pay something to to learn  
06:44 - and take that risk, okay? You want to do paid 
online practice exams that simulate the real exam.  
06:50 - So I do have paid practice exams that accompany 
this course that are on my platform exam Pro.  
06:57 - And that's how you can help support more of 
these free courses. Can you pass this without  
07:01 - taking practice exam Asher's a little bit harder? 
If this is an AWS exam? I'd say yes. for Azure.  
07:08 - It's kind of risky, the easy 100 Sure. Ai 900 dP 
900 sC 900? No, I think you should get a practice  
07:14 - exam, at least one, or go through the sample one, 
there's a sample one probably looking around for  
07:20 - on the Azure website. Let's just look at the exam 
guide break down here very shortly. And then in  
07:26 - the following video, we'll look at in more detail. 
So it's broken down into the following domains to  
07:30 - describe AI workloads and considerations describe 
fundamental principles of machine learning  
07:34 - on Azure describe features of computer vision 
workloads on Azure, describe features of natural  
07:38 - language processing workloads on Azure, describe 
features of conversational AI workloads on Azure.  
07:43 - And I want you to notice it says describe, 
describe, describe, describe, describe, that's  
07:48 - good, because that tells you it's not going to 
be super, super hard, right? If you start seeing  
07:53 - things that say, beyond describing identify, 
then you know, it's going to be a bit harder,  
07:58 - okay? The passing grade here is 700 out of 
1000. So that's around 70 70%, I would say  
08:04 - around because you could possibly fail with 70%. 
Because these things work on scaled scoring. For  
08:10 - response types, there's about 40 to 60 questions, 
and you can afford to get 12 to 18 questions  
08:15 - wrong. I put an asterisk there because there's not 
always just one question per like, per section,  
08:22 - but I'll talk about the here in a second. So some 
questions are worth more than one point. There's  
08:26 - no penalty for wrong questions. Some questions 
cannot be skipped. And the format of questions  
08:30 - can be multiple choice, multiple answer, drag and 
drop hot area case studies. Case Studies. I don't  
08:36 - remember I don't think I saw a case study on mind. 
But case studies will have a series of questions,  
08:42 - a series of questions that make up or come back 
to a particular business problem. And so those are  
08:48 - very interesting. That's why we have that asterik 
up here, okay. So for the duration, you get one  
08:53 - hour, that means about one minute per question. 
The time for this exam is 60 minutes, your seat  
08:58 - time is 90 minutes, seat time refers to the 
amount of time that you should take to allocate  
09:03 - for that exam. So this includes time to review the 
instructions, read, accept the NDA, complete the  
09:08 - exam and provide feedback at the exam. This is 
going to be valid for 24 months up to two years  
09:13 - before we have certification. And, you know, 
we'll proceed to the full exam guide now. Okay.
09:23 - Hey, this is Andrew Brown from exam Pro. And what 
we've pulled up here is the official exam outline  
09:28 - on the Microsoft website. If you want to find 
this yourself, just got to type in ai 100, Azure  
09:34 - or Microsoft, you should be able to easily find 
it, the page looks like this. And what I want  
09:38 - you to do is scroll on down because we're looking 
for skills measured and from there, we're going  
09:42 - to download the skills outline. And once we have 
that open, you might want to bump up the text. And  
09:47 - so what you'll always see in these documents is a 
red text at the top saying hey, we've updated the  
09:52 - track as your loves updating their courses with 
minute updates that don't generally affect the  
09:58 - outcome of the study. But it does get a lot 
people worrying. So we say, well, is your core  
10:03 - set of data? So no, no, they're just making minor 
changes. Because they'll do this like five times  
10:07 - a year. And so if there was a major revision, 
what would happen is they would change it. So  
10:12 - instead of being the AI 900, to be like, the 
AI 901, or 9902, we saw that recently with the,  
10:20 - the AI 100 words now the ai, ai 102 or 103. 
Sorry. So you know, just watch out for those.  
10:27 - And if it's a major revision, then yes, the 
course you would need a completely new course,  
10:31 - and it would not match. But for minors, it's 
going to be my new thing. So if we scroll on down,  
10:36 - and a lot of times, they'll just cross out 
what they've changed in this one in particular,  
10:40 - they did not show us in detail, you'd 
have to read through the comparison.  
10:43 - But we'll look at the new listing here. and work 
our way through here is to describe artificial or  
10:48 - AI workloads and considerations. So here we 
just kind of describing the generalities of  
10:53 - AI. So prediction forecasting, this is because 
when we use auto ml prediction would be  
10:59 - classification and regression and forecasting 
would be that real time series forecasting,  
11:03 - I suppose, identity features anomaly detection. So 
not a lot in the exam for this. So we we touched  
11:09 - on it briefly, computer vision workloads, there's 
a lot of stuff under computer vision, as you'll  
11:14 - find out through the course, and LP and knowledge, 
mining workloads, conversational AI, workloads.  
11:20 - And again, these are all the concepts, not how to 
use the services, then you have the responsible AI  
11:24 - section. And so Microsoft has these six principles 
that they really want you to know. And they push  
11:30 - it throughout all their AI services. So those are 
the 16 liter. Now, they're not that hard to learn,  
11:36 - then describe fundamental principles of machine 
learning on Azure. So here, it's just describing  
11:40 - regression, classification and clustering. We 
have a lot of practical experience with these  
11:46 - in the course. So you will understand 
at the end what these are used for.  
11:50 - For core machine learning concepts, we can 
identify features and labels in a data set.  
11:54 - So that's their data labeling service, describe 
how training validation data sets are used in  
11:58 - machine learning. So we touch on that describe how 
machine learning algorithms are used for training,  
12:03 - select and interpret model valuations of 
metrics for classification regression,  
12:07 - a lot of these deal categories in auto ml because 
it automatically does it, but we can see how it  
12:13 - does that. Okay. Well, I think having to 
do it ourselves, identify core tasks and  
12:17 - creating a machine learning solution. So describe 
common features of data ingestion, preparation,  
12:22 - feature engineering, selection, 
features of model training, evaluation,  
12:26 - features of model deployment and management. 
And then we have described no code solution. So  
12:33 - auto ml, they like to call it automated. ml, 
but really, the industry just calls it auto ml.  
12:40 - Then there's the designer for building pipelines. 
Here's where we see some changes. So identify  
12:46 - features of image classification, features 
of object detection solution. So semantic  
12:51 - segmentation is gone, which is great, because I 
don't even know what that is. So it's great that  
12:56 - it's out there, OCR solutions, and then you have 
face detection. Then under computer vision tasks,  
13:02 - we have computer vision, custom vision, face 
services form recognizer tones. There's a  
13:06 - lot around computer vision. For NLP we have key 
phrase extraction, identity recognition, sentiment  
13:11 - analysis, language modeling, speech recognition, 
synthesis, this one doesn't really appear much.  
13:17 - It's kind of a concept not so much something we 
have to do. Then there's translation. We have nlp,  
13:24 - nlp stuff. So text analytics, Luis, or Louis, 
I'm not sure which way to pronounce it. Speech  
13:30 - service and text, translator text. Then down 
below below, we have a conversational AI. So  
13:36 - building out web chat bots, and characters of 
conversation AI solutions looks like these two  
13:43 - have telephone and personal digital assistants 
not sure why they decided to remove that. But  
13:47 - that's okay. I think that's fine. q&a maker and 
Azure bot, I really like this service, by the way.  
13:53 - So yeah, there we go. That is the outline. 
And now we'll jump into the actual course.
14:02 - Hey, this is Andrew Brown from exam Pro. And we 
are looking at the layers of machine learning. So  
14:06 - here I have this thing that looks 
like kind of an onion. And what it is,  
14:09 - it's just describing the relationship 
between these ml terms related to AI,  
14:15 - and we'll just work our way through here starting 
at the top. So artificial intelligence, also known  
14:19 - as AI is when machines that perform jobs that 
mimic human behavior. So it doesn't describe  
14:25 - how it does that. But it's just the fact 
that that's what AI is one layer underneath  
14:30 - we have machine learning. So machines that get 
better at a task without explicit programming.  
14:34 - Then we have deep learning. So these are machines 
that have an artificial neural network inspired by  
14:38 - the human brain to solve complex problems. And 
if you're talking about some of that actually  
14:43 - assembles either ml or deep learning models or 
algorithms that's a data scientist or person with  
14:49 - multidisciplinary skills and math statistics, 
predictive modeling machine learning to make  
14:53 - future predictions. So what you need to understand 
is that AI is just the outcome, right? And so AI  
14:58 - could be using m Underneath, or deep learning, or 
a combination of both or just FL statements, okay?
15:10 - Alright, so let's take a look here at the 
key elements of AI. So AI is the software  
15:14 - that imitates human behaviors and capabilities. 
And there are key elements according to Azure  
15:18 - or Microsoft as to what makes up AI. So let's 
go through this list quickly here. So we have  
15:24 - machine learning, which is the foundation of an 
AI system that can learn predict like a human,  
15:28 - you have anomaly detection. So detect outliers or 
things out of place, like a human computer vision,  
15:34 - be able to see like a human natural language 
processing, also known as NLP, be able to  
15:40 - process human languages and refer a context, you 
know, like a human, conversational AI be able to  
15:45 - hold a conversation with a human. So, you know, 
I wrote here, according to Microsoft and Azure,  
15:52 - because you know, the global definition is a bit 
different. But I just wanted to put this here,  
15:58 - because I've definitely seen this as an exam 
question. And so we're going to have to go with  
16:02 - Asher's definition here. Okay. Let's define what 
is a data set. So a data set is a logical grouping  
16:12 - of units of data that are closely related to 
or share the same data structure. And there  
16:17 - are publicly available datasets that are used in 
learning of statistics, data analytics and machine  
16:23 - learning. I just want to cover a couple here. So 
the first is the M NIST database. So images of  
16:28 - handwritten digits use to test classify cluster 
image processing algorithms commonly used when  
16:33 - learning how to build computer vision ml models 
to translate handwritten into or handwriting into  
16:39 - digital text. So it's just a bunch of handwritten 
numbers and letters. And then another very popular  
16:46 - data set is the common objects in context cocoa 
dataset. So this is a dataset which contains  
16:51 - many common images using a JSON file, cocoa format 
that identify objects or segments within an image.  
16:57 - And so this data set has a lot of stuff in its 
object segmentations recognition and IP context,  
17:02 - super pixel stuff, segmentation, they have a lot 
of images, and a lot of objects. So there's a lot  
17:08 - of stuff in there. So why am I talking about 
this, and in particular cocoa data sets? Well,  
17:13 - when you use Azure Machine Learning Studio, 
it has a daily data labeling service. And  
17:20 - the thing is, is that it can actually export out 
into cocoa format. So that's why I want you to  
17:24 - get exposure to what cocoa was. And the other 
thing is, is that when you're building out Azure  
17:28 - Machine Learning pipelines, you they actually have 
open datasets, as you'll see later in the course,  
17:35 - that shows you that you can just 
use very common ones. And so  
17:39 - you might see m NIST and the other one there. 
So I just wanted to get you some exposure. Okay.  
17:48 - Let's talk about data labeling. So this 
is the process of identifying raw data,  
17:52 - so images, text files, videos, and adding one 
or more meaningful and informative labels to  
17:58 - provide context so a machine learning model 
can learn. So with supervised machine learning,  
18:02 - labeling is a prerequisite to produce training 
data. And each piece of data will generally be  
18:06 - labeled by a human. The reason why I say generally 
here is because with Azure Data labeling service,  
18:13 - they can actually do ml assisted labeling. So 
with unsupervised machine learning labels will  
18:18 - be produced by the machine and may not be human 
readable. And then one other thing I want to touch  
18:24 - on is the term called ground truth. So this is 
a proper, a properly labeled data set that you  
18:29 - can use as the objective standard to train and 
assess a given model is often called ground truth,  
18:34 - the accuracy of your train model will depend on 
the accuracy of your ground truth. Now using Azure  
18:40 - as tools I've ever seen and used that word ground 
truth, I see that a lot in AWS, and even this  
18:44 - graphic here is from AWS. But I just want to make 
sure you are familiar with all that stuff. Okay.
18:54 - Let's compare supervised unsupervised and 
reinforcement learning. Starting at the top,  
18:58 - we got supervised learning, this is where the data 
has been labeled for training. And it's considered  
19:03 - task driven, because you're trying to make a 
prediction get a value back. So when the labels  
19:07 - are known, and you want a precise outcome, when 
you need a specific value returned, and so you're  
19:12 - going to be using classification and regression 
in these cases. For unsupervised learning,  
19:16 - this is where data that has not been labeled, 
the ML model needs to do its own labeling.  
19:21 - This is considered data driven. It's trying to 
recognize a structure or a pattern. And so this  
19:25 - is when the labels are not known. And the outcome 
does not need to be precise when you're trying  
19:30 - to make sense of data. So you have clustering, 
dimensionality reduction and Association. Have  
19:35 - you ever heard this term before? The idea is it's 
trying to reduce the amount of dimensions to make  
19:39 - it easier to work with the data. So make sense of 
the data, right? We have reinforcement learning.  
19:45 - So this is where there is no data. There's 
an environment and an ml model generates data  
19:50 - and makes many attempts to reach a goal. So this 
is considered decisions driven. And so this is for  
19:55 - game AI learning tasks robot navigation, when 
you've seen someone code In a video game that  
20:00 - can play itself, that's what this is. If you're 
wondering, this is not all the types of machine  
20:04 - learning. And these specific, unsupervised 
and supervised is considered classical machine  
20:10 - learning because they have heavily rely on 
statistics and math to produce the outcome.  
20:15 - But there you go. So what is a neural network? 
Well, it's often described as mimicking the brain,  
20:24 - it's a neuron or node that represents an 
algorithm. So data is inputted into a neuron and  
20:28 - based on the output, the data will be passed to 
one of many connected neurons, that it connections  
20:33 - between neurons is weighted, I really should 
have highlighted that one that's very important.  
20:37 - The network is organized into layers, there will 
be an input layer, one too many hidden layers and  
20:42 - an open layer. So here's an example of a very 
simple neural network. Notice the nn, a lot of  
20:48 - times you'll see this in ML as an abbreviation for 
neural networks. And sometimes neural networks are  
20:53 - just called neural nets. So just understand that's 
the same term here. What is deep learning? This is  
20:57 - a neural network that has three or more hidden 
layers, it's considered deep learning, because  
21:01 - at this point, it's it's not human readable to 
understand what's going on within those layers.  
21:07 - What is Ford fried, so neural networks, where 
they have connections between nodes that do not  
21:12 - form a cycle, they always move forward. So that's 
just describes a forward pass through the network,  
21:18 - you'll see fn n, which stands for forward feed 
neural network just described that type of  
21:22 - network, then there's about back propagation, 
which are in forward feed networks. This is  
21:27 - where we move backwards through the neural net, 
adjusting the weights to improve the outcome on  
21:31 - next iteration. This is how a neural net learns. 
The way the backpropagation knows to do this  
21:35 - is that there's a loss function. So a function 
that compared the ground true to the prediction  
21:40 - to determine the error rate how bad the network 
performs. So when it gets to the end, it's going  
21:44 - to perform that calculation, and then it's going 
to do its back propagation, adjust the weights,  
21:50 - then you have activation functions, I'm just going 
to clear this up here. So activation functions.  
21:57 - They're an algorithm applied to a hidden 
layer node that affects connected output. So  
22:02 - for this entire hidden layer, they'll all have 
the same one here and just kind of affects  
22:07 - how it learns and like how the weighting works, 
so it's part of backpropagation. And just the  
22:12 - learning process, there's a concept of dense so 
when the next layer increases the amount of nodes  
22:16 - and you have a sparse so when the next layer 
decreases the amount of notes. Anytime you  
22:20 - see something going from a dense layer to a 
sparse later, that's usually called dimensional  
22:24 - dimensionality reduction because you're reducing 
the amount of dimensions because the amount of  
22:28 - nodes in your network determines 
the dimensions you have. Okay.  
22:36 - What is a GPU? Well, it's a general processing 
unit that is specially designed to quickly render  
22:41 - high resolution images and videos concurrently. 
GPUs can perform parallel operations on multiple  
22:46 - sets of data. So they are commonly used for non 
graphical tasks such as machine learning, and  
22:51 - scientific computation. So a CPU has an average 
of four to 16. processor cores, a GPU can have  
22:56 - 1000s of processor cores, so something that has 48 
GPUs can have as many as 40,000 cores. Here's an  
23:02 - image I grabbed right off the Nvidia website. 
And so it really illustrates very well, like  
23:08 - how this would be really good for machine learning 
or neural networks. Because neural networks have  
23:14 - a bunch of nodes. They're very repetitive tasks, 
you can spread them across a lot of cores, that's  
23:18 - gonna work out really great. So GPUs are suited 
for repetitive and highly parallel computing tasks  
23:23 - such as rendering, graphics, cryptocurrency 
mining, deep learning and machine learning.
23:32 - We're talking about CUDA before we can let's talk 
about what Nvidia is. So Nvidia is a company that  
23:37 - manufactures graphical processing units for gaming 
and professional markets. If you play video games,  
23:41 - you've heard of Nvidia. So what is CUDA? It is 
the compute unified device architecture. It is  
23:47 - a parallel computing platform and API by Nvidia 
that allows developers to use CUDA enabled GPUs  
23:52 - for general purpose computing on GPUs. So GP GPUs, 
all major deep learning frameworks are integrated  
23:59 - with Nvidia deep learning SDK. The Nvidia deep 
learning SDK is a collection of Nvidia libraries  
24:05 - for deep learning. One of those libraries is 
the CUDA deep neural network library. So cu dnn  
24:11 - so CUDA, RC UD and n provide provides highly 
tuned implementations for standard routine such as  
24:16 - forward and back convolution 
convolutions really great for  
24:21 - computer vision, pooling normalization activation 
layers. So, you know, in the Azure certification  
24:29 - for the AI 900. They're not going 
to be talking about CUDA. But if  
24:32 - you understand these two things, you'll 
understand why GPUs really matter. Okay.
24:41 - All right, let's get a easy introduction 
into machine learning pipeline. So this  
24:46 - one is definitely not an exhaustive one, and 
we're definitely gonna see more complex ones  
24:50 - throughout this course. But let's get to 
it here. So starting on the left hand side,  
24:54 - we might start with data labeling. This is very 
important when you're doing supervised learning  
24:59 - because you need to to label your data set, the 
ML model can learn by example, during training,  
25:04 - this stage and the feature engineers nearing 
stage or is considered pre processing because  
25:09 - we are preparing our data to be trained for the 
model. When we move on to feature engineering,  
25:15 - the idea here is that ml models can only work 
with numerical data. So we need to translate it  
25:19 - into a format that it can understand. So extract 
out the important data that the ML model needs to  
25:24 - focus on. Okay, then there's the training step. So 
your model needs to learn how to become smarter,  
25:31 - it will perform multiple iterations 
getting smarter with each iteration,  
25:35 - you might also have a hyper parameter tuning 
step here, it says tuning but should say tuning.  
25:42 - But the ML model can have different parameters. 
So you can use ml to try out many different  
25:46 - parameters to optimize the outcome. When you 
get to deep learning, it's impossible to tweak  
25:51 - the parameters by hand. So you have to use hyper 
parameter tuning, then you have serving, sometimes  
25:57 - known as deploying. But you know, when we say 
deploy, we talked about the entire pipeline,  
26:01 - not necessarily just the ML model step. So 
we need to make an ml model accessible. So we  
26:06 - serve it by hosting in a virtual machine or 
container. When we're talking about Azure  
26:12 - machine learning, it's either going to be an Azure 
Kubernetes service or Azure Container instance.  
26:16 - And you have inference. So inference is the act 
of request, requesting to make a prediction,  
26:23 - so you send your payload with either CSV 
or whatever, and you get back the results,  
26:29 - you have a real time endpoint and batch 
processing. So real time, it's just  
26:32 - there they can batch can be real time as well. 
But generally, it's slower. But the idea is that  
26:36 - do I? Am I making a single item prediction? Or am 
I giving you a bunch of data at once. And again,  
26:42 - this is a very simplified ml pipeline, I'm sure 
we'll revisit ml pipeline later in this course.  
26:51 - So let's compare the the terms forecasting and 
prediction. So forecasting, you make a prediction  
26:57 - with relevant data. It's great for analysis 
of trends, and it's not guessing. And when  
27:02 - you're talking about prediction, this is where you 
make a prediction without relevant data, you use  
27:06 - statistics to predict future outcomes, it's 
more of guessing. And he uses decision theory.  
27:10 - So imagine you have a bunch of data. And the 
idea is you're going to infer from that data,  
27:15 - okay, maybe it's a, maybe it's B, 
maybe it's C. And for prediction,  
27:19 - you don't have really much data, so you're 
going to have to kind of invent it. And the  
27:24 - idea is that you'll figure out what the outcome 
is there. These are extremely broad terms,  
27:27 - but you know, just so you have a high 
level view of these two things, okay.  
27:36 - So what are performance or evaluation metrics? 
Well, they are used to evaluate different machine  
27:40 - learning algorithms. So the idea is, you know, 
when your machine learning makes a prediction,  
27:44 - these are the metrics you're using to evaluate 
to determine, you know, is your ml model working  
27:49 - as you intended. So for different types of 
problems, different metrics matter, this  
27:53 - is absolutely not an exhaustive list. I just want 
you to get you exposure to these words and things  
27:59 - so that when you see them you go, Okay, 
I'll come back here and refer to this.  
28:03 - But lots of these are just it's not you it's 
not necessarily remember but classification  
28:06 - metrics you should know. So classification, 
we have accuracy, precision recall f1 score,  
28:11 - rockin AUC. For regression metrics. We 
have MSC, our MSC ma ranking metrics,  
28:17 - we have MLR, DCG and DCG. Statistical metrics, 
we have correlation, computer vision metrics,  
28:23 - we have psnr, SSI m, IOU and LP metrics, we 
have perplexity blue Meteor rogue, deep learning  
28:29 - related metrics. We have Inception score, I cannot 
say this person's name, but or I'm assuming it's  
28:35 - a person but this Inception distance. And 
there are two categories evaluation metrics,  
28:40 - we have internal evaluation. So metrics used to 
evaluate the internals of an ml model. So accuracy  
28:45 - f1 score precision, recall, I call them the 
famous for using all kinds of models and external  
28:53 - evaluation metrics used to evaluate the final 
prediction of an ml model. So yeah, don't get too  
29:00 - worked up here. I know that's a lot of stuff. 
The ones that matter, we will see again, okay.
29:09 - Let's take a look at Jupiter notebook. So these 
are web based applications for authoring documents  
29:14 - to combine live code narrative text equations, 
visualizations. So if you're doing data science,  
29:19 - or you're building ml models, you apps 
that are going to be working with Jupyter  
29:22 - notebooks. They're always integrated 
into cloud service providers ml tools.  
29:28 - So Jupyter Notebook actually came about from 
ipython. So ipython is the precursor of it. And  
29:32 - they extracted that feature out it became Jupyter 
Notebook I bought ipython is now a kernel to run  
29:38 - Python. So when you execute Python code here, 
it's using ipython, which is a version of Python  
29:45 - Jupyter Notebooks were overhauled and better 
integrated into an ID called Jupiter labs,  
29:49 - which we'll talk about here in a moment. And 
you generally want to open notebooks in labs,  
29:53 - the legacy web based interfaces known as 
Jupiter classic notebooks. This is what the  
29:57 - old one looks like you still open them up, 
but everyone uses Jupiter labs now. Okay,  
30:01 - so let's talk about Jupiter labs. Jupiter Labs is 
the next generation web based user interface, all  
30:06 - familiar features of the classic 
Jupyter Notebook is in a flexible,  
30:09 - powerful user interface. It has notebooks, 
a terminal, a text editor, a file browser,  
30:14 - rich outputs, Jupiter labs will eventually replace 
the classic Jupyter notebooks. So there you go.  
30:24 - We keep mentioning regression, but let's talk 
about it in more detail here. So we kind of  
30:28 - understand the concept. So regression is the 
process of finding a function. to correlate  
30:32 - a labeled data set gnosis is labeled, that 
means it's going to be for supervised learning  
30:37 - into a continuous variable number. So another 
way to say it is predict this variable in the  
30:41 - future. So the future is just means like that 
continuous variable doesn't have to be time. But  
30:46 - that's just a good example of regression. So what 
will the temperature be next week? So we will be  
30:53 - 20? Celsius? How would we determine that? Well, we 
would have vectors, so dots, they're plotted on a  
30:58 - graph that has multiple dimensions, the dimensions 
could be greater than just x and y, you could have  
31:03 - many. And then you have a regression line. This 
is the line that's going through our data set.  
31:08 - And and that's going to help us figure out how to 
predict the value. So how would we do that? Well,  
31:15 - we we need to calculate the distance of a 
vector from the regression line, which is  
31:18 - called an error. And so different regression 
algorithms use the error to predict different  
31:23 - variable a future variable. So just to look at 
this graphic here, so here's our regression line.  
31:28 - And here is a dot like a vector or a piece 
of information. And this distance from the  
31:33 - line that the actual distance is what we're 
going to use in our ML model to figure out  
31:38 - if we were to plot another line up here, 
right? You know, we compare this line to  
31:43 - all the other lines, okay? And that's how we find 
similarity. And what will commonly see for this  
31:48 - is mean squared error, root mean squared error, 
mean? absolute error. So MSE, mrsc, and Ma. Okay.
32:01 - Let's take a closer look at the concepts 
of classification. So classification is the  
32:05 - process of finding a function to divide a labeled 
data set. So again, this is supervised learning  
32:11 - into classes or categories, so predict a category 
to apply to the inputted data. So will it rain  
32:17 - next Saturday, will it be sunny or rainy? So we 
have our data set. And the idea is we're drawing  
32:22 - through this a classification line to divide the 
data set. So regression we're measuring the line  
32:27 - to or the vectors to the line. And this line is 
just what side of the line is that on? If it's on  
32:32 - this side, then it's sunny. If it's on this side, 
it's rainy. Okay. For classification algorithms,  
32:37 - we got large logistic regression decision 
trees, random forests, neural networks,  
32:43 - Naive Bayes, k nearest neighbor, also known as k 
and n, and support vector machines. svms. Okay.  
32:55 - Let's take a closer look at clustering. 
So clustering is the process of grouping  
32:58 - unlabeled data. So unlabeled data means it's 
unsupervised learning based on similarities  
33:04 - and differences. So the outcome could be grouped 
data based on similarities or differences. I guess  
33:08 - it's the same description up here. But imagine 
we have a graph and we have data. And the idea  
33:13 - is we draw boundaries around that to see similar 
groups. So maybe we're recommending purchases to  
33:19 - Windows computers, or recommending purchase to Mac 
computers. Now remember, this is unlabeled data,  
33:24 - so the label is being inferred, or, or 
they're just saying these things are similar,  
33:28 - right? So clustering algorithms, we got k means 
K, mi dois, identity base hierarchial. Okay.  
33:40 - Hey, this is Andrew Brown from exam Pro. And 
we're looking at the confusion matrix. And this  
33:44 - is a table to visualize the mall predictions, 
the predicted versus the ground truth labels,  
33:47 - the actual also known as that error matrix, 
and they're useful for classification problems  
33:52 - to determine if our, if our classification is 
working as we think it is. So imagine we have  
33:58 - a question how many bananas did this person eat 
or these people eat? And so we have this kind of  
34:05 - box here where we have predicted versus actual, 
and it's really comparing the ground truth,  
34:10 - and what the model predicted, right? And so on the 
exam, they'll ask you questions like, Okay, well  
34:16 - imagine that. And they might not even say yes or 
no, maybe like zero and one. And so what they're  
34:22 - saying is, you know, imagine you have, you want to 
tell us the true positives, right? And so the idea  
34:28 - is, they won't show you the labels here, but you 
know, one in one would be a true positive and zero  
34:33 - and zero would be a false negative. Okay? Another 
thing they'll ask you about these confusion matrix  
34:40 - is, is the size of them. So the idea is that we're 
looking right now at a oops, just gonna erase that  
34:48 - there. But we're looking at a binary classifier 
because we have one label and just two labels,  
34:54 - right, one and two, okay, but we you could have 
three say one, two, and three. So how would you  
34:58 - calculate that well, Be a third cell over here you 
know and so it's gonna be an excellent predictive  
35:04 - because we're only gonna have ground truth 
versus prediction. And so that's how you'll  
35:08 - know it will be six the size will be six might 
not say cells, but we'll just say six. Okay.
35:17 - So to understand anomaly detection, let's define 
quickly what is an anomaly so an abnormal thing  
35:23 - that is marked by deviation from the norm or 
standard. So, anomaly detection is the process of  
35:29 - finding outliers within a data set color anomaly 
so detecting when a piece of data or access  
35:33 - patterns appear suspicious or malicious. So use 
cases for anomaly detection can be data cleaning,  
35:40 - intrusion detection, fraud detection, system 
health monitoring, event detection and sensory  
35:45 - or sensor networks, ecosystem disturbances, 
detection of critical and cascading flaws,  
35:51 - anomaly detection is by hand is a very tedious 
process of using ml for a knowledge section is  
35:56 - more efficient and accurate. And Azure 
has a service called anomaly detector  
36:00 - detects anomalies in data to quickly find, 
quickly identify and troubleshoot issues.  
36:10 - So computer vision is when we use machine 
learning neural networks to gain high level  
36:14 - understanding of digital images or videos. So for 
computer vision deep learning algorithms, we have  
36:20 - convolution neural networks. These are for image 
and video recognition. They're inspired after how  
36:25 - the human eye actually processes information, and 
sends it back to the brain to be processed. You  
36:29 - have recurrent neural networks RNNs, which are 
generally used for handwriting recognition or  
36:34 - speech recognition. Of course, these algorithms 
have other applications, but these are the most  
36:38 - common use cases for them. four types of 
computer vision, we have image classification,  
36:43 - so look at an image or video and classify its 
place in a category object detection. So identify  
36:49 - objects within an image or video and apply labels 
and location boundaries. semantic segmentation,  
36:54 - so identify segments or objects by drawing 
pixel masks around them so great for objects  
36:58 - and movement, image analysis, so analyze an 
image or video to apply descriptive context  
37:05 - labels. So maybe an employee is sitting at a desk 
in Tokyo would be something that image analysis  
37:10 - would do optical character recognition, or OCR, 
find text in images or videos and extract them  
37:17 - into digital text for editing facial detection, 
so detect faces in a photo or video, and dry  
37:23 - location boundary and label their expression. So 
for computer vision to some things around Azure  
37:29 - Microsoft services, there's one called seeing 
AI. It's an app developed by Microsoft for iOS.  
37:35 - So you use your device camera to identify 
objects, people and objects, and the app is  
37:39 - audibly describes those objects for people with 
visual impairments. It's totally free. If you  
37:43 - have an iOS app, I have an Android phone so I 
cannot use it. But I hear it's great. Some of  
37:48 - the Azure computer vision service offerings is 
computer vision. So analyze images and videos,  
37:52 - extract descriptions, tags, objects and texts, 
custom vision so custom image classification  
37:58 - object detection models using your own images 
face so detect and identify people and emotions  
38:04 - and images form recognizer, so translate scan 
documents into key value or tabular editable data.  
38:15 - So natural language processing, also known as 
NLP is machine learning that can understand the  
38:20 - context of a corpus corpus being a body 
of related text. So NLP is enable you  
38:25 - to analyze and interpret text within documents 
and email messages interpret, or contextualize  
38:30 - spoken tokens. So for example, maybe customer 
sentiment analysis whether customers happy or sad,  
38:35 - synthesize speech, so a voice 
assistants assistant talking to you  
38:39 - automatically translate spoken or written 
phrases and sentences between languages,  
38:43 - interpret spoken or written commands and determine 
appropriate actions. A very famous example for a  
38:49 - voice assistant specifically or virtual assistant 
for Microsoft is Cortana. He uses the Bing  
38:54 - search engine to perform tasks such as setting 
reminders and answering questions for the user.  
38:58 - And if you're on a Windows 10 machine, it's 
very easy to activate Cortana by accident.  
39:03 - When we were talking about Azure as MLP offering 
we have text an analytic so sentiment analysis to  
39:08 - find out what customers think. Find topic. Topic 
relevant phrases using key phrase extraction,  
39:13 - identify the language of the text with language 
detection, detect and categorize entities in your  
39:18 - text. With named entity recognition for translator 
we have real time text translation and multi  
39:22 - language support. For speech service. We 
have transcribe audible speech into readable  
39:28 - searchable text, and then we have language 
understand understanding, also known as Louis  
39:34 - natural language processing service that enables 
you to understand human language in your own  
39:38 - application website, chatbots IoT device and 
more when we talk about conversational AI,  
39:44 - it usually generally uses NLP so that's 
where you'll see that overlap next, okay.
39:53 - Let's take a look here at conversational AI, 
which is technology that can participate in  
39:57 - conversations with humans. So we have chatbots 
voice assistant In an interactive voice  
40:01 - recognition systems, which is like the second 
version to interactive voice response system,  
40:06 - so you know, when you call in and they say 
press these numbers that is a response to some  
40:10 - and recognition system is when they can actually 
take human speech and translate that into action.  
40:16 - So the use cases here would be online customer 
support replaces human agents for, for replying  
40:20 - about customer FAQs, maybe shipping questions 
anything about customer support, accessibility, so  
40:26 - voice opera UI for those who are visually impaired 
HR processes, so employee training, onboarding,  
40:31 - updating employee information. I've never seen it 
used like that. But that's what they say is the  
40:35 - use case healthcare accessible, affordable health 
care. So maybe you're doing a claim process. I've  
40:40 - never seen this. But maybe in the US where you do 
more your claims and everything is privatized, it  
40:44 - makes more sense, Internet of Things, IoT devices. 
So Amazon, Alexa Apple, Siri, Google Home,  
40:50 - and I suppose Cortana, but it doesn't really have 
a particular device. So that's why I didn't list  
40:54 - it their computer software, so autocomplete 
search on phone or desktop. So that would be  
40:59 - Cortana. Something it could do. For the two 
services that around conversational AI for Azure,  
41:04 - we have q&a maker so create a conversational 
Question and Answer bot from your existing  
41:09 - content, also known as a knowledge base, 
and Azure bot service intelligent service  
41:14 - bot service that scales on demand used 
for creating publishing managing bots. So  
41:18 - the idea is you make your bot here and 
then you deploy it with this. Okay.  
41:27 - Let's take a look here at responsible AI which 
focuses on ethical, transparent and accountable  
41:31 - uses ai ai technology, Microsoft puts into 
practice responsible AI via its six Microsoft  
41:36 - AI principles. This whole thing is invented by 
Microsoft. And so you know, it's not necessarily  
41:41 - a standard but it's something that Microsoft 
is pushing hard to have people adopt Okay,  
41:46 - so we The first thing we have is fairness. 
So this is an AI system, which should treat  
41:50 - all people fairly, we have reliability and 
safety and AI systems should perform reliably  
41:55 - and safely. Privacy and Security AI system should 
be secure and respect privacy inclusiveness  
42:01 - system should empower everyone and engage people 
transparency, AI systems should be understandable  
42:06 - accountability. People should be accountable 
for AI systems and we need to know these in  
42:12 - greater detail. So we're going to have a 
short little video on each of these okay.  
42:20 - First arlis is fairness. So AI systems should 
treat all people fairly so an AI system can  
42:25 - reinforce existing social societal stereotype, 
stereotypical bias can be introduced during the  
42:32 - development of a pipeline. So an AI system that 
are used to allocate or withhold opportunities,  
42:37 - resources or information in domains such as 
criminal justice, employee employment in hiring,  
42:42 - finance, and credit. So an example here would be 
an ml model designed to select a final applicant  
42:47 - for hiring pipeline without incorporating any 
bias based on gender, ethnicity or may result  
42:51 - in unfair advantage. So as your ml can tell you 
how each feature can influence a models prediction  
42:57 - for bias. One thing that could be of use is fair 
learn. So it's an open source Python project to  
43:02 - help data scientists to improve fairness and the 
AI systems. At the time of I made this course,  
43:07 - a lot of their stuff is still in preview. So 
you know, it's the fairness component is it's  
43:11 - not 100% there, but it's great to see 
that they're getting that along. Okay.  
43:19 - So we are on to our second AI principle for 
Microsoft and this one is AI systems should  
43:23 - perform reliably and safely. So AI software 
must be rigorously tested to ensure they work  
43:28 - as expected before released to the end user. If 
there are scenarios where AI is making mistakes,  
43:33 - it is important to release a report quantified 
risks and harms to end users so they are  
43:37 - informed of the shortcomings of an AI solution. 
Something you should really remember for the exam,  
43:41 - they'll definitely ask that AI were concern  
43:44 - for reliability, safety for humans is 
critically important. autonomous vehicles,  
43:49 - health diagnosis, suggestions, prescriptions 
and autonomous weapon systems. They didn't  
43:53 - mention this in their content. And I was just like 
doing some additional resource research. I'm like,  
43:57 - yeah, you really don't want mistakes when you have 
automated weapons or ethical You shouldn't have  
44:02 - them at all. But hey, that's, that's just how the 
world works. But yeah, this is this category here.  
44:11 - We're on to our third Microsoft AI 
principle AI system should be secure and  
44:16 - respect privacy. So AI can require vast 
amounts of data to train deep machine ml  
44:20 - models, the nature of an ml model may require 
personally identifiable information. So P eyes,  
44:27 - it is important that we ensure protection of user 
data that is not leak or disclosed. In some cases,  
44:32 - ml models can be run locally on a user's device. 
So their PII is remain on their device avoiding  
44:38 - the vulnerability. This is called this is like 
edge computing. So that's the concept there  
44:42 - AI security principles to check malicious actors. 
So data origin and lineage data use internal  
44:48 - versus external data corruption considerations, 
anomaly detection. So there you go.
44:56 - We're on to the fourth Microsoft principles of 
assets. Should empower everyone and engage people.  
45:01 - If we can design AI solutions for the minority 
of users, they can design AI solutions to the  
45:05 - majority of users. So we're talking about minority 
groups, we're talking about physical ability,  
45:08 - gender, sexual orientation, ethnicity, 
other factors. This one's really simple.  
45:13 - In terms of practicality, it doesn't 100% 
make sense, because if you've worked with  
45:18 - groups that are deaf and blind developing 
technology for them, a lot of times they need  
45:22 - specialized solutions. But the approach here is 
that, you know, if we can design for the minority,  
45:27 - we can design for all that is the principle 
there. So that's what we need to know. Okay.  
45:35 - Let's take a look here at transparency. 
So AI system should be understandable. So  
45:39 - interpretability, and intelligibility is when the 
end user can understand the behavior of UI. So  
45:44 - transparency of AI systems can 
result in mitigating unfairness  
45:48 - help developers debug their AI systems 
gaining more trust from our users,  
45:52 - those builds a, those who build AI systems should 
be open about why they're using AI open about the  
45:57 - limitations of the AI systems. adopting an open 
source AI framework can provide transparency,  
46:02 - at least from a technical perspective on 
the internal workings of an AI system.  
46:11 - We are on to the last Microsoft AI principle 
here people should be accountable for AI  
46:16 - systems. So the structure put in place to 
consistently enacting AI principles and  
46:20 - taking them into account AI systems should work 
within frameworks of governments, organizational  
46:24 - principles, ethical and legal standards that 
are clearly defined principles guide Microsoft  
46:29 - and how they develop, sell an advocate when 
working with third parties and this push  
46:33 - towards regulation towards a principle. So 
this is Microsoft saying, Hey, everybody adopt  
46:38 - our model. There are many other models, I guess 
it's great that Microsoft is taking the charge  
46:43 - there, I just feel that it needs to be a bit 
more well developed. But what we'll do is look  
46:47 - at some more practical examples so we can better 
understand how to apply their principles. Okay.  
46:56 - So if we really want to understand how 
to apply the Microsoft AI principles,  
46:59 - they've great created this nice little tool via a 
free web app for practical scenarios. So they have  
47:03 - these cards, you can read through these cards, 
they're color coded for different scenarios,  
47:08 - and there's a website so let's go take a 
look at that and see what we can learn Okay.  
47:15 - All right, so we're here on the guidelines 
for human AI interaction so we can better  
47:19 - understand the how to put into 
practice the Microsoft AI principles.  
47:24 - They have 18 cards and let's work our way through 
here and see the examples the first one our list,  
47:29 - make clear what the system can do help the users 
understand what the AI system is capable of doing.  
47:33 - So here PowerPoint quickstart builders and 
builds an online outline to help you get started  
47:38 - researching subject displays suggested topics 
that help you understand the features capability.  
47:44 - Then we have the Bing app shows examples 
of types of things you can search for.  
47:49 - Apple Watch displays all metrics attracts 
explains how going on the second card we  
47:54 - have make clear how well the system can do 
what it can do. So here we have office new  
48:00 - companion experience ideas doc alongside your 
work, and offers one click assistance but  
48:05 - grammar design data insights, richer images 
and more. The unassuming term ideas coupled  
48:10 - with label previews, help set expectations 
and presented suggestions. The recommender  
48:15 - in Apple Music uses language such as we will 
think you'll like to communicate uncertainty.  
48:22 - The Help page for Outlook webmail explains 
the filtering into focused and other  
48:26 - and we'll start working right away but we'll 
get better with use, making clear the mistakes  
48:31 - will happen and you teach the product and 
set overrides onto our red cards. Here.  
48:37 - We have time surfaces based on context 
time when to act or interrupt based  
48:41 - on the user's current task environment. 
When it's time to leave for appointments,  
48:45 - Outlook sends a time to leave notification 
with directions for both driving and public  
48:50 - transit taking into account current location 
of that location real time traffic information.  
48:56 - And then we have after using Apple Maps routing, 
it remembers when you're parked your car when you  
49:01 - open the app after a little while it suggests 
routing to the location of the parked car.  
49:06 - All these Apple examples make me think that 
Microsoft has some kind of partnership with Apple.  
49:10 - I guess I guess Microsoft or or Bill Gates did 
own Apple shares. So maybe they're closer than  
49:16 - we think, show contextually relevant information 
time when to act or interrupt based on user's  
49:21 - current task and environment. Powered by 
machine learning acronyms in Word helps you  
49:25 - understand shorthand employed in your own work 
environment relative to current OpenDocument.
49:32 - On walmart.com, when the user is looking at 
a product, such as gaming console recommends  
49:37 - accessories and games that would go with 
it. When a user searches for movies, Google  
49:42 - shows results including showtimes near 
the user's location for the current data  
49:47 - onto our fifth card here. Match based. 
We didn't we didn't miss this one, right.  
49:52 - Yeah, we did. Okay, so we're on the fifth one here 
match relevant social norms ensure experiences  
49:57 - delivered in a way that users would expect Given 
the social cultural context when editor identifies  
50:03 - ways to improve writing style prints optionals 
politely consider using. That's the Canadian way  
50:09 - being polite. Google Photos is able to recognize 
pets and use the wording important cats and dogs  
50:15 - recognizing that for many pets are an important 
part of one's family. And you know what? When I  
50:21 - started renting my new house, I said, you know, 
these are probably dogs and my landlord said,  
50:26 - Well, of course pets are part of the 
family. That was something I like to hear.  
50:30 - Cortana uses semi formal town apologizing when 
unable to find a contact, which is polite and  
50:37 - socially appropriate. I like that. Okay, 
mitigate social biases ensure AI system,  
50:43 - languages and behaviors do not reinforce 
undesirable unfair stereotypes and biases.  
50:48 - My analytics summarizes how you spend your time at 
work, and suggest ways to work smarter one ways to  
50:53 - mitigate bias is by using gender neutral icons 
to represent important people sounds good to me.  
50:58 - A Bing search for SEO or doctor shows images of 
diverse people in terms of gender and ethnicity.  
51:04 - Sounds good to me. The predictive keyboard for 
Android suggests both genders when typing a  
51:10 - pronoun starting with the letter H. We're onto our 
yellow cards support efficient invocation so make  
51:17 - it easy to invoke or request system services 
when needed. So Flash Fill is a helpful time  
51:22 - saver in Excel that can be easily invoked with on 
canvas interactions and that keep you in flow on  
51:29 - amazon.com Oh, hey there got Amazon. In addition 
to the system giving recommendations as you Rouse  
51:35 - you can manually invoke additional recommendations 
from the recommender for your menu. design ideas  
51:41 - in Microsoft PowerPoint can be invoked with the 
with the press of a button if needed. I cannot  
51:47 - stand it when that pops up. He's up to tell it to 
leave me alone. Okay, support efficient, dismal,  
51:53 - efficient. Does Mazal dismissal Oh, support 
efficient dismissal? Okay, make it easy to dismiss  
52:02 - or ignore under undesired AI system services. 
Okay, that sounds good to me. Microsoft forms  
52:07 - allows you to create custom surveys, quizzes, 
polls, questionnaires and forms some choices,  
52:11 - questions triggers suggested options, but just 
beneath the relevant question, this suggestion can  
52:16 - be easily ignored and dismissed. Instagram allows 
the user to easily hide report ads that have been  
52:22 - suggested by AI by tapping the ellipses 
at the top of the right of the ad.  
52:27 - Siri can be easily dismissed by saying Never 
mind. always telling my Alexa Nevermind. Support  
52:37 - efficient correction make it easy to edit, refine 
or recover the AI system when the when the AI  
52:43 - system is wrong, so I'll Auto altex automatically 
generates alt text for photographs by using  
52:48 - intelligence services in the cloud descriptions 
can be easily modified by clicking the alt text  
52:53 - button in the ribbon once you set a reminder, 
with Siri, the UI displays a tap to edit link.  
52:59 - When being automatically correct spelling errors 
in search queries it provides the option to revert  
53:04 - to the query as originally typed with one click 
onto a card number 10. Scope services when in  
53:11 - doubt, engage in dis ambiguous, Anubis, disk 
and big uation or gracefully degrade the AI  
53:19 - system service when uncertain about a user's 
goal. So an auto replacement word is uncertain  
53:24 - of a correction it engages in disambiguation by 
displaying multiple options you can select from  
53:31 - certain will let you know it has trouble hearing 
if you don't respond or talk or, or speak too  
53:35 - softly. Bing Maps will provide multiple routing 
options when, when unable to recommend best one  
53:41 - we're onto card number 11. make clear why the 
system did what it did enable users to access  
53:47 - an explanation of why the AI system behaved as it 
did. Office Online recommends document documents  
53:53 - based on history and activity descriptive text 
above each document makes it clear why the  
53:58 - recommendation is shown. product recommendations 
on amazon.com include why recommend recommended  
54:04 - link that shows that what products in the user 
shopping history and forums. The recommendations  
54:10 - Facebook enables you to access an explanation 
about why you are seeing each ad in the news feed
54:18 - onto our green cards. So remember recent 
interactions so maintain short term memory  
54:23 - and allow the user to make efficient references 
to that memory. When attaching a file outlook  
54:27 - offers a list of recent files including recently 
copied file links. Look also remembers people  
54:33 - you have interacted with recently and 
displays them when addressing a new email  
54:39 - being searched remember, some recent queries and 
search can be continued. conversationally. How  
54:44 - old is he after a search for Keanu Reeves? Siri 
carries over the contacts from one interaction to  
54:50 - the next a text message is created from the person 
you told Siri to message to onto card number 13  
54:57 - lucky number 13 learn from user behavior personal 
user experience by learning from their actions  
55:02 - over time. Tap on the search bar in Office 
applications and search lists the top three  
55:07 - commands on your screen that you're most likely 
to need to personalize the technology called  
55:13 - zero query doesn't even need to type in the search 
bar to provide a personalized predictive answer.  
55:19 - amazon.com gives personalized product 
recommendations recommendations based on previous  
55:23 - purchases on the card 14. update and adapt 
cautiously limit disruptive changes when updating  
55:30 - adaptive adapting the AI systems behaviors so 
PowerPoint designer improves slides for office  
55:35 - 365 subscribers by automatically generating design 
ideas from to choose from designer has integrated  
55:42 - new capabilities such as smart graphics, icons, 
suggestions and existing user experience ensuring  
55:46 - the updates are not disruptive. Office tell office 
Tell me feature shows dynamically recommended  
55:53 - items and it doesn't need a try area to minimize 
disruptive changes onto card number 15. Encouraged  
56:01 - granular feedback enabled users to provide 
feedback indicating their preferences during  
56:06 - regular interactions with the AI system so 
ideas in Excel empowers you to understand  
56:10 - your data through high level visual summaries, 
trends and patterns encourages feedback on each  
56:14 - suggestion by asking is this helpful? Not only 
does Instagram provide the option to hide specific  
56:20 - ads, but it's also solicits feedback to 
understand why the ad is not relevant.  
56:24 - In Apple's music app love dislike buttons are 
prominent, easily accessible. Number 16 convey  
56:31 - the consequences of user actions immediately 
under update or convey how user actions will  
56:34 - impact future behaviors of the AI system. You can 
get stock and geographic data types in Excel It is  
56:40 - easy as typing text into a cell and converting 
it to stock data type or geographic geographic  
56:47 - data type. When you perform the conversion action, 
an icon immediately appears in the converted cells  
56:52 - upon tapping the like Dislike button for each 
recommendation. In Apple Music, a pop up informs  
56:58 - the user that they'll receive more or fewer 
similar recommendations onto card number 17.  
57:03 - Or almost near the end provide global controls 
allow the user to globally customize the system.  
57:09 - System monitors and how it behaves so editor 
expands on spelling and grammar. Checking  
57:15 - capabilities of words include more advanced 
proofing and editing designed to ensure document  
57:20 - is readable editor can flag a range of critique 
types and allow to customize the thing is is that  
57:25 - in Word, it's so awful spell checking, I don't 
understand like it's been years and the spell  
57:31 - checking never gets better. So the guy implore 
better spell checking. I think being search  
57:36 - provides settings that impact that the types 
of results the engine will return, for example,  
57:41 - safe search. Then we have Google Photos allows you 
to to turn location history on and off for future  
57:48 - photos. It's kind of funny seeing like being in 
there about like using AI because at one point,  
57:53 - it's almost pretty certain that Bing was copying 
just google search indexes to learn how to index.  
57:59 - I don't know that's Microsoft for you. We're 
onto card 18 notify users about changes  
58:04 - informed user when AI system adds or updates as 
capabilities. Then what's new dialog in office  
58:10 - informs you about changes by giving an overview 
of the latest features and updates, including  
58:14 - updates to AI features in Outlook web to help 
tab includes a what's new section that covers  
58:20 - updates. So there we go. We made it to 
the end of the list. I hope that was a  
58:25 - fun lesson for you. And there I hope that we 
could kind of match up the the responsibly  
58:31 - I I kind of wish what they would have done is 
actually mapped it out here and say word match,  
58:35 - but I guess it's kind of an isolate service that 
kind of ties in. So I guess there we go, Okay.
58:44 - Hey, this is Andrew Brown from 
exam Pro. And we're looking at  
58:47 - Azure cognitive services. And this is 
a comprehensive family of AI services,  
58:51 - and cognitive API's to help you build 
intelligent apps. So create customizable,  
58:55 - pre trained models built with breakthrough AI 
researchers I put that in quotations I'm kind  
59:00 - of throwing some shade at Microsoft Azure just 
because it's their marketing material, right?  
59:05 - deploy Cognitive Services anywhere from cloud to 
the edge. With containers get started quickly no  
59:11 - machine learning expertise required. But I think 
it helps to have a bit of background knowledge  
59:16 - developed with strict ethical standards. Microsoft 
loves talking about the responsible. There's  
59:22 - responsible AI stuff, empowering responsible use 
with industry leading tools and guidelines. So  
59:28 - let's do a quick breakdown of the types of 
services in this family. So for decision we have  
59:32 - anomaly detector identify potential problems early 
on content moderator detect potentially offensive  
59:38 - or unwanted content, personalize or create 
rich personalized experiences for every user.  
59:43 - For languages we have language understanding, also 
known as alue is Louis I don't know I didn't put  
59:48 - the initialism there but don't worry, we'll see it 
again. Build natural language understanding into  
59:52 - app spots and IoT devices q&a maker create 
a conversational Question and Answer layer  
59:57 - over your data text analytics. detect sentiment. 
So sentiment is like whether customers are happy,  
60:03 - sad, glad, keep phrases and named entities 
translator detect and translate more than  
60:09 - 90 supported languages. For speech, we have 
speech to text to transcribe audible speech  
60:14 - into readable search text, text to speech convert 
text to lifelike speech for natural interfaces,  
60:20 - speech translation, so integrate real time speech 
translation into your apps, Speaker recognition,  
60:26 - identify and verify the people speaking based 
on audio for vision. We have computer vision,  
60:33 - so analyze content and images and videos 
custom vision, so analyze or sorry,  
60:38 - customize image record or image 
recognition to fit your business needs,  
60:43 - face detect and identify people and 
emotions in images. So there you go.  
60:53 - So as your cognitive services is an umbrella 
AI service that enables customers to access  
60:58 - multiple AI services with an API key and API 
endpoint, so what you do is you go create a  
61:03 - new cognitive service. And once you're there, 
it's going to generate two keys and an endpoint.  
61:08 - And that is what you're using generally for 
authentication with the various AI services  
61:12 - programmatically. And that is something that 
is key to the service that you need to know.
61:20 - So knowledge mining is a discipline in AI that 
uses a combination of intelligence services to  
61:25 - quickly learn from vast amounts of information. 
So it allows organizations to deeply understand  
61:30 - and easily explore information, uncover hidden 
insights and find relationships and patterns  
61:34 - at scale. So we have ingest, enrich and explore 
as our three steps. So for ingest content from  
61:40 - a range of sources using connectors to first 
and third party data stores. So we might have  
61:44 - structured data such as databases csvs. 
The csvs would more be semi structured,  
61:50 - but we're not going to get into that level 
of detail unstructured data. So PDFs, videos,  
61:54 - images and audio for enrich the content with AI 
capabilities that let you extract information,  
62:00 - find patterns and deepen understanding. So 
cognitive services like vision, language, speech,  
62:05 - decision, and search, and explore the newly 
indexed data via search bots, existing businesses,  
62:11 - applications and data visualizations and rich, 
structured data, customer relationship management,  
62:17 - rap systems, Power BI, this whole knowledge mining 
thing is a thing but like, I believe that the  
62:22 - whole model around this is so that Azure shows you 
how you can use the cognitive services to solve  
62:28 - things without having to invent new solutions. So 
let's look at a bunch of use cases that Azure has  
62:33 - and see what where we can find some useful use. 
So the first one here is for content research. So  
62:39 - when organizations task employees review and 
research of technical data, it can be tedious  
62:44 - to read page after page of dense Tex knowledge 
mining helps employees quickly review these dense  
62:49 - materials. So you have a document and in the 
Richmond step, you could be doing printed text  
62:54 - recognition key phrase extraction, sharpen or 
sharpen skills, technical keyword, sanitation,  
63:00 - format, definition minor large scale vocabulary 
matcher, you put it through a search service,  
63:05 - and now you have search reference library, 
so it makes things a lot easier to work with.  
63:10 - Now, we have audit risk compliance management 
so developers could use knowledge mining to help  
63:15 - attorneys quickly identify entities of importance 
from discovery documents and flag important ideas  
63:20 - across documents that we have documents. So clause 
extraction clause classification, TV power risk,  
63:27 - named identity extraction, key phrase extraction, 
language detection, automate translation, then you  
63:33 - put it back into a search index and now you can 
use it our management platform or a word plug  
63:37 - in. And so we have business process management in 
industries where bidding competition is fierce, or  
63:43 - when the diagnosis of a problem must be quick or 
in near real time, companies use knowledge mining  
63:49 - to avoid costly mistakes. So the client drilling 
and completion reports, document processor,  
63:55 - ai services and custom models queue for human 
validation, Intelligent Automation, you send it  
64:01 - to a back end system or a data lake and or a data 
lake and then you do your analytics dashboard.  
64:07 - Then we have customer support and feedback 
analysis. So for many companies, customer support  
64:12 - is costly and efficient. Knowledge mining can 
help customer support teams quickly find the right  
64:17 - answers for a customer inquiry or assess customer 
sentiment at scale. So you have your source data,  
64:23 - you do your document cracking use cognitive 
skills, so pre trained services or custom.  
64:28 - You have enriched documents. From here you're 
going to do your projections and have a knowledge  
64:32 - store you're gonna have a search index, and 
then do your analytics something like Power BI,  
64:37 - we have digital assessment management. There's a 
lot of these but it really helps you understand  
64:40 - how cognitive services are going to be useful. 
Given the amount of unstructured data created  
64:45 - daily, many companies are struggling to make 
use of or find information within their files.  
64:50 - Knowledge mining through a search index 
makes it easy for end customers and  
64:53 - employees to locate what they're looking 
for faster. So you can just like art,  
64:57 - metadata and the actual images themselves for 
the top player. geopoint extractor biographical  
65:02 - richer than down below we're tagging, we're 
custom object detector similar image tagger,  
65:06 - we put it in a search index, they love those 
search indexes. And now you have an art Explorer.  
65:12 - We have contract management, this is the 
last one here, many companies create products  
65:17 - for multiple sectors. Hence the business 
opportunities with different vendors and  
65:21 - buyers increase exponentially. Knowledge mining 
can help organizations to scour 1000s of pages of  
65:26 - sources to create Accurate Bids. So here we have 
RFP documents. This will actually probably come  
65:32 - back later in the original set, but we will will 
will do risk extraction, print text recognition,  
65:38 - key phrase extraction, organizational 
extraction engineering standards  
65:42 - will create a search index and put it 
here, this will bring back data. Also,  
65:46 - metadata extraction will come back here. And then 
this is just like a continuous pipeline, okay.
65:54 - Hey, this is Andrew Brown from exam Pro. And 
we are looking at face service. And Azure face  
65:59 - service provides an AI algorithm that can detect 
recognize and analyze human faces and images,  
66:04 - such as a face and an image face with specific 
attributes, face landmarks similar faces the same  
66:11 - face as a specific identity across a gallery 
of images. So here's an example of an image  
66:17 - that I ran that will do in the follow along. 
And what it's done is it's drawn a bounding  
66:21 - box around the image. And there's this ID and 
this is a unique identifier, string for each  
66:26 - detected face in an image. And these can be unique 
across a gallery, which is really useful as well.  
66:31 - Another cool thing you can do is face landmarks. 
So the idea is that you have a face and it can  
66:37 - identify very particular components of it. And 
up to 27 predefined landmarks is what is provided  
66:43 - with this face service. Another interesting thing 
is face attributes. So you can check whether  
66:49 - they're wearing accessories, accessories, so think 
like earrings or lip rings, determine its age,  
66:55 - the blurriness of the image, what kind of emotion 
is being experienced the exposure of the image,  
67:01 - you know, the contrast, facial hair, gender, 
glasses, your hair in general, the head pose,  
67:08 - there's a lot of information around that makeup, 
which seems to be limited, like when we ran it  
67:12 - here in the lab, all we got back was eye makeup 
and lip makeup. But hey, we get some information,  
67:18 - whether they're wearing a mask, noise, so 
whether there's artifacts like visual artifacts,  
67:23 - or occlusion, so whether an object is blocking 
the parts of the face, and then they simply have  
67:29 - a boolean value for whether the person smiling or 
not, which I assume is a very common component.  
67:34 - So that's pretty much all we really need to 
know about the face service. And there you go.  
67:42 - Hey, this is Andrew Brown from exam prep, and we 
are looking at the speech and translate service.  
67:46 - So Azure is translate service is a translation 
services the name implies, and it can translate 90  
67:52 - languages and dialects. And I was even surprised 
to find out that it can translate into calling on  
67:57 - and it uses neural machine translation and Mt 
replacing its legacy to statistical machine  
68:03 - translation SMT. So what my guess here is that 
statistical meaning that it used classical  
68:09 - machine learning back in 2010, and, and then they 
decided to switch it over to neural networks,  
68:14 - which, of course, would be a lot more accurate 
as your transit service can support a custom  
68:19 - translator. So it allows you to extend the service 
for translation based on your business domain use  
68:24 - cases. So if you use a lot of technical words and 
things like that, then you can fine tune that or  
68:29 - particular phrases. Then there's the other 
service, Azure speech service. And this is a,  
68:34 - a speech, synthesis service, a service. So 
what can do speech to text text to speech  
68:40 - and speech translation, so it's 
synthesizing creating new voices. Okay,  
68:44 - so we have speech to text. So real time speech 
to text batch batching multidevice, conversation,  
68:50 - conversation, transcription. And you can create 
custom speech models and you have text to speech.  
68:55 - So this utilizes a speech synthesis markup 
language, so it's just a way of formatting it,  
69:00 - and it can create custom voices. Then you have 
the voice assistance of integrates with the Bot  
69:05 - Framework SDK, and speech recognition. So speaker 
verification and identification. So there you go.
69:15 - Hey, this is Andrew Brown from exam Pro. 
And we were looking at text analytics and  
69:20 - this is a service for NLP so natural language 
processing for text mining and text analysis. So  
69:26 - text analytics can perform sentiment analysis, 
so find out what people think about your brand  
69:30 - or topics. So features provide sentiment 
labels, such as negative, neutral positive,  
69:35 - then you have opinion mining, which is an 
aspect based sentiment analysis. It's for  
69:40 - granular information about the opinions related 
to aspects. Then you have key phrase extraction.  
69:45 - So quickly identify the main concepts in text. 
You have language detections that detect the  
69:50 - language of an input, a text that it's written 
in, and you have named entity recognition,  
69:55 - so ner so identify and categorize entities in your 
text as people places off objects and quantities,  
70:01 - and subset of any AR is personally identifiable 
information. So P eyes, let's just look at a few  
70:08 - of these more in detail. Some of them are very 
obvious, but some of these would help to have  
70:12 - an example. So the first we're looking at is key 
phrase extraction. So quickly identify the main  
70:16 - concepts in text. So key phrase extraction works 
best when you when you give bigger amounts of  
70:20 - text to work on. This is the opposite of sentiment 
analysis, which performs better on smaller amounts  
70:24 - of text. So document sizes can be 5000 or fewer 
characters per document. And you can have up to  
70:32 - 1000 items per collection. So imagine you have a 
movie review with a lot of text in here and you  
70:37 - want to extract out the key phrases. So here it 
is, sideboard ship, enterprise, surface travels,  
70:43 - things like that, then you have named entity 
recognition. So this detects words and phrases  
70:48 - mentioned in unstructured data that can be 
associated with one or more semantic types. And  
70:53 - so here's an example. I think this is medicine, 
bass. And so the idea is that it's identifying,  
70:59 - it's identifying these words or phrases, and then 
it's applying a semantic type. So it's saying like  
71:05 - this is like diagnosis is the medication class 
and stuff like that. semantic type could be more  
71:11 - broad. So there's location events, a habit 
location, twice here, person diagnosis age,  
71:15 - and there is a predefined set, I believe that is 
in Azure that you should expect, but they have  
71:20 - a generic one. And then there's one that's for 
health. We're looking at sentiment analysis, this  
71:25 - graphic makes it make a lot more sense when we're 
splitting between sentiment and opinion mining.  
71:31 - The idea here is that sentiment analysis will 
apply labels and confidence scores to text at  
71:35 - the sentence and document level. And so labels 
could include negative positive, mixed or neutral  
71:42 - and will have a confidence score ranging from 
zero to one. And so over here, we have a sentiment  
71:47 - analysis of this line here and in saying that 
this was a negative sentiment. But look, there's  
71:51 - something that's positive and there's something 
that's negative, so was it really negative,  
71:55 - and that's where opinion mining gets really 
useful because it has more granular data,  
71:59 - where we have a subject and we have an opinion, 
right and so here we can see the room was great,  
72:04 - but the staff was unfriendly negative. 
So we have a bit of a split there Okay.
72:14 - Hey, this is Angie brown from exam pro and we 
are looking at optical character recognition,  
72:18 - also known as OCR, and this is the process of 
extracting printed or handwritten text into a  
72:23 - digital and editable format. So OCR can be applied 
to photos of street signs, products, documents,  
72:29 - invoices, bills, financial reports, articles and 
more. And so here's an example of us extracting  
72:35 - out nutritional data or nutritional facts off the 
back of a food product. So Azure has two different  
72:43 - kinds of API's that can perform OCR. They have the 
OCR API and the read API. So the OCR API uses an  
72:50 - older recognition model. It supports only images, 
it executes synchronous notes synchronously,  
72:56 - returning immediately, when it detects texts, it's 
suited for less text, it supports more languages,  
73:03 - it's easier to implement. And on the other 
side, we have the read API. So this is an  
73:08 - updated recognition model supports images 
and PDFs, executes asynchronously. paralyzes  
73:15 - tasks per line for faster results, suited 
for lots of tax supports a few languages,  
73:21 - and it's a bit more difficult to implement. 
And so when we want to use this service,  
73:25 - we're going to be using computer vision SDK, 
okay. Hey, this is Andrew Brown from exam Pro,  
73:36 - and we're taking a look here at form recognizer 
service. This is a specialized OCR service that  
73:41 - translates printed text into digital and editable 
content. It pervert preserves the structure and  
73:46 - relationships of the form like data. That's what 
makes it so special. So form recognizer is used  
73:51 - to automate data entry in your applications 
and enrich your document search capabilities.  
73:55 - It can identify key value pairs selection marks 
table structures, it can produce output structures  
74:01 - such as original file relationships, bounding 
boxes, confidence score, and form recognizer is  
74:07 - composed of a custom document processing models, 
pre built models for invoices, receipts, IDs,  
74:12 - business cards, the model layouts, let's talk 
about the layout here. So extract text selection  
74:17 - marks table structures along with bounding box 
coordinates from documents form. recognizer can  
74:21 - extract text selection marks and table structures. 
The row and column numbers associate with the text  
74:27 - using high definition optical character 
enhancement models. That is totally useless text.  
75:10 - Hey, this is Andrew Brown from exam Pro. And we 
are looking at form recognizers service. And this  
75:14 - is a specialized service for OCR. That translates 
printed text into digital editable content.  
75:19 - But the magic here is that that preserves the 
structure and relationship of form like data. So  
75:24 - there's an invoice you see those magenta lines, 
it's saying identify that form like data. So  
75:30 - for recognizer is used to automate data entry 
in your applications and enrich your document  
75:34 - search capabilities. And it can identify key value 
pairs, selection of marks, tables, structures,  
75:39 - and it can put structures such as original file 
relationships, bounding box boxes, confidence  
75:43 - scores. It's composed of customer custom document 
processing model, pre built models for invoices,  
75:50 - receipts, IDs, business cards, it's based 
on this layout model. And there you go.
76:00 - So let's touch upon custom models. So custom 
models allow you to extract text key value  
76:05 - pairs selection marks in tabular data from your 
forms. These models are trained with your data,  
76:09 - so they're tailored to your 
forms, you only need five samples,  
76:13 - sample input forms to start, a trained document 
processing model can output structured data that  
76:17 - includes the relationship and the original 
form document. After you train the model,  
76:20 - you can test and retrain it and eventually 
use it reliably extract data from  
76:24 - more forms according to your needs. You have two 
learning options, you have unsupervised learning  
76:29 - to understand the layout and relationships 
between fields entries in your forms. And  
76:32 - you have supervised learning to extract values of 
interest using the labeled form. So we've covered  
76:37 - unsupervised and supervised learning, so you're 
going to be very familiar with these two. Okay.  
76:46 - So form recognizer service has many pre built 
models that are easy to get started with. And  
76:52 - so let's go look at them and see what kind 
of fields that extracts out by default.  
76:56 - So the first is receipts. So sales receipts 
from Australia, Canada, Great Britain,  
77:00 - India and United States will work great here and 
the fields that will extract out his receipt type  
77:04 - merchant name, merchant phone number, merchant 
address, transaction, date, transaction time,  
77:08 - total subtotal, tax tip, items, name, quantity, 
price, total price, there's information that is on  
77:14 - a receipt that you're not getting out of 
these fields. And that's where you make  
77:17 - your own custom model right. For Business cards. 
It's only available for English business cards,  
77:22 - but we can extract our contact names first name, 
last name, company names, departments, job titles,  
77:27 - emails, websites, addresses, mobile phones, 
faxes, work phones, and other phone numbers.  
77:32 - Not sure how many people are using business cards 
these days, but hey, they have it as an option  
77:38 - for invoices, extract data from invoices in 
various formats and return structured data.  
77:42 - So we have customer name, customer ID, 
purchase order, invoice, ID, invoice, date,  
77:46 - due date, vendor name, vendor address, 
vendor address, receipt, customer address,  
77:50 - customer address, receipt and billing address, 
billing address, receipt shipping address,  
77:55 - subtotal, total tax invoice, 
total amount to service address,  
77:59 - remittance address, start service start date 
and end date, previous unpaid balance and then  
78:04 - they even have one for line items. So items amount 
description, quantity, unit price, Product Code,  
78:11 - unit date, tax, and then for IDs which could be 
worldwide passports, US driver's license, things  
78:17 - like that. You have fields such as country region, 
date of birth, date of expiry expiration document  
78:24 - name, first name, last name, nationality, sex, 
machine readable zone, I'm not sure what that is  
78:30 - document type, and address and region. And 
there are some additional features with some  
78:36 - of these bottles. We didn't really cover them 
it's not that important but yeah, there we go.
78:45 - Hey, this is Andrew Brown from exam Pro, and we're 
looking at natural understanding or Lewis or Luis  
78:51 - depends on how you'd like to say it. And this 
is a no code ml service to build language,  
78:56 - natural language into apps, bots and IoT devices 
have quickly create enterprise ready custom models  
79:02 - that continuously improve so Louis I'm gonna 
just call it Louis because that's what I prefer  
79:07 - is access via its own isolate domain@lewis.ai 
at a utilizes NLP and NLU so NLU is the ability  
79:16 - to perform or ability to transform a linguistic 
statement to a representation that enables you  
79:21 - to understand your users naturally. And it is 
intended to focus on intention and extraction,  
79:26 - okay, so where the users want, or was or what the 
users want, and what the users are talking about.  
79:32 - So the loose application is composed of a schema 
and a schema is auto generated for you when you  
79:38 - use the Louis AI web interface. So you definitely 
are going to be reading this by hand, but it just  
79:42 - helps to see what's kind of in there. If you do 
have some programmatic skills, you obviously you  
79:46 - can make better use of the service isn't just the 
web interface. But the schema defines intention.  
79:51 - So what the users are asking for a loose app 
always contains a nun intent. We'll talk about  
79:56 - why that is in a moment. And entities what parts 
of the intent is used to determine the answer.  
80:02 - Then you also have utterances. So examples of the 
user input that includes intent and entities to  
80:07 - train the ML model to match predictions against 
the real user input. So an intent requires one  
80:13 - or more example utterance for training. And it is 
recommended to have 15 to 30 example utterances  
80:18 - to explicitly train to ignore an utterance 
you use the nun intent. So, intent  
80:25 - classifies that user as utterances and entities 
extract data from utterances. So hopefully it  
80:31 - understands I always get this stuff mixed up, it 
always takes me a bit of time to understand there  
80:35 - is more than just these things is like features 
and other things. But you know, for the 900, we  
80:41 - don't need to go that deep. Okay, just to skip to 
visualizing this to make a bit easier. So imagine  
80:46 - we have this, this utterance here, these would 
be the identities that we have to end Toronto,  
80:52 - this is example utterance. And then the idea is 
that you'd have the intent and the intent. And if  
80:56 - you look at this keyword here, this really helps 
Word says classify is that's what it is. It's  
81:00 - a classification of this example utterance, and 
that's how the ML model is going to learn, okay.
81:10 - Hey, this is Andrew Brown from exam prep, and 
we're looking at q&a maker service. And this is a  
81:15 - cloud based NLP service that allows you to create 
a natural conversational layer over your data.  
81:20 - So q&a maker is hosted on its own iyslah 
domain at q&a maker.ai it will help the most,  
81:25 - it will help you find the most appropriate 
answer from any input from your custom knowledge  
81:29 - base of information. So you can commonly it's 
commonly used to build conversation clients,  
81:34 - which includes social apps chatbots speech enabled 
desktop applications. q&a maker doesn't store  
81:41 - customer data, all the customer data stored in 
the region, the customer deploys the dependent  
81:45 - services instances within Okay, so let's look at 
some of the use cases for this. So when you have  
81:51 - static information, you can use q&a maker in your 
knowledge base. The answers this knowledge base is  
81:56 - custom to your needs, which you've built with 
documents such as PDF URLs, where you want to  
82:00 - provide the same answer to repeat question command 
when different users submit the same question the  
82:05 - answers is returned when you want to filter stack 
information based on meta information. So meta tag  
82:11 - data is provide provides additional filtering 
options relevant to your client application  
82:15 - users and information common metadata information 
includes chitchat content type, format, content,  
82:20 - purpose, content, freshness. And there's the use 
case when you want to manage a bot conversation  
82:25 - that includes static information. So your 
knowledge base takes takes the user conversational  
82:30 - text, or command and answers that if the answer 
is part of a pre determined conversation flow,  
82:35 - represented in the knowledge base with multiple 
turnkey contexts the bot can easily provide this  
82:40 - flow. So q&a maker import your content into a 
knowledge base of questions and answer pairs.  
82:46 - And q&a maker can build your knowledge base 
from an existing document manual, or website,  
82:51 - your all docx PDF. I thought this was the coolest 
thing. So you can just basically have anyone write  
82:55 - a docx. As long as it has a heading and a text. 
They can even extract images and I'll just turn it  
83:01 - into the bot. It just saves you so much time It's 
crazy. It will use ml to extract the question and  
83:06 - answer pairs. The content of the question and 
answer pairs include all the alternate forms  
83:11 - of the question metadata tags used to filter 
choices. During the search, follow up prompts  
83:16 - to continue to search refinement, refinement, 
q&a maker stores, answers text in markdown.  
83:22 - Once your knowledge base is imported, you can 
fine tune the important results by editing the  
83:26 - question and answer pairs. As seen here. There 
is the chat box. So you can converse with your  
83:31 - bot through a chat box. I wouldn't say it's 
particularly a feature of q&a maker, but I just  
83:36 - want you to know that's how you would interact 
with it. So when you're using the q&a maker AI,  
83:40 - the Azure bot service the bot composer, or via 
channels, you'll get an embeddable one, you'll  
83:45 - see this box where you can start typing in your 
questions and and get back the answers to test  
83:50 - it. Here. An example is a multi turn conversation. 
So somebody asked a question, a generic question.  
83:55 - And that said, Hey, are you talking about AWS or 
Azure, which is kinda like a follow up prompt. And  
84:00 - we'll talk about multiturn here in a second, but 
that's something I want you to know about. Okay.  
84:04 - So chit chat is a feature in q&a maker that 
allows you to easily add pre populated sets of  
84:09 - top chit chats into your knowledge base. The 
data set has about 100 scenarios of chit chat in  
84:14 - voices of multiple personas. So the idea 
is like if someone says something random,  
84:18 - like how are you doing? What's the weather today, 
things that your bot wouldn't necessarily know.  
84:22 - It has like canned answers, and it's going to be 
different based on how you want the response to be  
84:27 - okay. There's a concept of layered ranking. So the 
q&a maker system is a layered ranking approach the  
84:34 - data stored in Azure Search, which also serves 
as the first ranking layer, the top result for  
84:39 - from Azure Search are then passed through q&a 
makers and LP ranking model to produce the final  
84:45 - results and confidence score. Just touching on 
multi turn conversation is a follow up prompt  
84:51 - and context to manage the multiple turns known 
as multi turn for your bot from one question  
84:55 - to another when a question can't be answered 
in a single turn. That is when you're using  
84:59 - multi turn conversation. So q&a maker provides 
multi turn prompts and active learning to help  
85:04 - you improve your questions based on key answer 
pairs, and it gives you the opportunity to connect  
85:08 - questions and answer pairs. The connection allows 
the client app position to provide a top answer  
85:13 - and provide more questions refine the search for 
a final answer. After the knowledge base receives  
85:18 - questions from users at the Publish endpoint, 
can I make replies active learning to these  
85:22 - rules or questions to suggest changes to your 
knowledge base to improve the quality alright.
85:31 - Hey, this is Andrew Brown from exam Pro. And we 
are looking at Azure bot service. So the Azure  
85:35 - bot services an intelligent serverless bot service 
that scales on demand used for creating publishing  
85:41 - and managing bots. So you can register and 
publish a variety of bots from the Azure portal.  
85:44 - So here there's a bunch of ones I've never heard 
of, probably with third party providers partnered  
85:48 - with Azure. And then there's the ones that we 
would know like the Azure health health bot,  
85:52 - the Azure bot, or the webapp bot, which 
is more of a generic one. So Azure bot  
85:58 - service bop bop bot service can integrate your bot 
with other Azure, Microsoft or third party service  
86:05 - services via channel so you can have a direct 
line out Alexa, office 365, Facebook, Keke line,  
86:14 - Microsoft Teams, Skype, Twilio and more. Alright, 
and two things that are commonly associated with  
86:20 - the Azure bot service is the Bot Framework and bot 
composer. In fact, it was really hard just to make  
86:26 - make this slide here because they just weren't 
very descriptive on it. Because I wanted to push  
86:29 - these other two things here. Let's talk about 
the Bot Framework SDK. So the Bot Framework SDK,  
86:34 - which is now version four is an Open Source 
SDK that enables developers to model and build  
86:39 - sophisticated conversations. The Bot Framework 
along with the Azure bot service provides an  
86:44 - end to end workflow. So we can design build, 
test, publish, connect, and evaluate. Are  
86:51 - bots. okay with this. With this framework, 
developers can create bots that use speech,  
86:55 - understand natural language, handle questions, 
answers, and more. The Bot Framework includes  
87:00 - a modular, extensible SDK for building bots, as 
well as tools, templates and related AI services.  
87:05 - Then you have Bot Framework composer. And this 
is built on top of the Bot Framework SDK. It's  
87:11 - an open source IV for developers to author test 
provision and manage conversational experiences.  
87:16 - You can download, it's an app on Windows OS 
X, and Linux is probably built using like  
87:22 - web technology. And so here is the actual app 
there. And so you can see there's kind of a  
87:27 - bit of a flow and things you can do in there. So 
you can either you see or note to build your bot,  
87:31 - you can deploy the bot to the Azure web apps or 
Azure Functions. You have templates to build q&a  
87:37 - maker bot enterprise or personal assistant bot 
language bot calendar, or people bought. You can  
87:42 - test and debug via the Bot Framework emulator, and 
has a built in package manager. There's a lot more  
87:47 - to these things. But again, at the AI 900 this 
is all we need to know. But yeah, there you go.  
87:56 - Hey, this is Andrew Brown from exam Pro. And we 
are looking at Azure Machine Learning service,  
88:01 - I want you to know there's a classic 
version of the service, it's still  
88:04 - accessible in the portal. This is not 
an exam, we are going to 100% avoid it.  
88:09 - It has severe limitations, we cannot transfer 
anything over from the closet to the new one.  
88:14 - So the one we're going to focus on is the Azure 
Machine Learning service. You do create studios  
88:18 - within it. So you'll hear me say Azure Machine 
Learning Studio and I'm referring to the new one,  
88:22 - a service that simplifies running AI ml work 
related workloads allowing you to build flexible  
88:26 - automated ml pipelines, use Python or R run 
deep learning workloads such as TensorFlow,  
88:32 - we can make Jupyter Notebooks in here. So 
build and document your machine learning  
88:35 - models as you build them, share and collaborate 
Azure Machine Learning SDK for Python. So an SDK  
88:40 - designed specifically to interact with the Azure 
Machine Learning Services. It does ml Ops, machine  
88:45 - learning operations, so end to end automation 
of ml model pipelines, CIC D training inference,  
88:51 - Azure Machine Learning designer. So this is a drag 
and drop interface to visually build test deploy  
88:56 - machine learning models, technically, pipelines, I 
guess, as a data labeling service, assemble a team  
89:02 - of humans to label your training data responsible 
in machine learning. So model fairness,  
89:07 - through disparity metrics, and mitigate unfairness 
at the time of the service is not very good,  
89:11 - but it's supposed to tie in with the responsible 
AI that Microsoft is always promoting. Okay.
89:21 - So once we launch our own studio with 
an Azure Machine Learning service,  
89:24 - you're gonna get this nice big bar, navigation 
left hand side, it shows you there's a lot of  
89:29 - stuff that's in here. So let's just break it down 
on what all these things are. So for authoring,  
89:32 - we got notebooks these are Jupyter, notebooks 
and ID to write Python code to build ml models.  
89:36 - They kind of have their own preview, which I don't 
really like. But there's a way to bridge it over  
89:40 - to Jupyter Notebooks or into Visual Studio 
code. We have auto ml completely automated  
89:44 - process to build and train ml models. So if you're 
limited to only three types of models, but still,  
89:49 - that's great. We have the designers of visual 
drag and drop designer to construct end to end  
89:53 - ml pipelines. For assets we have data sets of 
data that you can upload which we will be used  
89:58 - which will be used for training Experiments when 
you run a training job, they are detailed here,  
90:03 - pipelines, ml workflows, you have built or 
have used in the designer model. So a model  
90:09 - registry containing train models that can be 
deployed endpoints. So when you deploy a model,  
90:14 - it's hosted on accessible endpoint. So you're 
going to be able to access it via a REST API,  
90:19 - or maybe the SDK for managing got compute 
the underlying computing instances used  
90:24 - for notebooks, training and inference, 
environments, reproducible Python environment for  
90:30 - machine learning experiments, data stores a data 
repository where your data resides, data labeling,  
90:36 - so you have a human with ml assisted labeling 
to label your data for supervised learning,  
90:41 - Link services, external service, you can connect 
to the workspace such as Azure synapse analytics.  
90:50 - Let's take a look at the types of compute that is 
available in our Azure Machine Learning Studio,  
90:55 - we got four categories, we have compute 
instances, development workstations that  
91:00 - data scientists can use to work with data and 
models, compute clusters to scalable clusters of  
91:05 - VMs, for on demand processing, experimentation, 
code, deployment targets for predictive services  
91:10 - that use your trained models, and attach compute 
links to existing Azure compute resources such as  
91:16 - Azure VMs. And Azure Data brick clusters. Now, 
what's interesting here is like with this compute,  
91:23 - you can see that you can open it in Jupiter labs, 
Jupiter VS code, our studio and terminal. But  
91:29 - you can you can work with your computers as your 
development workstations directly in the studio,  
91:34 - which that's the way I do it. What's interesting 
is for inference, that's when you're want to make  
91:39 - a prediction, you use Azure Kubernetes service or 
Azure Container instance, I didn't see it show up  
91:44 - under here. So I'm kind of confused whether that's 
where it appears. Maybe we'll discover as we do  
91:49 - the follow logs that they do appear here, but 
I'm not sure about that one. But yeah, those are  
91:53 - the four there, okay. So within Azure Machine 
Learning Studio, we can do some data labeling,  
92:02 - so we create data labeling jobs to prepare your 
ground truth. For supervised learning, you have  
92:06 - two options human in the loop labeling, you have 
a team of humans that will apply labeling, these  
92:10 - are humans, you grant access to labeling, machine 
learning assists to deal with labeling, you will  
92:15 - use ml to perform labeling. So you can export the 
label data for machine learning, experimentation,  
92:21 - any time, your users often export multiple times 
and train different models. rather than wait for  
92:26 - all the images to be labeled. Images, labels 
can be exported in cocoa format. That's why  
92:31 - we talked about cocoa a lot earlier in our data 
set section as your machine learning data set.  
92:36 - And this is the data set format that makes it easy 
to use for training and Azure machine learning. So  
92:41 - generally, you want to use that format. The idea 
is you would choose a labeling Task Type. And that  
92:46 - way you would have this UI and then people go in 
and just click buttons and do the labeling. Okay.  
92:55 - So as your ml data store securely connects you 
to storage services on Azure without putting your  
93:01 - authentication credentials and the integrity 
of your original data source at risk. So here  
93:05 - is the example of data sources that are available 
to us in the studio. And let's just go quickly  
93:09 - through them. So we have Azure Blob Storage. This 
is data that is stored as objects distributed  
93:13 - across many machines, as your file share a 
mountable file share via SMB and NFS protocols  
93:18 - as your data lake storage Gen two, this blob 
searches for vast amounts of big data analytics,  
93:24 - as your SQL is a fully managed MS SQL relational 
database as your Postgres database, this is an  
93:29 - open source relational database, often considered 
an object related database preferred by developers  
93:35 - as your MySQL, another open 
source relational database,  
93:37 - the most popular one and considered 
a pure relational database, okay.
93:45 - So as your ml data sets makes it easy to register 
your datasets for use with your ml workloads. So  
93:51 - what you do is you'd add a data set and you 
get a bunch of metadata associated with it.  
93:56 - And you can also upload a dish like the data set 
again to have multiple versions. So you'll have  
94:00 - a current version and a latest version, it's 
very easy to get started working with them,  
94:05 - because we'll have some sample code that's for 
the Azure ML SDK to import that into, into your  
94:11 - Jupyter notebooks. For datasets, you can generate 
profiles that will give you summary statistics,  
94:16 - distribution of data and more, you will have to 
use a compute instance to generate that data.  
94:20 - So you'd press the Generate profile, and you'd 
have that stored I think it's in blob storage.  
94:25 - There are open data sets is they're publicly 
hosted data sets that are commonly used for  
94:28 - learning how to build ml models. So if you go 
to open data sets, you just choose one. And so  
94:34 - this is a curated list of open data sets 
that you can quickly add to your data store.  
94:37 - Great for learning how to use auto 
ml or Azure Machine Learning designer  
94:41 - or any kind of ml workload if you're 
new to it. That's why we covered  
94:46 - amnesty and cocoa earlier just because those are 
some common data sets there. But there you go.  
94:55 - Take a look here at Azure ML experiments. This is 
a logical grouping of Azure runs and runs Act is  
95:01 - the act of running ml tasks on a virtual machine 
or container. So here's a list of them. And it can  
95:06 - run various types of ml tasks. So scripts could 
be pre processing, auto ml, a training pipeline,  
95:12 - but what it's not gonna include is inference. And 
what I mean is once you've deployed your model or  
95:16 - pipeline, and you make predictions via request, 
it's just not going to show up under here. Okay?  
95:27 - Okay, so we have Azure ML pipelines, which is 
an executable workflow of a complete machine  
95:32 - learning task Not to be confused with Azure 
pipelines, which is part of Azure DevOps,  
95:36 - or Data Factory, which has its own pipelines, it's 
a total, totally separate thing here. So subtasks  
95:41 - are encapsulated as a series of steps within the 
pipeline. Independent steps allow multiple data  
95:46 - scientists to work on the same pipeline at the 
same time without overtaxing compute resources.  
95:51 - Separate steps also make it easy to use 
different compute type sizes for each step.  
95:55 - When you rerun a pipeline, the run jumps 
to the steps that need to be rerun,  
95:59 - such as the updated training script steps do not 
need to be rerun, and they will be skipped. After  
96:04 - a pipeline has been published, you can configure 
a REST endpoint, which allows you to rerun the  
96:09 - pipeline from any platform or stack. There's two 
ways to build pipelines, you can use the Azure  
96:14 - ML designer or pre bakley, using Azure Machine 
Learning Python SDK. So here's an example of some  
96:20 - code. Just make a note here, I mean, it's not 
that important. But notice as you create steps,  
96:26 - okay, and then you assemble all the 
steps into a pipeline here. Alright.
96:35 - So Azure Machine Learning designer lets you 
quickly build as your ml pipelines without  
96:39 - having to write any code. So here is what it looks 
like. And over there, you can see our pipeline  
96:44 - is quite visual. And on the left hand side, you 
have a bunch of assets you can drag out that are  
96:49 - pre built there. So it's a really fast way for 
building a pipeline. So you do have to have a good  
96:54 - understanding of ml pipelines end to end to make 
good use of it. Once you've trained your pipeline,  
96:59 - you can create an inference pipeline, so you drop 
down and you'd say whether you want it to be real  
97:04 - or batch, or you can toggle between them later. 
So I mean, there's a lot to this service. But  
97:10 - for the 100, we don't have 
to go diving too deep, okay.  
97:18 - So as your ml models are the model registry 
allows you to create, manage and track your  
97:22 - registered models as incremental versions under 
the same name. So each time you register a model  
97:27 - with the same name as an existing one, the 
registry assures that it's a new version.  
97:31 - Additionally, you can provide metadata tags 
and use tags when you search for models.  
97:36 - So yeah, it's just really easy way to share 
and deploy or download your models, okay?  
97:45 - As your MLM points allow you to deploy 
machine learning models as a web service.  
97:48 - So the workflow for deploying models, 
register the model, prepare an entry script,  
97:52 - prepare an inference configuration, deploy the 
model locally to ensure everything works, compute,  
97:57 - choose a compute, Target, redeploy the model to 
the cloud test the resulting web service. So we  
98:03 - have two options here real time endpoints endpoint 
that provides remote access to invoke the ML model  
98:09 - service running on either Azure Kubernetes 
service Eks, or Azure Container instances ACI,  
98:15 - then we have pipeline endpoint. So endpoint that 
provides remote access to invoke an ml pipeline,  
98:21 - you can parameterize the pipeline endpoint 
for manage repeatability in batch scoring  
98:24 - and retraining scenarios. And so you can deploy 
a model to an endpoint yet, it will either be  
98:31 - deployed to a Eks or ACI, as we said earlier, and 
the thing is, is that when you do do that, just  
98:37 - understand that that's going to be shown under 
the A Ks or ACI within the Azure portal. It's  
98:42 - not consolidated under the Azure Machine Learning 
Studio. When you've deployed a real time endpoint,  
98:47 - you can test the endpoint by sending either a 
single request or batch request. So they have  
98:50 - a nice form here with single or it's like here, 
it's a CSV that you can send. So there you go.  
99:00 - So Azure has a built in Jupiter like notebook 
editor, so you can build and train your ml models.  
99:06 - And so here is an example of it. I personally 
don't like it too much. But that's okay,  
99:10 - because we have some other options. To make it 
easier. All you do is you choose your compute  
99:14 - instance, to run the notebook, you'll choose your 
kernel, which is a pre loaded programming language  
99:19 - and programming libraries for different use 
cases. But that's a Jupiter kernel concept there.  
99:25 - So you can open the notebook at a 
more familiar ID such as VS code,  
99:28 - Jupyter, notebook classic or Jupiter lab. So you 
go there, drop it down, choose it and open it  
99:33 - up. And now you're in a more familiar territory. 
The VS code one is exactly the same experience as  
99:39 - the one in Azure or Azure ML studio. I 
personally don't like it. I think most  
99:44 - people are going to be using the notebooks but 
it's great that they have all those options.
99:53 - So Azure automated machine learning, also 
known as auto ml automates the process of  
99:57 - creating an ml model. So with Azure auto, ml 
you supply a data set, choose a test type,  
100:02 - then auto ml will train and tune your model. So 
here are test types, let's quickly go through  
100:06 - them. So we have classification, when you need 
to make a prediction based on several classes, so  
100:10 - binary classification, multi class classification 
regression, when you need to predict a continuous  
100:16 - number value, and then time series forecasting 
when you need to predict the value based on time.  
100:21 - So just look at them a little bit more in detail. 
So classification is a type of supervised learning  
100:25 - in which the model learns using training data and 
apply those learnings to new data. So here is an  
100:31 - example. Or this is just the option here. And so 
the goal of classification is to predict which  
100:36 - categories new data will fall into based on 
learning from its training data. So binary  
100:40 - classification is a record is labeled out of two 
possible labels. So maybe it's true or false, zero  
100:46 - or one, just two values. multiclass classification 
is a record is labeled out of range of out of a  
100:52 - range of labels. And so can be like happy, sad, 
mad or rad. And just, you know, I can see there's  
100:57 - a spelling mistake there. But yeah, there should 
be an F. So let's just correct that. There we go.  
101:03 - You can also apply deep learning and so if you 
turned deep learning on you probably want to use  
101:07 - a GPU compute instance, just because or compute 
cluster because deep learning really prefers  
101:15 - GPUs. Okay. Looking at regression, it's also a 
type of supervised learning where the model learns  
101:21 - using training data and applies those learnings 
to new data, but it's a bit different, where the  
101:25 - goal of aggression is to predict a variable in the 
future, then you have time series forecasting and  
101:30 - this sounds a lot like regression because it is, 
so forecast revenue inventory sales or customer  
101:38 - demand, an automated time series experiment that 
is treated as a multivariate regression problem,  
101:44 - past time series values are pivoted to become 
additional dimensions for the regressor together  
101:48 - with other predictors, unlike classical time 
series methods has an advantage of naturally  
101:54 - incorporating multiple contextual variables and 
their relationship to one another during training.  
102:00 - So use cases here or dance configurations, I 
should say, holiday detection and future position  
102:05 - time series, deep learning neural networks. 
So you got auto ri ma profit forecast TCN.  
102:13 - Many models supports through 
grouping, rolling origin,  
102:16 - cross validation, configurable labs rolling 
window aggregate features, so there you go.  
102:26 - So within auto ml, we have data 
guardrails, and these are run by  
102:30 - auto ml when automatic feature rotation is 
enabled, it's a sequence of checks to ensure  
102:35 - high quality input data is being used to 
train the model. So just to show you some  
102:39 - information here. So the idea is that could 
apply validation split handling so the input  
102:44 - data has been split for validation to improve 
the performance, then you have missing feature  
102:49 - value imputation so no features missing 
values were detected in training data,  
102:55 - high cardinality feature detection, your inputs 
were analyzed, and no high cardinality features  
102:59 - were detected. High cardinality means 
like if you have too many dimensions,  
103:02 - it becomes very dense or hard to process the 
data. So that's something good to check against.  
103:12 - Let's talk about auto ml is automatic feature 
isolation. So during model training with auto ml,  
103:18 - one of the following scaling or normalization 
techniques will be applied to each model. The  
103:22 - first is standard scale rapper standardized 
features by removing the mean and scaling to  
103:27 - unit variants. min max scalar transform features 
by scaling each feature by the columns minimum  
103:32 - maximum max ABS scalar scale each feature by its 
maximum absolute value, robust scale our scales  
103:38 - features by the quantitative quantal range, a 
PCA linear dimensionality reduction using single  
103:44 - value decomposition of the data to project it 
to lower dimensional space. dimension reduction  
103:52 - is very useful if your data is too complex. And 
let's say you have data you have too many labels  
103:57 - like 20 3040 labels for like four categories to 
pick out of you want to reduce the dimensions  
104:03 - so that your machine learning model is not 
overwhelmed. So then you have truncated SVD  
104:08 - wrappers. So the transformer performs linear 
dimensionality reduction by means of truncated  
104:13 - single singular value decomposition contrary to 
PCA, the estimator does not send her the data  
104:18 - before computing the singular value 
decomposition, which means it can work with  
104:22 - spicy sparse matrices, efficiently sparse 
normalization to each sample that is each  
104:28 - row of the data matrix which with at least 
one zero component is rescaled independently  
104:33 - of other samples, that is norm. So one 
l or two l two, I can refer to it or L.
104:40 - Anyway, I one and, and I two. Okay? So the thing 
is, is that on the exam, they're probably not  
104:48 - going to be asking these questions but I just 
like to get you exposure, but I just want to  
104:51 - show you that auto ml is doing all this. This 
is like pre processing stuff, you know, like  
104:56 - this is stuff that you'd have to do, and so it's 
just taking care of the stuff for Are you okay?  
105:04 - So within Azure auto ml, they have a feature 
called model selection. And this is the task  
105:09 - of selecting a statistical model from a set of 
candidate models. And Azure auto ml will use  
105:14 - different, or many different ml algorithms that 
will recommend the best performing candidates. So  
105:19 - here's a list. And I want to just point out, down 
below, there's three pages, there's 53 models,  
105:25 - that's a lot of models. And so you can see that 
the one I chose is its top candidate was called  
105:30 - voting ensemble, that's an ensemble algorithm, 
that's where you take two weak ml models,  
105:36 - combine them together to make a more stronger 
one. And notice here, it will show us the results.  
105:42 - And this is what we're looking for, which is the 
primary metric, the highest value should indicate  
105:46 - that that's the model we should want to use, you 
can get an explanation of the model called that's  
105:52 - known as explainability. And now if 
you're a data scientist, you might  
105:56 - be a bit smarter and say, Well, I know this one 
should be better. So I'll use this and tweak it.  
106:00 - But you know, if you don't know you're 
doing just go with the top line, okay.  
106:08 - So we just saw that we had a top candidate model,  
106:11 - and there could be an explanation to understand 
as to the effectiveness of this, this is called  
106:15 - MX L. So machine learning explainability This 
is the process of explaining interpreting  
106:20 - ml or deep learning models, MX, m, l x, can help 
machine learning developers to better understand  
106:27 - interpret models behavior. So after your top 
candidate models selected by Azure auto ml,  
106:32 - you can get an explanation of internals 
of various factors. So model performance  
106:36 - data set, explore aggregate feature importance, 
individual feature importance. So I mean, yeah,  
106:42 - this is aggregate. So what it's looking at, and 
it's actually cut off here, but it's saying that  
106:45 - these are the most important ones that affect 
how the the models outcome. So I think this is  
106:51 - the diabetes data data set. So BMI would be one, 
that would be a huge influence there, there, okay.  
107:02 - So the primary metric is a parameter that 
determines the metric to be used during  
107:06 - the mall training for optimization. So we for 
classification, we have a few and regression  
107:10 - and time series, we have a few. But you'll have 
these task types. And underneath, you'll choose  
107:14 - the additional configuration. And that's where 
you can override the primary metric, it might  
107:18 - just auto detect it for you. So you don't have to 
because it might sample some of your data set to  
107:22 - just kind of guess. But you might have to override 
it yourself. Just going through some scenarios.  
107:27 - And we'll break it down into two categories. So 
here we have suited for larger datasets that are  
107:31 - well balanced. well balanced means that your data 
set like is evenly distributed. So if you have  
107:37 - classifications for A and B, let's say you have 
100, and 100, they're well balanced, right,  
107:42 - you don't have one data set much a subset 
of your data set much larger than the other  
107:46 - that's labeled. So for accuracy, this is 
great for image classification, sentiment  
107:50 - analysis term prediction, for average precision 
score weighted is for sentiment analysis, nor  
107:55 - macro recall term prediction for precision score 
weighted, uncertain as to what that would be good  
107:59 - for maybe sentiment analysis suited for smaller 
data sets that are imbalanced. So that's where  
108:04 - your data set like you might have like 10 records 
for one and 500 for the other on the label.  
108:09 - So you have AUC weighted fraud detection, image 
classification, anomaly detection, spam detection,  
108:15 - on to regression scenarios, we'll break it down 
into ranges. So when you have a very wide range,  
108:21 - Spearman correlation works really well are 
to score. This is great for airline delay  
108:26 - salary estimation, but resolution time, 
when you're looking at smaller ranges,  
108:31 - we're talking about normalized root square 
mean to error. So price predictions,  
108:35 - review tips, score predictions, 
for normalized mean absolute error,  
108:39 - it's going to be just another one here, they 
don't give a description for time series,  
108:43 - it's the same thing. It's just in the context 
of time series of forecasting. Alright.
108:54 - Another option we can change is the validation 
type when we're setting up our ML model. So  
108:57 - validation, model validation is when we compare 
the results of our training data set to our test  
109:02 - data set model validation occurs after we train 
the model. And so you can just drop it down  
109:06 - there, and we have some options. So auto k fold 
cross validation, Monte Carlo cross validation,  
109:11 - train validation split, I'm not going to really 
get into the details of that. I don't think it'll  
109:16 - show up on the AI 900 exam. But I just want you to 
be aware of that you do have those options, okay.  
109:25 - Hey, this is Andrew Brown from exam Pro. And 
we're taking a look here at custom vision.  
109:29 - And this is a fully managed no code service 
to quickly build your own classification,  
109:34 - and object detection ml models. The service is 
hosted on its own isolate domain at www custom  
109:40 - vision.ai. So the first idea is you upload your 
images of bring your own labelled images or custom  
109:45 - vision to quickly add tags to any unlabeled data 
images. You use the labeled images to teach custom  
109:52 - vision, the concepts you care about, which is 
training, and you use a simple REST API that calls  
109:58 - to quickly tag images. With your new custom 
computer vision model so you can evaluate, okay.  
110:08 - So when we launch custom vision, we have to 
create a project. And with that, we need to choose  
110:13 - a project type. And we have classification and 
object detection. Reviewing classification, here,  
110:19 - you have the option between multi labels. So 
when you want to apply many tags to an image,  
110:23 - so think of an image that contains both a cat and 
a dog, you have multi class, so when you only have  
110:28 - one possible tag to apply to an image, so it's 
either an apple, banana, and orange, it's not  
110:34 - multiples of these things. 
You have object detection,  
110:36 - this is when we want to detect various objects 
in an image. And you also need to choose a  
110:41 - domain a domain is a Microsoft managed data 
set that is used for training the ML model.  
110:45 - There are different domains that are suited 
for different use cases. So let's go take a  
110:49 - look first at image classification domains. So 
here is the big list of domains being over here.  
110:55 - Okay, and we'll go through these here. So 
general is optimized for a broad range of  
111:00 - image classification tasks. If none of the none 
of the other specified domains are appropriate,  
111:05 - or you're unsure of which domain to choose Select 
one of the general domains so G, or a one is  
111:11 - optimized for better accuracy with comparable 
inference time as general domain recommended for  
111:15 - larger datasets or more difficult user scenarios. 
This domain requires more training time,  
111:21 - then you have a to optimize for better accuracy 
with faster adverts times than a one and general  
111:27 - domains recommended for more most datasets this 
domain requires less training time, then general  
111:33 - and a one, you have food optimized for photographs 
or dishes as you would see them on a restaurant  
111:38 - menu. If you want to classify photographs of 
individual fruits or vegetables use food domains.  
111:44 - So that we have optimized for recognizable 
landmarks both natural and artificial. This domain  
111:49 - works best when landmark is clearly visible in 
the photograph, this domain works even if the lend  
111:54 - mark is slightly obstructed by people in front of 
it. Then you have retail so optimized for images  
112:02 - that are found in a shopping cart or shopping 
website. If you want a high precision classifying  
112:07 - classified in between dresses, pants shirts 
uses domain contact domains optimized for the  
112:12 - constraints of real time classification on the 
edge. Okay, then we have object detection domain,  
112:20 - so this one's a lot shorter, so I'll get 
through a lot quicker. So optimize for  
112:23 - a broad range of object detection tasks if 
none of the other domains are appropriate,  
112:28 - or you're unsure of which domain choose the 
general one a one optimize for better accuracy and  
112:32 - comparable inference time than the general domain 
recommended for most accurate region. locations,  
112:37 - larger data sets or more difficult use case 
scenarios, the domain requires more training  
112:41 - results are not deterministic expect plus minus 
1% mean average precision difference with the same  
112:48 - training data provided you have logo optimized for 
finding brands, logos and images products on shelf  
112:55 - so optimized for detecting and classifying 
products on the shelf. So there you go.
113:04 - Okay, so let's get some more practical knowledge 
of the service. So for image classification,  
113:08 - you're gonna upload multiple images and apply 
single or multiple labels to the entire image. So  
113:14 - here I have a bunch of images uploaded. And then 
I have my tags over here. And they could either be  
113:18 - multi or singular. For object detection, you apply 
tags to objects in an image for data labeling,  
113:23 - and you hover your cursor over the image custom 
vision uses ml to show bounding bounding boxes  
113:28 - of possible objects that are not yet been labeled. 
If it does not detect it, you can also just click  
113:33 - and drag to draw out whatever square you want. 
So here's one where I tagged it up quite a bit,  
113:38 - you have to have at least 50 images on every 
tag to train. So just be aware of that when you  
113:44 - are tagging your images. When you're training, 
your model is ready when you and you have two  
113:48 - options. So you have quick training that's trained 
quickly, but it will be less accurate, you have  
113:52 - advanced training, this increases compute time to 
improve your results. So for advanced training,  
113:57 - basically, you just have this thing that you move 
to the right. With each iteration of training,  
114:02 - our ML model will improve the evaluation metrics. 
So precision recall, it's going to vary. We're  
114:06 - going to talk about the metrics here in a moment, 
but the probability threshold value determines  
114:10 - when to stop training, when our evaluation metric 
meets our desired thresholds. So these are just  
114:15 - additional options where when you're training, 
you can move this left to right, and these left  
114:20 - to right, okay. And then when we get a results 
back, we're gonna get some metrics here. So  
114:27 - we have evaluation master. So we have precision 
being exact, inaccurate, selects items that are  
114:31 - relevant, recalls that sensitivity or known 
as true positive rate, how many relevant items  
114:37 - returned average precision, it's important 
that you remember these because they might  
114:41 - ask you that on the exam. So for cut when we're 
looking at object detection, and we're looking  
114:47 - at the evaluation metric outcomes for this one, we 
have precision recall and mean average precision.  
114:53 - Once we have deployed our pipeline, it makes sense 
that we go ahead and give it a quick test to make  
114:57 - sure it's working correctly to press Click Click 
test button and you can upload your image and it  
115:02 - will tell you so this one says it's worth, when 
you're ready to publish, you just hit the publish  
115:08 - button. And then you'll get some prediction 
URL and information so you can invoke it.  
115:14 - One other feature that's kind of useful 
is the smart labelers. So once you've  
115:18 - loaded some training data within a canal, 
make suggestions, right. So you can't do this  
115:23 - right away. But once it has some data, it's like, 
it's like kind of a prediction that is not 100%,  
115:28 - guaranteed, right, and it just helps you build 
up your training data set a lot faster. Very  
115:33 - useful. If you have a very large data set, 
this is known as ml assisted labeling, okay.  
115:43 - Hey, this is Andrew Brown from exam Pro. And in 
this follow along, we're gonna set up a studio  
115:48 - with an Azure Machine Learning service, so that 
it will be the basis for all the fall logs here.  
115:53 - So what I want you to do is go all the way the 
top here, and type in Azure machine learning.  
115:58 - And you're looking for this one that looks like 
a science bottle here. And we'll go ahead and  
116:03 - create ourselves our Machine Learning Studio. And 
so I'll create a new one here, and I'll just say,  
116:11 - my studio. I will hit OK. And we'll name the 
workspace. So I will say my work workplace
116:26 - will maybe say and I'll 
workplace here. For containers,  
116:30 - there are nodes that we will create all that 
stuff for us. I'll hit create and create.  
116:39 - And so what we're going to do here 
is just wait for that creation, okay?  
116:44 - Alright, so after a short little wait there, it 
looks like our studio set up. So we'll go to that  
116:47 - resource launch the studio and where are now in. 
So there's a lot of stuff in here. But generally,  
116:53 - the first thing you'll ever want to do is get 
yourself a notebook going. So in the top left  
116:57 - corner, I'm going to go to notebooks. And what 
we'll need to do is load some files in here. Now  
117:02 - they do have some Sample Files, like how to use 
Azure ML. So if we just quickly go through here,  
117:10 - you know, maybe we'll want to look at something 
like Ms NIST here. And we'll go ahead and open  
117:15 - this one. And we'll just go ahead and clone 
this. And we'll just clone it over here.  
117:25 - Okay, and the idea is that we want to get 
this notebook running. And so notebooks have  
117:29 - to be backed by some kind of compute. So up 
here, it says, No compute found, and etc. So  
117:35 - what we can do here, I'm just gonna go back to my 
files, oh, it went back there for me. But what I'm  
117:39 - going to do is go all the way down. Actually, I'll 
just expand this up here makes it a bit easier,  
117:43 - close this tab out. But what we'll do is go 
down to compute. And here we have our four  
117:48 - types of compute to compute instances 
is when we're running notebooks,  
117:51 - compute clusters, is when we're doing 
training inference clusters is when we have  
117:56 - a inference pipeline. And then attached computers 
bringing things like hdn sites or data bricks into  
118:03 - here, but for compute instances is what we need, 
we'll get ahead and go new, you'll notice they  
118:07 - have the option between CPU and GPU. GPU is much 
more expensive. So it's like 90 cents per hour.  
118:13 - For a notebook, we do not need anything super 
powerful. Notice, it'll say here, development  
118:17 - on notebooks, IDs, lightweight testing, here, it's 
as classical ml model training, auto ml pipelines,  
118:23 - etc. So I want to make this a bit cheaper 
for us here. Because we're going to be using  
118:29 - the notebook to run cognitive services and those 
costs next to nothing like they don't take much  
118:34 - compute power. And for some other ones, we 
might do something a bit larger. For this,  
118:38 - this is good enough. So I'll go ahead and 
hit next. I'm just gonna say my notebook  
118:43 - instance here. We'll go ahead and hit Create. 
And so we're just gonna have to wait for that  
118:49 - to finish creating and running and when 
it is, I'll see you back here in a moment.  
118:53 - Alright, so after a short little wait there, it 
looks like our server is running. And you can even  
118:57 - see here it shows you you can launch in Jupiter 
labs, Jupiter VS code, our studio or the terminal.  
119:03 - But what I'm going to do is go back all the way 
to our notebooks just so we have some consistency  
119:07 - here, I want you to notice that it's now running 
on this compute. If it's not, you can go ahead and  
119:11 - select it. And it also loaded in Python 3.6, there 
is 3.8. Right now, it's not a big deal which one  
119:18 - you use. But that is the kernel, like how it will 
run this stuff. Now, this is all interesting. But  
119:23 - I don't want to run this right now what I want to 
do is get those cognitive services into here. So  
119:30 - what we can do is just go up here and we'll choose 
editors and edit in Jupiter lab. What that should  
119:38 - do is open up a new tab here is it opening. If 
it's not opening, what we can do is go to compute.  
119:46 - Sometimes it's a bit more responsive. If we just 
click there, it's the same way of getting to it.  
119:50 - I don't know why, but just sometimes that link 
doesn't work when you're in the notebook. And what  
119:54 - we can do is while we're in here now we can see 
that this is where this is An example project is  
120:01 - okay. But what we want to do is get 
those cognitive services in here. So  
120:07 - I don't know if I showed it to you yet, but 
I have a repository, I just gotta go find  
120:11 - it. It's somewhere on my screen. Here it is. 
Okay, so I have a repo called the free az, AZ  
120:18 - night free, AZ I should be ai 900, I think I'll 
go ahead and change that, or that is going to get  
120:25 - confusing. Okay, so what I want you to 
do here is, we'll get this loaded in. So  
120:32 - this is a public directory, I'm just thinking, 
there's a couple ways we can do it, we can go  
120:36 - and I use the terminal to grab it, what I'm going 
to do is I'm just going to go download the zip.  
120:44 - And this is just one of the easiest ways to 
install it, and we need to place it somewhere.  
120:49 - So here are my downloads. And I'm just going 
to drag it out here. Okay. And what we'll do  
120:57 - is upload that there. So I can't remember if it 
lets you upload entire folders, we'll give it a  
121:01 - go see if it lets us maybe rename this to the 
free AZ or ai 900 there, we'll say open. Yeah,  
121:10 - so it's individual file. So it's not that big of a 
deal, but we can go ahead and select it like that.  
121:17 - And maybe we'll just take him to the folder 
and here we'll say this cognitive services.
121:25 - Okay. And what we'll do here is keep on 
uploading some stuff. So we have assets.
121:40 - So I have a couple loose files there. And 
I know we have crew groups will have crew.  
121:51 - Oops. Sometimes it's not as responsive. We want 
OCR, I believe we have on called movie reviews.  
122:03 - So we'll go into OCR here and upload the files 
that we have. So we have a few files there.  
122:12 - And we'll go back a directory here. And I 
know movie reviews are just static files.
122:23 - And we have an objects folder. So 
we will go back here to objects.  
122:39 - And then we'll go back and to crew 
and we need a folder called Wharf
122:44 - a folder called Crusher, a folder called data. 
And so for each of these, we have some images.  
122:55 - Think Ron Wharf, right? Yeah, we are okay, 
great. So we will quickly upload all these  
123:04 - I will technically we don't really 
need to upload any of these walls,  
123:07 - these images we don't but I'm going to 
put them here anyway, I just remembered  
123:10 - that these we just upload directly to the 
service. But because I'm already doing it,  
123:15 - I'm just gonna put them here, even though 
we're not going to do anything with them.  
123:25 - All right. And so now we are all set up to do 
some cognitive services. So I'll see you the  
123:31 - next video. Alright, so now that we have our work 
environment set up, what we can do is go ahead  
123:37 - and get Cognitive Services hooked up, because we 
need that service in order to interact with it.  
123:42 - Because if we open up any of these, you're gonna 
notice we have a cognitive key endpoint that we're  
123:47 - going to need. So what I want you to do is go 
back to your Azure Portal. And at the top here,  
123:53 - we'll type in cognitive services. Now the thing 
is, is that all these services are individualized,  
123:59 - but at some point, they did group them together, 
and you're able to use them through a unified key  
124:04 - and API endpoint. That's what this is. And that's 
the way we're going to do it. So let's say add,  
124:10 - and it brought us to the marketplace. 
So I'm just going to type in cognitive  
124:17 - services. And then just click this 
one here. And we'll hit Create.  
124:26 - And we'll make a new one here. I'm gonna call 
my cogs services, say, Okay, I prefer to be  
124:34 - in US East, I believe in us West, it's fine. 
And so in here, we'll just say my cog services.  
124:42 - And if it doesn't like that, I'll 
just put some numbers in. There we go.  
124:46 - We'll do standard so we will be charged something 
for that. Let's go take a look at the pricing.  
124:54 - So you can see that the pricing is quite 
variable here, but it's like you'd have to  
124:58 - do 1000 transactions. Before you are billed, 
so I think we're going to be okay for billing.  
125:04 - We'll check boxes here, we'll go down below, 
it's telling us about responsible AI. Notice,  
125:09 - sometimes services will actually have you checkbox 
it. But in this case, it just tells us there.  
125:17 - And we'll go ahead and hit Create.  
125:25 - And I don't believe this took very long, so 
we'll give it a second here. Yep, it's all  
125:29 - deployed. So we'll go to this resource here. And 
what we're looking for are keys and endpoints.  
125:36 - And so we have two keys and two endpoints, we 
only need a single key. So I'm going to copy this  
125:40 - endpoint over, we're gonna go over to Jupiter 
lab, and I'm just going to paste this in here.  
125:45 - I'm just gonna put it in all the ones 
that need it. So this one needs one.  
125:50 - This one needs one. This one needs one. And this 
one needs one. And we will show the key here,  
126:01 - I guess doesn't show but it copies. Of course, 
I will end up deleting my key before you ever  
126:05 - see it. But this is something you don't want to 
share publicly. And usually, you don't want to  
126:10 - embed keys directly into a notebook. But this is 
the only way to do it. So this is how it is with  
126:15 - Azure. So yeah, all our keys are installed. Going 
back to the cognitive services, nothing super  
126:22 - exciting here. But it does tell us what services 
work with it. You'll see there's an asterisk  
126:26 - beside custom vision, because we're gonna access 
that through another app. But yeah, cognitive  
126:31 - services all set up. And so that means we are 
ready to start doing some of these labs. Okay.
126:41 - All right. So let's take a look here 
at computer vision first. And computer  
126:44 - vision is actually used for a variety 
of different services. As you will see,  
126:48 - it's kind of an umbrella for a lot of different 
things. But the one in particular that we're  
126:52 - looking at here is describe image in stream. 
If we go over here to the documentation,  
126:57 - this operation generates description of 
image in a human readable language. And with  
127:01 - complete sentences, the description is 
based on a collection of content tags,  
127:04 - which also returned by the operation. Okay, so 
let's go see what that looks like in action.  
127:09 - So the first thing is, is that we need to install 
this Azure Cognitive Services vision computer  
127:14 - vision. Now we do have a kernel and these aren't 
installed by default, they're not part of the  
127:21 - machine learning the Azure Machine Learning SDK 
for Python, I believe that's pre installed. But  
127:28 - these AI services are not. So what we'll do 
is go ahead and run it this way. And you'll  
127:32 - notice where it says pip install, that's how 
it knows to install. And once that is done,  
127:36 - we'll go run our requirements here. So we have 
the OS, which is for usually handling up like OS  
127:43 - layer stuff, we have met matplotlib, which is to 
visually plot things, and we're gonna use that to  
127:49 - show images and draw borders, we need to handle 
images. I'm not sure if we're using NumPy here,  
127:55 - but I have NumPy loaded. And then here we have the 
Azure Cognitive Services vision, computer vision,  
128:00 - we're going to load the client. And then we have 
the credentials. And these are generic credentials  
128:05 - for the cognitive services credentials. It's 
commonly used for most of the services and  
128:09 - some exceptions, they the API's do not support 
them yet, but I imagine they will in the future.  
128:14 - So just notice that when we run something, it will 
show a number. If there's an asterisk, it means it  
128:17 - hasn't ran yet. So I'll go ahead and hit play up 
here. So it was an Astra can we get her to, and  
128:22 - we'll go ahead and hit play again. And now those 
are loaded in and so we'll go ahead and hit play.  
128:29 - Okay, so here we have just packaged our 
credentials together. So we passed our key  
128:33 - into here, and then will now load in the client, 
install, pass our endpoint and our key. Okay,  
128:40 - so hit play. So now we we just want to load 
our image. So here we're loading assets  
128:46 - data dot jpg, just make sure that that is 
there. So we have assets, and there it is.  
128:50 - And we're going to load it as a stream 
because you have to pass streams along.  
128:53 - So hit play. You'll see that it now ran. And 
so now we'll go ahead and make that call.  
129:01 - Okay, great. And so we're getting some data back. 
And notice we have some properties person while  
129:05 - indoor man pointing captions. It's 
not showing all the information,  
129:09 - sometimes you have to extract it out. But we'll 
take a look here. So this is a way of showing  
129:14 - matplotlib in line. I don't think we have to 
run it here, but I have it in here anyway.  
129:18 - And so what it's going to do is it's going to 
show us the image, right? So it's going to print  
129:24 - us the image, and it's going to grab whatever 
caption is returns to see how there's captions.  
129:29 - So we're going to iterate through the captions. 
That's going to give us a confidence score saying  
129:34 - it thinks it's this so let's 
see what it comes up with.  
129:38 - Okay, and so here it says brand spider spider 
looking at a camera. So that is the actor who  
129:43 - plays data on Star Trek as a confidence score. 
57 point 45% even though it's 100% correct,  
129:50 - they probably don't know contextual things 
like in the sense of like pop culture,  
129:54 - like they don't know probably search for 
characters, but they're gonna be able to identify  
129:58 - celebrities because it's in their database. 
So that is the first introduction to computer,  
130:05 - computer vision there. But the key things you want 
to remember here is that we use this describe an  
130:08 - image stream. And that we get this confidence 
score and we get this contextual information.  
130:14 - Okay. And so that's the first I'll 
move on to maybe custom vision next.
130:23 - Alright, so let's take a look at custom vision. 
So we can do some classification and object  
130:28 - detection. So the thing is, is that it's possible, 
it's possible to launch custom vision through the  
130:36 - Marketplace. So if we go, we're not going to 
do it this way. If you type in custom vision,  
130:40 - it never shows up here. But if you go to the 
marketplace here, and type in custom vision,  
130:46 - and you go here, you can create it this way. But 
the way I like to do it, I think it's a lot easier  
130:51 - to do is we'll go up the top here and type in 
custom vision.ai. And you'll come to this website.  
130:56 - And what you'll do is go ahead and sign in, 
it's going to connect to your Azure account.  
131:00 - And once you're in, you can go ahead here and 
create a new project. So the first one here  
131:03 - is I'm just gonna call this the Star Trek crew. 
We're gonna use this to identify different Star  
131:08 - Trek members, we'll go down here, and we haven't 
yet created a resource. So we'll go create new,  
131:15 - my custom vision resource. We'll drop this 
down, we'll put this in our cog services,  
131:23 - we'll go stick with us West as much as we 
can. Here, we have fo and so fo is blocked  
131:30 - out for me to just choose. So I think 
fo is the free tier, but I don't get it.  
131:36 - And once we're back here, we'll go down below and 
choose our standard. And we're going to have a lot  
131:41 - of options here. So we have between classification 
and object detection. So classification,  
131:46 - is when you have an image and you just want to 
say, what, what is this image, right. And so we  
131:51 - have two modes where we can say, let's apply 
multiple labels. So let's say there were two  
131:55 - people in the photo or whether there was a dog and 
cat. And I think this example is a dog and a cat.  
132:00 - Or you just have a single class where it's like, 
what is the one thing that is in this photo, it  
132:05 - can only be of one of the particular categories. 
This is the one we're going to do multiclass,  
132:09 - we have a bunch of different domains here. 
And if you want to, you can go ahead and read  
132:13 - about all the different domains and their best 
use case, but we're going to stick with a two  
132:18 - that is optimized for it. So that's faster, 
right. And that's really good for our demo.  
132:22 - So we're going to choose general a two, I'm 
going to go ahead and create this project.  
132:27 - And so now what we need to do is start labeling 
our arc our content. So what we'll do is I just  
132:34 - want to go ahead and create the tags ahead 
of time. So we'll say Wharf will have data.  
132:41 - And we'll have Crusher. And now what we'll do 
is we'll go ahead and upload as images. So,  
132:47 - you know, we upload in the Jupyter Notebook, but 
it was totally not necessary. So here is data,  
132:52 - because we're going to do it all through here. And 
we'll just apply the data tag to them all at once,  
132:55 - which saves us a lot of time, I love that will 
upload now Worf. And I don't want to upload them  
133:03 - all I have this one quick test image we're going 
to use to make sure that this works correctly.  
133:08 - And I'm going to choose Worf. And 
then we'll go ahead and add Beverly.  
133:17 - There she is. Beverly Crusher. Okay, so we 
have all of our images. And I don't know how  
133:24 - this one got in here, but it's under Worf, it 
works out totally fine. So what I want to do  
133:31 - is go ahead and train this model, because they're 
all labeled. So we have a ground truth. And we'll  
133:37 - let it go ahead and train. So we'll go and press 
train. And we have two options, quick training,  
133:41 - advanced training, advanced training, where 
we can increase the time for better accuracy.  
133:45 - But honestly, we just want to do quick training. 
So I'll go ahead and do quick training. And it's  
133:50 - going to start it's iterative process. Notice on 
the left hand side, we have probability threshold,  
133:55 - the minimum probability score for a prediction to 
be valid when calculating calculating precision,  
134:00 - and recall. So we, the thing is, is that if 
it doesn't at least meet that requirements,  
134:05 - it will quit out. And if it gets above that, that 
it might quit out early, just because it's good  
134:11 - enough. Okay. So training doesn't take too long, 
it might take five to 10 minutes, I can't remember  
134:15 - how long it takes. But what I'll do is I'll see 
you back here in a moment, okay. All right. So  
134:20 - after waiting a short little while here, it looks 
like our results are done, we get 100% matches. So  
134:26 - these are our evaluation metrics to say whether 
the model was achieved its actual goal or not.  
134:33 - So we have precision recall. And I believe this 
is average precision. And so it says that it did  
134:39 - a really good job. So that means that it should 
have no problem matching up an image. So in the  
134:44 - top right corner, we have this button that is 
called Quick tests. And this is going to give  
134:47 - us the opportunity to quickly test these. So 
what we'll do is browse our files locally here.  
134:54 - And actually I'm going to go to Yeah, we'll 
go here and we have war. And so I have this  
135:01 - quick image here, we'll test that we'll 
see if it actually matches up to be worth.
135:05 - And it says 98.7%. Worse, that's pretty 
good. I also have some additional images  
135:10 - here I just put into the repo to test 
against, and we'll see what it matches  
135:14 - up. Because I thought it'd be interesting to 
do something that is not necessarily them,  
135:18 - but it's something pretty close to, you know, 
it's pretty close to what those are. Okay.  
135:23 - So we'll go to crew here, and First we'll try you. 
Okay, and who is the Borg, so he's kind of like an  
135:31 - Android. And so we can see he mostly matches to 
data. So that's pretty good. We'll give another  
135:36 - one go. martock is a click on so he should be 
matched up to Worf. Very strong match to work.  
135:41 - That's pretty good. And then polaski. She is a 
doctor and female, so she should get matched up to  
135:47 - Beverly Crusher. And she does. So this works out 
pretty darn well. And I hadn't even tried that. So  
135:53 - it's pretty exciting. So now let's say we want to 
go ahead and well, if we want to make predictions,  
136:00 - we could do them in bulk here. I believe 
that you could do them in bulk. But anyway.  
136:08 - Yeah, I guess I always thought this was like, I 
could have swore, yeah, if we didn't have these  
136:11 - images before, I think that it actually has an 
upload option, it's probably just a quick test.  
136:15 - So I'm a bit confused there. But 
anyway, so now that this is ready,  
136:19 - what we can do is go ahead and publish it so that 
it is publicly accessible. So we'll just say here,  
136:24 - crew model. Okay, and we'll drop that 
down, say publish. And once it's published,  
136:34 - now we have this public URL. So this is an 
endpoint that we can go hit programmatically.  
136:39 - I'm not going to do that. I mean, we could use 
postman to do that. But my point is, is that we've  
136:44 - basically figured it out for classification. 
So now that we've done classification,  
136:49 - let's go back here to the division here. And let's 
now let's go ahead and do object detection. Okay.  
137:02 - Alright, so we're still in custom vision, let's 
go ahead and try out object detection. So object  
137:05 - detection is when you can identify particular 
items in a scene. And so this one is going to  
137:11 - be combat just we're going to call it because 
we're going to try to detect combat, we have  
137:15 - more domains, here, we're gonna stick with a 
general a one. And we'll go ahead and create this  
137:20 - project here. And so what we need to do is add a 
bunch of images, I'm going to go ahead and create  
137:26 - our tag, which is going to be called combat, you 
can look for multiple different kinds of labels,  
137:32 - but then you need a lot of images. So we're 
just gonna keep it simple and have that there,  
137:36 - I'm going to go ahead and add some 
images. And we're going to go back  
137:40 - a couple steps here, into our objects. And here 
I have a bunch of photos, and we need exactly  
137:44 - 15 to train. So we got 1-234-567-8910 1112 1314 
1516. And so I threw an additional image in here,  
137:54 - this is the batch test. So we'll leave that out. 
And we'll see if that picks up really well. And,  
138:00 - yeah, we got them all here. And so we'll go ahead 
and upload those. And we'll hit upload files.  
138:07 - Okay. And we'll say done, and we can 
now begin to labels. We'll click into  
138:13 - here and what I want to do if you hover 
over it should start detecting things.  
138:17 - If it doesn't, you can click and drag bolt, 
click this one. They're all con badges,  
138:20 - so we're not going to tag anything else 
here. Okay. So go here, hover over is it  
138:24 - gonna give me the combat? No, so I'm just 
right clicking and dragging to get it.
138:27 - Okay. Okay, do we get this combat? Yes. 
Do we get this one? Yep. Simple as that.  
138:41 - Okay, it doesn't always get 
it, but most cases it does.
138:48 - Okay, didn't get that one. 
So we'll just drag it out.  
138:55 - Okay, it's not getting that one.  
139:00 - It's interesting. Like, that one's pretty clear. 
But it's interesting what it picks out and what  
139:04 - does what does not grab it. So it's not getting 
this one, probably because the photo doesn't have  
139:08 - enough contrast. And this one has a lot hoping 
that that gives us more data to work with here.  
139:16 - Yeah, I think the higher the 
contrast easier for it to  
139:21 - detect those. It's not getting that one. 
Not getting that one. Okay, there we go.
139:35 - Yes, there are a lot I know as 
some of these ones that are packed,  
139:38 - but there's only like three 
photos that are like this.  
139:45 - They have badges but they're slightly 
different. So we're gonna leave those out.  
139:50 - I think it actually had that one, 
but we'll just tag it anyway.  
140:03 - And hopefully this will be worth the effort here.
140:10 - There we go.
140:14 - I think that was the last one. Okay, great. So 
we have all of our tag photos. And what we can  
140:19 - do is go ahead and train the model, same 
option, quick training, advanced training,  
140:23 - we're gonna do a quick training here. And notice 
that the options are slightly different, we have  
140:26 - probably threshold. And then we have overlap 
thresholds. So the minimum percentage of overlap  
140:30 - between predicted bounding boxes and ground truth 
boxes to be considered for correct prediction.  
140:36 - So I'll see you back here when it is done. 
Alright, so after waiting a little bit a while  
140:40 - here, it looks like it's done. It's trained. And 
so precision is at 75%. So precision, the number  
140:46 - will tell you if a tag is predicted by your model, 
how likely that it's likely to be. So how likely  
140:52 - did a guess right? Then you have recall? So the 
number will tell you out of the tags, which should  
140:56 - be predicted correctly, what percentage does your 
model correctly find? So we have 100%. And then  
141:02 - you have mean average precision, this number will 
tell you the overall object detector performance  
141:07 - across all the tags. Okay, so what we'll do is 
we'll go ahead and do a quick test on this model.  
141:16 - And we'll see how it does. I can't remember if I 
actually even ran this. So it'll be curious to see  
141:20 - the first one here. It's not as clearly 
visible, it's part of their uniform. So  
141:25 - I'm not expecting you to pick it up. But we'll 
see what it does. It picks up pretty much all of  
141:29 - them. exception, this one is definitely not a con 
badge. But that's okay. Alicia suggests obviously,  
141:36 - the probability is above the selected threshold. 
So if we increase it, we'll just bring it down a  
141:43 - bit. So there it kind of improves it. If 
we move it around back and forth. Okay.  
141:49 - So I imagined it via the API, we could choose 
that let's go look at our other sample image here.  
141:56 - I'm not seeing it. Where did I save it? Let me 
just double check, make sure that it's in the  
142:05 - correct directory here. Okay. Yeah, I saved it 
to the wrong place just a moment. I will place it  
142:20 - just call that bench test to one second.  
142:28 - Okay, and so I'll just browse here again. 
And so here we have another one. See if  
142:33 - it picks up the badge right here. There 
we go. So looks like it works. So yeah,  
142:38 - I guess custom vision is pretty easy to use, 
and pretty darn good. So what we'll do is close  
142:45 - this off and make our way back to our Jupiter 
labs to move on to our our next lab here, okay.  
142:58 - All right, so let's move on to the face service. 
So just go ahead and double click there on the  
143:01 - left hand side. And what we'll do is work our way 
from the top. So the first thing we need to do  
143:05 - is make sure that we have the computer vision 
installed. So the face service is part of the  
143:10 - Computer Vision API. And once that is done, we'll 
go ahead and do our imports. Very similar to last  
143:17 - one. But here we're using the face client, we're 
still using the cognitive service credentials  
143:23 - will populate our keys, will you make the face 
client and authenticate. And we're going to use  
143:28 - the same image we used prior with our computer 
vision, so the data one there, and we'll go ahead  
143:35 - and print out the results. And so we get an object 
back. So it's not very clear what it is. But  
143:39 - here if we hit show, okay, here, it's data, and 
it's identifying the face IDs are going through  
143:45 - this code. So we're just saying open the image, 
we're going to set up our figure for plotting,  
143:51 - it's going to say, Well, how many faces did 
it detect in the photo, and so here it says,  
143:55 - detected one face, it will iterate through it. 
And then we'll create a bounding box around the  
144:01 - images, we can do that because it returns back the 
face rectangles, we get a top left, right, etc.  
144:07 - And we will draw that wrangle on top. So 
we have magenta, I could change it to like  
144:12 - three if I wanted to. I don't know what the other 
colors are. So I'm not even going to try but yeah,  
144:16 - there it is. And then we annotate with the face 
ID that's the unique identifier for the face.  
144:21 - And then we show the image. Okay, so that's 
one. And then if we wanted to get more  
144:26 - detailed information, like attributes such as age, 
emotion, makeup or gender, this resolution image  
144:32 - wasn't large enough. So I had to find a different 
image and do that. So that's one thing you need  
144:37 - to know is if it's not large enough, we won't 
process it. So we're just loading data large.  
144:43 - Very similar process, but it is the same thing 
detect with stream but now we're passing in  
144:52 - return face attributes. And so here 
we're saying the attributes we want.  
144:56 - And there's that list and we went through 
it in the lecture content and so here  
144:59 - We'll go ahead and run this. And so 
we're getting more information. So  
145:03 - that magenta line is a bit hard to see. 
I'm just gonna increase that to three.
145:08 - Okay, still really hard to see. But 
that's okay. So approximately age 44,  
145:13 - I think the actor was a bit younger than 
that. Data technically is male presenting,  
145:18 - but he's an Android. So it doesn't necessarily 
have a gender, I suppose. He actually is wearing  
145:22 - a lot of makeup. But all it detects is it I guess 
it's only Pickler on the lips and the eyes. So it  
145:28 - says he doesn't have makeup. So maybe there's 
a color, you know, like eyeshadow or stuff. And  
145:31 - we would detect that in terms of personality. 
I like how he's a 002 points, percent. Sad,  
145:38 - but he's neutral, right. So just going 
through the code here very quickly. So again,  
145:42 - it's the number of faces so it detected one face. 
And then we draw a bounding box around the face  
145:48 - for the detected attributes, it's returned back 
in the data here. So we just say, get the phase  
145:55 - attributes, turn it into a dictionary. 
And then we can just get those values and  
146:00 - iterate over it. So that's as 
complicated as it is. And so there we go.
146:10 - Alright, so we're on to our next cognitive 
service. Let's take a look at form recognizer.  
146:16 - Alright, and so form recognizer, it tries 
to identify, like forms and turns them into  
146:22 - readable things. And so they have one for receipts 
in particular. So at the top, finally, we're not  
146:27 - using computer computer vision, we actually 
have a different one. So this one's Azure AI  
146:32 - form recognizer. So run that there. But this one 
in particular isn't up to date in terms of using  
146:38 - it like, notice, all the other ones are using 
the cognitive service credential. So for this,  
146:45 - we actually had to use the Azure Key credential, 
which was annoying, I tried to use the other one  
146:50 - to be consistent, but I couldn't use it. Okay, 
so what we'll do is run our keys like before,  
146:56 - we have a client very similar process. And 
this time, we actually have a receipt. And so  
147:03 - we have begin recognize receipt. So it's going 
to analyze the receipt information. And then it's  
147:07 - what it's going to do is show us the image. Okay, 
just so we have a reference to look at the images  
147:13 - and actually yellow, it's a white background. 
I don't know why when it renders out here,  
147:16 - it does that, but that's just what happens. And it 
even obscures the server name. I don't know why.  
147:23 - But anyway, if we go down below, this is returned 
results up here, right, so we got our results.  
147:30 - And so if we just print out the results, here, 
we can see we get a recognized forum back,  
147:35 - we get fields, and some additional things. And 
if we go into the fields itself, we see there's  
147:40 - a lot more information, if you can make out like 
here, it says merchant phone number, form field  
147:45 - label value, and there's a number 512707. 
So for these things here, like the receipts,  
147:54 - if we can just find the API quickly here, 
it has predefined fields. I'm not sure.  
148:02 - Yeah, business card, etc. Like 
if we just type in merchant,  
148:10 - I'm just trying to see if there's a big old list 
here. It's not really showing us a full list. But  
148:14 - these are predefined things that are returned, 
right? So they've defined those. Maybe it's over  
148:20 - here. There we go. So these are the predefined 
ones that extracts out. So we have receipt, type,  
148:26 - merchant name, etc, etc. And so if we go back to 
here, you can see I have a field called merchant  
148:33 - name. So we get there it says Alamo Drafthouse 
cinema, let's say we want to try to get that  
148:37 - balance. Maybe we can try to figure out which 
one it is. I never ran this myself when I made  
148:42 - it. So we'll see what it is. But here it has total 
price. What's interesting is that these, this is a  
148:47 - space. So it's a kind of unusual, you think it'd 
be together, but let's see if that works. Okay,  
148:55 - doesn't like that. Maybe that's just a typo on 
their part. Okay, so we get none. Let's try price.
149:04 - See what it picks up? Nope, nothing.  
149:09 - We know that the phone numbers there. 
So we'll give the phone number.
149:14 - There we go. So you know, it's an OK service. 
But, you know, you know, your your mileage will  
149:21 - vary based on what you do there. Maybe we could 
try total, because that makes more sense, right?
149:28 - Ah,
149:28 - yeah, there we go. Okay, great. So 
yeah, it is pulling out the information.  
149:32 - And so that's pretty much all you need 
to know about that service there. Okay.
149:40 - Let's take a look at some of our OCR capabilities 
here. I believe that's in computer vision. So  
149:45 - we'll go ahead and open that up. At the top here 
we'll install computer vision as we did before,  
149:51 - very similar to the other computer 
vision tasks, but this time we have  
149:54 - a couple of ones here that I'll explain that as we 
go through here. We'll load our keys. We'll Do our  
150:00 - credentials will load the client. Okay, and then 
we have this function here called printed text.  
150:07 - So what this function is going to do is it's 
going to print out the results of whatever text it  
150:13 - processes. Okay, so the idea is that we're going 
to feed in an image, and it's going to give us  
150:19 - back out the text for the image. So we'll run this 
function. And I have two different images, because  
150:25 - I actually ran it on the first one, and the 
results were terrible. And so I got a second image  
150:30 - and it was a bit better. Okay, so we'll go ahead 
and run this, it's going to show us the image.  
150:34 - Okay, and so this is the photo, it was supposed 
to extract out Star Trek The Next Generation, but  
150:38 - because of the artifacts and size of the image, 
we get back, not English, okay. So you know,  
150:45 - maybe a high resolution image, it would have a 
better a better time there. But that is what we  
150:50 - got back. Okay. So let's go take a look at our 
second image and see how it did. And this one,  
150:56 - I'm surprised that I actually extracts out a 
lot more information, you can see realize a  
151:00 - hard time with the Star Trek font, but we get Deep 
Space Nine, nine, a visitor tells all life death,  
151:05 - some errors here, so it's not perfect. But you 
know, you can see that it does something here.  
151:11 - Now there is the iOS. This is like for OCR, where 
we have like first very simple images and texts.  
151:16 - This is where we use the recognized printed text 
in stream. But if we're doing this for larger  
151:21 - amounts of text, and we want to do this, want this 
analyzed a synchronously, then we want to use the  
151:27 - read API, and it's a little bit more involved. So 
what we'll do here is load a different image. And  
151:32 - this is a script, we'll look at the image here 
in a moment. But here we read in stream, and we  
151:38 - create these operations. Okay. And what it will 
do is it will asynchronous asynchronously send  
151:44 - all the information over. Okay. So I think this 
is supposed to be results here. Minor typo. And  
151:53 - we will go ahead and give that a run. Okay, so 
here you can see it's extract out the image if  
151:59 - we want to see this image. I thought I thought 
I showed this image here, but I guess I don't.  
152:06 - Yes, this plot image here to show 
us the image. path. It's up here.  
152:16 - It doesn't want to show us it's funny because 
this one up here is showing us No problem, right?  
152:21 - Um, well, I can just show you 
the image. It's not a big deal.  
152:28 - But I'm not sure why it's not showing up 
here today. So if we go to your assets here,  
152:35 - I go to OCR. I'm just gonna open this up. Hope 
it's opening up in Photoshop. And so this is what  
152:43 - it's transcribing. Okay, so this is a thing. 
This is like a guide to Star Trek where they  
152:47 - talk about like, you know, what, what makes 
Star Trek Star Trek. So just looking here, it's  
152:52 - actually pretty darn good. Okay. But like read 
API is a lot more efficient, because it can work  
152:59 - asynchronously. And so when you have a lot 
of texts, that's what you want to do, okay?  
153:04 - Like it's feeding in each individual line, right, 
so that it can be more effective that way. So  
153:09 - let's go look at some handwritten stuff. So just 
in case the image doesn't pop up, we'll go ahead  
153:13 - and open this one. And so this is a handwritten 
note that William Shatner wrote to a fan of Star  
153:21 - Trek, and it's basically incomprehensible. 
I don't know if you can read that here. But  
153:26 - see, was very something he was something 
hospital and healthy was something he was  
153:34 - something I can't even read it. Okay, so 
let's see what the machine thinks here.  
153:41 - And it says image path, it's called path. Let's 
just change that out. We hadn't run that. Run that  
153:50 - there. And we'll go ahead and run it. And here we 
got the image. So poner us very sick, he was the  
153:59 - hospital his BD was, etc. Beat nobody lost. His 
family knew Captain halden. So reads better than  
154:06 - how I could read it, honestly, like it is. It's 
really hard, right? Like, if you looked at this,  
154:11 - like, that looks like difficult was bt healthy. 
I could see why it's guessing like that,  
154:18 - right? dying. It's like that looks like dying 
to me. You don't I mean, so it's just poorly  
154:24 - hand handwritten, but I mean, it's pretty 
good for what it is. So yeah, there you go.
154:33 - Alright, so let's take a look at another 
cognitive service here. And this one is text  
154:36 - analysis. And so what we'll do is install 
the Azure cognitive services, language text  
154:43 - analytics here. So go ahead and hit run. Alright, 
and once that's installed, this one actually is  
154:50 - using the cognitive services credential, so it's a 
little bit more standard with our other ones here.  
154:55 - We'll go ahead and run that there. We'll make our 
credentials low clients. And this one, what we're  
155:01 - going to do is try to determine sentiment and 
understand why people like a particular movie  
155:06 - or not. So I've loaded a bunch of reviews, they 
are again, I can show you the data, if it helps.  
155:14 - And so I'm just trying to find my right 
folder here. And so if we go back,  
155:19 - look, our movie reviews, here's like a review, 
someone wrote, first contact just works. It's  
155:24 - works as a rousing chapter in the Star Trek to 
lesser set works as a mainstream entertainment.  
155:28 - So different reviews for Star Trek First Contact, 
which was a very popular movie back in the day. So  
155:35 - what we'll do is as we will load the reviews, 
so it's just iterating through the text files  
155:41 - and showing us what the reviews are. so here we 
can see all the written text had a lot of trouble  
155:46 - getting the last one to display, but it does 
get loaded in. And so here we're using the the  
155:52 - text analysis to show us key phrases because 
maybe that would give us an indicator.  
155:58 - And so that's the object back but maybe that'll 
give us an indicator as to like what people are  
156:02 - saying as important things so here we see 
Borg ship, enterprise smaller ship escapes,  
156:06 - neutral zone travels, contact damage, co writer 
beautiful mind sophisticate science fiction,  
156:13 - best whales, Leonard Nimoy. Okay. wealth of 
unrealized potential filmmaker john Franks.  
156:21 - Okay, so very interesting stuff as here Borg 
ship again, you've seen Borg ship a lot.  
156:26 - So that is kind of key phrases, let's go get Cust 
or customer sentiment or how people felt about it,  
156:31 - do they like it or not. And so here, we just 
call sentiment. And what we'll do is if it's  
156:37 - above five, then it's positive, and it's below 
five, then it's a negative review. I think most  
156:41 - people thought it was very good film. So this one 
says it's pretty low nine. So let's go take a look  
156:48 - at that one. It wasn't actually showing rendered 
there. So maybe we'll have to open it up manually.  
156:53 - See if that's actually accurate, 
it's empty. So there you go.  
156:57 - I guess we had a blank one in there. I must have 
forgot to paste it in. But that's okay. That's a  
157:02 - good indicator that, you know, that's what happens 
if you don't have it. So let's look at number one,  
157:06 - then, which is actually this one is nine, this 
is 04. This one here is eight. So open up eight.  
157:14 - When the board launch on Earth, the enterprise 
is sent to the neutral zone, etc, etc. However,  
157:19 - smaller ship escapes traveled of enterprise falls 
back. Meanwhile, the survivors, so like this is a  
157:25 - synopsis. It doesn't say whether they like it, or 
they don't. But it was before, I guess. So there's  
157:30 - nothing positive about it. Right? If we were 
looking at one that was this one's pretty low,  
157:36 - which is no, no, it's not. It's one. So it 
seems like this person probably really liked it.  
157:42 - Or no, I guess that's actually pretty low. Because 
it's one it's not nine, nine is very high. Let's  
157:47 - take a look at this one. Review number two. If 
we go up here, the dog has improved the Sorry,  
157:54 - I'm going to turn the show but there's a wealth 
of unrealized potential. So that's a fair  
157:57 - one saying that maybe they don't like it as 
much. I don't know if they give it two stars,  
158:01 - right, we could probably actually correlate 
it with the actual results, because I did get  
158:05 - these off of IMDb and Rotten Tomatoes. But 
yeah, there you go. That is text analysis.
158:16 - Alright, so now we're on to q&a maker. And 
so we're not going to need to do anything  
158:20 - programmatically, because q&a maker is all about 
no code or low code to build out a questions and  
158:26 - answers bot service. So what we'll do is 
go all the way up to here. And I want you  
158:31 - to type in Q and maker.ai. Because as far as 
I'm aware of so accessible through the portal,  
158:36 - sometimes you can find these things. Again, if we 
go to the marketplace. I'm just curious. I could  
158:43 - just take a look here really quickly. Whenever 
it decides to log us in here. Okay, great. So  
158:47 - I'll go over to marketplace. And probably we 
type in q&a. Maybe we do something here q&a.
158:57 - Yep. So we go here. Give it a second here. 
Seems like Azure is a little bit slow right now.
159:09 - It's usually very fast. But 
you know, the service varies.  
159:14 - Well, it's not loading for me right now. 
But that's okay, because we're not going  
159:17 - to do it that way. Anyway. So you can go to q&a 
maker.ai. And what I want you to do is go all  
159:24 - at the top of the right corner, and we'll hit 
sign in. And what we'll be doing is connecting  
159:28 - via our single sign on with our account, so 
it already knows I have an account there.  
159:32 - I'm gonna give it a moment here. And I'm 
going to go ahead and just give it a second.
159:52 - There we go. So it says I don't have any knowledge 
base, which is true. So let's go ahead and create  
159:57 - ourselves a new knowledge base. And here we have 
the option Between stable and preview, I'm going  
160:01 - to stick with stable because I don't know what's 
in preview. I'm pretty happy with that. So we  
160:05 - need to connect q&a service q&a service to our 
knowledge base. And so back over here in Azure,  
160:12 - actually, I guess we do have to make one now 
that I remember, we actually have to create  
160:15 - a q&a maker service. So I'll go down here and put 
this under my cog services will say my queue at a  
160:24 - queue and a service might complain about the 
name. Yep, so I'll just put some numbers here.  
160:32 - We will pick a free tier sounds good, I'll go free 
what I actually get the option, that's what I will  
160:36 - choose. Down below, we'll choose free again, USB 
sounds great to me, it generates out the name,  
160:42 - it's the same name as here. So that's fine. We 
don't need App Insights, I'm going to leave it  
160:47 - enabled, because I think it changes the standard 
or zero when you do not have an enabled unusually.  
160:55 - And so we will create our q&a maker service, give 
it a moment here. And it says I remember it will  
161:03 - say like, even if you try it, it might have to 
wait 10 minutes for it to create the service.  
161:07 - So even though even after it's provisioned, it 
will take some time. So what we should do is  
161:11 - prepare our doc because it can take in a variety 
different files. I just want to show you here  
161:16 - that the q&a that a whole paper here formatting 
the guidelines. And basically it's pretty smart  
161:22 - about knowing where headings and answers is. So 
for unstructured data, we just have a heading,  
161:27 - and we have some text, let's write some things 
in here that we can think of. Since we're all  
161:30 - about certification, we should write some stuff 
here. So how many AWS certifications are there?  
161:37 - I believe right now, there are 11. 
eight of us certifications. Okay.  
161:46 - And maybe if we use our headings here, this 
would probably be a good idea here. Yeah.  
161:52 - Okay. Another one could be how many fundamental  
162:02 - Azure certifications are there. And 
we'll give this a heading. And we'll say  
162:17 - there are three as your I think there's 
three. There's other ones, right, like pirate  
162:24 - power platform and stuff. But just being Azure 
specific. There are three as your fundamental  
162:31 - certifications, certification, so we 
have the DP 900, the AI 900. The az  
162:40 - 900. I guess there's four, there's the 
SC 900. Right. So there are four. Okay.  
162:48 - We'll say which is the hardest. Azure 
Azure Association certification.  
163:08 - And what we'll say here is, I think, I mean, it's 
my is my opinion is it's the Azure administrator,  
163:15 - had some background noise there. That's 
why I was a bit pausing there. But the  
163:18 - Azure minister, az 104, I would say 
that's the hardest, which is harder.  
163:28 - The AWS or Azure certifications, 
I'd say Azure certifications  
163:39 - are harder. Because they check exact steps 
for implementation where AWS focuses on  
163:55 - concepts. Okay, so we have a bit of a 
knowledge base here. So I'll save it.  
164:01 - And assuming that this is ready, because we 
did a little bit time to put this together.  
164:05 - We'll go back to q&a, hit a refresh here. Give 
it a moment, drop it down, choose our service.
164:20 - And notice here that we have chitchat extraction 
and only extraction we're going to do to chat.  
164:25 - I will say my or this is the reference 
can be changed any time this would be like  
164:33 - a certification q&a. So here, we want 
to populate. So we'll go to files here,  
164:38 - I'm going to go to my desktop. 
And here it is. I'll open it.  
164:45 - We will choose professional town. Go ahead and 
create that. And so I'll see you back here in a  
164:49 - moment. Alright, so after waiting a short little 
time here, it loaded in our data. So you can see  
164:54 - that it figured out which is the question which 
is the answer and also has a bunch of default.  
164:59 - So Here if somebody was at something very silly, 
like, can you cry, I'll say I don't have a body.  
165:04 - It has a lot of information pre loaded for us, 
which is really nice. Why don't we go ahead and  
165:08 - test this? We could go and say, we'll go here 
and then we'll write in, say, like, hello.
165:22 - Say boring. This is good 
morning. Okay, so we'll say,  
165:28 - how many certifications are there? We didn't 
say AWS, but let's just see what happens.
165:41 - So to kind of infer even though we didn't say 
AWS in particular, so I noticed that there's  
165:46 - AWS and Azure, so how many fundamental Azure 
certifications, things like that, and so chose  
165:50 - AWS. So it's not like the perfect service, but 
it's pretty good. I wonder what would happen if we  
165:57 - placed in one that's like Azure, I don't know how 
many Azure certs there are, we'll just say like,  
166:01 - there's 1112, I can't ever remember, they're 
always adding more. But I want to close this here.  
166:06 - There we go. So let's just go add a new key pair 
here. And we'll say, how many Azure certification  
166:15 - are there, I should have said certifications, I'll 
probably just set one moment. So there, there are  
166:23 - 12, Azure certifications. Who knows how many they 
have, they have like 14 or something, say like,  
166:29 - between 11 and 14. They just added they just 
updated them too frequently. I can't keep track.  
166:37 - So we'll go here and we'll 
just say certifications.  
166:40 - And we will save and retrain. So 
we'll just wait here a moment.  
166:48 - Great. And so now we'll go ahead and test this 
again. So we'll say how many certifications  
166:56 - are there?  
167:01 - I see it's pulling the first 
answer. If I say Azure,  
167:05 - let's just see if it gets the right one here. 
How many Azure certifications are there?  
167:15 - Okay, so, you know, maybe you'd have to 
say you'd have to have a generic one for  
167:21 - that match. So if we go back here, and we 
say, how many certifications are there?  
167:30 - You say, you know, like, which certification? 
Which cert cloud service provider.  
167:43 - Here we got AWS Azure. Follow prompt, you 
can use guides through conversational flow  
167:51 - prompts are used to link q&a pairs and can be 
displayed. I haven't used this yet. But I mean,  
167:56 - it sounds like something that's pretty good. 
Because there is multi turn into so the idea  
168:01 - is that if you had to go through multiple steps, 
you could absolutely do that. We've tried a little  
168:06 - bit here, fall prompt you can use to guide us to 
convert props are used to link q&a pairs together,  
168:12 - text or button for suggested action. Oh, okay, 
so maybe we would just do like AWS link to q&a.  
168:17 - And then so search an existing q&a or create a new 
one. So let's say like, how many eight of us, oh,  
168:25 - okay, we're typing in context, this follows up 
will not be understood out of the context flow.  
168:32 - Sure. Because it should be within context, right. 
And here, we can do another one we say like
168:40 - Azure will say, how many Azure  
168:49 - contacts only. Whoops, that 
got away from me there.  
169:00 - We'll save that. And what 
we'll do is save and train.
169:12 - Go back here. And we'll say, how 
many certifications are there?  
169:21 - Enter. So we have to choose AWS. So there we 
go. So we got something that works pretty good  
169:27 - there. Since I'm happy with it, we can go ahead 
and go and publish that. So let's say publish.
169:35 - And now that it's published, 
we could use postman or curl to  
169:43 - trigger it. But what I want to do is create 
a bot because with Azure bot services,  
169:46 - then we can actually utilize it with other 
integrations right. It's great way to  
169:52 - use your bot or to actually host your bot. So 
we'll go over here and link it over. If you don't  
169:57 - click it, it doesn't pre loaded in so it's kind 
of a pain. If you lose Got to go back there and  
170:00 - click it again. But let's just say certification. 
que en de. And we will look through here. So I'm  
170:10 - going to go with free premium messages, 10k 
1k Premium message units, messages, I'm kind  
170:15 - of confused by the pricing. But f0 using means 
free. So that's what I'm gonna go for that SDK or  
170:19 - no GS, I'm gonna use no GS now that we're gonna do 
anything there with it. Go ahead and create that.  
170:26 - And I don't think this takes 
too long. We'll see here.
170:37 - Just go ahead and click on that there. I'll 
just wait here a bit. I'll see you back here  
170:42 - in a moment. All right. So after waiting, I 
don't know about five minutes there. It looks  
170:46 - like our bots services deployed, we'll go to that 
resource there. You can download the bot source  
170:52 - code. Actually, I never did this. So I don't know 
what it looks like. So be curious to see this.  
170:57 - Just to see what the code is. I assume that 
because we chose chose no GS, it would give  
171:01 - us that is the default there. So download 
this code as you're creating the source IP.  
171:07 - Not sure how long this takes. Maybe regretting 
clicking on that. But what we'll do is we'll  
171:13 - go in the left hand side here to channels 
because I just want to show here. Yeah, that  
171:19 - didn't download. We'll try it here in a second. 
But what we'll do is we'll go back up profile.  
171:28 - unspecified bar we talked about. 
Yeah, maybe it needs some time.  
171:41 - So you know, maybe we'll just give the bot 
a little bit of time here. I'm not sure why  
171:45 - it's giving us a hard time because this bot is 
definitely deployed. If we go over to our bots,  
171:49 - right. bot services, it is here. Sometimes 
there's like latency, you know, with  
171:57 - Azure. Oh, there we go. Okay, see works now. Fine, 
right. And so I want to show you that there's  
172:01 - different channels and these are just easy ways 
to integrate your bot and different services. So  
172:05 - whether you want her to use it with Alexa GroupMe, 
Skype telephony, Twilio Skype for Business,  
172:13 - apparently they don't have that anymore. 
Because they get small teams now, right. keek,  
172:17 - which I don't know, people still use that Slack, 
which that discord, telegram Facebook, email.  
172:24 - That's kind of cool. But teams teams is 
a really good one. I use teams. There's a  
172:28 - direct line channel, I don't know what that means. 
And there's web chat, which is just having like an  
172:32 - embed code. So if we go over, we can go and test 
it over here to start testing our web chat. And  
172:38 - so it's the same thing as before, we just say 
things like, how many certifications are there?  
172:49 - Azure, and get a clear answer back. We'll 
go back up to our overview. Let's try see  
172:55 - if we can download that code. Again. I 
was kind of curious what that looks like.
173:04 - Yes, it will download  
173:16 - a lot of code a.  
173:21 - There we go. So now we can hit download. And 
so there is the code, I'm going to go ahead  
173:25 - and open that up. So yeah, I guess when we 
chose JavaScript, that made a lot more sense.  
173:30 - Let's give it a little peek here. I'm just going 
to drop this on my desktop here. So just to make  
173:37 - a new folder here and call this bot code. Okay, 
I know you can't see what I'm doing here. But  
173:45 - let's go here, and gret, double click into 
here, and then just drag that code on him.  
173:55 - And then what we can do is open this up 
in VS code, I should have VS code running  
174:00 - somewhere around here. I'm gonna go ahead 
and open that off screen here. I'll just  
174:04 - show you my screen in a moment. Say show code, 
oops, File, Open Folder. botcon code, okay.  
174:16 - And all the way back here. And so we got a lot 
of code here. never looked at this before. But  
174:21 - you know, I'm a pretty good programmers. 
So it's not too hard for me to understand.  
174:27 - So it's like your API request, things 
like that. I guess it would just be like,  
174:30 - if you needed to integrate into your application, 
then it kind of shows you all the code. They're  
174:35 - just trying to see our dialogue 
choices. Nothing super exciting.  
174:43 - Okay, you know what I go and make the Was 
it the AI are the 100 whatever the data  
174:50 - scientists courses, I'm sure I'll be a lot 
more thorough here. But I'm just curious as  
174:54 - to what that looks like. Now, if we wanted 
to have an easy integration, we can get an  
174:59 - embedding code for this. So if we go back to 
our channels, I believe we can go and edit.  
175:09 - Ah, yeah. So here we have a code. So 
what I'll do is go back to Jupiter labs,  
175:13 - I'm just going to go make a new empty 
notebook. So let's go up here and say notebook.  
175:20 - And this can be for our q&a. Doesn't really 
matter what Colonel, say cute and a maker. Just  
175:29 - show like, if you wanted a very, very simple way 
of integrating your bot, we would go back over to  
175:38 - wherever it is here. Here, we are 
going to go ahead and copy this iframe.  
175:42 - I think it's percentage percentage 
HTML. So it treats this cell as HTML.  
175:49 - And I don't have any HTML to render. So we 
will place that in there. And notice we have  
175:54 - to replace our secret key. So I will go back here 
and I will show my key and we will copy that.  
176:01 - And we will paste that key in here.  
176:04 - And then we'll run this. And I can type 
in here. Where am I? just silly things.  
176:17 - Who are you? How many Azure certifications? Are 
there? Well, I wonder if I just leave the are  
176:26 - there off? Let's see if it's figures it out. Okay, 
cool. So yeah, I mean, that's pretty much it with  
176:32 - q&a maker. So yeah, that's great. So I think we're 
done here. And we can move on to checking out  
176:40 - Louis or Liu is learning understanding 
to make a more robust bot, okay.
176:50 - Alright, so we are on to our last cognitive 
service. And this one is going to be  
176:55 - Louis or Luis, depending on how you'd like to say 
it. It's Li s, which is language understanding. So  
177:00 - you type in L ui s.ai. And that's going to bring 
us up to this external websites. So part of Azure  
177:08 - just has its own domain. And so here, we'll choose 
our subscription. And we have no author authoring  
177:15 - source. So I guess we'll have to go ahead and 
create one ourselves. So get down here, and we  
177:19 - will choose my cognitive services as your resource 
name. So my off service or my cognitive service,  
177:33 - great new cognitive service 
account, but we already have one,  
177:36 - so I don't want to make another one. 
Right, it should show up here, right?  
177:43 - Or valid in the author authoring region. 
So it's possible that we're just in the  
177:47 - incorrect region. So we might end up creating 
two of these. And that's totally fine. I don't  
177:51 - care. It's as long as we get this work in here, 
because we're gonna delete everything, get the  
177:55 - end anyway. And so just say, my cog service, too. 
And we'll say West us because I think that maybe  
178:03 - we didn't choose one of these regions. Let's 
go double check. If we go back to our portal,  
178:10 - just the limitations of the service, right. So 
we'll go to my cog services here. I just want to  
178:16 - go cognitive services. So just want to see where 
this is deployed. And this is in us, West us.  
178:28 - Yeah, so I don't know why it's not showing up 
there. But whatever. If that sort of wants,  
178:31 - we'll give it what it wants, okay. shouldn't give 
us that much trouble, but pay, that's how it goes.  
178:41 - And so we have an authorized authoring 
service, I'm gonna refresh here and see if  
178:44 - it added a second one, it didn't. So all right. 
That's fine. So we'll just say, my sample bot  
178:54 - will use English as our culture. If 
nothing shows up here, don't worry,  
178:58 - you can choose it later on. I remember the 
first time I did this, it didn't show up.  
179:01 - And so now we have my cog service, my 
custom vision service, we want cog service.  
179:06 - So anyway, it tells us about schema, like how you 
make a schema animates talking about like body,  
179:14 - action, intent, and example utterance, but we're 
just gonna set up something very simple here. So  
179:19 - we're gonna create or attend, the one that we 
always see is flight booking. So I'll go here,  
179:25 - do that. And what we want to do is write an under 
and so like, book, me a flight to Toronto. Okay.  
179:36 - So if someone were to type that in, then the idea 
was it would return back the intent this value and  
179:41 - metadata around it. And we could programmatically 
provide code, right? So what we need is identity  
179:46 - identities and we can actually just click here and 
make one here. So enter name identity, and we'll  
179:52 - just call this location. Okay. Here we have option 
machine learned and list if you flip between it.  
179:58 - This is like a magic Have a ticket order 
and you have these values that can change,  
180:03 - or you just have a value that always stays 
the same like lists. So that's our airport.  
180:08 - That makes sense, we'll do that. If we 
go over to entities, we can see it here.  
180:17 - Alright, so nothing super exciting there. But 
what I want to show you is if we go ahead, and we  
180:23 - should probably add, fight booking should be about 
book flight. flight booking, flight booking. Okay,  
180:34 - so we'll go ahead and I know there's only 
one, we'll go ahead and train our model.
180:42 - Because we don't need to know tons, right, 
we cover a lot in the lecture content  
180:47 - to build a complex bot is 
more for the associate level.  
180:51 - But now what we can do is go ahead and test 
this and we'll say, book me a flight to Seattle.  
181:00 - Okay, and notice here it says 
book flight, we can go inspect it,  
181:03 - and we get some additional data. So top scoring, 
so it says how likely that was the intent.  
181:11 - Okay, so you get kind of an idea there, there's 
additional things here, it doesn't really matter.  
181:16 - We'll go back here, and we will 
go ahead and publish our model.  
181:20 - So we can put it into a production slot, you can 
see we have sentiment analysis, speech, priming,  
181:25 - we don't care about either of those things. We 
can go and see where our endpoint is. And so  
181:31 - now we have an endpoint that we can work 
with. So yeah, I mean, that's pretty much  
181:36 - all you really need to learn about Louis. But I 
think we're all done for cognitive services. So  
181:42 - we're going to keep around our notebook, 
because we're going to still use your Jupyter  
181:47 - Notebook for some other things. But what 
I want you to do is make your way over to  
181:53 - your resource groups. Because if you've been 
pretty clean, it's all within here, we'll just  
181:58 - take a look here. So we have our q&a. All of 
our stuff here, I'm just making sure it's all  
182:02 - there. And so I'm just gonna go ahead and delete 
this resource group. And that should wipe away  
182:08 - everything, okay? For the cognitive services 
part. Alright, so we're all good here. And I'm  
182:16 - just going to go off, and I'll leave this open, 
because it's always a pain to get back to it,  
182:21 - reopen it, but let's make our way back to 
the home here and the Azure Machine Learning  
182:26 - Studio. And now we can actually explore 
building up machine learning pipelines.
182:35 - Okay, so we are on to the ML kit, follow along 
here. So we're going to learn how to build some  
182:41 - pipelines, the first i think is the easiest will 
be auto automated ml are also known as auto ml.  
182:46 - The idea here is it's going to just build up the 
entire pipeline for us. So we don't have to do any  
182:50 - thinking we just say what kind of model we want 
to run and have it to make a prediction. So what  
182:56 - we'll do is a new automated ml, and we're going 
to need a data set. So I don't have one. But the  
183:00 - nice thing is they have these open datasets. So if 
you click here, you'll see there is a bunch here.  
183:06 - And a lot of these you'll come across quite often, 
not just on Azure, but other places like this  
183:11 - diabetes one, I seen it like everywhere, okay. And 
so like, if we just go click here, and maybe we  
183:17 - can read a bit more here. So diabetes data, set 
422 samples with 10 features, ideal for getting  
183:23 - started with machine learning algorithms. It's 
one of the popular psychic learn toy data sets.  
183:28 - It's probably where I've seen it before, though 
it's not showing up there. You scroll on down,  
183:32 - you can see the data sets available as your 
notebooks data, bricks and Azure synapse.  
183:38 - The thing is, we have these values of age, sex, 
BMI, BP and y is trying to make a prediction,  
183:44 - it's trying to say, what's the likelihood of you 
having diabetes or not? And so it's not boolean  
183:49 - value. So it's not a binary classifier. It's 
kind of on a like you, would you be doing binary  
183:54 - classifications? classification, say, do you have 
diabetes, or you can make a prediction to say,  
184:00 - what's the likelihood or this value if you gave 
another value in there. But anyway, this is the  
184:06 - predicted value, a lot of times this is x, so 
everything here is x. And this is considered y,  
184:12 - the actual prediction. So sometimes it's why 
and sometimes it's actually named what it is.  
184:17 - But that's just what it is here. So we'll close 
that off. And so we'll choose the diabetes set.  
184:22 - And it will be data set one. And so it will worry 
about feedback later. So we'll click on sample  
184:30 - diabetes will hit next. And here's going to try 
to figure out what kind of model that we want. We  
184:35 - have to create a new experiments a container to 
run the model in so I'll just say, diabetes. My  
184:42 - diabetes, it sounds a bit odd, but that's what 
it is the target call and we want to predict  
184:47 - is seeing the train to predict is the why it's 
usually the why we don't have a compute cluster.  
184:53 - So I'll go ahead and create a new compute. We 
have dedicated or low priority. Technically,  
184:59 - we It is low priority, but I just want this done 
low priority, but don't forget to compute nodes,  
185:06 - your job may be preempted. I'm gonna 
say with dedicated for the time being,  
185:10 - we're gonna stick with CPU. If we go with 
this, it does take about an hour to run.  
185:19 - So I ran this ticket about an hour. So if you 
don't mind, it's only going to cost you 15 cents.  
185:23 - But if you want this done a lot sooner, I'm going 
to try to do something a little bit more powerful.  
185:29 - So just trying to decide here, 
because if it only takes an hour,  
185:34 - I might run it on something more powerful, that's 
90 cents, that might be overkill, because it's not  
185:39 - really deep learning. It's just a statistical, 
statistical stuff. So true and large data set,  
185:47 - I wouldn't say it's large real time 
inference, other latency sensitive ones.
185:56 - How Bode? Why is this one, I'm just looking 
here, cuz this one's 29 cents, this one's more  
186:04 - expensive. But it has 32 gigabytes of RAM. This 
was 28 Oh, 14 gigabytes of RAM and storage. So  
186:12 - this one's our highest in the tier, again, you 
can choose this one, you just have to wait a lot  
186:16 - longer, I just want to see if it finishes a lot 
faster, okay, without having to go to the GPU  
186:20 - level. So I don't think GPU is gonna help too much 
here. The computer name is my diabetes machine.
186:32 - minimum number of nodes. You want to provision if 
you want dedicated nodes to set the count here,  
186:38 - maximum. I guess I just want one node, right?  
186:44 - We will go ahead and oops, complete 
name must be 216 characters long.  
186:52 - What is it? Is it too long? Okay, there we go.  
187:02 - We'll give it a moment here. Yeah, it's 
gonna spin up the cluster. So it does take  
187:08 - a little bit time to start this. So I'll 
see you back here when this is done. Okay.  
187:13 - Great. So after a short little wait there, it 
looks like our cluster is running. If we double  
187:17 - check here, we can go to compute, I believe that 
shows up under here under the compute clusters.  
187:22 - So there it is, this is slightly different. 
This one shows you applications and this one  
187:26 - is just size, etc. and click in here see nodes 
and runtimes. We'll go make our way back here.  
187:33 - And we'll go ahead and hit next. And notice that 
I think it actually will select what it generally  
187:38 - does, it'll look at your prediction value, 
maybe sample a bit of it and say, okay, you  
187:41 - probably want a regression thing. So to predict 
a continuous numeric values. So the thing is,  
187:46 - is that if it was a label, like text, or if it 
was just zero in one, it probably would choose  
187:50 - classification, because it's, you saw our y value 
is like a number that was all over the place.  
187:56 - It thinks it's regression. So I think that's a 
good indicator there. So let's go with regression.  
188:05 - You know, but you might want it as a binary 
classifier, but it's another story there.  
188:10 - So it's, as soon as we created it just started, 
it didn't give us the option to say, hey,  
188:14 - I want to start running it. Notice on here, it's 
going to do feature iteration. So that means it's  
188:19 - automatically gonna select out features for us, 
which is what we wanted to do. It's set up to do  
188:22 - regression, we have some configuration 
here. So training time is three hours,  
188:27 - doesn't mean it's gonna train for three hours. 
But that's, I guess it's time out for it.  
188:31 - You could set a metric score threshold, so it 
has to meet at least this to be successful. If  
188:36 - it's not going to do it probably would quit out 
early crossmember valve or cross validation, just  
188:41 - make sure the data is good. You can see blocked 
algorithms. So TensorFlow dnn, TensorFlow linear  
188:46 - regression, if it was using dnn. So deep learning 
neural network, I probably would have chosen the  
188:50 - GPU to see if it would go faster. Look at the 
primary metric gets normalized root square  
188:57 - root mean square error, sometimes on the exam 
will actually ask you like, what's the primary  
189:01 - metric for this thing. So it's good to take a 
look and see what they actually use. For that,  
189:06 - I'll probably be sure to highlight that 
stuff in the actual lecture content.  
189:11 - But this will take some time to run. We have 
data guard rails, it will actually not populate,  
189:16 - I guess until we've ran it. So we'll just let it 
run. And I'll see you back here when it's done.  
189:20 - Okay. All right. So after a very, very, very long 
wait, our auto ml job is done. It took 60 minutes.  
189:26 - Using a larger instance, didn't save me any time. 
I don't know if maybe if I ran a GPU instance,  
189:31 - it would be a lot faster. I'd be very curious 
to try that out. But not something for this  
189:37 - certification course. So we go into here and yeah, 
the cheaper instance was the same amount of time.  
189:41 - So it probably just needs GPUs. It really depends 
on the type of models it's running. So we have  
189:46 - a bunch of different algorithms in here. It ran 
about 42 different models. I thought of like last  
189:52 - time I ran it, I saw a lot more but you can see 
there's all kinds of models that it's running and  
189:57 - then it's going to choose the top candidates. 
So it shows Voting ensemble. So ensemble is  
190:04 - we don't cover really in the course because it 
gets too much into ml, but ensemble is when you  
190:08 - actually use two different weaker models and 
combine the results in order to make a more  
190:15 - powerful ml model. Okay. So here, we'll get some 
explanation. I tried this before, and I didn't get  
190:21 - really good information. So if we go here, like 
I don't have anything under model performance. So  
190:28 - this tab requires a ray of predicted values from 
the model to be supplied. We didn't supply any,  
190:34 - so we don't get any Data Explorer. So select a 
cohort of the data, that all the data is what we  
190:40 - have here. So like, here, we were seeing age. 
And I guess it's just giving us an indicator  
190:46 - about the age information. Use the slider 
to show just descending feature importance,  
190:52 - select up to three cohorts to see the 
feature important side by side. Okay.  
191:00 - So I guess, s five and BMI. I don't know what s 
five is, we'd have to look up the dataset. BMI is  
191:06 - your body mass index. So that's a clear indicator 
as to what affects whether you have diabetes or  
191:11 - not. So that makes sense. age doesn't seem to 
be a huge factor, which is kind of interesting.  
191:17 - Individual feature importance, we can go here 
and just kind of like narrow in and say, Okay,  
191:21 - well, why is this outlier 
over here? And they're like  
191:23 - age 79. Right? So it's kind of interesting to 
see that information. So it does give you some x  
191:30 - explanation as to, you know, 
why things are why they are.
191:35 - Over here, we have a little bit 
more different data. This is kind  
191:38 - of interesting model performance. I don't 
know what I'm looking at, but like here,  
191:42 - it's over a mean squared. So it's that 
mean squared calculation there again? Okay.  
191:56 - Yeah, it's something right. But 
anyway, the point is, is that,  
192:00 - that we finally get metrics, I guess we always 
had to click there, because that makes more sense.  
192:06 - So yeah, there's more values here. Sure. data 
transformation, sorts of data processing feature  
192:14 - engineering scaling techniques, and machine 
learning algorithm, auto ml. So you know,  
192:17 - if you were a real data scientists, 
all this stuff would make sense to you.  
192:21 - I think just with time, it'll, it'll make sense. 
But even at this point, I'm not sure. And I don't  
192:26 - care about the model, right? If you're building 
something for real, I'm sure the information  
192:30 - becomes a lot more valuable. So this model is 
done. And the idea is that we can deploy oops,  
192:38 - if we go back to the actual models, because we 
actually went into the map. So we go back to the
192:47 - auto ml here. I think you can deploy any 
model that you'd like. So you can go here  
192:53 - and deploy this, like if you prefer a 
different model, you could deploy it.
192:58 - If we go into data guard rails, we kind of skipped 
over that this is a way does automatic feature  
193:03 - rotation, so it's extracting the feature, 
so it handles the splitting, how it handles  
193:08 - missing features. Hi, Carbonell nowadays, like 
if you have too much data, it might have to do  
193:15 - dimensionality reduction. So that's just saying 
like, hey, if this is a problem, maybe we would  
193:21 - do some pre processing or stuff to make it 
easier to work with the data. So if we're  
193:25 - happy with this, we can go ahead and deploy it. 
So let's say deploy, just say infer my diabetes.  
193:36 - Here we have aka s and E's. Azure Container 
instance. Let's do Azure Kubernetes  
193:44 - Kubernetes services because we did the other one 
here. Say diabetes. Broad maybe a Ks diabetes.  
193:59 - Oh, compute name, sorry. One of the 
inference ones. Okay. So in order to  
194:07 - deploy this, we would have to create our 
pipeline. I'm not sure if I have enough  
194:11 - in my quota here, but let's go give it a go. So 
I think what it's wanting is one of these here.  
194:19 - I think we'd want this wherever 
we are, right? I'm not sure  
194:26 - where we are. If This Is Us, east or 
west here. Let's go check. Studio.  
194:37 - Azure Machine Learning  
194:42 - hits us. No, I never did this when I was. I 
just use use the Azure Container instance. But  
194:50 - I'm just curious here. say next. My diabetes
195:00 - prod,
195:02 - we will need to choose some nodes.  
195:09 - The number of nodes multiplied by the virtual 
machines, number of cores must be greater or equal  
195:13 - to 12. Okay? Now again, if you're not confident, 
like every concern about costs, you can just  
195:20 - again, watch, you don't have to do right. 
This is again, a fundamental certification,  
195:26 - it's not super important to get all 
the hands on experience yourself.  
195:31 - But I'm just trying to explore this so 
we can see, right, because I don't care  
195:34 - about costs. It's not a big deal to me, on 
my machine here, so probably I don't have  
195:43 - simple must use a vn SKU with more than 
two cores and four gigabytes. Well,  
195:47 - what did I choose? Did I not choose 
the right one? We'll try this again.  
195:59 - Oh, I chose three. Yeah, that's fair. What 
did it want 12 cores set before I think.  
196:19 - Invalid parameters, more details? Because 
that already exists based on that name a two.  
196:28 - It's given us all this trouble I this one 
will go ahead and delete you think like,  
196:33 - it wouldn't matter. Like I wouldn't 
have to delete it out. But that's fine.  
196:39 - This one failed. Now, what's the problem? quota 
exceeded so I can't do it. Because I don't I'd  
196:45 - have to go make a support request increase it. So 
it's not a real big deal. I guess what we could do  
196:51 - is instead of doing it on a KS, we just deploy 
to container instance, if it will let us  
196:57 - notice I don't have to fill anything 
additional. It'll just deploy I think.  
197:04 - Great. And so I guess we'll let that deploy. And 
I'll see you back here in a bit. Okay. Alright,  
197:11 - so I'm back here, checking it out on my are 
checking up on my auto ml here. So if we go  
197:16 - over to compute, we go to inference clusters, 
we don't have anything under there if we go  
197:21 - over to our experiments under our diabetes 
here. Because we did choose to deploy the model.  
197:34 - Right, we clicked deploy. So it should have 
created an ACI instance, let's make our way  
197:44 - over to the portal. The reason why it might not 
be showing up is because I'm just running out  
197:48 - of compute. Because again, it's a quota thing. 
It's not a big deal for us to get a deploy. So  
197:54 - we're gonna do anything with it. But yeah, so 
we can see that we have a container over here,  
197:59 - and it's running. So we must be able to see 
if we go to endpoints here. Here it is. Right,  
198:06 - I was under models as my problem. So 
pipeline endpoints, that would be something  
198:11 - I think that if we had deployed our designer, I 
thought we would have thought under there. But  
198:15 - here we have our binary pipeline, or our diabetes, 
prod pipelines. So if we wanted to like test data,  
198:21 - you know, we could pass stuff in here. I think if 
we wanted to try to just like see this in action,  
198:27 - I'm not sure if it's going to work, but we'll 
give it a go. So if we go into our sample  
198:31 - diabetes data set, and we just explore some of 
the data, we should be able to kind of select  
198:36 - out some values, because I don't know what 
these values mean. So let's just say like 36  
198:41 - oops, 36. But we already know that BMI is the 
major factor here. Sex is either one or two.  
198:47 - So we'll say to BMI, we'll say 25.3. The BP 
will be 83 or whatever. Oops. 83. Here. s 160.  
199:09 - s two can be 99.63 4545 and 5.10.  
199:23 - The only we're running out of metrics here 
82. What do I doesn't give us all them? Oh,  
199:34 - I guess it does. It's up to six. Okay, so 
let's go ahead and test that. So we get and  
199:39 - we got a result back 168. So that is auto ml all 
complete there for you. Yeah, so there you go.  
199:52 - Alright, so let's take a look here at the visual  
199:54 - designer because it's a great 
way to get started very easily.
199:59 - With If you don't know what you're doing, and you 
want something a little bit more advanced than  
200:03 - auto ml and have some customization, it's great to 
start with one of these samples. So let's go ahead  
200:07 - and expand and see what we have here. We have 
binary classification with custom Python script,  
200:12 - tune parameters for binary classification, 
multi class, multi class classification,  
200:17 - so letter recognition, text classification, all 
sorts of things. Usually binary classification,  
200:22 - classification is pretty easy. I'm looking for one 
that is pretty darn simple. Let's go take a look  
200:28 - here. So this says the sample shows how to filter 
based feature selection to selection features.  
200:34 - binary classification, so how to predictors 
related to customer relationships using binary  
200:39 - classes, how to handle imbalanced datasets, 
using smote. And modules, I'm not really  
200:43 - worried about balancing customized Python script 
to perform cost sensitive, binary classification,  
200:50 - tune parameters. So you tune model parameters,  
200:53 - best models during the training process, let's 
go with this one. This one seems okay to me.  
200:58 - And so what you can see here is that it's using 
a sample data set, I believe, I think this is a  
201:03 - sample. And if you wanted to see all of them, you 
can literally drag them out here and do things  
201:08 - with them. I haven't actually built one end to end 
yet for for this again, I don't think it's like  
201:14 - super important for this level of exam. But this 
just shows you that there's a pre built one, if  
201:20 - you've started to get the handle of ml, you know, 
the full pipeline. This isn't too confusing. So  
201:25 - at the beginning, here, we have our classification 
data. And then what it's going to do is say  
201:30 - select columns in the data set. So it says 
exclude column names work class, occupation,  
201:36 - native country, so it's doing some pre processing 
there, excluding that data might be interesting  
201:41 - to go look at that data set. So if we go over 
to our data sets tab, it should show up here,  
201:46 - I believe. Maybe because we haven't committed 
or submitted this, we can't see that data set  
201:54 - yet. But we'll look at it for a moment that we 
want to clean our data. So here's saying clean  
201:59 - all the columns. So custom substitution value, 
see if we can see what it's substituting out.
202:11 - It's not saying what's so clean missing data. So 
I'm not sure what it's cleaning out there. But
202:22 - because that would suggest that it's 
using some kind of custom script,  
202:25 - I'm not sure where it is. But that's okay. We have 
split data, pretty common to split your data. So  
202:30 - you would have a training and test data set, 
it's usually really good to randomize it. So you  
202:35 - want to randomize it, then split it. And that's, 
that's just so you get better results, that it has  
202:42 - model hyper parameter tuning. So the idea is 
that it's going to use ml to figure out the  
202:47 - the best parameters for tuning. Over here we have 
the two classes decision tree where it's going to  
202:52 - do some work there, it's going to score our model, 
and then it's going to evaluate our model and see  
202:57 - if it's successful. So this is all set up to go. 
So all we're going to do is go to the top here,  
203:01 - this is setting wheel here. And we need to choose 
some type of compute. So I'm going to go here,  
203:05 - and we have this one here. But I'm going 
to go create, as for my, my diabetes one,  
203:11 - I'm going to go ahead and make a new one. And 
we're going to say, recommend using a predefined  
203:18 - configuration to quickly set compute training. 
This one looks okay, I don't know if it needs two  
203:26 - nodes. But I guess we can do this one. So we'll 
just say binary was just like binary pipeline.  
203:34 - Okay. Say save, save. Hopefully, it's making 
good suggestion. And we will have to wait for  
203:40 - that to spin up. It's going to take a little bit 
of time. Okay, so I'll see you back here in a  
203:44 - moment. Alright, so I got a little message saying 
that that is ready. So what we can do, I think it  
203:49 - was here, my notebook instance. Now that's not 
it, but I definitely saw a pop up on my screen.  
203:55 - You might have saw it to you that to be paying 
close attention for that. But if you go over,  
204:00 - it says that it's it's ready to go. So what I'm 
going to do is make my way back over here, we're  
204:05 - going to select our compute, there is our binary 
pipeline, I'm going to select that. And there are  
204:11 - some other options, we're not gonna fiddle around 
with that, we're going to go ahead and hit submit.  
204:14 - So we need a new experiment. So I'm going to 
just say, binary pipeline. We'll hit submit.  
204:27 - Okay, and so this is now running. So after a 
little while here, we're going to start seeing  
204:31 - these go green. So this is not started. We'll 
give it a moment here. So we can see some kind  
204:36 - of animation. And there it goes, it's off to the 
races. There's not much to do here. This is going  
204:41 - to take a while. I don't know, I have never ran 
this one in particular. So I don't know if it's  
204:45 - an hour or 30 minutes. So I'll see you back when 
it's done running. But yeah, it's it's not that  
204:51 - fun to watch, but it's cool that you get a visual 
illustration. So I'll see you back in a bit.  
204:56 - I just wanted to peek in here and take a look at 
how it's progressing here and you can see it's  
205:00 - still going and it's just cleaning the data, 
it's still not done. I'm not sure how long  
205:05 - this has been running for if we go over to our 
experiments, and we go into our binary pipeline,  
205:10 - and we look at the runtime, we're about eight 
minutes in, and it hasn't done a whole lot. So  
205:15 - it's still cleaning the data, I would have thought 
a bit, it'd be a little bit faster. I'm kind of  
205:19 - used to using like AWS, and it goes, sage makers. 
This doesn't usually take this long. But I mean,  
205:26 - it's nice that it's, it's going here. But yeah, 
so we're almost out of the pre processing phase.  
205:31 - And we'll be on to the model tuning, okay. 
Alright, so after waiting a little while, it looks  
205:38 - like our pipeline is done. So if we make our way 
over to experiments and go to binary pipeline,  
205:43 - we can see that it took 14 minutes and 22 
seconds, we can go here and just see some  
205:49 - additional information, there's nothing really 
else to see we saw all the steps already ran,  
205:53 - so you can see them all here. Okay, and so let's 
say we want to there's nothing under metrics, but  
206:01 - able to actually slog data points, 
compare these data within across runs,  
206:05 - really did a single run, so there's nothing to 
compare. So let's say we were happy with this,  
206:09 - and we want to deploy this model, what what 
I'm going to do is go back to the designer,  
206:13 - click back here. And so now in the top right 
corner, we can create our inference pipeline. So  
206:21 - I can remember who submits going to 
run it, I don't want to run it again,  
206:26 - I just want to go ahead and create ourselves 
a real time or batch pipeline, let's say real  
206:30 - time bi pipeline here. And what this will 
do is it'll actually create a completely  
206:35 - different pipeline. So here's a completely 
new one. But it's specifically designed to  
206:41 - do deployment. Okay, so this is now one was 
for training the model. This one is actually  
206:46 - for taking in data and doing inference. Okay, so 
what we can do is, we can go ahead and just submit  
206:55 - this. That's it, we'll put this under our binary 
pipeline here. We'll go ahead and hit submit.  
207:03 - And I believe that we need a different kind 
of compute here. I'm surprised that it's  
207:07 - even running. I guess it has a compute 
there. So it's going to run and once it  
207:13 - finishes running that I believe 
that we can go ahead and
207:18 - deploy it. Okay, so let's just wait for that to 
finish. All right. Alright, so after a little  
207:23 - while, there, we ran our inference pipeline. And 
so it's definitely something that is ready for  
207:30 - use. The idea is that what we actually use, it's 
going to go through this web service input to this  
207:34 - web service output, but not so important at this 
level of certification. Let's see what it looks  
207:39 - like to go ahead and deploy it. So yep, we have 
the option between a real time endpoint and an  
207:45 - existing endpoint. We don't have an endpoint 
yet. So we'll just say, binary pipeline.  
207:52 - Okay. And notice we have the option 
between wants it lowercase binary pipeline.  
207:59 - And we have the option between Azure Kubernetes 
service and Azure Container instance,  
208:04 - it's a lot easier to deploy, I think, to container 
instance. So because it will be waiting forever  
208:08 - for Kubernetes to start up. So we're going to do 
container instance, we have some options like SSL  
208:13 - and things like that, not too worried about it. 
So we're just going to go ahead and hit deploy.  
208:19 - Okay. And so that is going to go ahead and deploy 
that. So we'll wait for this real time inference,  
208:28 - if we go over to our compute, it should spin up. 
So this is for Eks. I don't know if it'll show up  
208:37 - here. I think only I've seen things under here. 
But I think this will be for Azure Kubernetes  
208:41 - service. And I don't think we're gonna see it 
show up under there. However, we do not need to  
208:48 - be running this anymore. So we'll go ahead and 
delete the binary pipeline, because we're not,  
208:54 - we don't have it for any use right now. And we 
might need to free it up for something else. Okay.  
209:01 - So go ahead and delete it, we don't need 
it. And coming back to our pipeline,  
209:08 - or designer here, I'm just trying to 
see where we can keep track of it.  
209:16 - I know that it's deploying. So waiting for real 
time endpoint. So I'll see you back here when  
209:20 - this is done. Okay, takes a little bit of time. 
Alright, so I think our pipeline is done. If we  
209:25 - make our way over to endpoints, there it is the 
binary pipeline. If we wanted to go ahead there,  
209:28 - we could test the data. And so it actually 
already has some pre loaded data for us.  
209:35 - We had test. It's nice that it fills it in a 
we get some results back. Okay. So, I mean,  
209:44 - that we see like scored labels and income 
and score probability. So things like that,  
209:49 - that is useful. So it's getting back all all 
the results, but I don't think it has. Yeah,  
209:55 - it doesn't have scored labels and scored 
probabilities, which is the value we want to  
209:59 - come back here. So There are endpoints, and that 
is the end of our exploration with designer, okay?
210:11 - Alright, so let's take a look at what it would 
be to actually train a job programmatically  
210:15 - through the notebook. So remember, we saw these 
samples over here. And so we saw this image  
210:19 - classification, m NIST. And this is a very 
popular data set for doing computer vision.  
210:25 - And these are really great, if you want to really 
learn you should really go through these and just  
210:30 - read through them, because they're, they're 
probably very, very useful. I've done a lot of  
210:34 - this before. So for me, it's, it's just, it's not 
too hard to figure out. But I've actually never  
210:37 - ran this one. So let's run it together. Again, 
we want to be in Jupiter lab. So you can go here  
210:43 - and click it there or go to the compute. If it's 
been a bit finicky. And just here, we'll get a tab  
210:48 - open here. And we'll see how this goes. So what I 
want to do and is just make sure we're back here,  
210:55 - I can click into this one. And we have a few. 
So there's part one, and then we have the deploy  
211:04 - stage. So let's look at training. I don't know 
if we really need to deploy, but we'll give it a  
211:08 - read here. So in this tutorial, you train an ml 
model under compute resource resources will be  
211:13 - training and training and deployment workflow via 
the Azure Machine Learning service. In a notebook,  
211:19 - there's two parts to this. This is using 
the amnesty data set and psychic learn.  
211:24 - And with Azure Machine Learning probably SDK, it's 
a popular data set with 70,000, grayscale images,  
211:29 - each image is handwritten digits of 28 
times by 28 times pixels representing  
211:34 - numbers from zero to nine, the goal is to create 
multi class classifier to divide the digits in a  
211:39 - given image that represents. So we're gonna learn 
a few things here, but let's just jump into it.  
211:44 - So the first thing is that we need to import 
our packages. So here, it does that map  
211:50 - plot plot live in lines, just make sure that 
when we print things that we visually see them,  
211:54 - we're going to NumPy and then matplotlib itself, 
the Azure ML core, and then we're going to import  
212:00 - a workspace since we'll need one there. And 
then I guess it just checks the version making  
212:04 - sure if we have the right version here. Okay, so 
this is one point 28. Zero, it's pretty common,  
212:09 - even as an AWS, they'll have like a script in 
here to update it in case it is out of date.  
212:14 - I'm surprised it didn't include it in here, but 
that's okay. We'll scroll on down. And by the way,  
212:18 - we're using Python 3.6 Azure ML. If this is 
the future, that you know, they might retire  
212:23 - the old one, you're using 3.8. But you know, to 
generally work if it's in their sample data set,  
212:28 - I assume they try to maintain that. Okay, so 
connect to a workspace. So create a workspace  
212:32 - object from an existing workspace reads the file 
config dot JSON. So what we'll do is go run that  
212:38 - I assume it's kind of like a session. And so 
here it says, It's figured found our workplace.  
212:45 - So really, it's just it's not creating 
a workspace, it's just returning  
212:48 - the existing one so that we have it as a variable 
here, create an experiment. So that's pretty  
212:53 - clear. We saw experiments in the auto ml and 
the designer. So we'll just hit run there.  
213:00 - Okay. So we named it core ml. And we said 
experiment. I wonder if it actually created  
213:06 - one yet. Let's go over to experiment to see if 
it's there. So there's there cool, I was fast,  
213:11 - I thought it would like print something out, 
but it didn't do anything there. So creator,  
213:15 - attach an existing compute resource by using Azure 
Machine compute a managed service data scientists,  
213:20 - etc, etc. yada, yada, yada. So create a copy. 
creation of a compute takes about five minutes.  
213:28 - So let's see what it's trying to create. So we 
have some environment variables that wants to load  
213:33 - in I'm not sure how these are getting in here. I'm 
not sure we're environment variables are set in  
213:41 - Jupiter, or even how they get fitted 
in. But apparently they're somewhere.  
213:45 - But we have, it doesn't matter because these 
are defaulting. So here's a CPU cluster,  
213:50 - zero and four, it's going to use a standard D two 
v two, that is the cheapest one that we can run. I  
213:56 - kind of want something a little bit more powerful 
just for myself. Just because I want this to be  
214:00 - done a lot sooner. But again, you know, if you're 
don't have a lot of money, just stick with what's  
214:04 - there. Okay. So and this is CPU clusters. So if we 
go here, I just want to see what our options are.
214:16 - I'm  
214:17 - not sure why it's not showing us options here.  
214:24 - You don't have enough quota for the 
following VM sizes. So it probably  
214:28 - it's because I'm running 
more than one VM right now.  
214:33 - Yeah, so I've said I've hit my quota. Okay, so 
like I probably would have to request for an hour.  
214:39 - So I think this is the one I'm using. What's the 
difference here? This standard dv two v CPUs.  
214:52 - The same one, right? So request quota increase. 
I don't know if this is incident or not,  
214:57 - I'd have to make a support ticket. All that's 
going to take Long. So the thing is, is that  
215:02 - because the reason is is that I'm running the 
auto ml and the designer and the designer in the  
215:07 - background here trying to create all the workshops 
or the, the follow along at the same time. But  
215:12 - what I'll do is I'll just come back and when I'm 
not running one of those other ones, then I will,  
215:17 - I'll come back here and continue on. But we're 
just here at the step, we want to create a new  
215:22 - computer. Okay. All right, so I'm back and I freed 
up one of my compute instances, if I go over here,  
215:28 - now I just have the one cluster instance for 
my auto ml. But what we'll do here is again,  
215:35 - just read through this. So this will create a CPU 
cluster zero to four nodes, standard G two v two,  
215:40 - I guess we'll just stick with what what is here, 
I'm just reading through here, it looks like it  
215:46 - tries to find the compute target, it's going to 
provision it, it will create the cluster called  
215:51 - pool for a minimum numbers of nodes for a specific 
time. So wait for completion. So we'll go ahead  
215:56 - and hit play. And so that's going to go and create 
us a new cluster. So we're just going to have to  
216:02 - wait a little while here for to create about five 
minutes, and I'll see you back here in a moment.  
216:06 - Alright, so the cluster started up, if we go back 
over here, we can see that it's confirmed, I don't  
216:11 - know why it was so quick, but it went pretty quick 
there. So we're on the next section here, explore  
216:16 - the data. So download the emnes data set display 
some sample images. So it's just talking about it  
216:21 - being the open data set. The code retrieves in the 
file data set object, which is a subclass of data  
216:26 - set file data set references a single or multiple 
files of any format in your data store. The class  
216:32 - provides you with the ability to download or mouth 
files to your computer by creating a reference  
216:36 - to the data source location. Additionally, you 
register the data set to your workspace for easy  
216:41 - retrieval. During training. There's a bit more 
how tos, but we'll give it a good read here. So  
216:45 - we have the open data set and missed. It's kind 
of nice that they have that reference there. So  
216:50 - we have a data folder, we make the directory, we 
are getting the dataset, we download it, and then  
216:58 - we are registering it. So let's go ahead and run 
that. Not sure how fast but it shouldn't take too  
217:03 - long as it's running. We'll go over here the left 
hand side refresh, and we'll see if it appears.  
217:13 - Not as of yet. There it is. Go into here, maybe 
explore the data. I'm not sure how it would  
217:20 - look like because these are all images, right? 
Yeah, so they're in you byte Gz. So they're in  
217:26 - compressed files, we're not going to be able to 
see within them but they're definitely there. We  
217:30 - know they're there. So that that is now registered 
into our data set, display some sample images, so  
217:36 - load the compressed into a files into NumPy, then 
use matplotlib plot 30 random images from the data  
217:44 - set from above note, the step requires load 
data function, it's included in the utils. py  
217:48 - file is included in the sample folder, we 
have it over here, we just double click,  
217:53 - very simple file to load data. And we'll go ahead 
and run that. And it's pretty, pretty simple here.  
218:03 - So load data x train x test it are we setting up 
our training and testing data here, it kind of  
218:08 - looks like it because it says train and test data. 
That's when we usually see that kind of split.  
218:14 - And again, it's doing a random split. 
So that sounds pretty good to me.  
218:18 - Let's show some randomly chosen images. Yeah, so 
I guess they do set up the training data here.  
218:24 - And then down below, we're actually showing 
the images. So here's some random images  
218:28 - train on a remote cluster. So for this task to 
submit the job to run on the remote training  
218:32 - cluster to set up earlier submit your job. 
Create the directory, create a training  
218:37 - script created scripts, run configuration, submit 
the job. So first, we'll create our directory.
218:46 - And notice it created this directory over here. 
Because I guess it's going to put the training  
218:51 - file in there. And so this will actually write to 
a training file. This makes quite a bit of sense.  
218:56 - So if we click into here, it should now have a 
training file. It'll just give it a quick read,  
219:01 - see what's going on here. So a lot of times when 
you create these training files you have to do  
219:05 - and this is the same if you're using AWS, like 
when you're creating train, like or Sage maker,  
219:10 - you create a train file because it's part of 
frameworks is just how the frameworks work.  
219:13 - But you'll have these arguments. 
So it could be like parameters to  
219:18 - run for training. And there could be a whole 
sorts of ones here. Here they are loading in the  
219:26 - training and testing data. So it's the same stuff 
we saw earlier when we were just viewing the data.  
219:35 - Here it's doing a logistic regression. It's using 
lib. So linear, maybe linear learning model.  
219:41 - They're sitting multiclass on that there. And 
so what's going to do is fit so fit is actually  
219:48 - performing the training. And then what it's going 
to do is make a prediction on the test set. That  
219:54 - it's going we're going to get accuracy so we're 
getting kind of a score. So notice that it's using  
219:59 - accuracy As a valuation metric, I suppose, right. 
And then at the end, we're going to dump the data,  
220:07 - a lot of times, like you have to save the model 
somewhere. So they're outputting, the actual  
220:11 - weights of the neural network and all other 
stuff. It's a plk file. I don't know what that is.  
220:16 - But if you're using like TensorFlow, you would 
use TensorFlow serving at the end of this,  
220:20 - a lot of times frameworks, like pytorch, or 
TensorFlow, or MX net, they'll have a serving  
220:26 - layer. But since we're just using scikit, learn, 
which is very simple, it's just going to dump  
220:31 - out that file into our outputs, this is going 
to probably run a container. So this outputs  
220:36 - isn't going to necessarily be on the outputs into 
here, it's more like the outputs of the container.  
220:43 - And a lot of times the container will then 
place this somewhere. So like, it'll be saved  
220:49 - on the container. But it'll be passed out 
to the register or, or something like that,  
220:53 - like model registry. So anyway, we ran this, 
so that generated the file, we don't want to  
220:57 - keep on running this multiple times, I probably 
just overwrite the file. So it's not a big deal.  
221:01 - Here, it says notice how the script gets saved 
in the data model. So here, it's saying the data  
221:05 - data folder, I guess we didn't look at that. So if 
we go top here, I didn't see this is data folder.  
221:14 - wasn't really paying attention to where that 
was. Because it looks like where more so it's  
221:19 - loading the data in. So here, it saves the data 
that put anything written to this directory is  
221:23 - automatically uploaded to your workspace. So I 
guess that's just how it works. So it probably  
221:27 - will end up in here then. So you tell py reference 
the training script to load the dataset correctly,  
221:33 - and copy the file over. So we will run this to 
copy the file over. So I'm guessing did it put  
221:43 - it into here? I'm just wondering, yeah, so it just 
put it in there. Because when it actually packages  
221:48 - it for the container, it's going to bring that 
fall over because it's a dependency. So configure  
221:55 - the training jobs. So create a script, run 
config, the directory that contains the script,  
222:00 - the compute target, the training script, train, 
file, etc. Sometimes like in other frameworks,  
222:04 - we'll just call them estimators. But here's just 
called a script run config. So I'm just trying to  
222:12 - see what it's doing. So scikit learn is the 
dependency. Okay, sure. We'll just hit run.  
222:20 - Okay. And then down below here, we have script 
run config. So it looks like we're passing our  
222:28 - arguments that we're saying this is our data 
folder, which is apparently here, we're mounting  
222:33 - it. And then we're setting regularization to 0.5. 
Sometimes you'll pass in dependencies in here as  
222:40 - well, I guess these are technically our parameters 
that are getting configured up here at the top,  
222:46 - right. But sometimes you'll have dependencies 
if you're in it, including other files here.  
222:54 - And I guess that's up here, right? So see where 
it says environment. And then we're saying include  
222:59 - the Azure ML defaults into psychic learn, and 
stuff like that. And so then it gets passed in the  
223:04 - end. Does that make sense to me, we haven't ran 
that yet. Because we don't see any number here.  
223:09 - Submit the job to the cluster. So let's 
go ahead and do that. says it returns a  
223:16 - preparing a running state as soon as the job 
is completed. So it's in a starting state.
223:24 - Monitor remote run. So in total, the first 
run takes 10 minutes, but the second run  
223:29 - is as long as the dependencies and Azure 
ML. Farming don't change the same images  
223:33 - reuse and hence the Start Here Start Time is much 
faster. Here's what's happening while you wait.  
223:38 - The image creation a Docker image is created 
matching the Python environment specified by  
223:43 - the Azure ML environment. The image is built and 
stored in the ACR, the Azure Container Registry  
223:49 - associated with your workspace. Let's go 
take a look and see if that's the case.  
223:53 - Sometimes, like resources aren't visible to 
us, so I'm just curious, do we actually see it?
223:59 - Okay. And Yep, there it is. 
Okay, so that did not lie.
224:07 - So especially your workspace immigration uploading 
takes about five minutes the stage happens once.  
224:13 - For each Python environment. Since the 
containers cache subsequent runs during  
224:16 - image creation, a logs are stemmed to the run 
history, you can monitor the image creation  
224:21 - process process using these logs wherever those 
are, if you if the remote cluster requires more  
224:26 - nodes to execute the run than currently available, 
additional nodes are added automatically. Scaling  
224:31 - takes typically takes about five minutes. And 
I've seen this before, where if you're in your  
224:35 - compute here, and sometimes they'll just say 
like scaling because there's just not enough.  
224:41 - So running in the stage, the necessary scripts 
and files are sent to the compute target than the  
224:46 - data source or a mounted copy. The entry script 
is run Sentry script is actually the train.py  
224:51 - file. While the job is running STD out in the 
files is in the logs directory or stem to the  
224:57 - run history. You can monitor the runs progress 
using In these logs, the dot outputs directory  
225:03 - of the run is copied over to the run history in 
your workspace. So you can access these results.  
225:08 - You can check the progress of a running job in 
multiple ways. This tutorial uses the Jupiter  
225:12 - widget. So looks like we can run this, watch 
the progress. So maybe we will run that. And  
225:19 - so it's actually showing us the progress. 
That's kind of cool. I really like that.  
225:23 - So it's just a little widget, join 
us all the things that it's doing.  
225:27 - Let's go take a look and see what we can 
see under experiments and our run pipeline.  
225:32 - He was talking about things like 
outputs and things like that. So over  
225:35 - here in the outputs and logs, I'm just 
curious. Is it this is the same thing?  
225:47 - I'm not sure if this is the this tails. Yeah, 
it does tail, it just moves so we can actually  
225:52 - monitor it from here. I guess that's what 
it was talking about. so here we can see  
225:58 - that it's setting up Docker, it's actually 
building a Docker image. And then I'm not sure  
226:05 - did it send it to I mean, it's on ACR already. 
I think. It looks like it's still installing  
226:11 - extracting packages. So maybe it's actually 
running on the image now. So just wait there, we  
226:16 - pop back over here. You know, we can see probably 
the same information is identical. Yep, it is.  
226:23 - So we're three minutes in, it's probably 
not that fun to watch it in real time and,  
226:28 - and talk about it. So let's just wait until 
it's done. I'll see you back then. Okay.  
226:33 - Alright, so I'm about 17 minutes in here. I'm 
not seeing any more movement here. So it could be  
226:38 - that it is done. It does say if you run this next 
step here will wait for completion. Specify show  
226:45 - output to true for verbose log. So here actually 
did output a moment ago. So maybe it actually  
226:51 - was done. I just ran it twice. So I'm not sure if 
that's going to cause me issues there. So because  
227:01 - I can't run the next step, unless I stopped 
this guy individually cancel this one here.  
227:10 - I think I can just hit interrupt the colonel, 
there we go. Okay, so I think that it's done.  
227:16 - Okay, because it's 18 minutes in. And I 
don't see any more logging in here. It's  
227:20 - just not very clear. And also, the logs, we 
just have a lot of stuff going on here. Like,  
227:26 - this is so much. So you know, if we were keeping 
keeping pace, we probably would have saw all these  
227:31 - credit. Yeah, so another, we just had a few more 
outputs there. But I think that it's done. Okay.  
227:41 - It's just there's nothing definitively saying 
like, done. Do you know I'm saying and then  
227:45 - up here, it doesn't say, oh, oh, I guess 
it does say that. It's done. All right.  
227:49 - So yeah, I just never ran it with the school. So 
I just don't know. So I guess it does definitively  
227:54 - say that. I already ran this. So we don't need to 
run that. Again. I just feel like we'll get stuck  
227:59 - there. So let's take a look at the metrics. 
So regularization rate is 0.5 accuracy is  
228:07 - nine to nine is pretty good. The last step 
is train the script wrote in the output S.  
228:12 - S, sk learn, I want to see if it's actually in 
our environment here. I don't think it is. So  
228:20 - I'll put this somewhere. It's in our workspace 
somewhere, but it's just not. We just don't  
228:24 - know where it's right here. Okay. So they'll 
put it the actual model right there. And so  
228:32 - you can see the associated files 
that are ran, okay, we'll run it.  
228:37 - register the work model and space that you 
can work with other collaborators. Sure.  
228:41 - So if I click on that here, and we go back over to 
our models, it is now registered over here. Okay.  
228:50 - So we're done part one. I don't want to do all 
these other parts. Training is enough as it is,  
228:56 - but let's just take a look at the deploy 
stage. Okay, so for prerequisites.  
229:04 - We're saying have a workspace we have 
our we are loading our registered model.
229:10 - Okay, we register it where you have 
to import packages, we are going to  
229:17 - create scoring script, deploy to an ACI 
model, test the model, if you want to do this,  
229:23 - you can go through all the steps, it does talk 
about confusion matrix, and that is something that  
229:28 - can show up on the exam is actually talking 
about a confusion matrix. But we do cover that  
229:31 - in lecture content. So you generally understand 
what that is. But, you know, I'm just I'm too  
229:36 - tired. I don't want to run through all this. 
And there's not a whole lot of value other than  
229:40 - reading, reading through it yourself here. 
So I think we're all done here. Okay.
229:50 - Okay, one service we forgot to check out was data 
labeling. So let's go over there and give that a  
229:54 - go. So I'm going to go ahead and create ourselves 
a new project, I'd say my labeling project and  
229:59 - we can say Whether we want to classify images or 
text, we have multi class multi label bounding box  
230:05 - segmentation, let's go with multi class. I'll 
go back here for a second multiclass. Whoops.  
230:13 - I don't know if we create dataset, but we 
could probably upload some local files.  
230:19 - Let's say, my Star Trek dataset does  
230:26 - let us choose the image file type here. Good. 
So these are images. Gonna tell us what here.  
230:36 - It's very finicky this input here. 
file this it references a single  
230:40 - or multiple files in your public data store 
or private public URL. Okay, so we go next.  
230:46 - If we can upload files directly, that'd be nice. 
Ooh, upload a folder. I like that. So what we'll  
230:51 - do is we do have some images in the free AI 
here, under Cognitive Services assets we have
231:01 - we'll go back here and we'll say I 
think objects would be the easiest.  
231:09 - But we just want a folder right? 
So yeah, we'll just take objects.
231:14 - Yep, we'll upload the 17 files. Yep, we'll just 
let it stick to that path. That seems fine to me.  
231:25 - We'll go ahead and create it. And 
so now we have a data set there,  
231:29 - we'll go ahead and select that data set, we'll 
say next, your data says periodically check for  
231:33 - new data points and data points will be added 
as tasks, it doesn't matter. We're only doing  
231:38 - this for test. Enter the list of labels 
that we have TMG DS nine, Voyager tasks  
231:49 - toss. That's the types of Star Trek 
episodes. Label which Star Trek series  
232:05 - The images from, say next. I don't 
want enabled but you can have auto  
232:11 - enabled assistant labeler. I'm gonna 
say No, we'll create the project.  
232:19 - Okay, I'll just wait for that. Great. 
I'll see you back here in a moment. Okay.  
232:23 - All right. So I'm back here actually didn't have 
to wait long. I think it instantly runs. I just  
232:28 - assumed like I was waiting for a state that says 
completed. But it's not something we have to do.  
232:32 - So we have zero to 17 progress, we're going to 
go in here, we're going to go label some data,  
232:37 - we can view the instructions. It's not showing 
up here. But that's fine. If we go to tasks,  
232:41 - we can start labeling. So what season is 
this from or series, this is Voyager, we'll  
232:45 - hit submit. This is Voyager we'll hit submit. 
This is toss, we'll hit submit. This is TMG.  
232:53 - This is TMG. This is DS nine, 
DS nine, Voyager. wager TMG.  
233:06 - Ds nine, you get the idea though, you've got 
some options here like change the contrast, if  
233:11 - someone can't see the photo, or rotate it, this 
is Voyager, Voyager TMG, DS, nine, Voyager,  
233:24 - Voyager. And we're done. So we'll 
go back to our labeling job here,  
233:29 - we'll see we have the breakdown there in our data 
set is labeled. We can export our data set CSV,  
233:36 - cocoa, as your ml data set, I believe that means 
it will go back into the data sets over here. This  
233:42 - will make our lives a little bit easier. Go back 
to data labeling. Okay. So you just grant people  
233:50 - access to the studio, they'd be able to just go 
in here and jump into that job. Okay. If we go  
233:55 - over to the data set, I believe we should have a 
labeled version of it now. So my labeling project.  
234:00 - So I believe that is the labeled stuff here, 
right? Yep, so it's labeled. So there you go.  
234:08 - We're all done Azure machine learning. And 
so all that's left is to do some cleanup.
234:18 - Okay, so we're all done with Azure Machine 
Learning if we want to and go to our compute,  
234:22 - and just kill the services we have 
here. Now, if we go to the resource  
234:26 - group and delete everything, it'll 
take all these things down anyway,  
234:29 - but I'm just gonna go with a paranoid so I'm 
gonna just manually do this, okay. hit Delete.  
234:42 - Okay, so we'll go back to portal dot Azure 
calm. And I'm going to go to my resource groups,  
234:51 - and everything is contained. It should 
be all contained within my studio,  
234:54 - just be sure to check these other ones for that. 
And we can see all the stuff that we spun up.  
234:59 - We'll go ahead and hit Delete resource 
group. I don't know if it includes like,  
235:04 - because I don't see like Container Registry, 
right? So I know like it puts stuff there.  
235:10 - I guess it does. It's this Container Registry. 
So that's pretty much everything right?  
235:14 - And I'll take down everything So, and if you're 
paranoid, all you can do is go to all resources  
235:19 - and double check over here, because if there's 
anything running, it'll show up here, okay?  
235:23 - But that's pretty much it. And so 
just delete and we're all done.  
235:32 - Hey, this is Andrew Brown from exam Pro, and we're 
on to the AI 900 cheat sheet and this one is seven  
235:37 - pages long, so let's get to it. At 
the top of our list, we're starting  
235:41 - with artificial intelligence and machine that 
can perform jobs that mimic human behavior.  
235:45 - Machine learning a machine that gets better at 
a task, explicit programming, deep learning and  
235:49 - machine that has artificial neural nets. Inspired 
by the human brain to solve complex problems. a  
235:54 - data scientist is a person with multidisciplinary 
skills in math statistics, predictive modeling,  
235:59 - machine learning to make future predictions. 
data set is a logical grouping of units of data  
236:04 - that are closely related or share the same data 
structure. Examples of this would be m&s and cocoa  
236:09 - data labeling the process of identifying raw data, 
so images, text files, videos, and adding one or  
236:13 - more meaningful and informative labels to provide 
context to a machine learning model can learn  
236:18 - supervised learning data that has been labeled 
for training, unsupervised learning data that  
236:23 - has not been labeled. An ml model needs to do its 
own labeling, reinforcement learning, so there is  
236:28 - no data and there's an environment and an ml model 
generates data with many attempts to reach a goal.  
236:32 - You have neural networks also abbreviate to nn, 
a network of nodes organized into layers of input  
236:37 - hidden output that is used to train ml models. 
We have deep neural nets. So dnn, a neural net  
236:43 - that has three or more hidden layers to their deep 
learning backpropagation moves backwards through  
236:48 - neural net adjusting weights to improve outcome 
on the iteration. This is how a neural net learns  
236:53 - loss function, a function that compares the ground 
truth to the prediction to determine the error  
236:57 - rate, how bad the network performed, activation 
functions and algorithm applied to a hidden layer  
237:03 - node at that affects connected output. So Arielle 
use a very common one, you have a dense layer,  
237:09 - this is when the next layer increases the 
amount of nodes you have a sparse layer,  
237:12 - this is one of the next layer decreases the amount 
of nodes, you have GPUs that especially designed  
237:17 - to quickly render high resolution images and 
videos concurrently. Commonly used for non  
237:21 - graphical tasks such as machine learning and 
scientific computing. You have CUDA which is  
237:25 - a parallel computing platform and API by Nvidia 
that allows developers to use CUDA enabled GPUs  
237:32 - for general purpose computing, also known as GPU 
GPU. On to the second sheet here for ml pipeline,  
237:39 - we have pre processing, I didn't outline this 
in the course. So I'm going to just do that now.  
237:43 - So preparing data and feature engineering before 
passing data to ml model for training inference,  
237:47 - you might have data cleaning, so this 
is correcting errors within the data set  
237:51 - that could negatively impact the results data 
reduction, reducing the amount of data or applying  
237:55 - dimensionality reduction to reduce 
the dimensions of inputted vectors,  
238:00 - feature engineering, transforming data into 
numerical vectors to be ingested by the ML model,  
238:07 - sampling or resampling pouncing a data set to 
be uniform across labels by adding or removing  
238:14 - records. post processing translate the output 
of an ml model back into human readable format,
238:19 - training.
238:19 - In the process of training the model serving the 
process of deploying the model to an endpoint to  
238:24 - be used for inference inference invoking 
an ml model by sending requests expecting  
238:28 - back a prediction, we have real time endpoints 
that optimize optimize for small or single  
238:33 - item payloads. Returns results quickly usually 
uses a dedicated running server. batch transform  
238:38 - optimized for larger batch predictions server 
runs only for the duration of the batch. There's  
238:44 - forecasting make a prediction with relevant 
data analysts of trends edit stock guessing,  
238:49 - predicting make a future prediction with 
without relevant data using statistics to  
238:53 - predict future outcomes more of guessing 
using decision theory. For performance  
238:57 - and evaluation metrics are used to evaluate 
different machine learning algorithms just to
239:03 - select a few here classification we have accuracy 
f1 score precision recall, for regression metrics  
239:09 - we have MSE, our MSE, ma remember mean squared 
errors okay. Jupyter Notebooks a web based  
239:17 - application for author and documents combined live 
code narrative texts equations of visualizations.  
239:22 - classification is the process of finding 
a function to divide a label data set into  
239:28 - classes and categories. A confusion matrix is 
a table to visualize the model predictions of  
239:33 - predictive versus ground truth actual take the 
time to go look up how confusion matrix work,  
239:38 - because they will absolutely ask you questions 
on the exam for the 900. Okay, regression is  
239:43 - the process of finding a function to correlate a 
labeled data set into continuous variable numbers.  
239:48 - clustering is the process of grouping unlabeled 
data based on similarity and differences. Okay,  
239:52 - on to our third sheet here Cognitive Services 
an umbrella AI service that enables customers  
239:57 - to access multiple AI services with an API key 
an endpoint we have the category of decision so  
240:01 - anomaly detector identify potential problems early 
on content moderator detect potential offensive  
240:08 - or unwarranted content personalizer create 
rich personalized experience for everyone.  
240:13 - language understanding so build natural language 
understanding into the app spots in our devices  
240:18 - q&a maker create a conversational Question and 
Answer layer over the data. Text Analytics. detect  
240:25 - sentiment key phrases and add named entries. 
translator detect translate and more than 90  
240:30 - supported languages. For speech we have speech to 
text transcribe audible speech into readable text  
240:36 - text to speech convert text to lifelike speeches 
for more natural interfaces speech translation  
240:42 - integrate real time speech translation of your 
apps, speak recognition identify verify the people  
240:49 - speaking based on the audio for vision we have 
computer vision so analyze content and images and  
240:54 - videos custom vision customized image recognition 
to fit your business needs, face detect the  
241:00 - detect and identify people and emotions and 
images. Knowledge mining is a discipline in AI  
241:05 - that uses a combination of intelligence services 
to quickly learn from vast amounts of information.  
241:11 - And there's three things to this there's ingest of 
content from a range of sources using connectors  
241:16 - to the first and and third party data stores 
enrich the content with the AI capabilities  
241:21 - that let you extract information find patterns, 
deep understanding explore the newly indexed data  
241:26 - via search boxes, existing business applications 
and data visualizations onto our fourth sheet.  
241:33 - We have Mark stuff AI principles, so this 
is responsible AI remember there's six. So  
241:38 - fairness an AI system should treat all people 
fairly reliability and safety AI systems should  
241:42 - perform reliability and safety, privacy and 
security AI systems should be secure and respect  
241:48 - privacy. inclusiveness AI system should 
empower everyone engaged people, transparency,  
241:52 - AI systems should be understandable accountability 
people should be accountable for the AI systems,  
241:58 - common ml workloads. So for this, we have anomaly 
detection is the process of finding outliers with  
242:04 - a data set called anomaly. Computer Vision 
is when we use ml neural nets to gain high  
242:10 - level understanding of digital images and videos. 
And LP is the is the machine learning that can  
242:15 - understand the contents of a corpus or body of 
text. conversational AI is technology that can  
242:20 - participate in conversations with humans. 
I know it feels like we're repeating the  
242:23 - same thing quite in different ways. But that's 
the way we're going to learn well here, okay.  
242:27 - Azure Machine Learning service allows you to 
provision ml studio to build and maintain ml  
242:31 - models and pipelines. We have authors so 
either that we have notebooks that Jupyter  
242:36 - notebooks and ID to write Python code to 
build ml models. Remember, you can launch  
242:41 - it in Jupiter labs and VS code as well probably 
once you have an example just so you know,  
242:45 - auto ml completely automated process 
to build and train an ml model,  
242:50 - designer visual drag and drop designer construct 
and build pipelines. We have data sets of data  
242:56 - that you can upload, which will be used for 
training data can be versioned. Open datasets  
243:01 - are publicly hosted datasets are commonly used for 
learning how to build ml models. experiments are  
243:07 - logical grouping of runs, runs our ML tasks that 
perform on virtual machines or containers pipeline  
243:13 - so ml workflows you have built or have used in 
the designer, you have a training pipeline. So  
243:18 - pipelines to build in Train and ml model inference 
pipelines pipelines that are used to train  
243:23 - that use a trained model to make a prediction 
on real data. Then you have models as a model  
243:28 - registry containing trained models that can be 
deployed endpoints. When you deploy a model,  
243:33 - it's hosted on an accessible endpoints the REST 
API. So real time endpoints invokes an ml model  
243:37 - for inference pipeline endpoint, invokes 
that running on a pipeline. So for ci CD,  
243:43 - under manage, we have compute the underlying 
computing instances used for notebooks,  
243:47 - training inference, so we have compute instances, 
that all workstations that data scientists use to  
243:51 - work with the data models. This is generally 
for your notebooks, computer clusters,  
243:55 - scalable clusters for virtual machines on demand 
processing of experimental code, so training  
244:00 - and pre processing, inference clusters, deployment 
targets for predictive services that use for train  
244:05 - models. So for inference, attached compute links 
to existing Azure compute resources, such as  
244:10 - virtual machines, Azure data, bricks, clusters, 
there's another one in there, but it's not gonna  
244:14 - show up an exam probably a patchy Spark, 
but I guess it's covered under databricks.  
244:19 - So for environments that reproduce pre 
reproducible Python environment for machine  
244:23 - learning experts or experiments, data stores 
securely connect to your storage service on Azure  
244:31 - without putting your authentication credentials 
in so it has Blob Storage file share data,  
244:36 - data lake storage Gen two, as your SQL data as 
your Postgres MySQL database data labeling have  
244:42 - humans and ml assisted labeling to label your data 
for supervised learning human and loop labeling,  
244:47 - machine learning since the data data labeling, 
we have linked services so external services that  
244:53 - you can connect to a workspace such as Azure 
synapse analytics, I think that's the only  
244:57 - way you can connect right now, then for text 
analytics. So now we're out of the Azure Machine  
245:03 - Learning Services, we're ingesting the cognitive 
services so text analytics, sentiment analysis,  
245:09 - find out what people think of your brand or 
topic. Labels include negative, positive, mixed  
245:14 - or neutral confidence scores ranging from zero 
to one opinion mining granular information about  
245:18 - the opinions related to aspects granular data 
with a subject and opinion tied to a sentiment.  
245:23 - key phrase extraction quickly identifies the 
main concepts in text. Language detection  
245:27 - detects the language, input text is written in 
named entity recognition. ner detects words and  
245:33 - phrases mentioned in unstructured text that can 
be associated with one or more sentiment types.  
245:38 - We have Louis or Luis language understanding 
a no code ml service to build natural language  
245:43 - into apps, bots and IoT devices. Use NLU the 
ability to transform a linguistic statement to  
245:49 - a representative that enables you to understand 
your users naturally, Louis key schemas component  
245:54 - so we have intentions to user what the 
user is asking for. So Louis app contains  
245:58 - a nun intent entities, what parts of the entity 
intent is used to determine the answer utterances  
246:05 - examples of the user input that includes intent 
and entities who trained the ML model to match  
246:10 - predictions against the real user input for 
q&a maker generate a bot from a URL PDF,  
246:16 - and it's supposed to be do cx for docs, that's 
a spelling mistake, then go the doc x file,  
246:22 - multi turn conversation, so follow up 
prompts to narrow a specific answer  
246:26 - to chat personalized canned responses for 
Azure bot service allow you to host bots.  
246:31 - So you have the Bot Framework SDK, which is an 
end to end SDK to build, test, publish, connect,  
246:36 - evaluate bots, that's the entire pipeline that 
they describe Bot Framework composer a desktop  
246:42 - application to design bots, leverage the Bot 
Framework SDK, so there you go, that's the  
246:48 - whole cheat sheet. Usually, I would break it up 
for service but there's a lot of intermixing. So  
246:53 - that's why I did it this way. But, you know, 
good luck on your exam, and I hope you pass

Cleaned transcript:

Hey, this is Andrew Brown, your cloud instructor exam Pro, and I'm bringing you another complete study course. And this time, it's the Azure AI fundamentals made available here on Free Code Camp. So this course is designed to help you pass the exam and achieve Microsoft issued certification. And we're going to do that by providing you great lecture content, follow along to get that hands on experience, and cheat sheets for the day of your exam. So when you do get that certification, you can put it on your resume or LinkedIn, and show that you have that ASHRAE knowledge to get that cloud job or get that promotion. So I want to introduce myself, I'm previously the CTO of multiple edtech companies with 15 years, industry experienced five years specializing cloud service community hero, and I've published many free cloud courses. I love Star Trek and coconut water. So I just want to take a moment here to thank viewers like you, because it's you that make these free courses possible. And if you want to support more free courses, just like this one, a great way to do that is buying our additional study materials on exam pro.co. And for this exam, it's Ford slash ai 900. This will get you access to study notes, flashcards, quizlets, downloadable cheat sheets and practice exams. And you'll also be able to ask questions and get some learning support. So if you want to keep up to date with any of the more courses I am releasing, you can follow me on twitter at Andrew, Andrew route 10 share with me when you pass the exam, or what you might like to see as the next course. So there we go. Let's get to it. Hey, this is Andrew Brown from exam Pro. And we are at the start of our journey here learning about the AI 900 asking the most important question which is what is the a 900. So the Azure AI fundamentals certification is for those seeking in ML roles such as AI engineer or data scientists. And the certification will demonstrate a person can define and understand Azure cognitive services, ai concepts, knowledge mining, responsible AI basis of ml pipelines, classical ml models, auto ml and Azure ML studio. So you don't need to know super complicated ml knowledge here, but definitely helps to get you through there. But yeah, so this certification is generally referred to by its course code, the AIA 900 into the natural path for the Azure AI engineer or Azure Data Scientist certification. And this generally is an easy course to pass. It's great for those new to cloud or ml related technology, looking at our roadmap, you might be asking, okay, well, what are the paths? And what should I learn. And so here are my markers. And let's get out the annotation tool or laser pointer to see where we can go. Now if you already have your az 900. That's a great starting point before you take your ai 900. If you don't have your az 900, you can jump right into the 900. But I strongly recommend you go get that az 900 because it gives you General, General, foundational knowledge, it's just another thing that you should not have to worry about, which is just how to use Azure at a fundamental level. Do you need the DP 900 to take the 900 No, but a lot of people seem to like to to go this route where they want to have that data foundation before they move on to a to the AI 100 because they know that that is just broad knowledge is going to be useful there. So you know, it is apparent that you see a lot people getting the AI 900 and the DPI 900. Together, vana 100, the path is a little bit more clear, it's either going to be data scientists or AI engineer. So AI engineer is just the cognitive services turned up to 11, you have to know how to use the AI services in and out for data scientists is more focused on setting up actual pipelines, and things like that within the Azure Machine Learning Studio. So you just have to decide which path is for you. The data scientist is definitely harder than the AI engineer, I think the coaches was updated. So I just updated that to 102. And I think the AI engineers to be two separate, you had to take two separate courses. But now it's just a single one. So it's unified. But you know, if you aren't ready for the data sciences, some people like taking the AI engineer first and then doing the data scientist. So this is kind of like a warm up. Again, it's not 100% necessary, but it's just based on your personal learning style. And a lot of times people like to take the data engineer after the data scientists just to round out their complete knowledge. Now, if you already have the az 900 and the associate, you can safely go to the data scientist if you want to risk it, because this one is really hard. So if you've passed the easy one before, you know, you're gonna probably have a lot more confidence, learning about this stuff, all this fun foundational stuff at this level here. But of course, it's always recommended to go grab these foundational certs because sometimes course materials just do not cover that information. And so the obvious stuff is going to get left out. Okay. So moving forward here. So how long should you study to pass for the 900? Well, if you have one year's experience with Azure, you're looking at five hours as little as five hours could be up to 10 hours. If you have passed the az 900 to dp 900. Around 10 hours is the average. If you're completely new to ml AI, you're looking at 15 hours, this could get extended to 20 to 30 Again, it just depends on how green you are like how new you are to these concepts. But you know, I think on average, we're looking at 15 hours, the recommended study time is 30 minutes a day for 14 days should get you through it. You know, but, you know, just don't over study and just don't spend too little time, you know. So where do you take this example, you can take it in person at a test center or online from the convenience of your own home. So there's two popular test centers, there's psi and Pearson VUE, well, and I should say these are theirs. These are not necessarily test centers, per se, but they are a collection of test centers that are partnered with psi Pearson VUE so that you can easily take it at a local test center. If you ever heard the term Proctor that is that means a supervisor person that is monitoring you while you're taking the exam to them. When we talk about online exams, they'll say proctored exams to refer to the online component, if I had the option to meet in person online, always the in person because it's a controlled environment, it's way less stress us stress stressful. And, you know, online, there can so many things can go wrong. So you know, but it's up to your personal preference and your situation. Okay? What does it take to pass the exam? Well, you got to watch the lectures, and memorize key information, do hands on labs and follow along with your own Azure account, I would say that you could probably get away with just watching all the videos in this one without having to do but again, you know, it really does reinforce information. If you do take the time there. There is some stuff that is an Azure Machine Learning Studio, you might be wary of launching because we do have to run instances and they will cost money. So if you if you feel that you're not comfortable with that, just watching you should be okay. But when you get into the associate tier you absolutely you just have to expect to pay something to to learn and take that risk, okay? You want to do paid online practice exams that simulate the real exam. So I do have paid practice exams that accompany this course that are on my platform exam Pro. And that's how you can help support more of these free courses. Can you pass this without taking practice exam Asher's a little bit harder? If this is an AWS exam? I'd say yes. for Azure. It's kind of risky, the easy 100 Sure. Ai 900 dP 900 sC 900? No, I think you should get a practice exam, at least one, or go through the sample one, there's a sample one probably looking around for on the Azure website. Let's just look at the exam guide break down here very shortly. And then in the following video, we'll look at in more detail. So it's broken down into the following domains to describe AI workloads and considerations describe fundamental principles of machine learning on Azure describe features of computer vision workloads on Azure, describe features of natural language processing workloads on Azure, describe features of conversational AI workloads on Azure. And I want you to notice it says describe, describe, describe, describe, describe, that's good, because that tells you it's not going to be super, super hard, right? If you start seeing things that say, beyond describing identify, then you know, it's going to be a bit harder, okay? The passing grade here is 700 out of 1000. So that's around 70 70%, I would say around because you could possibly fail with 70%. Because these things work on scaled scoring. For response types, there's about 40 to 60 questions, and you can afford to get 12 to 18 questions wrong. I put an asterisk there because there's not always just one question per like, per section, but I'll talk about the here in a second. So some questions are worth more than one point. There's no penalty for wrong questions. Some questions cannot be skipped. And the format of questions can be multiple choice, multiple answer, drag and drop hot area case studies. Case Studies. I don't remember I don't think I saw a case study on mind. But case studies will have a series of questions, a series of questions that make up or come back to a particular business problem. And so those are very interesting. That's why we have that asterik up here, okay. So for the duration, you get one hour, that means about one minute per question. The time for this exam is 60 minutes, your seat time is 90 minutes, seat time refers to the amount of time that you should take to allocate for that exam. So this includes time to review the instructions, read, accept the NDA, complete the exam and provide feedback at the exam. This is going to be valid for 24 months up to two years before we have certification. And, you know, we'll proceed to the full exam guide now. Okay. Hey, this is Andrew Brown from exam Pro. And what we've pulled up here is the official exam outline on the Microsoft website. If you want to find this yourself, just got to type in ai 100, Azure or Microsoft, you should be able to easily find it, the page looks like this. And what I want you to do is scroll on down because we're looking for skills measured and from there, we're going to download the skills outline. And once we have that open, you might want to bump up the text. And so what you'll always see in these documents is a red text at the top saying hey, we've updated the track as your loves updating their courses with minute updates that don't generally affect the outcome of the study. But it does get a lot people worrying. So we say, well, is your core set of data? So no, no, they're just making minor changes. Because they'll do this like five times a year. And so if there was a major revision, what would happen is they would change it. So instead of being the AI 900, to be like, the AI 901, or 9902, we saw that recently with the, the AI 100 words now the ai, ai 102 or 103. Sorry. So you know, just watch out for those. And if it's a major revision, then yes, the course you would need a completely new course, and it would not match. But for minors, it's going to be my new thing. So if we scroll on down, and a lot of times, they'll just cross out what they've changed in this one in particular, they did not show us in detail, you'd have to read through the comparison. But we'll look at the new listing here. and work our way through here is to describe artificial or AI workloads and considerations. So here we just kind of describing the generalities of AI. So prediction forecasting, this is because when we use auto ml prediction would be classification and regression and forecasting would be that real time series forecasting, I suppose, identity features anomaly detection. So not a lot in the exam for this. So we we touched on it briefly, computer vision workloads, there's a lot of stuff under computer vision, as you'll find out through the course, and LP and knowledge, mining workloads, conversational AI, workloads. And again, these are all the concepts, not how to use the services, then you have the responsible AI section. And so Microsoft has these six principles that they really want you to know. And they push it throughout all their AI services. So those are the 16 liter. Now, they're not that hard to learn, then describe fundamental principles of machine learning on Azure. So here, it's just describing regression, classification and clustering. We have a lot of practical experience with these in the course. So you will understand at the end what these are used for. For core machine learning concepts, we can identify features and labels in a data set. So that's their data labeling service, describe how training validation data sets are used in machine learning. So we touch on that describe how machine learning algorithms are used for training, select and interpret model valuations of metrics for classification regression, a lot of these deal categories in auto ml because it automatically does it, but we can see how it does that. Okay. Well, I think having to do it ourselves, identify core tasks and creating a machine learning solution. So describe common features of data ingestion, preparation, feature engineering, selection, features of model training, evaluation, features of model deployment and management. And then we have described no code solution. So auto ml, they like to call it automated. ml, but really, the industry just calls it auto ml. Then there's the designer for building pipelines. Here's where we see some changes. So identify features of image classification, features of object detection solution. So semantic segmentation is gone, which is great, because I don't even know what that is. So it's great that it's out there, OCR solutions, and then you have face detection. Then under computer vision tasks, we have computer vision, custom vision, face services form recognizer tones. There's a lot around computer vision. For NLP we have key phrase extraction, identity recognition, sentiment analysis, language modeling, speech recognition, synthesis, this one doesn't really appear much. It's kind of a concept not so much something we have to do. Then there's translation. We have nlp, nlp stuff. So text analytics, Luis, or Louis, I'm not sure which way to pronounce it. Speech service and text, translator text. Then down below below, we have a conversational AI. So building out web chat bots, and characters of conversation AI solutions looks like these two have telephone and personal digital assistants not sure why they decided to remove that. But that's okay. I think that's fine. q&a maker and Azure bot, I really like this service, by the way. So yeah, there we go. That is the outline. And now we'll jump into the actual course. Hey, this is Andrew Brown from exam Pro. And we are looking at the layers of machine learning. So here I have this thing that looks like kind of an onion. And what it is, it's just describing the relationship between these ml terms related to AI, and we'll just work our way through here starting at the top. So artificial intelligence, also known as AI is when machines that perform jobs that mimic human behavior. So it doesn't describe how it does that. But it's just the fact that that's what AI is one layer underneath we have machine learning. So machines that get better at a task without explicit programming. Then we have deep learning. So these are machines that have an artificial neural network inspired by the human brain to solve complex problems. And if you're talking about some of that actually assembles either ml or deep learning models or algorithms that's a data scientist or person with multidisciplinary skills and math statistics, predictive modeling machine learning to make future predictions. So what you need to understand is that AI is just the outcome, right? And so AI could be using m Underneath, or deep learning, or a combination of both or just FL statements, okay? Alright, so let's take a look here at the key elements of AI. So AI is the software that imitates human behaviors and capabilities. And there are key elements according to Azure or Microsoft as to what makes up AI. So let's go through this list quickly here. So we have machine learning, which is the foundation of an AI system that can learn predict like a human, you have anomaly detection. So detect outliers or things out of place, like a human computer vision, be able to see like a human natural language processing, also known as NLP, be able to process human languages and refer a context, you know, like a human, conversational AI be able to hold a conversation with a human. So, you know, I wrote here, according to Microsoft and Azure, because you know, the global definition is a bit different. But I just wanted to put this here, because I've definitely seen this as an exam question. And so we're going to have to go with Asher's definition here. Okay. Let's define what is a data set. So a data set is a logical grouping of units of data that are closely related to or share the same data structure. And there are publicly available datasets that are used in learning of statistics, data analytics and machine learning. I just want to cover a couple here. So the first is the M NIST database. So images of handwritten digits use to test classify cluster image processing algorithms commonly used when learning how to build computer vision ml models to translate handwritten into or handwriting into digital text. So it's just a bunch of handwritten numbers and letters. And then another very popular data set is the common objects in context cocoa dataset. So this is a dataset which contains many common images using a JSON file, cocoa format that identify objects or segments within an image. And so this data set has a lot of stuff in its object segmentations recognition and IP context, super pixel stuff, segmentation, they have a lot of images, and a lot of objects. So there's a lot of stuff in there. So why am I talking about this, and in particular cocoa data sets? Well, when you use Azure Machine Learning Studio, it has a daily data labeling service. And the thing is, is that it can actually export out into cocoa format. So that's why I want you to get exposure to what cocoa was. And the other thing is, is that when you're building out Azure Machine Learning pipelines, you they actually have open datasets, as you'll see later in the course, that shows you that you can just use very common ones. And so you might see m NIST and the other one there. So I just wanted to get you some exposure. Okay. Let's talk about data labeling. So this is the process of identifying raw data, so images, text files, videos, and adding one or more meaningful and informative labels to provide context so a machine learning model can learn. So with supervised machine learning, labeling is a prerequisite to produce training data. And each piece of data will generally be labeled by a human. The reason why I say generally here is because with Azure Data labeling service, they can actually do ml assisted labeling. So with unsupervised machine learning labels will be produced by the machine and may not be human readable. And then one other thing I want to touch on is the term called ground truth. So this is a proper, a properly labeled data set that you can use as the objective standard to train and assess a given model is often called ground truth, the accuracy of your train model will depend on the accuracy of your ground truth. Now using Azure as tools I've ever seen and used that word ground truth, I see that a lot in AWS, and even this graphic here is from AWS. But I just want to make sure you are familiar with all that stuff. Okay. Let's compare supervised unsupervised and reinforcement learning. Starting at the top, we got supervised learning, this is where the data has been labeled for training. And it's considered task driven, because you're trying to make a prediction get a value back. So when the labels are known, and you want a precise outcome, when you need a specific value returned, and so you're going to be using classification and regression in these cases. For unsupervised learning, this is where data that has not been labeled, the ML model needs to do its own labeling. This is considered data driven. It's trying to recognize a structure or a pattern. And so this is when the labels are not known. And the outcome does not need to be precise when you're trying to make sense of data. So you have clustering, dimensionality reduction and Association. Have you ever heard this term before? The idea is it's trying to reduce the amount of dimensions to make it easier to work with the data. So make sense of the data, right? We have reinforcement learning. So this is where there is no data. There's an environment and an ml model generates data and makes many attempts to reach a goal. So this is considered decisions driven. And so this is for game AI learning tasks robot navigation, when you've seen someone code In a video game that can play itself, that's what this is. If you're wondering, this is not all the types of machine learning. And these specific, unsupervised and supervised is considered classical machine learning because they have heavily rely on statistics and math to produce the outcome. But there you go. So what is a neural network? Well, it's often described as mimicking the brain, it's a neuron or node that represents an algorithm. So data is inputted into a neuron and based on the output, the data will be passed to one of many connected neurons, that it connections between neurons is weighted, I really should have highlighted that one that's very important. The network is organized into layers, there will be an input layer, one too many hidden layers and an open layer. So here's an example of a very simple neural network. Notice the nn, a lot of times you'll see this in ML as an abbreviation for neural networks. And sometimes neural networks are just called neural nets. So just understand that's the same term here. What is deep learning? This is a neural network that has three or more hidden layers, it's considered deep learning, because at this point, it's it's not human readable to understand what's going on within those layers. What is Ford fried, so neural networks, where they have connections between nodes that do not form a cycle, they always move forward. So that's just describes a forward pass through the network, you'll see fn n, which stands for forward feed neural network just described that type of network, then there's about back propagation, which are in forward feed networks. This is where we move backwards through the neural net, adjusting the weights to improve the outcome on next iteration. This is how a neural net learns. The way the backpropagation knows to do this is that there's a loss function. So a function that compared the ground true to the prediction to determine the error rate how bad the network performs. So when it gets to the end, it's going to perform that calculation, and then it's going to do its back propagation, adjust the weights, then you have activation functions, I'm just going to clear this up here. So activation functions. They're an algorithm applied to a hidden layer node that affects connected output. So for this entire hidden layer, they'll all have the same one here and just kind of affects how it learns and like how the weighting works, so it's part of backpropagation. And just the learning process, there's a concept of dense so when the next layer increases the amount of nodes and you have a sparse so when the next layer decreases the amount of notes. Anytime you see something going from a dense layer to a sparse later, that's usually called dimensional dimensionality reduction because you're reducing the amount of dimensions because the amount of nodes in your network determines the dimensions you have. Okay. What is a GPU? Well, it's a general processing unit that is specially designed to quickly render high resolution images and videos concurrently. GPUs can perform parallel operations on multiple sets of data. So they are commonly used for non graphical tasks such as machine learning, and scientific computation. So a CPU has an average of four to 16. processor cores, a GPU can have 1000s of processor cores, so something that has 48 GPUs can have as many as 40,000 cores. Here's an image I grabbed right off the Nvidia website. And so it really illustrates very well, like how this would be really good for machine learning or neural networks. Because neural networks have a bunch of nodes. They're very repetitive tasks, you can spread them across a lot of cores, that's gonna work out really great. So GPUs are suited for repetitive and highly parallel computing tasks such as rendering, graphics, cryptocurrency mining, deep learning and machine learning. We're talking about CUDA before we can let's talk about what Nvidia is. So Nvidia is a company that manufactures graphical processing units for gaming and professional markets. If you play video games, you've heard of Nvidia. So what is CUDA? It is the compute unified device architecture. It is a parallel computing platform and API by Nvidia that allows developers to use CUDA enabled GPUs for general purpose computing on GPUs. So GP GPUs, all major deep learning frameworks are integrated with Nvidia deep learning SDK. The Nvidia deep learning SDK is a collection of Nvidia libraries for deep learning. One of those libraries is the CUDA deep neural network library. So cu dnn so CUDA, RC UD and n provide provides highly tuned implementations for standard routine such as forward and back convolution convolutions really great for computer vision, pooling normalization activation layers. So, you know, in the Azure certification for the AI 900. They're not going to be talking about CUDA. But if you understand these two things, you'll understand why GPUs really matter. Okay. All right, let's get a easy introduction into machine learning pipeline. So this one is definitely not an exhaustive one, and we're definitely gonna see more complex ones throughout this course. But let's get to it here. So starting on the left hand side, we might start with data labeling. This is very important when you're doing supervised learning because you need to to label your data set, the ML model can learn by example, during training, this stage and the feature engineers nearing stage or is considered pre processing because we are preparing our data to be trained for the model. When we move on to feature engineering, the idea here is that ml models can only work with numerical data. So we need to translate it into a format that it can understand. So extract out the important data that the ML model needs to focus on. Okay, then there's the training step. So your model needs to learn how to become smarter, it will perform multiple iterations getting smarter with each iteration, you might also have a hyper parameter tuning step here, it says tuning but should say tuning. But the ML model can have different parameters. So you can use ml to try out many different parameters to optimize the outcome. When you get to deep learning, it's impossible to tweak the parameters by hand. So you have to use hyper parameter tuning, then you have serving, sometimes known as deploying. But you know, when we say deploy, we talked about the entire pipeline, not necessarily just the ML model step. So we need to make an ml model accessible. So we serve it by hosting in a virtual machine or container. When we're talking about Azure machine learning, it's either going to be an Azure Kubernetes service or Azure Container instance. And you have inference. So inference is the act of request, requesting to make a prediction, so you send your payload with either CSV or whatever, and you get back the results, you have a real time endpoint and batch processing. So real time, it's just there they can batch can be real time as well. But generally, it's slower. But the idea is that do I? Am I making a single item prediction? Or am I giving you a bunch of data at once. And again, this is a very simplified ml pipeline, I'm sure we'll revisit ml pipeline later in this course. So let's compare the the terms forecasting and prediction. So forecasting, you make a prediction with relevant data. It's great for analysis of trends, and it's not guessing. And when you're talking about prediction, this is where you make a prediction without relevant data, you use statistics to predict future outcomes, it's more of guessing. And he uses decision theory. So imagine you have a bunch of data. And the idea is you're going to infer from that data, okay, maybe it's a, maybe it's B, maybe it's C. And for prediction, you don't have really much data, so you're going to have to kind of invent it. And the idea is that you'll figure out what the outcome is there. These are extremely broad terms, but you know, just so you have a high level view of these two things, okay. So what are performance or evaluation metrics? Well, they are used to evaluate different machine learning algorithms. So the idea is, you know, when your machine learning makes a prediction, these are the metrics you're using to evaluate to determine, you know, is your ml model working as you intended. So for different types of problems, different metrics matter, this is absolutely not an exhaustive list. I just want you to get you exposure to these words and things so that when you see them you go, Okay, I'll come back here and refer to this. But lots of these are just it's not you it's not necessarily remember but classification metrics you should know. So classification, we have accuracy, precision recall f1 score, rockin AUC. For regression metrics. We have MSC, our MSC ma ranking metrics, we have MLR, DCG and DCG. Statistical metrics, we have correlation, computer vision metrics, we have psnr, SSI m, IOU and LP metrics, we have perplexity blue Meteor rogue, deep learning related metrics. We have Inception score, I cannot say this person's name, but or I'm assuming it's a person but this Inception distance. And there are two categories evaluation metrics, we have internal evaluation. So metrics used to evaluate the internals of an ml model. So accuracy f1 score precision, recall, I call them the famous for using all kinds of models and external evaluation metrics used to evaluate the final prediction of an ml model. So yeah, don't get too worked up here. I know that's a lot of stuff. The ones that matter, we will see again, okay. Let's take a look at Jupiter notebook. So these are web based applications for authoring documents to combine live code narrative text equations, visualizations. So if you're doing data science, or you're building ml models, you apps that are going to be working with Jupyter notebooks. They're always integrated into cloud service providers ml tools. So Jupyter Notebook actually came about from ipython. So ipython is the precursor of it. And they extracted that feature out it became Jupyter Notebook I bought ipython is now a kernel to run Python. So when you execute Python code here, it's using ipython, which is a version of Python Jupyter Notebooks were overhauled and better integrated into an ID called Jupiter labs, which we'll talk about here in a moment. And you generally want to open notebooks in labs, the legacy web based interfaces known as Jupiter classic notebooks. This is what the old one looks like you still open them up, but everyone uses Jupiter labs now. Okay, so let's talk about Jupiter labs. Jupiter Labs is the next generation web based user interface, all familiar features of the classic Jupyter Notebook is in a flexible, powerful user interface. It has notebooks, a terminal, a text editor, a file browser, rich outputs, Jupiter labs will eventually replace the classic Jupyter notebooks. So there you go. We keep mentioning regression, but let's talk about it in more detail here. So we kind of understand the concept. So regression is the process of finding a function. to correlate a labeled data set gnosis is labeled, that means it's going to be for supervised learning into a continuous variable number. So another way to say it is predict this variable in the future. So the future is just means like that continuous variable doesn't have to be time. But that's just a good example of regression. So what will the temperature be next week? So we will be 20? Celsius? How would we determine that? Well, we would have vectors, so dots, they're plotted on a graph that has multiple dimensions, the dimensions could be greater than just x and y, you could have many. And then you have a regression line. This is the line that's going through our data set. And and that's going to help us figure out how to predict the value. So how would we do that? Well, we we need to calculate the distance of a vector from the regression line, which is called an error. And so different regression algorithms use the error to predict different variable a future variable. So just to look at this graphic here, so here's our regression line. And here is a dot like a vector or a piece of information. And this distance from the line that the actual distance is what we're going to use in our ML model to figure out if we were to plot another line up here, right? You know, we compare this line to all the other lines, okay? And that's how we find similarity. And what will commonly see for this is mean squared error, root mean squared error, mean? absolute error. So MSE, mrsc, and Ma. Okay. Let's take a closer look at the concepts of classification. So classification is the process of finding a function to divide a labeled data set. So again, this is supervised learning into classes or categories, so predict a category to apply to the inputted data. So will it rain next Saturday, will it be sunny or rainy? So we have our data set. And the idea is we're drawing through this a classification line to divide the data set. So regression we're measuring the line to or the vectors to the line. And this line is just what side of the line is that on? If it's on this side, then it's sunny. If it's on this side, it's rainy. Okay. For classification algorithms, we got large logistic regression decision trees, random forests, neural networks, Naive Bayes, k nearest neighbor, also known as k and n, and support vector machines. svms. Okay. Let's take a closer look at clustering. So clustering is the process of grouping unlabeled data. So unlabeled data means it's unsupervised learning based on similarities and differences. So the outcome could be grouped data based on similarities or differences. I guess it's the same description up here. But imagine we have a graph and we have data. And the idea is we draw boundaries around that to see similar groups. So maybe we're recommending purchases to Windows computers, or recommending purchase to Mac computers. Now remember, this is unlabeled data, so the label is being inferred, or, or they're just saying these things are similar, right? So clustering algorithms, we got k means K, mi dois, identity base hierarchial. Okay. Hey, this is Andrew Brown from exam Pro. And we're looking at the confusion matrix. And this is a table to visualize the mall predictions, the predicted versus the ground truth labels, the actual also known as that error matrix, and they're useful for classification problems to determine if our, if our classification is working as we think it is. So imagine we have a question how many bananas did this person eat or these people eat? And so we have this kind of box here where we have predicted versus actual, and it's really comparing the ground truth, and what the model predicted, right? And so on the exam, they'll ask you questions like, Okay, well imagine that. And they might not even say yes or no, maybe like zero and one. And so what they're saying is, you know, imagine you have, you want to tell us the true positives, right? And so the idea is, they won't show you the labels here, but you know, one in one would be a true positive and zero and zero would be a false negative. Okay? Another thing they'll ask you about these confusion matrix is, is the size of them. So the idea is that we're looking right now at a oops, just gonna erase that there. But we're looking at a binary classifier because we have one label and just two labels, right, one and two, okay, but we you could have three say one, two, and three. So how would you calculate that well, Be a third cell over here you know and so it's gonna be an excellent predictive because we're only gonna have ground truth versus prediction. And so that's how you'll know it will be six the size will be six might not say cells, but we'll just say six. Okay. So to understand anomaly detection, let's define quickly what is an anomaly so an abnormal thing that is marked by deviation from the norm or standard. So, anomaly detection is the process of finding outliers within a data set color anomaly so detecting when a piece of data or access patterns appear suspicious or malicious. So use cases for anomaly detection can be data cleaning, intrusion detection, fraud detection, system health monitoring, event detection and sensory or sensor networks, ecosystem disturbances, detection of critical and cascading flaws, anomaly detection is by hand is a very tedious process of using ml for a knowledge section is more efficient and accurate. And Azure has a service called anomaly detector detects anomalies in data to quickly find, quickly identify and troubleshoot issues. So computer vision is when we use machine learning neural networks to gain high level understanding of digital images or videos. So for computer vision deep learning algorithms, we have convolution neural networks. These are for image and video recognition. They're inspired after how the human eye actually processes information, and sends it back to the brain to be processed. You have recurrent neural networks RNNs, which are generally used for handwriting recognition or speech recognition. Of course, these algorithms have other applications, but these are the most common use cases for them. four types of computer vision, we have image classification, so look at an image or video and classify its place in a category object detection. So identify objects within an image or video and apply labels and location boundaries. semantic segmentation, so identify segments or objects by drawing pixel masks around them so great for objects and movement, image analysis, so analyze an image or video to apply descriptive context labels. So maybe an employee is sitting at a desk in Tokyo would be something that image analysis would do optical character recognition, or OCR, find text in images or videos and extract them into digital text for editing facial detection, so detect faces in a photo or video, and dry location boundary and label their expression. So for computer vision to some things around Azure Microsoft services, there's one called seeing AI. It's an app developed by Microsoft for iOS. So you use your device camera to identify objects, people and objects, and the app is audibly describes those objects for people with visual impairments. It's totally free. If you have an iOS app, I have an Android phone so I cannot use it. But I hear it's great. Some of the Azure computer vision service offerings is computer vision. So analyze images and videos, extract descriptions, tags, objects and texts, custom vision so custom image classification object detection models using your own images face so detect and identify people and emotions and images form recognizer, so translate scan documents into key value or tabular editable data. So natural language processing, also known as NLP is machine learning that can understand the context of a corpus corpus being a body of related text. So NLP is enable you to analyze and interpret text within documents and email messages interpret, or contextualize spoken tokens. So for example, maybe customer sentiment analysis whether customers happy or sad, synthesize speech, so a voice assistants assistant talking to you automatically translate spoken or written phrases and sentences between languages, interpret spoken or written commands and determine appropriate actions. A very famous example for a voice assistant specifically or virtual assistant for Microsoft is Cortana. He uses the Bing search engine to perform tasks such as setting reminders and answering questions for the user. And if you're on a Windows 10 machine, it's very easy to activate Cortana by accident. When we were talking about Azure as MLP offering we have text an analytic so sentiment analysis to find out what customers think. Find topic. Topic relevant phrases using key phrase extraction, identify the language of the text with language detection, detect and categorize entities in your text. With named entity recognition for translator we have real time text translation and multi language support. For speech service. We have transcribe audible speech into readable searchable text, and then we have language understand understanding, also known as Louis natural language processing service that enables you to understand human language in your own application website, chatbots IoT device and more when we talk about conversational AI, it usually generally uses NLP so that's where you'll see that overlap next, okay. Let's take a look here at conversational AI, which is technology that can participate in conversations with humans. So we have chatbots voice assistant In an interactive voice recognition systems, which is like the second version to interactive voice response system, so you know, when you call in and they say press these numbers that is a response to some and recognition system is when they can actually take human speech and translate that into action. So the use cases here would be online customer support replaces human agents for, for replying about customer FAQs, maybe shipping questions anything about customer support, accessibility, so voice opera UI for those who are visually impaired HR processes, so employee training, onboarding, updating employee information. I've never seen it used like that. But that's what they say is the use case healthcare accessible, affordable health care. So maybe you're doing a claim process. I've never seen this. But maybe in the US where you do more your claims and everything is privatized, it makes more sense, Internet of Things, IoT devices. So Amazon, Alexa Apple, Siri, Google Home, and I suppose Cortana, but it doesn't really have a particular device. So that's why I didn't list it their computer software, so autocomplete search on phone or desktop. So that would be Cortana. Something it could do. For the two services that around conversational AI for Azure, we have q&a maker so create a conversational Question and Answer bot from your existing content, also known as a knowledge base, and Azure bot service intelligent service bot service that scales on demand used for creating publishing managing bots. So the idea is you make your bot here and then you deploy it with this. Okay. Let's take a look here at responsible AI which focuses on ethical, transparent and accountable uses ai ai technology, Microsoft puts into practice responsible AI via its six Microsoft AI principles. This whole thing is invented by Microsoft. And so you know, it's not necessarily a standard but it's something that Microsoft is pushing hard to have people adopt Okay, so we The first thing we have is fairness. So this is an AI system, which should treat all people fairly, we have reliability and safety and AI systems should perform reliably and safely. Privacy and Security AI system should be secure and respect privacy inclusiveness system should empower everyone and engage people transparency, AI systems should be understandable accountability. People should be accountable for AI systems and we need to know these in greater detail. So we're going to have a short little video on each of these okay. First arlis is fairness. So AI systems should treat all people fairly so an AI system can reinforce existing social societal stereotype, stereotypical bias can be introduced during the development of a pipeline. So an AI system that are used to allocate or withhold opportunities, resources or information in domains such as criminal justice, employee employment in hiring, finance, and credit. So an example here would be an ml model designed to select a final applicant for hiring pipeline without incorporating any bias based on gender, ethnicity or may result in unfair advantage. So as your ml can tell you how each feature can influence a models prediction for bias. One thing that could be of use is fair learn. So it's an open source Python project to help data scientists to improve fairness and the AI systems. At the time of I made this course, a lot of their stuff is still in preview. So you know, it's the fairness component is it's not 100% there, but it's great to see that they're getting that along. Okay. So we are on to our second AI principle for Microsoft and this one is AI systems should perform reliably and safely. So AI software must be rigorously tested to ensure they work as expected before released to the end user. If there are scenarios where AI is making mistakes, it is important to release a report quantified risks and harms to end users so they are informed of the shortcomings of an AI solution. Something you should really remember for the exam, they'll definitely ask that AI were concern for reliability, safety for humans is critically important. autonomous vehicles, health diagnosis, suggestions, prescriptions and autonomous weapon systems. They didn't mention this in their content. And I was just like doing some additional resource research. I'm like, yeah, you really don't want mistakes when you have automated weapons or ethical You shouldn't have them at all. But hey, that's, that's just how the world works. But yeah, this is this category here. We're on to our third Microsoft AI principle AI system should be secure and respect privacy. So AI can require vast amounts of data to train deep machine ml models, the nature of an ml model may require personally identifiable information. So P eyes, it is important that we ensure protection of user data that is not leak or disclosed. In some cases, ml models can be run locally on a user's device. So their PII is remain on their device avoiding the vulnerability. This is called this is like edge computing. So that's the concept there AI security principles to check malicious actors. So data origin and lineage data use internal versus external data corruption considerations, anomaly detection. So there you go. We're on to the fourth Microsoft principles of assets. Should empower everyone and engage people. If we can design AI solutions for the minority of users, they can design AI solutions to the majority of users. So we're talking about minority groups, we're talking about physical ability, gender, sexual orientation, ethnicity, other factors. This one's really simple. In terms of practicality, it doesn't 100% make sense, because if you've worked with groups that are deaf and blind developing technology for them, a lot of times they need specialized solutions. But the approach here is that, you know, if we can design for the minority, we can design for all that is the principle there. So that's what we need to know. Okay. Let's take a look here at transparency. So AI system should be understandable. So interpretability, and intelligibility is when the end user can understand the behavior of UI. So transparency of AI systems can result in mitigating unfairness help developers debug their AI systems gaining more trust from our users, those builds a, those who build AI systems should be open about why they're using AI open about the limitations of the AI systems. adopting an open source AI framework can provide transparency, at least from a technical perspective on the internal workings of an AI system. We are on to the last Microsoft AI principle here people should be accountable for AI systems. So the structure put in place to consistently enacting AI principles and taking them into account AI systems should work within frameworks of governments, organizational principles, ethical and legal standards that are clearly defined principles guide Microsoft and how they develop, sell an advocate when working with third parties and this push towards regulation towards a principle. So this is Microsoft saying, Hey, everybody adopt our model. There are many other models, I guess it's great that Microsoft is taking the charge there, I just feel that it needs to be a bit more well developed. But what we'll do is look at some more practical examples so we can better understand how to apply their principles. Okay. So if we really want to understand how to apply the Microsoft AI principles, they've great created this nice little tool via a free web app for practical scenarios. So they have these cards, you can read through these cards, they're color coded for different scenarios, and there's a website so let's go take a look at that and see what we can learn Okay. All right, so we're here on the guidelines for human AI interaction so we can better understand the how to put into practice the Microsoft AI principles. They have 18 cards and let's work our way through here and see the examples the first one our list, make clear what the system can do help the users understand what the AI system is capable of doing. So here PowerPoint quickstart builders and builds an online outline to help you get started researching subject displays suggested topics that help you understand the features capability. Then we have the Bing app shows examples of types of things you can search for. Apple Watch displays all metrics attracts explains how going on the second card we have make clear how well the system can do what it can do. So here we have office new companion experience ideas doc alongside your work, and offers one click assistance but grammar design data insights, richer images and more. The unassuming term ideas coupled with label previews, help set expectations and presented suggestions. The recommender in Apple Music uses language such as we will think you'll like to communicate uncertainty. The Help page for Outlook webmail explains the filtering into focused and other and we'll start working right away but we'll get better with use, making clear the mistakes will happen and you teach the product and set overrides onto our red cards. Here. We have time surfaces based on context time when to act or interrupt based on the user's current task environment. When it's time to leave for appointments, Outlook sends a time to leave notification with directions for both driving and public transit taking into account current location of that location real time traffic information. And then we have after using Apple Maps routing, it remembers when you're parked your car when you open the app after a little while it suggests routing to the location of the parked car. All these Apple examples make me think that Microsoft has some kind of partnership with Apple. I guess I guess Microsoft or or Bill Gates did own Apple shares. So maybe they're closer than we think, show contextually relevant information time when to act or interrupt based on user's current task and environment. Powered by machine learning acronyms in Word helps you understand shorthand employed in your own work environment relative to current OpenDocument. On walmart.com, when the user is looking at a product, such as gaming console recommends accessories and games that would go with it. When a user searches for movies, Google shows results including showtimes near the user's location for the current data onto our fifth card here. Match based. We didn't we didn't miss this one, right. Yeah, we did. Okay, so we're on the fifth one here match relevant social norms ensure experiences delivered in a way that users would expect Given the social cultural context when editor identifies ways to improve writing style prints optionals politely consider using. That's the Canadian way being polite. Google Photos is able to recognize pets and use the wording important cats and dogs recognizing that for many pets are an important part of one's family. And you know what? When I started renting my new house, I said, you know, these are probably dogs and my landlord said, Well, of course pets are part of the family. That was something I like to hear. Cortana uses semi formal town apologizing when unable to find a contact, which is polite and socially appropriate. I like that. Okay, mitigate social biases ensure AI system, languages and behaviors do not reinforce undesirable unfair stereotypes and biases. My analytics summarizes how you spend your time at work, and suggest ways to work smarter one ways to mitigate bias is by using gender neutral icons to represent important people sounds good to me. A Bing search for SEO or doctor shows images of diverse people in terms of gender and ethnicity. Sounds good to me. The predictive keyboard for Android suggests both genders when typing a pronoun starting with the letter H. We're onto our yellow cards support efficient invocation so make it easy to invoke or request system services when needed. So Flash Fill is a helpful time saver in Excel that can be easily invoked with on canvas interactions and that keep you in flow on amazon.com Oh, hey there got Amazon. In addition to the system giving recommendations as you Rouse you can manually invoke additional recommendations from the recommender for your menu. design ideas in Microsoft PowerPoint can be invoked with the with the press of a button if needed. I cannot stand it when that pops up. He's up to tell it to leave me alone. Okay, support efficient, dismal, efficient. Does Mazal dismissal Oh, support efficient dismissal? Okay, make it easy to dismiss or ignore under undesired AI system services. Okay, that sounds good to me. Microsoft forms allows you to create custom surveys, quizzes, polls, questionnaires and forms some choices, questions triggers suggested options, but just beneath the relevant question, this suggestion can be easily ignored and dismissed. Instagram allows the user to easily hide report ads that have been suggested by AI by tapping the ellipses at the top of the right of the ad. Siri can be easily dismissed by saying Never mind. always telling my Alexa Nevermind. Support efficient correction make it easy to edit, refine or recover the AI system when the when the AI system is wrong, so I'll Auto altex automatically generates alt text for photographs by using intelligence services in the cloud descriptions can be easily modified by clicking the alt text button in the ribbon once you set a reminder, with Siri, the UI displays a tap to edit link. When being automatically correct spelling errors in search queries it provides the option to revert to the query as originally typed with one click onto a card number 10. Scope services when in doubt, engage in dis ambiguous, Anubis, disk and big uation or gracefully degrade the AI system service when uncertain about a user's goal. So an auto replacement word is uncertain of a correction it engages in disambiguation by displaying multiple options you can select from certain will let you know it has trouble hearing if you don't respond or talk or, or speak too softly. Bing Maps will provide multiple routing options when, when unable to recommend best one we're onto card number 11. make clear why the system did what it did enable users to access an explanation of why the AI system behaved as it did. Office Online recommends document documents based on history and activity descriptive text above each document makes it clear why the recommendation is shown. product recommendations on amazon.com include why recommend recommended link that shows that what products in the user shopping history and forums. The recommendations Facebook enables you to access an explanation about why you are seeing each ad in the news feed onto our green cards. So remember recent interactions so maintain short term memory and allow the user to make efficient references to that memory. When attaching a file outlook offers a list of recent files including recently copied file links. Look also remembers people you have interacted with recently and displays them when addressing a new email being searched remember, some recent queries and search can be continued. conversationally. How old is he after a search for Keanu Reeves? Siri carries over the contacts from one interaction to the next a text message is created from the person you told Siri to message to onto card number 13 lucky number 13 learn from user behavior personal user experience by learning from their actions over time. Tap on the search bar in Office applications and search lists the top three commands on your screen that you're most likely to need to personalize the technology called zero query doesn't even need to type in the search bar to provide a personalized predictive answer. amazon.com gives personalized product recommendations recommendations based on previous purchases on the card 14. update and adapt cautiously limit disruptive changes when updating adaptive adapting the AI systems behaviors so PowerPoint designer improves slides for office 365 subscribers by automatically generating design ideas from to choose from designer has integrated new capabilities such as smart graphics, icons, suggestions and existing user experience ensuring the updates are not disruptive. Office tell office Tell me feature shows dynamically recommended items and it doesn't need a try area to minimize disruptive changes onto card number 15. Encouraged granular feedback enabled users to provide feedback indicating their preferences during regular interactions with the AI system so ideas in Excel empowers you to understand your data through high level visual summaries, trends and patterns encourages feedback on each suggestion by asking is this helpful? Not only does Instagram provide the option to hide specific ads, but it's also solicits feedback to understand why the ad is not relevant. In Apple's music app love dislike buttons are prominent, easily accessible. Number 16 convey the consequences of user actions immediately under update or convey how user actions will impact future behaviors of the AI system. You can get stock and geographic data types in Excel It is easy as typing text into a cell and converting it to stock data type or geographic geographic data type. When you perform the conversion action, an icon immediately appears in the converted cells upon tapping the like Dislike button for each recommendation. In Apple Music, a pop up informs the user that they'll receive more or fewer similar recommendations onto card number 17. Or almost near the end provide global controls allow the user to globally customize the system. System monitors and how it behaves so editor expands on spelling and grammar. Checking capabilities of words include more advanced proofing and editing designed to ensure document is readable editor can flag a range of critique types and allow to customize the thing is is that in Word, it's so awful spell checking, I don't understand like it's been years and the spell checking never gets better. So the guy implore better spell checking. I think being search provides settings that impact that the types of results the engine will return, for example, safe search. Then we have Google Photos allows you to to turn location history on and off for future photos. It's kind of funny seeing like being in there about like using AI because at one point, it's almost pretty certain that Bing was copying just google search indexes to learn how to index. I don't know that's Microsoft for you. We're onto card 18 notify users about changes informed user when AI system adds or updates as capabilities. Then what's new dialog in office informs you about changes by giving an overview of the latest features and updates, including updates to AI features in Outlook web to help tab includes a what's new section that covers updates. So there we go. We made it to the end of the list. I hope that was a fun lesson for you. And there I hope that we could kind of match up the the responsibly I I kind of wish what they would have done is actually mapped it out here and say word match, but I guess it's kind of an isolate service that kind of ties in. So I guess there we go, Okay. Hey, this is Andrew Brown from exam Pro. And we're looking at Azure cognitive services. And this is a comprehensive family of AI services, and cognitive API's to help you build intelligent apps. So create customizable, pre trained models built with breakthrough AI researchers I put that in quotations I'm kind of throwing some shade at Microsoft Azure just because it's their marketing material, right? deploy Cognitive Services anywhere from cloud to the edge. With containers get started quickly no machine learning expertise required. But I think it helps to have a bit of background knowledge developed with strict ethical standards. Microsoft loves talking about the responsible. There's responsible AI stuff, empowering responsible use with industry leading tools and guidelines. So let's do a quick breakdown of the types of services in this family. So for decision we have anomaly detector identify potential problems early on content moderator detect potentially offensive or unwanted content, personalize or create rich personalized experiences for every user. For languages we have language understanding, also known as alue is Louis I don't know I didn't put the initialism there but don't worry, we'll see it again. Build natural language understanding into app spots and IoT devices q&a maker create a conversational Question and Answer layer over your data text analytics. detect sentiment. So sentiment is like whether customers are happy, sad, glad, keep phrases and named entities translator detect and translate more than 90 supported languages. For speech, we have speech to text to transcribe audible speech into readable search text, text to speech convert text to lifelike speech for natural interfaces, speech translation, so integrate real time speech translation into your apps, Speaker recognition, identify and verify the people speaking based on audio for vision. We have computer vision, so analyze content and images and videos custom vision, so analyze or sorry, customize image record or image recognition to fit your business needs, face detect and identify people and emotions in images. So there you go. So as your cognitive services is an umbrella AI service that enables customers to access multiple AI services with an API key and API endpoint, so what you do is you go create a new cognitive service. And once you're there, it's going to generate two keys and an endpoint. And that is what you're using generally for authentication with the various AI services programmatically. And that is something that is key to the service that you need to know. So knowledge mining is a discipline in AI that uses a combination of intelligence services to quickly learn from vast amounts of information. So it allows organizations to deeply understand and easily explore information, uncover hidden insights and find relationships and patterns at scale. So we have ingest, enrich and explore as our three steps. So for ingest content from a range of sources using connectors to first and third party data stores. So we might have structured data such as databases csvs. The csvs would more be semi structured, but we're not going to get into that level of detail unstructured data. So PDFs, videos, images and audio for enrich the content with AI capabilities that let you extract information, find patterns and deepen understanding. So cognitive services like vision, language, speech, decision, and search, and explore the newly indexed data via search bots, existing businesses, applications and data visualizations and rich, structured data, customer relationship management, rap systems, Power BI, this whole knowledge mining thing is a thing but like, I believe that the whole model around this is so that Azure shows you how you can use the cognitive services to solve things without having to invent new solutions. So let's look at a bunch of use cases that Azure has and see what where we can find some useful use. So the first one here is for content research. So when organizations task employees review and research of technical data, it can be tedious to read page after page of dense Tex knowledge mining helps employees quickly review these dense materials. So you have a document and in the Richmond step, you could be doing printed text recognition key phrase extraction, sharpen or sharpen skills, technical keyword, sanitation, format, definition minor large scale vocabulary matcher, you put it through a search service, and now you have search reference library, so it makes things a lot easier to work with. Now, we have audit risk compliance management so developers could use knowledge mining to help attorneys quickly identify entities of importance from discovery documents and flag important ideas across documents that we have documents. So clause extraction clause classification, TV power risk, named identity extraction, key phrase extraction, language detection, automate translation, then you put it back into a search index and now you can use it our management platform or a word plug in. And so we have business process management in industries where bidding competition is fierce, or when the diagnosis of a problem must be quick or in near real time, companies use knowledge mining to avoid costly mistakes. So the client drilling and completion reports, document processor, ai services and custom models queue for human validation, Intelligent Automation, you send it to a back end system or a data lake and or a data lake and then you do your analytics dashboard. Then we have customer support and feedback analysis. So for many companies, customer support is costly and efficient. Knowledge mining can help customer support teams quickly find the right answers for a customer inquiry or assess customer sentiment at scale. So you have your source data, you do your document cracking use cognitive skills, so pre trained services or custom. You have enriched documents. From here you're going to do your projections and have a knowledge store you're gonna have a search index, and then do your analytics something like Power BI, we have digital assessment management. There's a lot of these but it really helps you understand how cognitive services are going to be useful. Given the amount of unstructured data created daily, many companies are struggling to make use of or find information within their files. Knowledge mining through a search index makes it easy for end customers and employees to locate what they're looking for faster. So you can just like art, metadata and the actual images themselves for the top player. geopoint extractor biographical richer than down below we're tagging, we're custom object detector similar image tagger, we put it in a search index, they love those search indexes. And now you have an art Explorer. We have contract management, this is the last one here, many companies create products for multiple sectors. Hence the business opportunities with different vendors and buyers increase exponentially. Knowledge mining can help organizations to scour 1000s of pages of sources to create Accurate Bids. So here we have RFP documents. This will actually probably come back later in the original set, but we will will will do risk extraction, print text recognition, key phrase extraction, organizational extraction engineering standards will create a search index and put it here, this will bring back data. Also, metadata extraction will come back here. And then this is just like a continuous pipeline, okay. Hey, this is Andrew Brown from exam Pro. And we are looking at face service. And Azure face service provides an AI algorithm that can detect recognize and analyze human faces and images, such as a face and an image face with specific attributes, face landmarks similar faces the same face as a specific identity across a gallery of images. So here's an example of an image that I ran that will do in the follow along. And what it's done is it's drawn a bounding box around the image. And there's this ID and this is a unique identifier, string for each detected face in an image. And these can be unique across a gallery, which is really useful as well. Another cool thing you can do is face landmarks. So the idea is that you have a face and it can identify very particular components of it. And up to 27 predefined landmarks is what is provided with this face service. Another interesting thing is face attributes. So you can check whether they're wearing accessories, accessories, so think like earrings or lip rings, determine its age, the blurriness of the image, what kind of emotion is being experienced the exposure of the image, you know, the contrast, facial hair, gender, glasses, your hair in general, the head pose, there's a lot of information around that makeup, which seems to be limited, like when we ran it here in the lab, all we got back was eye makeup and lip makeup. But hey, we get some information, whether they're wearing a mask, noise, so whether there's artifacts like visual artifacts, or occlusion, so whether an object is blocking the parts of the face, and then they simply have a boolean value for whether the person smiling or not, which I assume is a very common component. So that's pretty much all we really need to know about the face service. And there you go. Hey, this is Andrew Brown from exam prep, and we are looking at the speech and translate service. So Azure is translate service is a translation services the name implies, and it can translate 90 languages and dialects. And I was even surprised to find out that it can translate into calling on and it uses neural machine translation and Mt replacing its legacy to statistical machine translation SMT. So what my guess here is that statistical meaning that it used classical machine learning back in 2010, and, and then they decided to switch it over to neural networks, which, of course, would be a lot more accurate as your transit service can support a custom translator. So it allows you to extend the service for translation based on your business domain use cases. So if you use a lot of technical words and things like that, then you can fine tune that or particular phrases. Then there's the other service, Azure speech service. And this is a, a speech, synthesis service, a service. So what can do speech to text text to speech and speech translation, so it's synthesizing creating new voices. Okay, so we have speech to text. So real time speech to text batch batching multidevice, conversation, conversation, transcription. And you can create custom speech models and you have text to speech. So this utilizes a speech synthesis markup language, so it's just a way of formatting it, and it can create custom voices. Then you have the voice assistance of integrates with the Bot Framework SDK, and speech recognition. So speaker verification and identification. So there you go. Hey, this is Andrew Brown from exam Pro. And we were looking at text analytics and this is a service for NLP so natural language processing for text mining and text analysis. So text analytics can perform sentiment analysis, so find out what people think about your brand or topics. So features provide sentiment labels, such as negative, neutral positive, then you have opinion mining, which is an aspect based sentiment analysis. It's for granular information about the opinions related to aspects. Then you have key phrase extraction. So quickly identify the main concepts in text. You have language detections that detect the language of an input, a text that it's written in, and you have named entity recognition, so ner so identify and categorize entities in your text as people places off objects and quantities, and subset of any AR is personally identifiable information. So P eyes, let's just look at a few of these more in detail. Some of them are very obvious, but some of these would help to have an example. So the first we're looking at is key phrase extraction. So quickly identify the main concepts in text. So key phrase extraction works best when you when you give bigger amounts of text to work on. This is the opposite of sentiment analysis, which performs better on smaller amounts of text. So document sizes can be 5000 or fewer characters per document. And you can have up to 1000 items per collection. So imagine you have a movie review with a lot of text in here and you want to extract out the key phrases. So here it is, sideboard ship, enterprise, surface travels, things like that, then you have named entity recognition. So this detects words and phrases mentioned in unstructured data that can be associated with one or more semantic types. And so here's an example. I think this is medicine, bass. And so the idea is that it's identifying, it's identifying these words or phrases, and then it's applying a semantic type. So it's saying like this is like diagnosis is the medication class and stuff like that. semantic type could be more broad. So there's location events, a habit location, twice here, person diagnosis age, and there is a predefined set, I believe that is in Azure that you should expect, but they have a generic one. And then there's one that's for health. We're looking at sentiment analysis, this graphic makes it make a lot more sense when we're splitting between sentiment and opinion mining. The idea here is that sentiment analysis will apply labels and confidence scores to text at the sentence and document level. And so labels could include negative positive, mixed or neutral and will have a confidence score ranging from zero to one. And so over here, we have a sentiment analysis of this line here and in saying that this was a negative sentiment. But look, there's something that's positive and there's something that's negative, so was it really negative, and that's where opinion mining gets really useful because it has more granular data, where we have a subject and we have an opinion, right and so here we can see the room was great, but the staff was unfriendly negative. So we have a bit of a split there Okay. Hey, this is Angie brown from exam pro and we are looking at optical character recognition, also known as OCR, and this is the process of extracting printed or handwritten text into a digital and editable format. So OCR can be applied to photos of street signs, products, documents, invoices, bills, financial reports, articles and more. And so here's an example of us extracting out nutritional data or nutritional facts off the back of a food product. So Azure has two different kinds of API's that can perform OCR. They have the OCR API and the read API. So the OCR API uses an older recognition model. It supports only images, it executes synchronous notes synchronously, returning immediately, when it detects texts, it's suited for less text, it supports more languages, it's easier to implement. And on the other side, we have the read API. So this is an updated recognition model supports images and PDFs, executes asynchronously. paralyzes tasks per line for faster results, suited for lots of tax supports a few languages, and it's a bit more difficult to implement. And so when we want to use this service, we're going to be using computer vision SDK, okay. Hey, this is Andrew Brown from exam Pro, and we're taking a look here at form recognizer service. This is a specialized OCR service that translates printed text into digital and editable content. It pervert preserves the structure and relationships of the form like data. That's what makes it so special. So form recognizer is used to automate data entry in your applications and enrich your document search capabilities. It can identify key value pairs selection marks table structures, it can produce output structures such as original file relationships, bounding boxes, confidence score, and form recognizer is composed of a custom document processing models, pre built models for invoices, receipts, IDs, business cards, the model layouts, let's talk about the layout here. So extract text selection marks table structures along with bounding box coordinates from documents form. recognizer can extract text selection marks and table structures. The row and column numbers associate with the text using high definition optical character enhancement models. That is totally useless text. Hey, this is Andrew Brown from exam Pro. And we are looking at form recognizers service. And this is a specialized service for OCR. That translates printed text into digital editable content. But the magic here is that that preserves the structure and relationship of form like data. So there's an invoice you see those magenta lines, it's saying identify that form like data. So for recognizer is used to automate data entry in your applications and enrich your document search capabilities. And it can identify key value pairs, selection of marks, tables, structures, and it can put structures such as original file relationships, bounding box boxes, confidence scores. It's composed of customer custom document processing model, pre built models for invoices, receipts, IDs, business cards, it's based on this layout model. And there you go. So let's touch upon custom models. So custom models allow you to extract text key value pairs selection marks in tabular data from your forms. These models are trained with your data, so they're tailored to your forms, you only need five samples, sample input forms to start, a trained document processing model can output structured data that includes the relationship and the original form document. After you train the model, you can test and retrain it and eventually use it reliably extract data from more forms according to your needs. You have two learning options, you have unsupervised learning to understand the layout and relationships between fields entries in your forms. And you have supervised learning to extract values of interest using the labeled form. So we've covered unsupervised and supervised learning, so you're going to be very familiar with these two. Okay. So form recognizer service has many pre built models that are easy to get started with. And so let's go look at them and see what kind of fields that extracts out by default. So the first is receipts. So sales receipts from Australia, Canada, Great Britain, India and United States will work great here and the fields that will extract out his receipt type merchant name, merchant phone number, merchant address, transaction, date, transaction time, total subtotal, tax tip, items, name, quantity, price, total price, there's information that is on a receipt that you're not getting out of these fields. And that's where you make your own custom model right. For Business cards. It's only available for English business cards, but we can extract our contact names first name, last name, company names, departments, job titles, emails, websites, addresses, mobile phones, faxes, work phones, and other phone numbers. Not sure how many people are using business cards these days, but hey, they have it as an option for invoices, extract data from invoices in various formats and return structured data. So we have customer name, customer ID, purchase order, invoice, ID, invoice, date, due date, vendor name, vendor address, vendor address, receipt, customer address, customer address, receipt and billing address, billing address, receipt shipping address, subtotal, total tax invoice, total amount to service address, remittance address, start service start date and end date, previous unpaid balance and then they even have one for line items. So items amount description, quantity, unit price, Product Code, unit date, tax, and then for IDs which could be worldwide passports, US driver's license, things like that. You have fields such as country region, date of birth, date of expiry expiration document name, first name, last name, nationality, sex, machine readable zone, I'm not sure what that is document type, and address and region. And there are some additional features with some of these bottles. We didn't really cover them it's not that important but yeah, there we go. Hey, this is Andrew Brown from exam Pro, and we're looking at natural understanding or Lewis or Luis depends on how you'd like to say it. And this is a no code ml service to build language, natural language into apps, bots and IoT devices have quickly create enterprise ready custom models that continuously improve so Louis I'm gonna just call it Louis because that's what I prefer is access via its own isolate domain@lewis.ai at a utilizes NLP and NLU so NLU is the ability to perform or ability to transform a linguistic statement to a representation that enables you to understand your users naturally. And it is intended to focus on intention and extraction, okay, so where the users want, or was or what the users want, and what the users are talking about. So the loose application is composed of a schema and a schema is auto generated for you when you use the Louis AI web interface. So you definitely are going to be reading this by hand, but it just helps to see what's kind of in there. If you do have some programmatic skills, you obviously you can make better use of the service isn't just the web interface. But the schema defines intention. So what the users are asking for a loose app always contains a nun intent. We'll talk about why that is in a moment. And entities what parts of the intent is used to determine the answer. Then you also have utterances. So examples of the user input that includes intent and entities to train the ML model to match predictions against the real user input. So an intent requires one or more example utterance for training. And it is recommended to have 15 to 30 example utterances to explicitly train to ignore an utterance you use the nun intent. So, intent classifies that user as utterances and entities extract data from utterances. So hopefully it understands I always get this stuff mixed up, it always takes me a bit of time to understand there is more than just these things is like features and other things. But you know, for the 900, we don't need to go that deep. Okay, just to skip to visualizing this to make a bit easier. So imagine we have this, this utterance here, these would be the identities that we have to end Toronto, this is example utterance. And then the idea is that you'd have the intent and the intent. And if you look at this keyword here, this really helps Word says classify is that's what it is. It's a classification of this example utterance, and that's how the ML model is going to learn, okay. Hey, this is Andrew Brown from exam prep, and we're looking at q&a maker service. And this is a cloud based NLP service that allows you to create a natural conversational layer over your data. So q&a maker is hosted on its own iyslah domain at q&a maker.ai it will help the most, it will help you find the most appropriate answer from any input from your custom knowledge base of information. So you can commonly it's commonly used to build conversation clients, which includes social apps chatbots speech enabled desktop applications. q&a maker doesn't store customer data, all the customer data stored in the region, the customer deploys the dependent services instances within Okay, so let's look at some of the use cases for this. So when you have static information, you can use q&a maker in your knowledge base. The answers this knowledge base is custom to your needs, which you've built with documents such as PDF URLs, where you want to provide the same answer to repeat question command when different users submit the same question the answers is returned when you want to filter stack information based on meta information. So meta tag data is provide provides additional filtering options relevant to your client application users and information common metadata information includes chitchat content type, format, content, purpose, content, freshness. And there's the use case when you want to manage a bot conversation that includes static information. So your knowledge base takes takes the user conversational text, or command and answers that if the answer is part of a pre determined conversation flow, represented in the knowledge base with multiple turnkey contexts the bot can easily provide this flow. So q&a maker import your content into a knowledge base of questions and answer pairs. And q&a maker can build your knowledge base from an existing document manual, or website, your all docx PDF. I thought this was the coolest thing. So you can just basically have anyone write a docx. As long as it has a heading and a text. They can even extract images and I'll just turn it into the bot. It just saves you so much time It's crazy. It will use ml to extract the question and answer pairs. The content of the question and answer pairs include all the alternate forms of the question metadata tags used to filter choices. During the search, follow up prompts to continue to search refinement, refinement, q&a maker stores, answers text in markdown. Once your knowledge base is imported, you can fine tune the important results by editing the question and answer pairs. As seen here. There is the chat box. So you can converse with your bot through a chat box. I wouldn't say it's particularly a feature of q&a maker, but I just want you to know that's how you would interact with it. So when you're using the q&a maker AI, the Azure bot service the bot composer, or via channels, you'll get an embeddable one, you'll see this box where you can start typing in your questions and and get back the answers to test it. Here. An example is a multi turn conversation. So somebody asked a question, a generic question. And that said, Hey, are you talking about AWS or Azure, which is kinda like a follow up prompt. And we'll talk about multiturn here in a second, but that's something I want you to know about. Okay. So chit chat is a feature in q&a maker that allows you to easily add pre populated sets of top chit chats into your knowledge base. The data set has about 100 scenarios of chit chat in voices of multiple personas. So the idea is like if someone says something random, like how are you doing? What's the weather today, things that your bot wouldn't necessarily know. It has like canned answers, and it's going to be different based on how you want the response to be okay. There's a concept of layered ranking. So the q&a maker system is a layered ranking approach the data stored in Azure Search, which also serves as the first ranking layer, the top result for from Azure Search are then passed through q&a makers and LP ranking model to produce the final results and confidence score. Just touching on multi turn conversation is a follow up prompt and context to manage the multiple turns known as multi turn for your bot from one question to another when a question can't be answered in a single turn. That is when you're using multi turn conversation. So q&a maker provides multi turn prompts and active learning to help you improve your questions based on key answer pairs, and it gives you the opportunity to connect questions and answer pairs. The connection allows the client app position to provide a top answer and provide more questions refine the search for a final answer. After the knowledge base receives questions from users at the Publish endpoint, can I make replies active learning to these rules or questions to suggest changes to your knowledge base to improve the quality alright. Hey, this is Andrew Brown from exam Pro. And we are looking at Azure bot service. So the Azure bot services an intelligent serverless bot service that scales on demand used for creating publishing and managing bots. So you can register and publish a variety of bots from the Azure portal. So here there's a bunch of ones I've never heard of, probably with third party providers partnered with Azure. And then there's the ones that we would know like the Azure health health bot, the Azure bot, or the webapp bot, which is more of a generic one. So Azure bot service bop bop bot service can integrate your bot with other Azure, Microsoft or third party service services via channel so you can have a direct line out Alexa, office 365, Facebook, Keke line, Microsoft Teams, Skype, Twilio and more. Alright, and two things that are commonly associated with the Azure bot service is the Bot Framework and bot composer. In fact, it was really hard just to make make this slide here because they just weren't very descriptive on it. Because I wanted to push these other two things here. Let's talk about the Bot Framework SDK. So the Bot Framework SDK, which is now version four is an Open Source SDK that enables developers to model and build sophisticated conversations. The Bot Framework along with the Azure bot service provides an end to end workflow. So we can design build, test, publish, connect, and evaluate. Are bots. okay with this. With this framework, developers can create bots that use speech, understand natural language, handle questions, answers, and more. The Bot Framework includes a modular, extensible SDK for building bots, as well as tools, templates and related AI services. Then you have Bot Framework composer. And this is built on top of the Bot Framework SDK. It's an open source IV for developers to author test provision and manage conversational experiences. You can download, it's an app on Windows OS X, and Linux is probably built using like web technology. And so here is the actual app there. And so you can see there's kind of a bit of a flow and things you can do in there. So you can either you see or note to build your bot, you can deploy the bot to the Azure web apps or Azure Functions. You have templates to build q&a maker bot enterprise or personal assistant bot language bot calendar, or people bought. You can test and debug via the Bot Framework emulator, and has a built in package manager. There's a lot more to these things. But again, at the AI 900 this is all we need to know. But yeah, there you go. Hey, this is Andrew Brown from exam Pro. And we are looking at Azure Machine Learning service, I want you to know there's a classic version of the service, it's still accessible in the portal. This is not an exam, we are going to 100% avoid it. It has severe limitations, we cannot transfer anything over from the closet to the new one. So the one we're going to focus on is the Azure Machine Learning service. You do create studios within it. So you'll hear me say Azure Machine Learning Studio and I'm referring to the new one, a service that simplifies running AI ml work related workloads allowing you to build flexible automated ml pipelines, use Python or R run deep learning workloads such as TensorFlow, we can make Jupyter Notebooks in here. So build and document your machine learning models as you build them, share and collaborate Azure Machine Learning SDK for Python. So an SDK designed specifically to interact with the Azure Machine Learning Services. It does ml Ops, machine learning operations, so end to end automation of ml model pipelines, CIC D training inference, Azure Machine Learning designer. So this is a drag and drop interface to visually build test deploy machine learning models, technically, pipelines, I guess, as a data labeling service, assemble a team of humans to label your training data responsible in machine learning. So model fairness, through disparity metrics, and mitigate unfairness at the time of the service is not very good, but it's supposed to tie in with the responsible AI that Microsoft is always promoting. Okay. So once we launch our own studio with an Azure Machine Learning service, you're gonna get this nice big bar, navigation left hand side, it shows you there's a lot of stuff that's in here. So let's just break it down on what all these things are. So for authoring, we got notebooks these are Jupyter, notebooks and ID to write Python code to build ml models. They kind of have their own preview, which I don't really like. But there's a way to bridge it over to Jupyter Notebooks or into Visual Studio code. We have auto ml completely automated process to build and train ml models. So if you're limited to only three types of models, but still, that's great. We have the designers of visual drag and drop designer to construct end to end ml pipelines. For assets we have data sets of data that you can upload which we will be used which will be used for training Experiments when you run a training job, they are detailed here, pipelines, ml workflows, you have built or have used in the designer model. So a model registry containing train models that can be deployed endpoints. So when you deploy a model, it's hosted on accessible endpoint. So you're going to be able to access it via a REST API, or maybe the SDK for managing got compute the underlying computing instances used for notebooks, training and inference, environments, reproducible Python environment for machine learning experiments, data stores a data repository where your data resides, data labeling, so you have a human with ml assisted labeling to label your data for supervised learning, Link services, external service, you can connect to the workspace such as Azure synapse analytics. Let's take a look at the types of compute that is available in our Azure Machine Learning Studio, we got four categories, we have compute instances, development workstations that data scientists can use to work with data and models, compute clusters to scalable clusters of VMs, for on demand processing, experimentation, code, deployment targets for predictive services that use your trained models, and attach compute links to existing Azure compute resources such as Azure VMs. And Azure Data brick clusters. Now, what's interesting here is like with this compute, you can see that you can open it in Jupiter labs, Jupiter VS code, our studio and terminal. But you can you can work with your computers as your development workstations directly in the studio, which that's the way I do it. What's interesting is for inference, that's when you're want to make a prediction, you use Azure Kubernetes service or Azure Container instance, I didn't see it show up under here. So I'm kind of confused whether that's where it appears. Maybe we'll discover as we do the follow logs that they do appear here, but I'm not sure about that one. But yeah, those are the four there, okay. So within Azure Machine Learning Studio, we can do some data labeling, so we create data labeling jobs to prepare your ground truth. For supervised learning, you have two options human in the loop labeling, you have a team of humans that will apply labeling, these are humans, you grant access to labeling, machine learning assists to deal with labeling, you will use ml to perform labeling. So you can export the label data for machine learning, experimentation, any time, your users often export multiple times and train different models. rather than wait for all the images to be labeled. Images, labels can be exported in cocoa format. That's why we talked about cocoa a lot earlier in our data set section as your machine learning data set. And this is the data set format that makes it easy to use for training and Azure machine learning. So generally, you want to use that format. The idea is you would choose a labeling Task Type. And that way you would have this UI and then people go in and just click buttons and do the labeling. Okay. So as your ml data store securely connects you to storage services on Azure without putting your authentication credentials and the integrity of your original data source at risk. So here is the example of data sources that are available to us in the studio. And let's just go quickly through them. So we have Azure Blob Storage. This is data that is stored as objects distributed across many machines, as your file share a mountable file share via SMB and NFS protocols as your data lake storage Gen two, this blob searches for vast amounts of big data analytics, as your SQL is a fully managed MS SQL relational database as your Postgres database, this is an open source relational database, often considered an object related database preferred by developers as your MySQL, another open source relational database, the most popular one and considered a pure relational database, okay. So as your ml data sets makes it easy to register your datasets for use with your ml workloads. So what you do is you'd add a data set and you get a bunch of metadata associated with it. And you can also upload a dish like the data set again to have multiple versions. So you'll have a current version and a latest version, it's very easy to get started working with them, because we'll have some sample code that's for the Azure ML SDK to import that into, into your Jupyter notebooks. For datasets, you can generate profiles that will give you summary statistics, distribution of data and more, you will have to use a compute instance to generate that data. So you'd press the Generate profile, and you'd have that stored I think it's in blob storage. There are open data sets is they're publicly hosted data sets that are commonly used for learning how to build ml models. So if you go to open data sets, you just choose one. And so this is a curated list of open data sets that you can quickly add to your data store. Great for learning how to use auto ml or Azure Machine Learning designer or any kind of ml workload if you're new to it. That's why we covered amnesty and cocoa earlier just because those are some common data sets there. But there you go. Take a look here at Azure ML experiments. This is a logical grouping of Azure runs and runs Act is the act of running ml tasks on a virtual machine or container. So here's a list of them. And it can run various types of ml tasks. So scripts could be pre processing, auto ml, a training pipeline, but what it's not gonna include is inference. And what I mean is once you've deployed your model or pipeline, and you make predictions via request, it's just not going to show up under here. Okay? Okay, so we have Azure ML pipelines, which is an executable workflow of a complete machine learning task Not to be confused with Azure pipelines, which is part of Azure DevOps, or Data Factory, which has its own pipelines, it's a total, totally separate thing here. So subtasks are encapsulated as a series of steps within the pipeline. Independent steps allow multiple data scientists to work on the same pipeline at the same time without overtaxing compute resources. Separate steps also make it easy to use different compute type sizes for each step. When you rerun a pipeline, the run jumps to the steps that need to be rerun, such as the updated training script steps do not need to be rerun, and they will be skipped. After a pipeline has been published, you can configure a REST endpoint, which allows you to rerun the pipeline from any platform or stack. There's two ways to build pipelines, you can use the Azure ML designer or pre bakley, using Azure Machine Learning Python SDK. So here's an example of some code. Just make a note here, I mean, it's not that important. But notice as you create steps, okay, and then you assemble all the steps into a pipeline here. Alright. So Azure Machine Learning designer lets you quickly build as your ml pipelines without having to write any code. So here is what it looks like. And over there, you can see our pipeline is quite visual. And on the left hand side, you have a bunch of assets you can drag out that are pre built there. So it's a really fast way for building a pipeline. So you do have to have a good understanding of ml pipelines end to end to make good use of it. Once you've trained your pipeline, you can create an inference pipeline, so you drop down and you'd say whether you want it to be real or batch, or you can toggle between them later. So I mean, there's a lot to this service. But for the 100, we don't have to go diving too deep, okay. So as your ml models are the model registry allows you to create, manage and track your registered models as incremental versions under the same name. So each time you register a model with the same name as an existing one, the registry assures that it's a new version. Additionally, you can provide metadata tags and use tags when you search for models. So yeah, it's just really easy way to share and deploy or download your models, okay? As your MLM points allow you to deploy machine learning models as a web service. So the workflow for deploying models, register the model, prepare an entry script, prepare an inference configuration, deploy the model locally to ensure everything works, compute, choose a compute, Target, redeploy the model to the cloud test the resulting web service. So we have two options here real time endpoints endpoint that provides remote access to invoke the ML model service running on either Azure Kubernetes service Eks, or Azure Container instances ACI, then we have pipeline endpoint. So endpoint that provides remote access to invoke an ml pipeline, you can parameterize the pipeline endpoint for manage repeatability in batch scoring and retraining scenarios. And so you can deploy a model to an endpoint yet, it will either be deployed to a Eks or ACI, as we said earlier, and the thing is, is that when you do do that, just understand that that's going to be shown under the A Ks or ACI within the Azure portal. It's not consolidated under the Azure Machine Learning Studio. When you've deployed a real time endpoint, you can test the endpoint by sending either a single request or batch request. So they have a nice form here with single or it's like here, it's a CSV that you can send. So there you go. So Azure has a built in Jupiter like notebook editor, so you can build and train your ml models. And so here is an example of it. I personally don't like it too much. But that's okay, because we have some other options. To make it easier. All you do is you choose your compute instance, to run the notebook, you'll choose your kernel, which is a pre loaded programming language and programming libraries for different use cases. But that's a Jupiter kernel concept there. So you can open the notebook at a more familiar ID such as VS code, Jupyter, notebook classic or Jupiter lab. So you go there, drop it down, choose it and open it up. And now you're in a more familiar territory. The VS code one is exactly the same experience as the one in Azure or Azure ML studio. I personally don't like it. I think most people are going to be using the notebooks but it's great that they have all those options. So Azure automated machine learning, also known as auto ml automates the process of creating an ml model. So with Azure auto, ml you supply a data set, choose a test type, then auto ml will train and tune your model. So here are test types, let's quickly go through them. So we have classification, when you need to make a prediction based on several classes, so binary classification, multi class classification regression, when you need to predict a continuous number value, and then time series forecasting when you need to predict the value based on time. So just look at them a little bit more in detail. So classification is a type of supervised learning in which the model learns using training data and apply those learnings to new data. So here is an example. Or this is just the option here. And so the goal of classification is to predict which categories new data will fall into based on learning from its training data. So binary classification is a record is labeled out of two possible labels. So maybe it's true or false, zero or one, just two values. multiclass classification is a record is labeled out of range of out of a range of labels. And so can be like happy, sad, mad or rad. And just, you know, I can see there's a spelling mistake there. But yeah, there should be an F. So let's just correct that. There we go. You can also apply deep learning and so if you turned deep learning on you probably want to use a GPU compute instance, just because or compute cluster because deep learning really prefers GPUs. Okay. Looking at regression, it's also a type of supervised learning where the model learns using training data and applies those learnings to new data, but it's a bit different, where the goal of aggression is to predict a variable in the future, then you have time series forecasting and this sounds a lot like regression because it is, so forecast revenue inventory sales or customer demand, an automated time series experiment that is treated as a multivariate regression problem, past time series values are pivoted to become additional dimensions for the regressor together with other predictors, unlike classical time series methods has an advantage of naturally incorporating multiple contextual variables and their relationship to one another during training. So use cases here or dance configurations, I should say, holiday detection and future position time series, deep learning neural networks. So you got auto ri ma profit forecast TCN. Many models supports through grouping, rolling origin, cross validation, configurable labs rolling window aggregate features, so there you go. So within auto ml, we have data guardrails, and these are run by auto ml when automatic feature rotation is enabled, it's a sequence of checks to ensure high quality input data is being used to train the model. So just to show you some information here. So the idea is that could apply validation split handling so the input data has been split for validation to improve the performance, then you have missing feature value imputation so no features missing values were detected in training data, high cardinality feature detection, your inputs were analyzed, and no high cardinality features were detected. High cardinality means like if you have too many dimensions, it becomes very dense or hard to process the data. So that's something good to check against. Let's talk about auto ml is automatic feature isolation. So during model training with auto ml, one of the following scaling or normalization techniques will be applied to each model. The first is standard scale rapper standardized features by removing the mean and scaling to unit variants. min max scalar transform features by scaling each feature by the columns minimum maximum max ABS scalar scale each feature by its maximum absolute value, robust scale our scales features by the quantitative quantal range, a PCA linear dimensionality reduction using single value decomposition of the data to project it to lower dimensional space. dimension reduction is very useful if your data is too complex. And let's say you have data you have too many labels like 20 3040 labels for like four categories to pick out of you want to reduce the dimensions so that your machine learning model is not overwhelmed. So then you have truncated SVD wrappers. So the transformer performs linear dimensionality reduction by means of truncated single singular value decomposition contrary to PCA, the estimator does not send her the data before computing the singular value decomposition, which means it can work with spicy sparse matrices, efficiently sparse normalization to each sample that is each row of the data matrix which with at least one zero component is rescaled independently of other samples, that is norm. So one l or two l two, I can refer to it or L. Anyway, I one and, and I two. Okay? So the thing is, is that on the exam, they're probably not going to be asking these questions but I just like to get you exposure, but I just want to show you that auto ml is doing all this. This is like pre processing stuff, you know, like this is stuff that you'd have to do, and so it's just taking care of the stuff for Are you okay? So within Azure auto ml, they have a feature called model selection. And this is the task of selecting a statistical model from a set of candidate models. And Azure auto ml will use different, or many different ml algorithms that will recommend the best performing candidates. So here's a list. And I want to just point out, down below, there's three pages, there's 53 models, that's a lot of models. And so you can see that the one I chose is its top candidate was called voting ensemble, that's an ensemble algorithm, that's where you take two weak ml models, combine them together to make a more stronger one. And notice here, it will show us the results. And this is what we're looking for, which is the primary metric, the highest value should indicate that that's the model we should want to use, you can get an explanation of the model called that's known as explainability. And now if you're a data scientist, you might be a bit smarter and say, Well, I know this one should be better. So I'll use this and tweak it. But you know, if you don't know you're doing just go with the top line, okay. So we just saw that we had a top candidate model, and there could be an explanation to understand as to the effectiveness of this, this is called MX L. So machine learning explainability This is the process of explaining interpreting ml or deep learning models, MX, m, l x, can help machine learning developers to better understand interpret models behavior. So after your top candidate models selected by Azure auto ml, you can get an explanation of internals of various factors. So model performance data set, explore aggregate feature importance, individual feature importance. So I mean, yeah, this is aggregate. So what it's looking at, and it's actually cut off here, but it's saying that these are the most important ones that affect how the the models outcome. So I think this is the diabetes data data set. So BMI would be one, that would be a huge influence there, there, okay. So the primary metric is a parameter that determines the metric to be used during the mall training for optimization. So we for classification, we have a few and regression and time series, we have a few. But you'll have these task types. And underneath, you'll choose the additional configuration. And that's where you can override the primary metric, it might just auto detect it for you. So you don't have to because it might sample some of your data set to just kind of guess. But you might have to override it yourself. Just going through some scenarios. And we'll break it down into two categories. So here we have suited for larger datasets that are well balanced. well balanced means that your data set like is evenly distributed. So if you have classifications for A and B, let's say you have 100, and 100, they're well balanced, right, you don't have one data set much a subset of your data set much larger than the other that's labeled. So for accuracy, this is great for image classification, sentiment analysis term prediction, for average precision score weighted is for sentiment analysis, nor macro recall term prediction for precision score weighted, uncertain as to what that would be good for maybe sentiment analysis suited for smaller data sets that are imbalanced. So that's where your data set like you might have like 10 records for one and 500 for the other on the label. So you have AUC weighted fraud detection, image classification, anomaly detection, spam detection, on to regression scenarios, we'll break it down into ranges. So when you have a very wide range, Spearman correlation works really well are to score. This is great for airline delay salary estimation, but resolution time, when you're looking at smaller ranges, we're talking about normalized root square mean to error. So price predictions, review tips, score predictions, for normalized mean absolute error, it's going to be just another one here, they don't give a description for time series, it's the same thing. It's just in the context of time series of forecasting. Alright. Another option we can change is the validation type when we're setting up our ML model. So validation, model validation is when we compare the results of our training data set to our test data set model validation occurs after we train the model. And so you can just drop it down there, and we have some options. So auto k fold cross validation, Monte Carlo cross validation, train validation split, I'm not going to really get into the details of that. I don't think it'll show up on the AI 900 exam. But I just want you to be aware of that you do have those options, okay. Hey, this is Andrew Brown from exam Pro. And we're taking a look here at custom vision. And this is a fully managed no code service to quickly build your own classification, and object detection ml models. The service is hosted on its own isolate domain at www custom vision.ai. So the first idea is you upload your images of bring your own labelled images or custom vision to quickly add tags to any unlabeled data images. You use the labeled images to teach custom vision, the concepts you care about, which is training, and you use a simple REST API that calls to quickly tag images. With your new custom computer vision model so you can evaluate, okay. So when we launch custom vision, we have to create a project. And with that, we need to choose a project type. And we have classification and object detection. Reviewing classification, here, you have the option between multi labels. So when you want to apply many tags to an image, so think of an image that contains both a cat and a dog, you have multi class, so when you only have one possible tag to apply to an image, so it's either an apple, banana, and orange, it's not multiples of these things. You have object detection, this is when we want to detect various objects in an image. And you also need to choose a domain a domain is a Microsoft managed data set that is used for training the ML model. There are different domains that are suited for different use cases. So let's go take a look first at image classification domains. So here is the big list of domains being over here. Okay, and we'll go through these here. So general is optimized for a broad range of image classification tasks. If none of the none of the other specified domains are appropriate, or you're unsure of which domain to choose Select one of the general domains so G, or a one is optimized for better accuracy with comparable inference time as general domain recommended for larger datasets or more difficult user scenarios. This domain requires more training time, then you have a to optimize for better accuracy with faster adverts times than a one and general domains recommended for more most datasets this domain requires less training time, then general and a one, you have food optimized for photographs or dishes as you would see them on a restaurant menu. If you want to classify photographs of individual fruits or vegetables use food domains. So that we have optimized for recognizable landmarks both natural and artificial. This domain works best when landmark is clearly visible in the photograph, this domain works even if the lend mark is slightly obstructed by people in front of it. Then you have retail so optimized for images that are found in a shopping cart or shopping website. If you want a high precision classifying classified in between dresses, pants shirts uses domain contact domains optimized for the constraints of real time classification on the edge. Okay, then we have object detection domain, so this one's a lot shorter, so I'll get through a lot quicker. So optimize for a broad range of object detection tasks if none of the other domains are appropriate, or you're unsure of which domain choose the general one a one optimize for better accuracy and comparable inference time than the general domain recommended for most accurate region. locations, larger data sets or more difficult use case scenarios, the domain requires more training results are not deterministic expect plus minus 1% mean average precision difference with the same training data provided you have logo optimized for finding brands, logos and images products on shelf so optimized for detecting and classifying products on the shelf. So there you go. Okay, so let's get some more practical knowledge of the service. So for image classification, you're gonna upload multiple images and apply single or multiple labels to the entire image. So here I have a bunch of images uploaded. And then I have my tags over here. And they could either be multi or singular. For object detection, you apply tags to objects in an image for data labeling, and you hover your cursor over the image custom vision uses ml to show bounding bounding boxes of possible objects that are not yet been labeled. If it does not detect it, you can also just click and drag to draw out whatever square you want. So here's one where I tagged it up quite a bit, you have to have at least 50 images on every tag to train. So just be aware of that when you are tagging your images. When you're training, your model is ready when you and you have two options. So you have quick training that's trained quickly, but it will be less accurate, you have advanced training, this increases compute time to improve your results. So for advanced training, basically, you just have this thing that you move to the right. With each iteration of training, our ML model will improve the evaluation metrics. So precision recall, it's going to vary. We're going to talk about the metrics here in a moment, but the probability threshold value determines when to stop training, when our evaluation metric meets our desired thresholds. So these are just additional options where when you're training, you can move this left to right, and these left to right, okay. And then when we get a results back, we're gonna get some metrics here. So we have evaluation master. So we have precision being exact, inaccurate, selects items that are relevant, recalls that sensitivity or known as true positive rate, how many relevant items returned average precision, it's important that you remember these because they might ask you that on the exam. So for cut when we're looking at object detection, and we're looking at the evaluation metric outcomes for this one, we have precision recall and mean average precision. Once we have deployed our pipeline, it makes sense that we go ahead and give it a quick test to make sure it's working correctly to press Click Click test button and you can upload your image and it will tell you so this one says it's worth, when you're ready to publish, you just hit the publish button. And then you'll get some prediction URL and information so you can invoke it. One other feature that's kind of useful is the smart labelers. So once you've loaded some training data within a canal, make suggestions, right. So you can't do this right away. But once it has some data, it's like, it's like kind of a prediction that is not 100%, guaranteed, right, and it just helps you build up your training data set a lot faster. Very useful. If you have a very large data set, this is known as ml assisted labeling, okay. Hey, this is Andrew Brown from exam Pro. And in this follow along, we're gonna set up a studio with an Azure Machine Learning service, so that it will be the basis for all the fall logs here. So what I want you to do is go all the way the top here, and type in Azure machine learning. And you're looking for this one that looks like a science bottle here. And we'll go ahead and create ourselves our Machine Learning Studio. And so I'll create a new one here, and I'll just say, my studio. I will hit OK. And we'll name the workspace. So I will say my work workplace will maybe say and I'll workplace here. For containers, there are nodes that we will create all that stuff for us. I'll hit create and create. And so what we're going to do here is just wait for that creation, okay? Alright, so after a short little wait there, it looks like our studio set up. So we'll go to that resource launch the studio and where are now in. So there's a lot of stuff in here. But generally, the first thing you'll ever want to do is get yourself a notebook going. So in the top left corner, I'm going to go to notebooks. And what we'll need to do is load some files in here. Now they do have some Sample Files, like how to use Azure ML. So if we just quickly go through here, you know, maybe we'll want to look at something like Ms NIST here. And we'll go ahead and open this one. And we'll just go ahead and clone this. And we'll just clone it over here. Okay, and the idea is that we want to get this notebook running. And so notebooks have to be backed by some kind of compute. So up here, it says, No compute found, and etc. So what we can do here, I'm just gonna go back to my files, oh, it went back there for me. But what I'm going to do is go all the way down. Actually, I'll just expand this up here makes it a bit easier, close this tab out. But what we'll do is go down to compute. And here we have our four types of compute to compute instances is when we're running notebooks, compute clusters, is when we're doing training inference clusters is when we have a inference pipeline. And then attached computers bringing things like hdn sites or data bricks into here, but for compute instances is what we need, we'll get ahead and go new, you'll notice they have the option between CPU and GPU. GPU is much more expensive. So it's like 90 cents per hour. For a notebook, we do not need anything super powerful. Notice, it'll say here, development on notebooks, IDs, lightweight testing, here, it's as classical ml model training, auto ml pipelines, etc. So I want to make this a bit cheaper for us here. Because we're going to be using the notebook to run cognitive services and those costs next to nothing like they don't take much compute power. And for some other ones, we might do something a bit larger. For this, this is good enough. So I'll go ahead and hit next. I'm just gonna say my notebook instance here. We'll go ahead and hit Create. And so we're just gonna have to wait for that to finish creating and running and when it is, I'll see you back here in a moment. Alright, so after a short little wait there, it looks like our server is running. And you can even see here it shows you you can launch in Jupiter labs, Jupiter VS code, our studio or the terminal. But what I'm going to do is go back all the way to our notebooks just so we have some consistency here, I want you to notice that it's now running on this compute. If it's not, you can go ahead and select it. And it also loaded in Python 3.6, there is 3.8. Right now, it's not a big deal which one you use. But that is the kernel, like how it will run this stuff. Now, this is all interesting. But I don't want to run this right now what I want to do is get those cognitive services into here. So what we can do is just go up here and we'll choose editors and edit in Jupiter lab. What that should do is open up a new tab here is it opening. If it's not opening, what we can do is go to compute. Sometimes it's a bit more responsive. If we just click there, it's the same way of getting to it. I don't know why, but just sometimes that link doesn't work when you're in the notebook. And what we can do is while we're in here now we can see that this is where this is An example project is okay. But what we want to do is get those cognitive services in here. So I don't know if I showed it to you yet, but I have a repository, I just gotta go find it. It's somewhere on my screen. Here it is. Okay, so I have a repo called the free az, AZ night free, AZ I should be ai 900, I think I'll go ahead and change that, or that is going to get confusing. Okay, so what I want you to do here is, we'll get this loaded in. So this is a public directory, I'm just thinking, there's a couple ways we can do it, we can go and I use the terminal to grab it, what I'm going to do is I'm just going to go download the zip. And this is just one of the easiest ways to install it, and we need to place it somewhere. So here are my downloads. And I'm just going to drag it out here. Okay. And what we'll do is upload that there. So I can't remember if it lets you upload entire folders, we'll give it a go see if it lets us maybe rename this to the free AZ or ai 900 there, we'll say open. Yeah, so it's individual file. So it's not that big of a deal, but we can go ahead and select it like that. And maybe we'll just take him to the folder and here we'll say this cognitive services. Okay. And what we'll do here is keep on uploading some stuff. So we have assets. So I have a couple loose files there. And I know we have crew groups will have crew. Oops. Sometimes it's not as responsive. We want OCR, I believe we have on called movie reviews. So we'll go into OCR here and upload the files that we have. So we have a few files there. And we'll go back a directory here. And I know movie reviews are just static files. And we have an objects folder. So we will go back here to objects. And then we'll go back and to crew and we need a folder called Wharf a folder called Crusher, a folder called data. And so for each of these, we have some images. Think Ron Wharf, right? Yeah, we are okay, great. So we will quickly upload all these I will technically we don't really need to upload any of these walls, these images we don't but I'm going to put them here anyway, I just remembered that these we just upload directly to the service. But because I'm already doing it, I'm just gonna put them here, even though we're not going to do anything with them. All right. And so now we are all set up to do some cognitive services. So I'll see you the next video. Alright, so now that we have our work environment set up, what we can do is go ahead and get Cognitive Services hooked up, because we need that service in order to interact with it. Because if we open up any of these, you're gonna notice we have a cognitive key endpoint that we're going to need. So what I want you to do is go back to your Azure Portal. And at the top here, we'll type in cognitive services. Now the thing is, is that all these services are individualized, but at some point, they did group them together, and you're able to use them through a unified key and API endpoint. That's what this is. And that's the way we're going to do it. So let's say add, and it brought us to the marketplace. So I'm just going to type in cognitive services. And then just click this one here. And we'll hit Create. And we'll make a new one here. I'm gonna call my cogs services, say, Okay, I prefer to be in US East, I believe in us West, it's fine. And so in here, we'll just say my cog services. And if it doesn't like that, I'll just put some numbers in. There we go. We'll do standard so we will be charged something for that. Let's go take a look at the pricing. So you can see that the pricing is quite variable here, but it's like you'd have to do 1000 transactions. Before you are billed, so I think we're going to be okay for billing. We'll check boxes here, we'll go down below, it's telling us about responsible AI. Notice, sometimes services will actually have you checkbox it. But in this case, it just tells us there. And we'll go ahead and hit Create. And I don't believe this took very long, so we'll give it a second here. Yep, it's all deployed. So we'll go to this resource here. And what we're looking for are keys and endpoints. And so we have two keys and two endpoints, we only need a single key. So I'm going to copy this endpoint over, we're gonna go over to Jupiter lab, and I'm just going to paste this in here. I'm just gonna put it in all the ones that need it. So this one needs one. This one needs one. This one needs one. And this one needs one. And we will show the key here, I guess doesn't show but it copies. Of course, I will end up deleting my key before you ever see it. But this is something you don't want to share publicly. And usually, you don't want to embed keys directly into a notebook. But this is the only way to do it. So this is how it is with Azure. So yeah, all our keys are installed. Going back to the cognitive services, nothing super exciting here. But it does tell us what services work with it. You'll see there's an asterisk beside custom vision, because we're gonna access that through another app. But yeah, cognitive services all set up. And so that means we are ready to start doing some of these labs. Okay. All right. So let's take a look here at computer vision first. And computer vision is actually used for a variety of different services. As you will see, it's kind of an umbrella for a lot of different things. But the one in particular that we're looking at here is describe image in stream. If we go over here to the documentation, this operation generates description of image in a human readable language. And with complete sentences, the description is based on a collection of content tags, which also returned by the operation. Okay, so let's go see what that looks like in action. So the first thing is, is that we need to install this Azure Cognitive Services vision computer vision. Now we do have a kernel and these aren't installed by default, they're not part of the machine learning the Azure Machine Learning SDK for Python, I believe that's pre installed. But these AI services are not. So what we'll do is go ahead and run it this way. And you'll notice where it says pip install, that's how it knows to install. And once that is done, we'll go run our requirements here. So we have the OS, which is for usually handling up like OS layer stuff, we have met matplotlib, which is to visually plot things, and we're gonna use that to show images and draw borders, we need to handle images. I'm not sure if we're using NumPy here, but I have NumPy loaded. And then here we have the Azure Cognitive Services vision, computer vision, we're going to load the client. And then we have the credentials. And these are generic credentials for the cognitive services credentials. It's commonly used for most of the services and some exceptions, they the API's do not support them yet, but I imagine they will in the future. So just notice that when we run something, it will show a number. If there's an asterisk, it means it hasn't ran yet. So I'll go ahead and hit play up here. So it was an Astra can we get her to, and we'll go ahead and hit play again. And now those are loaded in and so we'll go ahead and hit play. Okay, so here we have just packaged our credentials together. So we passed our key into here, and then will now load in the client, install, pass our endpoint and our key. Okay, so hit play. So now we we just want to load our image. So here we're loading assets data dot jpg, just make sure that that is there. So we have assets, and there it is. And we're going to load it as a stream because you have to pass streams along. So hit play. You'll see that it now ran. And so now we'll go ahead and make that call. Okay, great. And so we're getting some data back. And notice we have some properties person while indoor man pointing captions. It's not showing all the information, sometimes you have to extract it out. But we'll take a look here. So this is a way of showing matplotlib in line. I don't think we have to run it here, but I have it in here anyway. And so what it's going to do is it's going to show us the image, right? So it's going to print us the image, and it's going to grab whatever caption is returns to see how there's captions. So we're going to iterate through the captions. That's going to give us a confidence score saying it thinks it's this so let's see what it comes up with. Okay, and so here it says brand spider spider looking at a camera. So that is the actor who plays data on Star Trek as a confidence score. 57 point 45% even though it's 100% correct, they probably don't know contextual things like in the sense of like pop culture, like they don't know probably search for characters, but they're gonna be able to identify celebrities because it's in their database. So that is the first introduction to computer, computer vision there. But the key things you want to remember here is that we use this describe an image stream. And that we get this confidence score and we get this contextual information. Okay. And so that's the first I'll move on to maybe custom vision next. Alright, so let's take a look at custom vision. So we can do some classification and object detection. So the thing is, is that it's possible, it's possible to launch custom vision through the Marketplace. So if we go, we're not going to do it this way. If you type in custom vision, it never shows up here. But if you go to the marketplace here, and type in custom vision, and you go here, you can create it this way. But the way I like to do it, I think it's a lot easier to do is we'll go up the top here and type in custom vision.ai. And you'll come to this website. And what you'll do is go ahead and sign in, it's going to connect to your Azure account. And once you're in, you can go ahead here and create a new project. So the first one here is I'm just gonna call this the Star Trek crew. We're gonna use this to identify different Star Trek members, we'll go down here, and we haven't yet created a resource. So we'll go create new, my custom vision resource. We'll drop this down, we'll put this in our cog services, we'll go stick with us West as much as we can. Here, we have fo and so fo is blocked out for me to just choose. So I think fo is the free tier, but I don't get it. And once we're back here, we'll go down below and choose our standard. And we're going to have a lot of options here. So we have between classification and object detection. So classification, is when you have an image and you just want to say, what, what is this image, right. And so we have two modes where we can say, let's apply multiple labels. So let's say there were two people in the photo or whether there was a dog and cat. And I think this example is a dog and a cat. Or you just have a single class where it's like, what is the one thing that is in this photo, it can only be of one of the particular categories. This is the one we're going to do multiclass, we have a bunch of different domains here. And if you want to, you can go ahead and read about all the different domains and their best use case, but we're going to stick with a two that is optimized for it. So that's faster, right. And that's really good for our demo. So we're going to choose general a two, I'm going to go ahead and create this project. And so now what we need to do is start labeling our arc our content. So what we'll do is I just want to go ahead and create the tags ahead of time. So we'll say Wharf will have data. And we'll have Crusher. And now what we'll do is we'll go ahead and upload as images. So, you know, we upload in the Jupyter Notebook, but it was totally not necessary. So here is data, because we're going to do it all through here. And we'll just apply the data tag to them all at once, which saves us a lot of time, I love that will upload now Worf. And I don't want to upload them all I have this one quick test image we're going to use to make sure that this works correctly. And I'm going to choose Worf. And then we'll go ahead and add Beverly. There she is. Beverly Crusher. Okay, so we have all of our images. And I don't know how this one got in here, but it's under Worf, it works out totally fine. So what I want to do is go ahead and train this model, because they're all labeled. So we have a ground truth. And we'll let it go ahead and train. So we'll go and press train. And we have two options, quick training, advanced training, advanced training, where we can increase the time for better accuracy. But honestly, we just want to do quick training. So I'll go ahead and do quick training. And it's going to start it's iterative process. Notice on the left hand side, we have probability threshold, the minimum probability score for a prediction to be valid when calculating calculating precision, and recall. So we, the thing is, is that if it doesn't at least meet that requirements, it will quit out. And if it gets above that, that it might quit out early, just because it's good enough. Okay. So training doesn't take too long, it might take five to 10 minutes, I can't remember how long it takes. But what I'll do is I'll see you back here in a moment, okay. All right. So after waiting a short little while here, it looks like our results are done, we get 100% matches. So these are our evaluation metrics to say whether the model was achieved its actual goal or not. So we have precision recall. And I believe this is average precision. And so it says that it did a really good job. So that means that it should have no problem matching up an image. So in the top right corner, we have this button that is called Quick tests. And this is going to give us the opportunity to quickly test these. So what we'll do is browse our files locally here. And actually I'm going to go to Yeah, we'll go here and we have war. And so I have this quick image here, we'll test that we'll see if it actually matches up to be worth. And it says 98.7%. Worse, that's pretty good. I also have some additional images here I just put into the repo to test against, and we'll see what it matches up. Because I thought it'd be interesting to do something that is not necessarily them, but it's something pretty close to, you know, it's pretty close to what those are. Okay. So we'll go to crew here, and First we'll try you. Okay, and who is the Borg, so he's kind of like an Android. And so we can see he mostly matches to data. So that's pretty good. We'll give another one go. martock is a click on so he should be matched up to Worf. Very strong match to work. That's pretty good. And then polaski. She is a doctor and female, so she should get matched up to Beverly Crusher. And she does. So this works out pretty darn well. And I hadn't even tried that. So it's pretty exciting. So now let's say we want to go ahead and well, if we want to make predictions, we could do them in bulk here. I believe that you could do them in bulk. But anyway. Yeah, I guess I always thought this was like, I could have swore, yeah, if we didn't have these images before, I think that it actually has an upload option, it's probably just a quick test. So I'm a bit confused there. But anyway, so now that this is ready, what we can do is go ahead and publish it so that it is publicly accessible. So we'll just say here, crew model. Okay, and we'll drop that down, say publish. And once it's published, now we have this public URL. So this is an endpoint that we can go hit programmatically. I'm not going to do that. I mean, we could use postman to do that. But my point is, is that we've basically figured it out for classification. So now that we've done classification, let's go back here to the division here. And let's now let's go ahead and do object detection. Okay. Alright, so we're still in custom vision, let's go ahead and try out object detection. So object detection is when you can identify particular items in a scene. And so this one is going to be combat just we're going to call it because we're going to try to detect combat, we have more domains, here, we're gonna stick with a general a one. And we'll go ahead and create this project here. And so what we need to do is add a bunch of images, I'm going to go ahead and create our tag, which is going to be called combat, you can look for multiple different kinds of labels, but then you need a lot of images. So we're just gonna keep it simple and have that there, I'm going to go ahead and add some images. And we're going to go back a couple steps here, into our objects. And here I have a bunch of photos, and we need exactly 15 to train. So we got 12345678910 1112 1314 1516. And so I threw an additional image in here, this is the batch test. So we'll leave that out. And we'll see if that picks up really well. And, yeah, we got them all here. And so we'll go ahead and upload those. And we'll hit upload files. Okay. And we'll say done, and we can now begin to labels. We'll click into here and what I want to do if you hover over it should start detecting things. If it doesn't, you can click and drag bolt, click this one. They're all con badges, so we're not going to tag anything else here. Okay. So go here, hover over is it gonna give me the combat? No, so I'm just right clicking and dragging to get it. Okay. Okay, do we get this combat? Yes. Do we get this one? Yep. Simple as that. Okay, it doesn't always get it, but most cases it does. Okay, didn't get that one. So we'll just drag it out. Okay, it's not getting that one. It's interesting. Like, that one's pretty clear. But it's interesting what it picks out and what does what does not grab it. So it's not getting this one, probably because the photo doesn't have enough contrast. And this one has a lot hoping that that gives us more data to work with here. Yeah, I think the higher the contrast easier for it to detect those. It's not getting that one. Not getting that one. Okay, there we go. Yes, there are a lot I know as some of these ones that are packed, but there's only like three photos that are like this. They have badges but they're slightly different. So we're gonna leave those out. I think it actually had that one, but we'll just tag it anyway. And hopefully this will be worth the effort here. There we go. I think that was the last one. Okay, great. So we have all of our tag photos. And what we can do is go ahead and train the model, same option, quick training, advanced training, we're gonna do a quick training here. And notice that the options are slightly different, we have probably threshold. And then we have overlap thresholds. So the minimum percentage of overlap between predicted bounding boxes and ground truth boxes to be considered for correct prediction. So I'll see you back here when it is done. Alright, so after waiting a little bit a while here, it looks like it's done. It's trained. And so precision is at 75%. So precision, the number will tell you if a tag is predicted by your model, how likely that it's likely to be. So how likely did a guess right? Then you have recall? So the number will tell you out of the tags, which should be predicted correctly, what percentage does your model correctly find? So we have 100%. And then you have mean average precision, this number will tell you the overall object detector performance across all the tags. Okay, so what we'll do is we'll go ahead and do a quick test on this model. And we'll see how it does. I can't remember if I actually even ran this. So it'll be curious to see the first one here. It's not as clearly visible, it's part of their uniform. So I'm not expecting you to pick it up. But we'll see what it does. It picks up pretty much all of them. exception, this one is definitely not a con badge. But that's okay. Alicia suggests obviously, the probability is above the selected threshold. So if we increase it, we'll just bring it down a bit. So there it kind of improves it. If we move it around back and forth. Okay. So I imagined it via the API, we could choose that let's go look at our other sample image here. I'm not seeing it. Where did I save it? Let me just double check, make sure that it's in the correct directory here. Okay. Yeah, I saved it to the wrong place just a moment. I will place it just call that bench test to one second. Okay, and so I'll just browse here again. And so here we have another one. See if it picks up the badge right here. There we go. So looks like it works. So yeah, I guess custom vision is pretty easy to use, and pretty darn good. So what we'll do is close this off and make our way back to our Jupiter labs to move on to our our next lab here, okay. All right, so let's move on to the face service. So just go ahead and double click there on the left hand side. And what we'll do is work our way from the top. So the first thing we need to do is make sure that we have the computer vision installed. So the face service is part of the Computer Vision API. And once that is done, we'll go ahead and do our imports. Very similar to last one. But here we're using the face client, we're still using the cognitive service credentials will populate our keys, will you make the face client and authenticate. And we're going to use the same image we used prior with our computer vision, so the data one there, and we'll go ahead and print out the results. And so we get an object back. So it's not very clear what it is. But here if we hit show, okay, here, it's data, and it's identifying the face IDs are going through this code. So we're just saying open the image, we're going to set up our figure for plotting, it's going to say, Well, how many faces did it detect in the photo, and so here it says, detected one face, it will iterate through it. And then we'll create a bounding box around the images, we can do that because it returns back the face rectangles, we get a top left, right, etc. And we will draw that wrangle on top. So we have magenta, I could change it to like three if I wanted to. I don't know what the other colors are. So I'm not even going to try but yeah, there it is. And then we annotate with the face ID that's the unique identifier for the face. And then we show the image. Okay, so that's one. And then if we wanted to get more detailed information, like attributes such as age, emotion, makeup or gender, this resolution image wasn't large enough. So I had to find a different image and do that. So that's one thing you need to know is if it's not large enough, we won't process it. So we're just loading data large. Very similar process, but it is the same thing detect with stream but now we're passing in return face attributes. And so here we're saying the attributes we want. And there's that list and we went through it in the lecture content and so here We'll go ahead and run this. And so we're getting more information. So that magenta line is a bit hard to see. I'm just gonna increase that to three. Okay, still really hard to see. But that's okay. So approximately age 44, I think the actor was a bit younger than that. Data technically is male presenting, but he's an Android. So it doesn't necessarily have a gender, I suppose. He actually is wearing a lot of makeup. But all it detects is it I guess it's only Pickler on the lips and the eyes. So it says he doesn't have makeup. So maybe there's a color, you know, like eyeshadow or stuff. And we would detect that in terms of personality. I like how he's a 002 points, percent. Sad, but he's neutral, right. So just going through the code here very quickly. So again, it's the number of faces so it detected one face. And then we draw a bounding box around the face for the detected attributes, it's returned back in the data here. So we just say, get the phase attributes, turn it into a dictionary. And then we can just get those values and iterate over it. So that's as complicated as it is. And so there we go. Alright, so we're on to our next cognitive service. Let's take a look at form recognizer. Alright, and so form recognizer, it tries to identify, like forms and turns them into readable things. And so they have one for receipts in particular. So at the top, finally, we're not using computer computer vision, we actually have a different one. So this one's Azure AI form recognizer. So run that there. But this one in particular isn't up to date in terms of using it like, notice, all the other ones are using the cognitive service credential. So for this, we actually had to use the Azure Key credential, which was annoying, I tried to use the other one to be consistent, but I couldn't use it. Okay, so what we'll do is run our keys like before, we have a client very similar process. And this time, we actually have a receipt. And so we have begin recognize receipt. So it's going to analyze the receipt information. And then it's what it's going to do is show us the image. Okay, just so we have a reference to look at the images and actually yellow, it's a white background. I don't know why when it renders out here, it does that, but that's just what happens. And it even obscures the server name. I don't know why. But anyway, if we go down below, this is returned results up here, right, so we got our results. And so if we just print out the results, here, we can see we get a recognized forum back, we get fields, and some additional things. And if we go into the fields itself, we see there's a lot more information, if you can make out like here, it says merchant phone number, form field label value, and there's a number 512707. So for these things here, like the receipts, if we can just find the API quickly here, it has predefined fields. I'm not sure. Yeah, business card, etc. Like if we just type in merchant, I'm just trying to see if there's a big old list here. It's not really showing us a full list. But these are predefined things that are returned, right? So they've defined those. Maybe it's over here. There we go. So these are the predefined ones that extracts out. So we have receipt, type, merchant name, etc, etc. And so if we go back to here, you can see I have a field called merchant name. So we get there it says Alamo Drafthouse cinema, let's say we want to try to get that balance. Maybe we can try to figure out which one it is. I never ran this myself when I made it. So we'll see what it is. But here it has total price. What's interesting is that these, this is a space. So it's a kind of unusual, you think it'd be together, but let's see if that works. Okay, doesn't like that. Maybe that's just a typo on their part. Okay, so we get none. Let's try price. See what it picks up? Nope, nothing. We know that the phone numbers there. So we'll give the phone number. There we go. So you know, it's an OK service. But, you know, you know, your your mileage will vary based on what you do there. Maybe we could try total, because that makes more sense, right? Ah, yeah, there we go. Okay, great. So yeah, it is pulling out the information. And so that's pretty much all you need to know about that service there. Okay. Let's take a look at some of our OCR capabilities here. I believe that's in computer vision. So we'll go ahead and open that up. At the top here we'll install computer vision as we did before, very similar to the other computer vision tasks, but this time we have a couple of ones here that I'll explain that as we go through here. We'll load our keys. We'll Do our credentials will load the client. Okay, and then we have this function here called printed text. So what this function is going to do is it's going to print out the results of whatever text it processes. Okay, so the idea is that we're going to feed in an image, and it's going to give us back out the text for the image. So we'll run this function. And I have two different images, because I actually ran it on the first one, and the results were terrible. And so I got a second image and it was a bit better. Okay, so we'll go ahead and run this, it's going to show us the image. Okay, and so this is the photo, it was supposed to extract out Star Trek The Next Generation, but because of the artifacts and size of the image, we get back, not English, okay. So you know, maybe a high resolution image, it would have a better a better time there. But that is what we got back. Okay. So let's go take a look at our second image and see how it did. And this one, I'm surprised that I actually extracts out a lot more information, you can see realize a hard time with the Star Trek font, but we get Deep Space Nine, nine, a visitor tells all life death, some errors here, so it's not perfect. But you know, you can see that it does something here. Now there is the iOS. This is like for OCR, where we have like first very simple images and texts. This is where we use the recognized printed text in stream. But if we're doing this for larger amounts of text, and we want to do this, want this analyzed a synchronously, then we want to use the read API, and it's a little bit more involved. So what we'll do here is load a different image. And this is a script, we'll look at the image here in a moment. But here we read in stream, and we create these operations. Okay. And what it will do is it will asynchronous asynchronously send all the information over. Okay. So I think this is supposed to be results here. Minor typo. And we will go ahead and give that a run. Okay, so here you can see it's extract out the image if we want to see this image. I thought I thought I showed this image here, but I guess I don't. Yes, this plot image here to show us the image. path. It's up here. It doesn't want to show us it's funny because this one up here is showing us No problem, right? Um, well, I can just show you the image. It's not a big deal. But I'm not sure why it's not showing up here today. So if we go to your assets here, I go to OCR. I'm just gonna open this up. Hope it's opening up in Photoshop. And so this is what it's transcribing. Okay, so this is a thing. This is like a guide to Star Trek where they talk about like, you know, what, what makes Star Trek Star Trek. So just looking here, it's actually pretty darn good. Okay. But like read API is a lot more efficient, because it can work asynchronously. And so when you have a lot of texts, that's what you want to do, okay? Like it's feeding in each individual line, right, so that it can be more effective that way. So let's go look at some handwritten stuff. So just in case the image doesn't pop up, we'll go ahead and open this one. And so this is a handwritten note that William Shatner wrote to a fan of Star Trek, and it's basically incomprehensible. I don't know if you can read that here. But see, was very something he was something hospital and healthy was something he was something I can't even read it. Okay, so let's see what the machine thinks here. And it says image path, it's called path. Let's just change that out. We hadn't run that. Run that there. And we'll go ahead and run it. And here we got the image. So poner us very sick, he was the hospital his BD was, etc. Beat nobody lost. His family knew Captain halden. So reads better than how I could read it, honestly, like it is. It's really hard, right? Like, if you looked at this, like, that looks like difficult was bt healthy. I could see why it's guessing like that, right? dying. It's like that looks like dying to me. You don't I mean, so it's just poorly hand handwritten, but I mean, it's pretty good for what it is. So yeah, there you go. Alright, so let's take a look at another cognitive service here. And this one is text analysis. And so what we'll do is install the Azure cognitive services, language text analytics here. So go ahead and hit run. Alright, and once that's installed, this one actually is using the cognitive services credential, so it's a little bit more standard with our other ones here. We'll go ahead and run that there. We'll make our credentials low clients. And this one, what we're going to do is try to determine sentiment and understand why people like a particular movie or not. So I've loaded a bunch of reviews, they are again, I can show you the data, if it helps. And so I'm just trying to find my right folder here. And so if we go back, look, our movie reviews, here's like a review, someone wrote, first contact just works. It's works as a rousing chapter in the Star Trek to lesser set works as a mainstream entertainment. So different reviews for Star Trek First Contact, which was a very popular movie back in the day. So what we'll do is as we will load the reviews, so it's just iterating through the text files and showing us what the reviews are. so here we can see all the written text had a lot of trouble getting the last one to display, but it does get loaded in. And so here we're using the the text analysis to show us key phrases because maybe that would give us an indicator. And so that's the object back but maybe that'll give us an indicator as to like what people are saying as important things so here we see Borg ship, enterprise smaller ship escapes, neutral zone travels, contact damage, co writer beautiful mind sophisticate science fiction, best whales, Leonard Nimoy. Okay. wealth of unrealized potential filmmaker john Franks. Okay, so very interesting stuff as here Borg ship again, you've seen Borg ship a lot. So that is kind of key phrases, let's go get Cust or customer sentiment or how people felt about it, do they like it or not. And so here, we just call sentiment. And what we'll do is if it's above five, then it's positive, and it's below five, then it's a negative review. I think most people thought it was very good film. So this one says it's pretty low nine. So let's go take a look at that one. It wasn't actually showing rendered there. So maybe we'll have to open it up manually. See if that's actually accurate, it's empty. So there you go. I guess we had a blank one in there. I must have forgot to paste it in. But that's okay. That's a good indicator that, you know, that's what happens if you don't have it. So let's look at number one, then, which is actually this one is nine, this is 04. This one here is eight. So open up eight. When the board launch on Earth, the enterprise is sent to the neutral zone, etc, etc. However, smaller ship escapes traveled of enterprise falls back. Meanwhile, the survivors, so like this is a synopsis. It doesn't say whether they like it, or they don't. But it was before, I guess. So there's nothing positive about it. Right? If we were looking at one that was this one's pretty low, which is no, no, it's not. It's one. So it seems like this person probably really liked it. Or no, I guess that's actually pretty low. Because it's one it's not nine, nine is very high. Let's take a look at this one. Review number two. If we go up here, the dog has improved the Sorry, I'm going to turn the show but there's a wealth of unrealized potential. So that's a fair one saying that maybe they don't like it as much. I don't know if they give it two stars, right, we could probably actually correlate it with the actual results, because I did get these off of IMDb and Rotten Tomatoes. But yeah, there you go. That is text analysis. Alright, so now we're on to q&a maker. And so we're not going to need to do anything programmatically, because q&a maker is all about no code or low code to build out a questions and answers bot service. So what we'll do is go all the way up to here. And I want you to type in Q and maker.ai. Because as far as I'm aware of so accessible through the portal, sometimes you can find these things. Again, if we go to the marketplace. I'm just curious. I could just take a look here really quickly. Whenever it decides to log us in here. Okay, great. So I'll go over to marketplace. And probably we type in q&a. Maybe we do something here q&a. Yep. So we go here. Give it a second here. Seems like Azure is a little bit slow right now. It's usually very fast. But you know, the service varies. Well, it's not loading for me right now. But that's okay, because we're not going to do it that way. Anyway. So you can go to q&a maker.ai. And what I want you to do is go all at the top of the right corner, and we'll hit sign in. And what we'll be doing is connecting via our single sign on with our account, so it already knows I have an account there. I'm gonna give it a moment here. And I'm going to go ahead and just give it a second. There we go. So it says I don't have any knowledge base, which is true. So let's go ahead and create ourselves a new knowledge base. And here we have the option Between stable and preview, I'm going to stick with stable because I don't know what's in preview. I'm pretty happy with that. So we need to connect q&a service q&a service to our knowledge base. And so back over here in Azure, actually, I guess we do have to make one now that I remember, we actually have to create a q&a maker service. So I'll go down here and put this under my cog services will say my queue at a queue and a service might complain about the name. Yep, so I'll just put some numbers here. We will pick a free tier sounds good, I'll go free what I actually get the option, that's what I will choose. Down below, we'll choose free again, USB sounds great to me, it generates out the name, it's the same name as here. So that's fine. We don't need App Insights, I'm going to leave it enabled, because I think it changes the standard or zero when you do not have an enabled unusually. And so we will create our q&a maker service, give it a moment here. And it says I remember it will say like, even if you try it, it might have to wait 10 minutes for it to create the service. So even though even after it's provisioned, it will take some time. So what we should do is prepare our doc because it can take in a variety different files. I just want to show you here that the q&a that a whole paper here formatting the guidelines. And basically it's pretty smart about knowing where headings and answers is. So for unstructured data, we just have a heading, and we have some text, let's write some things in here that we can think of. Since we're all about certification, we should write some stuff here. So how many AWS certifications are there? I believe right now, there are 11. eight of us certifications. Okay. And maybe if we use our headings here, this would probably be a good idea here. Yeah. Okay. Another one could be how many fundamental Azure certifications are there. And we'll give this a heading. And we'll say there are three as your I think there's three. There's other ones, right, like pirate power platform and stuff. But just being Azure specific. There are three as your fundamental certifications, certification, so we have the DP 900, the AI 900. The az 900. I guess there's four, there's the SC 900. Right. So there are four. Okay. We'll say which is the hardest. Azure Azure Association certification. And what we'll say here is, I think, I mean, it's my is my opinion is it's the Azure administrator, had some background noise there. That's why I was a bit pausing there. But the Azure minister, az 104, I would say that's the hardest, which is harder. The AWS or Azure certifications, I'd say Azure certifications are harder. Because they check exact steps for implementation where AWS focuses on concepts. Okay, so we have a bit of a knowledge base here. So I'll save it. And assuming that this is ready, because we did a little bit time to put this together. We'll go back to q&a, hit a refresh here. Give it a moment, drop it down, choose our service. And notice here that we have chitchat extraction and only extraction we're going to do to chat. I will say my or this is the reference can be changed any time this would be like a certification q&a. So here, we want to populate. So we'll go to files here, I'm going to go to my desktop. And here it is. I'll open it. We will choose professional town. Go ahead and create that. And so I'll see you back here in a moment. Alright, so after waiting a short little time here, it loaded in our data. So you can see that it figured out which is the question which is the answer and also has a bunch of default. So Here if somebody was at something very silly, like, can you cry, I'll say I don't have a body. It has a lot of information pre loaded for us, which is really nice. Why don't we go ahead and test this? We could go and say, we'll go here and then we'll write in, say, like, hello. Say boring. This is good morning. Okay, so we'll say, how many certifications are there? We didn't say AWS, but let's just see what happens. So to kind of infer even though we didn't say AWS in particular, so I noticed that there's AWS and Azure, so how many fundamental Azure certifications, things like that, and so chose AWS. So it's not like the perfect service, but it's pretty good. I wonder what would happen if we placed in one that's like Azure, I don't know how many Azure certs there are, we'll just say like, there's 1112, I can't ever remember, they're always adding more. But I want to close this here. There we go. So let's just go add a new key pair here. And we'll say, how many Azure certification are there, I should have said certifications, I'll probably just set one moment. So there, there are 12, Azure certifications. Who knows how many they have, they have like 14 or something, say like, between 11 and 14. They just added they just updated them too frequently. I can't keep track. So we'll go here and we'll just say certifications. And we will save and retrain. So we'll just wait here a moment. Great. And so now we'll go ahead and test this again. So we'll say how many certifications are there? I see it's pulling the first answer. If I say Azure, let's just see if it gets the right one here. How many Azure certifications are there? Okay, so, you know, maybe you'd have to say you'd have to have a generic one for that match. So if we go back here, and we say, how many certifications are there? You say, you know, like, which certification? Which cert cloud service provider. Here we got AWS Azure. Follow prompt, you can use guides through conversational flow prompts are used to link q&a pairs and can be displayed. I haven't used this yet. But I mean, it sounds like something that's pretty good. Because there is multi turn into so the idea is that if you had to go through multiple steps, you could absolutely do that. We've tried a little bit here, fall prompt you can use to guide us to convert props are used to link q&a pairs together, text or button for suggested action. Oh, okay, so maybe we would just do like AWS link to q&a. And then so search an existing q&a or create a new one. So let's say like, how many eight of us, oh, okay, we're typing in context, this follows up will not be understood out of the context flow. Sure. Because it should be within context, right. And here, we can do another one we say like Azure will say, how many Azure contacts only. Whoops, that got away from me there. We'll save that. And what we'll do is save and train. Go back here. And we'll say, how many certifications are there? Enter. So we have to choose AWS. So there we go. So we got something that works pretty good there. Since I'm happy with it, we can go ahead and go and publish that. So let's say publish. And now that it's published, we could use postman or curl to trigger it. But what I want to do is create a bot because with Azure bot services, then we can actually utilize it with other integrations right. It's great way to use your bot or to actually host your bot. So we'll go over here and link it over. If you don't click it, it doesn't pre loaded in so it's kind of a pain. If you lose Got to go back there and click it again. But let's just say certification. que en de. And we will look through here. So I'm going to go with free premium messages, 10k 1k Premium message units, messages, I'm kind of confused by the pricing. But f0 using means free. So that's what I'm gonna go for that SDK or no GS, I'm gonna use no GS now that we're gonna do anything there with it. Go ahead and create that. And I don't think this takes too long. We'll see here. Just go ahead and click on that there. I'll just wait here a bit. I'll see you back here in a moment. All right. So after waiting, I don't know about five minutes there. It looks like our bots services deployed, we'll go to that resource there. You can download the bot source code. Actually, I never did this. So I don't know what it looks like. So be curious to see this. Just to see what the code is. I assume that because we chose chose no GS, it would give us that is the default there. So download this code as you're creating the source IP. Not sure how long this takes. Maybe regretting clicking on that. But what we'll do is we'll go in the left hand side here to channels because I just want to show here. Yeah, that didn't download. We'll try it here in a second. But what we'll do is we'll go back up profile. unspecified bar we talked about. Yeah, maybe it needs some time. So you know, maybe we'll just give the bot a little bit of time here. I'm not sure why it's giving us a hard time because this bot is definitely deployed. If we go over to our bots, right. bot services, it is here. Sometimes there's like latency, you know, with Azure. Oh, there we go. Okay, see works now. Fine, right. And so I want to show you that there's different channels and these are just easy ways to integrate your bot and different services. So whether you want her to use it with Alexa GroupMe, Skype telephony, Twilio Skype for Business, apparently they don't have that anymore. Because they get small teams now, right. keek, which I don't know, people still use that Slack, which that discord, telegram Facebook, email. That's kind of cool. But teams teams is a really good one. I use teams. There's a direct line channel, I don't know what that means. And there's web chat, which is just having like an embed code. So if we go over, we can go and test it over here to start testing our web chat. And so it's the same thing as before, we just say things like, how many certifications are there? Azure, and get a clear answer back. We'll go back up to our overview. Let's try see if we can download that code. Again. I was kind of curious what that looks like. Yes, it will download a lot of code a. There we go. So now we can hit download. And so there is the code, I'm going to go ahead and open that up. So yeah, I guess when we chose JavaScript, that made a lot more sense. Let's give it a little peek here. I'm just going to drop this on my desktop here. So just to make a new folder here and call this bot code. Okay, I know you can't see what I'm doing here. But let's go here, and gret, double click into here, and then just drag that code on him. And then what we can do is open this up in VS code, I should have VS code running somewhere around here. I'm gonna go ahead and open that off screen here. I'll just show you my screen in a moment. Say show code, oops, File, Open Folder. botcon code, okay. And all the way back here. And so we got a lot of code here. never looked at this before. But you know, I'm a pretty good programmers. So it's not too hard for me to understand. So it's like your API request, things like that. I guess it would just be like, if you needed to integrate into your application, then it kind of shows you all the code. They're just trying to see our dialogue choices. Nothing super exciting. Okay, you know what I go and make the Was it the AI are the 100 whatever the data scientists courses, I'm sure I'll be a lot more thorough here. But I'm just curious as to what that looks like. Now, if we wanted to have an easy integration, we can get an embedding code for this. So if we go back to our channels, I believe we can go and edit. Ah, yeah. So here we have a code. So what I'll do is go back to Jupiter labs, I'm just going to go make a new empty notebook. So let's go up here and say notebook. And this can be for our q&a. Doesn't really matter what Colonel, say cute and a maker. Just show like, if you wanted a very, very simple way of integrating your bot, we would go back over to wherever it is here. Here, we are going to go ahead and copy this iframe. I think it's percentage percentage HTML. So it treats this cell as HTML. And I don't have any HTML to render. So we will place that in there. And notice we have to replace our secret key. So I will go back here and I will show my key and we will copy that. And we will paste that key in here. And then we'll run this. And I can type in here. Where am I? just silly things. Who are you? How many Azure certifications? Are there? Well, I wonder if I just leave the are there off? Let's see if it's figures it out. Okay, cool. So yeah, I mean, that's pretty much it with q&a maker. So yeah, that's great. So I think we're done here. And we can move on to checking out Louis or Liu is learning understanding to make a more robust bot, okay. Alright, so we are on to our last cognitive service. And this one is going to be Louis or Luis, depending on how you'd like to say it. It's Li s, which is language understanding. So you type in L ui s.ai. And that's going to bring us up to this external websites. So part of Azure just has its own domain. And so here, we'll choose our subscription. And we have no author authoring source. So I guess we'll have to go ahead and create one ourselves. So get down here, and we will choose my cognitive services as your resource name. So my off service or my cognitive service, great new cognitive service account, but we already have one, so I don't want to make another one. Right, it should show up here, right? Or valid in the author authoring region. So it's possible that we're just in the incorrect region. So we might end up creating two of these. And that's totally fine. I don't care. It's as long as we get this work in here, because we're gonna delete everything, get the end anyway. And so just say, my cog service, too. And we'll say West us because I think that maybe we didn't choose one of these regions. Let's go double check. If we go back to our portal, just the limitations of the service, right. So we'll go to my cog services here. I just want to go cognitive services. So just want to see where this is deployed. And this is in us, West us. Yeah, so I don't know why it's not showing up there. But whatever. If that sort of wants, we'll give it what it wants, okay. shouldn't give us that much trouble, but pay, that's how it goes. And so we have an authorized authoring service, I'm gonna refresh here and see if it added a second one, it didn't. So all right. That's fine. So we'll just say, my sample bot will use English as our culture. If nothing shows up here, don't worry, you can choose it later on. I remember the first time I did this, it didn't show up. And so now we have my cog service, my custom vision service, we want cog service. So anyway, it tells us about schema, like how you make a schema animates talking about like body, action, intent, and example utterance, but we're just gonna set up something very simple here. So we're gonna create or attend, the one that we always see is flight booking. So I'll go here, do that. And what we want to do is write an under and so like, book, me a flight to Toronto. Okay. So if someone were to type that in, then the idea was it would return back the intent this value and metadata around it. And we could programmatically provide code, right? So what we need is identity identities and we can actually just click here and make one here. So enter name identity, and we'll just call this location. Okay. Here we have option machine learned and list if you flip between it. This is like a magic Have a ticket order and you have these values that can change, or you just have a value that always stays the same like lists. So that's our airport. That makes sense, we'll do that. If we go over to entities, we can see it here. Alright, so nothing super exciting there. But what I want to show you is if we go ahead, and we should probably add, fight booking should be about book flight. flight booking, flight booking. Okay, so we'll go ahead and I know there's only one, we'll go ahead and train our model. Because we don't need to know tons, right, we cover a lot in the lecture content to build a complex bot is more for the associate level. But now what we can do is go ahead and test this and we'll say, book me a flight to Seattle. Okay, and notice here it says book flight, we can go inspect it, and we get some additional data. So top scoring, so it says how likely that was the intent. Okay, so you get kind of an idea there, there's additional things here, it doesn't really matter. We'll go back here, and we will go ahead and publish our model. So we can put it into a production slot, you can see we have sentiment analysis, speech, priming, we don't care about either of those things. We can go and see where our endpoint is. And so now we have an endpoint that we can work with. So yeah, I mean, that's pretty much all you really need to learn about Louis. But I think we're all done for cognitive services. So we're going to keep around our notebook, because we're going to still use your Jupyter Notebook for some other things. But what I want you to do is make your way over to your resource groups. Because if you've been pretty clean, it's all within here, we'll just take a look here. So we have our q&a. All of our stuff here, I'm just making sure it's all there. And so I'm just gonna go ahead and delete this resource group. And that should wipe away everything, okay? For the cognitive services part. Alright, so we're all good here. And I'm just going to go off, and I'll leave this open, because it's always a pain to get back to it, reopen it, but let's make our way back to the home here and the Azure Machine Learning Studio. And now we can actually explore building up machine learning pipelines. Okay, so we are on to the ML kit, follow along here. So we're going to learn how to build some pipelines, the first i think is the easiest will be auto automated ml are also known as auto ml. The idea here is it's going to just build up the entire pipeline for us. So we don't have to do any thinking we just say what kind of model we want to run and have it to make a prediction. So what we'll do is a new automated ml, and we're going to need a data set. So I don't have one. But the nice thing is they have these open datasets. So if you click here, you'll see there is a bunch here. And a lot of these you'll come across quite often, not just on Azure, but other places like this diabetes one, I seen it like everywhere, okay. And so like, if we just go click here, and maybe we can read a bit more here. So diabetes data, set 422 samples with 10 features, ideal for getting started with machine learning algorithms. It's one of the popular psychic learn toy data sets. It's probably where I've seen it before, though it's not showing up there. You scroll on down, you can see the data sets available as your notebooks data, bricks and Azure synapse. The thing is, we have these values of age, sex, BMI, BP and y is trying to make a prediction, it's trying to say, what's the likelihood of you having diabetes or not? And so it's not boolean value. So it's not a binary classifier. It's kind of on a like you, would you be doing binary classifications? classification, say, do you have diabetes, or you can make a prediction to say, what's the likelihood or this value if you gave another value in there. But anyway, this is the predicted value, a lot of times this is x, so everything here is x. And this is considered y, the actual prediction. So sometimes it's why and sometimes it's actually named what it is. But that's just what it is here. So we'll close that off. And so we'll choose the diabetes set. And it will be data set one. And so it will worry about feedback later. So we'll click on sample diabetes will hit next. And here's going to try to figure out what kind of model that we want. We have to create a new experiments a container to run the model in so I'll just say, diabetes. My diabetes, it sounds a bit odd, but that's what it is the target call and we want to predict is seeing the train to predict is the why it's usually the why we don't have a compute cluster. So I'll go ahead and create a new compute. We have dedicated or low priority. Technically, we It is low priority, but I just want this done low priority, but don't forget to compute nodes, your job may be preempted. I'm gonna say with dedicated for the time being, we're gonna stick with CPU. If we go with this, it does take about an hour to run. So I ran this ticket about an hour. So if you don't mind, it's only going to cost you 15 cents. But if you want this done a lot sooner, I'm going to try to do something a little bit more powerful. So just trying to decide here, because if it only takes an hour, I might run it on something more powerful, that's 90 cents, that might be overkill, because it's not really deep learning. It's just a statistical, statistical stuff. So true and large data set, I wouldn't say it's large real time inference, other latency sensitive ones. How Bode? Why is this one, I'm just looking here, cuz this one's 29 cents, this one's more expensive. But it has 32 gigabytes of RAM. This was 28 Oh, 14 gigabytes of RAM and storage. So this one's our highest in the tier, again, you can choose this one, you just have to wait a lot longer, I just want to see if it finishes a lot faster, okay, without having to go to the GPU level. So I don't think GPU is gonna help too much here. The computer name is my diabetes machine. minimum number of nodes. You want to provision if you want dedicated nodes to set the count here, maximum. I guess I just want one node, right? We will go ahead and oops, complete name must be 216 characters long. What is it? Is it too long? Okay, there we go. We'll give it a moment here. Yeah, it's gonna spin up the cluster. So it does take a little bit time to start this. So I'll see you back here when this is done. Okay. Great. So after a short little wait there, it looks like our cluster is running. If we double check here, we can go to compute, I believe that shows up under here under the compute clusters. So there it is, this is slightly different. This one shows you applications and this one is just size, etc. and click in here see nodes and runtimes. We'll go make our way back here. And we'll go ahead and hit next. And notice that I think it actually will select what it generally does, it'll look at your prediction value, maybe sample a bit of it and say, okay, you probably want a regression thing. So to predict a continuous numeric values. So the thing is, is that if it was a label, like text, or if it was just zero in one, it probably would choose classification, because it's, you saw our y value is like a number that was all over the place. It thinks it's regression. So I think that's a good indicator there. So let's go with regression. You know, but you might want it as a binary classifier, but it's another story there. So it's, as soon as we created it just started, it didn't give us the option to say, hey, I want to start running it. Notice on here, it's going to do feature iteration. So that means it's automatically gonna select out features for us, which is what we wanted to do. It's set up to do regression, we have some configuration here. So training time is three hours, doesn't mean it's gonna train for three hours. But that's, I guess it's time out for it. You could set a metric score threshold, so it has to meet at least this to be successful. If it's not going to do it probably would quit out early crossmember valve or cross validation, just make sure the data is good. You can see blocked algorithms. So TensorFlow dnn, TensorFlow linear regression, if it was using dnn. So deep learning neural network, I probably would have chosen the GPU to see if it would go faster. Look at the primary metric gets normalized root square root mean square error, sometimes on the exam will actually ask you like, what's the primary metric for this thing. So it's good to take a look and see what they actually use. For that, I'll probably be sure to highlight that stuff in the actual lecture content. But this will take some time to run. We have data guard rails, it will actually not populate, I guess until we've ran it. So we'll just let it run. And I'll see you back here when it's done. Okay. All right. So after a very, very, very long wait, our auto ml job is done. It took 60 minutes. Using a larger instance, didn't save me any time. I don't know if maybe if I ran a GPU instance, it would be a lot faster. I'd be very curious to try that out. But not something for this certification course. So we go into here and yeah, the cheaper instance was the same amount of time. So it probably just needs GPUs. It really depends on the type of models it's running. So we have a bunch of different algorithms in here. It ran about 42 different models. I thought of like last time I ran it, I saw a lot more but you can see there's all kinds of models that it's running and then it's going to choose the top candidates. So it shows Voting ensemble. So ensemble is we don't cover really in the course because it gets too much into ml, but ensemble is when you actually use two different weaker models and combine the results in order to make a more powerful ml model. Okay. So here, we'll get some explanation. I tried this before, and I didn't get really good information. So if we go here, like I don't have anything under model performance. So this tab requires a ray of predicted values from the model to be supplied. We didn't supply any, so we don't get any Data Explorer. So select a cohort of the data, that all the data is what we have here. So like, here, we were seeing age. And I guess it's just giving us an indicator about the age information. Use the slider to show just descending feature importance, select up to three cohorts to see the feature important side by side. Okay. So I guess, s five and BMI. I don't know what s five is, we'd have to look up the dataset. BMI is your body mass index. So that's a clear indicator as to what affects whether you have diabetes or not. So that makes sense. age doesn't seem to be a huge factor, which is kind of interesting. Individual feature importance, we can go here and just kind of like narrow in and say, Okay, well, why is this outlier over here? And they're like age 79. Right? So it's kind of interesting to see that information. So it does give you some x explanation as to, you know, why things are why they are. Over here, we have a little bit more different data. This is kind of interesting model performance. I don't know what I'm looking at, but like here, it's over a mean squared. So it's that mean squared calculation there again? Okay. Yeah, it's something right. But anyway, the point is, is that, that we finally get metrics, I guess we always had to click there, because that makes more sense. So yeah, there's more values here. Sure. data transformation, sorts of data processing feature engineering scaling techniques, and machine learning algorithm, auto ml. So you know, if you were a real data scientists, all this stuff would make sense to you. I think just with time, it'll, it'll make sense. But even at this point, I'm not sure. And I don't care about the model, right? If you're building something for real, I'm sure the information becomes a lot more valuable. So this model is done. And the idea is that we can deploy oops, if we go back to the actual models, because we actually went into the map. So we go back to the auto ml here. I think you can deploy any model that you'd like. So you can go here and deploy this, like if you prefer a different model, you could deploy it. If we go into data guard rails, we kind of skipped over that this is a way does automatic feature rotation, so it's extracting the feature, so it handles the splitting, how it handles missing features. Hi, Carbonell nowadays, like if you have too much data, it might have to do dimensionality reduction. So that's just saying like, hey, if this is a problem, maybe we would do some pre processing or stuff to make it easier to work with the data. So if we're happy with this, we can go ahead and deploy it. So let's say deploy, just say infer my diabetes. Here we have aka s and E's. Azure Container instance. Let's do Azure Kubernetes Kubernetes services because we did the other one here. Say diabetes. Broad maybe a Ks diabetes. Oh, compute name, sorry. One of the inference ones. Okay. So in order to deploy this, we would have to create our pipeline. I'm not sure if I have enough in my quota here, but let's go give it a go. So I think what it's wanting is one of these here. I think we'd want this wherever we are, right? I'm not sure where we are. If This Is Us, east or west here. Let's go check. Studio. Azure Machine Learning hits us. No, I never did this when I was. I just use use the Azure Container instance. But I'm just curious here. say next. My diabetes prod, we will need to choose some nodes. The number of nodes multiplied by the virtual machines, number of cores must be greater or equal to 12. Okay? Now again, if you're not confident, like every concern about costs, you can just again, watch, you don't have to do right. This is again, a fundamental certification, it's not super important to get all the hands on experience yourself. But I'm just trying to explore this so we can see, right, because I don't care about costs. It's not a big deal to me, on my machine here, so probably I don't have simple must use a vn SKU with more than two cores and four gigabytes. Well, what did I choose? Did I not choose the right one? We'll try this again. Oh, I chose three. Yeah, that's fair. What did it want 12 cores set before I think. Invalid parameters, more details? Because that already exists based on that name a two. It's given us all this trouble I this one will go ahead and delete you think like, it wouldn't matter. Like I wouldn't have to delete it out. But that's fine. This one failed. Now, what's the problem? quota exceeded so I can't do it. Because I don't I'd have to go make a support request increase it. So it's not a real big deal. I guess what we could do is instead of doing it on a KS, we just deploy to container instance, if it will let us notice I don't have to fill anything additional. It'll just deploy I think. Great. And so I guess we'll let that deploy. And I'll see you back here in a bit. Okay. Alright, so I'm back here, checking it out on my are checking up on my auto ml here. So if we go over to compute, we go to inference clusters, we don't have anything under there if we go over to our experiments under our diabetes here. Because we did choose to deploy the model. Right, we clicked deploy. So it should have created an ACI instance, let's make our way over to the portal. The reason why it might not be showing up is because I'm just running out of compute. Because again, it's a quota thing. It's not a big deal for us to get a deploy. So we're gonna do anything with it. But yeah, so we can see that we have a container over here, and it's running. So we must be able to see if we go to endpoints here. Here it is. Right, I was under models as my problem. So pipeline endpoints, that would be something I think that if we had deployed our designer, I thought we would have thought under there. But here we have our binary pipeline, or our diabetes, prod pipelines. So if we wanted to like test data, you know, we could pass stuff in here. I think if we wanted to try to just like see this in action, I'm not sure if it's going to work, but we'll give it a go. So if we go into our sample diabetes data set, and we just explore some of the data, we should be able to kind of select out some values, because I don't know what these values mean. So let's just say like 36 oops, 36. But we already know that BMI is the major factor here. Sex is either one or two. So we'll say to BMI, we'll say 25.3. The BP will be 83 or whatever. Oops. 83. Here. s 160. s two can be 99.63 4545 and 5.10. The only we're running out of metrics here 82. What do I doesn't give us all them? Oh, I guess it does. It's up to six. Okay, so let's go ahead and test that. So we get and we got a result back 168. So that is auto ml all complete there for you. Yeah, so there you go. Alright, so let's take a look here at the visual designer because it's a great way to get started very easily. With If you don't know what you're doing, and you want something a little bit more advanced than auto ml and have some customization, it's great to start with one of these samples. So let's go ahead and expand and see what we have here. We have binary classification with custom Python script, tune parameters for binary classification, multi class, multi class classification, so letter recognition, text classification, all sorts of things. Usually binary classification, classification is pretty easy. I'm looking for one that is pretty darn simple. Let's go take a look here. So this says the sample shows how to filter based feature selection to selection features. binary classification, so how to predictors related to customer relationships using binary classes, how to handle imbalanced datasets, using smote. And modules, I'm not really worried about balancing customized Python script to perform cost sensitive, binary classification, tune parameters. So you tune model parameters, best models during the training process, let's go with this one. This one seems okay to me. And so what you can see here is that it's using a sample data set, I believe, I think this is a sample. And if you wanted to see all of them, you can literally drag them out here and do things with them. I haven't actually built one end to end yet for for this again, I don't think it's like super important for this level of exam. But this just shows you that there's a pre built one, if you've started to get the handle of ml, you know, the full pipeline. This isn't too confusing. So at the beginning, here, we have our classification data. And then what it's going to do is say select columns in the data set. So it says exclude column names work class, occupation, native country, so it's doing some pre processing there, excluding that data might be interesting to go look at that data set. So if we go over to our data sets tab, it should show up here, I believe. Maybe because we haven't committed or submitted this, we can't see that data set yet. But we'll look at it for a moment that we want to clean our data. So here's saying clean all the columns. So custom substitution value, see if we can see what it's substituting out. It's not saying what's so clean missing data. So I'm not sure what it's cleaning out there. But because that would suggest that it's using some kind of custom script, I'm not sure where it is. But that's okay. We have split data, pretty common to split your data. So you would have a training and test data set, it's usually really good to randomize it. So you want to randomize it, then split it. And that's, that's just so you get better results, that it has model hyper parameter tuning. So the idea is that it's going to use ml to figure out the the best parameters for tuning. Over here we have the two classes decision tree where it's going to do some work there, it's going to score our model, and then it's going to evaluate our model and see if it's successful. So this is all set up to go. So all we're going to do is go to the top here, this is setting wheel here. And we need to choose some type of compute. So I'm going to go here, and we have this one here. But I'm going to go create, as for my, my diabetes one, I'm going to go ahead and make a new one. And we're going to say, recommend using a predefined configuration to quickly set compute training. This one looks okay, I don't know if it needs two nodes. But I guess we can do this one. So we'll just say binary was just like binary pipeline. Okay. Say save, save. Hopefully, it's making good suggestion. And we will have to wait for that to spin up. It's going to take a little bit of time. Okay, so I'll see you back here in a moment. Alright, so I got a little message saying that that is ready. So what we can do, I think it was here, my notebook instance. Now that's not it, but I definitely saw a pop up on my screen. You might have saw it to you that to be paying close attention for that. But if you go over, it says that it's it's ready to go. So what I'm going to do is make my way back over here, we're going to select our compute, there is our binary pipeline, I'm going to select that. And there are some other options, we're not gonna fiddle around with that, we're going to go ahead and hit submit. So we need a new experiment. So I'm going to just say, binary pipeline. We'll hit submit. Okay, and so this is now running. So after a little while here, we're going to start seeing these go green. So this is not started. We'll give it a moment here. So we can see some kind of animation. And there it goes, it's off to the races. There's not much to do here. This is going to take a while. I don't know, I have never ran this one in particular. So I don't know if it's an hour or 30 minutes. So I'll see you back when it's done running. But yeah, it's it's not that fun to watch, but it's cool that you get a visual illustration. So I'll see you back in a bit. I just wanted to peek in here and take a look at how it's progressing here and you can see it's still going and it's just cleaning the data, it's still not done. I'm not sure how long this has been running for if we go over to our experiments, and we go into our binary pipeline, and we look at the runtime, we're about eight minutes in, and it hasn't done a whole lot. So it's still cleaning the data, I would have thought a bit, it'd be a little bit faster. I'm kind of used to using like AWS, and it goes, sage makers. This doesn't usually take this long. But I mean, it's nice that it's, it's going here. But yeah, so we're almost out of the pre processing phase. And we'll be on to the model tuning, okay. Alright, so after waiting a little while, it looks like our pipeline is done. So if we make our way over to experiments and go to binary pipeline, we can see that it took 14 minutes and 22 seconds, we can go here and just see some additional information, there's nothing really else to see we saw all the steps already ran, so you can see them all here. Okay, and so let's say we want to there's nothing under metrics, but able to actually slog data points, compare these data within across runs, really did a single run, so there's nothing to compare. So let's say we were happy with this, and we want to deploy this model, what what I'm going to do is go back to the designer, click back here. And so now in the top right corner, we can create our inference pipeline. So I can remember who submits going to run it, I don't want to run it again, I just want to go ahead and create ourselves a real time or batch pipeline, let's say real time bi pipeline here. And what this will do is it'll actually create a completely different pipeline. So here's a completely new one. But it's specifically designed to do deployment. Okay, so this is now one was for training the model. This one is actually for taking in data and doing inference. Okay, so what we can do is, we can go ahead and just submit this. That's it, we'll put this under our binary pipeline here. We'll go ahead and hit submit. And I believe that we need a different kind of compute here. I'm surprised that it's even running. I guess it has a compute there. So it's going to run and once it finishes running that I believe that we can go ahead and deploy it. Okay, so let's just wait for that to finish. All right. Alright, so after a little while, there, we ran our inference pipeline. And so it's definitely something that is ready for use. The idea is that what we actually use, it's going to go through this web service input to this web service output, but not so important at this level of certification. Let's see what it looks like to go ahead and deploy it. So yep, we have the option between a real time endpoint and an existing endpoint. We don't have an endpoint yet. So we'll just say, binary pipeline. Okay. And notice we have the option between wants it lowercase binary pipeline. And we have the option between Azure Kubernetes service and Azure Container instance, it's a lot easier to deploy, I think, to container instance. So because it will be waiting forever for Kubernetes to start up. So we're going to do container instance, we have some options like SSL and things like that, not too worried about it. So we're just going to go ahead and hit deploy. Okay. And so that is going to go ahead and deploy that. So we'll wait for this real time inference, if we go over to our compute, it should spin up. So this is for Eks. I don't know if it'll show up here. I think only I've seen things under here. But I think this will be for Azure Kubernetes service. And I don't think we're gonna see it show up under there. However, we do not need to be running this anymore. So we'll go ahead and delete the binary pipeline, because we're not, we don't have it for any use right now. And we might need to free it up for something else. Okay. So go ahead and delete it, we don't need it. And coming back to our pipeline, or designer here, I'm just trying to see where we can keep track of it. I know that it's deploying. So waiting for real time endpoint. So I'll see you back here when this is done. Okay, takes a little bit of time. Alright, so I think our pipeline is done. If we make our way over to endpoints, there it is the binary pipeline. If we wanted to go ahead there, we could test the data. And so it actually already has some pre loaded data for us. We had test. It's nice that it fills it in a we get some results back. Okay. So, I mean, that we see like scored labels and income and score probability. So things like that, that is useful. So it's getting back all all the results, but I don't think it has. Yeah, it doesn't have scored labels and scored probabilities, which is the value we want to come back here. So There are endpoints, and that is the end of our exploration with designer, okay? Alright, so let's take a look at what it would be to actually train a job programmatically through the notebook. So remember, we saw these samples over here. And so we saw this image classification, m NIST. And this is a very popular data set for doing computer vision. And these are really great, if you want to really learn you should really go through these and just read through them, because they're, they're probably very, very useful. I've done a lot of this before. So for me, it's, it's just, it's not too hard to figure out. But I've actually never ran this one. So let's run it together. Again, we want to be in Jupiter lab. So you can go here and click it there or go to the compute. If it's been a bit finicky. And just here, we'll get a tab open here. And we'll see how this goes. So what I want to do and is just make sure we're back here, I can click into this one. And we have a few. So there's part one, and then we have the deploy stage. So let's look at training. I don't know if we really need to deploy, but we'll give it a read here. So in this tutorial, you train an ml model under compute resource resources will be training and training and deployment workflow via the Azure Machine Learning service. In a notebook, there's two parts to this. This is using the amnesty data set and psychic learn. And with Azure Machine Learning probably SDK, it's a popular data set with 70,000, grayscale images, each image is handwritten digits of 28 times by 28 times pixels representing numbers from zero to nine, the goal is to create multi class classifier to divide the digits in a given image that represents. So we're gonna learn a few things here, but let's just jump into it. So the first thing is that we need to import our packages. So here, it does that map plot plot live in lines, just make sure that when we print things that we visually see them, we're going to NumPy and then matplotlib itself, the Azure ML core, and then we're going to import a workspace since we'll need one there. And then I guess it just checks the version making sure if we have the right version here. Okay, so this is one point 28. Zero, it's pretty common, even as an AWS, they'll have like a script in here to update it in case it is out of date. I'm surprised it didn't include it in here, but that's okay. We'll scroll on down. And by the way, we're using Python 3.6 Azure ML. If this is the future, that you know, they might retire the old one, you're using 3.8. But you know, to generally work if it's in their sample data set, I assume they try to maintain that. Okay, so connect to a workspace. So create a workspace object from an existing workspace reads the file config dot JSON. So what we'll do is go run that I assume it's kind of like a session. And so here it says, It's figured found our workplace. So really, it's just it's not creating a workspace, it's just returning the existing one so that we have it as a variable here, create an experiment. So that's pretty clear. We saw experiments in the auto ml and the designer. So we'll just hit run there. Okay. So we named it core ml. And we said experiment. I wonder if it actually created one yet. Let's go over to experiment to see if it's there. So there's there cool, I was fast, I thought it would like print something out, but it didn't do anything there. So creator, attach an existing compute resource by using Azure Machine compute a managed service data scientists, etc, etc. yada, yada, yada. So create a copy. creation of a compute takes about five minutes. So let's see what it's trying to create. So we have some environment variables that wants to load in I'm not sure how these are getting in here. I'm not sure we're environment variables are set in Jupiter, or even how they get fitted in. But apparently they're somewhere. But we have, it doesn't matter because these are defaulting. So here's a CPU cluster, zero and four, it's going to use a standard D two v two, that is the cheapest one that we can run. I kind of want something a little bit more powerful just for myself. Just because I want this to be done a lot sooner. But again, you know, if you're don't have a lot of money, just stick with what's there. Okay. So and this is CPU clusters. So if we go here, I just want to see what our options are. I'm not sure why it's not showing us options here. You don't have enough quota for the following VM sizes. So it probably it's because I'm running more than one VM right now. Yeah, so I've said I've hit my quota. Okay, so like I probably would have to request for an hour. So I think this is the one I'm using. What's the difference here? This standard dv two v CPUs. The same one, right? So request quota increase. I don't know if this is incident or not, I'd have to make a support ticket. All that's going to take Long. So the thing is, is that because the reason is is that I'm running the auto ml and the designer and the designer in the background here trying to create all the workshops or the, the follow along at the same time. But what I'll do is I'll just come back and when I'm not running one of those other ones, then I will, I'll come back here and continue on. But we're just here at the step, we want to create a new computer. Okay. All right, so I'm back and I freed up one of my compute instances, if I go over here, now I just have the one cluster instance for my auto ml. But what we'll do here is again, just read through this. So this will create a CPU cluster zero to four nodes, standard G two v two, I guess we'll just stick with what what is here, I'm just reading through here, it looks like it tries to find the compute target, it's going to provision it, it will create the cluster called pool for a minimum numbers of nodes for a specific time. So wait for completion. So we'll go ahead and hit play. And so that's going to go and create us a new cluster. So we're just going to have to wait a little while here for to create about five minutes, and I'll see you back here in a moment. Alright, so the cluster started up, if we go back over here, we can see that it's confirmed, I don't know why it was so quick, but it went pretty quick there. So we're on the next section here, explore the data. So download the emnes data set display some sample images. So it's just talking about it being the open data set. The code retrieves in the file data set object, which is a subclass of data set file data set references a single or multiple files of any format in your data store. The class provides you with the ability to download or mouth files to your computer by creating a reference to the data source location. Additionally, you register the data set to your workspace for easy retrieval. During training. There's a bit more how tos, but we'll give it a good read here. So we have the open data set and missed. It's kind of nice that they have that reference there. So we have a data folder, we make the directory, we are getting the dataset, we download it, and then we are registering it. So let's go ahead and run that. Not sure how fast but it shouldn't take too long as it's running. We'll go over here the left hand side refresh, and we'll see if it appears. Not as of yet. There it is. Go into here, maybe explore the data. I'm not sure how it would look like because these are all images, right? Yeah, so they're in you byte Gz. So they're in compressed files, we're not going to be able to see within them but they're definitely there. We know they're there. So that that is now registered into our data set, display some sample images, so load the compressed into a files into NumPy, then use matplotlib plot 30 random images from the data set from above note, the step requires load data function, it's included in the utils. py file is included in the sample folder, we have it over here, we just double click, very simple file to load data. And we'll go ahead and run that. And it's pretty, pretty simple here. So load data x train x test it are we setting up our training and testing data here, it kind of looks like it because it says train and test data. That's when we usually see that kind of split. And again, it's doing a random split. So that sounds pretty good to me. Let's show some randomly chosen images. Yeah, so I guess they do set up the training data here. And then down below, we're actually showing the images. So here's some random images train on a remote cluster. So for this task to submit the job to run on the remote training cluster to set up earlier submit your job. Create the directory, create a training script created scripts, run configuration, submit the job. So first, we'll create our directory. And notice it created this directory over here. Because I guess it's going to put the training file in there. And so this will actually write to a training file. This makes quite a bit of sense. So if we click into here, it should now have a training file. It'll just give it a quick read, see what's going on here. So a lot of times when you create these training files you have to do and this is the same if you're using AWS, like when you're creating train, like or Sage maker, you create a train file because it's part of frameworks is just how the frameworks work. But you'll have these arguments. So it could be like parameters to run for training. And there could be a whole sorts of ones here. Here they are loading in the training and testing data. So it's the same stuff we saw earlier when we were just viewing the data. Here it's doing a logistic regression. It's using lib. So linear, maybe linear learning model. They're sitting multiclass on that there. And so what's going to do is fit so fit is actually performing the training. And then what it's going to do is make a prediction on the test set. That it's going we're going to get accuracy so we're getting kind of a score. So notice that it's using accuracy As a valuation metric, I suppose, right. And then at the end, we're going to dump the data, a lot of times, like you have to save the model somewhere. So they're outputting, the actual weights of the neural network and all other stuff. It's a plk file. I don't know what that is. But if you're using like TensorFlow, you would use TensorFlow serving at the end of this, a lot of times frameworks, like pytorch, or TensorFlow, or MX net, they'll have a serving layer. But since we're just using scikit, learn, which is very simple, it's just going to dump out that file into our outputs, this is going to probably run a container. So this outputs isn't going to necessarily be on the outputs into here, it's more like the outputs of the container. And a lot of times the container will then place this somewhere. So like, it'll be saved on the container. But it'll be passed out to the register or, or something like that, like model registry. So anyway, we ran this, so that generated the file, we don't want to keep on running this multiple times, I probably just overwrite the file. So it's not a big deal. Here, it says notice how the script gets saved in the data model. So here, it's saying the data data folder, I guess we didn't look at that. So if we go top here, I didn't see this is data folder. wasn't really paying attention to where that was. Because it looks like where more so it's loading the data in. So here, it saves the data that put anything written to this directory is automatically uploaded to your workspace. So I guess that's just how it works. So it probably will end up in here then. So you tell py reference the training script to load the dataset correctly, and copy the file over. So we will run this to copy the file over. So I'm guessing did it put it into here? I'm just wondering, yeah, so it just put it in there. Because when it actually packages it for the container, it's going to bring that fall over because it's a dependency. So configure the training jobs. So create a script, run config, the directory that contains the script, the compute target, the training script, train, file, etc. Sometimes like in other frameworks, we'll just call them estimators. But here's just called a script run config. So I'm just trying to see what it's doing. So scikit learn is the dependency. Okay, sure. We'll just hit run. Okay. And then down below here, we have script run config. So it looks like we're passing our arguments that we're saying this is our data folder, which is apparently here, we're mounting it. And then we're setting regularization to 0.5. Sometimes you'll pass in dependencies in here as well, I guess these are technically our parameters that are getting configured up here at the top, right. But sometimes you'll have dependencies if you're in it, including other files here. And I guess that's up here, right? So see where it says environment. And then we're saying include the Azure ML defaults into psychic learn, and stuff like that. And so then it gets passed in the end. Does that make sense to me, we haven't ran that yet. Because we don't see any number here. Submit the job to the cluster. So let's go ahead and do that. says it returns a preparing a running state as soon as the job is completed. So it's in a starting state. Monitor remote run. So in total, the first run takes 10 minutes, but the second run is as long as the dependencies and Azure ML. Farming don't change the same images reuse and hence the Start Here Start Time is much faster. Here's what's happening while you wait. The image creation a Docker image is created matching the Python environment specified by the Azure ML environment. The image is built and stored in the ACR, the Azure Container Registry associated with your workspace. Let's go take a look and see if that's the case. Sometimes, like resources aren't visible to us, so I'm just curious, do we actually see it? Okay. And Yep, there it is. Okay, so that did not lie. So especially your workspace immigration uploading takes about five minutes the stage happens once. For each Python environment. Since the containers cache subsequent runs during image creation, a logs are stemmed to the run history, you can monitor the image creation process process using these logs wherever those are, if you if the remote cluster requires more nodes to execute the run than currently available, additional nodes are added automatically. Scaling takes typically takes about five minutes. And I've seen this before, where if you're in your compute here, and sometimes they'll just say like scaling because there's just not enough. So running in the stage, the necessary scripts and files are sent to the compute target than the data source or a mounted copy. The entry script is run Sentry script is actually the train.py file. While the job is running STD out in the files is in the logs directory or stem to the run history. You can monitor the runs progress using In these logs, the dot outputs directory of the run is copied over to the run history in your workspace. So you can access these results. You can check the progress of a running job in multiple ways. This tutorial uses the Jupiter widget. So looks like we can run this, watch the progress. So maybe we will run that. And so it's actually showing us the progress. That's kind of cool. I really like that. So it's just a little widget, join us all the things that it's doing. Let's go take a look and see what we can see under experiments and our run pipeline. He was talking about things like outputs and things like that. So over here in the outputs and logs, I'm just curious. Is it this is the same thing? I'm not sure if this is the this tails. Yeah, it does tail, it just moves so we can actually monitor it from here. I guess that's what it was talking about. so here we can see that it's setting up Docker, it's actually building a Docker image. And then I'm not sure did it send it to I mean, it's on ACR already. I think. It looks like it's still installing extracting packages. So maybe it's actually running on the image now. So just wait there, we pop back over here. You know, we can see probably the same information is identical. Yep, it is. So we're three minutes in, it's probably not that fun to watch it in real time and, and talk about it. So let's just wait until it's done. I'll see you back then. Okay. Alright, so I'm about 17 minutes in here. I'm not seeing any more movement here. So it could be that it is done. It does say if you run this next step here will wait for completion. Specify show output to true for verbose log. So here actually did output a moment ago. So maybe it actually was done. I just ran it twice. So I'm not sure if that's going to cause me issues there. So because I can't run the next step, unless I stopped this guy individually cancel this one here. I think I can just hit interrupt the colonel, there we go. Okay, so I think that it's done. Okay, because it's 18 minutes in. And I don't see any more logging in here. It's just not very clear. And also, the logs, we just have a lot of stuff going on here. Like, this is so much. So you know, if we were keeping keeping pace, we probably would have saw all these credit. Yeah, so another, we just had a few more outputs there. But I think that it's done. Okay. It's just there's nothing definitively saying like, done. Do you know I'm saying and then up here, it doesn't say, oh, oh, I guess it does say that. It's done. All right. So yeah, I just never ran it with the school. So I just don't know. So I guess it does definitively say that. I already ran this. So we don't need to run that. Again. I just feel like we'll get stuck there. So let's take a look at the metrics. So regularization rate is 0.5 accuracy is nine to nine is pretty good. The last step is train the script wrote in the output S. S, sk learn, I want to see if it's actually in our environment here. I don't think it is. So I'll put this somewhere. It's in our workspace somewhere, but it's just not. We just don't know where it's right here. Okay. So they'll put it the actual model right there. And so you can see the associated files that are ran, okay, we'll run it. register the work model and space that you can work with other collaborators. Sure. So if I click on that here, and we go back over to our models, it is now registered over here. Okay. So we're done part one. I don't want to do all these other parts. Training is enough as it is, but let's just take a look at the deploy stage. Okay, so for prerequisites. We're saying have a workspace we have our we are loading our registered model. Okay, we register it where you have to import packages, we are going to create scoring script, deploy to an ACI model, test the model, if you want to do this, you can go through all the steps, it does talk about confusion matrix, and that is something that can show up on the exam is actually talking about a confusion matrix. But we do cover that in lecture content. So you generally understand what that is. But, you know, I'm just I'm too tired. I don't want to run through all this. And there's not a whole lot of value other than reading, reading through it yourself here. So I think we're all done here. Okay. Okay, one service we forgot to check out was data labeling. So let's go over there and give that a go. So I'm going to go ahead and create ourselves a new project, I'd say my labeling project and we can say Whether we want to classify images or text, we have multi class multi label bounding box segmentation, let's go with multi class. I'll go back here for a second multiclass. Whoops. I don't know if we create dataset, but we could probably upload some local files. Let's say, my Star Trek dataset does let us choose the image file type here. Good. So these are images. Gonna tell us what here. It's very finicky this input here. file this it references a single or multiple files in your public data store or private public URL. Okay, so we go next. If we can upload files directly, that'd be nice. Ooh, upload a folder. I like that. So what we'll do is we do have some images in the free AI here, under Cognitive Services assets we have we'll go back here and we'll say I think objects would be the easiest. But we just want a folder right? So yeah, we'll just take objects. Yep, we'll upload the 17 files. Yep, we'll just let it stick to that path. That seems fine to me. We'll go ahead and create it. And so now we have a data set there, we'll go ahead and select that data set, we'll say next, your data says periodically check for new data points and data points will be added as tasks, it doesn't matter. We're only doing this for test. Enter the list of labels that we have TMG DS nine, Voyager tasks toss. That's the types of Star Trek episodes. Label which Star Trek series The images from, say next. I don't want enabled but you can have auto enabled assistant labeler. I'm gonna say No, we'll create the project. Okay, I'll just wait for that. Great. I'll see you back here in a moment. Okay. All right. So I'm back here actually didn't have to wait long. I think it instantly runs. I just assumed like I was waiting for a state that says completed. But it's not something we have to do. So we have zero to 17 progress, we're going to go in here, we're going to go label some data, we can view the instructions. It's not showing up here. But that's fine. If we go to tasks, we can start labeling. So what season is this from or series, this is Voyager, we'll hit submit. This is Voyager we'll hit submit. This is toss, we'll hit submit. This is TMG. This is TMG. This is DS nine, DS nine, Voyager. wager TMG. Ds nine, you get the idea though, you've got some options here like change the contrast, if someone can't see the photo, or rotate it, this is Voyager, Voyager TMG, DS, nine, Voyager, Voyager. And we're done. So we'll go back to our labeling job here, we'll see we have the breakdown there in our data set is labeled. We can export our data set CSV, cocoa, as your ml data set, I believe that means it will go back into the data sets over here. This will make our lives a little bit easier. Go back to data labeling. Okay. So you just grant people access to the studio, they'd be able to just go in here and jump into that job. Okay. If we go over to the data set, I believe we should have a labeled version of it now. So my labeling project. So I believe that is the labeled stuff here, right? Yep, so it's labeled. So there you go. We're all done Azure machine learning. And so all that's left is to do some cleanup. Okay, so we're all done with Azure Machine Learning if we want to and go to our compute, and just kill the services we have here. Now, if we go to the resource group and delete everything, it'll take all these things down anyway, but I'm just gonna go with a paranoid so I'm gonna just manually do this, okay. hit Delete. Okay, so we'll go back to portal dot Azure calm. And I'm going to go to my resource groups, and everything is contained. It should be all contained within my studio, just be sure to check these other ones for that. And we can see all the stuff that we spun up. We'll go ahead and hit Delete resource group. I don't know if it includes like, because I don't see like Container Registry, right? So I know like it puts stuff there. I guess it does. It's this Container Registry. So that's pretty much everything right? And I'll take down everything So, and if you're paranoid, all you can do is go to all resources and double check over here, because if there's anything running, it'll show up here, okay? But that's pretty much it. And so just delete and we're all done. Hey, this is Andrew Brown from exam Pro, and we're on to the AI 900 cheat sheet and this one is seven pages long, so let's get to it. At the top of our list, we're starting with artificial intelligence and machine that can perform jobs that mimic human behavior. Machine learning a machine that gets better at a task, explicit programming, deep learning and machine that has artificial neural nets. Inspired by the human brain to solve complex problems. a data scientist is a person with multidisciplinary skills in math statistics, predictive modeling, machine learning to make future predictions. data set is a logical grouping of units of data that are closely related or share the same data structure. Examples of this would be m&s and cocoa data labeling the process of identifying raw data, so images, text files, videos, and adding one or more meaningful and informative labels to provide context to a machine learning model can learn supervised learning data that has been labeled for training, unsupervised learning data that has not been labeled. An ml model needs to do its own labeling, reinforcement learning, so there is no data and there's an environment and an ml model generates data with many attempts to reach a goal. You have neural networks also abbreviate to nn, a network of nodes organized into layers of input hidden output that is used to train ml models. We have deep neural nets. So dnn, a neural net that has three or more hidden layers to their deep learning backpropagation moves backwards through neural net adjusting weights to improve outcome on the iteration. This is how a neural net learns loss function, a function that compares the ground truth to the prediction to determine the error rate, how bad the network performed, activation functions and algorithm applied to a hidden layer node at that affects connected output. So Arielle use a very common one, you have a dense layer, this is when the next layer increases the amount of nodes you have a sparse layer, this is one of the next layer decreases the amount of nodes, you have GPUs that especially designed to quickly render high resolution images and videos concurrently. Commonly used for non graphical tasks such as machine learning and scientific computing. You have CUDA which is a parallel computing platform and API by Nvidia that allows developers to use CUDA enabled GPUs for general purpose computing, also known as GPU GPU. On to the second sheet here for ml pipeline, we have pre processing, I didn't outline this in the course. So I'm going to just do that now. So preparing data and feature engineering before passing data to ml model for training inference, you might have data cleaning, so this is correcting errors within the data set that could negatively impact the results data reduction, reducing the amount of data or applying dimensionality reduction to reduce the dimensions of inputted vectors, feature engineering, transforming data into numerical vectors to be ingested by the ML model, sampling or resampling pouncing a data set to be uniform across labels by adding or removing records. post processing translate the output of an ml model back into human readable format, training. In the process of training the model serving the process of deploying the model to an endpoint to be used for inference inference invoking an ml model by sending requests expecting back a prediction, we have real time endpoints that optimize optimize for small or single item payloads. Returns results quickly usually uses a dedicated running server. batch transform optimized for larger batch predictions server runs only for the duration of the batch. There's forecasting make a prediction with relevant data analysts of trends edit stock guessing, predicting make a future prediction with without relevant data using statistics to predict future outcomes more of guessing using decision theory. For performance and evaluation metrics are used to evaluate different machine learning algorithms just to select a few here classification we have accuracy f1 score precision recall, for regression metrics we have MSE, our MSE, ma remember mean squared errors okay. Jupyter Notebooks a web based application for author and documents combined live code narrative texts equations of visualizations. classification is the process of finding a function to divide a label data set into classes and categories. A confusion matrix is a table to visualize the model predictions of predictive versus ground truth actual take the time to go look up how confusion matrix work, because they will absolutely ask you questions on the exam for the 900. Okay, regression is the process of finding a function to correlate a labeled data set into continuous variable numbers. clustering is the process of grouping unlabeled data based on similarity and differences. Okay, on to our third sheet here Cognitive Services an umbrella AI service that enables customers to access multiple AI services with an API key an endpoint we have the category of decision so anomaly detector identify potential problems early on content moderator detect potential offensive or unwarranted content personalizer create rich personalized experience for everyone. language understanding so build natural language understanding into the app spots in our devices q&a maker create a conversational Question and Answer layer over the data. Text Analytics. detect sentiment key phrases and add named entries. translator detect translate and more than 90 supported languages. For speech we have speech to text transcribe audible speech into readable text text to speech convert text to lifelike speeches for more natural interfaces speech translation integrate real time speech translation of your apps, speak recognition identify verify the people speaking based on the audio for vision we have computer vision so analyze content and images and videos custom vision customized image recognition to fit your business needs, face detect the detect and identify people and emotions and images. Knowledge mining is a discipline in AI that uses a combination of intelligence services to quickly learn from vast amounts of information. And there's three things to this there's ingest of content from a range of sources using connectors to the first and and third party data stores enrich the content with the AI capabilities that let you extract information find patterns, deep understanding explore the newly indexed data via search boxes, existing business applications and data visualizations onto our fourth sheet. We have Mark stuff AI principles, so this is responsible AI remember there's six. So fairness an AI system should treat all people fairly reliability and safety AI systems should perform reliability and safety, privacy and security AI systems should be secure and respect privacy. inclusiveness AI system should empower everyone engaged people, transparency, AI systems should be understandable accountability people should be accountable for the AI systems, common ml workloads. So for this, we have anomaly detection is the process of finding outliers with a data set called anomaly. Computer Vision is when we use ml neural nets to gain high level understanding of digital images and videos. And LP is the is the machine learning that can understand the contents of a corpus or body of text. conversational AI is technology that can participate in conversations with humans. I know it feels like we're repeating the same thing quite in different ways. But that's the way we're going to learn well here, okay. Azure Machine Learning service allows you to provision ml studio to build and maintain ml models and pipelines. We have authors so either that we have notebooks that Jupyter notebooks and ID to write Python code to build ml models. Remember, you can launch it in Jupiter labs and VS code as well probably once you have an example just so you know, auto ml completely automated process to build and train an ml model, designer visual drag and drop designer construct and build pipelines. We have data sets of data that you can upload, which will be used for training data can be versioned. Open datasets are publicly hosted datasets are commonly used for learning how to build ml models. experiments are logical grouping of runs, runs our ML tasks that perform on virtual machines or containers pipeline so ml workflows you have built or have used in the designer, you have a training pipeline. So pipelines to build in Train and ml model inference pipelines pipelines that are used to train that use a trained model to make a prediction on real data. Then you have models as a model registry containing trained models that can be deployed endpoints. When you deploy a model, it's hosted on an accessible endpoints the REST API. So real time endpoints invokes an ml model for inference pipeline endpoint, invokes that running on a pipeline. So for ci CD, under manage, we have compute the underlying computing instances used for notebooks, training inference, so we have compute instances, that all workstations that data scientists use to work with the data models. This is generally for your notebooks, computer clusters, scalable clusters for virtual machines on demand processing of experimental code, so training and pre processing, inference clusters, deployment targets for predictive services that use for train models. So for inference, attached compute links to existing Azure compute resources, such as virtual machines, Azure data, bricks, clusters, there's another one in there, but it's not gonna show up an exam probably a patchy Spark, but I guess it's covered under databricks. So for environments that reproduce pre reproducible Python environment for machine learning experts or experiments, data stores securely connect to your storage service on Azure without putting your authentication credentials in so it has Blob Storage file share data, data lake storage Gen two, as your SQL data as your Postgres MySQL database data labeling have humans and ml assisted labeling to label your data for supervised learning human and loop labeling, machine learning since the data data labeling, we have linked services so external services that you can connect to a workspace such as Azure synapse analytics, I think that's the only way you can connect right now, then for text analytics. So now we're out of the Azure Machine Learning Services, we're ingesting the cognitive services so text analytics, sentiment analysis, find out what people think of your brand or topic. Labels include negative, positive, mixed or neutral confidence scores ranging from zero to one opinion mining granular information about the opinions related to aspects granular data with a subject and opinion tied to a sentiment. key phrase extraction quickly identifies the main concepts in text. Language detection detects the language, input text is written in named entity recognition. ner detects words and phrases mentioned in unstructured text that can be associated with one or more sentiment types. We have Louis or Luis language understanding a no code ml service to build natural language into apps, bots and IoT devices. Use NLU the ability to transform a linguistic statement to a representative that enables you to understand your users naturally, Louis key schemas component so we have intentions to user what the user is asking for. So Louis app contains a nun intent entities, what parts of the entity intent is used to determine the answer utterances examples of the user input that includes intent and entities who trained the ML model to match predictions against the real user input for q&a maker generate a bot from a URL PDF, and it's supposed to be do cx for docs, that's a spelling mistake, then go the doc x file, multi turn conversation, so follow up prompts to narrow a specific answer to chat personalized canned responses for Azure bot service allow you to host bots. So you have the Bot Framework SDK, which is an end to end SDK to build, test, publish, connect, evaluate bots, that's the entire pipeline that they describe Bot Framework composer a desktop application to design bots, leverage the Bot Framework SDK, so there you go, that's the whole cheat sheet. Usually, I would break it up for service but there's a lot of intermixing. So that's why I did it this way. But, you know, good luck on your exam, and I hope you pass
