With timestamps:

00:00 - this course will help you master Lang
00:02 - chain a revolutionary AI first framework
00:05 - Lang chain enables developers to build
00:07 - context aware reasoning applications by
00:10 - linking large language models with
00:12 - external data sources for advanced
00:14 - natural language processing applications
00:17 - starting with the basics of text
00:19 - processing and vectorization and
00:21 - advancing to the nuanced aspects of
00:23 - laying chains expression language and
00:25 - retrieval techniques you'll gain
00:27 - hands-on experience in building real
00:29 - world AI applications Tom chant
00:32 - developed this course he is an
00:34 - experienced developer and course creator
00:36 - at scrimba hey there free code campers
00:39 - and welcome to this interactive course
00:42 - where you're going to learn how to use
00:43 - Lang chain JS to build a context aware
00:47 - chatbot that can answer questions on a
00:49 - specific document we provid it we're
00:53 - starting with the basics so you don't
00:55 - need any prior experience with Lang
00:57 - chain JS or even with AI the only
01:00 - prerequisites for this course are a
01:02 - knowledge of working with apis and
01:04 - vanilla
01:05 - JavaScript now Lang chain has a
01:08 - reputation for a steep learning curve
01:10 - but using its new expression language we
01:13 - make the journey much
01:15 - easier this is a Project based course
01:17 - with challenges and that means you're
01:19 - going to get your hands on the keyboard
01:22 - writing code throughout and if you're
01:25 - wondering how you can access the project
01:26 - code don't worry we've got you covered
01:28 - from the interact active version of this
01:30 - course on sca.com you can pause the
01:33 - video edit the code and run the projects
01:36 - right there in your browser or you can
01:39 - download the code from the scrims and
01:41 - run them locally if you prefer the link
01:44 - is in the description and one final
01:47 - thing if you enjoyed this course do hit
01:49 - the thumbs up right here on YouTube and
01:52 - if you'd like to get in touch reach out
01:53 - to me I'm on Twitter or X at TP chant
01:58 - okay let's Dive In
02:06 - hang chain to build aiiowed applications
02:09 - I am super excited to bring you this
02:11 - course on using one of the most
02:13 - mind-blowing Technologies in the
02:15 - emerging AI Universe we are going to
02:17 - study embeddings we'll be working with
02:20 - Vector stores we'll be building
02:22 - templates and creating prompts from
02:24 - those templates next we'll look at
02:26 - setting up chains and we'll do that
02:28 - using Lang chains expression language
02:31 - which is a more expressive and
02:32 - accessible way to write Lang chain we'll
02:34 - be looking at the pipe method to join
02:36 - elements of a chain together we'll be
02:38 - retrieving data from a vector store and
02:41 - then we'll use the runnable sequence
02:43 - class which is a really cool way to
02:44 - create complex chains in Lang chain and
02:47 - there's loads more plus challenges this
02:50 - is our project we are building a bot
02:52 - which is knowledgeable on information we
02:54 - give it so this bot is going to know all
02:57 - about our platform sca.com and we're
02:59 - going to feed it 3,000 words of
03:01 - information and then we'll be able to
03:02 - interrogate it on this information this
03:05 - is one of the most powerful uses of AI
03:07 - and it's a process that Lang chain makes
03:09 - easy now in this course we're going to
03:11 - focus on the Lang chain syntax the
03:13 - project is built in vanilla JavaScript
03:15 - and you're welcome to refactor it into
03:17 - any framework or Library you wish I'm
03:19 - assuming you have a solid knowledge of
03:21 - vanilla JavaScript but you don't need to
03:23 - be an expert if you've worked with apis
03:25 - and asynchronous JavaScript a little bit
03:27 - before you're going to be absolutely
03:29 - fine
03:30 - my name is Tom chant and I will be your
03:32 - tutor for this course you can find me on
03:34 - Twitter or X or whatever you want to
03:36 - call it I am @ TP chant and you can
03:39 - click this link and visit me there it's
03:41 - always good to hear how you got on and
03:43 - any feedback you might have now before
03:46 - you begin why not head over to our
03:48 - Discord server and meet the community
03:50 - and let everybody know that you're
03:51 - starting this course in the today I will
03:54 - Channel and again this slide is a link
03:56 - to that page on our Discord server
03:59 - before we we dive into the concepts and
04:01 - code let's get an intro to Lang chain
04:03 - from the founding software engineer of
04:05 - Lang chain Jacob
04:12 - Lee Yes L chain is a framework that
04:14 - helps developers build context aware
04:16 - reasoning
04:17 - applications Lang chain was born out of
04:19 - the realization that the process of
04:21 - developing sophisticated AI powered apps
04:24 - could be significantly streamlined by
04:25 - factoring out some common abstractions
04:27 - while much of current AI app development
04:29 - akes place in the realm of python
04:31 - there's a clear need for better tooling
04:32 - for the more webd focused JavaScript
04:34 - Community too hence L chain comes in two
04:36 - flavors and one of the aims of Lang
04:38 - chain JS is to make large language
04:40 - models and techniques around working
04:41 - with them more accessible to this
04:43 - broader audience in this course you'll
04:45 - be learning how to use Lang chjs to
04:46 - build your own apps it focuses in on
04:49 - conversational retrieval over document
04:51 - giving an llm access to the specific
04:53 - information in that document so it can
04:55 - answer questions and continue a logical
04:57 - contextualized conversation on topics
04:59 - Beyond its original training data along
05:01 - the way you will use models prompts and
05:03 - Alpha parsers some of the basic building
05:05 - blocks of lank chain we'll create chains
05:07 - of calls that enable us to connect up
05:09 - the various stages in the process need
05:11 - to get the desired output we'll be using
05:13 - L chain with superbase and the open AI
05:14 - API but one of the beauties of this
05:16 - framework is that components are easily
05:18 - swappable so you can work with a myriad
05:20 - of databases Vector stores and llms
05:23 - switch between them and find the one
05:25 - which works for you best good luck and I
05:27 - really hope you enjoy this course and
05:29 - enjoy using L chain to spin up really
05:31 - powerful AI web
05:40 - apps of how this app is going to work in
05:44 - this first section we need to work with
05:46 - our data so this chatbot is going to be
05:48 - knowledgeable about a specific Topic in
05:51 - this case the scrimber platform so we're
05:53 - going to start off with an information
05:55 - source which holds the knowledge we want
05:57 - the chatbot to have and we're going to
06:00 - pass this document to a splitter and the
06:03 - splitter is a lang chain tool which will
06:05 - split the document into chunks and then
06:07 - we're going to use an embeddings model
06:09 - from open AI to create vectors from each
06:12 - chunk and we're going to save those
06:14 - chunks to our Vector store which in this
06:16 - case is going to be a superbase vector
06:18 - store when we get to this point the
06:20 - vector store is established it's got all
06:22 - of the knowledge we want it to have and
06:24 - we won't need to repeat this process
06:26 - unless we want to give it more knowledge
06:28 - by adding some new data now if you don't
06:31 - understand the justification for each
06:33 - step or you're not really clear on
06:34 - vectors and embeddings yet don't worry
06:37 - we're going to go into a lot more detail
06:39 - when we actually write the code this is
06:41 - just an overview now once we've got the
06:44 - vector store set up we need to create
06:46 - the app to use it and the flow of that
06:48 - app is going to look like this we start
06:51 - off with a user which we've got
06:52 - represented right here and that user
06:55 - will input something probably a question
06:57 - they have about scrimber and we're going
07:00 - to do two things with that question
07:02 - we're going to save it to a conversation
07:03 - memory store which will hold the entire
07:06 - conversation and we'll use an open AI
07:08 - model to convert it to a standalone
07:11 - question and that just means we're going
07:13 - to reduce it to a very concise question
07:16 - with no unnecessary words we'll then
07:18 - take that Standalone question and we'll
07:20 - use an open AI embeddings model to
07:22 - create vectors from it we'll send those
07:25 - vectors to our superbase Vector store
07:27 - and we'll get back the chunk or chunk
07:29 - with the nearest match therefore the
07:31 - chunks that are most likely to contain
07:33 - the answer to our question the last
07:35 - stage is to use an open AI model to get
07:38 - the final answer and to give it the best
07:40 - chance of getting a good answer we're
07:42 - going to give it three pieces of
07:44 - information we're going to bring down
07:46 - the nearest matches from the vector
07:47 - store we'll give it the original user
07:50 - input and we'll also give it the
07:52 - conversation memory which will be the
07:55 - entire conversation that has taken place
07:56 - so far so that might be a very very long
07:59 - conversation or this might be the first
08:01 - question in the conversation now we're
08:03 - going to take the response that we get
08:04 - and we'll store it in the conversation
08:06 - memory ready to continue the
08:08 - conversation and also we'll give it back
08:10 - to the user by rendering it to the Dom
08:13 - now again this is a very high level
08:15 - overview and it might well cause some
08:16 - confusion for example what actually is a
08:19 - standalone question and why are we using
08:22 - it right here but then down here we're
08:24 - using the original user input and not
08:26 - the Standalone question but rather than
08:28 - get caught up that right now and give
08:30 - very theoretical answers what I want to
08:32 - do is keep referring to this diagram as
08:35 - we write the code and we're going to
08:37 - tackle those questions as they come up
08:39 - one by one so if this diagram looks
08:41 - horribly confusing right now don't worry
08:43 - at all we are going to take it step by
08:46 - step okay in the next Grim let's go
08:48 - right back to the beginning and think
08:50 - about getting some data into our Vector
08:52 - store but before we do that I'm going to
08:54 - bring in a scrim from my colleague Gil
08:56 - who's going to give you a detailed
08:58 - overview of embedded ings so sit back
09:00 - and take a few minutes to watch that
09:08 - next powered search shapes many parts of
09:11 - your daily lives every day you interact
09:13 - with platforms sifting through massive
09:16 - amounts of data from text and images to
09:18 - audio and video think about Amazon
09:20 - recommending products or search engines
09:22 - refining your queries social media
09:24 - platforms curate tailored content while
09:27 - services like YouTube Netflix and
09:29 - Spotify offer suggestions based on your
09:32 - preferences now Advanced AIS despite
09:34 - their capabilities don't truly
09:36 - understand the real world as we do they
09:38 - can't grasp the actual meaning or Nuance
09:40 - of a video title song or news article so
09:43 - how exactly do AIS and platforms like
09:45 - Spotify Netflix and YouTube truly get us
09:49 - how is it that they appear to understand
09:51 - predict and respond to us as effectively
09:53 - as if not better than people well the
09:56 - magic behind this capability involves a
09:58 - blend of algorithms AI models and huge
10:01 - amounts of data but a larger part of the
10:03 - answer involves embeddings you see when
10:06 - you present a question to an AI it first
10:08 - needs to translate it into a format it
10:10 - can understand so you can think of
10:12 - embeddings as the language that AI
10:15 - understands the term embedding is a
10:17 - mathematical concept that refers to
10:19 - placing one object into a different
10:22 - space think of it like taking a word or
10:24 - sentence which is in a Content space and
10:26 - transforming it into a different
10:28 - representation like a set of numbers in
10:30 - a vector space all while preserving its
10:32 - original meaning and the relationships
10:35 - between other words and phrases AI
10:37 - systems process lots of data from user
10:40 - inputs to information and databases at
10:42 - the heart of this processing are
10:44 - embeddings which are vectors
10:45 - representing that data transforming
10:47 - content like search queries photos songs
10:50 - or videos into vectors gives machines
10:53 - the power to effectively compare
10:56 - categorize and understand the content in
10:58 - a way that Almost Human so how is all of
11:01 - this possible well it isn't exactly as
11:03 - easy as just turning data into vectors
11:05 - so before we go any deeper let's take a
11:07 - closer look at what vectors are think of
11:09 - a vector as a coordinate or point in
11:11 - space and to keep things simple we'll
11:13 - have a look at this 2D graph with an X
11:15 - and Y AIS let's say that a word like cat
11:18 - is translated into a vector like 4.5
11:21 - 12.2 which is this point this Vector
11:25 - encapsulates the meaning and nuances of
11:27 - the word cat in a way an AI model can
11:30 - understand and then we have the word
11:31 - feline represented by a nearby Vector of
11:34 - 4.7 12.6 so we'll place that point on
11:37 - the graph now words that have similar
11:40 - meanings are numerically similar and
11:42 - tend to be closely positioned in the
11:43 - vector space so this closeness implies
11:46 - that cat and Feline have similar
11:47 - meanings now let's say we have the word
11:50 - or vectors for kitten which might also
11:52 - be close to cat and Feline but maybe
11:54 - slightly further apart due to its age
11:56 - related Nuance now a dog dog is
11:59 - different but still in the same general
12:02 - domain of domesticated animals so the
12:04 - word dog might be represented by a
12:06 - vector that's not too distant but
12:08 - clearly in a different region let's say
12:10 - 7.5 10.5 and even a phrase like Man's
12:14 - Best Friend which is a colloquial term
12:16 - for a dog could be represented by a
12:18 - vector that's close to the vector for
12:20 - dog on the other hand a word like
12:22 - building is not related in meaning to
12:24 - any of these so its Vector would be much
12:27 - further apart let's say 15 .3
12:30 - 3.9 here's another example that
12:32 - demonstrates how embeddings might
12:33 - capture semantic meaning and
12:35 - relationships between words let's say we
12:37 - have the word King represented by the
12:39 - vector 25 then man is the vector 13 and
12:45 - woman is represented by the vector 14
12:48 - now let's do some quick Vector
12:49 - arithmetic we'll start with a vector for
12:51 - King then subtract the vector for man to
12:53 - remove the male context and add the
12:55 - vector for woman to introduce new
12:57 - context after performing this Vector
12:59 - math our resulting Vector is
13:02 - 26 so we'll plot that point on the graph
13:05 - and let's say there's another word in
13:06 - our space queen represented by the
13:09 - vector 2 6.2 right here well this Vector
13:12 - is extremely close to the resulting
13:14 - Vector so we might identify queen as the
13:18 - most similar word based on that Vector
13:20 - just as a trained AI model would now a
13:23 - two-dimensional graph is a massive
13:25 - simplification as real world embeddings
13:28 - often exist in much higher dimensional
13:30 - spaces sometimes spanning hundreds or
13:32 - even thousands of dimensions for example
13:34 - the actual Vector embedding for the word
13:36 - Queen might have values across multiple
13:38 - Dimensions each Dimension or number in
13:40 - this Vector might capture a different
13:42 - semantic or contextual aspect of the
13:44 - word Queen for instance royalty
13:47 - Cleopatra or even chess this is what
13:49 - allows the AIS to recognize and
13:51 - differentiate between these contexts
13:53 - when the word is used in different
13:54 - scenarios now imagine embedding hundreds
13:57 - of thousands of words and phrases into
13:59 - this high-dimensional space some words
14:01 - will naturally gravitate closer to one
14:03 - another due to their similarities
14:05 - forming clusters While others are
14:07 - further apart or sparsely distributed in
14:10 - the space these relationships between
14:12 - vectors are extremely useful think back
14:14 - to spotify's method of embedding tracks
14:16 - in a vector space tracks that are
14:18 - positioned closely together are likely
14:19 - to be played one after the other all
14:22 - right so what else can we do with
14:23 - embeddings and how are they used in the
14:24 - real world well you can imagine how
14:26 - embeddings have revolutionized our daily
14:29 - experiences for example search engines
14:31 - have evolved to understand the essence
14:33 - of your queries and content moving
14:35 - beyond mere keyword matching and
14:37 - recommendation systems with the aid of
14:39 - embedding suggest products movies or
14:41 - songs that truly resonate with our
14:43 - preferences and purchase history for
14:45 - example Netflix uses them to create a
14:47 - tailored and personalized platform to
14:49 - maximize engagement and retention also
14:52 - in the healthcare industry embeddings
14:53 - are used to analyze medical images and
14:55 - extract information doctors can use to
14:58 - diagnose diseases and in the finance
15:00 - World embeddings help with analyzing
15:02 - financial data and making predictions
15:04 - about stock prices or currency exchange
15:06 - rates so every time you interact with an
15:08 - AI chatbot every time an app recommends
15:11 - something behind the scenes embeddings
15:13 - are at work translating data into
15:15 - meaning all right so how are these
15:17 - embeddings actually created well let's
15:19 - dive into that
15:25 - next that Lang chain integrates with and
15:28 - if you go to the Lang chain docs and
15:30 - check out modules retrieval Vector
15:32 - stores and Integrations you will see a
15:35 - whole long list of them and by the way
15:37 - this slide is a link through to that
15:39 - page in the docs now for this project
15:41 - we're going to use super base and
15:44 - superbase is a really popular and very
15:46 - userfriendly Vector store but one of the
15:49 - beauties of Lang chain is that it's
15:51 - actually really easy to swap out your
15:52 - vector store so if you want to
15:54 - experiment with various possible
15:56 - Integrations that won't be too much
15:58 - trouble at all now our first task is to
16:00 - set up a superbase account so so head
16:03 - over to superb.com and again this slide
16:06 - is a link to superb.com and let's select
16:09 - start your project and there you can
16:11 - sign up with your email or GitHub and
16:13 - once you've completed the sign up you'll
16:15 - end up at the dashboard so let's go to
16:18 - new project and we need to give the
16:20 - project a name I'm going to call this
16:21 - one scrim bot and we'll also need a
16:24 - database password and I'm just going to
16:26 - allow it to generate a random password
16:28 - password for me and lastly I just need
16:30 - to select my location I'm here in
16:33 - Western Europe not that far from London
16:34 - so that will do just fine then we can
16:37 - scroll down to the bottom and just click
16:39 - create new project now you'll wait a
16:42 - while while it initializes a couple of
16:44 - minutes at the most and eventually
16:46 - you're going to end up here and that
16:48 - shows us that we've got one database
16:50 - successfully set up now if you click on
16:52 - this tables icon on the left hand side
16:55 - here in the table editor tab is where
16:57 - you could manually set up a table but we
16:59 - don't need to do any of that because
17:01 - Lang chain is going to do it all for us
17:03 - if we go back to the Lang chain docs to
17:05 - the super base Integrations page and we
17:07 - just scroll down we get this chunk of
17:09 - code and we're going to run this in our
17:11 - superbase database and it's going to do
17:13 - everything that we need now I've just
17:15 - pasted this code into scrimber just so
17:18 - it's easier to look at and just to be
17:20 - clear we're not running SQL or SQL right
17:23 - here in scrimba we'll be running it in
17:25 - super base and I'll show you how in just
17:27 - a moment but I just wanted to put it
17:29 - right here so we can have a look at it
17:31 - in a bit more detail so what this code
17:33 - does is it enables a PG Vector extension
17:37 - in superbase it then creates our table
17:40 - with everything that we need in that
17:42 - table and it sets the embeddings to 1536
17:45 - and that's an important number because
17:47 - the open AI embeddings model use 1536
17:51 - Dimensions it also gives us this match
17:54 - documents function and it's this
17:56 - function which actually does the job of
17:57 - finding the the nearest match so later
18:00 - we'll be using this function to take the
18:02 - vectors from a question and find the
18:05 - nearest vectors from the text chunks
18:07 - because that will identify the text
18:09 - chunks which are most likely to contain
18:12 - the correct answer so all we need to do
18:15 - with this code is copy it go back to
18:17 - superbase and come into the SQL editor
18:20 - tab which is this second one down and
18:23 - paste it right in there now at the
18:26 - moment it's just called Untitled query
18:28 - which is not a great name so I'm just
18:30 - going to change mine to match documents
18:32 - because that's what this main function
18:33 - in it is called now it tells us click
18:36 - run to execute your query so we can do
18:38 - that right here and there we are it says
18:40 - success and if we go to the table editor
18:43 - and we click on documents we can see
18:46 - that we've got our empty table we've got
18:47 - the ID the content the metadata and the
18:50 - embedding I.E the vector and that is
18:53 - what we asked for right here where we
18:55 - said create table documents so that has
18:58 - worked just fine we've got this warning
19:00 - about allowing Anonymous access but
19:03 - don't worry about that right now this is
19:04 - going to be absolutely fine for
19:06 - prototyping now if you'd like to take a
19:08 - deeper dive into vectors and embeddings
19:10 - and exactly what superbase is doing here
19:12 - do check out this blog post that I've
19:14 - linked to right here it's got quite a
19:16 - lot of
19:18 - information okay so now our database is
19:21 - ready to go so in the next Grim let's
19:23 - start getting some vectors into the
19:24 - vector
19:27 - store
19:33 - next which is going to be the knowledge
19:34 - for our chatbot and split it into chunks
19:37 - and the idea is that each chunk will be
19:39 - big enough to hold a piece of
19:40 - information so what you want to avoid is
19:43 - having something like this one chunk
19:45 - where we say we update our course is and
19:48 - a second chunk saying on a regular basis
19:51 - that piece of information needs to be in
19:53 - one chunk else it's pretty much useless
19:56 - now a chunk will often not be that small
19:58 - in fact lots of our chunks are going to
20:00 - hold whole paragraphs so there'll be
20:02 - something much more like that and it
20:04 - doesn't matter at all if there's more
20:06 - than one piece of information in a chunk
20:07 - and in fact that's very likely going to
20:09 - happen all we're trying to achieve here
20:11 - is being able to give an AI Model A
20:14 - smallest chunk of text from which it can
20:16 - find the answer to a question what we
20:18 - could do is just upload a massive
20:20 - document with every request to say the
20:22 - open AI API but that would be very very
20:26 - expensive with tokens so what we're do
20:28 - here is much more economical much more
20:30 - performance and much more scalable okay
20:33 - that's the theory and this is the actual
20:35 - text that we're going to be using for
20:37 - our chatbot it's about 3,000 words long
20:40 - and I've checked and vetted all of the
20:42 - information myself that's really
20:44 - important if the information in the
20:46 - document at the start is faulty you're
20:48 - never going to make a good chatbot now
20:50 - I'm using a text file here for
20:52 - Simplicity but Lang chain has actually
20:54 - got several tools for working with
20:56 - different formats so you can click
20:59 - through to this section in the docs and
21:01 - it will show you options like passing
21:03 - PDF or extracting just the text from
21:05 - HTML to give you just two examples but
21:08 - here we're going to keep it simple and
21:10 - just use a text file so in index.js then
21:14 - I've got this TR catch we're fetching in
21:16 - scrimba
21:27 - dasinfozentrum
21:34 - knowledge source for our chatbot and now
21:36 - we need to split it and we're going to
21:38 - do that using a tool from Lang chain
21:41 - which will do most of the work for us so
21:44 - I've got the Lang chain dependency
21:45 - already installed and in scrimber we do
21:48 - that using a three dot menu that appears
21:50 - when you hover over dependencies you
21:52 - can't see it in the recording but when
21:54 - you click on that three dot menu and
21:56 - select ad dependency a dialog box
21:58 - appears and I can type in Lang chain and
22:00 - it does the rest for me outside of
22:02 - scrimba of course you can use npm
22:04 - install Lang chain okay now Lang chain
22:07 - offers us a couple of tools to split
22:09 - text there is the character text
22:10 - splitter and the recursive character
22:13 - text splitter I'm going to use the
22:14 - latter which is just a little bit more
22:16 - sophisticated but to be honest you can
22:18 - use either and I didn't see a big
22:20 - difference in performance between them
22:21 - in this app so let's import the tool
22:24 - from Lang chain and if you want the more
22:26 - basic character text splitter it's just
22:29 - going to look like that okay let's save
22:31 - a new instance of this recursive
22:33 - character text splitter to a const
22:36 - splitter and then we'll save our output
22:39 - we'll call splitter and use the create
22:41 - documents method now we need to pass
22:44 - create documents an array and inside the
22:47 - array we can list out what we want to be
22:49 - split we've only got one file text but
22:52 - if we wanted to upload to a vector store
22:54 - from multiple files we could list them
22:56 - out in here okay let's just log out the
22:59 - output and see what we get and I'll hit
23:02 - save and then in the console we can just
23:05 - see a promise and that's because this is
23:07 - an async process so we need to await it
23:11 - and there we are as soon as we do that
23:13 - we see our chunks of text and I'm just
23:15 - going to copy one of those chunks and
23:17 - bring it over to Output MD so we can
23:20 - have a look at it so it looks like this
23:22 - we've got page content and that has got
23:24 - the actual chunk of data now it's
23:26 - actually much much long longer than that
23:28 - the scrimba console truncates it heavily
23:31 - and then we've also got the metadata the
23:33 - metadata is quite interesting it gives
23:35 - us the location of that text Chunk lines
23:38 - 1 to 14 so what we're actually looking
23:42 - at if we go back to scrimba info. text
23:44 - is right from the beginning from 1 down
23:47 - to 14 so that is our text Chunk now we W
23:52 - be using this location data in our app
23:54 - but it is good to know for future
23:56 - reference that if you need to refer to
23:58 - where some information came from you
24:00 - have got that information right there
24:02 - provided to you by this text splitting
24:05 - tool now this text splitter that we've
24:07 - used the recursive character text
24:09 - splitter has made some assumptions about
24:11 - chunk size it actually defaults to a
24:14 - th000 characters does chunk size matter
24:17 - well yeah you bet it does larger chunks
24:19 - get more context smaller chunks get more
24:22 - granular semantic info if you go to
24:25 - either Extreme Performance could suffer
24:27 - and if chunks are really big sending
24:29 - that off to the AI models will get
24:31 - expensive now of course there's no best
24:33 - way of splitting and you do have to
24:35 - consider the text you're working with
24:37 - I've experimented a bit and I want to go
24:39 - for a chunk size of 500 not 1,000 so I'm
24:42 - going to come in here with an object and
24:46 - override the default
24:48 - settings so I'll set my chunk size to
24:51 - 500 again we'll hit save and what you
24:53 - can see is where before the first chunk
24:56 - went from lines 1 to 4 14 now we're
24:59 - going from lines 1 to 8 so the chunk is
25:01 - a little bit smaller and I'm just going
25:03 - to use Chrome Dev tools not scribus
25:05 - console to get the text from the first
25:08 - two
25:09 - chunks okay so there we are we've got
25:11 - the first text Chunk and the second text
25:14 - Chunk right here and what you'll notice
25:17 - is that these text chunks are actually
25:19 - split quite nicely into paragraphs this
25:22 - first one ends at a very natural point
25:24 - and this second one does as well and
25:26 - that is not an accident and you'll also
25:29 - notice we've got some overlap here this
25:31 - is a repetition of what we've got right
25:34 - here and again that is not accidental so
25:37 - if we just go back to this object and I
25:40 - can come in here and show you a couple
25:42 - more defaults firstly we have these
25:45 - separators and these separators are an
25:47 - array and they go in order so we've got
25:50 - the double new line to break a paragraph
25:52 - the single new line the space and then
25:55 - no space so what this recursive
25:58 - character text splitter does is actually
26:00 - quite complex it uses those separators
26:03 - to split text into chunks based on the
26:05 - size but prioritizing keeping paragraphs
26:09 - then sentences then words together
26:12 - intact so that's why in these text
26:14 - chunks we've got right here we've got
26:16 - more than one piece of information but
26:18 - that's absolutely fine because they're
26:20 - both contained inside a chunk and they
26:23 - both finish on a natural break and we've
26:25 - got some overlap now we can also
26:28 - override the overlap and it actually
26:30 - defaults to 200 which is quite a lot so
26:33 - I'm going to come in here and this is
26:34 - called chunk overlap and I'm going to
26:37 - set it to 50 so 10% of our chunk size
26:40 - which is a good rule of thumb to start
26:42 - with and you can always experiment now
26:44 - you won't always see overlap and that's
26:46 - because when the paragraphs fit neatly
26:48 - into the chunks and the overlap size is
26:50 - not big enough to include a whole
26:52 - sentence then the overlap won't be
26:54 - applied it will actually only be applied
26:57 - if the chunk size does not end with one
26:59 - of the first two separators so the
27:01 - double new line or the single new line
27:04 - so that is all a little bit confusing
27:06 - and I wouldn't recommend that you worry
27:08 - about it too much do play around with it
27:10 - try different strategies also try the
27:12 - more basic character text splitter and
27:14 - just see which one gets you the best
27:16 - performance with your chatbot and
27:18 - remember you can always come back and
27:20 - adjust it later if necessary now I
27:22 - should just add that if you've got some
27:24 - other separators in your document it's
27:26 - quite popular to use say the double
27:28 - hashtag you can also just add them to
27:31 - this array it does accept custom
27:34 - separators okay I haven't got any of
27:36 - them in our document so I'm going to
27:38 - delete that and the next thing to do
27:40 - then is to take these chunks of text and
27:43 - get them uploaded to the vector store so
27:46 - let's look at that in the next
27:52 - scrim to store and the first thing that
27:55 - I'm going to do is come in here and save
27:58 - our superbase URL and API key to consts
28:03 - and we can get all of the information
28:04 - that we need from the dashboard if you
28:06 - come down here to settings and select
28:08 - API here we have got the project URL and
28:12 - the API key and I've saved mine in my
28:15 - environment variables so I'm just going
28:17 - to come in here and set up the
28:26 - consts and of course you'll need to use
28:28 - whatever you called your URL and API key
28:32 - when you set up your M variable now I'm
28:34 - also going to bring in the open aai API
28:36 - key as we're going to need that in just
28:38 - a
28:41 - moment next I need to set up a superbase
28:44 - client and we do that using the create
28:46 - client method which we get from
28:49 - superbase so before we can use that I
28:51 - need to add the superbase dependency and
28:53 - there we are the superbase dependency
28:55 - has appeared and I'm just going to put
28:57 - the the name of the dependency in a
28:58 - comment just so you can see it a little
29:00 - bit more clearly okay now we've got that
29:03 - superbase dependency we need to import
29:05 - the create client method from
29:10 - it and now we can use create client to
29:12 - set up a
29:14 - client now let's use create client with
29:16 - our URL and API key to set up a
29:19 - superbase
29:21 - client in the final stage of this we do
29:24 - two things at once we're going to create
29:26 - our embedded and we're going to upload
29:28 - them to the vector store to do that we
29:31 - need two tools from nchain the superbase
29:33 - vector store and the open AI embeddings
29:35 - class so let me just import them
29:47 - quickly okay first we're going to use
29:49 - the superbase vector store class so down
29:52 - here I will say await because it does
29:56 - work asynchronously
29:58 - super base Vector store and we're going
29:59 - to use its from documents method and we
30:03 - need to give this method three pieces of
30:05 - information the first one is the output
30:08 - that we've got right here so remember
30:10 - the output is the chunks of text that
30:13 - we've
30:14 - split next we need to tell it how to
30:16 - create our embeddings and we're going to
30:18 - do that with the open AI embeddings that
30:21 - we've just imported so we'll set up a
30:23 - new instance of that class and we just
30:26 - need to pass it at open AI API key which
30:29 - we've got right here now we could pass
30:32 - that as a key value pair like this but
30:35 - as the key and the value are actually
30:37 - the same we don't need to do that we can
30:39 - use the shorthand version Okay the third
30:42 - piece of information is an object
30:43 - holding our superbase
30:45 - details so it needs our client and again
30:48 - we don't need to write out the key value
30:50 - pair we can just use the shorthand
30:52 - version it also needs a table name and
30:55 - if we go back to superbase we can just
30:57 - see that when we go to the tables tab
31:00 - our table name is just
31:03 - documents and just before we run into an
31:05 - error let's remember that this needs to
31:08 - be a string okay let's hit save and see
31:11 - if it works and actually we should have
31:14 - logged something out there with a
31:15 - success message but I totally forgot but
31:18 - now we can go back to our superbase
31:20 - dashboard and I'm just going to come up
31:22 - here to the tables Tab and let's go to
31:24 - documents and we can see it's worked we
31:27 - have got our data we've got the ID the
31:30 - content which is the text for each chunk
31:33 - the metadata and the actual embedding
31:36 - here are our vectors and remember this
31:38 - field is huge we have got 1536
31:42 - Dimensions here okay now we've got our
31:44 - vectors in our Vector store we can start
31:46 - work on the app that's going to allow us
31:48 - to query this Vector store so when
31:50 - you're ready for that let's
31:56 - go
32:00 - already have over on the right hand side
32:03 - you can see that we've still got our
32:04 - dependencies from Lang chain and
32:07 - superbase now this event listener at the
32:10 - top is listening out for user input so
32:12 - it's basically picking up clicks on this
32:15 - button when a user submits a question
32:17 - we've also still got our API key here
32:20 - that we're bringing in from our
32:21 - environment variable and that just
32:23 - leaves this function progress
32:25 - conversation socalled because it
32:27 - progresses the conversation onwards and
32:29 - also does the heavy lifting of updating
32:31 - the Dom so the user can actually see
32:34 - what response they've got back from the
32:36 - AI now it's all fairly straightforward
32:38 - JavaScript but do feel free to pause and
32:40 - take a look through and also do check
32:41 - out the HTML and CSS if you want to
32:44 - again it's fairly standard stuff okay so
32:47 - that's the code we're starting with so
32:49 - where do we go next with this project
32:52 - well let's head back to the diagram and
32:54 - see where we go first so we take the
32:56 - users input we've got the event listener
32:58 - listening out for that and then we do
33:00 - two things we've got the conversation
33:02 - memory right here and the Standalone
33:04 - question right here I want to focus in
33:06 - on the Standalone question first so
33:09 - let's start by unraveling its Mysteries
33:11 - and see what it actually is and why we
33:13 - want it so we'll come on to that in the
33:16 - next
33:25 - scrim question and just to be clear
33:27 - there are two things we need to do with
33:29 - the users's input they're creating the
33:31 - Standalone question and adding the
33:33 - user's input to the conversation memory
33:35 - now we're actually going to deal with
33:36 - memory at the very end so let's just
33:39 - ignore that for the time being and
33:41 - concentrate in on the Standalone
33:42 - question so what actually is a
33:45 - standalone question well a standalone
33:47 - question is just a question reduced to
33:49 - the minimum number of words needed to
33:51 - express the request for information okay
33:54 - but why do we want one well we can't
33:56 - control control what a user asks our
33:58 - chatbot and actually a user could easily
34:01 - ask a question in a way that's likely to
34:03 - get a vague or even an inaccurate
34:05 - response and we really want to avoid
34:07 - that so imagine you have an online
34:10 - clothes shop and a user asks something
34:12 - like this I'm thinking of buying one of
34:14 - your t-shirts but I need to know what
34:16 - your returns policy is as some T-shirts
34:18 - just don't fit me and I don't want to
34:20 - waste money it's a perfectly reasonable
34:22 - question but let's remember how chatbots
34:25 - work we're trying to find the nearest
34:27 - matching vectors and therefore the chunk
34:29 - of text that will likely hold the answer
34:31 - to our question we want to create an
34:33 - embedding from this question and find
34:35 - the nearest matching vector and
34:37 - therefore the chunk of text that will
34:39 - likely hold the answer to the question
34:41 - in our Vector database but there's so
34:43 - much going on with this question that
34:44 - the vector for it will be polluted
34:46 - remember a vector represents the
34:48 - semantic meaning not the precise words
34:51 - so what we want to do to maximize the
34:54 - accuracy of our chatbot is just extract
34:56 - the intended semantic meaning from this
34:58 - question what we want to focus in on is
35:01 - I need to know what your returns policy
35:03 - is and we can reduce that to a
35:05 - standalone question that will simply be
35:08 - this what is your returns policy that is
35:10 - what the user wants to know and so that
35:13 - is what we need to search for in our
35:15 - knowledge document so that will be our
35:17 - first task and to do that we need to
35:20 - take a look at how prompts work in Lang
35:22 - chain and also how we send them off to
35:24 - the llm the large language model so
35:27 - let's do that in the next
35:37 - scrim chain to set up a simple prompt
35:40 - and just for demonstration purposes
35:42 - we're going to move away from our main
35:43 - project and imagine we're building an
35:45 - app which generates a promotional tweet
35:47 - for a given product we've got the Lang
35:49 - chain dependency already installed right
35:51 - here and of course we're bringing in the
35:53 - open AI API key next we need to to
35:56 - import two things from Lang chain first
35:59 - the chat open aai class and we also need
36:02 - the prompt template class so let's set
36:04 - up our llm and save it to a const so
36:06 - we'll take the chat open AI class and
36:09 - pass in our API
36:11 - key now we could instead pass in nothing
36:15 - and if we do that Lang chain will check
36:17 - process. mv. openai API key and use it
36:20 - if it's available so we could actually
36:23 - delete this line of code as well but to
36:25 - be honest I think it's clear if we just
36:27 - add it manually so I'm going to do it
36:29 - that way and also if we wanted to
36:31 - override any open AI defaults we could
36:34 - do that right here in this object for
36:36 - example we could change the temperature
36:39 - setting so now this llm will use a
36:42 - temperature of 0.5 but to be honest I
36:45 - don't want that I'm just going to leave
36:46 - everything at default for now okay now
36:49 - let's create a template for our tweet
36:51 - and I'm going to save it in a const
36:53 - tweet
36:54 - template and I'm just going to say
36:56 - generate a promotional tweet for a
36:58 - product from this product
37:02 - description and we need to give it a
37:04 - product description so I'm just going to
37:06 - put the product description variable
37:08 - inside curly
37:10 - braces now as soon as you see the curly
37:12 - braces you might think that this is a
37:14 - standard JavaScript template literal
37:16 - it's actually not there's no dollar sign
37:18 - needed here this input variable will be
37:20 - picked up by Lang chain and of course we
37:22 - could have several input variables here
37:24 - if we wanted to if this prompt was going
37:26 - to be a little bit more complex we could
37:28 - also have for example price and anything
37:31 - else we needed but let's just keep this
37:33 - really simple okay now we need to turn
37:35 - this template into a prompt so I'm going
37:37 - to set up a const for the
37:39 - prompt and now we can use the prompt
37:42 - template class and that comes with the
37:44 - from template method and then we just
37:47 - need to pass in our template and before
37:49 - we go any further why don't we just log
37:51 - out the Tweet prompt and see what we've
37:54 - got I'll hit save and we we're getting
37:56 - an error let's just check the console
37:59 - and it says chat open AI cannot be
38:01 - invoked without new okay that's a pretty
38:03 - clear error we needed the new keyword
38:05 - right here let's try that one more time
38:07 - and there we are we've got our prompt
38:09 - and just so we can look at it a little
38:10 - bit more easily I'm just going to bring
38:12 - it into a markdown file and format it
38:15 - okay so we can see that we've got our
38:16 - prompt right here just as we wrote it
38:18 - and the interesting thing is that we've
38:20 - got this input variables property and
38:23 - that is holding product desk so that's
38:25 - telling the prompt which inputs to
38:28 - expect and this is an array so if we had
38:30 - lots of input variables they would just
38:32 - be listed out in this array and now that
38:34 - the prompt is expecting them it's going
38:36 - to throw an error if it doesn't get them
38:39 - another interesting thing to notice here
38:41 - that the template format is an F string
38:43 - and an FST string is something which
38:45 - comes from Python and it is the kind of
38:47 - python equivalent to a template literal
38:50 - okay so that is our prompt ready to go
38:53 - so in the next Grim let's take a look at
38:55 - how we can use it to generate some
39:05 - content template is time to set up our
39:08 - first chain in Lang chain I'll set up a
39:10 - const tweet chain to hold it and what
39:13 - this chain will consist of is our prompt
39:16 - chained to our llm and to join the two
39:18 - parts of the chain together we use the
39:20 - pipe
39:21 - method and I'm going to pass in the llm
39:25 - so the pipe method is joining the two
39:27 - aspects of the chain it takes the output
39:29 - from the first which will be the prompt
39:31 - and it passes it to the llm it's a
39:34 - really simple chain with just one
39:36 - connection again let's just log that out
39:38 - and see what we
39:39 - get and again I'm just going to paste
39:42 - that into our markdown
39:44 - file okay we've got quite a lot going on
39:46 - here but I want to draw your attention
39:48 - particularly to runnable sequence
39:50 - because that is a theme that we're going
39:52 - to be coming back to quite a bit in this
39:54 - course but just by way of an intro what
39:56 - we've got here is a runnable sequence
39:59 - and we can see that this object right
40:01 - here is called First and that is the
40:03 - first element in our chain we've got our
40:06 - prompt right here with our input
40:08 - variables and then we've also got last
40:11 - which is the last section of the chain
40:13 - and here you can see that we have got
40:15 - our open AI llm or large language model
40:19 - now if this were a more complex chain
40:22 - here in the middle we would see some
40:23 - more steps but for now it's just enough
40:25 - to know that this runnable sequence
40:28 - exists okay so if we go back to index.js
40:32 - what we need to do next then is invoke
40:34 - this chain to start the sequence in
40:36 - motion and we're going to do that with a
40:38 - method called invoke so let's set up a
40:41 - response and we need to await our tweet
40:44 - chain because this is an asynchronous
40:46 - process now let's call the invoke method
40:49 - on the
40:50 - chain and we'll just log out the
40:52 - response now if we hit save right now
40:54 - what do you think is going to happen
40:56 - well we're getting an error down in the
40:57 - console it says missing value for input
41:00 - product desk well that figures because
41:02 - look we're expecting an inut variable
41:05 - right here product desk and we're not
41:07 - actually introducing a product
41:08 - description to the chain so what we need
41:10 - to do then is pass in an object right
41:13 - where we invoke the chain so you can
41:16 - think of this as passing something in
41:18 - right to the beginning of the chain and
41:20 - what we need to pass in is a product
41:22 - description so product desk will be our
41:25 - key and then the value will be whichever
41:28 - product we want a tweet for okay let's
41:31 - save that again and down in the console
41:33 - we have got our tweet and in fact we can
41:36 - Zone in specifically on the Tweet by
41:38 - saying response.
41:39 - content and there we are we've got our
41:42 - tweet it's even got some emoji and some
41:44 - hashtags so this is working okay this
41:47 - would be a great time just to pause make
41:49 - sure you understand this code maybe
41:51 - generate a couple of tweets perhaps make
41:53 - this tweet template a little bit more
41:55 - complex with multiple input variables
41:57 - and then when you're ready we'll go back
41:58 - to our main project where we need a
42:00 - prompt and a model and that will give us
42:02 - a great opportunity to have our first
42:04 - challenge so when you're ready for that
42:07 - just move
42:14 - on prompt template and chat open AI
42:17 - already imported we've also got our open
42:20 - AI API key and the llm already set up so
42:25 - here is your challenge I want you to
42:27 - create a prompt to turn a user's
42:28 - question into a standalone question and
42:31 - I've just put a hint here the AI
42:33 - understands the concept of a standalone
42:35 - question you don't need to explain it
42:37 - just ask for it then create a chain with
42:41 - the prompt and the model and lastly
42:43 - invoke the chain remembering to pass in
42:45 - a question and for now we'll just log
42:48 - out the response to check it's working
42:50 - now just to be clear in this challenge
42:52 - we are not going to be wiring this up to
42:54 - the chat interface yet if we were going
42:56 - to do that we would have to actually
42:58 - come down here into this progress
43:00 - conversation function and update some of
43:02 - the logic in here it's a little bit of
43:04 - unnecessary complication at this point
43:06 - we will be coming on to that later also
43:09 - I've given you the four consts that
43:11 - you're actually going to need to
43:12 - complete this Challenge and we're just
43:14 - logging out the response right here I've
43:16 - also given you some hints above each
43:18 - const and I've done that because there
43:20 - is quite a lot of new syntax here and of
43:22 - course you are welcome to go back to
43:23 - that scrim to check the syntax if you
43:25 - need to but hopefully with these hints
43:28 - that might not be necessary now when you
43:31 - do this just remember that the question
43:32 - you pass in should be quite a long-
43:34 - winded question because the idea is to
43:37 - reduce that question down to just the
43:39 - bare minimum so you want to make that a
43:41 - long question with some unnecessary
43:42 - words it's only then that we'll be able
43:45 - to see that it works okay pause now take
43:47 - all the time you need and I'll see you
43:49 - back here in just a
43:54 - moment okay okay so hopefully that went
43:57 - just fine so we'll start off with the
44:00 - Standalone question template this is
44:02 - going to be a string and we're just
44:04 - going to ask for what we want so I'm
44:07 - going to say given a question convert it
44:09 - to a standalone
44:11 - question and then I'm going to say
44:13 - question colon and here I'll use the
44:16 - curly braces and we'll introduce the
44:18 - input variable which is question now
44:21 - I'll invite the model to provide us with
44:23 - the Standalone question by saying
44:26 - Standalone question and just finishing
44:28 - on a colon so that's like an invitation
44:31 - to complete right now we need the prompt
44:34 - so let's set Standalone question prompt
44:36 - equals to prompt template and we'll use
44:39 - the pr template method and pass in
44:42 - Standalone question template then we'll
44:44 - set up the chain so we'll take the
44:46 - prompt that we've just created and we're
44:48 - going to link up the chain with the pipe
44:50 - method and we're piping the llm and
44:54 - remember we've got the llm already set
44:56 - up right here okay to check it's working
44:59 - we need to invoke it and pass in a
45:01 - question so we'll take that chain and
45:03 - this is an async process so we'll await
45:06 - the Standalone question chain call the
45:09 - invoke method and we'll pass in an
45:12 - object we've got the input variable that
45:14 - the prompt is expecting right here it's
45:16 - question and for our question I'm going
45:18 - to go for something quite
45:21 - longwinded I've asked what are the
45:23 - technical requirements for running
45:24 - scrimba I only have a very old laptop
45:27 - which is not that powerful okay we're
45:29 - logging out the response so let's hit
45:31 - save and see what we get down in the
45:33 - console and there we are can a very old
45:36 - laptop meet the technical requirements
45:38 - for running scrimber it's reduced that
45:40 - question down to a standalone question
45:43 - it's just asking for the precise
45:44 - information and it's removed unnecessary
45:47 - words okay that is the first part done
45:50 - so now is a really good time to take a
45:52 - break and relax especially if you've
45:53 - been working at this for a while and
45:56 - when you're ready to go on we're going
45:57 - to look at how we can take our
45:58 - Standalone question to get chunks of
46:00 - matching text from the vector store but
46:03 - there is quite a lot to do that there
46:04 - are several steps in that process and
46:06 - some preparation so for now take a
46:08 - moment to chill and come back when
46:10 - you're
46:18 - ready and see where we are and we are
46:21 - right here so we've taken the users's
46:23 - input and we've created this Standalone
46:25 - question from it and the next step will
46:28 - be to create embeddings from that
46:29 - Standalone question and then take that
46:31 - to the vector store to find the closest
46:34 - match so if we have a standalone
46:36 - question like what is a scrim we'll use
46:38 - the embeddings model to create our
46:40 - vector and then in the vector store we
46:42 - will search through all of the vectors
46:44 - we'll find the closest match and we'll
46:46 - take that chunk of text now this diagram
46:48 - is a little bit of a simplification we
46:50 - might not just take one chunk of text we
46:52 - might take two three even 10 chunks of
46:55 - text and a bit later on we'll look at
46:57 - how we can exert control over how many
46:59 - chunks we retrieve from the vector store
47:02 - okay before we can do any of that we
47:03 - need some basic setup so I've bought in
47:06 - a few Imports here which we've seen
47:08 - before we've got the superbase vector
47:10 - store class I've also bought in the open
47:12 - AI embeddings from Lang chain and create
47:15 - client both of which we used before when
47:17 - uploading to the vector store so those
47:19 - are all of the Imports we need for now
47:21 - and then down here I've set up a new
47:23 - instance of embeddings I've passed in
47:25 - the open a API key do remember that if
47:27 - you don't pass anything in Lang Chain by
47:30 - default will look for process. mv.
47:32 - openai API key in this format and do it
47:35 - for you I'm going to pass it in here
47:37 - manually just for clarity okay we've
47:40 - also got our superbase API key and the
47:42 - superbase URL and we've used that to set
47:44 - up the client and again that is
47:46 - identical code to when we were uploading
47:48 - to the data store okay now things get a
47:50 - bit different to what we did before
47:52 - let's start a new instance of the
47:53 - superbase vector store in a const and
47:55 - and we're going to pass it our
47:56 - embeddings
47:58 - model and we'll also pass it a
48:01 - configuration object in this
48:02 - configuration object we only really need
48:04 - to pass it the client but again just for
48:07 - clarity I'm also going to pass it the
48:09 - table
48:10 - name and our table name in superbase was
48:13 - documents we can see that one right here
48:16 - in the superbase dashboard and I'm also
48:18 - going to give it the query name and
48:21 - again if we look at the superbase
48:22 - dashboard we've got this query match
48:24 - documents which we named after this
48:26 - function match documents so let's pass
48:29 - that in right here so these two are both
48:32 - defaults but I think it's a good idea to
48:34 - put them in here right now because if in
48:37 - the future you're working with a more
48:38 - advanced use case perhaps you've got
48:40 - more than one table or more than one
48:42 - query name it's important that you know
48:44 - that this is where you can configure
48:45 - your vector store okay now this is quite
48:48 - different to what we were doing when we
48:49 - were uploading to the vector store we
48:51 - actually want to retrieve something and
48:53 - without Lang chain we would have to
48:54 - actually write a ton of code to do that
48:56 - but now all we're going to do is create
48:58 - a new const and I'm going to call it
49:00 - Retriever and I'm going to set it equals
49:02 - to our Vector
49:03 - store and I'm going to call the as
49:06 - retriever method on it and that is all
49:09 - we need to do this as retriever method
49:11 - knows to go to the vector store and
49:13 - instead of inserting more data to use
49:15 - the matching documents function that we
49:17 - added here to find the nearest matches
49:20 - so that makes it nice and easy and if
49:22 - you'll remember this match documents
49:24 - query came straight from the Lang chain
49:26 - docks so all of that provided by Lang
49:29 - chain just makes life really easy okay
49:32 - this retriever is now finished and that
49:34 - means it can now be added as an element
49:35 - in a lang chain chain so let's do that
49:38 - next we're just going to come in here
49:40 - down to our chain which we've got right
49:43 - here and we're just going to pipe the
49:45 - retriever on the
49:46 - end so what should now happen is that we
49:49 - invoke the chain and we pass in this
49:52 - question at the top of the chain the
49:54 - Standalone question prompt creates its
49:56 - own prompt from that question it's piped
49:59 - to the llm to get the Standalone
50:01 - question and that should now be piped to
50:03 - the retriever to retrieve the nearest
50:05 - chunks from the vector store well we're
50:08 - logging out the response let's hit save
50:10 - and see what happens and if we open up
50:12 - the console we're getting an error and
50:14 - it's one of those kind of vague errors
50:16 - e. replace is not a function so I think
50:19 - what we should do next is isolate our
50:20 - Retriever and see if it's working so I'm
50:23 - actually going to delete this pipe that
50:25 - we put on the end of this chain and I'm
50:26 - going to set up a second response right
50:28 - here and this is just a test so I'm just
50:31 - going to call it response two and what
50:33 - we're going to await here is our
50:35 - retriever we'll call the invoke method
50:38 - and we'll just pass in a standalone
50:40 - question and I'm going to follow on from
50:43 - this question here and just say will
50:45 - scrimber work on an old
50:48 - laptop okay let's just log out response
50:50 - 2 and see what we get and look we are
50:53 - getting some chunks and we can see that
50:56 - we've actually got four chunks and the
50:58 - first one starts what are the technical
51:01 - requirements so that is looking good for
51:03 - an answer to this question so I'm
51:05 - thinking that our retriever is working
51:07 - just fine so why is it not working in
51:09 - this chain well I think if we log out
51:11 - the original response as well we might
51:13 - see an answer to that question so let's
51:15 - run this again and now we've got quite a
51:17 - lot of data in our console too much to
51:19 - see easily so I'm just going to paste it
51:21 - into a markdown file okay so what we can
51:24 - see is that where we were logging out
51:25 - the original response we have got this
51:29 - object right here and we've got the
51:31 - Standalone question what are the minimum
51:33 - technical requirements for running
51:35 - scrimba can I use it on an old less
51:37 - powerful laptop so that was the
51:39 - Standalone question here we've got our
51:41 - chunks and we've got four of them and
51:44 - that's great so what exactly is the
51:46 - problem if you want just pause for a
51:47 - moment and see if you can figure it
51:51 - out okay so maybe youve figured out that
51:54 - if we have a look at the chain what
51:56 - we're actually doing when we pass things
51:58 - along the chain is passing them along in
52:00 - their expected data type so we invoke
52:04 - this chain with this string and the
52:07 - Standalone question prompt is expecting
52:09 - a string and the llm is expecting the
52:12 - output of the Standalone question prompt
52:14 - it knows what format that's coming in
52:16 - but when we try to pipe the retriever on
52:18 - the end what we're doing is we're
52:21 - passing along this object but what we've
52:24 - just seen from our experiment
52:26 - is that the retriever works with a
52:28 - string so the only problem here is that
52:30 - we need to be passing along just the
52:32 - string from this response now we could
52:36 - do that with DOT or bracket notation but
52:38 - Lang chain actually gives us a better
52:40 - way of doing it and it introduces us to
52:41 - the concept of an output passer and that
52:44 - is something that you should know about
52:45 - so let's look at that
52:52 - next so Lang chain offers various output
52:56 - passes to suit some specific situations
52:58 - so for example you might need to Output
53:01 - in Json or you might even need binary
53:04 - data we're not doing anything so complex
53:06 - here but we can use the most simple
53:08 - output passer which is the string output
53:11 - passer and as the name suggests it's
53:13 - going to pass the output as a string so
53:16 - if we go back to our code I've put this
53:19 - chain back to how it was we're piping in
53:21 - the retriever at the end and we're just
53:23 - logging out the response now we already
53:25 - know that that doesn't work we're
53:27 - getting this error so now let's import
53:29 - the string output
53:32 - passer and all I need to do is add a new
53:35 - instance of the string output passer to
53:36 - the chain and it's going to come right
53:38 - in here so we're taking what we get back
53:41 - from the llm and that is what we want to
53:44 - pass as string so let's come in here and
53:46 - we're going to pipe in the string output
53:49 - passer and the string output passer is a
53:52 - class and in here I just want a new
53:54 - instance of it it and let's invoke that
53:57 - and when we hit save and I open up the
54:00 - console there we are it is working we
54:03 - have chained everything together and now
54:05 - we are getting the four chunks back from
54:07 - the vector store and just to see the
54:10 - string output pass in action let's
54:12 - quickly delete the retriever so I'll
54:14 - just delete this off the end now when I
54:16 - hit save the output from the llm is just
54:19 - a string before without the output
54:22 - passer we were getting an object okay
54:25 - let's just put all of that back so that
54:28 - is pretty cool we're definitely making
54:30 - progress however you might be looking at
54:32 - this and thinking we've got pipe and
54:34 - pipe and pipe how many pipes are there
54:36 - going to be in this chain and also this
54:39 - chain is called Standalone question
54:41 - chain but we've just added the retriever
54:43 - on the end and that is a little bit
54:45 - strange so why don't we just come in
54:47 - here right now and do a mini refactor
54:49 - and I'm just going to call this chain
54:51 - chain and of course we need to update
54:53 - that where we invoke it shortly we'll be
54:56 - breaking this chain up and doing a
54:57 - bigger refactor but first I want to show
55:00 - you the limits of what we can do with
55:02 - just the pipe method so next let's think
55:04 - about using what we've got down in the
55:06 - console to try to formulate an answer to
55:09 - our question now I think we going to run
55:11 - into not one but two problems so let's
55:14 - go and check that
55:22 - out feeling it was starting to get kind
55:24 - of messy and we've got another prompt to
55:26 - add so to tidy things up a bit I've
55:28 - taken all of the retriever code and
55:31 - given it its own file here inside this
55:33 - utils folder I'm just exporting it and
55:36 - then importing it back into index JS
55:39 - right here the retriever is pretty
55:41 - stable now we will do a tiny bit more
55:43 - with it later but it's good to tuck it
55:45 - out of the way and we'll do that as we
55:47 - go along just to keep things here in
55:49 - index.js manageable okay let's go back
55:52 - to the app so we've got this chain down
55:54 - here which we're invoking with this
55:56 - question and the question becomes a
55:59 - standalone question which we're using to
56:01 - get relevant chunks of data from the
56:03 - vector store so if we have a look at our
56:05 - flow diagram we've done all of this
56:07 - we're right here we're finding the
56:09 - nearest match from the vector store and
56:11 - what we want to do next is create a
56:14 - prompt to get us the answer now we're
56:17 - going to do that using three pieces of
56:19 - information we need the nearest match
56:21 - from the vector store we need the user
56:23 - input which we're getting right here
56:25 - that's not the Standalone question
56:27 - that's the original question that the
56:28 - user asked and we also need the
56:31 - conversation which will later be stored
56:34 - in memory now we're not doing that until
56:36 - the end so I'm going to leave the
56:38 - conversation crossed out for now now it
56:41 - might cause you a bit of confusion that
56:42 - we're creating a standalone question
56:44 - here but we're not actually using it to
56:46 - get the answer so just to explain the
56:49 - role of the Standalone question is to
56:51 - get the most relevant information from
56:53 - the vector store the the original
56:55 - question might well contain more data
56:58 - which the llm can actually use to
56:59 - generate a more relevant conversational
57:02 - answer but we wouldn't want to use that
57:04 - data to find matches in the vector store
57:07 - let me just show you what I mean with a
57:09 - little diagram so imagine a user comes
57:11 - to our chatbot and they say I'm a
57:13 - complete beginner and really nervous is
57:15 - scrimba for me well we reduce that to a
57:18 - standalone question is scrimba suitable
57:21 - for beginners now if we use that
57:24 - question to generate our final answer
57:26 - our final answer would look something
57:28 - like this yes scrimba is suitable for
57:31 - beginners and that's a perfectly correct
57:33 - reasonable answer and it will leave the
57:35 - user feeling I think fairly neutral now
57:39 - if we use this original question when we
57:41 - form the final answer we can actually
57:43 - use the extra information that we didn't
57:46 - use when we were going to the vector
57:47 - store so that means we'll be able to
57:50 - form a more conversational response yes
57:53 - scrim is perfect for you we're a
57:55 - friendly community so don't be nervous
57:57 - and that will actually give the end user
57:59 - a much more positive feeling from the
58:02 - chatbot okay so that's why we're using
58:04 - the original question in our answer
58:06 - template so now it's time for a
58:09 - challenge and here it is I want you to
58:11 - create a template and prompt to get an
58:13 - answer to the user's original question
58:16 - remember to include the original
58:17 - question and the text chunks we got back
58:19 - from the vector store as input variables
58:22 - and you can call those input variables
58:24 - question and context and I've just put
58:27 - this warning here that if you go ahead
58:29 - and add this to the chain this is where
58:31 - you're going to find the limitations of
58:33 - chaining pipe methods together so don't
58:36 - be surprised when you get an error you
58:38 - might already see where the error is
58:39 - coming from and in fact there's going to
58:41 - be more than one problem to solve here
58:43 - and it will take us several scrims so
58:46 - really just take this challenge as
58:47 - prompt template practice and I've put
58:50 - here a wish list of what we want this
58:52 - chatbot to be able to do so you can and
58:54 - think about that as you're writing your
58:57 - prompt okay pause now get this challenge
59:00 - sorted and we'll have a look together in
59:02 - just a
59:07 - minute okay so hopefully you managed to
59:09 - do that just fine I'm going to come in
59:12 - here then and create an answer
59:14 - template and this is the text I've
59:17 - written you are a helpful and
59:19 - enthusiastic support bot who can answer
59:21 - a given question about scrimber based on
59:23 - the context Prov divided try to find the
59:26 - answer in the context so I've put that
59:29 - in there just to emphasize that the
59:30 - context is the knowledge that we want
59:32 - the chatbot to use if you really don't
59:34 - know the answer say I'm sorry I don't
59:36 - know the answer to that and direct the
59:38 - questioner to email help at sca.com
59:41 - don't try to make up an answer and
59:43 - always speak as if you were chatting to
59:45 - a friend so so hopefully we'll get a
59:47 - nice friendly engaging response from the
59:50 - chatbot now we just need to add in the
59:52 - input variables so we've got context and
59:56 - remember that will be the chunks we got
59:57 - back from the vector store and we've got
60:00 - question and then I'll finish the prompt
60:02 - with an invitation to
60:04 - create okay now we need to use this
60:06 - template in a
60:09 - prompt and let's come down here and
60:11 - we'll add it to the chain so we can just
60:13 - pipe it right on the end and is this
60:16 - going to work well no I've already given
60:19 - you a spoiler this is going to cause us
60:21 - an error and the error we're getting is
60:24 - missing Val for input context so that's
60:27 - pretty understandable we're not passing
60:29 - in context in the format it's expecting
60:32 - so what we need to do at this point is
60:34 - rewind and start sorting out the data we
60:36 - get back from the retriever it's a
60:38 - little bit of a process but let's start
60:39 - that in the next
60:47 - scrim the output from the retriever is
60:50 - this array of objects with metadata and
60:53 - all we want is the text and we're not
60:55 - trying to get that text into any special
60:56 - format we just want a string so what we
60:59 - can do is create a function to extract
61:01 - the text join it into a string and we
61:04 - can just add that function to the chain
61:06 - so basically what we want this function
61:08 - to do is map over this array get the
61:11 - text from page content and join it into
61:14 - a string so I'm going to call this
61:16 - function combine
61:18 - [Music]
61:20 - documents and it can take in docs as a
61:22 - parameter and that will be what we get
61:24 - back from the Retriever and now we just
61:26 - need to return the docs having mapped
61:28 - over them and returned the page content
61:31 - as we can see down in the console it's
61:32 - the page content that we're interested
61:35 - in and we need to join them together so
61:38 - let's use the join method and we'll take
61:40 - a double new line As the
61:42 - separator let's see if that works so I'm
61:44 - just going to add this function onto the
61:46 - chain and now let's open up the console
61:49 - at the moment we've got the array of
61:51 - objects and when we hit save we get back
61:54 - one one big string so that is working
61:57 - that's really good let's try chaining on
61:59 - then the answer prompt that we've got
62:01 - right here and I'll hit save and we get
62:05 - an error we're still missing the value
62:07 - for the input context and I did warn you
62:10 - there are multiple problems here so now
62:13 - what we see are the limitations of this
62:15 - pipe method we can give the answer
62:17 - prompt a string but we don't have an
62:19 - easy way to give it the object it wants
62:21 - remember it wants both context and
62:23 - question but luckily we have got another
62:25 - way to make Chains It's an alternative
62:27 - syntax called the runnable sequence we
62:29 - did touch on it earlier and it gives us
62:31 - more options now it's quite a big topic
62:33 - we're going to take several scrims to go
62:35 - over it but before we dive into that it
62:37 - might be a good time to take a break if
62:39 - you've been doing this for a while but
62:41 - just before we finish in my effort to
62:43 - keep this code base a little bit tidy
62:45 - I'm just going to take this function
62:47 - right here and move it into the utils
62:50 - folder right when you're ready and
62:52 - rested let's move on
63:01 - I've got a feature from a language app
63:03 - which is going to take in a sentence and
63:05 - we've got one here I Don't Like Mondays
63:07 - and it's going to do three things now
63:09 - you'll notice I've marked some errors in
63:12 - this sentence we've got a lowercase
63:14 - first letter we're missing an apostrophe
63:17 - in don't we've got a D on the end of
63:19 - liked which makes no grammatical sense
63:22 - and we're missing the uppercase first
63:23 - letter Mondays now the first llm call is
63:27 - going to solve the punctuation errors so
63:29 - we'll get uppercase I apostrophe and
63:32 - uppercase M we've still got the
63:34 - grammatical error but that's dealt with
63:36 - by the second llm call now the sentence
63:39 - is correct and we'll use the third llm
63:41 - call to translate that sentence to a
63:43 - given language in this case French now
63:46 - if that sounds a little contrived to you
63:48 - well it probably is I was just looking
63:50 - for a not too complex way of
63:51 - demonstrating a few things with runnable
63:53 - sequence so so just bear with me now
63:56 - we've already got the first two prompts
63:58 - here so we've got the punctuation prompt
64:00 - and the grammar prompt and then we're
64:03 - invoking the chain but at the moment the
64:05 - chain doesn't
64:07 - exist so let's build it using a runnable
64:09 - sequence and we're going to start that
64:11 - off by importing the runnable
64:13 - sequence now let's come down here and
64:16 - set up the chain so we'll say runnable
64:19 - sequence which comes with a from method
64:22 - and we pass in an array
64:25 - and in this array we can start building
64:26 - the chain so we'll start with the
64:28 - punctuation prompt and let's just open
64:31 - up the console let's just hit save and
64:33 - see if that works and it actually fails
64:36 - and that's happening because we've only
64:37 - got one thing in the runnable sequence a
64:40 - runnable sequence needs at least two
64:42 - elements in the chain so let's add in
64:44 - the llm and I'll hit save and this time
64:48 - it looks like it's working we've got
64:50 - down in the console I Don't Like Mondays
64:53 - and as you can can see the punctuation
64:56 - has been corrected the grammar mistake
64:58 - liked with a d on the end is still there
65:01 - okay next let's add the string output
65:04 - passer and again I'll hit save and there
65:07 - we are we're getting our string so this
65:09 - is looking like a nice neat way to do
65:11 - chains let's just add in the grammar
65:13 - prompt we'll hit save oh and that one
65:16 - fails and again we've hit the problem
65:19 - that the template in grammar point is
65:22 - expecting an input variable punctuated
65:24 - sentence so let's just rewind now these
65:27 - three elements in the chain work because
65:30 - what they pass along is in the format
65:32 - expected by the next element in the
65:33 - chain now one thing that's really cool
65:35 - with runnable sequence is that you can
65:37 - put an arrow function in the chain and
65:39 - log out exactly what's happening at that
65:41 - point in the chain to see what's going
65:42 - on so let's try that now so preve result
65:46 - is a parameter in an arrow function and
65:49 - I can call it whatever I want but preve
65:51 - result seems like a good idea as we're
65:53 - seeing the result of the previous action
65:55 - so let's hit save and we're going to get
65:57 - an error because we've broken the chain
65:59 - but we will see what we're logging out
66:02 - we've got the sentence property holding
66:03 - I Don't Like Mondays and the language of
66:06 - French and that is just logging out the
66:08 - result of our invocation that we can see
66:11 - right here so what I want to do next is
66:15 - move this line of code down the chain
66:17 - let's hit save again and we see the
66:20 - punctuation prompt let's move it down
66:22 - one more step
66:24 - and now we see the result of the llm and
66:26 - move it down to the end of the chain and
66:29 - we see the result of the string output
66:31 - passer and this is of course where we
66:34 - hit a problem because the string output
66:36 - passer is giving us a string and the
66:38 - grammar prompt which is what comes next
66:40 - in the chain is hoping for an object
66:43 - which looks like
66:45 - this punctuated sentence which is the
66:48 - input variable and then a string with
66:50 - whatever we got back from the string
66:51 - output passer in here so we could
66:54 - actually do that we could come in here
66:56 - and build the object so we've got the
66:58 - key punctuated uncore sentence and now
67:01 - for the value we can just take pretty
67:03 - much what we've got here although of
67:05 - course we're going to delete the
67:08 - console.log so we just return the
67:10 - previous result and look at that it
67:13 - works we're not actually seeing the
67:14 - final result yet but that's just because
67:16 - we need to chain in the llm and the
67:18 - string output passer let's hit save
67:21 - again and there we are now we've got our
67:23 - Center it is completely correct there's
67:26 - no grammatical errors and no punctuation
67:28 - errors so this has worked but I don't
67:32 - think it's very elegant and I don't
67:33 - think it's very intuitive so in the next
67:36 - Grim let's refactor this and we're going
67:38 - to find a neater way of doing it but let
67:41 - me just warn you before we finish this
67:43 - trick with the arrow function in a
67:45 - runnable sequence is actually very very
67:48 - useful in some situations and we are
67:50 - going to be using it in a challenge just
67:52 - a little bit later so don't forget
68:01 - it working but it's not the most elegant
68:05 - so let's refactor now one option is to
68:08 - go for a runnable sequence in a runnable
68:11 - sequence so I'll come right in here
68:14 - delete this and create a new runnable
68:16 - sequence right here and we do that in
68:19 - exactly the same way with the from
68:21 - method and now I just need to list these
68:23 - three inside that runnable sequence and
68:26 - of course delete them from here and
68:29 - let's just tidy up the formatting and
68:31 - actually that line of code is so long
68:33 - it's never going to look too good let's
68:35 - just hit save and see what
68:37 - happens so it does work it works
68:40 - perfectly well but likewise I think this
68:43 - is rather awkward so what I want to do
68:46 - is actually extract this chain so I'll
68:49 - create a const punctuation chain and
68:52 - that is going to hold this runnable
68:55 - sequence and now we can have whatever
68:57 - this chain returns be the value of this
69:00 - key value pair let's just bring this
69:03 - back onto one line okay let's give it a
69:06 - try and that works and by the way this
69:09 - could be a runable sequence or it could
69:11 - be a pipe chain it actually just doesn't
69:14 - matter at all you might find for these
69:15 - chains which are just a prompt an llm
69:18 - and an output passer that using the pipe
69:21 - method is just as good it makes the code
69:23 - slightly sh water and it's a matter of
69:25 - personal taste as to which one is easier
69:27 - to read now I'm going to leave this as a
69:30 - runnable sequence and I've just realized
69:31 - I've got an unnecessary trailing comma
69:33 - right there now I think this is going to
69:35 - look neater still if we also extract the
69:38 - grammar
69:41 - chain and I'll just hit save and there
69:44 - we are that works just as well Okay so
69:47 - we've got the runnable sequence working
69:49 - but we need to remember the last part of
69:52 - our app which is a transl ation and
69:55 - where we invoke this chain we are
69:57 - passing in a language French and we're
70:00 - not actually using that as yet in any of
70:02 - our templates but when it comes to
70:04 - having the translation template we are
70:07 - going to need it so I'm thinking I'll
70:09 - just come in here right now and I will
70:11 - add in that translation template and
70:13 - there we are we've got a translation
70:15 - template and a prompt which comes from
70:17 - it so what we'll be needing to do next
70:20 - is potentially give it its own
70:22 - chain and of course we need to add the
70:25 - translation chain to the main chain now
70:28 - do you think this is going to work well
70:30 - of course it's not we've been here
70:32 - before we're missing the input variable
70:34 - for language and that figures because
70:37 - we're not actually doing anything with
70:39 - language we're invoking it here but we
70:41 - haven't bought it into this chain and we
70:43 - certainly haven't passed it down the
70:44 - chain now strange as it may seem taking
70:47 - this original input and passing it down
70:50 - the chain is not a trivial thing to do
70:53 - and Lang chain actually give us a
70:55 - special tool to do just that so let's
70:57 - check that out in the next
71:05 - scrim slightly strange app we've added
71:08 - the translation chain so we should be
71:10 - able to get this translation of our
71:13 - original sentence now it's not working
71:16 - and we know why it doesn't have access
71:19 - to the input variables it wants it wants
71:21 - the language and grammatically correct
71:24 - sentence input variables now the
71:27 - grammatically correct sentence input
71:29 - variable should be coming straight from
71:30 - the grammar chain so we should be able
71:32 - to break this one into an object and add
71:35 - that really easily but what about this
71:37 - language input variable we know it's
71:40 - coming in here where we invoke the chain
71:42 - but to pass it down through the chain we
71:45 - actually need to use a special tool and
71:48 - that special tool is called a runnable
71:51 - pass through and we're going to import
71:53 - it from from exactly the same place that
71:54 - we get the runnable sequence so we can
71:57 - just add it in right here okay so we use
72:00 - it like this let's come to the first
72:03 - element in the chain and we've got this
72:05 - object and I'm just going to bring it
72:06 - down onto two lines and now I'll add a
72:10 - new key value pair and the key is going
72:12 - to be original input and the value is
72:16 - going to be a new instance of runnable
72:18 - pass through now let's come in here to
72:21 - this second object and again I'm going
72:23 - to bring it onto multiple lines and here
72:26 - I'm going to add a property original
72:28 - input and for the value we're going to
72:31 - use an arrow function just to log out
72:33 - what the original input is and just
72:36 - before we save that I'm just going to
72:38 - comment out this line and now we won't
72:40 - get confused by what we see in the
72:42 - console and we're seeing out errors but
72:45 - interestingly we're also logging out the
72:47 - original input and we can see what
72:49 - original input is in that object it's
72:52 - the sentence I don't liked Mondays with
72:54 - no punctuation or grammar and the
72:56 - language French so it's actually this
72:59 - object here that we passed in when we're
73:01 - invoking the chain so what we can do now
73:04 - then is destructure original input and
73:08 - now we're destructuring we need to use
73:10 - parentheses and then we should be able
73:12 - to access the language just with
73:14 - notation let's it save and see if that
73:17 - works and it does work we're getting the
73:19 - language French so let's just delete the
73:22 - console.log and will just return the
73:24 - language and of course we need to update
73:26 - the key here to language because that is
73:29 - the input variable that we're expecting
73:32 - in this template and let's hit save and
73:34 - see if it's worked oh and I think I
73:36 - forgot to reinstate this line which
73:38 - means we'll be waiting a long time let's
73:40 - try that again and it does work we are
73:43 - logging out our French sentence so we've
73:46 - successfully removed all of the errors
73:49 - and translated it into French and the
73:51 - really important thing here is that
73:53 - we've managed to pass an original input
73:56 - right down through the chain so we can
73:58 - get access to language in the final
74:00 - element of the chain now just before we
74:03 - move on I want to say a quick word about
74:05 - formatting it might well be that if you
74:08 - do use a runnable sequence here you
74:10 - think it looks better if we bring down
74:12 - the elements in these arrays onto their
74:14 - own
74:17 - lines and for me I think that probably
74:19 - does look a bit
74:21 - neater but that is a matter of personal
74:24 - taste now have a good look at what we've
74:27 - done here because everything that we've
74:29 - done here is aping something that we
74:31 - need to do in our app so when we go back
74:34 - to the app I'm going to give you a big
74:37 - Challenge and when you've completed that
74:39 - challenge the app should be
74:40 - fundamentally working bar one or two
74:42 - Loose Ends so have a good look at this
74:44 - code make sure you understand everything
74:47 - and then when you're ready move
74:52 - on
74:57 - we have studied everything we need to
74:59 - know to get this app to some kind of
75:01 - minimum viable product what I mean by
75:03 - that is by the end of the challenge we
75:06 - should be able to see an answer to this
75:08 - question down in the console and after
75:10 - I've shown you my solution we're going
75:12 - to wire up all of the Dom elements and
75:14 - then we've just got a few final features
75:16 - to add now I've imported the runnable
75:19 - pass through and the runnable sequence
75:21 - for you so you don't need to do that and
75:23 - let's just quickly check out the flow
75:25 - diagram to remind us where we are so
75:28 - we've basically done all of this and now
75:30 - we're going to be getting the answer
75:31 - using the nearest match and the user
75:33 - input we haven't wired up the
75:36 - conversation yet we're doing that last
75:38 - so I'm going to leave that crossed out
75:40 - you don't need to worry about it so your
75:43 - challenge is this I want you to set up a
75:46 - runnable sequence so the Standalone
75:49 - question prompt passes the Standalone
75:51 - question to the Retriever and the
75:52 - retriever passes the combined docs as
75:55 - context to the answer prompt remember
75:58 - the answer prompt should also have
76:00 - access to the original question when you
76:02 - finish the challenge you should see a
76:04 - conversational answer to our question in
76:06 - the console now I think the neatest way
76:08 - to do this is to create three chains one
76:11 - for each section so there'll be a
76:13 - standalone question chain a retriever
76:14 - chain and an answer chain and then you
76:17 - can bring them together in a runnable
76:19 - sequence but that said you might choose
76:21 - to do it a bit differently now I'm going
76:23 - to give you a choice here if you're up
76:25 - for a challenge you can go ahead and get
76:26 - to work but if you'd like a bit more
76:29 - guidance I've got a file up here called
76:32 - hints. MD and in here I've listed out
76:34 - the steps that I would take to get this
76:36 - working so you can refer to that if you
76:38 - need to whether you look at that file or
76:40 - not definitely feel free to go back to
76:42 - the previous scrims to check out the
76:44 - syntax as you do this and take as long
76:46 - as you need this can be pretty confusing
76:48 - at first but it really pays dividends to
76:51 - figure this out yourself either with or
76:53 - without the extra help in this file
76:55 - rather than just watching my solution so
76:57 - really give this Your Best Shot pause
77:00 - now get to work on this and then move on
77:02 - to the next Grim when you're ready to
77:03 - see my way of doing it and it's probably
77:06 - worth opening the next Grim in a new tab
77:08 - so you can keep your solution to hand
77:10 - for comparison
77:18 - purposes and massive congratulations if
77:21 - you did a few tricky bits to remember
77:23 - there now I'll show you how I did it and
77:26 - remember my way isn't the one true way
77:29 - if you got it working your way might be
77:31 - just as good or even better than mine so
77:34 - I'm going to come in here and set up
77:36 - three
77:37 - chains the Standalone question chain the
77:40 - retriever chain and the answer chain and
77:44 - then this here we will reuse as the main
77:46 - chain where we bring everything together
77:48 - on this first chain I'm just going to
77:50 - use the pipe method and I'm going to
77:52 - chain together the Standalone question
77:54 - prompt the llm and the string output
77:57 - [Music]
77:58 - passer now you could do this here with a
78:01 - runnable sequence it's totally up to you
78:03 - I've gone for the pipe method on these
78:05 - basic chains because it just keeps the
78:06 - code a little bit shorter now we could
78:09 - also bring these pipes onto new lines
78:12 - and that might make things a little bit
78:14 - neater still okay I want to add this
78:16 - first chain to our main chain so I'm
78:19 - just going to delete all of the code
78:20 - that we've got here and then the main
78:22 - chain is going to be a runnable
78:25 - sequence and then we'll pass in the
78:27 - Standalone question chain now let's just
78:29 - think for a second this chain will give
78:31 - us two things it will give us the
78:33 - Standalone question and the original
78:35 - question that we'll need in the answer
78:37 - prompt so let's create an object and
78:40 - we'll have the Standalone question and
78:42 - the value there will be whatever's
78:44 - provided by the Standalone question
78:46 - chain and we can have another property
78:48 - which is the original input and that
78:51 - will be a new instance of the the
78:53 - runnable pass through okay and onto the
78:55 - retriever chain and here we faced a bit
78:57 - of a gotcha the retriever chain isn't
79:00 - expecting an input variable but it needs
79:02 - the string we get from the Standalone
79:04 - question prompt so let's get at that
79:07 - using an arrow function and that means
79:10 - that this chain will need to be a
79:12 - runnable
79:14 - sequence so the first thing in this
79:16 - chain will be the arrow function that
79:18 - gets us the Standalone
79:20 - question so we'll take preve result as
79:22 - the parameter and we'll return pre
79:25 - result. Standalone
79:28 - question next in the chain we need the
79:30 - retriever itself and finally we want to
79:33 - extract the text from the array of
79:35 - objects we got back from the Retriever
79:37 - and we do that with the combined
79:39 - documents function so in this chain
79:42 - we're taking the previous result and
79:44 - we're extracting the Standalone question
79:46 - from it that's going to give us the
79:48 - string which we pass to the retriever
79:50 - the retriever finds the nearest matches
79:53 - and brings them back to us as an array
79:55 - of objects and the combined documents
79:57 - function extracts just the text from
79:59 - that array of objects and joins it into
80:02 - a string okay let's add the retriever
80:04 - chain to our main
80:06 - chain and what the retriever chain needs
80:09 - to pass on is the context because that
80:11 - is what the answer template is expecting
80:14 - as well as the original question so for
80:17 - the context that's coming here with the
80:20 - key as context the value is the
80:22 - retriever chain and then for the
80:25 - original question here we're going to
80:27 - take the original input that we've
80:29 - passed through with this runnable pass
80:31 - through and extract the question from
80:35 - it and now we just need to deal with the
80:38 - answer chain and the answer chain is
80:40 - going to be much like the Standalone
80:42 - question chain so we'll set that equals
80:44 - to the answer prompt and we'll pipe it
80:47 - into the
80:48 - llm and likewise we need a new instance
80:51 - of the string output passer
80:54 - okay that is the last link in the chain
80:56 - so let's just add it on here in the main
80:58 - chain and now I'm going to hit save and
81:01 - let's see if we get an answer and look
81:04 - we do it says scrimba is designed to be
81:07 - lightweight and can be used on low speec
81:09 - PCS so your old laptop should work just
81:12 - fine so what we're getting there is an
81:15 - answer to this question and that is
81:17 - really really good that is exactly what
81:19 - we want and the answer we're getting is
81:21 - accurate it's from information that we
81:23 - gave it in our original document and has
81:26 - been turned into a conversational answer
81:28 - by the llm so again congratulations if
81:31 - you got it to this point now the end is
81:34 - almost in sight in the next scrim I'm
81:35 - going to wire up the UI that's pure
81:38 - JavaScript there's no AI involved in
81:40 - that if you fancy it as a challenge go
81:42 - ahead and do it most of the code is done
81:44 - for you if not just feel free to sit
81:46 - back and watch on this
81:51 - one
81:56 - haven't really touched we've got this
81:59 - event listener which is listening out
82:00 - for a submit event which will fire when
82:03 - this button is clicked and that's going
82:06 - to call this progress conversation
82:08 - function now if we come right down to
82:10 - the bottom we can check out that
82:12 - function and all we need to do to get
82:14 - this working is to take this chain
82:16 - invocation right here cut it and paste
82:19 - it in here now I'm going to delete
82:22 - delete the console.log we don't need
82:24 - that and also we want to get rid of this
82:27 - hardwired question and we're going to
82:29 - invoke with whatever the user has
82:31 - inputed and we've got that stored in
82:34 - this Con right here question which is
82:36 - coming from user input. value and that
82:38 - is just being collected from this input
82:40 - on submit and that is all we need to do
82:43 - let's hit save and now I'm going to ask
82:46 - it a question and I'm going to ask it
82:48 - how long it will take to get a code
82:50 - review for a solo project oh and we're
82:53 - getting an error and it's saying result
82:55 - is not defined and I think what I've
82:57 - done here is I've called this one
82:59 - response and this one down here result
83:02 - well it doesn't really matter how we
83:04 - changed this let's change this one to
83:06 - response and I'll ask it the same
83:09 - question and there we are we get a great
83:12 - answer and that answer has come right
83:14 - from the document that we gave it at the
83:16 - beginning so that is really really good
83:19 - now I'm going to ask it if it knows
83:20 - about me Tom Chan is a scriber teacher
83:23 - that's reassuring it knows that now
83:26 - let's start again and I want to do an
83:28 - experiment I'm going to introduce myself
83:31 - and of course it doesn't know I'm a
83:32 - scrimer teacher we've just refreshed it
83:33 - doesn't know anything about me and in
83:35 - fact I might just make up a
83:37 - name so I've introduced myself and asked
83:40 - what is scrimber and there we are we get
83:43 - a great answer it gives us a lot of
83:45 - detail greets me by my name so it's
83:47 - being really conversational now I want
83:49 - to ask it what is my name
83:53 - and of course it doesn't know and the
83:54 - good thing is it's advising us to email
83:56 - help sca.com so it's got that straight
83:59 - from our prompt I think that was right
84:01 - there in the answer prompt here we are
84:05 - email help.com if you don't know the
84:07 - answer so that's working really well now
84:10 - the fact that it doesn't know my name
84:11 - when I gave it my name right here and
84:13 - it's repeated it once tells us what we
84:16 - already knew which is that this chatbot
84:19 - has no memory so the last thing that we
84:22 - need to do then is add some memory to
84:24 - this app and we've got all of this left
84:27 - on our flow diagram plenty of arrows
84:29 - going on so it looks really complicated
84:31 - but actually there's not much more to do
84:33 - but we'll come on to that in the next
84:42 - scrim some memory now there's two parts
84:45 - to this there's some JavaScript to set
84:47 - up and then we'll need to wire the
84:49 - conversation memory into the chain so
84:51 - that the answer prompt has got access to
84:54 - it now I'm going to cover the JavaScript
84:56 - setup in this scrim and you're going to
84:58 - have the job of wiring it into the chain
85:00 - in a challenge in the next scrim so
85:02 - firstly I want to create a const to hold
85:04 - the memory and this memory is going to
85:07 - be an array and I'm just going to call
85:08 - it conversation history so we'll come
85:11 - outside of the progress conversation
85:13 - function and this is where I'll set it
85:16 - up and I think I'll abbreviate it to com
85:19 - history now let's add to the
85:21 - conversation history every time a user
85:23 - submits a question and every time we get
85:25 - a response back from the chain so we'll
85:28 - come down here and we can say conf. push
85:31 - and we'll push the question and the
85:35 - response okay that deals with a couple
85:37 - of arrows on this flow diagram so the
85:40 - user input is now saved to the
85:42 - conversation history and the answer we
85:44 - get back from the chain is also added to
85:46 - the conversation history but at the
85:48 - moment all we're going to have stored in
85:50 - this array is a bunch of strings which
85:53 - is fine but it's going to help the AI
85:56 - understand this better if it's clear
85:58 - which strings come from humans and which
86:00 - come from AI so I'm going to write a
86:03 - function just to add a human or AI label
86:06 - to each string and then we're going to
86:08 - join them together into one big string
86:10 - which we can use in our chain now again
86:13 - in the interest of keeping index.js as
86:16 - clean and clear as we possibly can I'm
86:18 - going to create this function in utils
86:21 - so we'll say function format com history
86:24 - and it will take in an array of messages
86:27 - and then it's going to map over those
86:29 - messages now the Callback will have each
86:32 - message as a parameter and also the
86:34 - index and we're just going to work out
86:36 - whether it's a human or AI message by
86:38 - doing some basic maths on the
86:44 - index so what we're assuming here is
86:47 - that the human speaks first so the
86:49 - zeroth message will be a human and all
86:51 - of the odd number numbers will be Ai and
86:53 - all of the even numbers will be human
86:56 - now we just want to join them together
86:57 - into one string and let's just join them
87:00 - on a new line character now when we call
87:03 - the function on the conversation history
87:05 - what it would do is it will take the
87:07 - array of strings and convert it to this
87:10 - so there's one long string but it's
87:12 - really clear what the human said and
87:15 - what the AI said okay let's delete this
87:18 - and we'll export this function and
87:21 - import it back back into
87:25 - index.js right so the setup is done and
87:27 - in the next Grim you can wire this into
87:30 - the
87:36 - chain let's wire up this memory now at
87:40 - the moment on this diagram we need the
87:42 - conversation history right here where we
87:44 - get the final answer to our question but
87:47 - I think it might help performance if we
87:49 - have access to the conversation history
87:51 - where we're creating ating the
87:52 - Standalone question it might just help
87:54 - it create a better Standalone question
87:56 - in some Edge situations so I'm going to
87:59 - add another arrow to this diagram now we
88:02 - want our conversation history from our
88:04 - memory store in two places the
88:06 - Standalone question and the answer chain
88:09 - at the very end and that means it's time
88:11 - for a super challenge or a mediumsized
88:14 - super challenge it's not quite as big as
88:16 - the previous super challenge but there
88:18 - is quite a lot to do so I want you to
88:21 - pass history into the chain as comore
88:25 - history at the point where we invoke it
88:28 - and I've just put remember to make use
88:29 - of our format com history
88:32 - function now if we just go back to where
88:35 - we invoke the chain this is where you
88:37 - can pass in comcore history and I've
88:41 - just put the underscore in just to keep
88:43 - up with the same format that we've used
88:45 - throughout once you've passed that in
88:47 - and you've used the format com history
88:50 - function The Next Step will be to come
88:52 - to the Standalone question template and
88:54 - make use of con history in that template
88:57 - and all I mean by that is come here into
89:00 - the template we're going to add con
89:03 - history as an input variable somewhere
89:05 - and just update the instruction so it
89:07 - knows it can use the conversation
89:09 - history as a resource as well when
89:12 - you've done that we need to come to the
89:14 - answer chain and we need to make sure it
89:16 - has access to com history so again down
89:20 - in the chain you're going to need to do
89:22 - something around here once you've done
89:25 - that and the answer chain has got access
89:27 - to com history again you need to come to
89:30 - the answer template and use an input
89:33 - variable to add com history somewhere in
89:36 - here and instruct it on how to use it
89:39 - now if I were you I would just use
89:40 - wording in here that makes it clear that
89:43 - it can use the conversation history to
89:45 - help it find the best answer but that
89:47 - the source of knowledge Remains the
89:49 - context so I'll leave it up to you to
89:51 - think of some work wording there and
89:52 - then you can see what I did and of
89:54 - course in the real world you'd have to
89:56 - play around with this prompt template
89:57 - and just do some experimentation until
89:59 - you get results that you're happy with
90:02 - okay once you've done that we should be
90:03 - done so you can test it by giving the
90:06 - chatbot some information and checking in
90:08 - the next question to see if it remembers
90:10 - it so a really easy way of doing that is
90:12 - exactly what I did before tell it your
90:14 - name and see if it remembers your name
90:17 - okay quite a lot to do there so pause
90:19 - now give this your best shot and we'll
90:21 - have a look at it together together in
90:22 - just a
90:28 - moment okay hopefully you managed to do
90:31 - that just fine so let's come down to
90:34 - where we invoke the chain and I've
90:37 - already added the comore history
90:39 - property and the value there will be con
90:42 - history however before we send that off
90:45 - we want to format it so let's take our
90:47 - function format com history and we'll
90:50 - just pass in com history
90:52 - okay let's go back to the challenge so
90:54 - we've got com history in the chain and
90:57 - we've formatted it let's go to the
90:59 - Standalone question template to make use
91:01 - of com history right there and I'm going
91:03 - to convert this to back ticks and I'm
91:06 - just doing that because like with the
91:08 - answer template we're going to end up
91:10 - with a list of input variables so it's
91:13 - just a bit neater when it's back ticks
91:15 - and we can spread it out onto multiple
91:16 - lines now I'm going to update the text
91:19 - right here so I've just said given some
91:22 - conversation history if any and a
91:24 - question convert the question to a
91:26 - standalone question and then here we've
91:29 - got our requests for input and then here
91:32 - I'm going to bring things down onto a
91:34 - new line and we'll add the conversation
91:37 - history input
91:38 - variable so now we need to make sure the
91:41 - answer chain has access to com history
91:44 - and we need to edit the answer template
91:46 - to make use of it well that's actually
91:48 - two things in one let's come down to the
91:50 - chain and then right here we're using
91:52 - this runnable pass through to get access
91:55 - to the original question now we can do
91:58 - exactly the same thing with com history
92:01 - so let's copy that down onto a new line
92:04 - change this to com history and change
92:07 - question to comore
92:10 - history okay now let's come up to the
92:12 - answer template and I'm just going to
92:15 - make some changes here so I'm going to
92:17 - say you can answer a given question
92:19 - about scrimber based on the context
92:21 - provided did and the conversation
92:23 - history I'm going to leave this sentence
92:25 - intact try to find the answer in the
92:27 - context but I'm going to add in here if
92:30 - the answer is not given in the context
92:32 - find the answer in the conversation
92:33 - history if possible now I think that's
92:36 - going to work but again in the real
92:38 - world we would have to do some testing
92:39 - here try a few different ways of wording
92:42 - this and just see what got us the
92:44 - results that we wanted now let's come in
92:46 - here and we'll just add the com history
92:50 - okay it looks like we're done so so
92:51 - let's come back up to the challenge and
92:53 - it's just telling us to test it so let's
92:55 - hit save and I'll say my name's Tom what
92:58 - is
92:59 - scrimber and it's given us an answer to
93:01 - that question and now I'll ask it what
93:04 - my name is and it says your name is Tom
93:07 - so the memory is working and we have
93:10 - successfully created a chatbot
93:12 - knowledgeable about our document and
93:14 - with a memory and that is a pretty
93:16 - awesome thing to do when you remember
93:18 - how Unthinkable this would have been
93:20 - even just a couple of years years ago
93:22 - let's just refresh and ask it one more
93:24 - question and I'm going to ask it what is
93:26 - the scrimba community
93:28 - like and it says the scrimba community
93:30 - is a global community of friendly and
93:32 - helpful coders we believe in learning to
93:35 - code as a community activity so we've
93:37 - set up a Discord server where you can
93:39 - meet fellow coders share your problems
93:41 - and solutions and network and that is
93:43 - actually very true and you can come and
93:46 - hang out with us there so this project
93:48 - is done congratulations on making it
93:50 - through to the end and let's just take
93:52 - one more scrim to recap what we've
94:01 - learned well every question I've asked
94:03 - it I'm happy with the results but that
94:06 - won't always be the case so what can you
94:08 - do when performance is not up to
94:10 - standard well there are a few tweaks you
94:12 - can make firstly you could go right back
94:15 - to where we set up the original document
94:18 - and you could alter the chunk size we
94:20 - went for a chunk size of 500 characters
94:23 - you could go larger if you think your
94:24 - chatbot needs more context and you could
94:26 - go smaller if you think it needs more
94:28 - granularity you could also change the
94:30 - overlap size we went for an overlap of
94:32 - about 10% next you could look at the
94:35 - number of chunks which are actually
94:37 - retrieved and I did mention earlier that
94:39 - we would say something about that so
94:41 - let's quickly come over here to the
94:43 - utils folder and find our retriever now
94:47 - right here we've got this Vector store
94:49 - as Retriever and we we can pass a number
94:52 - in here and that will control how many
94:55 - chunks we get back from the vector store
94:57 - let's just see what's happening at the
94:59 - moment if we head down to our chain and
95:01 - in fact we want the retriever chain here
95:03 - we are so let's just come in here and
95:05 - I'm going to add in an arrow function
95:07 - and log out what this retriever is
95:09 - giving us now this is going to break the
95:11 - app but if we just open up the console
95:14 - and I'm going to ask any question now
95:16 - down in the console we can see that
95:18 - we've got four chunks let's come right
95:21 - back here to the Retriever and I'm just
95:24 - going to whack this up to 10 and I'll
95:26 - ask a question and there we are we get
95:29 - 10 chunks down in the console and we
95:31 - could of course take it down to as low
95:33 - as one and we get one chunk now I'm
95:36 - going to leave that empty so it defaults
95:38 - to four I think that is absolutely fine
95:41 - for this use case but do be aware of
95:43 - that setting because it can be pretty
95:45 - useful if you get more chunks you can
95:46 - provide more context but if you feel
95:48 - your answers are a bit vague and off
95:50 - topic you might want to reduce that
95:52 - number and just focus in on the best
95:54 - quality chunks because remember the
95:57 - retriever is going to give you the
95:58 - closest matches first okay let's just
96:01 - take that out of index.js because else
96:04 - it's going to cause US problems now you
96:06 - have got other options you can look to
96:09 - prompt engineering so just come here to
96:12 - where we've created our templates the
96:14 - answer template the Standalone question
96:16 - template and you can just think about
96:18 - how you can tighten those up I think Al
96:20 - promps are pretty good here but if you
96:21 - find you're not getting the performance
96:23 - you expect you can mix things up go into
96:25 - more detail add some examples and just
96:28 - experiment and see what you get now
96:30 - lastly you shouldn't forget the open AI
96:33 - settings again in index.js we've got the
96:36 - llm setup right here and what we could
96:39 - do of course is open this up and in here
96:43 - we can use any open AI settings we want
96:45 - so we've got for example temperature and
96:48 - when working with your own knowledge bot
96:49 - it might be a good idea to set that to
96:51 - zero remember the higher the temperature
96:53 - the more daring the AI gets and we don't
96:56 - particularly want daring here we're not
96:57 - trying to be creative so actually
96:59 - setting that to zero might be a pretty
97:01 - good idea now you could also change
97:03 - which model you're using GPT 4 GPT 3.5
97:07 - any other new model that comes out and
97:09 - likewise here we've got access to
97:11 - frequency penalty and presence penalty
97:14 - and they might not be so relevant for
97:16 - this use case but they are there if you
97:18 - need to use them as well as any other
97:20 - setting that you can use with the open
97:22 - AI API so do be aware of that and you
97:25 - can always come in here and change
97:26 - things if you need to I'm of the opinion
97:29 - that if it's not broken you don't need
97:30 - to fix it so I'm actually going to leave
97:32 - this exactly as it
97:34 - was okay so those are five things that
97:37 - you can look at if you're not getting
97:38 - the performance you expect and so
97:40 - hopefully that will be useful if you do
97:42 - run into problems okay we're pretty much
97:44 - done with this so let's just take one
97:46 - more scrim to recap what we've
97:50 - studied
97:55 - ation on finishing this course you now
97:58 - have a really strong foothold in the
98:00 - world of Lang chain and you've got the
98:01 - skills you need to build powerful
98:04 - scalable AI applications quickly let's
98:07 - just recap what we studied firstly we
98:09 - used a text splitting tool from Lang
98:11 - chain to split our documents into chunks
98:14 - we created vectors for those chunks
98:16 - using an open AI embeddings model we
98:19 - then set up some templates and prompts
98:21 - and we can see examples of those right
98:23 - here when the prompts were ready to go
98:26 - we chained them to large language models
98:28 - and the string output passer using the
98:30 - pipe method and again we can see an
98:33 - example of that right here now those
98:35 - were the more basic chains and we came
98:37 - on to more complex chains a little bit
98:39 - later but first we took the user's input
98:42 - and vectorized it and then found the
98:44 - closest matches from the chunks in our
98:47 - Vector store we use those chunks to
98:49 - generate the final answer and we chained
98:51 - it all together using a runnable
98:53 - sequence which is a really cool way to
98:55 - build a more complex chain wow that was
98:58 - quite a lot so why don't you head over
99:00 - to our Discord Channel and go to the
99:02 - today I did Channel this slide is a link
99:05 - right through to that channel and brag
99:07 - to the world about what you've achieved
99:09 - and with that all that's left to say is
99:11 - thank you very much for taking this
99:13 - course do feel free to reach out to me
99:15 - on Twitter at TP chant it is always good
99:17 - to hear from you and always good to get
99:19 - feedback and till next time good
99:25 - luck

Cleaned transcript:

this course will help you master Lang chain a revolutionary AI first framework Lang chain enables developers to build context aware reasoning applications by linking large language models with external data sources for advanced natural language processing applications starting with the basics of text processing and vectorization and advancing to the nuanced aspects of laying chains expression language and retrieval techniques you'll gain handson experience in building real world AI applications Tom chant developed this course he is an experienced developer and course creator at scrimba hey there free code campers and welcome to this interactive course where you're going to learn how to use Lang chain JS to build a context aware chatbot that can answer questions on a specific document we provid it we're starting with the basics so you don't need any prior experience with Lang chain JS or even with AI the only prerequisites for this course are a knowledge of working with apis and vanilla JavaScript now Lang chain has a reputation for a steep learning curve but using its new expression language we make the journey much easier this is a Project based course with challenges and that means you're going to get your hands on the keyboard writing code throughout and if you're wondering how you can access the project code don't worry we've got you covered from the interact active version of this course on sca.com you can pause the video edit the code and run the projects right there in your browser or you can download the code from the scrims and run them locally if you prefer the link is in the description and one final thing if you enjoyed this course do hit the thumbs up right here on YouTube and if you'd like to get in touch reach out to me I'm on Twitter or X at TP chant okay let's Dive In hang chain to build aiiowed applications I am super excited to bring you this course on using one of the most mindblowing Technologies in the emerging AI Universe we are going to study embeddings we'll be working with Vector stores we'll be building templates and creating prompts from those templates next we'll look at setting up chains and we'll do that using Lang chains expression language which is a more expressive and accessible way to write Lang chain we'll be looking at the pipe method to join elements of a chain together we'll be retrieving data from a vector store and then we'll use the runnable sequence class which is a really cool way to create complex chains in Lang chain and there's loads more plus challenges this is our project we are building a bot which is knowledgeable on information we give it so this bot is going to know all about our platform sca.com and we're going to feed it 3,000 words of information and then we'll be able to interrogate it on this information this is one of the most powerful uses of AI and it's a process that Lang chain makes easy now in this course we're going to focus on the Lang chain syntax the project is built in vanilla JavaScript and you're welcome to refactor it into any framework or Library you wish I'm assuming you have a solid knowledge of vanilla JavaScript but you don't need to be an expert if you've worked with apis and asynchronous JavaScript a little bit before you're going to be absolutely fine my name is Tom chant and I will be your tutor for this course you can find me on Twitter or X or whatever you want to call it I am @ TP chant and you can click this link and visit me there it's always good to hear how you got on and any feedback you might have now before you begin why not head over to our Discord server and meet the community and let everybody know that you're starting this course in the today I will Channel and again this slide is a link to that page on our Discord server before we we dive into the concepts and code let's get an intro to Lang chain from the founding software engineer of Lang chain Jacob Lee Yes L chain is a framework that helps developers build context aware reasoning applications Lang chain was born out of the realization that the process of developing sophisticated AI powered apps could be significantly streamlined by factoring out some common abstractions while much of current AI app development akes place in the realm of python there's a clear need for better tooling for the more webd focused JavaScript Community too hence L chain comes in two flavors and one of the aims of Lang chain JS is to make large language models and techniques around working with them more accessible to this broader audience in this course you'll be learning how to use Lang chjs to build your own apps it focuses in on conversational retrieval over document giving an llm access to the specific information in that document so it can answer questions and continue a logical contextualized conversation on topics Beyond its original training data along the way you will use models prompts and Alpha parsers some of the basic building blocks of lank chain we'll create chains of calls that enable us to connect up the various stages in the process need to get the desired output we'll be using L chain with superbase and the open AI API but one of the beauties of this framework is that components are easily swappable so you can work with a myriad of databases Vector stores and llms switch between them and find the one which works for you best good luck and I really hope you enjoy this course and enjoy using L chain to spin up really powerful AI web apps of how this app is going to work in this first section we need to work with our data so this chatbot is going to be knowledgeable about a specific Topic in this case the scrimber platform so we're going to start off with an information source which holds the knowledge we want the chatbot to have and we're going to pass this document to a splitter and the splitter is a lang chain tool which will split the document into chunks and then we're going to use an embeddings model from open AI to create vectors from each chunk and we're going to save those chunks to our Vector store which in this case is going to be a superbase vector store when we get to this point the vector store is established it's got all of the knowledge we want it to have and we won't need to repeat this process unless we want to give it more knowledge by adding some new data now if you don't understand the justification for each step or you're not really clear on vectors and embeddings yet don't worry we're going to go into a lot more detail when we actually write the code this is just an overview now once we've got the vector store set up we need to create the app to use it and the flow of that app is going to look like this we start off with a user which we've got represented right here and that user will input something probably a question they have about scrimber and we're going to do two things with that question we're going to save it to a conversation memory store which will hold the entire conversation and we'll use an open AI model to convert it to a standalone question and that just means we're going to reduce it to a very concise question with no unnecessary words we'll then take that Standalone question and we'll use an open AI embeddings model to create vectors from it we'll send those vectors to our superbase Vector store and we'll get back the chunk or chunk with the nearest match therefore the chunks that are most likely to contain the answer to our question the last stage is to use an open AI model to get the final answer and to give it the best chance of getting a good answer we're going to give it three pieces of information we're going to bring down the nearest matches from the vector store we'll give it the original user input and we'll also give it the conversation memory which will be the entire conversation that has taken place so far so that might be a very very long conversation or this might be the first question in the conversation now we're going to take the response that we get and we'll store it in the conversation memory ready to continue the conversation and also we'll give it back to the user by rendering it to the Dom now again this is a very high level overview and it might well cause some confusion for example what actually is a standalone question and why are we using it right here but then down here we're using the original user input and not the Standalone question but rather than get caught up that right now and give very theoretical answers what I want to do is keep referring to this diagram as we write the code and we're going to tackle those questions as they come up one by one so if this diagram looks horribly confusing right now don't worry at all we are going to take it step by step okay in the next Grim let's go right back to the beginning and think about getting some data into our Vector store but before we do that I'm going to bring in a scrim from my colleague Gil who's going to give you a detailed overview of embedded ings so sit back and take a few minutes to watch that next powered search shapes many parts of your daily lives every day you interact with platforms sifting through massive amounts of data from text and images to audio and video think about Amazon recommending products or search engines refining your queries social media platforms curate tailored content while services like YouTube Netflix and Spotify offer suggestions based on your preferences now Advanced AIS despite their capabilities don't truly understand the real world as we do they can't grasp the actual meaning or Nuance of a video title song or news article so how exactly do AIS and platforms like Spotify Netflix and YouTube truly get us how is it that they appear to understand predict and respond to us as effectively as if not better than people well the magic behind this capability involves a blend of algorithms AI models and huge amounts of data but a larger part of the answer involves embeddings you see when you present a question to an AI it first needs to translate it into a format it can understand so you can think of embeddings as the language that AI understands the term embedding is a mathematical concept that refers to placing one object into a different space think of it like taking a word or sentence which is in a Content space and transforming it into a different representation like a set of numbers in a vector space all while preserving its original meaning and the relationships between other words and phrases AI systems process lots of data from user inputs to information and databases at the heart of this processing are embeddings which are vectors representing that data transforming content like search queries photos songs or videos into vectors gives machines the power to effectively compare categorize and understand the content in a way that Almost Human so how is all of this possible well it isn't exactly as easy as just turning data into vectors so before we go any deeper let's take a closer look at what vectors are think of a vector as a coordinate or point in space and to keep things simple we'll have a look at this 2D graph with an X and Y AIS let's say that a word like cat is translated into a vector like 4.5 12.2 which is this point this Vector encapsulates the meaning and nuances of the word cat in a way an AI model can understand and then we have the word feline represented by a nearby Vector of 4.7 12.6 so we'll place that point on the graph now words that have similar meanings are numerically similar and tend to be closely positioned in the vector space so this closeness implies that cat and Feline have similar meanings now let's say we have the word or vectors for kitten which might also be close to cat and Feline but maybe slightly further apart due to its age related Nuance now a dog dog is different but still in the same general domain of domesticated animals so the word dog might be represented by a vector that's not too distant but clearly in a different region let's say 7.5 10.5 and even a phrase like Man's Best Friend which is a colloquial term for a dog could be represented by a vector that's close to the vector for dog on the other hand a word like building is not related in meaning to any of these so its Vector would be much further apart let's say 15 .3 3.9 here's another example that demonstrates how embeddings might capture semantic meaning and relationships between words let's say we have the word King represented by the vector 25 then man is the vector 13 and woman is represented by the vector 14 now let's do some quick Vector arithmetic we'll start with a vector for King then subtract the vector for man to remove the male context and add the vector for woman to introduce new context after performing this Vector math our resulting Vector is 26 so we'll plot that point on the graph and let's say there's another word in our space queen represented by the vector 2 6.2 right here well this Vector is extremely close to the resulting Vector so we might identify queen as the most similar word based on that Vector just as a trained AI model would now a twodimensional graph is a massive simplification as real world embeddings often exist in much higher dimensional spaces sometimes spanning hundreds or even thousands of dimensions for example the actual Vector embedding for the word Queen might have values across multiple Dimensions each Dimension or number in this Vector might capture a different semantic or contextual aspect of the word Queen for instance royalty Cleopatra or even chess this is what allows the AIS to recognize and differentiate between these contexts when the word is used in different scenarios now imagine embedding hundreds of thousands of words and phrases into this highdimensional space some words will naturally gravitate closer to one another due to their similarities forming clusters While others are further apart or sparsely distributed in the space these relationships between vectors are extremely useful think back to spotify's method of embedding tracks in a vector space tracks that are positioned closely together are likely to be played one after the other all right so what else can we do with embeddings and how are they used in the real world well you can imagine how embeddings have revolutionized our daily experiences for example search engines have evolved to understand the essence of your queries and content moving beyond mere keyword matching and recommendation systems with the aid of embedding suggest products movies or songs that truly resonate with our preferences and purchase history for example Netflix uses them to create a tailored and personalized platform to maximize engagement and retention also in the healthcare industry embeddings are used to analyze medical images and extract information doctors can use to diagnose diseases and in the finance World embeddings help with analyzing financial data and making predictions about stock prices or currency exchange rates so every time you interact with an AI chatbot every time an app recommends something behind the scenes embeddings are at work translating data into meaning all right so how are these embeddings actually created well let's dive into that next that Lang chain integrates with and if you go to the Lang chain docs and check out modules retrieval Vector stores and Integrations you will see a whole long list of them and by the way this slide is a link through to that page in the docs now for this project we're going to use super base and superbase is a really popular and very userfriendly Vector store but one of the beauties of Lang chain is that it's actually really easy to swap out your vector store so if you want to experiment with various possible Integrations that won't be too much trouble at all now our first task is to set up a superbase account so so head over to superb.com and again this slide is a link to superb.com and let's select start your project and there you can sign up with your email or GitHub and once you've completed the sign up you'll end up at the dashboard so let's go to new project and we need to give the project a name I'm going to call this one scrim bot and we'll also need a database password and I'm just going to allow it to generate a random password password for me and lastly I just need to select my location I'm here in Western Europe not that far from London so that will do just fine then we can scroll down to the bottom and just click create new project now you'll wait a while while it initializes a couple of minutes at the most and eventually you're going to end up here and that shows us that we've got one database successfully set up now if you click on this tables icon on the left hand side here in the table editor tab is where you could manually set up a table but we don't need to do any of that because Lang chain is going to do it all for us if we go back to the Lang chain docs to the super base Integrations page and we just scroll down we get this chunk of code and we're going to run this in our superbase database and it's going to do everything that we need now I've just pasted this code into scrimber just so it's easier to look at and just to be clear we're not running SQL or SQL right here in scrimba we'll be running it in super base and I'll show you how in just a moment but I just wanted to put it right here so we can have a look at it in a bit more detail so what this code does is it enables a PG Vector extension in superbase it then creates our table with everything that we need in that table and it sets the embeddings to 1536 and that's an important number because the open AI embeddings model use 1536 Dimensions it also gives us this match documents function and it's this function which actually does the job of finding the the nearest match so later we'll be using this function to take the vectors from a question and find the nearest vectors from the text chunks because that will identify the text chunks which are most likely to contain the correct answer so all we need to do with this code is copy it go back to superbase and come into the SQL editor tab which is this second one down and paste it right in there now at the moment it's just called Untitled query which is not a great name so I'm just going to change mine to match documents because that's what this main function in it is called now it tells us click run to execute your query so we can do that right here and there we are it says success and if we go to the table editor and we click on documents we can see that we've got our empty table we've got the ID the content the metadata and the embedding I.E the vector and that is what we asked for right here where we said create table documents so that has worked just fine we've got this warning about allowing Anonymous access but don't worry about that right now this is going to be absolutely fine for prototyping now if you'd like to take a deeper dive into vectors and embeddings and exactly what superbase is doing here do check out this blog post that I've linked to right here it's got quite a lot of information okay so now our database is ready to go so in the next Grim let's start getting some vectors into the vector store next which is going to be the knowledge for our chatbot and split it into chunks and the idea is that each chunk will be big enough to hold a piece of information so what you want to avoid is having something like this one chunk where we say we update our course is and a second chunk saying on a regular basis that piece of information needs to be in one chunk else it's pretty much useless now a chunk will often not be that small in fact lots of our chunks are going to hold whole paragraphs so there'll be something much more like that and it doesn't matter at all if there's more than one piece of information in a chunk and in fact that's very likely going to happen all we're trying to achieve here is being able to give an AI Model A smallest chunk of text from which it can find the answer to a question what we could do is just upload a massive document with every request to say the open AI API but that would be very very expensive with tokens so what we're do here is much more economical much more performance and much more scalable okay that's the theory and this is the actual text that we're going to be using for our chatbot it's about 3,000 words long and I've checked and vetted all of the information myself that's really important if the information in the document at the start is faulty you're never going to make a good chatbot now I'm using a text file here for Simplicity but Lang chain has actually got several tools for working with different formats so you can click through to this section in the docs and it will show you options like passing PDF or extracting just the text from HTML to give you just two examples but here we're going to keep it simple and just use a text file so in index.js then I've got this TR catch we're fetching in scrimba dasinfozentrum knowledge source for our chatbot and now we need to split it and we're going to do that using a tool from Lang chain which will do most of the work for us so I've got the Lang chain dependency already installed and in scrimber we do that using a three dot menu that appears when you hover over dependencies you can't see it in the recording but when you click on that three dot menu and select ad dependency a dialog box appears and I can type in Lang chain and it does the rest for me outside of scrimba of course you can use npm install Lang chain okay now Lang chain offers us a couple of tools to split text there is the character text splitter and the recursive character text splitter I'm going to use the latter which is just a little bit more sophisticated but to be honest you can use either and I didn't see a big difference in performance between them in this app so let's import the tool from Lang chain and if you want the more basic character text splitter it's just going to look like that okay let's save a new instance of this recursive character text splitter to a const splitter and then we'll save our output we'll call splitter and use the create documents method now we need to pass create documents an array and inside the array we can list out what we want to be split we've only got one file text but if we wanted to upload to a vector store from multiple files we could list them out in here okay let's just log out the output and see what we get and I'll hit save and then in the console we can just see a promise and that's because this is an async process so we need to await it and there we are as soon as we do that we see our chunks of text and I'm just going to copy one of those chunks and bring it over to Output MD so we can have a look at it so it looks like this we've got page content and that has got the actual chunk of data now it's actually much much long longer than that the scrimba console truncates it heavily and then we've also got the metadata the metadata is quite interesting it gives us the location of that text Chunk lines 1 to 14 so what we're actually looking at if we go back to scrimba info. text is right from the beginning from 1 down to 14 so that is our text Chunk now we W be using this location data in our app but it is good to know for future reference that if you need to refer to where some information came from you have got that information right there provided to you by this text splitting tool now this text splitter that we've used the recursive character text splitter has made some assumptions about chunk size it actually defaults to a th000 characters does chunk size matter well yeah you bet it does larger chunks get more context smaller chunks get more granular semantic info if you go to either Extreme Performance could suffer and if chunks are really big sending that off to the AI models will get expensive now of course there's no best way of splitting and you do have to consider the text you're working with I've experimented a bit and I want to go for a chunk size of 500 not 1,000 so I'm going to come in here with an object and override the default settings so I'll set my chunk size to 500 again we'll hit save and what you can see is where before the first chunk went from lines 1 to 4 14 now we're going from lines 1 to 8 so the chunk is a little bit smaller and I'm just going to use Chrome Dev tools not scribus console to get the text from the first two chunks okay so there we are we've got the first text Chunk and the second text Chunk right here and what you'll notice is that these text chunks are actually split quite nicely into paragraphs this first one ends at a very natural point and this second one does as well and that is not an accident and you'll also notice we've got some overlap here this is a repetition of what we've got right here and again that is not accidental so if we just go back to this object and I can come in here and show you a couple more defaults firstly we have these separators and these separators are an array and they go in order so we've got the double new line to break a paragraph the single new line the space and then no space so what this recursive character text splitter does is actually quite complex it uses those separators to split text into chunks based on the size but prioritizing keeping paragraphs then sentences then words together intact so that's why in these text chunks we've got right here we've got more than one piece of information but that's absolutely fine because they're both contained inside a chunk and they both finish on a natural break and we've got some overlap now we can also override the overlap and it actually defaults to 200 which is quite a lot so I'm going to come in here and this is called chunk overlap and I'm going to set it to 50 so 10% of our chunk size which is a good rule of thumb to start with and you can always experiment now you won't always see overlap and that's because when the paragraphs fit neatly into the chunks and the overlap size is not big enough to include a whole sentence then the overlap won't be applied it will actually only be applied if the chunk size does not end with one of the first two separators so the double new line or the single new line so that is all a little bit confusing and I wouldn't recommend that you worry about it too much do play around with it try different strategies also try the more basic character text splitter and just see which one gets you the best performance with your chatbot and remember you can always come back and adjust it later if necessary now I should just add that if you've got some other separators in your document it's quite popular to use say the double hashtag you can also just add them to this array it does accept custom separators okay I haven't got any of them in our document so I'm going to delete that and the next thing to do then is to take these chunks of text and get them uploaded to the vector store so let's look at that in the next scrim to store and the first thing that I'm going to do is come in here and save our superbase URL and API key to consts and we can get all of the information that we need from the dashboard if you come down here to settings and select API here we have got the project URL and the API key and I've saved mine in my environment variables so I'm just going to come in here and set up the consts and of course you'll need to use whatever you called your URL and API key when you set up your M variable now I'm also going to bring in the open aai API key as we're going to need that in just a moment next I need to set up a superbase client and we do that using the create client method which we get from superbase so before we can use that I need to add the superbase dependency and there we are the superbase dependency has appeared and I'm just going to put the the name of the dependency in a comment just so you can see it a little bit more clearly okay now we've got that superbase dependency we need to import the create client method from it and now we can use create client to set up a client now let's use create client with our URL and API key to set up a superbase client in the final stage of this we do two things at once we're going to create our embedded and we're going to upload them to the vector store to do that we need two tools from nchain the superbase vector store and the open AI embeddings class so let me just import them quickly okay first we're going to use the superbase vector store class so down here I will say await because it does work asynchronously super base Vector store and we're going to use its from documents method and we need to give this method three pieces of information the first one is the output that we've got right here so remember the output is the chunks of text that we've split next we need to tell it how to create our embeddings and we're going to do that with the open AI embeddings that we've just imported so we'll set up a new instance of that class and we just need to pass it at open AI API key which we've got right here now we could pass that as a key value pair like this but as the key and the value are actually the same we don't need to do that we can use the shorthand version Okay the third piece of information is an object holding our superbase details so it needs our client and again we don't need to write out the key value pair we can just use the shorthand version it also needs a table name and if we go back to superbase we can just see that when we go to the tables tab our table name is just documents and just before we run into an error let's remember that this needs to be a string okay let's hit save and see if it works and actually we should have logged something out there with a success message but I totally forgot but now we can go back to our superbase dashboard and I'm just going to come up here to the tables Tab and let's go to documents and we can see it's worked we have got our data we've got the ID the content which is the text for each chunk the metadata and the actual embedding here are our vectors and remember this field is huge we have got 1536 Dimensions here okay now we've got our vectors in our Vector store we can start work on the app that's going to allow us to query this Vector store so when you're ready for that let's go already have over on the right hand side you can see that we've still got our dependencies from Lang chain and superbase now this event listener at the top is listening out for user input so it's basically picking up clicks on this button when a user submits a question we've also still got our API key here that we're bringing in from our environment variable and that just leaves this function progress conversation socalled because it progresses the conversation onwards and also does the heavy lifting of updating the Dom so the user can actually see what response they've got back from the AI now it's all fairly straightforward JavaScript but do feel free to pause and take a look through and also do check out the HTML and CSS if you want to again it's fairly standard stuff okay so that's the code we're starting with so where do we go next with this project well let's head back to the diagram and see where we go first so we take the users input we've got the event listener listening out for that and then we do two things we've got the conversation memory right here and the Standalone question right here I want to focus in on the Standalone question first so let's start by unraveling its Mysteries and see what it actually is and why we want it so we'll come on to that in the next scrim question and just to be clear there are two things we need to do with the users's input they're creating the Standalone question and adding the user's input to the conversation memory now we're actually going to deal with memory at the very end so let's just ignore that for the time being and concentrate in on the Standalone question so what actually is a standalone question well a standalone question is just a question reduced to the minimum number of words needed to express the request for information okay but why do we want one well we can't control control what a user asks our chatbot and actually a user could easily ask a question in a way that's likely to get a vague or even an inaccurate response and we really want to avoid that so imagine you have an online clothes shop and a user asks something like this I'm thinking of buying one of your tshirts but I need to know what your returns policy is as some Tshirts just don't fit me and I don't want to waste money it's a perfectly reasonable question but let's remember how chatbots work we're trying to find the nearest matching vectors and therefore the chunk of text that will likely hold the answer to our question we want to create an embedding from this question and find the nearest matching vector and therefore the chunk of text that will likely hold the answer to the question in our Vector database but there's so much going on with this question that the vector for it will be polluted remember a vector represents the semantic meaning not the precise words so what we want to do to maximize the accuracy of our chatbot is just extract the intended semantic meaning from this question what we want to focus in on is I need to know what your returns policy is and we can reduce that to a standalone question that will simply be this what is your returns policy that is what the user wants to know and so that is what we need to search for in our knowledge document so that will be our first task and to do that we need to take a look at how prompts work in Lang chain and also how we send them off to the llm the large language model so let's do that in the next scrim chain to set up a simple prompt and just for demonstration purposes we're going to move away from our main project and imagine we're building an app which generates a promotional tweet for a given product we've got the Lang chain dependency already installed right here and of course we're bringing in the open AI API key next we need to to import two things from Lang chain first the chat open aai class and we also need the prompt template class so let's set up our llm and save it to a const so we'll take the chat open AI class and pass in our API key now we could instead pass in nothing and if we do that Lang chain will check process. mv. openai API key and use it if it's available so we could actually delete this line of code as well but to be honest I think it's clear if we just add it manually so I'm going to do it that way and also if we wanted to override any open AI defaults we could do that right here in this object for example we could change the temperature setting so now this llm will use a temperature of 0.5 but to be honest I don't want that I'm just going to leave everything at default for now okay now let's create a template for our tweet and I'm going to save it in a const tweet template and I'm just going to say generate a promotional tweet for a product from this product description and we need to give it a product description so I'm just going to put the product description variable inside curly braces now as soon as you see the curly braces you might think that this is a standard JavaScript template literal it's actually not there's no dollar sign needed here this input variable will be picked up by Lang chain and of course we could have several input variables here if we wanted to if this prompt was going to be a little bit more complex we could also have for example price and anything else we needed but let's just keep this really simple okay now we need to turn this template into a prompt so I'm going to set up a const for the prompt and now we can use the prompt template class and that comes with the from template method and then we just need to pass in our template and before we go any further why don't we just log out the Tweet prompt and see what we've got I'll hit save and we we're getting an error let's just check the console and it says chat open AI cannot be invoked without new okay that's a pretty clear error we needed the new keyword right here let's try that one more time and there we are we've got our prompt and just so we can look at it a little bit more easily I'm just going to bring it into a markdown file and format it okay so we can see that we've got our prompt right here just as we wrote it and the interesting thing is that we've got this input variables property and that is holding product desk so that's telling the prompt which inputs to expect and this is an array so if we had lots of input variables they would just be listed out in this array and now that the prompt is expecting them it's going to throw an error if it doesn't get them another interesting thing to notice here that the template format is an F string and an FST string is something which comes from Python and it is the kind of python equivalent to a template literal okay so that is our prompt ready to go so in the next Grim let's take a look at how we can use it to generate some content template is time to set up our first chain in Lang chain I'll set up a const tweet chain to hold it and what this chain will consist of is our prompt chained to our llm and to join the two parts of the chain together we use the pipe method and I'm going to pass in the llm so the pipe method is joining the two aspects of the chain it takes the output from the first which will be the prompt and it passes it to the llm it's a really simple chain with just one connection again let's just log that out and see what we get and again I'm just going to paste that into our markdown file okay we've got quite a lot going on here but I want to draw your attention particularly to runnable sequence because that is a theme that we're going to be coming back to quite a bit in this course but just by way of an intro what we've got here is a runnable sequence and we can see that this object right here is called First and that is the first element in our chain we've got our prompt right here with our input variables and then we've also got last which is the last section of the chain and here you can see that we have got our open AI llm or large language model now if this were a more complex chain here in the middle we would see some more steps but for now it's just enough to know that this runnable sequence exists okay so if we go back to index.js what we need to do next then is invoke this chain to start the sequence in motion and we're going to do that with a method called invoke so let's set up a response and we need to await our tweet chain because this is an asynchronous process now let's call the invoke method on the chain and we'll just log out the response now if we hit save right now what do you think is going to happen well we're getting an error down in the console it says missing value for input product desk well that figures because look we're expecting an inut variable right here product desk and we're not actually introducing a product description to the chain so what we need to do then is pass in an object right where we invoke the chain so you can think of this as passing something in right to the beginning of the chain and what we need to pass in is a product description so product desk will be our key and then the value will be whichever product we want a tweet for okay let's save that again and down in the console we have got our tweet and in fact we can Zone in specifically on the Tweet by saying response. content and there we are we've got our tweet it's even got some emoji and some hashtags so this is working okay this would be a great time just to pause make sure you understand this code maybe generate a couple of tweets perhaps make this tweet template a little bit more complex with multiple input variables and then when you're ready we'll go back to our main project where we need a prompt and a model and that will give us a great opportunity to have our first challenge so when you're ready for that just move on prompt template and chat open AI already imported we've also got our open AI API key and the llm already set up so here is your challenge I want you to create a prompt to turn a user's question into a standalone question and I've just put a hint here the AI understands the concept of a standalone question you don't need to explain it just ask for it then create a chain with the prompt and the model and lastly invoke the chain remembering to pass in a question and for now we'll just log out the response to check it's working now just to be clear in this challenge we are not going to be wiring this up to the chat interface yet if we were going to do that we would have to actually come down here into this progress conversation function and update some of the logic in here it's a little bit of unnecessary complication at this point we will be coming on to that later also I've given you the four consts that you're actually going to need to complete this Challenge and we're just logging out the response right here I've also given you some hints above each const and I've done that because there is quite a lot of new syntax here and of course you are welcome to go back to that scrim to check the syntax if you need to but hopefully with these hints that might not be necessary now when you do this just remember that the question you pass in should be quite a long winded question because the idea is to reduce that question down to just the bare minimum so you want to make that a long question with some unnecessary words it's only then that we'll be able to see that it works okay pause now take all the time you need and I'll see you back here in just a moment okay okay so hopefully that went just fine so we'll start off with the Standalone question template this is going to be a string and we're just going to ask for what we want so I'm going to say given a question convert it to a standalone question and then I'm going to say question colon and here I'll use the curly braces and we'll introduce the input variable which is question now I'll invite the model to provide us with the Standalone question by saying Standalone question and just finishing on a colon so that's like an invitation to complete right now we need the prompt so let's set Standalone question prompt equals to prompt template and we'll use the pr template method and pass in Standalone question template then we'll set up the chain so we'll take the prompt that we've just created and we're going to link up the chain with the pipe method and we're piping the llm and remember we've got the llm already set up right here okay to check it's working we need to invoke it and pass in a question so we'll take that chain and this is an async process so we'll await the Standalone question chain call the invoke method and we'll pass in an object we've got the input variable that the prompt is expecting right here it's question and for our question I'm going to go for something quite longwinded I've asked what are the technical requirements for running scrimba I only have a very old laptop which is not that powerful okay we're logging out the response so let's hit save and see what we get down in the console and there we are can a very old laptop meet the technical requirements for running scrimber it's reduced that question down to a standalone question it's just asking for the precise information and it's removed unnecessary words okay that is the first part done so now is a really good time to take a break and relax especially if you've been working at this for a while and when you're ready to go on we're going to look at how we can take our Standalone question to get chunks of matching text from the vector store but there is quite a lot to do that there are several steps in that process and some preparation so for now take a moment to chill and come back when you're ready and see where we are and we are right here so we've taken the users's input and we've created this Standalone question from it and the next step will be to create embeddings from that Standalone question and then take that to the vector store to find the closest match so if we have a standalone question like what is a scrim we'll use the embeddings model to create our vector and then in the vector store we will search through all of the vectors we'll find the closest match and we'll take that chunk of text now this diagram is a little bit of a simplification we might not just take one chunk of text we might take two three even 10 chunks of text and a bit later on we'll look at how we can exert control over how many chunks we retrieve from the vector store okay before we can do any of that we need some basic setup so I've bought in a few Imports here which we've seen before we've got the superbase vector store class I've also bought in the open AI embeddings from Lang chain and create client both of which we used before when uploading to the vector store so those are all of the Imports we need for now and then down here I've set up a new instance of embeddings I've passed in the open a API key do remember that if you don't pass anything in Lang Chain by default will look for process. mv. openai API key in this format and do it for you I'm going to pass it in here manually just for clarity okay we've also got our superbase API key and the superbase URL and we've used that to set up the client and again that is identical code to when we were uploading to the data store okay now things get a bit different to what we did before let's start a new instance of the superbase vector store in a const and and we're going to pass it our embeddings model and we'll also pass it a configuration object in this configuration object we only really need to pass it the client but again just for clarity I'm also going to pass it the table name and our table name in superbase was documents we can see that one right here in the superbase dashboard and I'm also going to give it the query name and again if we look at the superbase dashboard we've got this query match documents which we named after this function match documents so let's pass that in right here so these two are both defaults but I think it's a good idea to put them in here right now because if in the future you're working with a more advanced use case perhaps you've got more than one table or more than one query name it's important that you know that this is where you can configure your vector store okay now this is quite different to what we were doing when we were uploading to the vector store we actually want to retrieve something and without Lang chain we would have to actually write a ton of code to do that but now all we're going to do is create a new const and I'm going to call it Retriever and I'm going to set it equals to our Vector store and I'm going to call the as retriever method on it and that is all we need to do this as retriever method knows to go to the vector store and instead of inserting more data to use the matching documents function that we added here to find the nearest matches so that makes it nice and easy and if you'll remember this match documents query came straight from the Lang chain docks so all of that provided by Lang chain just makes life really easy okay this retriever is now finished and that means it can now be added as an element in a lang chain chain so let's do that next we're just going to come in here down to our chain which we've got right here and we're just going to pipe the retriever on the end so what should now happen is that we invoke the chain and we pass in this question at the top of the chain the Standalone question prompt creates its own prompt from that question it's piped to the llm to get the Standalone question and that should now be piped to the retriever to retrieve the nearest chunks from the vector store well we're logging out the response let's hit save and see what happens and if we open up the console we're getting an error and it's one of those kind of vague errors e. replace is not a function so I think what we should do next is isolate our Retriever and see if it's working so I'm actually going to delete this pipe that we put on the end of this chain and I'm going to set up a second response right here and this is just a test so I'm just going to call it response two and what we're going to await here is our retriever we'll call the invoke method and we'll just pass in a standalone question and I'm going to follow on from this question here and just say will scrimber work on an old laptop okay let's just log out response 2 and see what we get and look we are getting some chunks and we can see that we've actually got four chunks and the first one starts what are the technical requirements so that is looking good for an answer to this question so I'm thinking that our retriever is working just fine so why is it not working in this chain well I think if we log out the original response as well we might see an answer to that question so let's run this again and now we've got quite a lot of data in our console too much to see easily so I'm just going to paste it into a markdown file okay so what we can see is that where we were logging out the original response we have got this object right here and we've got the Standalone question what are the minimum technical requirements for running scrimba can I use it on an old less powerful laptop so that was the Standalone question here we've got our chunks and we've got four of them and that's great so what exactly is the problem if you want just pause for a moment and see if you can figure it out okay so maybe youve figured out that if we have a look at the chain what we're actually doing when we pass things along the chain is passing them along in their expected data type so we invoke this chain with this string and the Standalone question prompt is expecting a string and the llm is expecting the output of the Standalone question prompt it knows what format that's coming in but when we try to pipe the retriever on the end what we're doing is we're passing along this object but what we've just seen from our experiment is that the retriever works with a string so the only problem here is that we need to be passing along just the string from this response now we could do that with DOT or bracket notation but Lang chain actually gives us a better way of doing it and it introduces us to the concept of an output passer and that is something that you should know about so let's look at that next so Lang chain offers various output passes to suit some specific situations so for example you might need to Output in Json or you might even need binary data we're not doing anything so complex here but we can use the most simple output passer which is the string output passer and as the name suggests it's going to pass the output as a string so if we go back to our code I've put this chain back to how it was we're piping in the retriever at the end and we're just logging out the response now we already know that that doesn't work we're getting this error so now let's import the string output passer and all I need to do is add a new instance of the string output passer to the chain and it's going to come right in here so we're taking what we get back from the llm and that is what we want to pass as string so let's come in here and we're going to pipe in the string output passer and the string output passer is a class and in here I just want a new instance of it it and let's invoke that and when we hit save and I open up the console there we are it is working we have chained everything together and now we are getting the four chunks back from the vector store and just to see the string output pass in action let's quickly delete the retriever so I'll just delete this off the end now when I hit save the output from the llm is just a string before without the output passer we were getting an object okay let's just put all of that back so that is pretty cool we're definitely making progress however you might be looking at this and thinking we've got pipe and pipe and pipe how many pipes are there going to be in this chain and also this chain is called Standalone question chain but we've just added the retriever on the end and that is a little bit strange so why don't we just come in here right now and do a mini refactor and I'm just going to call this chain chain and of course we need to update that where we invoke it shortly we'll be breaking this chain up and doing a bigger refactor but first I want to show you the limits of what we can do with just the pipe method so next let's think about using what we've got down in the console to try to formulate an answer to our question now I think we going to run into not one but two problems so let's go and check that out feeling it was starting to get kind of messy and we've got another prompt to add so to tidy things up a bit I've taken all of the retriever code and given it its own file here inside this utils folder I'm just exporting it and then importing it back into index JS right here the retriever is pretty stable now we will do a tiny bit more with it later but it's good to tuck it out of the way and we'll do that as we go along just to keep things here in index.js manageable okay let's go back to the app so we've got this chain down here which we're invoking with this question and the question becomes a standalone question which we're using to get relevant chunks of data from the vector store so if we have a look at our flow diagram we've done all of this we're right here we're finding the nearest match from the vector store and what we want to do next is create a prompt to get us the answer now we're going to do that using three pieces of information we need the nearest match from the vector store we need the user input which we're getting right here that's not the Standalone question that's the original question that the user asked and we also need the conversation which will later be stored in memory now we're not doing that until the end so I'm going to leave the conversation crossed out for now now it might cause you a bit of confusion that we're creating a standalone question here but we're not actually using it to get the answer so just to explain the role of the Standalone question is to get the most relevant information from the vector store the the original question might well contain more data which the llm can actually use to generate a more relevant conversational answer but we wouldn't want to use that data to find matches in the vector store let me just show you what I mean with a little diagram so imagine a user comes to our chatbot and they say I'm a complete beginner and really nervous is scrimba for me well we reduce that to a standalone question is scrimba suitable for beginners now if we use that question to generate our final answer our final answer would look something like this yes scrimba is suitable for beginners and that's a perfectly correct reasonable answer and it will leave the user feeling I think fairly neutral now if we use this original question when we form the final answer we can actually use the extra information that we didn't use when we were going to the vector store so that means we'll be able to form a more conversational response yes scrim is perfect for you we're a friendly community so don't be nervous and that will actually give the end user a much more positive feeling from the chatbot okay so that's why we're using the original question in our answer template so now it's time for a challenge and here it is I want you to create a template and prompt to get an answer to the user's original question remember to include the original question and the text chunks we got back from the vector store as input variables and you can call those input variables question and context and I've just put this warning here that if you go ahead and add this to the chain this is where you're going to find the limitations of chaining pipe methods together so don't be surprised when you get an error you might already see where the error is coming from and in fact there's going to be more than one problem to solve here and it will take us several scrims so really just take this challenge as prompt template practice and I've put here a wish list of what we want this chatbot to be able to do so you can and think about that as you're writing your prompt okay pause now get this challenge sorted and we'll have a look together in just a minute okay so hopefully you managed to do that just fine I'm going to come in here then and create an answer template and this is the text I've written you are a helpful and enthusiastic support bot who can answer a given question about scrimber based on the context Prov divided try to find the answer in the context so I've put that in there just to emphasize that the context is the knowledge that we want the chatbot to use if you really don't know the answer say I'm sorry I don't know the answer to that and direct the questioner to email help at sca.com don't try to make up an answer and always speak as if you were chatting to a friend so so hopefully we'll get a nice friendly engaging response from the chatbot now we just need to add in the input variables so we've got context and remember that will be the chunks we got back from the vector store and we've got question and then I'll finish the prompt with an invitation to create okay now we need to use this template in a prompt and let's come down here and we'll add it to the chain so we can just pipe it right on the end and is this going to work well no I've already given you a spoiler this is going to cause us an error and the error we're getting is missing Val for input context so that's pretty understandable we're not passing in context in the format it's expecting so what we need to do at this point is rewind and start sorting out the data we get back from the retriever it's a little bit of a process but let's start that in the next scrim the output from the retriever is this array of objects with metadata and all we want is the text and we're not trying to get that text into any special format we just want a string so what we can do is create a function to extract the text join it into a string and we can just add that function to the chain so basically what we want this function to do is map over this array get the text from page content and join it into a string so I'm going to call this function combine documents and it can take in docs as a parameter and that will be what we get back from the Retriever and now we just need to return the docs having mapped over them and returned the page content as we can see down in the console it's the page content that we're interested in and we need to join them together so let's use the join method and we'll take a double new line As the separator let's see if that works so I'm just going to add this function onto the chain and now let's open up the console at the moment we've got the array of objects and when we hit save we get back one one big string so that is working that's really good let's try chaining on then the answer prompt that we've got right here and I'll hit save and we get an error we're still missing the value for the input context and I did warn you there are multiple problems here so now what we see are the limitations of this pipe method we can give the answer prompt a string but we don't have an easy way to give it the object it wants remember it wants both context and question but luckily we have got another way to make Chains It's an alternative syntax called the runnable sequence we did touch on it earlier and it gives us more options now it's quite a big topic we're going to take several scrims to go over it but before we dive into that it might be a good time to take a break if you've been doing this for a while but just before we finish in my effort to keep this code base a little bit tidy I'm just going to take this function right here and move it into the utils folder right when you're ready and rested let's move on I've got a feature from a language app which is going to take in a sentence and we've got one here I Don't Like Mondays and it's going to do three things now you'll notice I've marked some errors in this sentence we've got a lowercase first letter we're missing an apostrophe in don't we've got a D on the end of liked which makes no grammatical sense and we're missing the uppercase first letter Mondays now the first llm call is going to solve the punctuation errors so we'll get uppercase I apostrophe and uppercase M we've still got the grammatical error but that's dealt with by the second llm call now the sentence is correct and we'll use the third llm call to translate that sentence to a given language in this case French now if that sounds a little contrived to you well it probably is I was just looking for a not too complex way of demonstrating a few things with runnable sequence so so just bear with me now we've already got the first two prompts here so we've got the punctuation prompt and the grammar prompt and then we're invoking the chain but at the moment the chain doesn't exist so let's build it using a runnable sequence and we're going to start that off by importing the runnable sequence now let's come down here and set up the chain so we'll say runnable sequence which comes with a from method and we pass in an array and in this array we can start building the chain so we'll start with the punctuation prompt and let's just open up the console let's just hit save and see if that works and it actually fails and that's happening because we've only got one thing in the runnable sequence a runnable sequence needs at least two elements in the chain so let's add in the llm and I'll hit save and this time it looks like it's working we've got down in the console I Don't Like Mondays and as you can can see the punctuation has been corrected the grammar mistake liked with a d on the end is still there okay next let's add the string output passer and again I'll hit save and there we are we're getting our string so this is looking like a nice neat way to do chains let's just add in the grammar prompt we'll hit save oh and that one fails and again we've hit the problem that the template in grammar point is expecting an input variable punctuated sentence so let's just rewind now these three elements in the chain work because what they pass along is in the format expected by the next element in the chain now one thing that's really cool with runnable sequence is that you can put an arrow function in the chain and log out exactly what's happening at that point in the chain to see what's going on so let's try that now so preve result is a parameter in an arrow function and I can call it whatever I want but preve result seems like a good idea as we're seeing the result of the previous action so let's hit save and we're going to get an error because we've broken the chain but we will see what we're logging out we've got the sentence property holding I Don't Like Mondays and the language of French and that is just logging out the result of our invocation that we can see right here so what I want to do next is move this line of code down the chain let's hit save again and we see the punctuation prompt let's move it down one more step and now we see the result of the llm and move it down to the end of the chain and we see the result of the string output passer and this is of course where we hit a problem because the string output passer is giving us a string and the grammar prompt which is what comes next in the chain is hoping for an object which looks like this punctuated sentence which is the input variable and then a string with whatever we got back from the string output passer in here so we could actually do that we could come in here and build the object so we've got the key punctuated uncore sentence and now for the value we can just take pretty much what we've got here although of course we're going to delete the console.log so we just return the previous result and look at that it works we're not actually seeing the final result yet but that's just because we need to chain in the llm and the string output passer let's hit save again and there we are now we've got our Center it is completely correct there's no grammatical errors and no punctuation errors so this has worked but I don't think it's very elegant and I don't think it's very intuitive so in the next Grim let's refactor this and we're going to find a neater way of doing it but let me just warn you before we finish this trick with the arrow function in a runnable sequence is actually very very useful in some situations and we are going to be using it in a challenge just a little bit later so don't forget it working but it's not the most elegant so let's refactor now one option is to go for a runnable sequence in a runnable sequence so I'll come right in here delete this and create a new runnable sequence right here and we do that in exactly the same way with the from method and now I just need to list these three inside that runnable sequence and of course delete them from here and let's just tidy up the formatting and actually that line of code is so long it's never going to look too good let's just hit save and see what happens so it does work it works perfectly well but likewise I think this is rather awkward so what I want to do is actually extract this chain so I'll create a const punctuation chain and that is going to hold this runnable sequence and now we can have whatever this chain returns be the value of this key value pair let's just bring this back onto one line okay let's give it a try and that works and by the way this could be a runable sequence or it could be a pipe chain it actually just doesn't matter at all you might find for these chains which are just a prompt an llm and an output passer that using the pipe method is just as good it makes the code slightly sh water and it's a matter of personal taste as to which one is easier to read now I'm going to leave this as a runnable sequence and I've just realized I've got an unnecessary trailing comma right there now I think this is going to look neater still if we also extract the grammar chain and I'll just hit save and there we are that works just as well Okay so we've got the runnable sequence working but we need to remember the last part of our app which is a transl ation and where we invoke this chain we are passing in a language French and we're not actually using that as yet in any of our templates but when it comes to having the translation template we are going to need it so I'm thinking I'll just come in here right now and I will add in that translation template and there we are we've got a translation template and a prompt which comes from it so what we'll be needing to do next is potentially give it its own chain and of course we need to add the translation chain to the main chain now do you think this is going to work well of course it's not we've been here before we're missing the input variable for language and that figures because we're not actually doing anything with language we're invoking it here but we haven't bought it into this chain and we certainly haven't passed it down the chain now strange as it may seem taking this original input and passing it down the chain is not a trivial thing to do and Lang chain actually give us a special tool to do just that so let's check that out in the next scrim slightly strange app we've added the translation chain so we should be able to get this translation of our original sentence now it's not working and we know why it doesn't have access to the input variables it wants it wants the language and grammatically correct sentence input variables now the grammatically correct sentence input variable should be coming straight from the grammar chain so we should be able to break this one into an object and add that really easily but what about this language input variable we know it's coming in here where we invoke the chain but to pass it down through the chain we actually need to use a special tool and that special tool is called a runnable pass through and we're going to import it from from exactly the same place that we get the runnable sequence so we can just add it in right here okay so we use it like this let's come to the first element in the chain and we've got this object and I'm just going to bring it down onto two lines and now I'll add a new key value pair and the key is going to be original input and the value is going to be a new instance of runnable pass through now let's come in here to this second object and again I'm going to bring it onto multiple lines and here I'm going to add a property original input and for the value we're going to use an arrow function just to log out what the original input is and just before we save that I'm just going to comment out this line and now we won't get confused by what we see in the console and we're seeing out errors but interestingly we're also logging out the original input and we can see what original input is in that object it's the sentence I don't liked Mondays with no punctuation or grammar and the language French so it's actually this object here that we passed in when we're invoking the chain so what we can do now then is destructure original input and now we're destructuring we need to use parentheses and then we should be able to access the language just with notation let's it save and see if that works and it does work we're getting the language French so let's just delete the console.log and will just return the language and of course we need to update the key here to language because that is the input variable that we're expecting in this template and let's hit save and see if it's worked oh and I think I forgot to reinstate this line which means we'll be waiting a long time let's try that again and it does work we are logging out our French sentence so we've successfully removed all of the errors and translated it into French and the really important thing here is that we've managed to pass an original input right down through the chain so we can get access to language in the final element of the chain now just before we move on I want to say a quick word about formatting it might well be that if you do use a runnable sequence here you think it looks better if we bring down the elements in these arrays onto their own lines and for me I think that probably does look a bit neater but that is a matter of personal taste now have a good look at what we've done here because everything that we've done here is aping something that we need to do in our app so when we go back to the app I'm going to give you a big Challenge and when you've completed that challenge the app should be fundamentally working bar one or two Loose Ends so have a good look at this code make sure you understand everything and then when you're ready move on we have studied everything we need to know to get this app to some kind of minimum viable product what I mean by that is by the end of the challenge we should be able to see an answer to this question down in the console and after I've shown you my solution we're going to wire up all of the Dom elements and then we've just got a few final features to add now I've imported the runnable pass through and the runnable sequence for you so you don't need to do that and let's just quickly check out the flow diagram to remind us where we are so we've basically done all of this and now we're going to be getting the answer using the nearest match and the user input we haven't wired up the conversation yet we're doing that last so I'm going to leave that crossed out you don't need to worry about it so your challenge is this I want you to set up a runnable sequence so the Standalone question prompt passes the Standalone question to the Retriever and the retriever passes the combined docs as context to the answer prompt remember the answer prompt should also have access to the original question when you finish the challenge you should see a conversational answer to our question in the console now I think the neatest way to do this is to create three chains one for each section so there'll be a standalone question chain a retriever chain and an answer chain and then you can bring them together in a runnable sequence but that said you might choose to do it a bit differently now I'm going to give you a choice here if you're up for a challenge you can go ahead and get to work but if you'd like a bit more guidance I've got a file up here called hints. MD and in here I've listed out the steps that I would take to get this working so you can refer to that if you need to whether you look at that file or not definitely feel free to go back to the previous scrims to check out the syntax as you do this and take as long as you need this can be pretty confusing at first but it really pays dividends to figure this out yourself either with or without the extra help in this file rather than just watching my solution so really give this Your Best Shot pause now get to work on this and then move on to the next Grim when you're ready to see my way of doing it and it's probably worth opening the next Grim in a new tab so you can keep your solution to hand for comparison purposes and massive congratulations if you did a few tricky bits to remember there now I'll show you how I did it and remember my way isn't the one true way if you got it working your way might be just as good or even better than mine so I'm going to come in here and set up three chains the Standalone question chain the retriever chain and the answer chain and then this here we will reuse as the main chain where we bring everything together on this first chain I'm just going to use the pipe method and I'm going to chain together the Standalone question prompt the llm and the string output passer now you could do this here with a runnable sequence it's totally up to you I've gone for the pipe method on these basic chains because it just keeps the code a little bit shorter now we could also bring these pipes onto new lines and that might make things a little bit neater still okay I want to add this first chain to our main chain so I'm just going to delete all of the code that we've got here and then the main chain is going to be a runnable sequence and then we'll pass in the Standalone question chain now let's just think for a second this chain will give us two things it will give us the Standalone question and the original question that we'll need in the answer prompt so let's create an object and we'll have the Standalone question and the value there will be whatever's provided by the Standalone question chain and we can have another property which is the original input and that will be a new instance of the the runnable pass through okay and onto the retriever chain and here we faced a bit of a gotcha the retriever chain isn't expecting an input variable but it needs the string we get from the Standalone question prompt so let's get at that using an arrow function and that means that this chain will need to be a runnable sequence so the first thing in this chain will be the arrow function that gets us the Standalone question so we'll take preve result as the parameter and we'll return pre result. Standalone question next in the chain we need the retriever itself and finally we want to extract the text from the array of objects we got back from the Retriever and we do that with the combined documents function so in this chain we're taking the previous result and we're extracting the Standalone question from it that's going to give us the string which we pass to the retriever the retriever finds the nearest matches and brings them back to us as an array of objects and the combined documents function extracts just the text from that array of objects and joins it into a string okay let's add the retriever chain to our main chain and what the retriever chain needs to pass on is the context because that is what the answer template is expecting as well as the original question so for the context that's coming here with the key as context the value is the retriever chain and then for the original question here we're going to take the original input that we've passed through with this runnable pass through and extract the question from it and now we just need to deal with the answer chain and the answer chain is going to be much like the Standalone question chain so we'll set that equals to the answer prompt and we'll pipe it into the llm and likewise we need a new instance of the string output passer okay that is the last link in the chain so let's just add it on here in the main chain and now I'm going to hit save and let's see if we get an answer and look we do it says scrimba is designed to be lightweight and can be used on low speec PCS so your old laptop should work just fine so what we're getting there is an answer to this question and that is really really good that is exactly what we want and the answer we're getting is accurate it's from information that we gave it in our original document and has been turned into a conversational answer by the llm so again congratulations if you got it to this point now the end is almost in sight in the next scrim I'm going to wire up the UI that's pure JavaScript there's no AI involved in that if you fancy it as a challenge go ahead and do it most of the code is done for you if not just feel free to sit back and watch on this one haven't really touched we've got this event listener which is listening out for a submit event which will fire when this button is clicked and that's going to call this progress conversation function now if we come right down to the bottom we can check out that function and all we need to do to get this working is to take this chain invocation right here cut it and paste it in here now I'm going to delete delete the console.log we don't need that and also we want to get rid of this hardwired question and we're going to invoke with whatever the user has inputed and we've got that stored in this Con right here question which is coming from user input. value and that is just being collected from this input on submit and that is all we need to do let's hit save and now I'm going to ask it a question and I'm going to ask it how long it will take to get a code review for a solo project oh and we're getting an error and it's saying result is not defined and I think what I've done here is I've called this one response and this one down here result well it doesn't really matter how we changed this let's change this one to response and I'll ask it the same question and there we are we get a great answer and that answer has come right from the document that we gave it at the beginning so that is really really good now I'm going to ask it if it knows about me Tom Chan is a scriber teacher that's reassuring it knows that now let's start again and I want to do an experiment I'm going to introduce myself and of course it doesn't know I'm a scrimer teacher we've just refreshed it doesn't know anything about me and in fact I might just make up a name so I've introduced myself and asked what is scrimber and there we are we get a great answer it gives us a lot of detail greets me by my name so it's being really conversational now I want to ask it what is my name and of course it doesn't know and the good thing is it's advising us to email help sca.com so it's got that straight from our prompt I think that was right there in the answer prompt here we are email help.com if you don't know the answer so that's working really well now the fact that it doesn't know my name when I gave it my name right here and it's repeated it once tells us what we already knew which is that this chatbot has no memory so the last thing that we need to do then is add some memory to this app and we've got all of this left on our flow diagram plenty of arrows going on so it looks really complicated but actually there's not much more to do but we'll come on to that in the next scrim some memory now there's two parts to this there's some JavaScript to set up and then we'll need to wire the conversation memory into the chain so that the answer prompt has got access to it now I'm going to cover the JavaScript setup in this scrim and you're going to have the job of wiring it into the chain in a challenge in the next scrim so firstly I want to create a const to hold the memory and this memory is going to be an array and I'm just going to call it conversation history so we'll come outside of the progress conversation function and this is where I'll set it up and I think I'll abbreviate it to com history now let's add to the conversation history every time a user submits a question and every time we get a response back from the chain so we'll come down here and we can say conf. push and we'll push the question and the response okay that deals with a couple of arrows on this flow diagram so the user input is now saved to the conversation history and the answer we get back from the chain is also added to the conversation history but at the moment all we're going to have stored in this array is a bunch of strings which is fine but it's going to help the AI understand this better if it's clear which strings come from humans and which come from AI so I'm going to write a function just to add a human or AI label to each string and then we're going to join them together into one big string which we can use in our chain now again in the interest of keeping index.js as clean and clear as we possibly can I'm going to create this function in utils so we'll say function format com history and it will take in an array of messages and then it's going to map over those messages now the Callback will have each message as a parameter and also the index and we're just going to work out whether it's a human or AI message by doing some basic maths on the index so what we're assuming here is that the human speaks first so the zeroth message will be a human and all of the odd number numbers will be Ai and all of the even numbers will be human now we just want to join them together into one string and let's just join them on a new line character now when we call the function on the conversation history what it would do is it will take the array of strings and convert it to this so there's one long string but it's really clear what the human said and what the AI said okay let's delete this and we'll export this function and import it back back into index.js right so the setup is done and in the next Grim you can wire this into the chain let's wire up this memory now at the moment on this diagram we need the conversation history right here where we get the final answer to our question but I think it might help performance if we have access to the conversation history where we're creating ating the Standalone question it might just help it create a better Standalone question in some Edge situations so I'm going to add another arrow to this diagram now we want our conversation history from our memory store in two places the Standalone question and the answer chain at the very end and that means it's time for a super challenge or a mediumsized super challenge it's not quite as big as the previous super challenge but there is quite a lot to do so I want you to pass history into the chain as comore history at the point where we invoke it and I've just put remember to make use of our format com history function now if we just go back to where we invoke the chain this is where you can pass in comcore history and I've just put the underscore in just to keep up with the same format that we've used throughout once you've passed that in and you've used the format com history function The Next Step will be to come to the Standalone question template and make use of con history in that template and all I mean by that is come here into the template we're going to add con history as an input variable somewhere and just update the instruction so it knows it can use the conversation history as a resource as well when you've done that we need to come to the answer chain and we need to make sure it has access to com history so again down in the chain you're going to need to do something around here once you've done that and the answer chain has got access to com history again you need to come to the answer template and use an input variable to add com history somewhere in here and instruct it on how to use it now if I were you I would just use wording in here that makes it clear that it can use the conversation history to help it find the best answer but that the source of knowledge Remains the context so I'll leave it up to you to think of some work wording there and then you can see what I did and of course in the real world you'd have to play around with this prompt template and just do some experimentation until you get results that you're happy with okay once you've done that we should be done so you can test it by giving the chatbot some information and checking in the next question to see if it remembers it so a really easy way of doing that is exactly what I did before tell it your name and see if it remembers your name okay quite a lot to do there so pause now give this your best shot and we'll have a look at it together together in just a moment okay hopefully you managed to do that just fine so let's come down to where we invoke the chain and I've already added the comore history property and the value there will be con history however before we send that off we want to format it so let's take our function format com history and we'll just pass in com history okay let's go back to the challenge so we've got com history in the chain and we've formatted it let's go to the Standalone question template to make use of com history right there and I'm going to convert this to back ticks and I'm just doing that because like with the answer template we're going to end up with a list of input variables so it's just a bit neater when it's back ticks and we can spread it out onto multiple lines now I'm going to update the text right here so I've just said given some conversation history if any and a question convert the question to a standalone question and then here we've got our requests for input and then here I'm going to bring things down onto a new line and we'll add the conversation history input variable so now we need to make sure the answer chain has access to com history and we need to edit the answer template to make use of it well that's actually two things in one let's come down to the chain and then right here we're using this runnable pass through to get access to the original question now we can do exactly the same thing with com history so let's copy that down onto a new line change this to com history and change question to comore history okay now let's come up to the answer template and I'm just going to make some changes here so I'm going to say you can answer a given question about scrimber based on the context provided did and the conversation history I'm going to leave this sentence intact try to find the answer in the context but I'm going to add in here if the answer is not given in the context find the answer in the conversation history if possible now I think that's going to work but again in the real world we would have to do some testing here try a few different ways of wording this and just see what got us the results that we wanted now let's come in here and we'll just add the com history okay it looks like we're done so so let's come back up to the challenge and it's just telling us to test it so let's hit save and I'll say my name's Tom what is scrimber and it's given us an answer to that question and now I'll ask it what my name is and it says your name is Tom so the memory is working and we have successfully created a chatbot knowledgeable about our document and with a memory and that is a pretty awesome thing to do when you remember how Unthinkable this would have been even just a couple of years years ago let's just refresh and ask it one more question and I'm going to ask it what is the scrimba community like and it says the scrimba community is a global community of friendly and helpful coders we believe in learning to code as a community activity so we've set up a Discord server where you can meet fellow coders share your problems and solutions and network and that is actually very true and you can come and hang out with us there so this project is done congratulations on making it through to the end and let's just take one more scrim to recap what we've learned well every question I've asked it I'm happy with the results but that won't always be the case so what can you do when performance is not up to standard well there are a few tweaks you can make firstly you could go right back to where we set up the original document and you could alter the chunk size we went for a chunk size of 500 characters you could go larger if you think your chatbot needs more context and you could go smaller if you think it needs more granularity you could also change the overlap size we went for an overlap of about 10% next you could look at the number of chunks which are actually retrieved and I did mention earlier that we would say something about that so let's quickly come over here to the utils folder and find our retriever now right here we've got this Vector store as Retriever and we we can pass a number in here and that will control how many chunks we get back from the vector store let's just see what's happening at the moment if we head down to our chain and in fact we want the retriever chain here we are so let's just come in here and I'm going to add in an arrow function and log out what this retriever is giving us now this is going to break the app but if we just open up the console and I'm going to ask any question now down in the console we can see that we've got four chunks let's come right back here to the Retriever and I'm just going to whack this up to 10 and I'll ask a question and there we are we get 10 chunks down in the console and we could of course take it down to as low as one and we get one chunk now I'm going to leave that empty so it defaults to four I think that is absolutely fine for this use case but do be aware of that setting because it can be pretty useful if you get more chunks you can provide more context but if you feel your answers are a bit vague and off topic you might want to reduce that number and just focus in on the best quality chunks because remember the retriever is going to give you the closest matches first okay let's just take that out of index.js because else it's going to cause US problems now you have got other options you can look to prompt engineering so just come here to where we've created our templates the answer template the Standalone question template and you can just think about how you can tighten those up I think Al promps are pretty good here but if you find you're not getting the performance you expect you can mix things up go into more detail add some examples and just experiment and see what you get now lastly you shouldn't forget the open AI settings again in index.js we've got the llm setup right here and what we could do of course is open this up and in here we can use any open AI settings we want so we've got for example temperature and when working with your own knowledge bot it might be a good idea to set that to zero remember the higher the temperature the more daring the AI gets and we don't particularly want daring here we're not trying to be creative so actually setting that to zero might be a pretty good idea now you could also change which model you're using GPT 4 GPT 3.5 any other new model that comes out and likewise here we've got access to frequency penalty and presence penalty and they might not be so relevant for this use case but they are there if you need to use them as well as any other setting that you can use with the open AI API so do be aware of that and you can always come in here and change things if you need to I'm of the opinion that if it's not broken you don't need to fix it so I'm actually going to leave this exactly as it was okay so those are five things that you can look at if you're not getting the performance you expect and so hopefully that will be useful if you do run into problems okay we're pretty much done with this so let's just take one more scrim to recap what we've studied ation on finishing this course you now have a really strong foothold in the world of Lang chain and you've got the skills you need to build powerful scalable AI applications quickly let's just recap what we studied firstly we used a text splitting tool from Lang chain to split our documents into chunks we created vectors for those chunks using an open AI embeddings model we then set up some templates and prompts and we can see examples of those right here when the prompts were ready to go we chained them to large language models and the string output passer using the pipe method and again we can see an example of that right here now those were the more basic chains and we came on to more complex chains a little bit later but first we took the user's input and vectorized it and then found the closest matches from the chunks in our Vector store we use those chunks to generate the final answer and we chained it all together using a runnable sequence which is a really cool way to build a more complex chain wow that was quite a lot so why don't you head over to our Discord Channel and go to the today I did Channel this slide is a link right through to that channel and brag to the world about what you've achieved and with that all that's left to say is thank you very much for taking this course do feel free to reach out to me on Twitter at TP chant it is always good to hear from you and always good to get feedback and till next time good luck
