With timestamps:

00:00 - In this project-based course, you will learn how to build AI-powered apps
00:04 - with the chat GPT, DALL-E, and GPT-4 APIs.
00:08 - Tom Chant developed this course.
00:10 - Tom is a front-end developer and course creator with Scrimba.
00:14 - Hey there free code campers and welcome to this interactive course
00:17 - where you are going to build mind-blowing AI-powered applications
00:22 - with features that didn't seem possible even just a few months ago.
00:26 - We'll be starting with the very basics so you don't need
00:30 - any previous knowledge of the OpenAI API.
00:33 - In fact, the only prerequisites for this course
00:36 - are basic vanilla JavaScript and some curiosity.
00:40 - Now the special thing about this course is that it's project-based
00:43 - and it comes with a ton of challenges and that means
00:46 - you're going to have your hands on the keyboard, writing code,
00:49 - and tackling challenges throughout.
00:51 - And if you're wondering how you can get your hands on the project code,
00:54 - don't worry, we've got you covered.
00:56 - In the interactive version of this course over on Scrimba,
01:00 - you can pause the video, edit the code, and run the projects
01:04 - right there in your browser.
01:06 - Or if you prefer, you can download the code from those scrims
01:09 - and run the projects locally.
01:11 - The link is right down there in the description.
01:14 - Oh, one last thing before we start.
01:16 - If you enjoyed this course, please hit that thumbs up
01:19 - right here on YouTube.
01:20 - That will be much appreciated.
01:22 - And if you'd like to get in touch, reach out to me on Twitter.
01:26 - I am @tpchant.
01:28 - OK, let's get started.
01:35 - Welcome to building AI apps with chat GPT, DALL-E, and GPT-4.
01:41 - I am super excited to bring you this course
01:44 - on the technology everybody is talking about.
01:48 - We are going to study how to use the OpenAI API.
01:52 - We'll be using various OpenAI models,
01:54 - including the latest GPT-4 model.
01:57 - We'll be looking at prompt engineering.
01:59 - We'll be building chat bots.
02:01 - And we'll be fine tuning a model on our own data.
02:05 - And along the way, of course, there will be lots more
02:08 - and plenty of challenges.
02:10 - Now, I need to say a quick word about GPT-4.
02:13 - We'll be using the GPT-4 API in the second project in this course.
02:18 - Now, at present, there is a waiting list
02:20 - to get your hands on the API.
02:23 - No doubt in the near future, the waitlist will go
02:26 - and the API will be available to everybody.
02:28 - But for now, while the waitlist exists,
02:31 - why not click on this screenshot, which will take you through to the signup page
02:36 - and you can join the waiting list there if it still exists.
02:40 - OK, so what are we going to build?
02:42 - We'll start off by exploiting the power of OpenAI
02:45 - to be creative and generate human-standard words and images
02:49 - to build this cool movie pitch app,
02:52 - which turns a one-sentence idea into a full movie outline.
02:57 - Then we'll use the GPT-4 model to create an Ask Me Anything chat bot
03:02 - called Know It All.
03:03 - And we'll use a Google Firebase database
03:06 - so the user can persist the conversation they have with the chat bot
03:09 - and pick it up at a later date after refreshing the browser.
03:13 - Lastly, we're going to stay with the chat bot concept.
03:16 - But under the hood, everything will be different.
03:19 - We will enter the world of fine-tuning.
03:21 - This is where we upload our own dataset
03:24 - and we create a chat bot that can answer specific questions
03:27 - from our own data.
03:30 - This skill is really essential
03:32 - if you want to use a chat bot for a specific purpose,
03:35 - such as to handle customer service issues with your company.
03:38 - And with this project, we'll also learn a really neat skill
03:41 - for whenever you're working with APIs with secret keys.
03:45 - We'll look at deploying the site with Netlify with the API key hidden
03:50 - so you can share your project without fear of the API key being compromised.
03:55 - This solves the really big problem that we have
03:57 - when we're using APIs with secret keys in front-end projects.
04:02 - Now, there are not too many prerequisites for this course.
04:05 - We're going to be working in vanilla JavaScript
04:07 - and I'll assume you have a reasonable knowledge of it.
04:09 - Now, the JavaScript in this isn't very complicated for the most part.
04:13 - As long as you understand the basics of a fetch request,
04:16 - you'll be absolutely fine.
04:18 - And if you're rusty on fetch requests, don't worry at all.
04:21 - We'll actually go through it all step by step
04:23 - and I don't think you'll have a problem.
04:25 - Apart from that, we'll be focusing fully on the AI
04:28 - and working through it stage by stage.
04:31 - Who am I?
04:32 - My name is Tom Chant and I'll be your tutor for this course.
04:36 - You can find me on Twitter.
04:37 - My handle is at tpchant
04:40 - and it's always good to hear how you got on with the course.
04:42 - Now, before you get started,
04:44 - why don't you head over to Scrimba's Discord server
04:47 - and go to the Today I Will channel
04:49 - and let the community know that you're starting this course.
04:52 - Studying is more fun and more productive when it's done together.
04:56 - So, why not interact with fellow students on the Discord community,
05:00 - encourage each other and help each other along.
05:02 - And then, without further ado, let's jump straight in to the first project.
05:07 - In 2006, Terry Rocio and Bill Marcille
05:11 - sold the rights to the Denzel Washington film Deja Vu
05:15 - for a record-breaking $5 million
05:18 - and it went on to gross more than $180 million worldwide.
05:23 - Have you ever dreamed of making it big in the movies,
05:26 - coming up with that one idea that will make you rich
05:30 - beyond your wildest dreams?
05:32 - Well, the most important thing is to be aware
05:35 - of your wildest dreams.
05:37 - Well, the most successful projects start with one single idea,
05:41 - but once you've had that idea, you need to work with it,
05:44 - brainstorm around it, spend hour upon hour working late into the night
05:49 - to forge it into something marketable.
05:52 - Or do you?
05:53 - What if you could take one simple idea, just a single sentence,
05:56 - and let the power of OpenAI's large language model
06:00 - do the rest of the work for you?
06:02 - What used to be science fiction is now science fact.
06:06 - This is movie pitch, and on the outside, it is so simple.
06:11 - We put a one-sentence idea for a movie in here,
06:14 - we click send, and that is all we have to do.
06:18 - We sit back and we wait for maybe 10 seconds or 15 seconds,
06:22 - and then what we get back when OpenAI has done its thing
06:26 - is the makings of a great movie.
06:28 - We get some artwork for the cover, a great title,
06:32 - and a list of stars, and most importantly of all,
06:35 - we get the synopsis that brings the idea to life,
06:38 - and all of this is brought to us by the power of OpenAI.
06:42 - Now, to get this app working, we have got some work to do.
06:45 - So what will we be studying?
06:47 - Well, we will be using the OpenAI API.
06:51 - We'll be working with models and understanding what tokens are.
06:55 - We'll learn about crafting prompts and how to tweak those prompts
06:58 - to get results and how we can use examples to train the model.
07:02 - We'll be looking at extracting information from texts
07:06 - and also generating images just with words,
07:10 - and by the end of all that, we'll have a great foundation
07:13 - in how to use OpenAI in front-end projects.
07:16 - Now, I want to give you a warning.
07:18 - As we build this project, the API key for OpenAI
07:22 - will be sat there on the front end, and that means it's visible.
07:25 - Anybody could go in through DevTools to the Network tab
07:28 - and they could steal your API key.
07:31 - Now, while you're developing locally, that is fine,
07:33 - but don't share your project with an API key
07:37 - or publish it to GitHub without ignoring the API key
07:40 - because that will compromise the API key.
07:43 - Now, later in this course, but not in this project,
07:46 - we're going to look at how we can deploy our apps
07:48 - with the API key safely stored on a server
07:51 - where no one else can see it.
07:53 - But in the meantime, just be mindful of what you're doing
07:55 - with your API key.
07:57 - OK, so when you're ready, let's jump into the code
07:59 - and check out the HTML and CSS that I've got waiting for you.
08:09 - So let's have a look at the code that we've already got,
08:12 - and then we'll get straight on to the AI.
08:14 - So here in index.html, there's nothing particularly strange
08:18 - or unusual going on.
08:19 - I've bought in a couple of Google fonts.
08:21 - We've got Playfair and Poppins.
08:24 - And then down in the body, we've got a header section
08:27 - which has just got the movie pitch logo
08:29 - that you can see right here.
08:30 - And then underneath that, we've got a main,
08:33 - and that is divided up into two sections.
08:36 - This first section here is all about the setup,
08:40 - and that is everything that you can see right here.
08:43 - So this is where we collect the user's idea
08:46 - with this text area, and we interact with the movie boss.
08:50 - And you can see that we've got his speech bubble right here
08:52 - with this rather braggadocious statement
08:55 - that we've actually got hard coded right here in the HTML.
08:59 - Now, the second section that we've got down here,
09:02 - this is dedicated to the output.
09:03 - This is where we'll actually display the finished product
09:06 - that the AI creates for us.
09:08 - Now, we can't see that section in the mini browser at the moment,
09:11 - because if we have a look at the CSS,
09:13 - it's actually set to display none.
09:15 - So we'll bring that into view when we're ready for it.
09:19 - Now, sticking with the CSS, we've got a bunch of styles in here.
09:22 - It's really unlikely that there's anything particularly new or strange,
09:26 - but by all means, feel free to pause and familiarize yourself with it.
09:29 - To be honest, we won't be referring to it that much as the course goes on.
09:33 - We'll just touch on it now and again as we need to.
09:36 - Now, over in index.js, we have already got a little bit of JavaScript.
09:41 - I've taken control of the three elements
09:45 - that we're actually working with already.
09:47 - So we've got the text area, which we've already seen down here.
09:51 - The setup input container, that is the container for the text area and the button.
09:56 - And then we've also taken control of the movie boss text,
10:00 - and that is what you can see right here inside this speech bubble.
10:03 - Now, we've got an event listener and it's listening out for clicks on this button.
10:07 - And when it detects a click, it does a couple of things.
10:11 - Firstly, it checks if there's anything inside the text area right here.
10:16 - If there isn't, it does nothing.
10:18 - If there is, it will replace this text area and this button with a loading GIF.
10:24 - And we're just bringing that in right here from the images folder.
10:29 - And also, it will update the speech bubble with some new text.
10:33 - This is actually hard coded into the JavaScript.
10:36 - And that is actually the last bit of human written text
10:39 - that you're going to see for a while.
10:41 - From this point onwards, we'll mostly be working with AI generated text.
10:46 - Let's just put something in that text area and check it's working.
10:49 - So there's my one sentence idea for a movie.
10:52 - An evil genius wants to take over the world with AI,
10:55 - and only one brave mouse can stop him.
10:58 - Let's hit send.
10:59 - And as soon as we do that, we get this loading SVG
11:02 - just to communicate to the user that something is happening.
11:05 - And we've updated the speech bubble.
11:08 - Just wait a second while my digital brain digests that.
11:11 - Okay, so everything is working as we want it to.
11:14 - And the next thing we need to do is start applying some AI to this.
11:18 - We want to actually get an AI generated response to our idea.
11:22 - But before we take the first step into this mystical and magical world of AI,
11:26 - we need to talk about some more mundane matters,
11:29 - like how we can actually access the open AI API from within our app.
11:33 - That's how we'll get our hands on these powers.
11:37 - So let's come on to that next.
11:38 - Next, let's get our hands on an open AI API key.
11:47 - And that means signing up.
11:49 - So why don't you head over to the open AI homepage?
11:53 - And this slide is actually a clickable link.
11:55 - So if you just click on this screenshot, it will take you straight there.
11:59 - Now, when you're there, just pause for a moment
12:01 - to check out some of these really beautiful images
12:04 - that they've created with their Dali image generation model.
12:07 - And we will be looking at image generation later in this course.
12:11 - Now, once you've feasted your eyes on the pictures,
12:13 - head over to where it says API.
12:15 - And from there, you'll need to go to sign up.
12:18 - And then you can choose your sign up method,
12:20 - and you'll need to confirm with a phone number.
12:23 - Now, once you're in from the dashboard,
12:25 - you can click your avatar up here and select View API Keys.
12:30 - Now, they only show you the API key once when it's first generated.
12:34 - After that, it will be obscured like this.
12:37 - So be sure to copy and paste it somewhere safe as soon as you get it.
12:41 - But if you lose it, don't worry.
12:42 - From here, you can actually delete it and get a new one.
12:46 - And like all API keys, be sure to keep it secret.
12:49 - In this project we're building, you will see my API key.
12:52 - But rest assured that by the time this recording goes public,
12:55 - I will have deleted it.
12:57 - OK, so that's the API key.
12:59 - Now, while we're here, let's just say a quick word about credit.
13:03 - If we click on Usage here, it will take us through to a page
13:07 - where we can see how much credit we've got remaining.
13:11 - Now, at the time of recording, when you sign up,
13:13 - you get some free credit to play with.
13:16 - I got $18 of credit, which was valid for three months.
13:19 - And it actually looks like I'm nearly through that.
13:22 - But I have been a heavy user.
13:24 - So you'll probably find that the free credit you get
13:27 - will get you a very long way.
13:29 - Now, when that credit has expired or been used up,
13:32 - it is a pay as you go model.
13:34 - Check the website for the latest info on that.
13:38 - OK, now let's take our API key and build our first request.
13:43 - When you're ready for that, let's move on.
13:48 - Now we've got our API key, we can start using the API.
13:52 - But before we can make our first fetch request,
13:54 - we need an API endpoint.
13:57 - Let's head back to the OpenAI website
13:59 - and click through to the docs.
14:01 - And this slide is a clickable link, of course,
14:04 - and it will take you straight there.
14:06 - Now, the OpenAI docs are pretty comprehensive.
14:08 - And right here on the first page,
14:11 - it tells us that the completions endpoint
14:13 - is at the center of our API.
14:15 - OK, that sounds interesting, the completions endpoint.
14:19 - We're going to check it out.
14:20 - But before we do that, you might well
14:22 - be asking, what is a completion?
14:25 - And completion is a word used a lot in OpenAI.
14:28 - So let's say you want some information,
14:30 - like you want to know who the first person to walk
14:32 - on the moon was.
14:33 - Let's send that question to OpenAI via the API,
14:37 - and it will think about it and send back a completion.
14:40 - And that completion is, of course, Neil Armstrong.
14:44 - So in the world of OpenAI, a completion
14:46 - is a response from the API that fulfills your request.
14:50 - OK, let's click through to the completions endpoint section
14:54 - in the docs.
14:55 - And here we get a ton of info and some code examples.
14:59 - And each example is given in node.js, PHP, and curl.
15:03 - But what I want to focus on is this.
15:06 - We have the completions API endpoint right here,
15:09 - and it tells us we can use the POST method.
15:12 - So now we've got that info.
15:14 - Let's come back to index.js, and I'm
15:17 - going to save it in a const called URL.
15:19 - And in the next scrim, let's start writing a fetch request.
15:30 - Let's start this API call by laying out
15:32 - the bare bones of the fetch request.
15:35 - So we need fetch, and then we open up the brackets,
15:38 - and we pass in the completions endpoint,
15:40 - and we've got that saved here as URL.
15:43 - Now we need an object, and inside this object,
15:45 - we're going to need method, headers, and body.
15:48 - And if we check back with the docs,
15:50 - we know that the method is POST.
15:52 - It tells us that right here next to the endpoint.
15:57 - Now the docs don't actually give us an example fetch request
16:00 - in JavaScript, but they do give us this curl example,
16:04 - and we can adapt it.
16:05 - So let's take a closer look.
16:07 - Our headers are going to need a content type of application
16:10 - JSON, and we'll also need an authorization of bearer
16:13 - with our API key.
16:15 - So let's go ahead and put those in.
16:24 - OK.
16:24 - Now we need a body, and this is where
16:26 - we get into open AI specifics.
16:28 - The body for an open AI request needs a model and a prompt.
16:34 - So I'll come down here and add the body,
16:36 - and then I'm going to say JSON.stringify,
16:40 - and I'll pass in an object with model and prompt.
16:46 - Now if we look back at this screenshot,
16:47 - it actually gives us a model right here, text-davinci-003.
16:53 - So let's break the golden rule of coding
16:55 - and just copy a line of code that we don't actually
16:58 - understand.
16:59 - We haven't discussed what open AI models actually are
17:02 - or what options we have available to us yet,
17:05 - but what I want to do is get this example working,
17:08 - get the first API request actually into our app,
17:11 - and then we'll rewind and talk about models in more detail.
17:14 - For now, just know that open AI has got various models
17:18 - that we can use, and today we're going
17:20 - to use this one called text-davinci-003.
17:23 - OK.
17:24 - So I'll put that in a string right here.
17:27 - OK.
17:28 - So we've got the model, and now we need the prompt,
17:30 - and we're going to talk about prompts a lot in this course.
17:33 - But for now, what we need to know is this.
17:36 - In this example that we saw in the previous scrim,
17:40 - we asked a question.
17:41 - Who was the first person to walk on the moon?
17:43 - And we got the completion, Neil Armstrong.
17:47 - Well, the prompt is this part.
17:49 - Who was the first person to walk on the moon?
17:51 - The prompt is whatever we ask the open AI API for.
17:55 - Now, sometimes prompts are really, really simple just
17:58 - like this, and sometimes they're really very complex,
18:00 - as you'll see a bit later in this course.
18:03 - Now, they've given us an example prompt in the docs.
18:06 - Say this is a test, which, to be honest, is not very creative.
18:10 - So instead of using that, I'm going
18:12 - to ask an easy but real world question for this prompt.
18:15 - I'm just going to say, what is the capital of Spain?
18:20 - And that is all we need to do for a simple prompt.
18:23 - We just put it in a string.
18:25 - OK.
18:26 - Now we need to change some thens.
18:28 - So we'll take the response, and we'll call Jason on it.
18:33 - And then we'll take the data from that response
18:35 - and log it out.
18:38 - OK, let's open up the console, and I'll hit Save.
18:41 - And we've run into an error.
18:42 - It's a pretty silly one.
18:43 - It's just a typo on my part.
18:45 - But why don't you just pause now and see if you can debug that?
18:49 - And I'll give you a bit of a clue.
18:50 - Have a quick look at this slide.
18:55 - OK, did you spot it?
18:57 - I've actually put bearer colon and then the API key.
19:01 - I think that is causing the problem.
19:03 - Let's try it one more time.
19:05 - And there we are.
19:05 - We're getting a response.
19:07 - So I'm just going to copy this from the console
19:09 - and paste it in the editor, just so we
19:11 - can see it a little bit more easily.
19:14 - And as we can see, this is an object.
19:15 - It's got loads of data in it.
19:17 - And we'll be talking more about some of the properties
19:19 - in this object as we go along.
19:21 - But for now, we just need to focus on one word, Madrid.
19:25 - Because that shows us it works.
19:27 - OpenAI is speaking to us, and it knows all about Spain.
19:31 - Now, OK, knowing facts is great.
19:33 - But we're actually interested in emotion and creativity.
19:36 - So I'm actually just going to change things a little bit.
19:39 - Let's delete this.
19:40 - And I'm going to change my prompt
19:42 - to say something more emotional.
19:47 - I've said sound sympathetic in five words or less.
19:51 - Let's hit Save.
19:53 - And again, I'll just paste the response right here.
19:56 - And look what we've got.
19:57 - I'm here for you.
19:58 - It's giving us sympathy.
20:00 - And what's really cool about that
20:02 - is that now the AI is speaking to us as if it were human.
20:06 - It's speaking with heartfelt, or at least CPU felt, emotion.
20:11 - And that is what we're looking for.
20:13 - OK, let's take what we've got here
20:15 - and apply it to our project.
20:17 - And I haven't forgotten this model
20:20 - that we've got here, text da Vinci 003.
20:23 - It's still shrouded in mystery.
20:24 - But I want you to get your hands on the keyboard,
20:26 - start getting practice, start building muscle memory
20:29 - before I do any more talking.
20:31 - So let's move on to a challenge with this next.
20:39 - OK, so back in our app, we've got an event listener right
20:42 - here.
20:43 - And it's listening out for clicks on this button.
20:46 - And what it does is it checks to see if the user has given us
20:50 - any input.
20:51 - It displays a loading message and updates the text
20:55 - that we see in the speech bubble right here.
20:58 - Now, we're not ready to work with any user inputted text yet.
21:02 - So I'm actually just going to comment out this if.
21:05 - And if I save that now, this code
21:07 - should run as soon as we click this button.
21:10 - And there we are.
21:11 - We get our loader.
21:12 - And we've updated the text that we've got in the speech bubble.
21:16 - So let's go ahead and make an API call
21:18 - to get an enthusiastic response that we can pass to our user.
21:22 - And of course, later, we'll refactor this
21:24 - so the AI is actually responding to the specific idea
21:28 - that the user gave us.
21:29 - But for now, the response we get back from the AI
21:32 - is just going to be generic.
21:34 - So I'm going to call a function in here called fetch bot reply.
21:39 - And then I'll come down here and I'll
21:41 - do the donkey work of setting up the skeleton of this function.
21:44 - And now it's over to you.
21:46 - Here's a challenge to finish off this function.
21:49 - And I'm just going to paste it right here inside the function
21:51 - body because that is where I want you to write code.
21:55 - So your challenge is this.
21:57 - I want you to make a fetch request to the OpenAI API.
22:02 - And I've put all of the details that you need up here.
22:06 - So you'll have to put your own API key in here.
22:08 - And this is the URL to the endpoint.
22:12 - Now the prompt should request an enthusiastic response
22:15 - in no more than five words.
22:17 - And you know how to do that because in the previous scrim
22:19 - we did exactly the same thing to get a sympathetic response.
22:23 - For now, you can just log out the completion
22:26 - to check it's working.
22:27 - Now before you start, there are two things I want to say.
22:30 - The first is, if what you get from the completion
22:32 - isn't quite what you wanted or what you expected,
22:35 - don't worry.
22:36 - We're going to talk a lot more about using prompts
22:38 - and troubleshooting the issues that you can have.
22:41 - The purpose of this challenge is just to get the syntax right
22:44 - so we are actually getting a completion back from OpenAI.
22:48 - Secondly, by all means, go back to the previous scrims
22:51 - just to have a look at the syntax that we need.
22:54 - But don't copy and paste.
22:56 - You're going to do yourself a lot of favors
22:58 - by writing this out by hand.
22:59 - All of that practice just builds muscle memory
23:02 - and builds up your fluency.
23:03 - OK, pause now, get this challenge sorted,
23:06 - and I'll see you back here in just a minute.
23:14 - OK, so hopefully you managed to do that just fine.
23:18 - So what we need to do then is come in here
23:20 - and set up a fetch request.
23:22 - And we'll pass in the URL that we've got stored right here.
23:26 - And then we'll open up the object.
23:28 - And we know that the method is post.
23:31 - And then we need the headers.
23:33 - And that will be an object with two key value pairs.
23:36 - The first is the content type, which will be application Jason.
23:41 - And the second will be authorization.
23:43 - And that will be a string with the word bearer and our API
23:47 - key.
23:50 - Then we need the body.
23:51 - And the body is going to need a model and a prompt.
23:55 - So firstly, we say Jason.stringify.
23:59 - And then we pass in an object with model text DaVinci003.
24:04 - And prompt.
24:06 - And for my prompt, I'm going to say sound enthusiastic in five
24:10 - words or less.
24:11 - Because that is what we want the movie boss to do.
24:13 - We want him to sound enthusiastic.
24:17 - OK, let's change some VENs so we can deal with the response.
24:22 - So we call Jason on the response.
24:24 - And then we'll take the data from that response
24:26 - and log it out.
24:29 - OK, let's hit Save.
24:30 - And to fire up this function, all we need to do
24:33 - is hit the Send button.
24:36 - And we've got the loading SVG.
24:37 - And if I open up the console, there we are.
24:40 - It looks like we've got our completion.
24:42 - Let's just copy and paste that into the editor.
24:47 - And we can see we've got these two words right
24:49 - here excitedly enthusiastic.
24:52 - So it's taken our prompt very, very literally.
24:55 - Now the next thing that I want to do
24:57 - is actually get this completion to appear
24:59 - inside the speech bubble.
25:01 - So I'm just going to delete this challenge text
25:03 - so we've got a little bit more room to work with.
25:07 - Now we've already taken control of the text in the speech
25:10 - bubble.
25:11 - We've got the elements stored up here in this const movie
25:14 - boss text.
25:14 - So let's come down here, and instead of logging something
25:18 - out, I'm going to come onto a new line.
25:22 - And I'll say movie boss dot text dot inner text.
25:27 - And now we need to access our completion.
25:30 - And we've got that right here.
25:32 - So to get that, we need to say data dot choices.
25:36 - And choices holds an array.
25:39 - And we actually want what is in the first element of the array
25:42 - at position 0.
25:44 - And then we want to access the text property.
25:47 - OK, let's hit Save.
25:48 - And again, I'm just going to hit this button.
25:51 - And there we are.
25:52 - It says eager and excited in the speech bubble.
25:56 - Now we've got a little bit of a problem
25:57 - in that our design is kind of broken
25:59 - because the CSS can't really cope with such a short sentence.
26:03 - Or that's what it seems at the moment.
26:05 - Now I'm not too worried about that
26:07 - because this sentence is not going to stay short.
26:10 - Soon we're actually going to refactor this function.
26:13 - And what we'll get will be a longer completion, which
26:16 - is actually going to be relevant to the one line input
26:19 - that the user puts right here.
26:21 - But before we come on to that, there
26:23 - are one or two things that I want to cover.
26:25 - Firstly, these fetch requests are unnecessarily complex
26:29 - and long.
26:30 - So instead of writing out loads of these in our app,
26:33 - we can actually use open AI's dependency.
26:36 - And that is going to save us a lot of work
26:39 - as we move forwards.
26:40 - But before we do that, we need to talk about models
26:43 - because we have used text DaVinci 003
26:46 - and we haven't really got a handle on what that means yet.
26:49 - So let's come on to that next.
26:57 - So in the last two scrims, we have used text DaVinci 003
27:00 - as our model.
27:02 - So now let's ask the question, what is an AI model?
27:06 - Well, loosely speaking, an AI model
27:08 - is an algorithm that uses training data to recognize
27:11 - patterns and make predictions or decisions.
27:14 - Now, open AI has got various models
27:17 - geared towards different tasks.
27:19 - And some models are newer and therefore better than others.
27:22 - At the moment, there are two main categories of models
27:25 - that you might come across.
27:26 - There's the GPT-3, GPT-3.5, and GPT-4 models.
27:30 - And GPT-4 is actually just coming out right now.
27:33 - It's currently in beta.
27:35 - And these models are all about understanding and generating
27:38 - natural language.
27:39 - And they can also generate computer languages as well.
27:42 - Now, there's also the Codex models.
27:44 - These models are specifically designed
27:46 - to generate computer code, including
27:49 - translating natural language to computer code and vice versa.
27:52 - And you've probably seen examples of that online,
27:55 - even if you haven't tried it yet yourself.
27:57 - Open AI also has a model that filters content
28:00 - to remove or flag unsafe or sensitive text.
28:04 - In this project, we'll be using the text DaVinci 003 model,
28:08 - which is a GPT-3.5 model.
28:10 - Now, this is one of the newest models.
28:12 - It can provide long text output.
28:14 - And it's great of following instructions.
28:16 - There's GPT-4, which is fresh out.
28:19 - And we will be coming to that later in this course.
28:22 - There's the text Curie 001 model.
28:25 - And this is a very capable model.
28:27 - It's actually faster than text DaVinci 003.
28:31 - But it's not as good overall in terms
28:33 - of the language it creates.
28:35 - There's text Babbage 001, which is great for straightforward
28:39 - tasks and is also very, very fast.
28:41 - And then there's text Ada 001.
28:44 - And that is fine for basic tasks.
28:45 - And it's fast and cheap.
28:47 - Now, these models are in age order.
28:50 - So text DaVinci 003 is the newest on this list.
28:53 - Text Ada 001 is the oldest.
28:56 - And the older the models get, the less complex they are.
28:59 - So they run faster.
29:00 - And generally speaking, they're cheaper.
29:01 - But you should check the open AI docs for the latest prices.
29:06 - Now, the Curie, Babbage, and Ada models
29:08 - all provide shorter outputs than text DaVinci 003.
29:11 - But they might well still be capable of some or even
29:14 - all of the tasks you might want to do in a project.
29:16 - It depends what it is you want to achieve.
29:19 - But just remember, with the older models being cheaper,
29:21 - that means you can scale apps without incurring too
29:24 - many costs.
29:25 - And because they're faster, you'll
29:26 - experience less of the horrible laggy UX.
29:29 - You can sometimes get when you're working with any API,
29:32 - but particularly with artificial intelligence,
29:34 - as it obviously has to do so much computation.
29:37 - Now, so far, our app hasn't been too laggy.
29:40 - We've just been generating quite short, quick completions.
29:43 - But as our requests get more complex,
29:45 - we will see lag times increase.
29:48 - And now, you might well be asking, what model
29:50 - should I use in my project?
29:52 - Well, open AI's advice is this.
29:55 - Start with the best available model
29:57 - and downgrade to save time and costs where possible.
30:00 - So basically, get your app working
30:02 - so you're happy with its performance,
30:04 - and then experiment with cheaper models
30:06 - to see if you can get the same level of results.
30:09 - Now, as new models come out, prices will change.
30:12 - And probably, performance will increase on those new models,
30:15 - so you will have to do some experimentation.
30:17 - Now, to help you with that, in the next grim,
30:19 - I want to introduce you to a couple of really useful tools.
30:22 - And one of those tools will help you select the best model
30:25 - when you come to build your own projects.
30:28 - Let's move on to that next.
30:34 - I want to show you a couple of really useful tools that
30:37 - can help us work with open AI.
30:39 - The first one is particularly useful for model selection.
30:43 - This screenshot is from gpttools.com,
30:46 - and this is their prompt compare tool.
30:49 - And of course, this slide is a clickable link, which
30:51 - will take you through to their site.
30:53 - Now, to use this tool, you put your API key in right here,
30:57 - and then it allows you to set up two API
31:00 - calls side by side using different models.
31:04 - So let's just ask OpenAI for a description of Pablo Picasso's
31:09 - artistic style.
31:10 - And there we are.
31:11 - I've put the same prompt on each side.
31:13 - Now, for the first one, I'll use the model, or engine,
31:16 - as they call it here, of text DaVinci003,
31:19 - which is the same as we're using in our app.
31:21 - And for the second one, I'll use text Curie001,
31:25 - which is one of the older models.
31:27 - Now, down here underneath, we've got plenty of other settings,
31:30 - but I'm going to leave them all at their defaults.
31:32 - It will be the same on both sides.
31:34 - So now we can submit and see the results.
31:37 - The first one to come back is the Curie,
31:39 - which figures the older, simpler models are faster.
31:42 - Then we get back DaVinci.
31:44 - The big thing that we notice is that the texts are vastly
31:48 - differing in length.
31:49 - The newer model, the DaVinci, gives us way more words,
31:53 - though it was slower.
31:55 - And we can actually see the speed difference right here.
31:57 - This one took 5.8 seconds, and this one took just 1.1 seconds.
32:03 - Now, in our project, we are prioritizing language creation
32:06 - ability over speed and cost.
32:08 - So we're going to stick with text DaVinci003,
32:11 - but it's good to know for future reference
32:13 - that you can do some experimentation with this tool
32:16 - when selecting a model to use.
32:18 - Although this answer is short, the language it provides
32:21 - is actually perfect human standard English.
32:24 - So remember, those older models are not just there for legacy.
32:28 - They are actually really useful.
32:30 - Now, the second tool I want to show you
32:32 - is back on OpenAI's website, and it is the OpenAI Playground.
32:37 - And again, this slide is a clickable link.
32:40 - Now, the Playground is a really, really cool tool.
32:43 - You can select from some pre-written examples,
32:45 - or you can just come in here and write your own prompts.
32:49 - Let's ask about Picasso again.
32:51 - And when we click Submit, we can actually
32:53 - see the results we get highlighted in green.
32:56 - And this will allow you to practice
32:57 - with different models.
32:59 - Again, you can change it right here,
33:00 - but of course, you won't be seeing the results side by side.
33:04 - It also does allow you to play with the various other
33:06 - settings, some of which we'll be looking
33:08 - at later in this course.
33:09 - But what I think is the most useful thing here
33:12 - is that any time you can come up here
33:14 - to where it says View Code and get a code snippet,
33:18 - including the prompt you're generating right now,
33:21 - it will actually give you that prompt in either Node.js,
33:24 - Python, or curl.
33:26 - Now, I've selected the Node.js snippet right here.
33:29 - If we look carefully, we can see that it's not quite the same
33:32 - as the fetch request that we've already written.
33:35 - If we look at this first line, it's
33:36 - using the require keyword to work with a dependency.
33:40 - So at the moment, we couldn't just cut and paste this code
33:42 - as it is, but we could use lots of it in our JS.
33:46 - We've got, for example, the prompt right here.
33:49 - But look, the neatness and compactness
33:51 - of the API call being made here is definitely
33:54 - making me think that it's time to move away
33:56 - from the standard fetch request we've already written
33:59 - and move towards using the OpenAI dependency.
34:03 - In the long run, this is going to save us time
34:05 - and allow us to do less work.
34:07 - In the next grim, let's go back to the app
34:09 - and refactor our fetch request to use the OpenAI dependency.
34:13 - When you're ready for that, I'll see you there.
34:19 - So far, we've been using a fetch request to make API calls.
34:23 - But now, we're going to neaten things up a bit
34:25 - by switching to the OpenAI dependency.
34:28 - And this is actually going to save us time
34:30 - and allow us to write less code as the project progresses.
34:34 - Now, if we look back at the docs,
34:36 - we saw this code snippet for Node.js.
34:38 - And we can actually get a broad idea from this
34:41 - as to what we want to achieve.
34:43 - Let's just zoom in a little bit.
34:44 - OK, so we're going to need to get our hands
34:46 - on the configuration and OpenAI API constructors
34:50 - from the OpenAI dependency.
34:53 - And we'll actually be using the import keyword,
34:55 - not requiring them, because obviously, we're not
34:58 - working in Node.js.
35:00 - Also, like they're doing here, we
35:02 - are going to store our API key in a separate file.
35:05 - And we'll talk about that more in a moment.
35:07 - And then we'll use the two constructors
35:09 - that we're bringing in from the OpenAI dependency.
35:12 - And then we can update and simplify our fetch request.
35:15 - So firstly, let's come over here.
35:17 - And I'm going to set up a new file called mv.js.
35:21 - Now, you can't actually see me do this.
35:23 - But when you hover your cursor here, three dots appear.
35:26 - When you click on them, you get a menu.
35:28 - And you can select New File.
35:30 - I'm going to call this file mv.js.
35:33 - And there we are.
35:34 - You can see that file has now appeared.
35:37 - Now remember, because we're working on the front end,
35:39 - this file is totally visible.
35:42 - If you were taking this to production,
35:43 - this file would need to be server-side.
35:46 - And of course, you would also make sure you were ignoring it
35:48 - when pushing it to GitHub.
35:50 - But what we're doing today is purely on the front end.
35:53 - And this is absolutely fine while we're developing and running
35:56 - things locally.
35:57 - And as I said, towards the end of the course,
35:59 - we will look at how you can deploy your projects
36:01 - with your API key safely hidden.
36:04 - And then you'll be able to share your work
36:06 - and use it in your portfolios.
36:08 - OK, so in this mv file, I'm going to export a const.
36:12 - And this const will be process.
36:16 - And process is going to hold an object.
36:18 - And inside that object, we're going
36:20 - to have a key value pair where the key is nv.
36:24 - And the value will be an object with another key value pair.
36:28 - This time, the key will be openAI API key.
36:32 - And I've put that in uppercase letters with underscores
36:35 - separating each word.
36:37 - Now, the value in that key value pair will be our API key.
36:41 - So let's just copy and paste that from index.js.
36:46 - Now, we've done it like this with this environment variable
36:49 - to more closely mimic what we would
36:51 - be doing if this were a production app and this file
36:54 - were being stored on the server.
36:56 - And now that we've got the API key in one place,
36:59 - when you come to doing the challenges
37:01 - and we've got various API calls happening in the app,
37:04 - you'll only have to paste your API key in one place.
37:08 - Right, let's go back to index.js.
37:11 - And we're going to import that environment variable.
37:14 - So I'll come right up to the top.
37:16 - And I'll say import.
37:18 - And then in curly braces, we need process.
37:21 - And then we need from.
37:22 - And now we just need the path to our environment variable,
37:26 - which is just going to be slash nv.
37:29 - And this should work because in index.js,
37:31 - if we scroll down to the bottom, we've already
37:33 - got this piece of code right here, type equals module.
37:38 - So the browser knows to expect modular JavaScript.
37:42 - Now, let's check it's working.
37:44 - So what I'm going to do is comment out this API key.
37:48 - And I'll set up a new const called API key.
37:51 - And I'll have that store, the API key
37:54 - that we're importing with process.
37:56 - So it will be process.
37:59 - And then we're going to need.env and then.openai-api-key.
38:05 - Let's hit Save and see if it works.
38:07 - So I'll come in here, click the button.
38:10 - And there we are, enthusiastic and excited.
38:14 - OK, so we've got our API key where I want it.
38:17 - Next, let's install the openai-dependency.
38:20 - And we'll come on to that in the next scrim.
38:27 - OK, let's install the openai-dependency.
38:30 - And in scrimba, that's dead easy.
38:32 - Over here on the left-hand side, then,
38:34 - I'll come to the three-dot menu.
38:37 - And I've got the option to add dependency.
38:40 - And again, that is not recorded, but a dialog box appears.
38:44 - And I just need to enter the dependency I want.
38:46 - And in this case, that is just going
38:48 - to be openai, all as one word, and lowercase.
38:52 - I hit Add.
38:54 - And there we are.
38:55 - The dependency has appeared on the left-hand side.
38:57 - So scrimba has done all of the hard work
38:59 - for me in the background.
39:01 - Now, if you're working outside of the scrimba environment,
39:04 - perhaps you're following this on YouTube
39:06 - or creating your own project in VS Code or any other editor,
39:10 - we will talk about how you can work with the openai-dependency
39:13 - in just a couple of scrims' time.
39:15 - Now that we've got the dependency installed,
39:17 - we need to import two constructors
39:20 - from that dependency.
39:21 - Let's just check this code.
39:23 - So we're going to need configuration and the openai API.
39:28 - And they both have uppercase-first letters.
39:30 - Now, they use require here.
39:32 - We are using import.
39:34 - So let's just come up here and say import.
39:37 - And we need configuration and openai API.
39:42 - And just note that AI here has got uppercase letters
39:46 - that caught me out the first time.
39:48 - Now we're importing them from openai.
39:52 - And let's just sort out my typos.
39:54 - And the first thing that we're going to do
39:56 - is set up a new instance of this configuration constructor.
40:01 - So down here, I'm going to say const configuration
40:05 - with a lowercase c.
40:07 - And I'll set that equals to a new instance of configuration.
40:12 - And as we can see here, we'll need
40:14 - to pass in our API key inside an object.
40:19 - And we've actually got the code we need for that right here.
40:22 - Now, we need to use this second constructor, the openai API.
40:26 - So I'll come down here.
40:27 - And I'll set up a const openai.
40:30 - And this will store a new instance of the openai API.
40:35 - And we just need to pass in the new instance of configuration
40:39 - that we just created.
40:40 - Oh, and I've just noticed that I've made a mistake right here.
40:43 - The AI is uppercase, but API is actually camel case.
40:48 - So we need to change that here and here.
40:51 - Glad I saw that before we got a strange error.
40:54 - OK, now we can delete a load of code.
40:56 - So we're not going to need this URL anymore.
40:59 - We're not going to need the API key saved in here.
41:01 - We actually only used it here for a test.
41:03 - So let's delete all of that.
41:06 - And now in the next grim, we're going
41:08 - to come down here to this fetch bot reply function.
41:11 - And we're going to make a lot of changes to all of this code
41:14 - right here.
41:15 - When you're ready for that, I'll see you there.
41:23 - So we're going to tidy up this function a lot.
41:26 - All we actually want to keep is the model and the prompt.
41:29 - So I'm just going to delete everything else.
41:33 - Now, I am going to keep this line right here
41:35 - where we update the speech bubble.
41:36 - But I'm just going to comment it out for now.
41:38 - And we'll need to make a few changes to it in a moment.
41:41 - Now, I'm going to come in here and set up a const response.
41:46 - And now, as per the code we've got in here,
41:48 - we can await our new instance of the OpenAI API.
41:54 - So remember, we've got that stored as a const right here.
41:57 - Next, we need to tell it to use the create completion
42:00 - endpoint.
42:02 - And then, just like they've done here,
42:04 - we're going to pass in an object with our model and prompt.
42:08 - So let's just cut and paste that right in here.
42:11 - Now, let's log out the response.
42:14 - So I'm going to hit Save.
42:16 - And it looks like we're getting an error.
42:18 - It says syntax error, unexpected reserved word.
42:22 - So here's a quick debug challenge for you.
42:24 - Can you figure out what's going wrong here?
42:26 - Just pause now and take a second to look at that.
42:34 - OK, so hopefully you figured out that because we're
42:36 - using the await keyword right here,
42:39 - we actually need this function to be async.
42:43 - OK, let's try that again.
42:44 - And we're not getting any errors.
42:46 - So I'll press the button.
42:48 - And there we are.
42:48 - We're getting our response.
42:51 - And if we look down in the console,
42:52 - we're getting the completion, excited, passionate, eager.
42:56 - OK, so to get this rendered, this line of code
42:58 - needs to change just a little bit.
43:01 - Let's just bring it up here and uncomment it.
43:03 - So all we need to do here is take this from response.
43:08 - So it's response.data.choices.
43:10 - And then we'll take text from whatever
43:12 - is at the zero index of the choices array.
43:16 - Let's give it a go.
43:18 - And there we are.
43:19 - It is working, excited, enthusiastic, driven.
43:23 - Now, I'm just going to delete this console.log.
43:25 - And you might well notice that we've
43:27 - got a lot of white space up here.
43:29 - And actually, if I just copy and paste something
43:31 - from the console, you can see that the completion
43:35 - starts with white space.
43:37 - So here's a quick JavaScript revision challenge for you.
43:40 - How do you remove the white space
43:42 - from the beginning and the end of a string?
43:45 - Just pause now and do that.
43:51 - OK, so hopefully you remembered that we
43:53 - can do that with the trim method.
43:56 - So I'll chain that on the end.
43:59 - Let's hit Save.
44:00 - And I'll press the button one last time.
44:02 - And there we are.
44:04 - Now it is at least equally spaced out.
44:06 - And as I said before, the CSS wasn't really
44:09 - designed to cope with such a short completion.
44:12 - But soon, the text that we generate
44:14 - is going to be much longer.
44:16 - OK, we have seen various enthusiastic responses
44:19 - from OpenAI.
44:20 - But the next thing that I want to look at
44:22 - is making that response actually tailored
44:25 - to whatever the user puts inside this text area.
44:28 - So I want to get OpenAI responding to our user's input
44:32 - in a human manner.
44:34 - But before we do that, I did promise
44:35 - that we'd quickly look at running our code outside
44:38 - of the Scrimba environment.
44:40 - So the next grim is optional.
44:41 - If you're working on this in Scrimba,
44:43 - or if you've already set this up locally,
44:45 - you can actually skip the next grim and move straight ahead.
44:48 - When you're ready, let's move on.
44:50 - Let's see how we can get this project up and running
icon on the right-hand corner.
44:53 - in VS Code with the OpenAI dependency.
Let's see how we can get this project up and running
44:57 - in VS Code with the OpenAI dependency.
So the first thing to do is to come on to this or any other
45:00 - So the first thing to do is to come on to this or any other
Scrim, and you don't need to have a Scrimba account to do this.
45:04 - You can just click this cog icon down in the bottom right hand
Scrim, and you don't need to have a Scrimba account to do
45:06 - this.
45:07 - You can just click this cog icon down in the bottom right hand
corner.
45:08 - That's going to bring up a menu.
45:10 - Select Download as Zip.
corner.
45:11 - That's going to bring up a menu.
You need to unzip that package on your PC
45:13 - Select Download as Zip.
and then click on the
45:16 - You need to unzip that package on your PC
45:18 - and then open the folder in VS Code.
45:21 - So it's just File and Open Folder.
45:24 - And often, the folder has actually
45:26 - got this really convoluted, randomly generated name.
45:29 - And of course, you can change that to whatever you want.
45:32 - Now, inside this folder, we've got several files.
45:35 - This one here is called Read Me for a Reason.
45:38 - Let's open it and see what it says.
45:40 - And basically, it's telling us that we
45:42 - need to run npm install and npm start.
45:46 - So what we need to do then is open up the terminal.
45:49 - And when we do that, we will get a new instance of the terminal
45:52 - down here.
45:53 - And this is where we can write npm install.
45:57 - And I've just put it a little bit bigger there,
45:59 - just so you can read it.
46:00 - When you do that, npm install is going to do its thing.
46:04 - Once it's done down here, you can run npm start.
46:07 - And again, there it is, a little bit bigger.
46:10 - And that will do its thing.
46:11 - And eventually, down in the terminal,
46:14 - in the terminal, you will see this.
46:17 - And so to run the project locally now, all we need to do
46:20 - is go to this link.
46:22 - And the link is made of your local IP address and this port.
46:26 - So the port, in this case, is 5173.
46:29 - Now, you could replace your local IP address with localhost
46:32 - and then have the colon and then the port.
46:35 - That will also work just fine.
46:37 - The quickest way to click on this link
46:39 - is in Mac to hold down Command, in Windows
46:41 - to hold down Control, and then that link becomes clickable.
46:45 - And it should take you through to your browser
46:48 - with the project running.
46:49 - And you can see here, I'm accessing this on localhost,
46:52 - but the IP address would work there just fine.
47:01 - Before we get to work on our response from the AI,
47:04 - I want to make a slight change.
47:06 - When we have just one word in these keys,
47:09 - we don't need them to be wrapped in inverted commas.
47:12 - Now, it's a totally personal thing,
47:13 - but I prefer it without where possible.
47:16 - So let me just tidy this up a little bit.
47:18 - OK, I feel strangely better for doing that, even though it
47:21 - doesn't matter at all.
47:22 - Let's get to work with personalizing the response we
47:25 - get back from the AI.
47:27 - So at the moment, we've given it this rather basic prompt.
47:30 - And you only get back as much as you put in,
47:33 - so it's giving us this very boring, generic reply.
47:37 - And if we hit Send, we'll see an example of that.
47:40 - Excited and eager.
47:41 - OK, so thanks for that.
47:43 - But to be honest, we could have hardcoded it ourselves.
47:46 - What I want to do is uncomment this code here.
47:50 - So what's happening now is when a user clicks the Send button,
47:54 - the If clause here is going to check
47:55 - that there is actually some text in the text area.
47:58 - If there is, it's going to render the loading SVG
48:02 - and update the speech bubble to our first generic message.
48:06 - At that point, I also want it to call FetchBot.
48:09 - Now, if we're going to get a personalized response,
48:12 - then our prompt needs to have access
48:14 - to whatever the user entered into the text area.
48:18 - So let's take whatever that is and save it
48:20 - as a const user input.
48:23 - And we'll pass in user input when we call FetchBotReply.
48:26 - And let's bring it into FetchBotReply
48:28 - as the parameter outline, because it's
48:31 - going to be a one sentence outline of a movie.
48:34 - Now, the last change I'm going to make here
48:36 - is I just want to log out the response.
48:39 - OK, and now it's time for a challenge for you.
48:42 - And I'm just going to come in here,
48:44 - and I'm going to paste it right inside this object,
48:47 - because it's this prompt here that I'm
48:49 - going to ask you to refactor.
48:52 - So I want you to refactor this prompt so the AI gives
48:55 - an enthusiastic, personalized response to the user's input
49:00 - and says it needs a few moments to think about it.
49:03 - Now, a couple of things for you to think about.
49:05 - We can use the parameter outline in the prompt
49:08 - by converting that prompt to a template literal with backticks.
49:12 - And you might want to put the outline in inverted commas
49:16 - or speech marks to signal to open AI
49:19 - that this is a chunk of text that you're referring
49:22 - to with the rest of the prompt.
49:24 - And of course, you can experiment with the wording,
49:26 - but don't be afraid to just ask for what you want.
49:29 - There's not one correct way of writing this prompt.
49:31 - And afterwards, I'll show you my way.
49:34 - Now, just before you do that, I'm going to do you a favor.
49:37 - I'm actually going to break the rules again
49:39 - and add a line of code here that we haven't talked about yet.
49:42 - Now, I want to do that because I'm
49:44 - aware that we've looked at loads of theory
49:46 - and we've laid loads of foundations,
49:48 - but there hasn't been too much time for you
49:50 - to get your hands on the keyboard yet.
49:52 - So I just want to pop this line of code in here,
49:54 - because without it, this challenge
49:55 - would be ridiculously frustrating.
49:57 - And then we will talk about it afterwards.
50:00 - So all I'm going to do is add a property to this object, max
50:04 - tokens 60.
50:06 - OK, so pause now.
50:08 - Don't worry about this mysterious new property at all.
50:11 - Get this challenge sorted, and we'll have a look together
50:14 - in just a minute.
50:20 - OK, so hopefully you managed to do that just fine.
50:23 - So I'm going to come in here, and I'm
50:25 - going to completely replace this prompt.
50:27 - And what I'm going to say is this.
50:30 - Generate a short message to enthusiastically say,
50:33 - outline sounds interesting, and that you need some minutes
50:36 - to think about it.
50:37 - Mention one aspect of the sentence.
50:40 - So you'll have noticed that I've put outline
50:42 - in inverted commas, so the AI understands
50:44 - that I want it to deal with that specific line of text.
50:48 - And also, this last instruction will hopefully
50:51 - make OpenAI personalize the completion.
50:54 - Now, of course, to get access to outline,
50:56 - we need to swap these four backticks.
50:59 - OK, let's hit Save, and I'm just going
51:01 - to put a one-line idea in here.
51:05 - And I'm just going to say, a spy deep behind enemy lines
51:08 - falls in love with an enemy agent.
51:10 - Let's press Send.
51:12 - And look at that.
51:13 - We're getting a really long completion.
51:16 - And if you just read through that,
51:18 - you can see that it's actually like we're
51:19 - interacting with a human.
51:21 - It's being conversational.
51:23 - It's referring to our idea, and that
51:25 - is exactly what we want.
51:27 - Now, down in the console, we've got the response.
51:29 - And just bear with me while I copy and paste
51:32 - something from there.
51:34 - Now, before I explain why I've done that,
51:36 - I'm just going to actually use the same idea again.
51:40 - But this time, I'm going to remove this line of code
51:43 - that I added just before you did the challenge.
51:47 - So I said, a spy deep behind enemy lines
51:50 - falls in love with an enemy agent.
51:52 - Let's press Send and see what happens.
51:54 - And there we are.
51:55 - We get our response, but look, it is much shorter.
51:59 - A spy deep behind enemy lines falls in love with an,
52:02 - and then it stops.
52:04 - My answer is cut off.
52:06 - Now, I'm just going to copy and paste the same properties
52:09 - from the response.
52:11 - And what we can see there is that the first time
52:13 - with the more successful completion,
52:15 - we had the finished reason of stop.
52:17 - And the second time when the completion was actually not
52:21 - complete, where we got cut off, the finished reason
52:24 - was length.
52:25 - So generally speaking, a finished reason of length
52:28 - is bad news.
52:29 - It means OpenAI has not given us everything it wanted to.
52:34 - Now, also, at the end here, we see completion tokens 59
52:38 - in the first call and completion tokens 16 in the second call.
52:43 - So something we're doing with tokens
52:45 - is affecting the length of the completion we get.
52:47 - Now, as you're a seasoned coder, you've
52:49 - probably grasped that we can control
52:51 - the length of our completion to some extent
52:54 - with this max tokens property.
52:56 - But before we start using max tokens all over the place,
52:59 - we should really understand what a token is in OpenAI
53:02 - and what this number 60 really means.
53:05 - So in the next scrim, let's take a peek under the hood
53:08 - and take a dive into tokens and the max tokens property.
53:12 - When you're ready for that, move on.
53:14 - OK, let's talk about tokens and the max tokens property.
53:18 - So we know that when we add this max tokens of 60
53:24 - to our API request, we get plenty of text back.
53:28 - We also know that if we don't set max tokens,
53:31 - we get much less text back and it's actually not complete
53:35 - and therefore doesn't really make sense.
53:38 - And that's why we're doing this.
53:40 - It's not complete and therefore doesn't really make sense.
53:44 - And that's what we can actually see right here.
53:47 - So what exactly is going on with tokens
53:50 - and what even are tokens?
53:53 - OpenAI breaks down chunks of text for processing.
53:57 - Now, I say text.
53:59 - It depends on which model you're using and what you're doing.
54:01 - It could also be code that gets broken down into chunks.
54:04 - But we're working with text, so we
54:06 - need to think about tokens in the context of text.
54:09 - I think that each word or each syllable would be a token.
54:13 - But it's actually not as simple as that.
54:15 - Roughly speaking, a token is about 75% of a word.
54:20 - So 100 tokens is about 75 words.
54:24 - So the 60 token limit that we put on this fetch request
54:28 - would bring us back a maximum of about 40 words.
54:32 - When we didn't use max tokens at all,
54:35 - this one here actually defaulted to 16, which as we can see here
54:40 - is way too short for our needs.
54:42 - So that's an important lesson.
54:44 - If you don't allow enough tokens,
54:46 - your completion will be cut short.
54:48 - So you'll actually get less text back from OpenAI.
54:52 - So now you might be thinking, well, OK,
54:54 - a token is a chunk of text.
54:56 - But so what?
54:57 - Why do I need to think about limiting it?
54:59 - And the best way to make sure that your max token setting
55:02 - isn't causing you problems is to check the object you
55:05 - get with your response.
55:06 - If you see the finish reason of length, that is a bad sign.
55:11 - That means the text that you're getting back
55:13 - has been cut short.
55:15 - And if you see a finish reason of stop, that is a good sign.
55:18 - That means that OpenAI actually got to the end of the process
55:22 - and it's given you all of the text
55:23 - that it wanted to give you.
55:25 - So you might be thinking, OK, so a token is a chunk of text.
55:28 - But why does that matter?
55:30 - Well, there are some good reasons for knowing about tokens
55:33 - and being able to limit them.
55:34 - Each token incurs a charge and it takes time to process.
55:40 - So that gives you an incentive.
55:42 - If you limit the number of tokens,
55:44 - you can keep costs down and keep performance up.
55:48 - And that's really important, of course,
55:49 - for when you run out of free credit
55:51 - and if you're creating a production app,
55:53 - and that app scales to millions and millions of users.
55:55 - And why shouldn't it?
55:57 - Now, there's something really important about max tokens
56:00 - that we need to understand.
56:02 - Max tokens does not help us control how concise a text is.
56:08 - As we saw in our app, we get an incomplete response
56:11 - when the token count is low, not a more concise one.
56:14 - So as a tool to control how verbose
56:17 - or how expressive OpenAI is, max tokens is useless.
56:22 - And that begs the question, how should I use it?
56:24 - And the answer is, we should set it high enough
56:26 - to allow a full response from OpenAI.
56:29 - So you might just have to do a little bit of experimentation
56:32 - with that each time and just making sure the text
56:34 - that you get back from the API is not cut short.
56:38 - So how can we control the length of text we get from OpenAI?
56:42 - Well, we do that with prompt design.
56:45 - Good prompt design is everything.
56:47 - And good prompt design is the best way
56:49 - to ensure that the text we get from OpenAI
56:52 - is the length we want.
56:54 - Now, I actually think that the text
56:55 - that we got back when we had max tokens set to 60
56:58 - was just a little bit too long.
57:00 - So as we go through this project and we learn more
57:03 - about prompt design, we will come back
57:05 - and just do a little bit of refactoring here.
57:07 - But for now, I want to keep up the momentum,
57:10 - keep moving forwards.
57:11 - So let's start tackling our next API call, which
57:14 - is to generate a full synopsis from our one sentence movie
57:18 - idea.
57:19 - When you're ready for that, let's move on.
57:25 - OK, so we've got the one sentence idea from the user.
57:28 - Now let's make it into a professional synopsis
57:31 - for our movie.
57:32 - So I'm going to come down here and set up a function called
57:36 - fetch synopsis.
57:39 - And of course, this will be an async function.
57:42 - Now eventually, when the app's finished,
57:44 - we'll be displaying the synopsis to our user
57:47 - in this container, which will pop up
57:49 - when the finished movie pitch is ready to display.
57:52 - And the synopsis will actually be right down here
57:54 - at the bottom.
57:56 - Now, if we have a look over in the HTML,
57:59 - this is the section here, which is actually
58:01 - going to contain all of the output.
58:04 - And it's this paragraph down here, output text,
58:08 - which will contain the synopsis.
58:11 - So at the moment, this whole container is hidden by default.
58:14 - But if we go over to index.css and we just
58:17 - uncomment that one line there, now that container
58:20 - has appeared at the bottom.
58:22 - So in this new function that I've set up, fetch synopsis,
58:26 - what I'm going to do is take control of this paragraph
58:29 - that we've got right here with the ID of output text.
58:33 - And then we'll be able to display our synopsis right
58:35 - there just to check it's working.
58:37 - So inside the fetch synopsis function,
58:40 - we want to say document.getElementByID.
58:45 - And the ID is output-text.
58:49 - And we'll set the inner text to whatever text
58:52 - we get back from the API.
58:54 - And we can just borrow that line of code right there.
58:58 - Because in every case, the text that we get back from OpenAI
59:01 - comes in an object.
59:03 - And it is always actually in the same place.
59:06 - Now, I'm going to call fetch synopsis from right here
59:09 - inside this event listener.
59:11 - And I'll put it right underneath where we get the first reply
59:14 - to our initial idea.
59:17 - And I'll pass in the user input because that will also
59:20 - be needed to get the synopsis.
59:22 - And of course, let's take that in as a parameter right here.
59:26 - And again, I'll just call it outline.
59:28 - OK, now it's time for a big challenge for you.
59:31 - And I'm just going to paste it right in here.
59:35 - So I want you to set up an API call with model, prompt,
59:38 - and max tokens properties.
59:41 - The prompt should ask for a synopsis for a movie based
59:44 - on the outline supplied by the user, which of course,
59:47 - we're bringing in with this parameter outline.
59:50 - And just remember, when writing prompts,
59:52 - try to be specific, descriptive, and as detailed as possible
59:56 - about the desired outcome.
59:57 - Try to avoid fluffy and imprecise descriptions.
60:01 - So don't use phrases like sort of or roughly.
60:05 - Now, we said before that we can control
60:07 - the length of the output with good prompt design.
60:10 - Now, you could experiment here with asking
60:12 - for a particular word count.
60:14 - Say, give me a synopsis of 150 words.
60:18 - But I just want to warn you that that's
60:19 - likely to be quite imprecise.
60:21 - So don't be frustrated if the length of the text you get
60:24 - is not what you want.
60:26 - We will be dealing with that shortly.
60:28 - OK, there is quite a lot to do here,
60:30 - but you've got all of the skills you need to do this.
60:32 - So pause now and get it sorted.
60:39 - OK, so hopefully you managed to get a really good synopsis.
60:43 - Let's have a look together.
60:45 - Inside here, I am going to set up a const called response.
60:49 - And then we'll await an openAI create completion.
60:54 - And we need to pass that an object with our model prompt
60:57 - and max tokens.
60:59 - So the model is text da Vinci 003.
61:04 - I'll put my prompt in backticks so we can use the outline.
61:08 - And I'm just going to say, generate
61:10 - an engaging, professional, and marketable movie synopsis
61:14 - based on the following idea.
61:18 - And lastly, we need max tokens.
61:21 - Because remember, max tokens will actually default to 16.
61:24 - And we don't really need that comment there anymore.
61:27 - So I will say, max tokens.
61:30 - And I'm going to go for something pretty big
61:32 - because I don't want our text to get cut off.
61:34 - So I'll say 700, and we can always reduce that later.
61:39 - OK, let's hit Save and give it a try.
61:41 - So for my idea, I'm going with this.
61:45 - A madman develops a machine to control all humans.
61:50 - And only intelligent animals can save the human race.
61:54 - Although, would they really want to?
61:56 - Anyway, let's give that a try and see what we get.
62:00 - So we're going to get our first message from the API.
62:02 - Again, we've got this pretty long response.
62:05 - And we're waiting now for the synopsis.
62:08 - And I'm just going to scroll down
62:09 - because it's just going to appear right here
62:12 - inside this box.
62:13 - OK, and there we are.
62:14 - We get our synopsis.
62:16 - And the results we get back are fine.
62:18 - Now, I'm just going to copy and paste them because actually,
62:21 - you'll probably find that if you pause,
62:23 - you won't be able to scroll the mini browser.
62:25 - So let me just paste them right here inside this file.
62:30 - Now, the first thing that we can say
62:32 - is that that is pretty impressive.
62:34 - We've got a long body of text, very much as
62:37 - if it was written by a human.
62:39 - So pretty happy with that.
62:41 - And although it's long, it does finish naturally.
62:43 - I don't think it's actually been cut short
62:45 - by having too few tokens.
62:46 - I think 700 is plenty.
62:49 - But I do wonder if it's as good as it could be.
62:51 - Now, our prompt instruction that I've written here
62:54 - is fairly good.
62:56 - But it's actually quite hard to describe
62:58 - what exactly you want a longer passage of text
63:00 - to be like in detail.
63:03 - And as we've talked about several times,
63:05 - it's also quite hard to control the length.
63:07 - This is arguably quite long.
63:09 - Now, I've just counted the words here
63:11 - and it comes in at just over 160.
63:13 - That's actually not bad, but we can't really rely on that.
63:17 - Also, I can't help feeling that the story it's laid out
63:20 - is just a little bit woolly.
63:21 - It would be nice if it was just a little bit tighter,
63:24 - a little bit more concise.
63:26 - If we're going to grab the movie studio's attention,
63:28 - we need it to be really sharp.
63:31 - So what I want to look at next
63:33 - is how we can help OpenAI understand more
63:36 - about what we want.
63:38 - So far, we've been giving OpenAI
63:41 - these simple one-line instructions.
63:43 - What I want to try next is to include one or more examples
63:47 - actually inside the prompt to give OpenAI
63:50 - a push in the right direction,
63:51 - so it's going to give us more of what we want.
63:53 - So let's take a look at that next.
64:00 - So far, when writing prompts,
64:02 - we've just provided one or more sentences
64:04 - asking for what we want.
64:06 - You'll often hear this referred to as the zero-shot approach.
64:10 - And what that means is it's just an instruction.
64:12 - Now, the zero-shot approach works really well
64:15 - for many simple requests.
64:17 - But when we work with more complex requests,
64:20 - we run a greater risk of getting back results
64:22 - that don't really fulfill our needs.
64:24 - So we might get completions which are off-topic
64:27 - or too long, or perhaps the format
64:29 - is just not what we want.
64:31 - Now, I've got an example of that happening right here.
64:34 - Let me introduce you to Advertify,
64:36 - which is an app I've made to generate copy
64:39 - for advertising purposes.
64:40 - It's pretty simple.
64:41 - The user can input a product name,
64:43 - give a description, and a target market.
64:46 - They hit generate copy,
64:47 - and they get a bunch of text they can use in their adverts.
64:51 - And if we just have a quick look at the code,
64:53 - there's nothing unfamiliar going on here.
64:55 - We're bringing in the user inputs right here
64:58 - from the inputs and this text area,
65:00 - and then down here, we've got this one-line prompt,
65:04 - and it's a basic instruction,
65:06 - and it's just using the product name,
65:08 - product description, and the product target.
65:11 - So the instruction is just create 50 words
65:14 - of advertising copy for this product,
65:16 - which can be described as the product description
65:19 - and aimed at the product target market.
65:21 - And we might just tidy up that last back tick
65:24 - and bring it all onto one line.
65:26 - Let's see this in action, but before we do that,
65:28 - I'm just going to log out the response.
65:31 - Now I'm going to go for the same vegan fish cream example
65:34 - that we've got in the placeholder text.
65:36 - And I'll hit generate copy, and let's see what we get.
65:40 - And okay, it's working, but look,
65:42 - it's given me an ordered list.
65:44 - We've got one, two, three, four, five, and six.
65:47 - And it looks a little bit longer than 50 words,
65:49 - but if we open up the console,
65:51 - we can actually see that we've got this,
65:54 - and I'll just paste it in here.
65:56 - It says finish reason of length.
65:58 - And as I'm sure you'll recall,
66:00 - that means that OpenAI has actually cut us short.
66:04 - And if you look, number six is not complete.
66:07 - It just says, grab your vegan dash.
66:09 - And at that point we know that OpenAI
66:11 - was trying to give us more.
66:13 - Now I've tried this several times before recording
66:15 - with various prompts,
66:17 - and OpenAI really doesn't work that well with word counts.
66:20 - The five words or less that we used in a previous challenge
66:23 - does seem to work okay,
66:25 - but longer word counts often fail.
66:27 - Now, interestingly, OpenAI does know how long a tweet is.
66:31 - If we were to change this prompt and ask for a tweet,
66:34 - it will nearly always deliver something that's tweet length
66:37 - and throw in some hashtags as well.
66:39 - Okay, let's try something else
66:40 - and see if we can make this Advertify app
66:43 - work a little bit better.
66:45 - We're going to have a look at the few-shot approach.
66:48 - The few-shot approach is where we add
66:50 - one or more examples to our prompt.
66:53 - We do this because it helps the AI understand what we want.
66:57 - This is great for more complex tasks.
67:00 - So let's refactor our prompt.
67:02 - Firstly, I'm going to remove all of the variables
67:05 - and actually rewrite this instruction.
67:07 - So I've said user product name, a product description,
67:10 - and a target market to create advertising copy
67:13 - for a product.
67:14 - At the moment, we're not doing anything
67:16 - with the user input,
67:17 - so that's just gonna give us something really, really random.
67:20 - But what we're gonna do next is lay out an example.
67:23 - So staying inside the backticks,
67:25 - I'll come down onto a new line and here's my example.
67:29 - So I've got a product name, which is Flask Tie,
67:33 - a product description,
67:34 - a tie with a pouch to hold liquids
67:36 - and a straw to drink through,
67:38 - and the product target market is Office Workers.
67:42 - Now I've also put some advertising copy right here
67:45 - and that advertising copy is about 54 words long,
67:49 - which is the kind of length I'm looking for.
67:52 - Now, before this is going to work,
67:53 - we need to do a couple of things.
67:55 - Firstly, we need to bring back our variables
67:58 - and we're going to lay them out
67:59 - in the same style as our example.
68:02 - So in fact, I'm just going to copy and paste
68:03 - all of that text.
68:05 - And now let's bring in the user inputs.
68:08 - So Flask Tie, we can replace with product name.
68:12 - Product description, we can replace with product desk.
68:15 - And of course, target market will be product target.
68:19 - Now advertising copy, we are just going to leave that empty.
68:23 - So can you see what we've done here?
68:25 - We've given an example
68:27 - and then we've given a partly completed example.
68:30 - And this space right here is an invitation for OpenAI
68:34 - to go and complete.
68:36 - And if you'll remember,
68:37 - we're using the create completion endpoint.
68:40 - So OpenAI is just going to complete this block of text here
68:44 - in the same style as this block of text here.
68:47 - It's going to recognize that we want it
68:50 - to give us something like that,
68:52 - but tailored to the three variables
68:54 - that we've got right here.
68:56 - If we tested this right now, it would likely work.
68:58 - But there's one more thing that we can do to help the AI
69:01 - and to make our code more readable.
69:04 - What we're going to do is separate
69:06 - our instructions and context.
69:08 - Now OpenAI docs suggest that you can do this
69:11 - with three inverted commas or three hash symbols.
69:14 - I'm just going to go for the hash symbols.
69:16 - And I'm going to put one set right here
69:19 - after the instruction.
69:20 - And then I'll put another set down here
69:23 - in between my example
69:24 - and where we're asking OpenAI to do its thing.
69:28 - And so all that's doing
69:29 - is breaking up this body of text a little bit
69:32 - so OpenAI can see that it's dealing with different entities,
69:35 - an instruction, an example, and a request to go and complete.
69:40 - Okay, let's hit save and see if it works.
69:43 - So I'm going to put in the same vegan fish cream
69:47 - and I'll hit generate copy.
69:49 - Okay, let's check out the copy we've got.
69:51 - So firstly, we haven't got this ordered list.
69:54 - There's no numbers one to six.
69:56 - What we've got is a body of advertising text
69:59 - and that is what I wanted.
70:01 - Secondly, I've just checked the word count of this
70:04 - and it's actually coming in as 63 words.
70:06 - Now the example that I put here is actually 54 words.
70:11 - So there's only nine words between the two
70:14 - and that is pretty good.
70:15 - And if we have a look down in the console,
70:18 - we can see that the finished reason is stop.
70:20 - That makes sense because this is very much
70:23 - the end of the sentence and the end of the text
70:25 - so OpenAI has given us everything
70:28 - that it wanted to give us.
70:29 - So the example is doing a really good job
70:32 - of allowing us to show OpenAI what it is we want.
70:36 - Now remember, this is called the few shot
70:39 - and we can add one or more examples.
70:43 - We should remember however, that longer prompts cost more
70:46 - so you don't wanna go too far.
70:48 - But let's go ahead and add one more example
70:51 - and actually this is something
70:52 - that you're going to do as a challenge.
70:54 - So let's get straight onto that in the next script.
71:01 - Okay, so here's your challenge.
71:03 - Add a second example here,
71:05 - be sure to make it similar in design to the first.
71:08 - Now I've put that because we don't want to confuse the AI.
71:11 - The examples that we give should be in a similar format
71:15 - and they should be consistent
71:17 - else we're just creating confusion.
71:19 - Now I've also put here, remember to use separators.
71:22 - So that is these three triple hashtags
71:25 - and you can use them in exactly the same way
71:27 - to separate out your example from the request to create.
71:31 - Now one thing I want you to remember here
71:33 - is that the law of diminishing returns applies.
71:37 - Adding more and more examples
71:38 - will not guarantee better results.
71:41 - You might see great results or you might not.
71:43 - Also before you do this challenge,
71:45 - I just want to say that I'm aware
71:46 - that some people following this course
71:48 - might be at a disadvantage here
71:50 - because English is not their first language
71:52 - or perhaps language is just something they struggle with.
71:55 - So I've added a file up here called product.md
71:59 - and that has got an example that you can use.
72:01 - So you can copy the text
72:03 - and just stick to practicing the layout
72:05 - and the syntax of the prompt.
72:07 - Okay, pause now, get this challenge sorted
72:09 - and I'll see you back here in just a minute.
72:15 - Okay, hopefully you got that working.
72:17 - So I'm just going to come in here
72:19 - and I'm going to copy all of the texts
72:21 - that I've got right here
72:23 - and let's just put our second example underneath the first.
72:26 - First thing that I want to do is separate out the examples.
72:29 - So we'll borrow these hashtags.
72:31 - Now I'm just going to take the product name
72:34 - and that will be solar swim, product description.
72:38 - It's this crazy swimming costume with solar cells
72:41 - to charge your phone while you're on the beach
72:42 - or what have you.
72:44 - And we've got the target market,
72:45 - which is of course young adults
72:47 - who'll spend their money on anything.
72:49 - Finally, we've got the advertising copy
72:52 - and that is approximately the same length and style
72:56 - as the advertising copy we've got right here.
72:59 - So that should do the trick.
73:00 - Let's just remove that space and we'll hit save.
73:04 - And let's give it a go and I'll hit generate copy.
73:08 - And okay, again, that has been pretty successful.
73:11 - The word count is coming in at just under 50 words.
73:14 - Well, our examples were 54 words and 53 words respectively.
73:18 - So that is pretty good.
73:20 - And also I think we're getting pretty decent copy.
73:23 - Now it can be really, really hard to judge
73:25 - because open AI is not going to give you
73:27 - the same completion for the same prompt each time.
73:30 - So it's really hard to judge when you refactor a prompt
73:33 - exactly how much difference is made.
73:35 - But I think two examples is a pretty good amount
73:38 - for this few shot approach.
73:40 - And it's definitely had a beneficial effect on this app.
73:44 - So what we need to do now is go back to the function
73:46 - that generates our synopsis
73:48 - and see if the few shot approach will improve it.
73:51 - When you're ready for that, let's move on.
73:58 - Okay, so we have this synopsis fetch request
74:00 - and perhaps we can tighten up the synopsis we get back
74:03 - and improve it by adding an example or two
74:07 - with a few shot approach.
74:09 - Now, when you do this,
74:10 - you're welcome to use your own examples.
74:12 - In fact, I would encourage you to do that.
74:14 - If you're going to be working with open AI,
74:16 - then it's important to practice creating prompts.
74:20 - However, I have put an example synopsis up here
74:23 - in synopsis.md, which you can use if you'd like to
74:27 - when you complete this challenge.
74:29 - So your challenge is to refactor this prompt
74:32 - to use one or more examples,
74:35 - just like we did in the previous script.
74:37 - And remember to separate out the instruction
74:40 - from the example with either the triple hash
74:42 - or the triple inverted comma.
74:44 - Check back if you need to remind yourself of the syntax
74:47 - and I'll see you back here in just a moment.
74:55 - Okay, hopefully you managed to do that just fine.
74:58 - So I'm going to come in here
74:59 - and we're going to rework this prompt
75:03 - and I'm going to replace this with the following.
75:06 - Generate an engaging professional
75:08 - and marketable movie synopsis based on an outline.
75:12 - Now I'll come down onto a new line.
75:14 - I'm going to add my hashtag separator
75:17 - and then I'll add my example.
75:19 - And my example needs an outline and a synopsis.
75:24 - And I'm going to use the examples I prepared earlier.
75:26 - This is the outline and then we'll also take the synopsis.
75:31 - And I've made sure that this example synopsis
75:34 - is the kind of thing that I want to get back from OpenAI.
75:39 - Now, underneath that we'll add another separator
75:43 - and then it's outline.
75:45 - And this is where we need to bring in
75:46 - the user generated outline.
75:49 - And then the synopsis, which of course we can leave blank.
75:54 - Okay, let's hit save.
75:55 - And I'm just going to pop over to the CSS
75:58 - and I'll just uncomment this one one more time.
76:01 - So we'll see the synopsis appear here
76:03 - and let's put in a one sentence idea.
76:08 - So I've gone for a madman develops a machine
76:11 - to control all humans
76:12 - and only intelligent animals can save the human race.
76:16 - There we're getting our long initial response.
76:19 - And then down here, let's just wait for the synopsis.
76:23 - And there it is.
76:25 - Okay, the first thing I want to do is check the word count.
76:29 - And the good news is,
76:30 - it's almost exactly the same length as this example.
76:33 - There's less than 10 words in it.
76:36 - Now also, and this is subjective,
76:38 - but if you read through it,
76:39 - I think this is a pretty good synopsis.
76:42 - To me, it seems tight.
76:44 - It tells the whole story.
76:46 - It packs a lot of detail in.
76:47 - There's not too much fluff.
76:49 - So I'm pretty happy with that.
76:51 - And again, as always with OpenAI,
76:53 - it's really hard to make a comparison.
76:56 - But I think adding that example to the prompt
76:58 - has made a big difference.
77:00 - Now, of course, the synopsis makes this prompt quite big.
77:03 - And if we were to add several examples,
77:06 - we could end up burning tokens here unnecessarily.
77:10 - So if we were gonna take this to production,
77:12 - maybe we would run a bunch of tests with multiple synopses
77:16 - and then reduce the number to the minimum needed.
77:19 - Now, before we move on, I have got one more challenge for you.
77:23 - And I'm just going to come up here and paste it in.
77:27 - Okay, so the initial response that we get back
77:30 - when we first put our one line concept
77:33 - into the text area is pretty long.
77:35 - It doesn't actually break the layout,
77:37 - but it does just stretch things a bit too much.
77:40 - And I think it's all a bit unnecessary.
77:42 - What I prefer is to see something like this.
77:46 - So we've got a good personalized response,
77:48 - but it's just not that long.
77:50 - There's just enough there to make it clear
77:52 - that OpenAI has understood our request
77:56 - and is giving us a conversational response
77:58 - which references the one line input from the user.
78:01 - So here's a challenge for you.
78:04 - I want you to refactor this prompt
78:06 - to use examples of an outline
78:08 - and an enthusiastic response.
78:10 - Be sure to keep the length of your examples reasonably short,
78:13 - say 20 words or so.
78:15 - So we just don't want it to be massive like this one.
78:18 - Now again, it's great if you can work on this on your own,
78:21 - but I have put a file up here called outline.md
78:24 - and that has got some examples in it that you can use,
78:28 - but do write your own if you're able.
78:31 - Okay, pause now, get this challenge sorted
78:34 - and I'll see you back here in just a minute.
78:41 - Okay, hopefully you got that working just fine.
78:44 - So I'm going to come in here
78:46 - and I'm just going to change up this initial instruction.
78:50 - I'm saying generate a short message
78:52 - to enthusiastically say an outline sounds interesting
78:55 - and that you need some minutes to think about it.
78:57 - Now I'm not going to instruct it to mention something
79:00 - from the outline like we did before,
79:02 - mention one aspect of the sentence.
79:04 - What I want to do instead
79:05 - is show it how to do that with examples.
79:08 - So let's come in here and put the separator.
79:11 - And now I'm going to go over to outline.md
79:15 - and I'm going to bring in the examples
79:17 - that I've got right here.
79:19 - So I've got three examples
79:20 - and each one has got an outline and a message.
79:23 - So let's just label them clearly.
79:29 - And I'm going to separate each example with a separator.
79:34 - Okay, now let's come down onto a new line.
79:36 - We'll add another separator
79:37 - and then we'll say outline and message.
79:40 - And of course message will leave blank
79:42 - because that is what OpenAI is going to do for us.
79:45 - And outline will just be the outline
79:47 - that we've got coming in here as a parameter.
79:51 - Okay, let's hit save
79:53 - and I'm going to use the same example as I used before.
79:56 - And let's see what we get.
79:57 - But before we do that,
79:58 - let's actually just get rid of the output box
80:02 - that was at the bottom.
80:03 - And also just momentarily,
80:05 - I'm going to comment out this fetch synopsis function.
80:10 - So the app will only go so far as to call this function.
80:13 - I'll need to add my outline again and let's hit go.
80:19 - Okay, there we're getting a much shorter response.
80:22 - Wow, that's incredible.
80:24 - A madman and his machine?
80:25 - I'm feeling inspired already.
80:27 - Let me take a few moments to ponder it.
80:29 - So a much shorter completion, just like our examples,
80:33 - but still clearly referring to the outline I put in.
80:36 - Okay, now the app is working much more smoothly.
80:39 - But before we move on,
80:40 - there's just one little thing I want to tidy up.
80:43 - We've got a global variable up here.
80:46 - And actually we're only using it right here.
80:48 - So my bad for declaring it globally,
80:51 - I'm just going to move it down here inside that function.
80:55 - And I think that is better practice and a little bit neater.
80:58 - Now, next up in our app,
81:00 - we need to create an iconic title for our movie.
81:03 - But I think this would be a good moment for me
81:05 - to say a quick word about the architecture of this app.
81:08 - So let's just take literally a couple of minutes
81:10 - to talk about that.
81:16 - I just wanted to say a word
81:17 - about the architecture of this app.
81:20 - As always with code,
81:21 - there are loads of ways you could do this.
81:23 - So I want to say right now that the way I'm doing it here
81:26 - is definitely not the one best way.
81:29 - And in an app this size,
81:31 - actually it's not going to matter very much at all.
81:34 - But there are some things which we could do here
81:36 - which we're not doing and I wanted to give you
81:39 - an explanation as to why.
81:41 - So firstly, on the code reusability front,
81:45 - we are building objects for each of our calls to the API.
81:49 - And as you can see, before this app is finished,
81:51 - we will have done that multiple times.
81:54 - Now, would there be a way to set up one function
81:56 - to do the fetching and pass it an object with the prompt,
82:00 - max tokens, et cetera, every time we need it?
82:04 - Probably yes.
82:06 - And that would make our code much more reusable
82:08 - and less repetitive.
82:10 - But the reason I'm not doing that
82:12 - is just to let the AI take center stage
82:14 - and not get bogged down in JavaScript.
82:17 - Once you've got the basics of working with open AI,
82:20 - as you will by the end of this project,
82:21 - that is the time to start looking at ways of saving time,
82:24 - saving work and being more concise with the code.
82:27 - And likewise, normally I like to have my functions
82:30 - as much as possible just doing one thing.
82:33 - And we're breaking that rule here
82:35 - because we've got a function that's doing the fetching
82:37 - and it's also rendering out to the DOM.
82:40 - Now, we could potentially set up one render function
82:44 - and then have these fetch functions
82:45 - just return the response.
82:47 - And then when all of the API calls have completed,
82:50 - the render function could render out the results.
82:52 - It would be doable.
82:53 - We could store all of our completions in an object
82:56 - and have a function that checks
82:57 - if all of the fetch requests have responded successfully
83:00 - before triggering the render.
83:02 - But again, that would involve a whole lot more JavaScript
83:04 - when in this course, I really want to just focus on the AI.
83:09 - Now, at the end of the course,
83:10 - if you would like to refactor the code,
83:12 - that is not a bad idea at all.
83:13 - In fact, it's a really good idea.
83:15 - But I just wanted to give you a quick explanation now
83:18 - of why I chose to do it like this.
83:20 - Okay, with that done, let's go ahead and use open AI
83:23 - to generate a movie title that's so iconic,
83:26 - it's going to reverberate down through cinema history
83:29 - for the next 500 years.
83:31 - When you're ready for that, let's move on.
83:38 - We need a title for our movie.
83:40 - So let's go ahead and set up a function to do that.
83:44 - Now, I'm going to call this function fetch title.
83:48 - And as you know, by now,
83:49 - that is going to be an async function.
83:51 - Now, if we have a quick look over at index.html,
83:55 - we've got an element here, it's actually an H1.
83:58 - And that is where the title of our movie
84:00 - is going to be rendered.
84:02 - So back inside this function then,
84:05 - let's just deal with that first.
84:09 - Now we're going to generate this title from a synopsis.
84:13 - So we need to take that in as a parameter.
84:17 - I want to call this function
84:18 - from inside this fetch synopsis functions.
84:21 - So as soon as this response comes back with a synopsis,
84:25 - we want to send that synopsis straight back to open AI
84:28 - to get us a title.
84:30 - So I'll call the function right here.
84:32 - Now we need to pass in the synopsis
84:35 - and we've got that right here.
84:37 - But instead of writing out this code several times,
84:40 - why don't we actually set that up as a const called synopsis?
84:44 - And now we can use synopsis here
84:47 - and we'll also pass it in to the fetch title function.
84:51 - Okay, you should be pretty good at this by now.
84:53 - So I'm going to leave the rest of this function up to you.
84:56 - Just bear with me while I paste in your challenge.
84:59 - So I want you to write a prompt asking
85:02 - for a title based on a synopsis.
85:05 - If you'd like to, you can specify
85:07 - that the title should be gripping or alluring.
85:10 - And I say that just to remind you
85:12 - that being descriptive in prompt writing is a good thing.
85:15 - Secondly, remember that we need to add the model property
85:18 - and of course we should give it some max tokens.
85:21 - The default max tokens of 16 might be enough.
85:25 - That really depends on how long
85:27 - you think a movie title should be.
85:29 - I mean, some movie titles are really, really short.
85:31 - The Stephen King classic is, for example.
85:34 - Some are kind of medium to long, Pirates of the Caribbean,
85:38 - The Curse of the Black Pearl.
85:40 - And then some are quite ridiculous.
85:42 - Night of the Day of the Dawn of the Son
85:44 - of the Bride of the Return of the Revenge
85:45 - of the Terror of the Attack of the Evil Mutant
85:47 - Alien Flesh-Eating Hellbound Zombified Living Dead Part Two.
85:50 - And that film is real by the way, but it is a spoof film.
85:54 - Okay, I'll leave that up to you.
85:56 - Set some max tokens that you think are gonna cover
85:59 - the kind of title length that you want for your film.
86:02 - When I do this, I'm probably gonna set it
86:04 - to something like 10.
86:05 - Okay, pause now and go ahead and do this.
86:14 - Okay, so hopefully you managed to do that just fine.
86:17 - So I will come in here and set up my response.
86:21 - And then we will await the create completion call.
86:25 - And then inside the object, we need the model.
86:28 - We need the prompt.
86:29 - And my prompt is just going to be,
86:31 - generate a catchy movie title for this synopsis.
86:36 - And as for max tokens, I'm actually going to go for 15,
86:39 - just to give myself plenty of leeway
86:42 - if it does end up generating a really long title.
86:45 - Okay, let's hit save.
86:47 - And then down here, we've still got this display div visible.
86:51 - Remember in index.css, we've just commented out
86:54 - in a display none, so eventually it will be hidden
86:57 - and then we'll just show it at the end
86:58 - when all of the API calls have been completed
87:00 - and the finished product is ready to show.
87:03 - But for now, let's just watch
87:05 - when our synopsis and title appear.
87:08 - So I'm going to put in my one sentence movie idea.
87:11 - And this is quite a long one.
87:12 - A time traveler goes back to the 1980s
87:14 - to prevent a catastrophic event,
87:16 - but falls in love with someone from the past
87:19 - and must choose between saving the world
87:21 - or staying with them.
87:22 - Let's hit send.
87:24 - There we've got our first bit of feedback.
87:27 - Here comes the synopsis and here is our title.
87:32 - 1985 forever, James Mason's time traveling love story.
87:36 - Now I'm a little bit concerned here
87:37 - that we've got a quotation mark at the beginning,
87:40 - but not at the end.
87:41 - And if we just count up how many words we've got here
87:45 - and we bear in mind that these are quite long words,
87:47 - I'm just a little bit worried
87:49 - that my max tokens of 15 might not be enough.
87:51 - So I'm actually just going to err on the side of caution
87:54 - and whack that up to about 25.
87:56 - Let's try that again and see what happens.
88:01 - Okay, interesting.
88:02 - This time it's actually given us a much shorter title.
88:05 - So we don't actually know if the max tokens
88:07 - would have made a difference or not in that first attempt.
88:10 - I should have logged out the response.
88:12 - Then I would have been able to actually see
88:14 - the reason for finish.
88:15 - Was it stop or was it length?
88:17 - But nevermind, we've got a title and it's a good title.
88:21 - Or is it?
88:22 - That is rather a matter of taste
88:25 - and to some extent a matter of culture.
88:28 - Consider this.
88:29 - In the English speaking world,
88:31 - this spooky Bruce Willis film is called The Sixth Sense
88:34 - and millions and millions of people have seen it.
88:37 - In China, it's actually called He's a Ghost,
88:40 - which is just a much more direct name.
88:42 - And without wanting to give away spoilers,
88:45 - it's actually a title that gives away key points of the plot.
88:48 - So ideas for titles will vary.
88:51 - Now we know that we can adjust the kind of outcomes
88:54 - we get from OpenAI with how we phrase our prompts.
88:58 - And we also know that we can give examples
89:01 - and that will also tweak the kind of responses we get.
89:04 - But now I want to introduce a new property,
89:07 - which is called temperature.
89:09 - And we're just going to add that property right here
89:12 - to the object that we pass to OpenAI.
89:14 - Before we dive in and start using temperature,
89:17 - let's just have a quick bit of theory about what it does.
89:21 - So we know that OpenAI works by predicting the likelihood
89:25 - of one language chunk or token following another.
89:29 - So if we had a phrase like the dog,
89:31 - well, OpenAI could complete that as the dog was sleeping,
89:35 - the dog was hungry, the dog bit me.
89:38 - There are millions and millions of possibilities.
89:41 - What temperature does is it controls how often the model
89:45 - outputs a less likely token.
89:47 - So what it's doing is giving us some control
89:50 - over whether our completions are safe and predictable
89:54 - on the one hand or more creative and varied on the other hand.
89:58 - Now temperature is set from zero to one in increments of 0.01,
90:04 - although you are going to find it really hard
90:06 - to see the difference with small changes.
90:09 - Now it defaults to one, so all of the API requests
90:12 - we've made so far in our app have used
90:15 - the default temperature of one.
90:17 - Now lower temperatures, for example, at zero,
90:20 - will use fewer less likely tokens.
90:23 - So the completions will be less creative and more predictable.
90:27 - And this is absolutely great if you're looking
90:29 - for factual responses.
90:31 - Now higher temperatures will use more less likely tokens.
90:35 - So in doing that, you're going to get more variety
90:38 - and more creativity.
90:39 - Now I'm wondering if my titles are a bit too wacky.
90:42 - So I'm going to come in here and I'll actually
90:44 - set the temperature to 0.7.
90:48 - And let's save that and give it a try.
90:50 - So I'm using the same outline as before.
90:53 - And here we are.
90:54 - We've got a title back, The Power of Love,
90:57 - Billy Biggs Time Travel Adventure.
90:59 - OK, that is a pretty good title.
91:01 - Now I have to reiterate, it's very, very hard
91:04 - to compare completions because there's
91:06 - so much variety anyway.
91:08 - I could experiment more.
91:10 - I could take this down to zero.
91:12 - But to be honest, I'm actually happy with what
91:14 - I've got right here.
91:15 - So I'm going to leave temperature alone.
91:17 - But why don't you have a play around with it
91:20 - and see how you like the results,
91:21 - see if you get a big difference or just a small difference.
91:24 - And remember how we're generating our title.
91:27 - We're actually using the synopsis.
91:29 - And the synopsis was generated with a temperature setting
91:32 - of one by default.
91:34 - So if you lower the temperature here in Fetch Title
91:37 - and you're still getting something
91:38 - which is too weird for you, try actually
91:40 - changing the temperature property in this function
91:43 - right here.
91:44 - So there's no right or wrong way to do this.
91:47 - It's just a matter of taste.
91:49 - And of course, it's important to know about the temperature
91:51 - property and how it works.
91:53 - Now we've come a long way.
91:54 - There's only two more things that we need to do with this app.
91:58 - We need to get the cast.
91:59 - And we need to generate an image.
92:01 - So when you're ready for that, let's move on.
92:08 - In our finished product, we're going
92:09 - to have a star-studded cast so movie industry
92:12 - insiders can better visualize our story.
92:15 - And this is going to give us a great chance
92:17 - to look at text extraction using OpenAI.
92:20 - If we have a look at our Fetch Synopsis prompt,
92:23 - in this example, we have actually
92:25 - got the actor's names in brackets after each character.
92:30 - But we're not always seeing that in the output.
92:33 - In this screenshot we are, we've got Jeff Bridges
92:35 - and Emily Blunt.
92:37 - But in this screenshot, we're actually not getting that
92:39 - at all.
92:40 - We've got the characters, but no names for suggested actors.
92:44 - Now that's probably being a bit inconsistent because we're not
92:47 - specifically asking for it.
92:50 - Although we've got it in the example,
92:51 - we haven't got it in the instruction.
92:53 - So here's a very quick mini challenge for you.
92:56 - Ask for actors' names in brackets after each character.
92:59 - And I've just put here, you could also
93:01 - suggest that OpenAI thinks of actors that would particularly
93:05 - suit the role.
93:06 - OK, pause now and quickly sort out that challenge.
93:16 - OK, hopefully you got that to work just fine.
93:19 - I'm just going to add a sentence onto the end of the instruction.
93:22 - The synopsis should include actors' names in brackets
93:25 - after each character.
93:27 - Choose actors that would be ideal for the role.
93:30 - OK, let's save that and see if it's worked.
93:33 - So I'll put an idea in here and we'll press Send.
93:37 - And there we are.
93:38 - We've got our synopsis.
93:39 - And we can indeed see that we're getting the actors' names
93:43 - in brackets after each character.
93:46 - Now I've tested this and it is very consistent.
93:49 - OK, the next thing that we need then is a function
93:52 - so we can extract the stars' names from this text
93:55 - so they can be listed out right here just above our synopsis.
94:00 - You could do that with vanilla JavaScript,
94:03 - but it's actually going to be easier to let OpenAI do that
94:06 - for us.
94:07 - So I'm going to come down here then.
94:10 - And after we've generated the synopsis,
94:12 - at the same time as we ask for the title,
94:15 - I'm going to call another function called FetchStars.
94:19 - And of course, we'll pass in the synopsis.
94:21 - Now I'll come down here.
94:23 - And because you've done this lots and lots of times,
94:26 - I'm going to do all of the heavy lifting here
94:28 - and set up that function minus the prompt.
94:31 - Now we should set some max tokens here.
94:34 - I think we don't need very many.
94:36 - 30 will be more than enough.
94:39 - And I'm not actually going to set a temperature here.
94:41 - I've tested this.
94:42 - And it really doesn't make much difference at all.
94:45 - So we might as well leave it at its default.
94:48 - Now we can see on the screenshot that we want the stars right here.
94:51 - Let's just check the HTML.
94:54 - And we've got this H2 element with the ID of output-stars.
94:58 - So that is where we're going to render our stars.
95:02 - And of course, this function needs a parameter.
95:05 - OK, now it's time for a challenge.
95:07 - And I'm going to paste it in right here just above the prompt.
95:11 - I want you to use OpenAI to extract the names
95:14 - in brackets from our synopsis.
95:17 - Now I've left that challenge wide open because I
95:20 - think we've done enough prompt engineering
95:22 - by now for you to be able to figure out how to do this.
95:26 - If you feel ready for that, pause now.
95:28 - Go ahead, get it sorted.
95:29 - It's always best to try and figure things out on your own
95:32 - as much as possible.
95:33 - But if you'd like a few more pointers,
95:35 - or if you've tried it and you've got stuck,
95:37 - I have put a hint file up here.
95:40 - It's called hint.md.
95:41 - And it's just got a couple of pointers
95:43 - that will push you in the right direction.
95:45 - OK, pause now.
95:46 - Get this challenge sorted.
95:48 - Do some experimentation.
95:50 - Take all the time you need.
95:51 - And I'll see you back here when you've got it working
95:53 - and we'll have a look together.
96:01 - OK, hopefully you got that working just fine.
96:04 - So I'm going to come in here and I'm
96:06 - going to start off with a simple instruction.
96:09 - Extract the names in brackets from the synopsis.
96:13 - That should do the trick because OpenAI
96:15 - is more than capable of recognizing names and brackets.
96:20 - But what I do want to do, though, is add an example.
96:23 - So I'm going to come down here onto a new line
96:26 - and I'll use a divider.
96:27 - So that will be the triple hashtag.
96:29 - And now I'm actually going to take the example
96:32 - that we've got up here.
96:33 - It's this Top Gun one.
96:35 - So we can borrow all of that.
96:37 - And I'll just paste it in here.
96:39 - Now we've got all of the names in brackets.
96:42 - So let's just show OpenAI what it is we're looking for.
96:45 - So I'll say names and then a colon.
96:48 - And then I'll just list out all of the names from the brackets.
96:52 - So Tom Cruise, Val Kilmer, and Kelly McGillis.
96:55 - And if we have a look back at the slide,
96:57 - we're actually getting a comma separated list here.
97:01 - And that is actually what we want.
97:03 - So those commas are there for a reason.
97:06 - Now let's come onto a new line again and I'll use a divider.
97:10 - And now let's just put synopsis colon.
97:13 - And this is where we want to pass it,
97:15 - the synopsis that we're bringing in right here.
97:19 - And then underneath that, names.
97:21 - And we'll leave that blank
97:23 - because that is where OpenAI is going to do its thing.
97:27 - Okay, let's hit save and we'll see if it's working.
97:30 - I'll paste an idea in and hit send.
97:33 - And let's check out our results.
97:35 - So we've got a title and there we are.
97:37 - We have got our stars, Al Pacino, Kevin Hart,
97:41 - and Dwayne Johnson, what a lineup.
97:43 - That I think will just be a really, really cool film.
97:47 - And we've also got a decent synopsis.
97:50 - Now the final thing that we need
97:51 - to finish this app off is an image.
97:54 - But before we can do that,
97:56 - we need to learn about generating images with OpenAI.
97:59 - That's a little bit different
98:00 - to everything we've done so far.
98:02 - So let's investigate that next.
98:04 - Okay, let's take a look at generating images with OpenAI.
98:09 - So you've probably seen OpenAI's image generation tool,
98:13 - DALI, and if you haven't,
98:15 - you really should take a moment to go and play with it.
98:17 - This link is of course clickable.
98:19 - It will take you straight there.
98:20 - It really, really is a lot of fun.
98:23 - Now, as well as the DALI playground,
98:25 - we can access the OpenAI image API,
98:28 - which allows us to generate images in our application.
98:32 - And I'm actually building one such application right here.
98:35 - It's a really simple, fun game.
98:37 - All you have to do is describe a famous painting
98:40 - without saying the name of the painting
98:42 - or the name of the artist.
98:44 - So if I wanted to generate an image
98:46 - of the most famous painting in the world,
98:49 - I could say something like,
98:50 - a 16th century woman with long brown hair
98:52 - standing in front of a green vista with cloudy skies.
98:56 - She's looking at the viewer with a faint smile on her lips.
98:59 - Looking at the viewer with a faint smile on her lips.
99:02 - And if I give that description to OpenAI,
99:05 - hopefully it's going to give me a good likeness
99:07 - of the Mona Lisa,
99:08 - which is going to appear right here
99:10 - inside this picture frame.
99:12 - Now, all of these CSS and HTML for this app is already done.
99:16 - We just need to finish off the JavaScript
99:18 - and there's nothing unfamiliar happening here.
99:20 - The user is going to input their image description
99:22 - right here, click create.
99:24 - There's an event listener on that button
99:27 - and it is going to call generate image,
99:29 - generate image is going to call the OpenAI image API.
99:34 - So let's go ahead and set up a response right here.
99:37 - And up until now,
99:38 - this is where we've been using the create completion endpoint.
99:41 - And now we want to use the create image endpoint.
99:45 - Now, just like with create completion,
99:47 - we need to pass an object with a set of properties,
99:50 - but actually these are not the same properties
99:53 - as we've used before with create completion.
99:55 - We actually don't need to specify a model.
99:57 - We don't need to give it max tokens
99:59 - or temperature.
100:00 - Let's just have a quick look at the properties
100:02 - that we do need.
100:03 - So first up, we need a prompt.
100:05 - This will be a description of the image
100:07 - and we'll go into more detail about prompt writing
100:09 - for images in just a moment.
100:11 - The second property is N and N just stands for number
100:15 - and it controls the number of images
100:18 - we get back from OpenAI.
100:20 - Now we can pass it an integer between one and 10.
100:24 - So the maximum number of images we can get in one go is 10
100:28 - and it will actually default to one.
100:30 - So strictly speaking, I didn't need to add it here,
100:33 - but it's really, really important that you know it's there
100:35 - because in the future you might want to work
100:37 - with multiple images.
100:39 - Next up, we've got size and size takes in a string
100:43 - and that string is going to hold the size in pixels
100:46 - of the image we want.
100:48 - We've actually got three choices here.
100:50 - We can have 256 by 256, 512 by 512 or 1024 by 1024.
100:55 - Now the default here is the big one 1024 by 1024.
101:00 - And remember, bigger images cost more credit.
101:04 - So what you don't want to do is just always leave
101:06 - that at the default, take the biggest images
101:09 - and then resize it to something much smaller with CSS.
101:12 - That's really uneconomical.
101:14 - So just be careful to go for the image size that you want.
101:17 - Today we'll be going for the smaller image.
101:20 - And the last property we've got here is the response format
101:24 - and that is also a string.
101:26 - And the string can either be URL or B64 underscore JSON.
101:30 - So what this is giving us is the format of the completion.
101:34 - If it gives us a URL, we can just use that URL
101:37 - as a source inside an image element.
101:40 - And actually this will default to URL.
101:43 - And when you're doing the challenges,
101:44 - I recommend that you use URL.
101:47 - But there's a bit more we need to say about response format.
101:50 - Firstly, you need to be aware that OpenAI image URLs
101:54 - only last for one hour.
101:56 - So if you want to keep an image, you need to download it.
101:59 - As I'm recording this
102:00 - and my images need to last longer than an hour,
102:03 - I'm actually going to use this B64 underscore JSON method.
102:07 - And this is going to give me an encoded PNG image
102:10 - so I don't need to rely on OpenAI's URLs.
102:14 - Now, if you've never worked
102:15 - with a base 64 encoded image before,
102:18 - all it is is a massive chunk of code
102:21 - which the browser can interpret as an image.
102:24 - This is one that I've just pasted into VS code
102:26 - and it is absolutely huge.
102:29 - If you try and log this out in Scrimba,
102:31 - you'll likely actually crash the editor.
102:33 - You can search online for base 64 image to PNG conversions
102:38 - and you'll find plenty of sites
102:39 - where you can just paste in all of this code
102:42 - and it will just give you an image.
102:43 - But what's also important to know
102:45 - is that you can add a little bit of code
102:47 - just before the source in the image tag
102:49 - to tell the browser to expect a base 64 encoded image.
102:53 - And we're going to see that in just a moment.
102:56 - A quick word about prompt design.
102:58 - Prompt writing for images is actually less complex
103:00 - than the prompt writing we've done for text so far.
103:03 - All we really need to do is describe what we want in detail
103:07 - in a maximum of 1000 characters.
103:10 - Now, the more detailed the description,
103:11 - the more likely you are to get back
103:13 - the results that you want.
103:14 - So consider this, if you ask for a white dog,
103:17 - that's a bad description.
103:19 - You're not giving any detail at all
103:21 - and you're exerting no control
103:23 - over what you're going to get back.
103:24 - If on the other hand, you ask for a close-up
103:27 - studio photographic portrait of an old English sheepdog,
103:30 - well, that is a good description
103:32 - and then you can really start to imagine
103:34 - what you're going to get back
103:35 - and you'll actually find with that level of detail,
103:37 - it's quite easy to exert control over the images
103:41 - you get back from OpenAI.
103:43 - Let's get the rest of this coded out
103:44 - and then we can actually see it all in action.
103:47 - So I'm going to come in here
103:49 - and the first thing that we need is a prompt
103:52 - and that prompt is going to be
103:53 - whatever we've brought in here as a parameter
103:55 - which is whatever the user has inputted into the box here.
103:59 - Next, we need N and we only want one image today
104:02 - so I could leave out N, it does default to one
104:06 - but as I said, I just want to keep reminding you
104:08 - that it is there for when you need it.
104:11 - Next, we need size and that one is a string
104:15 - and I'm going for the smallest option
104:17 - which is 256 by 256
104:19 - and that is just a lowercase X right there in the middle.
104:23 - Now lastly, we want the response format
104:26 - and again, this is a string
104:28 - and I'm just going to set it to URL.
104:30 - Now that is the code that it's best for you to use
104:33 - when doing these challenges.
104:34 - You're going to get back a URL
104:36 - and I've already set up this image element right here.
104:40 - The source is using the URL
104:42 - and therefore the image is going to appear right here.
104:45 - As I said before, I can't do that.
104:48 - I actually need to use base64json
104:51 - so that's B64 underscore JSON
104:54 - and that does mean that I won't be taking the URL
104:57 - because actually this is not going to give us a URL.
105:00 - It's actually going to give us this base64 encoded image
105:03 - so I'm going to change URL to B64 underscore JSON.
105:07 - Now I would log that out to show you
105:09 - so you could see the full response
105:11 - but actually the base64 encoded JSON is so big
105:15 - as I think I said before, it actually crashed the browser
105:18 - so I won't do that but feel free to experiment.
105:21 - Now there's one last thing that I need to do
105:23 - to make this work.
105:25 - You're not going to need to do it
105:26 - if you're using the URL format
105:28 - but I'm just going to paste a little bit of code
105:30 - right in here and what this code does
105:33 - is it just tells the browser that what's coming up
105:37 - is actually a data version of an image.
105:40 - It's a PNG image and the data type is base64.
105:44 - Without that code right there, this will not work.
105:48 - So let's save it and see if we can describe
105:52 - the most famous picture of all time.
105:54 - So I'm putting in here a 16th century woman
105:57 - with long brown hair standing in front of a green vista
105:59 - with cloudy skies.
106:00 - She's looking at the viewer with a faint smile on her lips.
106:03 - I've got the original ready down here for comparison.
106:06 - Let's hit create.
106:08 - And there we are and that is actually not bad at all.
106:11 - In fact, I think it's pretty hard to tell
106:13 - which one is the original and which one is my creation.
106:17 - That is a good likeness of the Mona Lisa.
106:21 - Now in a way I've been quite lucky here.
106:23 - You do have to work quite a bit with image prompts
106:25 - if you've got a really exact idea of what you want
106:28 - and it does all just come down to being descriptive
106:30 - and being detailed and you'll be really surprised
106:33 - actually by how much open AI knows about styles.
106:36 - You can talk about impressionism
106:38 - or the style of Matisse or Picasso.
106:40 - You can talk about different lights, shades and hues.
106:43 - You can talk about anime and manga.
106:45 - Just go into as much detail about the image as you want to.
106:49 - Now I'm going to leave you to play with this.
106:51 - Hopefully you can describe a few more famous paintings.
106:54 - You might even do better than I've done.
106:56 - When you're ready, go back to the app.
106:57 - We're going to put these image generating skills
106:59 - to good use, perhaps not quite in the way
107:02 - you'd expect us to actually.
107:03 - All will be revealed in the next scrim.
107:12 - We need an iconic image to complement
107:14 - our synopsis and title.
107:16 - These ones from Jaws and Star Wars are really, really cool.
107:20 - And in an ideal world, we too could have cover art
107:23 - that featured our star-studded cast
107:25 - and had the title of the film right there
107:28 - blazing across the cover.
107:29 - Now, unfortunately, OpenAI is not
107:33 - going to generate recognizable famous faces for us.
107:37 - And also, it can't reliably add text to images.
107:41 - So what we're going to do instead
107:43 - is focus on illustrating aspects of the plot.
107:47 - We can still create some funky images.
107:49 - And the app is going to end up looking something like that.
107:53 - Now we know how to get images.
107:56 - And what we could do quite easily is just come down here
107:59 - and add an input field to the app
108:01 - and have our users write an image prompt.
108:03 - But the whole point of AI is that it's labor saving.
108:07 - So what we want to do is actually
108:09 - use the title and synopsis to generate an image prompt
108:12 - for us.
108:13 - And that is a key power of AI.
108:15 - You can use what you've generated
108:17 - to generate something else.
108:19 - So it's a kind of chain reaction.
108:20 - Now, we're going to use two functions to do this.
108:24 - The first function is going to generate the prompt.
108:27 - And the second function is going to use that prompt
108:29 - to generate an image.
108:31 - So I'm going to come down here and set up a new function
108:34 - called fetch image prompt.
108:36 - Now, fetch image prompt should take in both the title
108:39 - and the synopsis.
108:41 - And I'm going to call this function
108:43 - from the fetch title function that we've got right here.
108:47 - Fetch title takes in the synopsis.
108:50 - And it gets us the title.
108:52 - And so what I think we'll do to keep things a bit tidy
108:55 - is actually set up the title on a const right here,
108:59 - because we're actually going to use the title twice.
109:02 - We'll use it to update the DOM.
109:04 - And now we'll use it when we call the fetch image
109:07 - prompt function.
109:09 - So we'll pass in title and synopsis.
109:12 - OK, now before we generate any images,
109:14 - we should actually check in the console what prompt we get back
109:18 - to make sure it's good.
109:19 - So for now, I'm just going to log out the prompt
109:22 - that this function generates.
109:24 - And now, of course, we need to build the AI call.
109:27 - And again, I'm going to do most of the heavy lifting
109:29 - before I set you the challenge for the prompt writing.
109:32 - So we're using the same model.
109:34 - I've left the prompt blank for now.
109:36 - And I've set max tokens to 100, which should be easily enough.
109:41 - And now let me paste in a challenge.
109:44 - OK, so this is your challenge.
109:46 - I want you to write a prompt that
109:48 - will generate an image prompt that we
109:50 - can use to get the artwork for our movie idea.
109:54 - OpenAI has no knowledge of our characters, of course.
109:58 - So the image prompt needs descriptions, not names.
110:01 - What do I mean by that?
110:03 - Well, if we have a character in the synopsis called Katie,
110:06 - and maybe she's the protagonist, and our image prompt
110:10 - says Katie is standing in front of a burning building, well,
110:14 - of course, the image API has no idea
110:16 - who Katie is, no idea what she looks like.
110:19 - So what we need is a description of Katie and what she's doing.
110:23 - So maybe how tall she is, what she's wearing.
110:26 - Perhaps she's carrying an axe or some other weapon
110:29 - and is standing in front of a burning building.
110:32 - I've set this as quite an open challenge, and deliberately so.
110:35 - I want you to do some experimentation here.
110:38 - But I will say that you might find it best to use examples.
110:42 - So I have put some examples up here in this examples.md file.
110:47 - But do, of course, feel free to write your own examples
110:50 - or go online and find some synopses and titles
110:54 - of your favorite films.
110:56 - And perhaps you can use them as well.
110:58 - OK, pause now, get this challenge sorted,
111:01 - and I'll see you back here in just a moment.
111:10 - OK, hopefully you managed to do that just fine.
111:12 - So let's come in here and start working on the prompt.
111:17 - I'm going to start off with an instruction.
111:20 - I'm going to say, give a short description
111:22 - of an image which could be used to advertise a movie based
111:26 - on the title and synopsis.
111:28 - Now, I'm going to add to that, the description
111:31 - should be rich in visual detail, but contain no names.
111:36 - Now, this second sentence may or may not be effective.
111:41 - OpenAI prefers to be told what to do, not told what not to do.
111:46 - But I've had reasonable success with this,
111:48 - so I'm going to phrase it like that.
111:50 - Now, I'm going to add some examples.
111:53 - And I actually put three examples in the examples.md file,
111:57 - but I'm only going to use two.
111:59 - I think two is enough.
112:02 - So first, I'll use the separator,
112:04 - and now I'll paste in my examples.
112:07 - So let's just tidy that up and add any more
112:09 - separators that we need.
112:11 - Remember, what we want to do is separate the instruction
112:14 - from the examples and actually separate each example as well.
112:19 - And of course, the important part
112:21 - is to include our title, our synopsis,
112:23 - and leave OpenAI a space to create the image prompt.
112:28 - OK, now, just as I was doing that,
112:30 - I realized that up here, I've asked
112:32 - for a short description of an image which
112:35 - could be used to advertise a movie based
112:36 - on title and synopsis.
112:38 - And then down here, I've used image description as two words.
112:42 - Image description hyphenated and image description hyphenated.
112:46 - Now, OpenAI can probably cope with that kind of typo,
112:49 - but it might be worth just looking out
112:51 - for that kind of thing and just making sure we are consistent.
112:55 - So I'm just going to change both of those to two words.
112:58 - Now, I'm going to put temperature to 0.8.
113:02 - And I'm doing that because I found
113:03 - that some of my image prompts were just a little bit too
113:06 - wacky when it was left at the default one, which
113:10 - is the most creative setting.
113:12 - Now, it's really hard to tell with temperature
113:14 - whether you're making a big difference or not sometimes.
113:17 - It might just have been that the ideas that OpenAI gave me
113:20 - at that time were just particularly strange.
113:23 - Now, it can be really hard to tell sometimes
113:25 - if small temperature changes are making much difference.
113:29 - It could just be that the examples I got before
113:32 - happened to be particularly strange.
113:35 - But that said, I've tested this a few times,
113:38 - and 0.8 seems to be about right.
113:41 - OK, so let's hit Save, and we can test it
113:44 - and see what we get.
113:46 - So I'll press Send.
113:48 - And now let's open up the console
113:50 - because that is where our image prompt is going to appear.
113:52 - And there we are, a colorful image
113:54 - of a raccoon, elephant, alligator, and squirrel
113:57 - standing in the middle of a city street back to back facing
114:00 - menacingly towards the viewer.
114:02 - Around them, robotic and alien forces
114:05 - march in the background, weapons drawn.
114:07 - In the sky above, a giant robot claw looms ominously.
114:11 - Now, I think that is a really, really nice image prompt.
114:15 - Now, if you approach this challenge differently
114:17 - and you've got different results which you're still
114:19 - happy with, that is absolutely fine.
114:22 - There is no one right answer to this.
114:25 - OK, we've got an image prompt.
114:27 - So in the next scrim, let's go ahead and use it
114:29 - to generate an image.
114:31 - When you're ready, let's move on.
114:37 - OK, so it's time to get this image into place.
114:40 - We've generated a good prompt.
114:42 - So now I'm going to set up a new function called fetch image URL.
114:47 - And of course, that needs to be an async function.
114:50 - Now, to fetch the URL, it's going to need a prompt.
114:53 - And we generated the prompt.
114:55 - And we've got it back in this response right here.
114:58 - So it's going to take in a parameter.
115:00 - And I'm just going to call that parameter image prompt.
115:03 - Now, if we have a look at the HTML,
115:06 - we can see that down here inside the output section,
115:10 - we've got this div.
115:11 - And it's got the class of output image container
115:14 - and the ID of output image container.
115:16 - Well, that is, of course, where we're going to render the image.
115:19 - So let's come back to our function.
115:22 - And I'm just going to add one line of code right here.
115:26 - Now, the source of that image is actually
115:28 - going to be the URL that we get back from this fetch request
115:32 - to the API.
115:34 - So I'm actually going to leave that empty for now,
115:37 - because that is going to form part of your challenge.
115:40 - But before we get onto that challenge,
115:42 - there's just a couple more bits to do.
115:44 - I want to call this function from inside this fetch image
115:48 - prompt function.
115:49 - So I'll do that right here.
115:51 - And we need to pass in our image prompt.
115:53 - Well, we've got that right here.
115:55 - So let's just take that and paste it in there.
115:57 - And I'm just going to delete this console.log.
116:00 - Now, the other thing that I'm going to do
116:02 - is just comment this line of code once again.
116:06 - So that is just the display none on the output container.
116:10 - So we'll be able to see the image right there.
116:13 - OK, now bear with me while I paste in your challenge.
116:16 - So this is your challenge.
116:18 - Use the image prompt to generate an image.
116:20 - The image should be 512 by 512 pixels in size.
116:25 - We only want one image, and we want to get a URL back
116:29 - from OpenAI.
116:30 - Think about what properties you need
116:31 - to put in the object that you're going to pass to OpenAI.
116:35 - Now, one word of warning.
116:36 - If you find a lot of garbled text in your images,
116:39 - you could specifically request an image with no text.
116:43 - And what I'm talking about is this.
116:45 - Sometimes, OpenAI gives you images
116:47 - which have got this garbled, meaningless text.
116:51 - And this image has also got a really dodgy white border.
116:53 - So there's literally nothing to like about it.
116:56 - So you could just try adding to your prompt
116:58 - a sentence which says, I don't want any text in the image.
117:01 - OK, pause now.
117:02 - If you need to flip back to the scrim before last
117:04 - to refresh your memory, go for it.
117:07 - Get this challenge sorted, and I'll see you back here
117:09 - in just a minute.
117:15 - OK, hopefully you managed to do that just fine.
117:18 - So I'm going to come in here, and I'll set up my response.
117:23 - And we're going to await OpenAI.createImage.
117:28 - And we know we need to pass it an object.
117:31 - And the first thing we'll put in that object is a prompt.
117:35 - And the prompt is going to be this image prompt
117:37 - parameter we've got here.
117:38 - But I do want to add a sentence to try and avoid
117:42 - this garbled text on the image.
117:44 - So I'm actually going to put this in curly braces.
117:47 - We'll have the dollar sign, and we'll wrap it in backticks.
117:51 - And I'm just going to add there should be no text in this image.
117:55 - OK, next I'm going to add an n property.
117:58 - And we don't really need to do that.
118:00 - Again, just a reminder for you, we only want one image.
118:03 - And that is what we'll get by default.
118:05 - Now, for the image size, we wanted 512 by 512.
118:10 - And the response format will default to URL anyway.
118:13 - But let's just write it out.
118:16 - And all we need to do now, then, is deal with what we get back.
118:20 - And we're going to put it right here inside the source
118:22 - for this image.
118:23 - And in fact, I'm just going to delete this challenge text,
118:26 - so we've got a little bit more space.
118:27 - So the image element is already inside backticks.
118:30 - So I can come in here with the dollar sign and the curly
118:33 - braces.
118:34 - And it will be response.data.data.
118:38 - And I'll just close the mini browser,
118:39 - because this is getting kind of long.
118:41 - And we actually want the element at position zero.
118:45 - And from the object store there, we want the URL.
118:48 - Now, when you tried that, it should have worked perfectly.
118:51 - As I said before, I can't use this format in the Scrimba
118:54 - recorder, because the image will have disappeared by the time you
118:57 - actually get to watch this.
118:59 - So I'm going to make a couple of changes here.
119:01 - I'm going to change this to B64 underscore Jason.
119:05 - And I'm also going to go for a slightly smaller image,
119:09 - because actually, the encoded images are quite big.
119:11 - It's a little bit overwhelming for the mini browser, which
119:14 - can actually crash sometimes if you give it too much data.
119:17 - So I'm just going to go for the 256.
119:21 - A little bit more to do.
119:22 - I just need to tell the browser that this image is
119:25 - coming in a data format.
119:27 - And I also need to update this.
119:29 - It won't be a URL property.
119:30 - It will be B64 underscore Jason.
119:34 - OK, let's hit Save.
119:36 - And let's put a one sentence outline right here.
119:39 - And it's one we've seen before.
119:40 - It's the time traveler going back to the 80s and falling
119:43 - in love.
119:44 - Let's hit Send.
119:46 - OK, and we have our image.
119:49 - Now, the first thing to mention is
119:51 - this is a little bit small, because I've
119:53 - gone for a smaller image size.
119:54 - So I'm just going to make a little tweak to the CSS.
119:58 - I'm going to come in here where we've
120:00 - got the output image container, and we're
120:02 - looking for an image within that container.
120:04 - And I'm just going to change max width to width.
120:08 - OK, so everything is working.
120:12 - We've got a title.
120:13 - We've got an image.
120:14 - We've got some stars, and we've got a synopsis.
120:17 - So I think what we need to do now is put that to display none.
120:22 - And we just need to make a few little updates to the UX.
120:26 - Now, if you recall, what we want to happen at the end
120:29 - is that we get this view pitch button, which will then
120:32 - display whatever OpenAI has given us.
120:35 - So what I'm going to do is come down here into this function,
120:39 - and we're going to add some more logic right here.
120:42 - And the first bit of logic is just
120:43 - going to take this container that we've
120:45 - got here, which is currently holding the text
120:49 - area where we write and then this loading SVG,
120:53 - and it's just going to replace it with this pink button.
120:57 - So let's go ahead and put that right in here.
121:02 - Now, the next thing we need to do
121:03 - is actually wire up this button.
121:05 - So I'm going to set up an event listener right here.
121:09 - And what we'll do then is we'll set the setup container
121:13 - to display none.
121:15 - So that's the entirety of this section right here.
121:19 - So that is this entire white container
121:21 - you see in the mini browser.
121:23 - So let's take control of that container,
121:27 - and then we'll access its style and display,
121:30 - and we'll set that to none.
121:32 - Next, we want to take the output container, which
121:35 - is this one right here, and that is the one which
121:38 - is set to display none by default in the CSS.
121:41 - And we want that set to display flex.
121:45 - Now, while we're here, I think we
121:47 - should actually quickly update the message
121:49 - that we get from the movie boss at the very end.
121:52 - He should make some outrageous claim about his own ability
121:55 - and ask for some money.
121:57 - So we've already selected the movie boss text.
122:01 - So let's set the inner text of that to the following.
122:05 - He's going to say, this idea is so good, I'm jealous.
122:08 - It's going to make you rich for sure.
122:10 - Remember, I want 10%.
122:12 - OK, now for the moment of truth, let's
122:14 - see the whole thing in action.
122:17 - I'm going to say a pizza delivery rider infiltrates
122:21 - an organized crime group.
122:24 - Let's hit Save and cross our fingers.
122:26 - So we get the first message.
122:28 - We get the personalized response.
122:32 - Now we've got the View Pitch button,
122:34 - and we're getting an error.
122:35 - I've made a really silly mistake.
122:36 - You can probably see what it is.
122:38 - Just pause now and debug that.
122:42 - OK, so you probably noticed I've actually
122:45 - forgotten to put the inverted commas around these two.
122:48 - Let's try that again.
122:51 - There's the first message.
122:53 - And here is the View Pitch button.
122:56 - And there we are.
122:57 - We have got our image.
122:59 - We have got pizza delivery man, the Loire Vitale story.
123:03 - OK, that sounds pretty classy.
123:05 - It's John Travolta and Uma Thurman.
123:08 - That's a pretty good mix if you like Quentin Tarantino films.
123:11 - And let's check out the synopsis.
123:14 - And actually, that synopsis is pretty cool.
123:16 - I would go and see that movie.
123:19 - OK, so everything is working.
123:21 - So let's just take one more scrim
123:23 - to recap what we've done with this project
123:25 - and where you can go from here.
123:32 - Congratulations on finishing Movie Pitch.
123:35 - Now you've got all of the foundations
123:37 - you need to work with the OpenAI API in your apps.
123:42 - Let's just quickly recap what we've studied.
123:45 - So we set up the OpenAI API.
123:48 - And you can see that right here on lines 1 to 11.
123:52 - And we used the text DaVinci 003 model
123:55 - with the Create Completions endpoint.
123:58 - And you can see that down here on lines 25 and 26.
124:02 - Now we started off using the zero-shot approach.
124:05 - So we made a simple prompt request to the API
124:08 - with just a one-line instruction.
124:11 - We then upgraded that to the few-shot approach
124:14 - with multiple examples.
124:16 - And we can see that right here, lines 27 down to 39.
124:22 - And we used the few-shot approach
124:24 - to demonstrate to the API what sort of completion
124:28 - we were looking for.
124:29 - We also used the max tokens property.
124:32 - And you can see that right here.
124:34 - We did that to ensure the model had enough tokens
124:37 - to answer the query successfully.
124:39 - And of course, we also used the temperature setting.
124:42 - And we can see that down here on line 70.
124:45 - And that alters how daring our completions were.
124:48 - Higher temperatures allowed OpenAI's virtual imagination
124:52 - to go into overdrive.
124:53 - And lower temperatures kept things
124:55 - safer and more predictable.
124:57 - Now finally, if we come down here,
124:59 - we can see that we have got a slightly different endpoint
125:03 - in use right here.
125:05 - It's the create image endpoint.
125:07 - And we used it to generate images from text prompts.
125:11 - So wow, that was quite a lot.
125:13 - Now it's always a good idea to make a project your own.
125:16 - And there are multiple ways that you
125:18 - could take this project to the next level.
125:20 - Here are just a few ideas.
125:22 - So in terms of the JavaScript, at the moment,
125:24 - we're making a lot of API calls to the same endpoint.
125:28 - I wonder if you could find a way to think dry,
125:31 - do not repeat yourself, and have one function do the calling
125:34 - so we don't repeat the same line of code
125:38 - like this one that you see right here multiple times.
125:42 - So that would just involve a bit of neat refactoring.
125:45 - Now in terms of the AI, if you want
125:47 - to get anywhere in Hollywood or Bollywood,
125:50 - you're going to need a script.
125:52 - And the logical next step of this project
125:55 - is to have OpenAI create a script for your movie.
125:59 - Now that is a little bit tricky in terms of the amount
126:01 - of tokens you'd have to use.
126:03 - So you would have to break the process up into chunks.
126:05 - But it is doable.
126:07 - And then you could perhaps use the Create Image endpoint
126:10 - to create more detailed character sketches.
126:13 - Or you could tailor the app to a more specific genre,
126:16 - so have it create specifically manga or rom-coms.
126:19 - Now if you take these three ideas and sell the script,
126:22 - I want 5%.
126:24 - Now ultimately, the best way to consolidate your learning
126:27 - is to delete all of the code and start again from scratch.
126:31 - You can use it to practice your HTML, CSS, JavaScript,
126:35 - and your knowledge of the OpenAI API.
126:38 - Now of course, you wouldn't need to do it exactly the same way.
126:41 - You would do it as you think best
126:42 - and just justify the changes you make.
126:45 - So quite a lot of potential work to do there.
126:48 - But with the skills you've got, all of this is possible.
126:51 - Now whatever you do, make sure you take a break now
126:54 - and consolidate what you've learned before moving on
126:57 - to the next project.
127:03 - It's official.
127:04 - Chatbots are taking over the world.
127:06 - Loads of sites have them.
127:08 - And since the arrival of the more advanced chat GPT
127:11 - models, like text DaVinci 003, and now GPT 4,
127:15 - the model we're about to use, chatbots
127:17 - are going to skyrocket even more.
127:19 - And as they are one of the most common uses for AI in web dev,
127:23 - this course wouldn't be complete without one.
127:26 - So we're going to build a chatbot using
127:29 - GPT 4, the latest OpenAI model at the time of recording.
127:34 - And it looks like this.
127:35 - Know It All does exactly what it says.
127:38 - It answers your questions and converses with you
127:41 - at a human level.
127:42 - You can ask it anything and it will do its best to answer.
127:46 - Now I need to say a quick word about GPT 4.
127:48 - At the time of recording, you have
127:50 - to join a waiting list to get your hands on the GPT 4 API.
127:55 - Now I'm sure that will change in time.
127:57 - But if you haven't got GPT 4 API access now,
128:00 - you can click on this slide.
128:02 - It's a link which will take you to the waiting list sign up
128:04 - page.
128:05 - Now if you don't have access to the GPT 4 API yet, no problem.
128:10 - Everything we do will work with the GPT 3.5 Turbo model.
128:16 - And this is also a very, very capable model.
128:20 - So just take a note of that right now
128:22 - if you don't have the GPT 4 API yet.
128:24 - And then wherever in the course we use GPT 4,
128:27 - you can use GPT 3.5 Turbo instead.
128:32 - And then when you get the GPT 4 API access,
128:35 - you can just swap out GPT 3.5 Turbo for GPT 4
128:39 - as I'll be using in this project.
128:42 - So what exactly are we going to study?
128:44 - Well, we'll take our knowledge from the previous project
128:47 - and we're going to add in the chatbot specific syntax used
128:50 - by GPT 4 and the GPT 3.5 Turbo models.
128:54 - The syntax is exactly the same and it will work with both models
128:58 - just fine.
128:59 - We'll also look at how you can instruct the chatbot
129:01 - to behave in a particular way or have a specific personality.
129:05 - And then we'll look at something called presence penalty, which
129:08 - can control how likely the chatbot is to talk about new topics.
129:12 - And we'll also look at something called frequency penalty,
129:15 - which can control how repetitive the chatbot is
129:17 - in its choice of words and phrases.
129:20 - And finally, we'll change direction
129:22 - and look at how we can store our conversation in a database
129:25 - so it persists even if a user reloads the page
129:28 - or closes their browser.
129:30 - Now, I just want to give you a quick warning again
129:32 - that at the moment our API key is visible on the front end.
129:35 - So be sure to keep it safe, don't share it,
129:38 - and make sure you ignore the end file if you're
129:41 - publishing to a repo.
129:43 - OK, that's enough chat from me.
129:45 - I've already got some HTML and CSS prepared for this project.
129:49 - So let's take a look at that and then get this chatbot working.
129:56 - Let's take a look at the code we've already got.
129:59 - We've got a form component right here and a button,
130:02 - which inside a form component will act as a submit button.
130:07 - Now, this div up here, this is where the conversation unfolds.
130:11 - And in it, we've got this one hard coded message,
130:13 - how can I help you?
130:15 - And that is the message that we can see right here.
130:19 - Now, over in index.js, we've got exactly the same setup
130:23 - as before.
130:24 - These lines of code right here are the open AI setup,
130:28 - just like they were in the previous project.
130:30 - And remember, in terms of the API key
130:33 - that we're bringing in from this.m file,
130:36 - we will be looking at deploying projects with API keys hidden
130:39 - towards the end of the course.
130:40 - And always be sure not to expose your API key
130:44 - when you're deploying projects, sharing them,
130:46 - or publishing them to public repositories.
130:49 - Now here, we've taken control of the chatbot conversation div.
130:54 - And we can see that right here in the HTML.
130:57 - This is the ID right here.
130:58 - So it's referring to this div.
131:01 - And that is the div where the conversation takes place.
131:04 - Now, we've got that stored in this const.
131:06 - And I've declared that globally, as we'll
131:08 - need to use it in multiple functions.
131:11 - Now, moving on down, we've got this event listener.
131:14 - And it is listening out for any submit event.
131:16 - And that will be triggered by a user clicking on the button
131:19 - or pressing Enter to submit the form.
131:21 - When a submit event is detected, we build a new element
131:25 - with create element.
131:26 - We then add the necessary CSS classes
131:29 - and append it to the chatbot conversation div.
131:32 - Then we populate it with whatever the user has inputted.
131:36 - And at that point, we can go ahead and clear the input
131:39 - field.
131:41 - Now, this line of code right here just moves the dialogue
131:44 - down so the latest messages are always in view.
131:47 - Else, as the conversation grows, you'd
131:49 - keep having to scroll down manually.
131:51 - Now, this is quite a neat way of doing it.
131:53 - But you could also use scroll into view.
131:56 - But I found when you're running a mini browser in a big browser,
131:59 - that can sometimes fail.
132:01 - So I went for this approach.
132:03 - Finally, we have this render typewriter text function.
132:07 - As the name suggests, it will give a typewriter effect
132:10 - to the AI's text as it is rendered.
132:12 - Now, there are loads of ways to do typewriter text.
132:15 - Some you can do just with CSS.
132:17 - But I've gone for a JavaScript approach.
132:19 - And what happens in this function is, again, we
132:22 - create a new element.
132:23 - We add the necessary CSS classes.
132:26 - This one includes a blinking cursor class,
132:29 - which would just give a nice blinking cursor
132:31 - effect to the text as it's rendered.
132:34 - And in fact, you can see the CSS for that
132:36 - right at the end of the CSS file.
132:38 - We've got the animation right there.
132:41 - The speed at which each individual character is rendered
132:43 - is controlled by this set interval.
132:46 - And at the moment, I've got it set to 50 milliseconds.
132:50 - And you can, of course, change that
132:51 - if you would prefer a different speed.
132:54 - Now, we won't be working much with the rest of the CSS
132:56 - in this file.
132:57 - But do pause and check it out if you would like to.
133:00 - There's nothing particularly special going on.
133:02 - But we have got a bit of CSS grid up here
133:05 - where we deal with the chatbot header.
133:08 - So the CSS grid is just used for this layout up here.
133:12 - OK, so that is what we have so far.
133:14 - Let's move on and take an overview of how the AI is
133:16 - going to work in this project.
133:18 - It's a little bit different to what we've done so far.
133:21 - So let's look at that next.
133:26 - OK, let's take an overview of how the GPT-4 model works
133:30 - with chatbots.
133:31 - So we're going to use one function
133:33 - to make requests to the API.
133:36 - And in a conversation, we'll use that function multiple times.
133:40 - But we need to think about what we send in the prompt
133:42 - because there's actually a big problem for chatbots
133:44 - that we have to overcome.
133:46 - And I want to illustrate that for you right here.
133:48 - So what I've got here is a simple call to the API
133:52 - using the create completion endpoint and the text
133:56 - DaVinci 003 model, which is the model we
133:59 - used in the previous project.
134:00 - And I chose it here because it should look pretty familiar
134:02 - by now.
134:03 - And I can use it to illustrate the point
134:05 - that I want to make about chatbots
134:07 - without getting caught up in new syntax we haven't studied yet.
134:10 - Now, I'm going to come in here to this prompt.
134:12 - And I'm going to ask a question.
134:17 - Where were the 2001 Wimbledon tennis championships held?
134:21 - OK, let's call that function.
134:24 - And we'll hit Save and open up the console.
134:27 - And we get the answer.
134:29 - The 2001 Wimbledon tennis championships
134:31 - were held at the All England Lawn Tennis and Croquet
134:34 - Club in Wimbledon, London, England.
134:37 - OK, so it's a correct answer.
134:39 - Now let's ask it who won that year.
134:43 - And I'll hit Save.
134:45 - And it tells us the Philadelphia 76ers
134:47 - won the 1982 NBA championship.
134:51 - And this is what's called a hallucination.
134:53 - The AI makes up a linguistically plausible answer
134:57 - when it doesn't know the right answer.
135:00 - And we'll talk more about hallucinations
135:02 - later in this course.
135:03 - Now, it figures that it doesn't know the answer.
135:05 - The question who won it that year only
135:08 - makes sense in the context of the previous question, which
135:11 - had the keywords Wimbledon and 2001.
135:14 - And that exposes a big problem for chatbots,
135:18 - which is that models have no memory of past completions.
135:21 - And that means that all relevant information
135:24 - must be sent with each API request.
135:27 - So I need to refactor my second question
135:30 - to include all of the information
135:32 - that the model needs to know, i.e.
135:34 - who won Wimbledon in 2001.
135:38 - Let's run that.
135:40 - And it says Goran Ivanicevic won Wimbledon in 2001.
135:45 - And that is true, with the only problem
135:47 - being that it's assumed I was talking
135:49 - about the men's championships.
135:50 - The women's championships were won by Venus Williams
135:53 - that year, and she beat the Belgian Justine Ennan
135:56 - in three sets.
135:57 - So there's just a little example of the biases picked up
136:00 - by AI models as they're fed data from the open internet.
136:05 - OK, so when it comes to chatbots,
136:07 - in order for the model to interact properly so
136:09 - the conversation flows and remains logical,
136:12 - the model needs to know the context of the conversation.
136:15 - And we achieve that by sending the conversation as it exists
136:18 - so far with each request.
136:21 - So let's look at a diagram of how the main AI business logic
136:25 - is going to work with this chatbot.
136:28 - So the first thing that we'll need to do
136:30 - is store the conversation in an array.
136:32 - And that is what's represented here by this box.
136:35 - And this does have to be an array.
136:37 - The GPT-4 model and the endpoint we're going to be using
136:40 - have much stricter rules on syntax
136:42 - than the text DaVinci 003 model and the Create Chat
136:45 - completion endpoint.
136:47 - Now, once we've set up this array,
136:48 - we'll use some special OpenAI syntax
136:51 - to instruct the chatbot and tell it how we want it to behave.
136:54 - And then we'll store that instruction
136:56 - in an object at the first index of the array.
137:00 - And I've just put the curly braces either side
137:02 - of instructions just to make it clear
137:04 - that that will be an object and that this is an array of objects.
137:10 - So now we've got the instructions
137:11 - at the first index of the array.
137:14 - We will take some input from the user.
137:16 - And the first thing to do with that input
137:18 - is to render it to the DOM.
137:20 - Then we'll use some OpenAI syntax to format it.
137:23 - And again, we'll save it in an object in conversation array.
137:27 - Once that's done, we're going to send the entire conversation
137:30 - array off to the OpenAI API.
137:33 - And what we'll get back, of course, is the response.
137:36 - And what we'll need to do then is render the completion
137:39 - to the DOM and then place the completion in an object
137:42 - with the correct syntax that we'll be discussing shortly.
137:45 - And then we'll be adding it to conversation array.
137:49 - So right now, conversation array has got three objects.
137:52 - We've got the instructions, the user's input,
137:54 - and the first response from the API.
137:56 - And then as you can imagine, this process continues.
137:59 - We take the user's reply.
138:01 - We render it to the DOM.
138:02 - We place it in an object.
138:04 - We store that object in conversation array.
138:06 - And then we send it off to the API.
138:08 - We get back our response.
138:10 - And we render it to the DOM, place it in an object,
138:13 - and then store the object in conversation array.
138:16 - And that process continues with the user's response.
138:19 - And we just keep repeating as needed.
138:21 - And this can go on and on until you reach the token limit.
138:25 - But the token limit with the GPT-4 model
138:27 - is extremely generous.
138:29 - So it would have to be a very, very long conversation,
138:31 - indeed, to reach that limit.
138:34 - OK, that's the overview.
138:36 - Let's get to work.
138:40 - OK, so we need a place to store our conversation.
138:43 - This is going to be the single source of truth, as it were.
138:46 - The place where everything we ask of openAI
138:49 - and everything it replies to us is stored.
138:52 - So I'm going to come in here.
138:53 - And I'm going to set up a const called conversation array.
138:57 - And in fact, I'll abbreviate array to just R.
139:00 - And remember, this is going to store an array
139:03 - because the API needs to have this conversation
139:06 - as an array of objects.
139:09 - So let's look at the first object that we need.
139:11 - It's the object that holds the instruction.
139:14 - This is where we tell the chatbot how we want it to behave.
139:17 - Now, this instruction object consists of two key value pairs.
139:22 - And that will be the same for all of the objects in this array.
139:25 - They're going to follow a similar pattern.
139:27 - The first key will be role.
139:30 - And this should correspond to the value system.
139:32 - This is just telling openAI that what comes next
139:35 - is the instruction and not part of the conversation.
139:39 - The second key will be content.
139:40 - And this should correspond to a string with an instruction.
139:44 - And this is your chance to influence
139:46 - how the AI behaves and responds.
139:49 - So I'm going to come in here.
139:50 - And I'll set up the first object in this array.
139:53 - So the first key will be role.
139:56 - And we'll set that to the string system.
139:59 - And the second key will be content.
140:02 - And this will hold our instruction.
140:03 - And for now, I'm going to put something fairly mainstream.
140:07 - I've gone for you are a highly knowledgeable assistant.
140:10 - That is always happy to help.
140:12 - Now, later on in the project, when the chatbot
140:14 - is up and running, we will actually
140:16 - revisit this instruction and have some fun with it.
140:18 - But for now, let's keep things simple.
140:21 - OK, so we've got the conversation array
140:23 - and the instruction object set up.
140:25 - Now, let's deal with the user's inputted text,
140:28 - which we'll also be adding to this conversation array.
140:31 - So let's come on to that next.
140:37 - OK, so we have the instructions object in the array.
140:40 - And now we need to add the user's input.
140:43 - So in index.js, we've got this event listener listening out
140:48 - for a submit.
140:49 - And when it detects the submit, the anonymous function
140:52 - takes the user's input and saves it to this const user input.
140:58 - So what we need to do is take that incoming text
141:01 - and put it into an object and push it to conversation array.
141:05 - Now, we've already seen how the chatgpt model wants its objects.
141:09 - And we've got an example of it right here.
141:11 - We've got an object with two key value pairs, role and content.
141:15 - And what we're going to do next is actually very, very similar.
141:19 - The object is going to have two key value pairs.
141:23 - The first key will be role.
141:25 - And this should correspond to the value user.
141:28 - The second key will be content.
141:30 - And this should correspond to a string holding whatever
141:33 - the user has inputted.
141:35 - So right here inside the event listener's anonymous function,
141:39 - I have written you a challenge.
141:41 - I want you to push an object holding the user's input
141:44 - to conversation array.
141:46 - And then just log out conversation array
141:48 - to check it's worked.
141:50 - Now, I'm going to leave this slide right here
141:52 - so you know exactly what that object should hold.
141:55 - OK, pause now, get that sorted, and I'll see you back here
141:58 - in just a moment.
141:59 - OK, so hopefully, you managed to do that just fine.
142:03 - So I'm going to come in here, and I'm
142:06 - going to say conversation array, and I'll use.push.
142:12 - And we're going to push an object.
142:14 - And we know the first key will be role.
142:17 - And that will have the value of the string user.
142:20 - The second key will be content.
142:23 - And that will hold whatever object
142:25 - the user has inputted. OK, now let's log out conversation
142:29 - array and see if it's worked.
142:32 - So I'll open up the console, and I'll ask a question.
142:36 - And there we are.
142:37 - We are logging out conversation array.
142:40 - And you can see that we've got two objects.
142:43 - The first object has got the role of system
142:45 - and the content with our instruction.
142:47 - The second object has got the role
142:48 - of user and the content, what is your name,
142:51 - which is the question that I'm going to use.
142:53 - What is your name, which is the question that I just asked.
142:57 - OK, so that is perfect.
142:58 - We have got the conversation array growing.
143:01 - We've got the instructions object.
143:03 - We've got the user's inputted text.
143:05 - And the next step of the process is
143:07 - to send it all off to the OpenAI API.
143:10 - So in the next screen, let's look
143:11 - at how we actually use the Create Chat Completions
143:14 - endpoint, which is just a little bit different to the previous
143:17 - endpoints we've used.
143:18 - OK, so we've got our conversation so far,
143:21 - and it consists of two objects, the instructions object
143:24 - and the user input object.
143:26 - So let's work on sending them off to the API.
143:30 - So back in index.js then, when the event listener detects
143:35 - a submit, let's call a function called fetch reply.
143:39 - And I'll come down here and set the function up.
143:43 - And because this function will be called fetch,
143:46 - and because this function will be calling the API,
143:49 - this will be an async function.
143:52 - Next, let's store the response to a const
143:55 - and await the API call.
143:58 - Now, so far at this point, we've been
144:00 - using the create completion and create image endpoints.
144:04 - But now, we're going to use something different.
144:07 - Let's head over to the API docs and see what we can find.
144:10 - And this slide is, of course, a link to those docs.
144:14 - What I'm looking at here is the API reference
144:16 - for create chat completion.
144:18 - And there's a ton of info on this page,
144:20 - and we will be investigating it more soon.
144:23 - But I want to focus in on the code example
144:25 - they give us right here, and specifically this.
144:28 - It says openai.createchatcompletion.
144:32 - OK, let's go back to the code, and we're
144:34 - going to add create chat completion right here.
144:38 - And just like with the other endpoints,
144:40 - we're going to pass it an object.
144:42 - So in the next grim, let's head back to the docs
144:45 - and investigate what information this endpoint needs from us.
144:54 - OK, so let's complete the object we're
144:56 - going to send to the API.
144:58 - If we go back to the docs, we can
144:59 - see that two properties are required.
145:02 - They are the model and messages.
145:04 - Now, so far for the model, we've been using text da Vinci
145:07 - 003, which is a very capable model.
145:10 - But now we're going to use GPT-4.
145:14 - GPT-4 is the newest and most impressive openai model yet.
145:18 - It makes huge improvements on its predecessors,
145:21 - and that is why everyone's been talking about it.
145:24 - Now, if you're looking at these docs and thinking, wait there,
145:27 - it says GPT 3.5 Turbo here.
145:30 - Yes, the GPT-4 model is fresh out at the time of recording,
145:34 - and these docs need updating.
145:37 - So if you click through to these docs,
145:38 - you might find they look a bit different.
145:40 - Now, let's just click through to this endpoint compatibility
145:43 - table.
145:44 - We can see that GPT-4 is listed under chat completions.
145:49 - So this is going to work just fine.
145:52 - Now, for the messages property, slightly confusingly,
145:55 - the example they've given here is a bit of a simplification.
145:59 - So I'm going to ignore that and click through to chat format.
146:03 - Here we can see the format that it wants,
146:05 - and hopefully that looks familiar.
146:07 - It's an array with objects.
146:10 - And each object has got two key value pairs, role and content.
146:14 - And this is exactly what we have in conversation array.
146:18 - We've got an array of objects, and each object
146:21 - has got two key value pairs with role and content.
146:25 - And if we look closer here, we can
146:27 - see that the first object has the role of system,
146:30 - and the content is an instruction.
146:33 - So in the object we sent to the API,
146:35 - our messages property just needs to hold conversation array.
146:39 - So just to recap, the object we sent to the API
146:42 - will have two properties, model, which is GPT-4,
146:46 - and messages, which should hold conversation array.
146:49 - And you can think of messages as being
146:50 - a replacement for the prompt property
146:53 - that we used in the previous project.
146:56 - OK, so let's do this as a challenge.
146:58 - And we're working here inside fetch reply
147:01 - and inside the object that we're going to send to the API.
147:05 - I want you to give this object a model property of GPT-4,
147:09 - give it a messages property, which
147:11 - should hold our conversation array,
147:13 - and then ask a question, hit Send,
147:15 - and log out the response to see if it works.
147:18 - Now, just before you do that, you
147:20 - might have noticed that I'm not nudging you
147:22 - to include a max tokens property.
147:24 - In the past, we found that max tokens defaulted to 16.
147:27 - That was when we were using the text DaVinci 003
147:30 - model on the create chat completions end point.
147:35 - Things are different.
147:36 - Because we're building up the conversation
147:38 - and sending it with every request,
147:40 - trying to define a single max tokens figure is impossible.
147:44 - And in fact, the default with GPT-4 is much higher anyway,
147:47 - so it's not going to cause a problem.
147:50 - OK, code up this object, and then we'll have a look together.
147:59 - OK, so hopefully you got that working just fine.
148:03 - So this should be quite straightforward.
148:05 - The model is GPT-4, and of course, that is in a string.
148:10 - And the message is our conversation array.
148:14 - Let's just come down here and log out the response.
148:18 - And I'll hit Save, and I'm going to ask it a question.
148:23 - I've asked, what is the capital of Tunisia?
148:26 - And we've got a response, and I'm just
148:27 - going to copy that response and paste it into the editor
148:30 - so we can see it really clearly.
148:33 - We've got loads of info here, just like before,
148:35 - but the important bit is right here with the content.
148:39 - The capital of Tunisia is Tunis.
148:42 - So we have successfully made our first request
148:45 - to the GPT-4 model.
148:47 - OK, next we need to use this response in two ways.
148:51 - We need to update the DOM, and we
148:53 - need to update the conversation array.
148:55 - So let's look at that next.
148:56 - Next, we need to update the DOM, and we
149:03 - need to update conversation array.
149:05 - So we're going to go straight into a challenge,
149:07 - and I've got the challenge right here.
149:10 - We're working inside the fetch reply function.
149:13 - So I want you to pass the completion to the render
149:16 - typewriter text function so it updates the DOM.
149:20 - And we've got the render typewriter text function
149:22 - right here, and it takes in a parameter.
149:26 - Now once you've done that, push an object to conversation array.
149:30 - This object should have a similar format
149:32 - to the object we pushed in line 21,
149:34 - but the role should hold the string assistant,
149:36 - and the content should hold the completion we
149:38 - get back from the API.
149:40 - Let's just have a quick look at line 21.
149:42 - Here it is.
149:43 - It's actually line 22.
149:45 - We're pushing this object.
149:46 - It's got the role of user, and the content
149:49 - is the user input dot value.
149:51 - So the object that we pushed to conversation array
149:53 - is going to have a very similar format.
149:57 - Now once you've done that, you can log out conversation array
150:00 - to check it's working.
150:01 - And of course, you should see the DOM updated by the render
150:05 - typewriter text function.
150:07 - Now I've given you a big hint here.
150:08 - To save yourself some time and work,
150:11 - have a close look at the response
150:12 - before tackling number two.
150:14 - And I've actually left the response
150:16 - that we logged out in the last scrim right here,
150:19 - because that will help you.
150:21 - OK.
150:21 - Pause now, get this challenge sorted,
150:23 - and I'll see you back here in just a moment.
150:31 - OK, hopefully you got that working just fine.
150:33 - So I'm going to come in here, and I'll
150:35 - call the render typewriter text function.
150:38 - And what do we need to pass it?
150:40 - Well, if we come down to this response,
150:42 - we can see that we've got the actual completion right here.
150:45 - And we can access that by saying response dot data.
150:49 - And we want what we've got at the zero index of that array.
150:53 - And then we want to access the content in that object.
150:57 - So quite a convoluted line of code.
151:00 - But let's just write all of that in here.
151:02 - So it's response dot data dot choices.
151:05 - Then we want the element at position zero.
151:08 - Then we want to access the message property.
151:11 - And then the content.
151:13 - OK, next let's push an object to conversation array.
151:18 - Now luckily, and it's what I was getting at with this hint,
151:21 - I don't actually need to build an object.
151:25 - If we have a look again down at the response,
151:28 - let's see what we've got right here.
151:30 - Well, we've got an object, and it's
151:32 - got two key value pairs, role and content.
151:36 - The role is assistant, and the content is the completion.
151:39 - So that is exactly what we want already formatted.
151:43 - So I hope you noticed that, and you didn't extract the text
151:46 - and then write the code to build a new object.
151:50 - OK, so back up here, we're taking almost everything
151:53 - that we've got here.
151:55 - But because we want the whole object,
151:57 - we'll take away the dot content.
151:59 - And now let's just log out conversation array.
152:03 - I'll hit Save, and let's ask a question.
152:06 - What is the currency of Peru?
152:10 - And we've got an answer.
152:11 - The currency of Peru is the Peruvian soul.
152:14 - That's great.
152:14 - Fantastic answer.
152:16 - And if we open up the console and have a look,
152:19 - we can see conversation array.
152:22 - And it's got three objects in it, the instruction,
152:24 - the user inputted text, and now the object
152:27 - with the completion we've just got back from the API.
152:30 - OK, so our chatbot is essentially working.
152:34 - Now, I just want to ask it a couple of questions.
152:36 - What I'm keen to see is, is it keeping
152:38 - the context of the conversation?
152:40 - Does it have a memory that it's getting
152:42 - from our conversation array?
152:43 - So I'm going to ask it for pi.
152:50 - OK, and it's given me a very long answer.
152:53 - All I really wanted was the number 3.14159.
152:57 - But that's good, because now I can check its memory.
153:00 - I'm going to say, give it to me to three decimal places.
153:05 - And what's interesting to see here
153:07 - is, does it understand it to be pi, which we were just
153:11 - talking about?
153:12 - If it does, it's got a memory, and it's
153:14 - keeping the context of the conversation.
153:16 - Let's find out.
153:18 - And there we are, pi to three decimal places is 3.142.
153:23 - So it has a memory, and that is exactly what
153:26 - we want from a chatbot.
153:27 - OK, so we've pretty much nailed the chatbot,
153:29 - or at least its basics.
153:31 - So let's take a look at a few more tweaks
153:33 - we can make to really level up our chatbot skills.
153:36 - Now, one danger you can run into with chatbots
153:38 - is that they can get repetitive.
153:40 - No one likes a conversation with someone that just says
153:43 - the same thing over and over.
153:45 - So let's take a look at how we can deal with that next.
153:53 - At the moment, we're only using two properties
153:56 - when we make a request to the API.
153:58 - We're using the model and the messages.
154:01 - And they are both required.
154:03 - Now, in the previous project, we used max tokens, which,
154:06 - as I said, is not so helpful here.
154:08 - And we also use temperature, which you're welcome to add
154:11 - if you think your chatbot is not performing optimally.
154:14 - You can just go ahead and add it right here.
154:17 - And just like with the DaVinci model, it defaults to one.
154:20 - Now, I don't think we need to change the temperature on this,
154:23 - so I'm actually going to delete that.
154:25 - What I do want to look at, though,
154:27 - is two settings, frequency penalty and presence penalty.
154:31 - And what they do is offer some control
154:33 - over how repetitive our output is,
154:35 - because we want the language to sound natural,
154:38 - and we don't want to find ourselves saying,
154:40 - can you stop saying that?
154:42 - Now, we're going to look at these two settings together
154:44 - as they are similar and they're easy to confuse.
154:47 - So let's take presence penalty first.
154:50 - Presence penalty will be a number from minus two to two,
154:53 - and you can change it in increments of 0.01.
154:57 - It defaults to zero, so that is what we're using now
155:00 - because it's unset.
155:02 - At higher numbers, presence penalty increases
155:05 - the model's likelihood of talking about new topics.
155:08 - So what does that mean in the real world?
155:10 - Well, let's imagine a conversation between two friends,
155:14 - and this conversation is taking place
155:16 - at low presence penalty.
155:18 - So one friend says, hey, give me some good news.
155:21 - And the other friend says, Manchester United won six nil.
155:25 - It was the best game ever.
155:26 - I've never seen Real Madrid fans looking so unhappy.
155:29 - Manchester United are the best.
155:30 - Let me tell you about the game in detail.
155:33 - Hmm, we all know someone like that, right?
155:36 - Now, what would happen to this conversation
155:38 - if we could flip it to a high presence penalty?
155:42 - The first friend says, hey, give me some good news.
155:45 - And the reply is, well, my team won on Saturday.
155:48 - My investments are doing well.
155:49 - My brother's out of hospital, the sun's shining,
155:52 - and I'm getting married next June.
155:54 - So you can see that instead of obsessing
155:56 - over Manchester United,
155:57 - they're actually talking about more topics.
156:00 - Okay, let's compare that to frequency penalty.
156:03 - The settings are very similar.
156:05 - It's a number from minus two to two,
156:07 - and you can change it in increments of 0.01.
156:10 - It also defaults to zero.
156:12 - At higher numbers, it decreases the model's likelihood
156:16 - of repeating the exact same phrases.
156:19 - Okay, let's again imagine a conversation
156:21 - between two friends,
156:22 - and this will take place at low frequency penalty.
156:26 - So the first friend says, hey, how was your week?
156:29 - And the reply is,
156:31 - I went to a literally unbelievable party.
156:34 - There were literally millions of people trying to get in.
156:36 - Brad Pitt was there,
156:38 - and I spent literally the whole evening with him.
156:40 - Me and Brad are literally best friends now.
156:43 - And we've all met that person, right?
156:45 - The one who says literally a lot, or basically,
156:47 - or you know what I mean.
156:49 - Okay, if we repeat that at a high frequency penalty,
156:52 - hey, how was your week?
156:54 - I went to an amazing party.
156:56 - There were literally thousands of people trying to get in.
156:58 - Brad Pitt was there,
156:59 - and I spent the whole evening with him.
157:01 - Me and Brad are best friends now.
157:03 - Same unbelievable story,
157:04 - but the word literally has been used only once.
157:07 - So the frequency penalty will allow the word to be used,
157:11 - but it will stop it being overused.
157:14 - Okay, that's the theory.
157:16 - In the next scrim, let's get practical.
157:23 - Okay, so the conversation we looked at in the previous scrim
157:26 - was of course very contrived.
157:28 - Actually, presence penalty is a very subtle setting,
157:32 - and it can be hard to see it in action.
157:34 - In fact, it's only really noticeable
157:36 - when you're generating large amounts of text.
157:39 - So rather than watch me churning out loads of text,
157:42 - you're going to have to do some investigation on your own.
157:45 - So I'm going to come in here,
157:47 - and I'm going to set presence penalty to zero.
157:50 - And that is, of course, its default setting.
157:53 - And now I want you to spend some time experimenting.
157:56 - So you could just have a general chat with the chatbot
157:59 - at different presence penalty settings,
158:01 - but it's likely better if you ask it something
158:04 - that will generate a long answer.
158:06 - It's only then that you'll really see
158:08 - presence penalty in action.
158:10 - So here are some ideas.
158:12 - If you're a glass half full kind of person,
158:14 - you could ask it to tell you
158:16 - what is great and wonderful about the world.
158:19 - And if you're a glass half empty person,
158:21 - you could ask it to tell you
158:22 - what is wrong and terrible about the world.
158:25 - Either way, use the same prompt two or more times,
158:28 - but with different presence penalty settings
158:30 - and see what you get.
158:31 - And if you don't notice too much, don't worry.
158:34 - This is a subtle setting.
158:36 - And at least now you know it's there.
158:37 - So if you do have problems in this area down the line,
158:40 - you'll know what to do.
158:41 - Okay, take some time to do that now.
158:43 - And then when you're ready in the next grim,
158:45 - we'll look at frequency penalty.
158:52 - Okay, let's do some experimentation together
158:55 - with frequency penalty.
158:57 - And let's remind ourselves first
158:58 - what frequency penalty is supposed to do.
159:01 - We use a high frequency penalty
159:03 - to decrease the model's chances
159:05 - of repeating the same phrases.
159:08 - So what I'm going to do is set you a challenge
159:11 - which will involve generating some text
159:13 - which is likely to have some repeated words and phrases.
159:17 - Now, because we're going to generate a lot of text
159:19 - and it will actually be pretty painful
159:21 - to watch the render typewriter text function
159:24 - trundle through all of it,
159:26 - I've actually commented out that function call right here
159:29 - and I've replaced it with this console.log
159:32 - so we can just log out the completion.
159:35 - Now I've also put a file up here called output.md
159:38 - and we can paste our completions in there
159:41 - and save them for comparison.
159:43 - So here is your challenge.
159:45 - I want you to set the frequency penalty to zero.
159:49 - Give the chatbot this query,
159:51 - generate 20 ways to say you can't buy that
159:54 - because you're broke.
159:56 - Paste the results into output.md
159:59 - and then repeat that process
160:00 - with frequency penalty set to two.
160:03 - Once you've done that,
160:04 - you can examine the differences between the two outputs
160:06 - and see what frequency penalty is doing.
160:09 - Now I've just put a warning here,
160:10 - do not set frequency penalty to minus two
160:13 - and I've said that because it actually breaks things
160:15 - quite spectacularly,
160:17 - it will just churn out the same word again and again
160:20 - and again until all of the tokens it's allowed
160:23 - have been used up
160:24 - and in fact, it will probably crash the mini browser.
160:27 - Okay, pause now, have a go at that challenge.
160:30 - I'm going to go through the same process
160:32 - and we'll have a look at the two completions
160:33 - and compare them.
160:41 - Okay, so hopefully you managed to do that just fine.
160:43 - So I'm just going to quickly go through that process
160:46 - and paste my completions into output.md.
160:55 - Okay, so I've got my two outputs
160:57 - and if we compare them side by side,
160:59 - well, they both start okay.
161:01 - So we've got at frequency penalty zero,
161:03 - your financial situation doesn't permit you
161:06 - to make that purchase
161:07 - and then unfortunately,
161:09 - your funds are insufficient for buying that item.
161:12 - Both of those are fine, both great English.
161:15 - Let's have a look at the first two that we've got
161:18 - when frequency penalty was set to two.
161:21 - Unfortunately, your current financial situation
161:23 - doesn't allow for that purchase.
161:25 - It seems your wallet is feeling a bit light
161:27 - to acquire that item.
161:29 - Okay, a little bit quirky, but no problem.
161:32 - Now let's come to the very end
161:33 - because it's the last part that's going to be most difficult
161:36 - when the frequency penalty is high.
161:39 - Now number 20 here was with your financial setting,
161:43 - it's infeasible to add that to your possessions.
161:45 - Well, that's okay, it's correct English.
161:47 - It's a little bit strange.
161:49 - Now down here, we've got monetary scarcity
161:53 - dictates frugality and discipline
161:56 - despite passions give merit where it's due,
162:00 - nullifying all impulses towards luxury, at least for now.
162:04 - I can't decide if that's poetry or gibberish.
162:07 - I think it's just gibberish.
162:09 - And actually, if we look a little bit further up,
162:11 - we can see that by setting the frequency penalty high,
162:14 - we've really caused open AI problems
162:17 - because this is the first time
162:19 - that we're actually seeing bad English.
162:21 - We're actually getting completions
162:23 - that don't seem like they come from a human.
162:26 - Take for example, number 17.
162:28 - It's apparent after analyzing existing revenue streams,
162:32 - financing the stream can simply just about only be squeezed
162:35 - from another worldly dimension entirely.
162:38 - Don't know what that means.
162:40 - Or number 18, empty pockets rarely find footing
162:43 - when chasing certain ambitions.
162:46 - In our material world,
162:47 - might as well try catching stardust instead.
162:50 - Again, kind of poetic,
162:51 - but these weird little mistakes as well,
162:54 - there's a comma there, but no space.
162:56 - Now, one other thing I want to show you is this.
162:58 - I'm just going to highlight every instance
163:00 - of the word financial.
163:03 - Now we can see if I scroll down,
163:05 - that word appears seven times,
163:07 - but five of those times are when frequency penalty
163:10 - was set to zero and only two of them
163:13 - were when it was set to two.
163:15 - So that shows us that the frequency penalty
163:17 - is penalizing the word financial.
163:19 - It's only letting it appear twice.
163:22 - And when we're talking about money,
163:24 - it is quite an important word.
163:26 - Let's do the same thing with the word purchase.
163:30 - Again, the word purchase is used seven times,
163:32 - five times with frequency penalty at zero
163:35 - and just twice with frequency penalty set to two.
163:38 - So we can really see why the model is struggling
163:41 - to generate new language.
163:43 - Okay, so what is the bottom line?
163:46 - Well, here is my general advice
163:48 - for presence penalty and frequency penalty.
163:51 - And I'm going to express this as a flow chart.
163:53 - Firstly, ask yourself, is there a problem?
163:56 - If the answer is no, do nothing.
163:59 - If the answer is yes, then my advice is this.
164:03 - Don't go over one for either setting.
164:06 - Don't go under one for either setting.
164:08 - You can experiment outside of those parameters,
164:11 - but I would recommend staying inside them
164:13 - else you run the risk of getting
164:15 - some pretty strange results.
164:17 - And lastly, it's all about making small changes and testing.
164:21 - If you just make a small change and test,
164:23 - you'll very quickly find the settings
164:26 - which work for you and get you the results you want.
164:29 - So what are we going to do in this app?
164:31 - Well, I'm going to leave presence penalty at zero.
164:34 - I don't think it's doing too much for us anyway.
164:36 - And frequency penalty, I'm going to put to 0.3.
164:40 - And I'm going to do that just because
164:42 - I've had to play around with this for a long time.
164:44 - And I think that is where we get the best results.
164:48 - You of course are free to differ.
164:50 - Okay, so the chatbot is working pretty well.
164:52 - So next, I want to go back to where we started
164:55 - to this instruction right here
164:57 - and have a bit of fun with it
164:59 - and just see how we can alter the chatbots personality
165:02 - and why that might be useful.
165:04 - When you're ready for that, let's move on.
165:10 - Let's have some fun and change the chatbots personality.
165:14 - So here's a challenge for you.
165:15 - Update the content properties value
165:17 - to change the chatbots personality.
165:20 - So you just need to alter this string right here.
165:23 - You could ask it to be cheeky or funny or talk in rhyme
165:27 - or go for something practical.
165:28 - Maybe you want a shorter answer or even a one word answer.
165:32 - I'll leave that up to you.
165:33 - Pause now and have a go.
165:41 - Okay, hopefully you managed to get a good effect.
165:44 - Now I'm British, we love sarcasm.
165:46 - So I'm going to make this a sarcastic chatbot.
165:50 - And let me ask it something.
165:54 - And there we are, it's had a pretty good effect.
165:56 - It says you should definitely go to the most boring place
165:59 - on earth, your local park.
166:01 - Why explore beautiful beaches, breathtaking mountain
166:04 - landscapes or exotic cities with rich culture
166:07 - when you could just sit on a bench and watch the grass grow.
166:10 - Sounds like a dream vacation.
166:12 - Okay, that's pretty sarcastic and I like it.
166:15 - But actually there is a more serious side to this as well
166:19 - because we can actually get the chatbot
166:21 - to behave exactly as we want it to.
166:24 - Perhaps you want a chatbot that's going to interact
166:26 - with children, perhaps you want a chatbot
166:28 - that's going to interact with people for whom English
166:30 - or whichever language you're working in
166:32 - is not their first language.
166:34 - So you might want it to simplify the language a little bit.
166:37 - And perhaps you don't want such long answers.
166:39 - We have actually got some quite long answers so far.
166:42 - So I might change this to keep the answers short.
166:48 - And I'll ask it a big question.
166:49 - What is quantum computing?
166:52 - And look at that, the most concise definition
166:54 - you'll ever get, advanced computing using quantum bits.
166:58 - Okay, so that's good to know.
167:00 - You can control the chatbots personality
167:02 - and it really is that simple.
167:04 - Okay, so we're pretty much done
167:06 - with the mechanics of this chatbot.
167:09 - What I want to do next is use a Google Firebase database
167:12 - so we can persist the chats even when we refresh and reload.
167:16 - Let's do that next.
167:18 - Okay, so we're going to add a Google Firebase database
167:25 - to this project so we can persist the chat.
167:28 - Now, what does that mean?
167:29 - Well, let's check our specific aims.
167:32 - We want to persist the chat so that a user can pick up
167:35 - where they left off after a refresh or reload.
167:39 - So the conversation will be stored in the database
167:42 - and the user can close the page, turn off their laptop
167:44 - and come back and continue the conversation
167:46 - at a later date.
167:48 - We want to give the user the ability to reset the chat
167:52 - so they should be able to delete the conversation
167:54 - and start a fresh conversation from the beginning.
167:57 - So how is this going to work?
167:59 - Let's take a look at a diagram.
168:01 - At the moment, all of our code is on the front end
168:04 - and we've got an array holding our conversation.
168:08 - We send that off to open AI
168:10 - and it sends us back a response.
168:12 - What we're going to do now is remove the array
168:15 - where we store the conversation on the front end.
168:17 - We're going to create a database
168:19 - and we're going to store the conversation array
168:22 - in that database.
168:24 - When we need it, we'll bring it down to the front end
168:26 - and use it when we make calls to the open AI API.
168:30 - To start this off, we need a Google Firebase account
168:33 - and we need to set up a database.
168:35 - So let's start doing that next.
168:38 - We need a Google Firebase account.
168:46 - So head over to the homepage
168:47 - and actually the image on this slide is a link.
168:50 - Click on it and it's going to take you straight there.
168:54 - When you're there, click on Get Started
168:56 - and that is going to take you to a signup page.
168:58 - And once you've entered your details
169:00 - and set up your account,
169:01 - you'll end up on a page looking like this.
169:04 - Click Create Project
169:06 - and here we need to give the project a name.
169:08 - I'm going for know it all dash open AI
169:11 - but of course you can call yours whatever you want.
169:13 - And now it's going to ask us if we want Google Analytics
169:17 - and for this project, I'm going to turn it off.
169:19 - Now hit Create Project
169:21 - and it will take a few moments to set things up.
169:23 - And when it's ready, you can just click Continue.
169:26 - Now we've created our project, go over to Build
169:29 - and then from the dropdown menu, select Realtime Database.
169:34 - Click Next, click Create Database.
169:36 - But if you're interested in finding out a little bit
169:38 - about the different types of database that Firebase offers,
169:41 - then do click this link
169:43 - and that will give you a little bit more info.
169:45 - For this project, we're using the Realtime Database
169:48 - which is the easier one to work with.
169:50 - So create database and then in the popup,
169:53 - select the server that's nearest to you.
169:55 - I'm in the UK, so my nearest is Belgium.
169:58 - Click Next and it's going to offer us
170:00 - locked mode or test mode.
170:02 - I'm going for test mode
170:04 - and that does mean that anybody with my database reference
170:07 - will be able to view, edit or delete any of my data.
170:12 - Now that's fine for the purposes of this course.
170:14 - By the time this gets published, I will have locked this down.
170:17 - I recommend starting in test mode.
170:19 - It's just a little bit easier
170:20 - to not have to deal with the security rules
170:22 - at the beginning.
170:23 - You can always read up on that later
170:24 - and make some changes to the security rules if necessary.
170:27 - And there we are, we have our database.
170:30 - And the most important thing here is this URL.
170:33 - That is the URL we'll be using
170:36 - to communicate with our database.
170:38 - So let me just zoom in on that.
170:40 - There it is.
170:41 - I'm going to click here to copy it
170:42 - and then in our code,
170:44 - I'm just going to create a little comment here
170:46 - and paste it in.
170:47 - And we'll use that in the next grim
170:49 - when we start adding the Firebase setup
170:51 - to this code right here in index.js.
170:59 - We need to add the Firebase dependency.
171:02 - So I'm going to come over here to the three-dot menu.
171:05 - And when I click on that, a menu appears
171:07 - and you can't actually see that in the recording,
171:10 - but I'm going to select add dependency
171:12 - and a pop-up appears
171:13 - and I'm just going to type in Firebase.
171:16 - And I'll click add
171:17 - and we can see that the Firebase dependency
171:20 - has appeared over here on the left-hand side.
171:22 - Now, if you're working locally,
171:24 - you can install this via MPM
171:26 - and you can check out the Firebase docs on that right here.
171:30 - Again, this screenshot is a link
171:31 - just click it to go straight through to those docs.
171:34 - And by the way, you can also use a CDN to work with Firebase
171:38 - if you prefer to do it that way.
171:40 - Okay, now we've got the Firebase dependency setup.
171:43 - We need to import some methods.
171:45 - So I'm going to come right up here to the top
171:47 - and we'll say import
171:49 - and then inside curly braces,
171:51 - I'm going to say initialize app.
171:54 - And we're importing that from Firebase slash app.
171:58 - Next, I'm going to import get database and ref.
172:05 - And these are coming from Firebase slash database.
172:09 - Okay, that's all the methods that we need.
172:11 - Now I'm going to come down here
172:13 - and set up a const called app settings.
172:17 - And this is going to hold an object
172:19 - and it's going to have one key value pair.
172:23 - Database URL is the key
172:25 - and the value will be a string
172:27 - containing the URL that we got when we set up the database.
172:32 - And we can delete that comment now.
172:34 - Underneath app settings,
172:35 - I'm going to set up another const
172:37 - and this one will be database.
172:39 - And here we'll use one of the methods
172:41 - that we've just imported, get database.
172:44 - Next, I'm going to come down here and set up a const app.
172:48 - And now we will use one of the methods
172:51 - that we just imported.
172:53 - So I'm going to say initialize app
172:56 - and I'll pass in the app settings.
172:59 - Now we need a const database.
173:02 - And again, we'll use one of the methods
173:04 - we just imported, get database.
173:07 - And we'll pass in app.
173:09 - And finally, we'll set up a const
173:11 - called conversation in database.
173:15 - And I've just shortened database to DB.
173:18 - And we'll set that equals to ref
173:20 - and we'll pass in database.
173:23 - So what we've just done there
173:25 - is set up a const called conversation in DB
173:29 - and it stores a reference to our database.
173:32 - And that's really important because now conversation in DB
173:36 - is going to be our single source of truth
173:38 - for the conversation we have with the chatbot.
173:41 - Okay, those are all the basic settings we need for now.
173:44 - We will add a few more methods later as we go on.
173:47 - Now, before we do anything else,
173:49 - we do need to make a few changes to the HTML and CSS.
173:53 - I want to add a clear button right here.
173:56 - So I'm going to go over to index.html
173:59 - and I'll come in here and just paste in the clear button.
174:03 - And I'll hit save and of course we've broken the CSS.
174:06 - So let's head over to index.css
174:09 - and we need to make a couple of changes.
174:11 - So firstly, I'm going to come down here
174:14 - and paste in some CSS for the new button.
174:17 - And as soon as I do that,
174:18 - you can see that things start to change over here.
174:21 - Now I've used the selector clear-btn
174:24 - and if we have a quick look back at index.html,
174:27 - the button has already got that class added.
174:31 - And the most important part of this CSS
174:33 - is actually this property right here,
174:36 - grid area clear-btn.
174:39 - And we're using that because the layout up here
174:41 - uses CSS grid.
174:43 - So if we come up here to the chatbot header selector,
174:47 - this is where we're setting display grid
174:49 - and we've got the grid template areas right here.
174:52 - And I'm going to change this dot for clear-btn.
174:56 - And now things are starting to come into alignment.
174:59 - So clear-btn is the grid area
175:02 - that we've got on the clear-btn selector.
175:05 - Now we've still got a flat edge to this button
175:08 - and that's because at the moment down on line 137,
175:12 - we are currently selecting all buttons
175:14 - and we've got styles set up for this submit button
175:16 - that we've got right here.
175:18 - And the submit button has a border left of zero.
175:21 - Now, if we go back to index.html,
175:23 - we've got the submit button right here
175:26 - and it's actually already got a CSS class of submit-btn.
175:31 - So how about we come over here
175:33 - and we replace this with the more specific selector.
175:37 - Okay, that has had the effect
175:38 - of giving us a full rounded button,
175:41 - which is what we want right here.
175:43 - Now all we need to do is bring this into alignment
175:46 - by coming up to the support ID selector
175:49 - and taking this property right here,
175:51 - text align right and changing it to center.
175:55 - Okay, now we've got it looking how we want it
175:57 - and we're good to move on.
175:59 - Next, we need to figure out how to push our user's input
176:02 - to the database.
176:09 - Right here, we're pushing our user's input
176:11 - to conversation array,
176:12 - but now we want to store the user's input in the database.
176:15 - That is going to be the single source of truth.
176:18 - So let's use Firebase's push method
176:20 - to push it to the database instead.
176:23 - To get access to the push method,
176:24 - we need to add it to the list of imports.
176:27 - So I'll come up here and add it in right there.
176:31 - And then back down in the event listener,
176:33 - let's come in here
176:35 - and we're going to delete conversation array.
176:37 - And now we've got push
176:39 - and that is no longer the standard JavaScript array method.
176:42 - That is now the Firebase push method.
176:44 - And what we need to pass it
176:46 - is the reference of the database we want to add to.
176:49 - And the reference to that database we've got up here,
176:52 - it is conversation in DB.
176:54 - So we'll say push and then in brackets,
176:57 - conversation in DB, a comma,
177:00 - and then the object which we want to push.
177:03 - And that is of course, exactly the same object.
177:06 - And it's as easy as that.
177:07 - So now let's go ahead and type something in here
177:11 - and we'll hit send.
177:12 - And let's take a look at what we've got in the database.
177:15 - And there we are, it has appeared.
177:18 - If you log into your real time database,
177:20 - you'll pretty much instantly see
177:22 - exactly what we've just pushed to the database.
177:24 - So we've got the content, hello, know it all,
177:27 - and the role of user.
177:29 - So that is our object.
177:31 - Now doing that leaves conversation array
177:34 - looking pretty redundant.
177:35 - We don't want to store our conversation here anymore,
177:38 - but that leaves us with an issue.
177:40 - We've got this instruction object
177:43 - and we need to keep it somewhere.
177:44 - But storing it in the database
177:46 - with the rest of the conversation
177:47 - has actually got two disadvantages.
177:49 - Number one, if we ever want to edit the instruction
177:52 - to change the chatbots personality or behavior,
177:55 - we'll actually have to edit the database directly.
177:58 - And number two, when we want to clear the conversation,
178:01 - we're going to get caught up in JavaScript spaghetti
178:04 - making sure we don't delete the instruction,
178:06 - but do delete everything else.
178:08 - So we'll be making work for ourselves.
178:10 - The solution that I think is best
178:12 - is that we keep the instruction right here in index.js
178:15 - where it's easily accessible and easily editable,
178:18 - and we just add it to the array
178:20 - we're sending to OpenAI with each request.
178:23 - So what I'm going to do is change conversation array
178:26 - to instruction object.
178:29 - And now it's an object, let's delete the square brackets.
178:32 - And when we update fetch reply,
178:34 - we'll add this to the array we send off to the OpenAI API.
178:39 - And actually updating fetch reply
178:41 - is what we need to come on to next.
178:49 - The next thing we need to do
178:50 - is make some changes to fetch reply.
178:52 - We know we need to send the entire conversation
178:55 - with every request to the OpenAI API.
178:58 - But now that the conversation's stored on the database,
179:01 - we need to fetch it before we can include it
179:03 - with our request.
179:05 - And with Firebase, we do that with the get method.
179:08 - But before we can use the get method,
179:10 - we need to add it to our list of imports.
179:13 - So let's come up here and we'll just put it after push.
179:17 - Now get is a method, so we'll give it the brackets
179:20 - and we're going to pass in the reference to our database,
179:23 - just like we did with the push method.
179:26 - Now we're going to chain a then
179:28 - and here we'll have a callback function
179:30 - and that function gets a parameter snapshot.
179:34 - And snapshot is the data in our database
179:37 - as it exists at this time.
179:39 - Now inside the body of this function,
179:41 - let's be safe and use an if
179:43 - just to make sure a snapshot exists.
179:46 - So if, for example, writing to the database right here
179:49 - in the event listener failed,
179:51 - then we would want to know about that.
179:53 - So let's just say else, no data available.
179:56 - Because if what we send to the OpenAI API
180:00 - is not the array of objects it expects,
180:03 - it's going to give us an error anyway.
180:05 - So it's a good idea to check what's going on here.
180:08 - Now in here, we can make our request to the OpenAI API.
180:12 - So let's just cut and paste all of this code.
180:15 - And let's just format that nicely.
180:17 - Now we're actually not quite ready to call the API.
180:20 - So I'm actually going to comment out all of that code.
180:24 - And let's just come in here and log out snapshot.
180:27 - Now remember, we've already got something in our database.
180:30 - So there should be something for us to log out.
180:32 - So rather than write anything more in here
180:34 - and add to the database,
180:36 - I'm just going to call the fetch reply function.
180:39 - Now when I hit save, let's just see what we log out.
180:42 - Well, there we are.
180:43 - We can see the contents of our database.
180:46 - We've got an object.
180:47 - It's got a content key with hello know it all
180:49 - and a role key with user.
180:51 - But at the beginning,
180:52 - it's also got a Firebase identifier of some kind.
180:55 - And we don't want to be sending that to OpenAI.
180:58 - It would just really confuse it.
181:00 - All we want is an array of objects.
181:02 - So let's check what the Firebase docs say.
181:05 - And they talk about this val method and it says here,
181:08 - it extracts a JavaScript value from a data snapshot.
181:12 - Well, that is exactly what we want to do.
181:14 - Now, if you'd like to read up more about val
181:16 - in the Firebase docs, this screenshot is a link.
181:20 - So just click on it and it will take you straight there.
181:22 - So I'm going to come in here
181:25 - and where we've got snapshot, I'm going to call val.
181:28 - And let's just save and see what happens.
181:31 - Okay, so on its own,
181:32 - it doesn't make any difference.
181:33 - We've still got Firebase's identifier and then our object.
181:37 - So let's just check something else we can use from MDN.
181:41 - And again, this screenshot is a link to this page on MDN.
181:44 - Now object.values, let's see what it says here.
181:47 - It says the object.values static method
181:49 - returns an array of a given object's
181:52 - own innumerable string keyed property values.
181:55 - Okay, that sounds like a bit of a mouthful,
181:58 - but the important thing here is it returns an array
182:00 - and an array is what we want.
182:02 - So let's just experiment with it and see what we get.
182:05 - So I'll come in here and say objects.values
182:09 - and then in brackets, we'll have our snapshot
182:12 - on which we've called the val method.
182:15 - Okay, let's hit save.
182:16 - And look at that, down in the console,
182:19 - we have got an array and it's got our object in it.
182:23 - And that is the exact correct format
182:25 - that we need to send to the OpenAI API.
182:29 - But we're not quite ready to send it yet.
182:31 - We need to think about the instruction.
182:33 - So let's do a little bit more work
182:35 - in fetch reply in the next scrim.
182:41 - We need to get the array that we can see in the console
182:44 - off to the OpenAI API.
182:46 - So I'm going to come in here
182:47 - and resurrect the old conversation array.
182:51 - And we'll use it to store the array
182:53 - we get back from the database.
182:56 - And then if I uncomment all of this code,
182:59 - we're now going to send conversation array off
183:01 - to the OpenAI API with each request.
183:05 - Now to check it's working,
183:06 - I'm going to log out the response
183:08 - and I'll just take this opportunity
183:10 - to delete the console.log
183:12 - and also the function called to fetch reply
183:14 - that we were just using as a test.
183:16 - And now we can give this a try, but wait a second,
183:19 - we've actually forgotten about our instruction object.
183:23 - We need that in the conversation that we send
183:26 - to the OpenAI API.
183:28 - So we need to include it in conversation array.
183:30 - And in fact, you can do that as a challenge.
183:33 - So I'm just going to come in here
183:34 - and paste the challenge right there
183:36 - because that is where you'll need to write some code.
183:39 - And I've just said, add instruction object
183:41 - to the front of conversation array.
183:43 - But I've put a warning here,
183:45 - you're going to get an error when you do this.
183:47 - Try to debug it.
183:49 - Okay, we're already logging out the response.
183:51 - So pause now, get this challenge sorted
183:53 - and I'll see you back here in just a minute.
184:01 - Okay, hopefully you got it working.
184:02 - So we can do this with unshift.
184:06 - So we'll say conversation array dot unshift
184:09 - and then we'll pass in the instruction object.
184:13 - Okay, now let's hit save and we'll give it a try.
184:16 - Oh, but we're getting an error.
184:18 - And I did warn you that you would.
184:20 - Now it's saying unexpected reserved word.
184:23 - So what could be the problem here?
184:25 - Well, we're using await here
184:28 - and we can only use await inside an async function.
184:32 - And that's what we're doing, isn't it?
184:33 - We've got async right here.
184:35 - Well, actually no, because this await
184:37 - is inside the callback function that we've got right here.
184:41 - So that is the function that should be async.
184:43 - So let's delete async from here and add it in right here.
184:48 - Okay, let's save and try again.
184:50 - And there we are, it's working.
184:52 - So the final thing that we need to do with this function
184:55 - is uncomment these two lines of code right here.
184:58 - And in the next grim,
184:59 - let's see what changes we need to make to them.
185:05 - This line of code right here,
185:06 - which sends the completion to render typewriter text
185:10 - is absolutely fine.
185:11 - We don't need to change that.
185:13 - But here, we don't want to update conversation array.
185:18 - Remember, conversation array is now here
185:21 - and it just holds the array we get back from the database
185:24 - when we use this get method.
185:26 - Our single source of truth is on the database.
185:29 - So what we actually need to be doing right here
185:31 - is updating the database.
185:34 - So here is a challenge for you.
185:36 - I want you to add the completion to the database
185:39 - and then ask the chatbot something to check it's working.
185:43 - Now to complete this challenge,
185:44 - you can look back at the code we've already written.
185:46 - And remember, this code here gives us an object
185:50 - which is already formatted in the way we need it.
185:53 - Okay, pause now, get this challenge done
185:56 - and I'll see you back here and we'll have a look together.
186:05 - Okay, hopefully you managed to do that just fine.
186:07 - So we need to use the push method
186:10 - and we need to pass it the database ref,
186:12 - which is conversation in DB.
186:15 - And then we give it the object we want to push
186:17 - and we can do that with this code right here.
186:20 - Okay, let's delete this line of code
186:23 - and we'll save and give it a try.
186:25 - I'll ask it a simple question
186:27 - and it's given me a reply, it's looking good.
186:30 - Let's just see if we've successfully updated the database.
186:34 - So this is what we originally had in the database.
186:36 - We've made a few changes to it since then
186:38 - and now look, we've got my question,
186:40 - what is a Frisbee and the answer, a flying disk.
186:44 - So the code that we just wrote right here
186:46 - is definitely working.
186:48 - Okay, so things are going well,
186:50 - but we've got a bit of an issue.
186:51 - If I just refresh the mini browser,
186:53 - in the database, we have got a whole conversation
186:57 - that I don't know about as a user, it's not shown here.
187:01 - And that's a problem because if I now start typing
187:03 - a question, I'm not aware that I'm actually continuing
187:06 - a conversation and in fact, I might well have forgotten
187:09 - what I wrote before.
187:11 - So that means we've got two more jobs to do.
187:13 - We need to render the existing conversation out
187:16 - when the app loads so the user can see
187:18 - what they've already said.
187:20 - And we also need to wire up this start over button
187:23 - so that a user can reset the conversation
187:26 - whenever they want.
187:27 - So let's start on the first task of rendering out
187:29 - the conversation on load next.
187:35 - We need a function that will render out the conversation
187:38 - as it exists on the database when the app loads
187:42 - so the user can see what has already been discussed.
187:45 - Now I've set you a challenge to do this
187:47 - and it might seem like a big challenge,
187:49 - but actually this function is just going to recycle
187:52 - bits of code we've already written
187:54 - and it's a good opportunity for you to get some more
187:56 - practice with Firebase.
187:58 - So the challenge is this, create a function called
188:01 - render conversation from DB, which will render
188:05 - any existing conversation in the database.
188:08 - Now this function should be called when the app loads.
188:11 - Now take all the time you need to do this.
188:14 - There are a few things to think about
188:16 - and if you need any extra help, I've created a file up here
188:19 - called hint.md and it's got some tips in there
188:22 - that will really help you.
188:24 - Now if you don't want any help, pause now and get started.
188:28 - I'm going to open hint.md so those watching on YouTube
188:32 - can pause it and read it.
188:36 - Okay, good luck with the challenge and when you're ready,
188:39 - I'll show you my way.
188:46 - Okay, hopefully you managed to do that just fine.
188:48 - So let's come down here and set up this function.
188:52 - Now the first thing that we need this function to do
188:54 - is to get the conversation from the database.
188:58 - So we'll use the get method and we'll pass in
189:01 - the reference to the database.
189:04 - We'll chain on then and we know the callback function here
189:07 - will be an async function and it will have the snapshot
189:11 - as a parameter.
189:12 - Inside the body of this function, let's use an if
189:15 - to make sure the snapshot exists
189:17 - because if there is no snapshot,
189:19 - if there's nothing in the database,
189:20 - this function need not do anything.
189:23 - And now we want to get that snapshot as an array of objects
189:26 - and we can do that by saying object.values
189:29 - and we'll pass in snapshot and we'll need the val method.
189:34 - Now we can iterate over that array using a for each
189:38 - and we'll represent each item in the database
189:40 - with database object, which I'll shorten to dbobj.
189:45 - Now for each database object,
189:46 - we want to create a new speech bubble
189:49 - and we can do that with document.createElement
189:52 - and we'll create a div.
189:54 - Now we'll need to add some classes to that div
189:58 - and that goes with every speech bubble is just speech.
190:01 - Now the second CSS class is dependent on whether it's a human
190:05 - or the AI that's speaking.
190:07 - So I'm going to open up the back ticks
190:10 - and both classes start with speech dash
190:13 - and now I'll use dollar sign and curly braces
190:16 - and in here we're going to use a ternary.
190:19 - So I'm going to say database object.role
190:22 - and I'm going to ask if that is triple equals to user.
190:25 - If it is, this CSS class should be speech dash human.
190:30 - And if it's not, it should be speech dash AI.
190:34 - Okay, now we need to append that speech bubble
190:36 - to chatbot conversation,
190:38 - which we took control of right up here on line 23
190:42 - and apologies in hint.md.
190:44 - I think I said it was line 22, but I'm sure you found it.
190:48 - So let's say chatbot conversation dot append child
190:52 - and the element we want to append is new speech bubble.
190:55 - And lastly, we need to add the text to the speech bubble.
190:59 - Now I did say in hint.md, make sure that a malicious user
191:03 - can't use this to input JavaScript.
191:06 - So hopefully you didn't use inner HTML to do this.
191:10 - Text content is secure because anything which is inputted
191:13 - is just going to be passed as text.
191:15 - It cannot be passed as HTML with script tags
191:19 - and with executable JavaScript inside them.
191:21 - Lastly, we just want to move the conversation down
191:24 - so the latest message is always in view
191:28 - and we've done that several times in the app with scroll top
191:31 - and we're setting chatbot conversation dot scroll top
191:33 - equal to chatbot conversation dot scroll height.
191:36 - Now to check it's working, all we need to do is call it.
191:40 - And if we call it right there,
191:41 - it should run every time the app loads.
191:44 - Let's hit save and see what happens.
191:47 - And there we are, our conversation is rendered.
191:50 - So everything that we've got in the database,
191:53 - we can see it all right here is now rendered automatically
191:57 - as soon as we load.
191:58 - And we can see that the ternary has been successful
192:00 - because we've got different speech bubbles
192:02 - depending on whether it's the AI
192:04 - or the human that has spoken.
192:06 - Okay, that's a really good job.
192:08 - The final task then is to wire up this start over button.
192:12 - Let's do that in the next screen.
192:18 - Okay, so at the moment when the page loads,
192:20 - the conversation as it exists in the database
192:23 - renders automatically.
192:25 - What we want to do now is wire up this start over button
192:28 - so we can clear the conversation.
192:30 - So I'm going to come in here
192:32 - just above the last function we wrote,
192:34 - although it doesn't really matter where we write this code.
192:37 - And I'm going to say document dot get element by ID.
192:41 - And I want to take control of this button.
192:44 - So if we just check in the HTML quickly,
192:47 - we've got that button right here
192:48 - and it's got clear dash BTN.
192:51 - Now I'll add an event listener to listen out for clicks.
192:55 - Now, when a click is detected,
192:56 - we've got a very easy way of clearing the database.
193:00 - And to do that, we use the remove method.
193:03 - Now, of course, before we can use the remove method,
193:05 - we need to import it.
193:07 - So let's add it to the list right here.
193:10 - And all we need to do with remove,
193:11 - and you can probably guess this by now,
193:13 - is pass it the database reference.
193:17 - Now that is going to clear the database,
193:19 - but obviously it's not going to update the HTML.
193:22 - So let's add one more line
193:24 - and I'll say chatbotconversation dot innerHTML is equals two.
193:29 - And then we just need our hard coded message
193:32 - that goes right at the beginning of any conversation.
193:35 - And we can get that from the HTML.
193:37 - Here it is right here.
193:38 - So let's copy it and then we'll paste it here
193:40 - in inverted commas.
193:42 - And I'm just going to bring it all onto one line.
193:45 - And this is a safe use of innerHTML
193:48 - because this is hard coded HTML.
193:50 - The user has no way to access that
193:52 - or update it in any way.
193:54 - Okay, let's give it a try.
193:56 - So I'm going to come over here
193:58 - and I'll just click the start over button and look,
194:00 - everything disappears.
194:02 - We're back to our single hard coded message.
194:05 - Now let's check the database.
194:07 - I've just taken a screenshot of it.
194:08 - So that is how it looked before and now it's empty.
194:12 - So this is working.
194:14 - And if I come in here and I'll actually ask the chatbot,
194:18 - is this a new conversation?
194:20 - And it says yes.
194:21 - I'm going to ask it a couple of questions.
194:25 - Now let's refresh.
194:27 - The conversation is there.
194:29 - It seems to be working.
194:30 - Let's ask it if I've already asked it some questions.
194:35 - And it replies yes.
194:37 - What did I ask you?
194:39 - New convo, how are you questions?
194:41 - Well, yeah, that's a fair summary.
194:42 - I did ask it, is this a new conversation and how are you?
194:46 - And remember, it's giving us very short answers
194:48 - because we did leave the instruction right up here
194:53 - as you're an assistant that gives very short answers.
194:56 - Maybe let's just change that back to,
194:58 - you are a helpful assistant.
195:01 - What have I asked you today?
195:03 - And there we are, it knows everything.
195:05 - I'm going to click start over to clear the conversation.
195:08 - I'll try again, what have I asked you today?
195:12 - And look, it's telling us that it cannot recall
195:14 - any past interactions with us from separate sessions.
195:17 - And that proves that this is working.
195:19 - The start over button is doing exactly what we want it to.
195:23 - Okay, so that is a really good job.
195:25 - Congratulations on getting to the end of this project.
195:28 - Let's just take one last grim to recap what we've done
195:31 - and talk about where we go from here.
195:36 - Congratulations on finishing the know-it-all chatbot app.
195:40 - Now you have got the foundations you need
195:43 - to build any human language capable chatbot that you want.
195:47 - Let's just recap what we've studied.
195:49 - We have used the GPT-4 model
195:52 - and the create chat completions endpoint.
195:55 - We've given our chatbot a personality
195:57 - via the instruction object.
195:59 - We've seen how we can use the presence penalty setting
196:02 - to encourage the chatbot to talk about new topics.
196:05 - And we've used frequency penalty
196:07 - to control how repetitive the chatbot is
196:09 - when it selects words and phrases to use.
196:13 - We saved our conversation as a single source of truth
196:16 - in a database so the conversation can be persisted
196:19 - and the user can come back to it
196:21 - and pick it up at a later date.
196:23 - Now it's always a good idea
196:25 - to really make a project your own.
196:27 - So why don't you give the bot a specific function?
196:30 - So you could tweak the AI
196:32 - so that it has a specific purpose.
196:34 - This tech doesn't have to be a general purpose chatbot.
196:38 - It could be a coding expert, a poetry generator,
196:42 - an academic assistant.
196:43 - You could use the instruction object
196:45 - to train it to provide text
196:47 - in the specific style and format you need
196:49 - for writing reports at work, for example.
196:52 - The only limit is your imagination.
196:54 - And once you've done that,
196:55 - you could change the style and theme
196:58 - so it matches the chatbot's new purpose.
197:01 - And of course, it's always a good idea
197:02 - to build again from scratch.
197:04 - And if you don't need practice
197:05 - with the HTML, CSS, and JavaScript,
197:08 - you could just rebuild the API specific parts
197:10 - to really get that syntax to stick in your brain.
197:14 - It would also be good to add some error handling.
197:17 - We haven't really looked at error handling in this course.
197:19 - We're focusing very much on the AI.
197:22 - But whenever you're working with APIs,
197:24 - error handling is more than a good idea.
197:27 - It's essential really
197:28 - if you're taking anything to production.
197:30 - So do do a bit of research on that.
197:33 - But whatever you do, remember to take a break.
197:36 - Take a bit of time to consolidate what you've learned
197:38 - before moving on to the next project.
197:44 - Okay, so we are going to enter
197:46 - the fantastic world of fine tuning.
197:48 - And I've put here making AI models work for us.
197:52 - So what exactly do I mean by that?
197:55 - Well, in earlier projects,
197:56 - we have used two types of prompt.
197:59 - We've used the zero shot
198:00 - where we just give an instruction or ask a question.
198:04 - We've also used the few shot approach
198:06 - where we give an instruction
198:07 - and we use examples to demonstrate what we're looking for.
198:11 - Now those work fine for our purposes,
198:13 - but they have two big drawbacks.
198:16 - Firstly, prompts have size limits.
198:19 - We're limited in how much we can include in a prompt.
198:22 - Secondly, larger prompts use more tokens,
198:25 - so will be expensive when scaled.
198:28 - But there's actually a bigger problem than that.
198:31 - OpenAI's models have been trained on text
198:33 - openly available on the internet.
198:35 - Now that's great for when you want to use them
198:38 - for creativity, general Q&A,
198:41 - like we've been doing with our chatbot,
198:43 - translation and many other general tasks as well.
198:46 - But what it's not good for is answering questions
198:49 - that are specific to your circumstances.
198:52 - So consider this, you have a company
198:54 - and that company has specific policies and systems.
198:57 - So you have your own opening hours,
198:59 - you might have shipping fees, a returns policy,
199:03 - of course you'll have contact details
199:05 - and many other things besides.
199:07 - Now imagine you ask a chatbot like the one we just made,
199:10 - a specific question about your company.
199:13 - What are you going to get back?
199:15 - Well, you're going to get hallucinations.
199:18 - And what are hallucinations?
199:20 - Well, if the AI doesn't know the answer,
199:23 - it gives you a linguistically plausible incorrect answer.
199:27 - So it basically goes into the world of fantasy.
199:30 - Now, although they're improving,
199:32 - AI models are not that good at saying, I don't know.
199:35 - Remember, what these models do is predict the likelihood
199:38 - of a token or language chunk coming next.
199:41 - And this is one of the biggest problems with AI
199:43 - when working with facts.
199:44 - So if I ask what my company's opening hours are,
199:47 - it will likely say something like 9 a.m. to 5 p.m.
199:50 - and close on Sundays,
199:51 - just because that is a plausible answer.
199:53 - Now, fine tuning can help with this problem.
199:57 - By uploading your own dataset,
199:59 - you can give the model the information it needs
200:01 - to answer questions specific to your situation.
200:04 - So let's go.
200:05 - And we're going to start by thinking about
200:07 - how we can convert our chatbot,
200:09 - which we still have all of the code for right here,
200:11 - into a finely tuned support bot for my new company.
200:14 - So let's check that out next.
200:20 - Okay, so for this fine-tuned chatbot,
200:22 - we're going to use a lot of the same HTML and CSS
200:25 - as the previous project.
200:27 - But before we get to work on the AI,
200:29 - let's just make some quick changes to the design
200:32 - so it better fits our purpose.
200:34 - Up here, I've got the new We Wing It logo,
200:37 - which is just drone-logo.
200:38 - So let's just swap that out right here.
200:41 - And because that logo is slightly smaller,
200:43 - we need to just make a quick change to the CSS.
200:46 - Right here on line 54,
200:48 - the logo has got a width of 45 pixels.
200:51 - I'm going to change that to a width of 50 pixels.
200:54 - Okay, that's looking better.
200:55 - Now back in index.html, we need to change the name
201:00 - to We Wing It drones.
201:02 - And the sub-header will now be delivery support chat.
201:07 - And to give us a little bit of extra room in the header,
201:09 - I'm just going to bring this down to ID.
201:12 - And let's just update that.
201:14 - It doesn't do anything, it's just for aesthetics.
201:17 - Now I'm also going to change this one hard-coded message.
201:21 - So now it's going to say,
201:22 - how can I help you with your We Wing It drone delivery?
201:27 - And lastly, we should come up here and change the title,
201:31 - even though it doesn't make any difference
201:33 - in the mini browser.
201:34 - Okay, let's save that.
201:36 - And there we are, everything is looking fine.
201:38 - Now the rest of the CSS and HTML is going to stay the same,
201:42 - but under the hood, we're going to completely change
201:45 - the way we use open AI.
201:47 - So this is We Wing It,
201:49 - an accident-prone drone delivery company
201:52 - with some very unhappy customers.
201:54 - Now We Wing It needs a way to communicate
201:56 - with their customers quickly and easily,
201:58 - so they want an AI support bot.
202:00 - Let's just check what exactly we're trying to achieve here.
202:04 - We want a chat bot with the ability to answer questions
202:07 - specific to our company.
202:09 - If the chat bot doesn't know the answer,
202:11 - rather than hallucinate,
202:12 - it should advise the user to email or phone.
202:15 - Okay, let's crack on.
202:17 - Now we know what we're doing and why we're doing it.
202:20 - Let's take a high-level overview of the AI process
202:23 - to get an idea of how we're going to do it.
202:31 - So how do we get this highly tuned chat bot off the ground?
202:34 - Let's have a quick look at the whole process.
202:37 - Firstly, we need data.
202:38 - This is just like the examples we used
202:40 - in the few-shot prompts,
202:42 - but it's going to be much longer and specifically formatted.
202:45 - So we'll be using the CSV format
202:48 - or the comma-separated values format.
202:51 - It also works in JSON format,
202:53 - but basically at this point,
202:54 - we need to make sure the chat bot
202:56 - has got all of the information it needs
202:58 - to do its job properly.
203:00 - Now secondly, as the fine-tune process
203:02 - takes a long time to run, we don't want errors.
203:05 - So let's use OpenAI's data preparation tool
203:08 - to process our data so the format is correct
203:11 - and it won't get rejected.
203:13 - Now this is a CLI or command line interface tool,
203:16 - and if you haven't used the command line before,
203:19 - don't worry, we'll go through it step by step.
203:21 - The third thing we need to do is actually upload our data
203:25 - to OpenAI and tell it to make our fine-tuned model.
203:29 - Again, we do this with a CLI tool.
203:32 - It's very quick to get it going,
203:34 - but it can take a long, long time
203:36 - for the request to be processed.
203:39 - Now when that process ends,
203:41 - we will have our own special endpoint,
203:43 - which we can then use with our chat bot.
203:45 - But at that point, we'll need to think about the changes
203:48 - we need to make to our existing code,
203:50 - because to use the new model,
203:51 - we'll have to make some adjustments.
203:54 - All of the code we've got at the moment
203:55 - is very specific to the GPT-4 model we've been using.
203:59 - We'll have to change things up a little bit here.
204:02 - Okay, so that's how it works.
204:04 - Let's make a start on the data.
204:09 - Okay, so we need some data to fine-tune our model,
204:12 - so let's deal with that next.
204:15 - And the first question is,
204:16 - how much data do you need to fine-tune a model?
204:19 - Well, the advice from OpenAI is you should provide
204:23 - at least a few hundred high-quality examples
204:26 - ideally vetted by human experts.
204:29 - And what's more, OpenAI says
204:31 - increasing the number of examples
204:33 - is usually the best and most reliable way
204:36 - of improving performance.
204:38 - So if you were setting this up in the real world,
204:40 - you would basically grab all of your company's
204:42 - support tickets and customer service emails
204:45 - and anything else relevant that you can lay your hands on.
204:47 - And then as OpenAI recommends,
204:49 - you would have a human check it for accuracy and relevance.
204:53 - Now for the app we're building today,
204:55 - we're going to use a relatively small amount of data,
204:58 - but the principle is exactly the same.
204:59 - That is what I want to show you,
205:01 - the principle of how to fine-tune data.
205:04 - Now how we format that data is really important.
205:07 - OpenAI wants the data to be in JSON-L format
205:11 - and the docs give us an example.
205:13 - Now JSON-L is data formatted with JSON on each line.
205:18 - Each line has to be valid JSON in its own right
205:20 - and each line must end with a new line character.
205:23 - Now, if you haven't used JSON-L before, don't worry.
205:26 - We won't be writing it ourselves.
205:28 - We'll be using a special tool to create it.
205:31 - So we'll actually be working with much simpler CSV
205:34 - or comma separated values data
205:36 - and we'll let OpenAI's tool do the heavy lifting.
205:40 - Now as well as wanting JSON-L format,
205:43 - OpenAI gives us some further criteria
205:45 - for the format of our data.
205:47 - It wants each prompt to end with a separator
205:51 - to show where the prompt ends and the completion begins.
205:54 - It wants each completion to start with a white space
205:58 - and it wants each completion to end with a stop sequence
206:01 - to inform the model where the completion ends.
206:04 - Now the stop sequence is something
206:06 - that we haven't needed yet, but we will talk about it
206:09 - when we get to that point in the project
206:11 - and everything will become clear.
206:13 - To be honest, all of this sounds like a bit of a pain
206:16 - and actually in their example data,
206:18 - they didn't even show us exactly what they wanted
206:21 - with the stop sequences and the separator.
206:23 - So we don't even have that to help us.
206:26 - But good news is right here.
206:29 - You can use our CLI data preparation tool
206:32 - to easily convert your data into this format.
206:36 - So we're going to let that tool do everything for us.
206:39 - So don't worry at all if that looks intimidating.
206:43 - Okay, in the next grim,
206:44 - let's take a look at the data we'll be using.
206:50 - Okay, let's take a look at the data we're going to use.
206:53 - Now writing JSON out by hand is a pain.
206:56 - So I've organized this in a spreadsheet
206:58 - using comma separated values or CSV.
207:02 - All we've got here is two columns,
207:04 - prompt and completion.
207:06 - Each prompt is a question from a customer
207:08 - and each completion is an answer from customer service.
207:12 - So we've just got two columns.
207:14 - I think we can work with that.
207:15 - And I've got 38 pairs of prompts and completions.
207:19 - Ideally, I would have 10 times that number or even more.
207:23 - But this small dataset is going to allow us
207:25 - to see the principle of how fine tuning works.
207:28 - Now I've also pasted the CSV data into this file right here.
207:33 - And it doesn't look like it's formatted,
207:35 - but actually CSV formatting is really simple.
207:38 - We've got the two column headings right here.
207:41 - And then each line contains one prompt completion pair.
207:44 - So this is the prompt right here
207:47 - and this is the completion after the comma.
207:50 - Now I just want to draw your attention
207:52 - to the last few pairs because here I've done something
207:55 - which is just a little bit more complex.
207:58 - It's a little bit tricky to see.
208:00 - So I'm just going to take this last prompt completion pair
208:03 - and space it out a little bit.
208:05 - Now, instead of just having one question and one answer,
208:09 - the prompt actually consists of several parts.
208:12 - The first part is a summary.
208:14 - Then we've actually got a short conversation.
208:17 - So we've got something that the customer has asked
208:21 - and then we've got the agent's reply.
208:23 - And then the customer has responded to the agent.
208:27 - And then we finish with the agent and a space
208:31 - and then we get the completion at the very end.
208:35 - So basically all of that is the prompt
208:37 - and that is the completion.
208:40 - Now I've done that just to show you
208:42 - that these prompts don't have to just be one question
208:44 - and one answer.
208:45 - They can actually involve lots and lots of dialogue.
208:48 - So if you have got a lot of customer service data,
208:51 - you can format it in this way with the completion
208:54 - just being the final answer to the query
208:57 - and that is going to really help train your chatbot.
209:00 - Now you don't need to stick to one data style.
209:03 - As long as you're working with prompts and completions,
209:06 - you can mix as I've done here,
209:08 - some of the prompts will be one question
209:10 - and some of the prompts will actually be whole conversations.
209:14 - Okay, let me just put this data back as it was.
209:17 - Now, as we're going to be working with this data
209:19 - in the terminal, you need to download it
209:21 - or have some data of your own in a similar format.
209:24 - You can download it just by clicking on this slide
209:27 - and that's going to take you through
209:28 - to a Google Sheets version
209:30 - which you can save a local copy of
209:32 - or you can come down here to this cog icon
209:35 - and when you click on that, it will bring up a menu
209:37 - and click download a zip which will give you a zipped folder
209:40 - and when you unzip it,
209:42 - you'll see all of the files from this project.
209:45 - The one you're interested in of course
209:47 - is the We Wing It Data CSV.
209:49 - So I'm going to take that file
209:51 - and save it in my apps folder in a file called We Wing It
209:55 - and there we are.
209:56 - There is my data ready for the next step.
209:59 - Okay, we've got some work to do with this data
210:01 - before we can actually upload it and fine tune a model.
210:05 - We are going to use OpenAI's data preparation tool
210:07 - to do that for us but before we can do that,
210:10 - we need to set up our command line interface environment.
210:14 - So next, let's open up the terminal and tackle that.
210:21 - For the next part of the project,
210:23 - we're going to be using the terminal or command prompt.
210:26 - If you're new to using the terminal,
210:28 - it can look pretty intimidating.
210:30 - It's actually not that bad and to help you along,
210:33 - I've created a file over here called terminal-commands.md
210:38 - and I've pasted in all of the commands we're using
210:40 - in this scrim so you can refer to that quickly
210:42 - and easily if you need to.
210:44 - Now to open the command prompt in Windows,
210:47 - you can use the Windows key plus S
210:49 - and enter CMD in the search field.
210:51 - On MacBook, it should be right there in the launcher
210:54 - and of course in either case,
210:55 - you could use the terminal in VS Code.
210:58 - Now I've got the terminal open and the OpenAI tool
211:01 - that we're going to use requires Python 3.
211:04 - Now if you've already got Python 3 installed,
211:06 - then you're good to go.
211:08 - If you've never used Python before
211:09 - and you're starting to freak out,
211:11 - don't worry, we're not coding in Python.
211:13 - It's just needed in the background to run OpenAI's tools.
211:17 - Now you can check to see if you've got Python 3 installed
211:20 - with this command.
211:21 - So it's just Python 3 dash dash version
211:25 - and when I hit enter,
211:26 - it tells me that I've got Python 3.11.1.
211:30 - Now if you haven't got Python installed,
211:32 - you can click on this slide
211:33 - and that is going to take you to the official Python site
211:36 - or if you're on Mac, you can use Homebrew
211:39 - or if you're on Linux, you can probably get Python
211:41 - from your distros repository.
211:44 - Check their docs for details.
211:46 - Now in a moment,
211:47 - we're also going to need the pip package manager.
211:50 - If you have Python installed,
211:51 - you probably already have pip
211:53 - and you can check by doing pip dash dash version
211:56 - and that tells me that I have got pip installed.
212:00 - If for some reason you haven't got pip installed,
212:02 - this command right here, python3 dash m,
212:06 - ensure pip dash dash upgrade
212:08 - will install the latest version.
212:11 - Okay, now we should be good to go.
212:12 - So let's install the openAI CLI using pip install
212:17 - dash dash upgrade openAI.
212:20 - That should install the openAI tool
212:22 - and you can check that by running the openAI command
212:25 - and that should give you a list of commands
212:27 - that are specific to this openAI tool.
212:30 - Now there's one more thing I want to do.
212:32 - The fine tune model we create
212:34 - is going to be specific to us, only we can use it.
212:37 - So the openAI tool will need our API key.
212:41 - We don't need that to prepare the data,
212:43 - but we will need it before we upload.
212:45 - So we might as well do it right now.
212:47 - You can add the openAI key with this command.
212:50 - It's basically export and then everything that we've got
212:53 - in this mth.js file in this line of code right here.
212:57 - But be sure to swap out this colon for an equals.
213:00 - Now when you've done that, press enter
213:02 - and you're not going to get any special acknowledgement
213:05 - that your key's been accepted.
213:06 - Just assume it has been.
213:08 - And now with all of that setup complete,
213:10 - we're ready to work with the data separation tool.
213:13 - So let's get stuck into that next.
213:19 - Okay, so we've got the openAI CLI up and running
213:23 - and we've given it our API key.
213:25 - Now let's use it to prepare our data.
213:27 - And again, I've listed the CLI commands
213:30 - that I'm going to use in this file right here.
213:33 - So our first task is to cd to the folder
213:36 - where we've stored our data.
213:38 - Now I've saved my data in a folder called we-wingit
213:42 - in apps, which is in documents.
213:44 - So let's head back to the terminal
213:46 - and I'm going to say cd documents slash apps slash we-wingit.
213:51 - CD here stands for change directory.
213:54 - And of course you'll need to navigate to wherever it is
213:57 - you saved your data file.
213:59 - Okay, having navigated to the correct folder,
214:01 - we can start our fine-tune preparation.
214:04 - So we do that with this command.
214:06 - And what we're doing here is telling the terminal
214:08 - to use the openAI fine-tunes tool to prepare our data.
214:13 - Now this F flag is going to identify the file of data
214:17 - that we want to prepare.
214:19 - Now my data was called we-wingit-data.csv.
214:23 - So what I need to do is just put that
214:25 - right there on the end.
214:26 - Now at this point, you might run into a problem.
214:29 - The first few times I did this, it worked fine.
214:32 - And then suddenly at this point,
214:33 - I started to hit an error.
214:35 - Well, that's the nature of working with new technology.
214:38 - Things can change.
214:39 - And the error I got was this, missing pandas.
214:43 - Well, if openAI wants pandas, openAI gets pandas.
214:48 - So what you need to do then is say pip install openAI pandas.
214:53 - Once you've done that, hit enter, let it do its thing.
214:56 - And then we can try the data preparation task once again.
215:00 - So this is the exact same command.
215:03 - This time it works and it gives us some information.
215:06 - It knows our file is formatted as a CSV file.
215:09 - It also complains about the number
215:11 - of prompt completion pairs we're using and says,
215:13 - in general, we recommend having at least a few hundred
215:16 - examples.
215:17 - Well, we know that already,
215:18 - but this is for demonstration purposes.
215:20 - So it's absolutely fine.
215:22 - Now I'm going to save you reading the rest of this text here
215:26 - because actually it's telling us something we already know.
215:28 - We talked about the format of our data
215:30 - and all of the features that we need.
215:32 - And we know that we need a separator to inform the model
215:36 - when the prompt ends and the completion begins.
215:38 - We know that each completion needs to start
215:40 - with a single white space.
215:41 - And we know that each completion should end
215:43 - with a stop sequence to inform the model
215:46 - when the completion has ended.
215:48 - But this tool is great because it's basically going
215:51 - to do everything for us.
215:53 - So down at the bottom, it's already told us
215:55 - that it's necessary to convert this to JSON L.
215:58 - That's absolutely fine.
215:59 - It's recommending us to add a suffix separator
216:02 - to all of our prompts.
216:03 - Well, let's say yes to that.
216:05 - Now it's recommending adding a suffix ending
216:08 - of a new line character to all of the completions.
216:11 - Let's say yes to that and press enter.
216:13 - And now it's recommending us to add a white space character
216:16 - to the beginning of all of the completions.
216:19 - Again, let's say yes.
216:22 - Now we just need to confirm that we're ready to proceed
216:24 - and it goes through its process
216:26 - and it tells us that it has created this file for us.
216:31 - And it invites us to take a look,
216:32 - which is a really good idea.
216:34 - If you go back to your folder,
216:35 - wherever you stored your data,
216:37 - you should see this prepared JSON L file waiting for you.
216:41 - Now, just so we can see this data,
216:43 - I'm going to copy it into a file in the editor.
216:46 - And there we are.
216:47 - And you can see that it's added the prompt key
216:49 - and completion key to each pair.
216:52 - It's added white space before the start of every completion.
216:56 - It's added a new line character
216:58 - at the end of every completion.
217:00 - And of course it's added a separator
217:02 - at the end of every prompt.
217:04 - And we could have done all of that by hand
217:06 - and just used the tool to check,
217:08 - but wow, what a lot of boring work that would have been.
217:11 - Okay, next we need to fine tune our data.
217:14 - So let's come on to that in the next scrim.
217:20 - Okay, let's get to work on the fine tune.
217:23 - At the end of the data preparation process,
217:25 - we got this instruction.
217:27 - So let's just break down what that is telling us to do.
217:31 - Firstly, we're using the open AI CLI tool
217:34 - to access the API.
217:36 - Then we need to use this fine tunes dot create command.
217:40 - And we use the T flag here to introduce our training data.
217:44 - And we save that in a file called we dash wing it dash data
217:49 - underscore prepared dot JSON L.
217:52 - Our training data, which has been prepared
217:54 - into the JSON L format.
217:56 - Now we need to add just a little bit more to the end.
217:59 - The docs tell us that we can use an M flag
218:02 - to specify the base model to use.
218:04 - And we're going to use the DaVinci model.
218:07 - If we leave that blank,
218:08 - it will actually default to the older query model.
218:11 - Okay, let's put that command in the terminal.
218:14 - And when I press enter, we get this message.
218:18 - Now open AI needs to enqueue this and it takes some time.
218:21 - The quickest for me has been a few minutes.
218:23 - The longest was literally all night.
218:26 - Now down here, it says stream interrupted,
218:28 - client disconnected.
218:29 - That is really common.
218:31 - It doesn't mean that the fine tune process has stopped.
218:33 - It just means that open AI
218:35 - is no longer giving us live updates.
218:38 - And what we can do is take this command here,
218:40 - paste it and run it.
218:42 - And that will reconnect us to the information stream.
218:44 - And eventually we'll get this message with an emoji
218:47 - that tells us we have been successful.
218:50 - And then what we see down here
218:52 - is actually our very own fine tuned model.
218:55 - It's everything from DaVinci right up until the date.
218:59 - Now that's pretty cool,
219:00 - but before we can use that model in our app,
219:03 - we need to make some changes to our JavaScript.
219:05 - So let's go ahead and do that next.
219:11 - Okay, so to use this model,
219:13 - we need to update our existing code.
219:16 - The first thing to remember here
219:17 - is that we have fine tuned a DaVinci base model.
219:20 - And at the time of recording,
219:22 - you can't fine tune GPT-4.
219:24 - That means that we need to get rid of
219:26 - loads of this specific format.
219:28 - Now I've divided this process into several parts
219:31 - and we'll have challenges for them over several scrims.
219:34 - The first task is to change this array,
219:37 - which is holding the conversation.
219:38 - DaVinci models just need a string holding the conversation
219:41 - and we won't be using an instruction.
219:44 - Here are two mini challenges to do just that.
219:48 - Firstly, change conversation array to conversation string.
219:52 - And you can initialize it to an empty string.
219:55 - Now I've just put a warning here.
219:57 - Think about how this is going to work.
219:59 - Is there any other change you need to make?
220:01 - I'm just going to let you think about that.
220:04 - Once you've done that,
220:05 - you can come down here and update conversation string
220:09 - with just the user's input.
220:11 - So instead of pushing this whole object,
220:13 - all we actually want to push
220:14 - is whatever the user has inputted.
220:17 - Then you can just log out conversation string
220:20 - to check it's working.
220:21 - And I've disabled the fetch reply function
220:23 - as at the moment, we just get an error
220:25 - if we sent off the string instead of the array of objects
220:28 - to the current endpoint that we've got in our code
220:31 - right here.
220:32 - Okay, pause now, take all the time you need
220:35 - and I'll see you back here in just a moment.
220:44 - Okay, hopefully you managed to do that just fine.
220:46 - So let's just delete everything we've got here.
220:49 - We're going to make this conversation string.
220:51 - I'm just using the abbreviation STR
220:54 - and we'll initialize that to an empty string.
220:57 - Next, we need to come down here
220:59 - and update conversation string with just the user's input.
221:03 - So again, we need to convert conversation array
221:06 - to conversation string.
221:08 - And instead of push, we can just use plus equals.
221:13 - And let's log out conversation string and hit save.
221:17 - Okay, I'll just type something in the box
221:19 - and look, we get an error
221:22 - and it's a type error assignment to constant variable.
221:25 - Well, I did warn you here,
221:27 - think about how this is going to work.
221:28 - Is there any other change you need to make?
221:30 - Well, yes, we need to change this const for a let.
221:34 - Let's try again.
221:36 - And there we are, it is working.
221:38 - So that is two of the jobs off the list.
221:41 - Let's move on to what happens
221:43 - inside the fetch reply function.
221:49 - Next up, we need to change the end point.
221:51 - Create chat completion is specific to models like GPT-4.
221:56 - We are going back to the completions endpoint
221:58 - which uses create completion.
222:00 - So I'm just going to come in here and delete chat.
222:03 - Now for your challenge,
222:05 - I want you to swap out the model GPT-4
222:08 - for your fine tuned model.
222:10 - And you can get the model name
222:11 - from right at the end of the fine tune process.
222:15 - Now, if you've closed your terminal window, don't worry,
222:17 - log into open AI, go to the playground,
222:20 - come up here to where we've got model
222:23 - and just click this down arrow.
222:24 - That will bring up a list of all the regular models
222:27 - and also all of the fine tune models that you have created.
222:32 - Now for the second part of the challenge,
222:33 - our fine tune model needs a property
222:36 - called prompt not messages.
222:39 - So you just need to swap out messages for prompt.
222:42 - And you also need to update this reference
222:44 - to conversation array.
222:46 - Conversation array does not exist anymore.
222:49 - Once you've done that,
222:50 - you can run a test by logging out the response.
222:53 - Now, before you do the challenge,
222:54 - I'm just going to uncomment this fetch reply function call.
222:58 - And I have already commented out these two lines of code.
223:01 - When we've got the endpoint working properly,
223:04 - we'll uncomment them and make some adjustments.
223:07 - Okay, pause now, get this challenge sorted
223:09 - and I'll see you back here in just a moment.
223:17 - Okay, hopefully you managed to do that just fine.
223:20 - So I'm going to come down here
223:22 - and I'm going to just grab the name of my fine tune model
223:26 - and I've got it right here.
223:28 - And I just need to paste it right here.
223:31 - Next, this messages property needs to become prompt
223:34 - and conversation array needs to become conversation string.
223:38 - Okay, let's log out the response and see what we get.
223:42 - And I'm just going to hit save
223:43 - and ask a random question in here.
223:47 - Let's open up the console and see what we get.
223:50 - And there we are, we've got a response.
223:52 - And the response is a little bit strange.
223:54 - I'm just going to copy it into the editor
223:56 - so you can see it clearly.
223:58 - The response to my question, how are you today?
224:01 - Was bizarrely this, it sounds like you've been
224:06 - on a real roller coaster, I'm sorry.
224:08 - Well, that's okay.
224:09 - We've still got quite a long way to go
224:11 - before we get this fine tune chatbot working.
224:13 - So it doesn't matter at all
224:15 - that it's giving us quite a random answer.
224:17 - The important thing is we're managing to make an API call
224:21 - to this new model and we're getting a response.
224:24 - So in the next grim, let's get to work
224:25 - on these two lines of code right here.
224:28 - We need to make a few more changes to the JavaScript
224:33 - before we can do some testing.
224:35 - So at the moment in these two commented lines of code,
224:39 - we are pushing to the old conversation array
224:42 - and we're also sending our completion
224:44 - to render typewriter text.
224:46 - Those two lines of code are not going to work anymore.
224:49 - We need to make some changes.
224:51 - So firstly, let's uncomment this line of code
224:54 - and we'll change this to conversation string.
224:57 - And instead of using push, we will of course use plus equals.
225:02 - And what is it that we need
225:03 - to update conversation string with?
225:06 - Well, I've just pasted in the response that we got up here
225:10 - and it's pretty similar to the responses we were getting
225:13 - when we were using the chat GPT models.
225:15 - Now, if we look down here, this one ends with messages.
225:19 - Messages doesn't exist.
225:21 - We've got choices,
225:23 - but the completion is actually stored in text.
225:26 - So let's just change message for text.
225:30 - And of course we don't need these brackets.
225:33 - And now that we're updating conversation string,
225:35 - just with the completion,
225:37 - we can actually reuse this code here
225:39 - because we also need to send the completion
225:42 - to render typewriter text.
225:44 - So let's just paste that in here and now it should work.
225:48 - So I'm just going to delete a couple of console.logs
225:52 - and let's delete all of this ugly code as well
225:55 - and give it a test.
225:58 - And okay, it's working.
226:00 - And it said, I've got some lovely things
226:02 - for sale in my shop.
226:03 - I hope you like.
226:05 - Now the reason why it's finished on like with no punctuation
226:09 - is because now we're using a DaVinci model again,
226:12 - we need to set max tokens
226:14 - at the moment it is set to 16 by default.
226:17 - Let's set it to something much higher.
226:19 - And I'm just going to try that again.
226:27 - Wow, and now it's gone a little bit crazy.
226:29 - All I said was, hey there.
226:31 - And it's given us all of this stuff,
226:33 - which is sort of related to the data we uploaded.
226:38 - It certainly got the email address right.
226:40 - And it's given us some rather random phone numbers,
226:43 - although this one is not too far away
226:45 - from the one we told it about.
226:47 - And then it starts going on about using cookies
226:49 - on the website.
226:50 - Well, this is more than just hallucination.
226:53 - It's actually gone a bit crazy.
226:55 - And what's worse is that it still didn't finish
226:58 - on a complete sentence.
226:59 - So if we whacked max tokens up to something much higher,
227:02 - like a thousand, I've got a feeling
227:04 - that this crazy chatbot would go on and on.
227:07 - But hey-ho, at least it's working.
227:09 - The basic mechanics are fine.
227:10 - So in the next grim,
227:12 - let's start figuring out what's going wrong.
227:17 - So we have the fine-tune chatbot working or sort of,
227:21 - because actually it's giving us nonsensical gibberish.
227:25 - So let's just check the settings we have.
227:27 - The presence penalty and frequency penalty
227:30 - aren't doing anything very much.
227:32 - They're almost at their default.
227:33 - So I'm not too worried about them.
227:36 - Now with open AI, when the completions are a bit weird,
227:39 - the first thing we might think of is temperature.
227:42 - Remember, temperature controls how daring the model will be.
227:45 - How creative, how inventive.
227:47 - Now we don't want creative, we want factual.
227:50 - So let's put the temperature down to zero.
227:54 - And let's see if that has had any effect.
228:00 - Okay, that completion is like something from a horror movie.
228:03 - Now the chatbot is claiming to be a human.
228:05 - It's begging us not to kill it.
228:07 - And it's just saying, please, please, please.
228:09 - And then creating a word so long it actually breaks our CSS.
228:14 - So I'm just going to refresh to get rid of that,
228:17 - because it's just a bit scary.
228:19 - Okay, so the temperature hasn't magically solved the problem,
228:22 - but I think a low temperature will be useful.
228:24 - So I'm going to leave it there.
228:26 - Now, before we do anything else,
228:28 - let's go back to the criteria for the format of our data.
228:31 - The first one that we had says,
228:34 - each prompt ends with a separator to inform the model
228:37 - when the prompt ends and the completion begins.
228:40 - And actually when we were prepping the data,
228:42 - it told us that it would use this arrow as the separator.
228:46 - So what we need to do is add the separator
228:49 - to the end of our prompt.
228:50 - And I've got the JSONL file right here.
228:53 - And we can see that each prompt does indeed end
228:56 - with a space and an arrow separator.
229:00 - Now we're not adding those separators
229:01 - to our conversation string.
229:03 - And I think that might be causing part of the problem.
229:06 - So in fact, I'm just going to log out
229:08 - conversation string right here.
229:12 - And let's be brave and just see if the chatbot
229:15 - is going to scare us again.
229:23 - And it's done the same creepy thing again.
229:25 - Now let's just have a quick look down in the console.
229:28 - And what we can see there,
229:29 - if I just cut and paste it into the editor,
229:32 - is that we're just getting one continuous line of text.
229:36 - So hey, and I'm not a robot,
229:38 - look like they're being said by the same speaker.
229:41 - But in fact, that was our prompt.
229:43 - And this is the beginning of the completion.
229:46 - So we have got problems there.
229:48 - And that brings us to a challenge.
229:51 - And your challenge is this.
229:53 - I want you to add the arrow separator
229:55 - to the end of our prompt
229:56 - as it is added to conversation string.
229:59 - And I've put here a space before the arrow separator
230:02 - because that is how it's formatted in our JSON-L data.
230:06 - Okay, I'm going to leave the console.log that I just added.
230:10 - So when you're done with this,
230:11 - you can just type something into the chatbot
230:13 - and give it a test.
230:15 - Pause now, get that sorted,
230:17 - and I'll see you back here in just a minute.
230:25 - Okay, hopefully you managed to do that just fine.
230:27 - So I want to come down here and put this in backticks.
230:32 - And now we'll use the dollar sign and the curly braces
230:36 - so we can access the user input.value.
230:39 - I'm going to start with a space
230:41 - and I'm going to end with a space and an arrow separator.
230:45 - Okay, let's give that a try.
230:51 - Okay, so we've seen an improvement there.
230:54 - This is by no means perfect,
230:56 - but at least it's working a little bit.
230:58 - Now it feels like we're talking to a representative
231:01 - of the We Wing It company.
231:03 - It's got the email address.
231:05 - It's still giving us a load of hallucination and weirdness.
231:08 - And if we look down in the console,
231:10 - we have at least got our separator between hey,
231:13 - which was our prompt, and odia,
231:15 - which was the beginning of the completion.
231:18 - So we've made some small progress here.
231:21 - But what else could be going wrong?
231:23 - Let's just go back to our data formatting criteria.
231:26 - So we've sorted the first one.
231:28 - Secondly, it says each completion
231:30 - should start with a single white space.
231:34 - Well, if we have a look down in the console,
231:36 - that appears to be true.
231:38 - Or is it?
231:40 - I think the next thing that we should do
231:42 - is come down here to fetch reply
231:44 - and where we're updating conversation string,
231:48 - with the completion, we should explicitly add
231:51 - a white space at the beginning.
231:53 - So again, I'm going to put this in back ticks,
231:57 - use the dollar sign and curly braces
231:59 - to access the completion from the response.
232:01 - And now I'll add my white space at the beginning.
232:05 - Okay, let's hit save and try that one more time.
232:11 - And that's not really made any improvement.
232:14 - I think that's just as crazy as it was before.
232:16 - But we have now got that white space there for sure.
232:19 - We know it's there.
232:21 - We've ruled that out as a potential cause of problems.
232:24 - So let's go back to the data criteria
232:27 - and look at the third one.
232:28 - Each completion should end with a stop sequence
232:31 - to inform the model when the completion ends.
232:34 - Well, we're back with the mysterious stop sequence.
232:37 - We keep coming across it
232:38 - and we've never really looked at it in detail.
232:41 - So why don't we deal with that next?
232:48 - Let's take a look at the stop sequence,
232:50 - a way of stopping the model from generating tokens.
232:53 - So what exactly is it and how does it work?
232:56 - Well, it's an optional setting that tells the API
232:59 - to stop generating tokens at a given point.
233:02 - The completion will never contain the stop sequence
233:06 - and the stop sequence is an array.
233:09 - And that's because we can have multiple stops
233:11 - in a stop sequence.
233:13 - Now all of that is quite theoretical.
233:14 - So let's just jump straight into an example
233:16 - and you'll see what I mean.
233:18 - Right here, I've got a basic API request set up
233:22 - using the text DaVinci 003 model.
233:25 - And I'm just asking it to list some great books
233:27 - to read on the topic of coding.
233:29 - So I will uncomment this function call and hit save
233:33 - and let's open up the console and see what we get.
233:36 - And there we are, we get 10 good book recommendations.
233:40 - Now I'm going to come in here underneath the max tokens
233:43 - and I'm going to add a stop sequence
233:45 - and the property that we want is just stop.
233:48 - The stop sequence is an array
233:50 - and each individual stop will be a string.
233:53 - Now we can have up to four stops in this array.
233:56 - I'm only going to use one.
233:57 - And if we look at the formatting of what's in the console,
234:00 - each list item starts with a number and a dot.
234:05 - So it's one dot,
234:06 - code the hidden language of computer hardware and software
234:09 - by Charles Petzold,
234:10 - two dot, Python crash course by Eric Maths.
234:14 - So the stop that I put in here just to experiment with
234:16 - is going to be two dots.
234:18 - Let's see what happens when I press save.
234:21 - We get a list with only one item.
234:25 - So what happened here?
234:26 - Well, if you'll remember,
234:28 - the completion will not contain the stop sequence.
234:31 - So the model was prevented from writing two dots.
234:35 - And because it won't write a stop sequence,
234:37 - at that point, the completion is cut off.
234:40 - So if we wanted a list of five books,
234:42 - we could change this to six dot.
234:45 - And there we are, we have got a list of five books.
234:48 - Now you can use anything as a stop.
234:50 - You can put whatever characters in there
234:53 - you might find in your completion,
234:54 - but you just need to know what you're doing with it.
234:57 - Obviously, if you put something like the letter A,
235:00 - well, that's going to cause all sorts of problems.
235:03 - There we are.
235:04 - The completion has actually been cut off
235:05 - on the fourth letter of the book title,
235:08 - which was obviously an A.
235:09 - Now, when we're working with chat bots,
235:11 - it's very common to use the new line value
235:14 - as a stop sequence.
235:16 - Why?
235:16 - Because often you want the bot to give you
235:18 - one paragraph as an answer,
235:21 - and then you want the user to respond.
235:24 - If you don't use a stop sequence with a chat bot,
235:26 - you run the risk of the chat bot answering
235:28 - and asking questions and having a conversation with itself
235:32 - that becomes ever more bizarre and illogical
235:35 - until it hits the token limit and that stops it.
235:38 - Now with the general purpose chat bot we built with GPT-4,
235:41 - we didn't need to add a stop property
235:44 - because GPT-4 has actually got that specific syntax,
235:48 - the object with the role and the content properties.
235:51 - DaVinci and other models don't have that,
235:53 - so we can use the new line character as a stop
235:56 - to stop the bot from continuing the conversation on its own.
236:00 - Okay, let's go back to the app and add a stop property.
236:06 - Okay, so if we refer back to our slide,
236:09 - we can see that each completion should end
236:11 - with a stop sequence to inform the model
236:13 - when the completion ends.
236:15 - And when we were preparing our data,
236:18 - we actually selected this option
236:20 - to add a suffix ending of a new line character,
236:23 - which is backslash N.
236:25 - So as we add completions to our conversation,
236:28 - we need to include this new line character at the end
236:31 - as a stop sequence.
236:33 - Now we're also using this arrow as a separator,
236:36 - and when experimenting with this,
236:37 - I sometimes found arrows popping up in completions
236:41 - and some cases where the chat bot seemed
236:42 - to just get into a conversation with itself,
236:45 - asking itself questions and giving itself answers.
236:48 - So we're actually going to add the arrow
236:50 - as a stop sequence as well,
236:52 - because we never want the model to generate an arrow.
236:55 - Okay, so here is your challenge and it comes in two parts.
236:59 - Firstly, I want you to add the new line character,
237:01 - which is backslash N and the separator as a stop sequence.
237:06 - And actually, you don't need to have a space
237:08 - in the stop sequence.
237:10 - Secondly, I've said here,
237:11 - I want you to add something to the line below
237:14 - where we update conversation string.
237:16 - So that is this line of code right here.
237:19 - I'm going to leave you to think about that.
237:21 - What would it be good to add to that line?
237:23 - Okay, pause now, get that challenge sorted,
237:26 - and I'll see you back here in just a moment.
237:35 - Okay, so hopefully you managed to do that just fine.
237:38 - So let's come in here and we'll add our stop.
237:41 - And we know that that will be an array
237:43 - and it will be an array of strings.
237:45 - So the first string is going to be the new line character,
237:49 - and the second string will be the arrow separator.
237:52 - And of course, it doesn't matter what order they go in.
237:55 - Now, the second challenge,
237:56 - add something to the line below
237:58 - where we update conversation string.
238:00 - Well, if we refer back to this slide,
238:03 - we're going to add a suffix ending of backslash N
238:06 - to all completions.
238:08 - So this is the completion that we're getting back.
238:12 - Let's just add a new line character to the end of that.
238:15 - And what that is going to do
238:16 - is make it really clear to the model
238:18 - when it reads conversation string
238:20 - that it has reached the end of a completion
238:23 - because that is what the new line character will tell it.
238:26 - And likewise, the arrow separator
238:28 - that we're adding to the stop sequence here,
238:30 - which we're already adding to conversation string
238:33 - right here in the event listeners anonymous function,
238:37 - that will tell the model
238:38 - that it's reached the end of a prompt.
238:41 - Okay, let's hit save and see if it's worked.
238:44 - So I'll start again just by saying, hey.
238:48 - All right, and I'm still getting
238:49 - a pretty strange response here.
238:51 - Now I'm going to ask a question from our data.
238:54 - And if we have a quick look at the data,
238:55 - we can see that the customer support team
238:57 - has a phone number.
238:59 - So I'm going to ask it, what is your phone number?
239:02 - Okay, and look,
239:06 - we're getting a much better quality response.
239:09 - It knows the phone number.
239:11 - It is still hallucinating.
239:13 - We're only here from nine to six 30 weekdays.
239:16 - That is not actually true,
239:18 - but the answer is much shorter
239:19 - and it's much more manageable.
239:21 - Now I'm going to ask it a question
239:22 - it can't know the answer to
239:24 - because it simply isn't in the data.
239:27 - I'll ask it, who is your CEO?
239:30 - Okay, interesting.
239:31 - It completely makes up a person, Richard Quing.
239:34 - So for some reason, the large language model
239:37 - has decided that Richard Quing
239:38 - is the most probable name for a CEO of our company.
239:42 - And it's also made up an email address for him
239:45 - but using atwewingit.com.
239:47 - So again, it's a hallucination,
239:49 - but it's an interesting one.
239:50 - So I think where we are at the moment
239:52 - is that this chatbot works,
239:54 - it can see its data,
239:55 - it's giving us short manageable answers,
239:58 - which makes sense,
239:59 - but we're still getting too many hallucinations
240:01 - it is not accurate.
240:03 - Now we are going to get hallucinations
240:05 - because as I said at the beginning,
240:07 - we're not really using enough data.
240:09 - We've got enough data to show the principle,
240:11 - but not enough data for a full production ready app.
240:15 - But that said, there is one last really important aspect
240:18 - of fine tuning that I want to tell you about
240:21 - and it is an epochs.
240:24 - What does that mean?
240:25 - Well, when we were building the model,
240:27 - you might have noticed that as the model was fine tuning,
240:31 - it completed various epochs.
240:33 - And in this case, it completed four epochs.
240:37 - Well, maybe four epochs just wasn't enough.
240:40 - So in the next grim,
240:41 - let's talk about what epochs are,
240:43 - how we can get more of them
240:44 - and how that is going to help us.
240:46 - So let's check that out next.
240:52 - Let's talk about an epochs
240:54 - and I've put here the number of data cycles.
240:57 - So what exactly are an epochs?
241:00 - Well, the N stands for number.
241:02 - So it's the number of epochs.
241:04 - And what that basically means is that it controls
241:07 - the number of times open AI will cycle
241:09 - through the training dataset when fine tuning the model.
241:12 - Now it defaults to four
241:14 - and that is likely fine for larger datasets
241:16 - with hundreds or thousands of prompt completion pairs.
241:19 - But for smaller datasets, it isn't enough.
241:22 - The disadvantage of using higher numbers
241:25 - and therefore more cycles through the training data
241:27 - is that it costs more to do the fine tune
241:30 - with a small dataset like ours,
241:31 - it's still going to be pretty cheap.
241:33 - The other thing to think about is that
241:35 - if you're using a massive dataset,
241:37 - it will take a long time.
241:39 - Now an epochs is something we set
241:42 - when we send the training data to be fine tuned.
241:45 - We can't set it from within our app.
241:48 - So what we need to do is go back to the CLI
241:50 - and we're going to add this
241:51 - to the end of the fine tuning command.
241:54 - It's just dash dash N underscore epochs
241:57 - and then the number of epochs you want.
241:59 - And I'm going to go for 16.
242:01 - You don't have to go for 16,
242:03 - but I got much improved results using 16.
242:06 - You could go higher,
242:07 - but just remember it costs more each time.
242:09 - So that is the same command we used before.
242:12 - Here's the N epochs added on the end.
242:14 - And so the final command will look like this.
242:16 - It's everything we did before
242:18 - plus this N epochs on the end with the double dash.
242:22 - So here is a challenge for you.
242:25 - Use open AI CLI tool to build a new fine-tuned model
242:29 - with N epochs set to 16.
242:32 - You don't need to prepare the data again.
242:34 - That command is all you need.
242:36 - Once you've gone through that process,
242:37 - you will get a new model name
242:39 - and you can slide it in right here,
242:42 - replacing the old model.
242:44 - Once you've done that,
242:45 - go ahead and test it by asking it
242:47 - about weaving its phone number or email
242:49 - or anything else from the data that we've got right here.
242:53 - Go ahead and do that.
242:54 - It won't take very long at all to set up.
242:56 - It might take quite a while for the API
242:58 - to actually do the fine-tuning process.
243:00 - So make yourself a cup of tea
243:02 - and we'll have a look at this together when it's done.
243:10 - Okay, hopefully you got some good results with that.
243:12 - So I'm going to come over to the terminal.
243:14 - There is the command to create the fine tune.
243:17 - I'm going to add the N epoch 16 on the end and press enter.
243:21 - And then eventually having gone through its process,
243:24 - it's completed all of these epochs down to 16.
243:27 - And finally, I have got my new model
243:31 - and here it is right here.
243:33 - So let's go ahead and swap the old model for the new model.
243:39 - Okay, let's hit save and do some testing.
243:41 - So I'm going to start off asking about the phone number.
243:47 - Okay, and that is a really nice answer.
243:50 - It's got the phone number correct and it's also said,
243:53 - we prefer it if you only phone us in an emergency.
243:56 - Now that's really interesting
243:57 - because if we look at the data,
244:00 - we mentioned the phone number several times.
244:02 - If we come down to line 19, well, we can see here,
244:06 - we've got the straightforward question,
244:08 - what is your phone number?
244:09 - And if I just move the mini browser out of the way,
244:12 - it does actually say,
244:13 - we prefer it if people only phone us in an emergency
244:17 - and it gives exactly the same times as the chatbot
244:21 - told us right here.
244:22 - So the chatbot is successfully using our data
244:25 - and that is really, really good.
244:27 - And it's also suggested they email us instead.
244:30 - Okay, now I want to ask it something
244:32 - it can't know the answer to, who is your CEO?
244:35 - In the previous scrim, it hallucinated an answer to this.
244:38 - So let's see what it does.
244:41 - Who is your CEO?
244:44 - And it says, I'm sorry,
244:45 - I don't know the names of the people working here.
244:48 - Now that's really interesting.
244:49 - I've made sure to include in this data,
244:52 - various I don't know statements.
244:54 - If we have a look right here on line 18, for example,
244:58 - what material are your drones made of?
245:01 - Completion, I don't know the answer to that question.
245:04 - Check out line 24, who is your press officer?
245:08 - I'm sorry, I don't know the names of members of staff.
245:11 - And again, on line 32, we have, are your drones recyclable?
245:15 - I'm really sorry,
245:16 - I don't know the answer to that question.
245:19 - Now traditionally, AI is not very good at saying,
245:21 - I don't know, it prefers to hallucinate.
245:24 - But we've effectively given it permission to do that.
245:27 - So do bear that in mind when working with chatbot data,
245:30 - it's good to teach it to say, I don't know,
245:33 - as this will help stop it hallucinating.
245:36 - Let's just ask a few more questions.
245:38 - So I'm going to say, how much is insurance?
245:43 - And it's telling me it's three pounds 75 per delivery.
245:46 - And if I come up onto line eight, well, there we are.
245:50 - It says it's a flat rate of three pounds 75 per drone order.
245:54 - So again, that is pretty good.
245:57 - Let's ask it something a little bit more random.
246:00 - I've said your drone crashed into my house.
246:04 - Okay, it says, please contact us by email
246:06 - and we'll arrange to pay for any damage.
246:08 - I'm going to say, I want compensation.
246:14 - And there we are.
246:14 - It's giving a very logical rational answer.
246:17 - Now it's not being particularly humane.
246:19 - It's not being touchy feely.
246:21 - It's not being that conversational.
246:23 - And that, to be honest, is a shortcoming of our data.
246:26 - We just need much, much more data.
246:29 - And we need much more of this kind of data
246:31 - that we've got at the bottom,
246:33 - where we've got these entire conversations
246:36 - playing out before we get to the completion,
246:38 - because that is the quality data.
246:40 - That's what really helps the chatbot communicate well.
246:44 - So although I think we've done very well here,
246:47 - you will find limitations with this chatbot quite quickly.
246:51 - You're going to come across idiosyncrasies
246:53 - and hallucinations.
246:54 - But as I've said many times,
246:56 - if you wanted to take this to production,
246:58 - you would need a lot more data.
247:01 - But what we've done here has really proved the principle.
247:04 - It's working really well on a really small data set,
247:07 - so that is really good.
247:08 - And of course, your results might differ.
247:10 - Perhaps you're using your own data,
247:12 - or perhaps the model has simply been updated somewhat
247:16 - between the time that this was recorded
247:18 - and the time that you're actually building this app.
247:20 - And that is the nature of working
247:22 - with this new frontier in technology.
247:24 - Now the next thing that I want to do is take this app
247:27 - and deploy it live on the internet
247:29 - with the API key safely hidden.
247:31 - That's quite a process.
247:32 - We've got quite a bit to do.
247:34 - So let's make a start next.
247:40 - Okay, so our mission is to deploy our support bot,
247:43 - but keep the API key hidden.
247:45 - And we're going to do this using Netlify.
247:48 - So right now, if we were just to deploy this
247:50 - as a front-end only project,
247:52 - the API key would be visible right there
247:56 - on every user's machine and easily obtainable
247:59 - from the DevTools network tab.
248:02 - Let's just zoom in.
248:03 - Oops, there it is.
248:05 - Our API key is there for all to see.
248:08 - And that is what we need to avoid.
248:10 - What we want is for you to be able
248:12 - to share the projects you've made with OpenAI
248:15 - without fear of your key being compromised.
248:17 - And actually, the process we're about to go through
248:20 - can be used anytime you need to keep an API key hidden.
248:23 - It's not specific to OpenAI in any way.
248:27 - So how are we going to do this?
248:29 - Well, let's take a high-level overview.
248:32 - Now, at the moment, we've got a front-end project,
248:35 - and what we've been doing is storing the API key
248:38 - on the front-end, making a request to the OpenAI API,
248:42 - and getting back a completion that we use on the front-end.
248:46 - But now, we're going to stop doing that
248:48 - and do something completely different.
248:50 - We are going to send our request
248:51 - to a Netlify serverless function.
248:54 - The serverless function will have access to the API key
248:57 - from a secure store.
248:59 - The serverless function will then make the call
249:02 - to the OpenAI API.
249:04 - The API will pass the response back
249:06 - to the serverless function,
249:07 - and the serverless function will pass the response
249:10 - back to the front-end,
249:11 - and we will use the completion from that response.
249:15 - But most importantly, this API key in its special store
249:19 - is never visible to the front-end,
249:21 - so it won't be visible in the DevTools Network tab.
249:24 - Now, there are a few prerequisites
249:26 - that I'll just mention quickly.
249:28 - You are going to need a free Netlify account.
249:30 - That's really easy to get.
249:32 - I'm also going to assume
249:33 - you have a basic knowledge of GitHub,
249:35 - because we will be deploying to Netlify directly from GitHub.
249:39 - If you know how to publish a project to a GitHub repo,
249:42 - that is enough.
249:43 - We'll also be making fetch requests,
249:45 - and if you've followed this course so far,
249:47 - you won't have any problem with that.
249:48 - We won't really be doing anything more sophisticated
249:51 - than we've already done,
249:52 - and you'll also need VS Code or a similar editor.
249:56 - Okay, that's the overview.
249:57 - Let's put it into practice.
250:02 - So firstly, we need to get our project stored locally.
250:05 - So come down here and click on this cog icon,
250:08 - and that will bring up a menu.
250:10 - Click Download as Zip, and then unzip that folder
250:13 - and save it somewhere sensible.
250:15 - I've saved mine in this folder called
250:17 - wewingit underscore deploy,
250:19 - and it's inside another folder called apps.
250:22 - Now, let's open that in VS Code.
250:24 - Now, before we forget,
250:25 - and to save us from running into an error later,
250:27 - I want to open up index.js and come onto line two.
250:31 - We won't be importing the API key like this anymore.
250:34 - So let's go ahead and delete that import statement.
250:38 - If you skip this step,
250:39 - when we deploy the project to Netlify,
250:41 - you will get an error and it will be a bit frustrating.
250:44 - So best to remember to do it now.
250:46 - Next, we need to open up the terminal,
250:48 - and I'll zoom in on it,
250:50 - and I'm going to run npm install,
250:52 - and this will download the dependencies
250:54 - defined in the package.json file,
250:56 - and it will generate a node modules folder
250:58 - with the installed modules.
251:00 - The dependency that we've been using is the OpenAI API,
251:03 - and we still need it when we deploy to Netlify.
251:06 - Okay, we run that, we let it do its thing,
251:09 - and then we can see that the node modules folder
251:12 - has appeared.
251:13 - The next step is to publish to GitHub.
251:16 - So you're welcome to do this
251:17 - in whichever way you're comfortable with.
251:19 - I've got the GitHub and pull requests extension installed,
251:22 - so I will go with this icon right here,
251:25 - and I'm going to come down here
251:27 - and click on publish to GitHub.
251:29 - I'm going to publish mine to a private repository.
251:31 - Now it's asking me here which files should be included
251:34 - in the repository.
251:35 - Really important to ignore the m.js file,
251:39 - so I'll untick that.
251:41 - Now you could also delete it.
251:42 - It doesn't matter if you delete it or ignore it,
251:45 - just make sure you don't include it in your repo.
251:49 - If you choose to delete it,
251:50 - make sure you've got a copy of it somewhere handy
251:52 - as we will be using it again shortly.
251:55 - Okay, I'll click okay.
251:56 - I'll let VS Code do its thing,
251:58 - and then when the process is completed,
252:00 - we can see that we've got the gitignore file
252:02 - and m.js is in that file.
252:05 - So that is exactly what we want.
252:07 - And of course at this point,
252:08 - you can log into GitHub and check it's worked
252:10 - if you want to be sure.
252:11 - Okay, so we've got the project stored in a repo.
252:13 - We've either ignored or deleted the API key.
252:17 - Next, we need to set up a Netlify account.
252:20 - Okay, next up, we need a Netlify account.
252:26 - It's really easy to do, click on this slide,
252:29 - and it's gonna take you through to the signup page,
252:31 - which looks like this.
252:33 - Now I'm going to sign up with GitHub,
252:35 - and then once it's done its thing,
252:37 - it will take me to the dashboard.
252:40 - And from here, I'm going to select add new site.
252:44 - I'll import an existing project,
252:46 - and it will invite me to connect to a git provider.
252:49 - I'll choose GitHub,
252:51 - and it's going to say no repositories found.
252:53 - And that's absolutely fine.
252:55 - Click configure Netlify on GitHub,
252:57 - and this will open a pop-up page
252:59 - and you can scroll down
253:00 - and either select all repositories
253:02 - or select repositories individually.
253:04 - I'm going to do that one,
253:06 - and I'll choose the repository.
253:08 - I'll just type in we wing it underscore deploy
253:10 - because that's the name of my repo,
253:12 - and click save.
253:13 - And we'll be taken to a settings page.
253:16 - And on here, we can leave everything as it is
253:18 - and just scroll down to the bottom
253:20 - and click on deploy site.
253:23 - As the site deploys,
253:24 - we will see site deploy in progress right here
253:27 - and then eventually published.
253:29 - So now the site is live,
253:30 - we can click on open production deploy,
253:32 - and that is going to actually take us to our site.
253:35 - We've got our own funky Netlify URL right here,
253:39 - and let's see if it works.
253:40 - So we can just come down here,
253:41 - type in a question, press send,
253:44 - and we'll wait and nothing happens.
253:46 - Let's open up dev tools and we've got plenty of errors.
253:50 - Well, that isn't actually a problem.
253:52 - We've only done the first stage
253:54 - and actually it would be really weird if it worked.
253:57 - It doesn't have an API key right now.
254:00 - So our next step is to get our API key to Netlify,
254:04 - and we're going to store it
254:05 - in a Netlify environment variable.
254:07 - Let's come on to that next.
254:12 - What we need to do now is give Netlify the API key.
254:16 - Now, if you remember the function that we'll be using
254:18 - to actually do the heavy lifting of making the API call,
254:22 - we'll have special access to our API key.
254:25 - So on the Netlify site, come over here to site settings,
254:29 - and we're going to click on environment variables,
254:31 - and we can click add a variable,
254:33 - and then I'm going to select add a single variable.
254:36 - That takes us to this page,
254:38 - and we need to add a key and a value.
254:41 - So let's take a look at our code right here
254:43 - because this is essentially the same information.
254:45 - This is going to be our key,
254:47 - and the actual API key will be the value.
254:51 - So back on this form then,
254:53 - we'll have open AI API key in here,
254:56 - and the actual API key stored here as the value.
255:00 - Then click create variable,
255:02 - and we can see that the open AI API key has been added.
255:07 - So what we've done here is we've stored our API key
255:10 - in a Netlify environment variable.
255:13 - So now, if we scroll up and we launch our site,
255:17 - again, we can try asking a question,
255:19 - but we're going to have the same result.
255:21 - We'll still get errors.
255:23 - And that figures, Netlify has the API key,
255:26 - but we're not doing anything with it in the code.
255:29 - And anyway, at the moment,
255:30 - all we've got is our front-end code,
255:33 - and if it did work,
255:34 - the API key would still be visible in DevTools.
255:37 - The fact that we've stored it in an environment variable
255:40 - doesn't mean you can't then display it on the front-end.
255:44 - You actually can.
255:45 - That is why we need the serverless function
255:48 - to make the API call away from the front-end.
255:51 - Okay, so what we need to do next then
255:53 - is work on the serverless function.
255:56 - But in order to set up a serverless function,
255:58 - we need to get hands-on with the terminal again
256:00 - and install Netlify's CLI or command line interface.
256:05 - So let's do that next.
256:07 - Okay, so to integrate a serverless function,
256:12 - we need the Netlify CLI.
256:15 - So let's come back to the terminal,
256:16 - and just to help out,
256:18 - I've created a document here called terminal-commands.md,
256:22 - and I've just listed the terminal commands
256:25 - that we'll be using in this scrim.
256:28 - So we're going to come in here with the first command,
256:30 - which is install netlify-cli-g.
256:35 - So I'm installing this globally.
256:36 - That's what the dash G means.
256:38 - So we can hit enter,
256:39 - and eventually you'll get shown something like this.
256:42 - Now, if we just type netlify and hit enter,
256:45 - we'll get taken to a list of Netlify commands.
256:49 - And the one that we're really interested in
256:51 - is init for initialize.
256:54 - And we're going to use that to configure our site.
256:57 - So down here, let's say netlify init.
257:00 - And now it's going to ask us a series of questions.
257:03 - The first one is connect this directory
257:05 - to an existing Netlify site.
257:07 - Now we've got two options here,
257:09 - and we can just move up or down with the arrow keys.
257:12 - But the first option is the one we want,
257:13 - so I'll just press enter.
257:15 - Now it's asking us if we want to use
257:16 - the current git remote origin,
257:18 - and it's correctly identified
257:20 - the we wing it underscore deploy repository that I'm using.
257:24 - So that's fine, click enter.
257:26 - And there we are, it says directory linked.
257:29 - That is what we want to see.
257:30 - And if we go back to the full screen of VS code,
257:33 - we've now got a new.netlify folder.
257:37 - Okay, in the next grim, we'll use the CLI
257:40 - to give us the boilerplate for a serverless function.
257:46 - Okay, let's get to work on a serverless function.
257:49 - The Netlify CLI is going to give us some boilerplate.
257:52 - So in the terminal, let's say netlify functions colon create.
257:57 - We'll hit enter, and we're going to get some options.
258:00 - The first one is for an edge function.
258:02 - We don't want that, so we can use the arrow key
258:05 - to come down to serverless function, press enter.
258:08 - And it's going to ask us a few questions.
258:11 - The language that we want is JavaScript, so hit enter.
258:14 - Now it's telling us to pick a template.
258:17 - And we've got some interesting options here,
258:19 - including this one or fetch, which talks about APIs.
258:23 - But because we've already got the open AI dependency,
258:26 - and we're not really dealing with authentication,
258:29 - I'm going to go with this basic hello world function,
258:32 - and we're going to adapt it from there.
258:34 - So let's hit enter.
258:35 - Now it's asking me for a name.
258:38 - The default is hello world.
258:39 - We definitely don't want that.
258:41 - I'm going to go for fetch AI,
258:43 - which I think is a reasonably descriptive name.
258:46 - And the function is instantly created.
258:49 - And now if we look at the whole of VS code,
258:51 - we've got a new folder here, netlify.
258:54 - Let's click into it.
258:55 - Here's our function, fetch AI.
258:57 - Let's click on that.
258:59 - And there we are.
259:00 - That is the boilerplate for our serverless function.
259:03 - And at the moment it's basically returning hello subject.
259:08 - Well, we can see from this line of code right here
259:10 - that unless we put a query string parameter in,
259:13 - the subject will be world.
259:14 - So this should return hello world.
259:17 - And if we open up a browser
259:19 - and we actually navigate to.netlify slash functions
259:24 - slash the name of our function,
259:25 - and I have actually put that right here
259:28 - in this terminal commands MD file.
259:30 - And when we go to that URL, what we get is this message,
259:35 - hello world.
259:36 - So we're seeing the object returned
259:38 - from that serverless function.
259:41 - And so this URL is now my endpoint.
259:44 - This is what I'm going to call from the front end
259:46 - instead of calling the open AI API directly.
259:50 - So next we need to go back to the editor
259:52 - and make some changes to index.js.
259:55 - We want to make it so that instead of calling
259:57 - the open AI API directly,
259:59 - we make a fetch request to this new endpoint.
260:02 - So let's do that next.
260:08 - Okay, so we need to completely change
260:10 - this fetch reply function.
260:12 - So instead of calling the open AI API directly
260:15 - from the front end,
260:16 - now we're going to call the netlify serverless function.
260:19 - Now I'm going to set you a challenge to update fetch reply
260:23 - right here in the Scrimba editor.
260:25 - But when we're done,
260:26 - we're going to need to paste this into VS code
260:29 - and push it to GitHub to trigger a redeploy.
260:31 - First, we need a URL to call.
260:33 - And we've already seen the URL,
260:35 - we looked at it before,
260:36 - and I've got it right here in the URL bar.
260:39 - So I'm just going to come in here
260:40 - and paste it in a const called URL.
260:43 - And now it's time for a challenge.
260:45 - And it is quite a long challenge.
260:46 - And this is where I'm assuming
260:48 - you have some knowledge of fetch requests.
260:50 - But do take all of the time you need
260:52 - and feel free to search online if you need to.
260:55 - When I do this, I'm going to use async await,
260:58 - but you don't have to,
260:59 - you can use a fetch request and chain then statements
261:02 - if you prefer to do it that way.
261:04 - Also, I'm just going to comment these two lines of code.
261:09 - When we've got this working in the console,
261:10 - we can uncomment these and update them as needed.
261:14 - But we'll do that after we've got
261:16 - the serverless function working properly.
261:18 - Okay, let's check out the challenge.
261:21 - So I want you to make a fetch request to the URL
261:24 - that we've got saved here using the following details.
261:28 - The method should be POST.
261:30 - In the headers, the content type should be text slash plane.
261:34 - And the body should hold conversation string,
261:37 - which remember we have got stored right here.
261:40 - Now, once you've made the fetch request,
261:41 - you can save the response to a const and log it out.
261:45 - Then copy and paste the updated fetch reply function
261:48 - to VS code and delete any unnecessary code from index.js.
261:53 - Push the changes to GitHub to trigger a redeploy.
261:56 - And remember that will take some seconds,
261:58 - maybe as long as a minute.
261:59 - And then navigate to your Netlify site,
262:02 - put something in the text input.
262:04 - It doesn't actually matter what
262:05 - because it's not going to be rendered
262:07 - and it's not going to be sent to open AI.
262:09 - Hit send and then what you should see
262:11 - in the console is hello world.
262:14 - And that will show that from the front end,
262:16 - we're successfully managing to access the serverless function
262:19 - and get whatever it returns.
262:22 - Okay, quite a lot to do.
262:23 - So pause now, take all of the time you need
262:25 - and we'll have a look together in just a moment.
262:33 - Okay, so hopefully you managed to do that just fine.
262:36 - So I'm going to come down here
262:38 - and I'm going to set up a new const response
262:42 - and I'm going to await a fetch request.
262:46 - The first thing a fetch request needs is a URL.
262:49 - Well, we've got that saved right here.
262:52 - And now we need to pass it an object.
262:55 - So we've got all of the details here.
262:57 - Firstly, the method should be post
263:01 - and then we need some headers
263:03 - and this will hold an object
263:05 - and it's going to have a single key value pair.
263:07 - The key will be content type
263:10 - and the value will be text slash plane.
263:14 - Lastly, it needs a body
263:16 - and the body will hold conversation string.
263:21 - Okay, then I'm going to come underneath that
263:23 - and set up a const called data
263:26 - and we will await the response JSON.
263:30 - And let's just log out data.
263:33 - Now we've got plenty of code here that we don't need.
263:36 - All of this code down here, we can safely delete
263:40 - but I'm just going to comment it out
263:41 - because we will need to copy and paste it later.
263:44 - And likewise, right up at the top,
263:46 - all of this code that brings in open AI,
263:49 - we no longer need and I will comment it out.
263:52 - Then I'm going to copy the fetch reply function
263:55 - and I'm going to bring it over to VS code
263:57 - and I'm going to paste it in.
263:59 - And of course, I haven't copied over the code
264:01 - I commented out apart from these two lines
264:04 - which we will be using here in the future.
264:06 - Now staying in VS code, up at the top,
264:09 - we've got these lines of code here, we can delete them.
264:12 - When you've done that, you can save,
264:14 - push it to GitHub and that will trigger a Netlify redeploy.
264:18 - And remember, that will take a few seconds,
264:20 - maybe even a couple of minutes.
264:22 - When it's done, navigate to our site, send a message
264:26 - and then what you see in the console
264:28 - should be the object with hello world.
264:31 - And that will show you that the front-end code
264:33 - has successfully communicated with the serverless function.
264:37 - And that means we need to get to work
264:39 - on this serverless function, next. I've just pasted
264:45 - this serverless function into the Scrimba editor.
264:48 - And again, when we're done, it will need to go back
264:50 - into VS code so it can be pushed to GitHub
264:53 - to trigger a redeploy.
264:55 - Now, before we do anything else,
264:57 - let's delete all of the code we don't need.
265:00 - Okay, that is the bare bones of the serverless function.
265:04 - Now, what we've got at the end
265:05 - might look a little bit unfamiliar.
265:07 - Don't worry about that.
265:08 - That is just exporting this handler function
265:10 - to make it available to the rest of the app.
265:13 - And that syntax there is actually from Node.js
265:16 - so it's not familiar to front-end developers.
265:19 - The next thing to do is bring in open AI.
265:22 - And remember, we used npm install
265:25 - to load in the dependencies we need
265:27 - so we have access to them across our project,
265:30 - including in this serverless function.
265:32 - Now, the code we need to do that
265:33 - is code we're very familiar with.
265:35 - In fact, we just commented it out in index.js.
265:39 - So let's head over to index.js and let's uncomment that.
265:43 - Now, we will not be importing our API key
265:46 - from this mf.js file.
265:49 - Remember, we've ignored that or deleted it.
265:51 - It's not available on Netlify.
265:54 - So let's delete that line of code.
265:56 - Now, the API key that we've got stored
265:59 - in the Netlify environment variable
266:01 - is available to us in this serverless function.
266:04 - And we can access it using process.env
266:08 - and then the name of the API key.
266:10 - So luckily, we don't need to change this line of code at all
266:14 - because this is going to fetch our API key
266:17 - from the environment variable we set up earlier.
266:20 - So now we've bought in open AI,
266:23 - we need to actually use it down here inside the try block.
266:26 - So let's head back to index.js
266:29 - and we'll take the second block of code
266:31 - that we've got commented out right here.
266:33 - That is looking pretty good,
266:35 - but we need to do something with conversation string.
266:38 - We don't have access to conversation string in this file
266:42 - and we can't just import it as this serverless function
266:45 - will not be part of the front end code.
266:47 - Remember, we're actually calling this serverless function
266:50 - using a fetch request.
266:52 - And we've got the fetch request right here
266:54 - and conversation string is being sent
266:57 - in the body of the fetch request.
266:59 - Now back in fetch AI, we're taking in an event parameter.
267:04 - And from that event parameter,
267:06 - we can actually access the body.
267:08 - So I'm going to come down here
267:09 - and replace conversation string with event.body.
267:15 - So now when the fetch request comes in,
267:18 - the conversation string will be in the body,
267:20 - fetch AI takes in the event parameter
267:23 - and accesses conversation string,
267:25 - which will be stored in the body of the event object.
267:29 - Okay, I think that's looking good.
267:31 - We can't test it yet,
267:33 - because first we need to deal with
267:34 - what we actually get back from the API.
267:37 - We've got this return down here,
267:38 - so we need to do something with that.
267:41 - Let's come onto it in the next screen.
267:46 - Now we need to deal with what the serverless function
267:49 - gets back from the open AI API.
267:52 - Now that's no problem.
267:53 - We've worked with the API long enough by now
267:56 - to know that what we get back is a response object
267:59 - and that we need the data from it.
268:02 - So working down here, inside the return statement,
268:06 - and we've got the body, we're calling JSON.stringify
268:09 - and then in here, I've pasted a challenge for you.
268:13 - I want you to add a key value pair.
268:16 - The key should be reply
268:17 - and the value should be response.data.
268:21 - Now, once you've done that, paste the code
268:24 - into fetchai.js in VS Code
268:27 - and push it to GitHub to redeploy
268:29 - and see what gets logged out when you test.
268:32 - Okay, pause now, get that sorted,
268:34 - and I'll see you back here in just a moment.
268:43 - Okay, so all I need to do is come in here
268:45 - and add the key value pair.
268:47 - So the key is reply
268:49 - and the value is response.data.
268:53 - Okay, let's copy that and I'm going to paste it
268:55 - over here in VS Code and I've just formatted it
268:58 - a little bit more neatly
268:59 - and obviously without the challenge text.
269:02 - Now, I'll go through the process.
269:03 - We've gone through several times.
269:05 - I'm going to push it all up to GitHub
269:07 - and wait for the redeploy.
269:09 - When the redeploy is complete,
269:11 - I'm going to navigate to the site.
269:13 - I'm just going to say, hey,
269:14 - and then what we see in the console
269:16 - is the data from the response object
269:19 - and look what we've got here, a completion.
269:22 - Hi there, how can I help you?
269:24 - Okay, that is big progress.
269:26 - We have just got one more step to take
269:29 - where we take this completion and get it rendered.
269:32 - Let's do that next.
269:37 - Okay, so the last thing that we need to do
269:39 - is deal with these two lines of code.
269:42 - So let's go straight into a challenge.
269:44 - I want you to update the two commented lines of code
269:47 - to get this working.
269:49 - Once you've done that,
269:50 - you can push to GitHub to redeploy and then test.
269:53 - Now, when this was purely a front-end project,
269:56 - we were taking our completion from the response.
269:58 - Now we've got the response stored in data.
270:01 - So it's just a question of working out
270:02 - where you get the completion from within the data object.
270:06 - Okay, pause now, get it sorted
270:08 - and we'll have a look together in just a moment.
270:16 - Okay, hopefully you got that working just fine.
270:18 - Now, these lines of code are almost correct,
270:22 - but now we have this data object
270:24 - and what we want is stored in the reply.
270:27 - So I'm going to delete response.
270:30 - And let's go for data.reply.choices
270:34 - and everything else can stay the same.
270:37 - Okay, let's copy that over to VS Code
270:40 - and I'll just update these two lines right here.
270:42 - Then we'll push to GitHub to trigger a redeploy
270:45 - and I'll ask the chatbot a question.
270:47 - We get back a completion, it's looking good, it's working
270:51 - and best of all, when we open up the network tab,
270:54 - there is absolutely no sign of our API key.
270:58 - So there we are.
270:59 - We have successfully deployed our open AI project
271:03 - to the live internet with the API key hidden.
271:06 - And now you can safely share your projects
271:08 - and use them in your portfolio
271:10 - without fear of your API key being compromised.
271:14 - Now you might be thinking, wait there,
271:16 - anyone can just access my serverless function.
271:19 - They can write their own fetch request to the URL,
271:22 - which of course is available on the front end.
271:25 - And then from there, they'll be able to use
271:27 - and abuse my open AI API key.
271:30 - Well, they can't, this endpoint is only going
271:33 - to accept fetch requests from its own domain.
271:36 - Now we can prove that right here in Scrimba
271:39 - because if we want to make a fetch request
271:42 - to this endpoint right here from inside a scrim,
271:46 - that should not be possible
271:48 - because scrims are hosted on scrimba.com
271:50 - and this endpoint is using my custom Netlify URL.
271:55 - Okay, let's hit save and I'm going to open up the console
271:58 - and I'll just say, hey, and there we are, we get an error.
272:02 - We are not able to make a fetch request
272:05 - to that URL from within Scrimba.com.
272:08 - And you can repeat that test from your own domain,
272:10 - check out the dev tools and look in detail
272:13 - at the error you get.
272:14 - Now, of course, if for some reason we wanted other domains
272:17 - to be able to access our endpoint,
272:19 - we would be able to do that with the cause policy.
272:23 - Cause is the cross origin resource sharing
272:26 - and it is quite a big topic,
272:28 - but if it's completely unfamiliar to you,
272:29 - I do recommend you read up on it at some point
272:32 - because it is really important
272:34 - when you start working with APIs and endpoints.
272:37 - Okay, we are done with this project.
272:39 - Let's just take one more scrim to recap what we've studied
272:43 - and talk about where you go next.
272:48 - I just want to say massive congratulations
272:50 - on finishing this course.
272:52 - You now have the AI foundations you need
272:55 - to tackle almost any web dev related AI task.
272:59 - Let's just have a quick look back at what we've studied.
273:02 - So we looked at how to use the open AI API.
273:06 - We used various models, including the GPT-4 model,
273:09 - text DaVinci 003 and our own fine tuned model.
273:13 - We looked at prompt engineering,
273:15 - including the zero shot approach, the few shot approach
273:18 - and the specific object syntax needed
273:21 - for the create chat completions endpoints.
273:23 - We worked on building chatbots
273:25 - and on fine tuning a model using our own data.
273:29 - So the chatbot gave us answers
273:30 - specific to our circumstances.
273:33 - And of course, along the way,
273:34 - we've had loads of challenges.
273:36 - So where next for you and AI?
273:39 - Well, it would be great to fine tune with more data.
273:42 - You could get a fine tuned bot production ready.
273:45 - Now that will require you to find
273:48 - and organize a lot of data.
273:51 - Also keep an eye on the AI scene.
273:53 - There is a lot going on with text and images
273:56 - as we've seen here, but also with voice
273:58 - and of course with video.
274:00 - And in terms of how you use that in your personal projects,
274:03 - I recommend you find a need for AI and then build an app.
274:07 - So work on something that you've identified
274:10 - as a problem that needs solving and use AI to solve it.
274:14 - The best portfolio projects are unique to you.
274:18 - Now, whatever you do,
274:19 - why not head over to Scrimba's Discord server
274:22 - and go to the Today I Did channel.
274:24 - This screenshot is actually a link.
274:26 - It will take you straight there.
274:28 - And you can let everybody know
274:29 - that you finished the course and tell them how you got on.
274:32 - And when it comes to stretch goals in your own projects,
274:35 - why not share them in the I Built This channel.
274:38 - Show off your work and get great feedback from your peers.
274:42 - And lastly, do let me know how you got on with the course
274:45 - and show me what projects you've built.
274:47 - You can catch me right here on Twitter.
274:49 - And all that remains to be said is thank you very much
274:52 - for completing this course and I wish you all the very best.

Cleaned transcript:

In this projectbased course, you will learn how to build AIpowered apps with the chat GPT, DALLE, and GPT4 APIs. Tom Chant developed this course. Tom is a frontend developer and course creator with Scrimba. Hey there free code campers and welcome to this interactive course where you are going to build mindblowing AIpowered applications with features that didn't seem possible even just a few months ago. We'll be starting with the very basics so you don't need any previous knowledge of the OpenAI API. In fact, the only prerequisites for this course are basic vanilla JavaScript and some curiosity. Now the special thing about this course is that it's projectbased and it comes with a ton of challenges and that means you're going to have your hands on the keyboard, writing code, and tackling challenges throughout. And if you're wondering how you can get your hands on the project code, don't worry, we've got you covered. In the interactive version of this course over on Scrimba, you can pause the video, edit the code, and run the projects right there in your browser. Or if you prefer, you can download the code from those scrims and run the projects locally. The link is right down there in the description. Oh, one last thing before we start. If you enjoyed this course, please hit that thumbs up right here on YouTube. That will be much appreciated. And if you'd like to get in touch, reach out to me on Twitter. I am @tpchant. OK, let's get started. Welcome to building AI apps with chat GPT, DALLE, and GPT4. I am super excited to bring you this course on the technology everybody is talking about. We are going to study how to use the OpenAI API. We'll be using various OpenAI models, including the latest GPT4 model. We'll be looking at prompt engineering. We'll be building chat bots. And we'll be fine tuning a model on our own data. And along the way, of course, there will be lots more and plenty of challenges. Now, I need to say a quick word about GPT4. We'll be using the GPT4 API in the second project in this course. Now, at present, there is a waiting list to get your hands on the API. No doubt in the near future, the waitlist will go and the API will be available to everybody. But for now, while the waitlist exists, why not click on this screenshot, which will take you through to the signup page and you can join the waiting list there if it still exists. OK, so what are we going to build? We'll start off by exploiting the power of OpenAI to be creative and generate humanstandard words and images to build this cool movie pitch app, which turns a onesentence idea into a full movie outline. Then we'll use the GPT4 model to create an Ask Me Anything chat bot called Know It All. And we'll use a Google Firebase database so the user can persist the conversation they have with the chat bot and pick it up at a later date after refreshing the browser. Lastly, we're going to stay with the chat bot concept. But under the hood, everything will be different. We will enter the world of finetuning. This is where we upload our own dataset and we create a chat bot that can answer specific questions from our own data. This skill is really essential if you want to use a chat bot for a specific purpose, such as to handle customer service issues with your company. And with this project, we'll also learn a really neat skill for whenever you're working with APIs with secret keys. We'll look at deploying the site with Netlify with the API key hidden so you can share your project without fear of the API key being compromised. This solves the really big problem that we have when we're using APIs with secret keys in frontend projects. Now, there are not too many prerequisites for this course. We're going to be working in vanilla JavaScript and I'll assume you have a reasonable knowledge of it. Now, the JavaScript in this isn't very complicated for the most part. As long as you understand the basics of a fetch request, you'll be absolutely fine. And if you're rusty on fetch requests, don't worry at all. We'll actually go through it all step by step and I don't think you'll have a problem. Apart from that, we'll be focusing fully on the AI and working through it stage by stage. Who am I? My name is Tom Chant and I'll be your tutor for this course. You can find me on Twitter. My handle is at tpchant and it's always good to hear how you got on with the course. Now, before you get started, why don't you head over to Scrimba's Discord server and go to the Today I Will channel and let the community know that you're starting this course. Studying is more fun and more productive when it's done together. So, why not interact with fellow students on the Discord community, encourage each other and help each other along. And then, without further ado, let's jump straight in to the first project. In 2006, Terry Rocio and Bill Marcille sold the rights to the Denzel Washington film Deja Vu for a recordbreaking $5 million and it went on to gross more than $180 million worldwide. Have you ever dreamed of making it big in the movies, coming up with that one idea that will make you rich beyond your wildest dreams? Well, the most important thing is to be aware of your wildest dreams. Well, the most successful projects start with one single idea, but once you've had that idea, you need to work with it, brainstorm around it, spend hour upon hour working late into the night to forge it into something marketable. Or do you? What if you could take one simple idea, just a single sentence, and let the power of OpenAI's large language model do the rest of the work for you? What used to be science fiction is now science fact. This is movie pitch, and on the outside, it is so simple. We put a onesentence idea for a movie in here, we click send, and that is all we have to do. We sit back and we wait for maybe 10 seconds or 15 seconds, and then what we get back when OpenAI has done its thing is the makings of a great movie. We get some artwork for the cover, a great title, and a list of stars, and most importantly of all, we get the synopsis that brings the idea to life, and all of this is brought to us by the power of OpenAI. Now, to get this app working, we have got some work to do. So what will we be studying? Well, we will be using the OpenAI API. We'll be working with models and understanding what tokens are. We'll learn about crafting prompts and how to tweak those prompts to get results and how we can use examples to train the model. We'll be looking at extracting information from texts and also generating images just with words, and by the end of all that, we'll have a great foundation in how to use OpenAI in frontend projects. Now, I want to give you a warning. As we build this project, the API key for OpenAI will be sat there on the front end, and that means it's visible. Anybody could go in through DevTools to the Network tab and they could steal your API key. Now, while you're developing locally, that is fine, but don't share your project with an API key or publish it to GitHub without ignoring the API key because that will compromise the API key. Now, later in this course, but not in this project, we're going to look at how we can deploy our apps with the API key safely stored on a server where no one else can see it. But in the meantime, just be mindful of what you're doing with your API key. OK, so when you're ready, let's jump into the code and check out the HTML and CSS that I've got waiting for you. So let's have a look at the code that we've already got, and then we'll get straight on to the AI. So here in index.html, there's nothing particularly strange or unusual going on. I've bought in a couple of Google fonts. We've got Playfair and Poppins. And then down in the body, we've got a header section which has just got the movie pitch logo that you can see right here. And then underneath that, we've got a main, and that is divided up into two sections. This first section here is all about the setup, and that is everything that you can see right here. So this is where we collect the user's idea with this text area, and we interact with the movie boss. And you can see that we've got his speech bubble right here with this rather braggadocious statement that we've actually got hard coded right here in the HTML. Now, the second section that we've got down here, this is dedicated to the output. This is where we'll actually display the finished product that the AI creates for us. Now, we can't see that section in the mini browser at the moment, because if we have a look at the CSS, it's actually set to display none. So we'll bring that into view when we're ready for it. Now, sticking with the CSS, we've got a bunch of styles in here. It's really unlikely that there's anything particularly new or strange, but by all means, feel free to pause and familiarize yourself with it. To be honest, we won't be referring to it that much as the course goes on. We'll just touch on it now and again as we need to. Now, over in index.js, we have already got a little bit of JavaScript. I've taken control of the three elements that we're actually working with already. So we've got the text area, which we've already seen down here. The setup input container, that is the container for the text area and the button. And then we've also taken control of the movie boss text, and that is what you can see right here inside this speech bubble. Now, we've got an event listener and it's listening out for clicks on this button. And when it detects a click, it does a couple of things. Firstly, it checks if there's anything inside the text area right here. If there isn't, it does nothing. If there is, it will replace this text area and this button with a loading GIF. And we're just bringing that in right here from the images folder. And also, it will update the speech bubble with some new text. This is actually hard coded into the JavaScript. And that is actually the last bit of human written text that you're going to see for a while. From this point onwards, we'll mostly be working with AI generated text. Let's just put something in that text area and check it's working. So there's my one sentence idea for a movie. An evil genius wants to take over the world with AI, and only one brave mouse can stop him. Let's hit send. And as soon as we do that, we get this loading SVG just to communicate to the user that something is happening. And we've updated the speech bubble. Just wait a second while my digital brain digests that. Okay, so everything is working as we want it to. And the next thing we need to do is start applying some AI to this. We want to actually get an AI generated response to our idea. But before we take the first step into this mystical and magical world of AI, we need to talk about some more mundane matters, like how we can actually access the open AI API from within our app. That's how we'll get our hands on these powers. So let's come on to that next. Next, let's get our hands on an open AI API key. And that means signing up. So why don't you head over to the open AI homepage? And this slide is actually a clickable link. So if you just click on this screenshot, it will take you straight there. Now, when you're there, just pause for a moment to check out some of these really beautiful images that they've created with their Dali image generation model. And we will be looking at image generation later in this course. Now, once you've feasted your eyes on the pictures, head over to where it says API. And from there, you'll need to go to sign up. And then you can choose your sign up method, and you'll need to confirm with a phone number. Now, once you're in from the dashboard, you can click your avatar up here and select View API Keys. Now, they only show you the API key once when it's first generated. After that, it will be obscured like this. So be sure to copy and paste it somewhere safe as soon as you get it. But if you lose it, don't worry. From here, you can actually delete it and get a new one. And like all API keys, be sure to keep it secret. In this project we're building, you will see my API key. But rest assured that by the time this recording goes public, I will have deleted it. OK, so that's the API key. Now, while we're here, let's just say a quick word about credit. If we click on Usage here, it will take us through to a page where we can see how much credit we've got remaining. Now, at the time of recording, when you sign up, you get some free credit to play with. I got $18 of credit, which was valid for three months. And it actually looks like I'm nearly through that. But I have been a heavy user. So you'll probably find that the free credit you get will get you a very long way. Now, when that credit has expired or been used up, it is a pay as you go model. Check the website for the latest info on that. OK, now let's take our API key and build our first request. When you're ready for that, let's move on. Now we've got our API key, we can start using the API. But before we can make our first fetch request, we need an API endpoint. Let's head back to the OpenAI website and click through to the docs. And this slide is a clickable link, of course, and it will take you straight there. Now, the OpenAI docs are pretty comprehensive. And right here on the first page, it tells us that the completions endpoint is at the center of our API. OK, that sounds interesting, the completions endpoint. We're going to check it out. But before we do that, you might well be asking, what is a completion? And completion is a word used a lot in OpenAI. So let's say you want some information, like you want to know who the first person to walk on the moon was. Let's send that question to OpenAI via the API, and it will think about it and send back a completion. And that completion is, of course, Neil Armstrong. So in the world of OpenAI, a completion is a response from the API that fulfills your request. OK, let's click through to the completions endpoint section in the docs. And here we get a ton of info and some code examples. And each example is given in node.js, PHP, and curl. But what I want to focus on is this. We have the completions API endpoint right here, and it tells us we can use the POST method. So now we've got that info. Let's come back to index.js, and I'm going to save it in a const called URL. And in the next scrim, let's start writing a fetch request. Let's start this API call by laying out the bare bones of the fetch request. So we need fetch, and then we open up the brackets, and we pass in the completions endpoint, and we've got that saved here as URL. Now we need an object, and inside this object, we're going to need method, headers, and body. And if we check back with the docs, we know that the method is POST. It tells us that right here next to the endpoint. Now the docs don't actually give us an example fetch request in JavaScript, but they do give us this curl example, and we can adapt it. So let's take a closer look. Our headers are going to need a content type of application JSON, and we'll also need an authorization of bearer with our API key. So let's go ahead and put those in. OK. Now we need a body, and this is where we get into open AI specifics. The body for an open AI request needs a model and a prompt. So I'll come down here and add the body, and then I'm going to say JSON.stringify, and I'll pass in an object with model and prompt. Now if we look back at this screenshot, it actually gives us a model right here, textdavinci003. So let's break the golden rule of coding and just copy a line of code that we don't actually understand. We haven't discussed what open AI models actually are or what options we have available to us yet, but what I want to do is get this example working, get the first API request actually into our app, and then we'll rewind and talk about models in more detail. For now, just know that open AI has got various models that we can use, and today we're going to use this one called textdavinci003. OK. So I'll put that in a string right here. OK. So we've got the model, and now we need the prompt, and we're going to talk about prompts a lot in this course. But for now, what we need to know is this. In this example that we saw in the previous scrim, we asked a question. Who was the first person to walk on the moon? And we got the completion, Neil Armstrong. Well, the prompt is this part. Who was the first person to walk on the moon? The prompt is whatever we ask the open AI API for. Now, sometimes prompts are really, really simple just like this, and sometimes they're really very complex, as you'll see a bit later in this course. Now, they've given us an example prompt in the docs. Say this is a test, which, to be honest, is not very creative. So instead of using that, I'm going to ask an easy but real world question for this prompt. I'm just going to say, what is the capital of Spain? And that is all we need to do for a simple prompt. We just put it in a string. OK. Now we need to change some thens. So we'll take the response, and we'll call Jason on it. And then we'll take the data from that response and log it out. OK, let's open up the console, and I'll hit Save. And we've run into an error. It's a pretty silly one. It's just a typo on my part. But why don't you just pause now and see if you can debug that? And I'll give you a bit of a clue. Have a quick look at this slide. OK, did you spot it? I've actually put bearer colon and then the API key. I think that is causing the problem. Let's try it one more time. And there we are. We're getting a response. So I'm just going to copy this from the console and paste it in the editor, just so we can see it a little bit more easily. And as we can see, this is an object. It's got loads of data in it. And we'll be talking more about some of the properties in this object as we go along. But for now, we just need to focus on one word, Madrid. Because that shows us it works. OpenAI is speaking to us, and it knows all about Spain. Now, OK, knowing facts is great. But we're actually interested in emotion and creativity. So I'm actually just going to change things a little bit. Let's delete this. And I'm going to change my prompt to say something more emotional. I've said sound sympathetic in five words or less. Let's hit Save. And again, I'll just paste the response right here. And look what we've got. I'm here for you. It's giving us sympathy. And what's really cool about that is that now the AI is speaking to us as if it were human. It's speaking with heartfelt, or at least CPU felt, emotion. And that is what we're looking for. OK, let's take what we've got here and apply it to our project. And I haven't forgotten this model that we've got here, text da Vinci 003. It's still shrouded in mystery. But I want you to get your hands on the keyboard, start getting practice, start building muscle memory before I do any more talking. So let's move on to a challenge with this next. OK, so back in our app, we've got an event listener right here. And it's listening out for clicks on this button. And what it does is it checks to see if the user has given us any input. It displays a loading message and updates the text that we see in the speech bubble right here. Now, we're not ready to work with any user inputted text yet. So I'm actually just going to comment out this if. And if I save that now, this code should run as soon as we click this button. And there we are. We get our loader. And we've updated the text that we've got in the speech bubble. So let's go ahead and make an API call to get an enthusiastic response that we can pass to our user. And of course, later, we'll refactor this so the AI is actually responding to the specific idea that the user gave us. But for now, the response we get back from the AI is just going to be generic. So I'm going to call a function in here called fetch bot reply. And then I'll come down here and I'll do the donkey work of setting up the skeleton of this function. And now it's over to you. Here's a challenge to finish off this function. And I'm just going to paste it right here inside the function body because that is where I want you to write code. So your challenge is this. I want you to make a fetch request to the OpenAI API. And I've put all of the details that you need up here. So you'll have to put your own API key in here. And this is the URL to the endpoint. Now the prompt should request an enthusiastic response in no more than five words. And you know how to do that because in the previous scrim we did exactly the same thing to get a sympathetic response. For now, you can just log out the completion to check it's working. Now before you start, there are two things I want to say. The first is, if what you get from the completion isn't quite what you wanted or what you expected, don't worry. We're going to talk a lot more about using prompts and troubleshooting the issues that you can have. The purpose of this challenge is just to get the syntax right so we are actually getting a completion back from OpenAI. Secondly, by all means, go back to the previous scrims just to have a look at the syntax that we need. But don't copy and paste. You're going to do yourself a lot of favors by writing this out by hand. All of that practice just builds muscle memory and builds up your fluency. OK, pause now, get this challenge sorted, and I'll see you back here in just a minute. OK, so hopefully you managed to do that just fine. So what we need to do then is come in here and set up a fetch request. And we'll pass in the URL that we've got stored right here. And then we'll open up the object. And we know that the method is post. And then we need the headers. And that will be an object with two key value pairs. The first is the content type, which will be application Jason. And the second will be authorization. And that will be a string with the word bearer and our API key. Then we need the body. And the body is going to need a model and a prompt. So firstly, we say Jason.stringify. And then we pass in an object with model text DaVinci003. And prompt. And for my prompt, I'm going to say sound enthusiastic in five words or less. Because that is what we want the movie boss to do. We want him to sound enthusiastic. OK, let's change some VENs so we can deal with the response. So we call Jason on the response. And then we'll take the data from that response and log it out. OK, let's hit Save. And to fire up this function, all we need to do is hit the Send button. And we've got the loading SVG. And if I open up the console, there we are. It looks like we've got our completion. Let's just copy and paste that into the editor. And we can see we've got these two words right here excitedly enthusiastic. So it's taken our prompt very, very literally. Now the next thing that I want to do is actually get this completion to appear inside the speech bubble. So I'm just going to delete this challenge text so we've got a little bit more room to work with. Now we've already taken control of the text in the speech bubble. We've got the elements stored up here in this const movie boss text. So let's come down here, and instead of logging something out, I'm going to come onto a new line. And I'll say movie boss dot text dot inner text. And now we need to access our completion. And we've got that right here. So to get that, we need to say data dot choices. And choices holds an array. And we actually want what is in the first element of the array at position 0. And then we want to access the text property. OK, let's hit Save. And again, I'm just going to hit this button. And there we are. It says eager and excited in the speech bubble. Now we've got a little bit of a problem in that our design is kind of broken because the CSS can't really cope with such a short sentence. Or that's what it seems at the moment. Now I'm not too worried about that because this sentence is not going to stay short. Soon we're actually going to refactor this function. And what we'll get will be a longer completion, which is actually going to be relevant to the one line input that the user puts right here. But before we come on to that, there are one or two things that I want to cover. Firstly, these fetch requests are unnecessarily complex and long. So instead of writing out loads of these in our app, we can actually use open AI's dependency. And that is going to save us a lot of work as we move forwards. But before we do that, we need to talk about models because we have used text DaVinci 003 and we haven't really got a handle on what that means yet. So let's come on to that next. So in the last two scrims, we have used text DaVinci 003 as our model. So now let's ask the question, what is an AI model? Well, loosely speaking, an AI model is an algorithm that uses training data to recognize patterns and make predictions or decisions. Now, open AI has got various models geared towards different tasks. And some models are newer and therefore better than others. At the moment, there are two main categories of models that you might come across. There's the GPT3, GPT3.5, and GPT4 models. And GPT4 is actually just coming out right now. It's currently in beta. And these models are all about understanding and generating natural language. And they can also generate computer languages as well. Now, there's also the Codex models. These models are specifically designed to generate computer code, including translating natural language to computer code and vice versa. And you've probably seen examples of that online, even if you haven't tried it yet yourself. Open AI also has a model that filters content to remove or flag unsafe or sensitive text. In this project, we'll be using the text DaVinci 003 model, which is a GPT3.5 model. Now, this is one of the newest models. It can provide long text output. And it's great of following instructions. There's GPT4, which is fresh out. And we will be coming to that later in this course. There's the text Curie 001 model. And this is a very capable model. It's actually faster than text DaVinci 003. But it's not as good overall in terms of the language it creates. There's text Babbage 001, which is great for straightforward tasks and is also very, very fast. And then there's text Ada 001. And that is fine for basic tasks. And it's fast and cheap. Now, these models are in age order. So text DaVinci 003 is the newest on this list. Text Ada 001 is the oldest. And the older the models get, the less complex they are. So they run faster. And generally speaking, they're cheaper. But you should check the open AI docs for the latest prices. Now, the Curie, Babbage, and Ada models all provide shorter outputs than text DaVinci 003. But they might well still be capable of some or even all of the tasks you might want to do in a project. It depends what it is you want to achieve. But just remember, with the older models being cheaper, that means you can scale apps without incurring too many costs. And because they're faster, you'll experience less of the horrible laggy UX. You can sometimes get when you're working with any API, but particularly with artificial intelligence, as it obviously has to do so much computation. Now, so far, our app hasn't been too laggy. We've just been generating quite short, quick completions. But as our requests get more complex, we will see lag times increase. And now, you might well be asking, what model should I use in my project? Well, open AI's advice is this. Start with the best available model and downgrade to save time and costs where possible. So basically, get your app working so you're happy with its performance, and then experiment with cheaper models to see if you can get the same level of results. Now, as new models come out, prices will change. And probably, performance will increase on those new models, so you will have to do some experimentation. Now, to help you with that, in the next grim, I want to introduce you to a couple of really useful tools. And one of those tools will help you select the best model when you come to build your own projects. Let's move on to that next. I want to show you a couple of really useful tools that can help us work with open AI. The first one is particularly useful for model selection. This screenshot is from gpttools.com, and this is their prompt compare tool. And of course, this slide is a clickable link, which will take you through to their site. Now, to use this tool, you put your API key in right here, and then it allows you to set up two API calls side by side using different models. So let's just ask OpenAI for a description of Pablo Picasso's artistic style. And there we are. I've put the same prompt on each side. Now, for the first one, I'll use the model, or engine, as they call it here, of text DaVinci003, which is the same as we're using in our app. And for the second one, I'll use text Curie001, which is one of the older models. Now, down here underneath, we've got plenty of other settings, but I'm going to leave them all at their defaults. It will be the same on both sides. So now we can submit and see the results. The first one to come back is the Curie, which figures the older, simpler models are faster. Then we get back DaVinci. The big thing that we notice is that the texts are vastly differing in length. The newer model, the DaVinci, gives us way more words, though it was slower. And we can actually see the speed difference right here. This one took 5.8 seconds, and this one took just 1.1 seconds. Now, in our project, we are prioritizing language creation ability over speed and cost. So we're going to stick with text DaVinci003, but it's good to know for future reference that you can do some experimentation with this tool when selecting a model to use. Although this answer is short, the language it provides is actually perfect human standard English. So remember, those older models are not just there for legacy. They are actually really useful. Now, the second tool I want to show you is back on OpenAI's website, and it is the OpenAI Playground. And again, this slide is a clickable link. Now, the Playground is a really, really cool tool. You can select from some prewritten examples, or you can just come in here and write your own prompts. Let's ask about Picasso again. And when we click Submit, we can actually see the results we get highlighted in green. And this will allow you to practice with different models. Again, you can change it right here, but of course, you won't be seeing the results side by side. It also does allow you to play with the various other settings, some of which we'll be looking at later in this course. But what I think is the most useful thing here is that any time you can come up here to where it says View Code and get a code snippet, including the prompt you're generating right now, it will actually give you that prompt in either Node.js, Python, or curl. Now, I've selected the Node.js snippet right here. If we look carefully, we can see that it's not quite the same as the fetch request that we've already written. If we look at this first line, it's using the require keyword to work with a dependency. So at the moment, we couldn't just cut and paste this code as it is, but we could use lots of it in our JS. We've got, for example, the prompt right here. But look, the neatness and compactness of the API call being made here is definitely making me think that it's time to move away from the standard fetch request we've already written and move towards using the OpenAI dependency. In the long run, this is going to save us time and allow us to do less work. In the next grim, let's go back to the app and refactor our fetch request to use the OpenAI dependency. When you're ready for that, I'll see you there. So far, we've been using a fetch request to make API calls. But now, we're going to neaten things up a bit by switching to the OpenAI dependency. And this is actually going to save us time and allow us to write less code as the project progresses. Now, if we look back at the docs, we saw this code snippet for Node.js. And we can actually get a broad idea from this as to what we want to achieve. Let's just zoom in a little bit. OK, so we're going to need to get our hands on the configuration and OpenAI API constructors from the OpenAI dependency. And we'll actually be using the import keyword, not requiring them, because obviously, we're not working in Node.js. Also, like they're doing here, we are going to store our API key in a separate file. And we'll talk about that more in a moment. And then we'll use the two constructors that we're bringing in from the OpenAI dependency. And then we can update and simplify our fetch request. So firstly, let's come over here. And I'm going to set up a new file called mv.js. Now, you can't actually see me do this. But when you hover your cursor here, three dots appear. When you click on them, you get a menu. And you can select New File. I'm going to call this file mv.js. And there we are. You can see that file has now appeared. Now remember, because we're working on the front end, this file is totally visible. If you were taking this to production, this file would need to be serverside. And of course, you would also make sure you were ignoring it when pushing it to GitHub. But what we're doing today is purely on the front end. And this is absolutely fine while we're developing and running things locally. And as I said, towards the end of the course, we will look at how you can deploy your projects with your API key safely hidden. And then you'll be able to share your work and use it in your portfolios. OK, so in this mv file, I'm going to export a const. And this const will be process. And process is going to hold an object. And inside that object, we're going to have a key value pair where the key is nv. And the value will be an object with another key value pair. This time, the key will be openAI API key. And I've put that in uppercase letters with underscores separating each word. Now, the value in that key value pair will be our API key. So let's just copy and paste that from index.js. Now, we've done it like this with this environment variable to more closely mimic what we would be doing if this were a production app and this file were being stored on the server. And now that we've got the API key in one place, when you come to doing the challenges and we've got various API calls happening in the app, you'll only have to paste your API key in one place. Right, let's go back to index.js. And we're going to import that environment variable. So I'll come right up to the top. And I'll say import. And then in curly braces, we need process. And then we need from. And now we just need the path to our environment variable, which is just going to be slash nv. And this should work because in index.js, if we scroll down to the bottom, we've already got this piece of code right here, type equals module. So the browser knows to expect modular JavaScript. Now, let's check it's working. So what I'm going to do is comment out this API key. And I'll set up a new const called API key. And I'll have that store, the API key that we're importing with process. So it will be process. And then we're going to need.env and then.openaiapikey. Let's hit Save and see if it works. So I'll come in here, click the button. And there we are, enthusiastic and excited. OK, so we've got our API key where I want it. Next, let's install the openaidependency. And we'll come on to that in the next scrim. OK, let's install the openaidependency. And in scrimba, that's dead easy. Over here on the lefthand side, then, I'll come to the threedot menu. And I've got the option to add dependency. And again, that is not recorded, but a dialog box appears. And I just need to enter the dependency I want. And in this case, that is just going to be openai, all as one word, and lowercase. I hit Add. And there we are. The dependency has appeared on the lefthand side. So scrimba has done all of the hard work for me in the background. Now, if you're working outside of the scrimba environment, perhaps you're following this on YouTube or creating your own project in VS Code or any other editor, we will talk about how you can work with the openaidependency in just a couple of scrims' time. Now that we've got the dependency installed, we need to import two constructors from that dependency. Let's just check this code. So we're going to need configuration and the openai API. And they both have uppercasefirst letters. Now, they use require here. We are using import. So let's just come up here and say import. And we need configuration and openai API. And just note that AI here has got uppercase letters that caught me out the first time. Now we're importing them from openai. And let's just sort out my typos. And the first thing that we're going to do is set up a new instance of this configuration constructor. So down here, I'm going to say const configuration with a lowercase c. And I'll set that equals to a new instance of configuration. And as we can see here, we'll need to pass in our API key inside an object. And we've actually got the code we need for that right here. Now, we need to use this second constructor, the openai API. So I'll come down here. And I'll set up a const openai. And this will store a new instance of the openai API. And we just need to pass in the new instance of configuration that we just created. Oh, and I've just noticed that I've made a mistake right here. The AI is uppercase, but API is actually camel case. So we need to change that here and here. Glad I saw that before we got a strange error. OK, now we can delete a load of code. So we're not going to need this URL anymore. We're not going to need the API key saved in here. We actually only used it here for a test. So let's delete all of that. And now in the next grim, we're going to come down here to this fetch bot reply function. And we're going to make a lot of changes to all of this code right here. When you're ready for that, I'll see you there. So we're going to tidy up this function a lot. All we actually want to keep is the model and the prompt. So I'm just going to delete everything else. Now, I am going to keep this line right here where we update the speech bubble. But I'm just going to comment it out for now. And we'll need to make a few changes to it in a moment. Now, I'm going to come in here and set up a const response. And now, as per the code we've got in here, we can await our new instance of the OpenAI API. So remember, we've got that stored as a const right here. Next, we need to tell it to use the create completion endpoint. And then, just like they've done here, we're going to pass in an object with our model and prompt. So let's just cut and paste that right in here. Now, let's log out the response. So I'm going to hit Save. And it looks like we're getting an error. It says syntax error, unexpected reserved word. So here's a quick debug challenge for you. Can you figure out what's going wrong here? Just pause now and take a second to look at that. OK, so hopefully you figured out that because we're using the await keyword right here, we actually need this function to be async. OK, let's try that again. And we're not getting any errors. So I'll press the button. And there we are. We're getting our response. And if we look down in the console, we're getting the completion, excited, passionate, eager. OK, so to get this rendered, this line of code needs to change just a little bit. Let's just bring it up here and uncomment it. So all we need to do here is take this from response. So it's response.data.choices. And then we'll take text from whatever is at the zero index of the choices array. Let's give it a go. And there we are. It is working, excited, enthusiastic, driven. Now, I'm just going to delete this console.log. And you might well notice that we've got a lot of white space up here. And actually, if I just copy and paste something from the console, you can see that the completion starts with white space. So here's a quick JavaScript revision challenge for you. How do you remove the white space from the beginning and the end of a string? Just pause now and do that. OK, so hopefully you remembered that we can do that with the trim method. So I'll chain that on the end. Let's hit Save. And I'll press the button one last time. And there we are. Now it is at least equally spaced out. And as I said before, the CSS wasn't really designed to cope with such a short completion. But soon, the text that we generate is going to be much longer. OK, we have seen various enthusiastic responses from OpenAI. But the next thing that I want to look at is making that response actually tailored to whatever the user puts inside this text area. So I want to get OpenAI responding to our user's input in a human manner. But before we do that, I did promise that we'd quickly look at running our code outside of the Scrimba environment. So the next grim is optional. If you're working on this in Scrimba, or if you've already set this up locally, you can actually skip the next grim and move straight ahead. When you're ready, let's move on. Let's see how we can get this project up and running icon on the righthand corner. in VS Code with the OpenAI dependency. Let's see how we can get this project up and running in VS Code with the OpenAI dependency. So the first thing to do is to come on to this or any other So the first thing to do is to come on to this or any other Scrim, and you don't need to have a Scrimba account to do this. You can just click this cog icon down in the bottom right hand Scrim, and you don't need to have a Scrimba account to do this. You can just click this cog icon down in the bottom right hand corner. That's going to bring up a menu. Select Download as Zip. corner. That's going to bring up a menu. You need to unzip that package on your PC Select Download as Zip. and then click on the You need to unzip that package on your PC and then open the folder in VS Code. So it's just File and Open Folder. And often, the folder has actually got this really convoluted, randomly generated name. And of course, you can change that to whatever you want. Now, inside this folder, we've got several files. This one here is called Read Me for a Reason. Let's open it and see what it says. And basically, it's telling us that we need to run npm install and npm start. So what we need to do then is open up the terminal. And when we do that, we will get a new instance of the terminal down here. And this is where we can write npm install. And I've just put it a little bit bigger there, just so you can read it. When you do that, npm install is going to do its thing. Once it's done down here, you can run npm start. And again, there it is, a little bit bigger. And that will do its thing. And eventually, down in the terminal, in the terminal, you will see this. And so to run the project locally now, all we need to do is go to this link. And the link is made of your local IP address and this port. So the port, in this case, is 5173. Now, you could replace your local IP address with localhost and then have the colon and then the port. That will also work just fine. The quickest way to click on this link is in Mac to hold down Command, in Windows to hold down Control, and then that link becomes clickable. And it should take you through to your browser with the project running. And you can see here, I'm accessing this on localhost, but the IP address would work there just fine. Before we get to work on our response from the AI, I want to make a slight change. When we have just one word in these keys, we don't need them to be wrapped in inverted commas. Now, it's a totally personal thing, but I prefer it without where possible. So let me just tidy this up a little bit. OK, I feel strangely better for doing that, even though it doesn't matter at all. Let's get to work with personalizing the response we get back from the AI. So at the moment, we've given it this rather basic prompt. And you only get back as much as you put in, so it's giving us this very boring, generic reply. And if we hit Send, we'll see an example of that. Excited and eager. OK, so thanks for that. But to be honest, we could have hardcoded it ourselves. What I want to do is uncomment this code here. So what's happening now is when a user clicks the Send button, the If clause here is going to check that there is actually some text in the text area. If there is, it's going to render the loading SVG and update the speech bubble to our first generic message. At that point, I also want it to call FetchBot. Now, if we're going to get a personalized response, then our prompt needs to have access to whatever the user entered into the text area. So let's take whatever that is and save it as a const user input. And we'll pass in user input when we call FetchBotReply. And let's bring it into FetchBotReply as the parameter outline, because it's going to be a one sentence outline of a movie. Now, the last change I'm going to make here is I just want to log out the response. OK, and now it's time for a challenge for you. And I'm just going to come in here, and I'm going to paste it right inside this object, because it's this prompt here that I'm going to ask you to refactor. So I want you to refactor this prompt so the AI gives an enthusiastic, personalized response to the user's input and says it needs a few moments to think about it. Now, a couple of things for you to think about. We can use the parameter outline in the prompt by converting that prompt to a template literal with backticks. And you might want to put the outline in inverted commas or speech marks to signal to open AI that this is a chunk of text that you're referring to with the rest of the prompt. And of course, you can experiment with the wording, but don't be afraid to just ask for what you want. There's not one correct way of writing this prompt. And afterwards, I'll show you my way. Now, just before you do that, I'm going to do you a favor. I'm actually going to break the rules again and add a line of code here that we haven't talked about yet. Now, I want to do that because I'm aware that we've looked at loads of theory and we've laid loads of foundations, but there hasn't been too much time for you to get your hands on the keyboard yet. So I just want to pop this line of code in here, because without it, this challenge would be ridiculously frustrating. And then we will talk about it afterwards. So all I'm going to do is add a property to this object, max tokens 60. OK, so pause now. Don't worry about this mysterious new property at all. Get this challenge sorted, and we'll have a look together in just a minute. OK, so hopefully you managed to do that just fine. So I'm going to come in here, and I'm going to completely replace this prompt. And what I'm going to say is this. Generate a short message to enthusiastically say, outline sounds interesting, and that you need some minutes to think about it. Mention one aspect of the sentence. So you'll have noticed that I've put outline in inverted commas, so the AI understands that I want it to deal with that specific line of text. And also, this last instruction will hopefully make OpenAI personalize the completion. Now, of course, to get access to outline, we need to swap these four backticks. OK, let's hit Save, and I'm just going to put a oneline idea in here. And I'm just going to say, a spy deep behind enemy lines falls in love with an enemy agent. Let's press Send. And look at that. We're getting a really long completion. And if you just read through that, you can see that it's actually like we're interacting with a human. It's being conversational. It's referring to our idea, and that is exactly what we want. Now, down in the console, we've got the response. And just bear with me while I copy and paste something from there. Now, before I explain why I've done that, I'm just going to actually use the same idea again. But this time, I'm going to remove this line of code that I added just before you did the challenge. So I said, a spy deep behind enemy lines falls in love with an enemy agent. Let's press Send and see what happens. And there we are. We get our response, but look, it is much shorter. A spy deep behind enemy lines falls in love with an, and then it stops. My answer is cut off. Now, I'm just going to copy and paste the same properties from the response. And what we can see there is that the first time with the more successful completion, we had the finished reason of stop. And the second time when the completion was actually not complete, where we got cut off, the finished reason was length. So generally speaking, a finished reason of length is bad news. It means OpenAI has not given us everything it wanted to. Now, also, at the end here, we see completion tokens 59 in the first call and completion tokens 16 in the second call. So something we're doing with tokens is affecting the length of the completion we get. Now, as you're a seasoned coder, you've probably grasped that we can control the length of our completion to some extent with this max tokens property. But before we start using max tokens all over the place, we should really understand what a token is in OpenAI and what this number 60 really means. So in the next scrim, let's take a peek under the hood and take a dive into tokens and the max tokens property. When you're ready for that, move on. OK, let's talk about tokens and the max tokens property. So we know that when we add this max tokens of 60 to our API request, we get plenty of text back. We also know that if we don't set max tokens, we get much less text back and it's actually not complete and therefore doesn't really make sense. And that's why we're doing this. It's not complete and therefore doesn't really make sense. And that's what we can actually see right here. So what exactly is going on with tokens and what even are tokens? OpenAI breaks down chunks of text for processing. Now, I say text. It depends on which model you're using and what you're doing. It could also be code that gets broken down into chunks. But we're working with text, so we need to think about tokens in the context of text. I think that each word or each syllable would be a token. But it's actually not as simple as that. Roughly speaking, a token is about 75% of a word. So 100 tokens is about 75 words. So the 60 token limit that we put on this fetch request would bring us back a maximum of about 40 words. When we didn't use max tokens at all, this one here actually defaulted to 16, which as we can see here is way too short for our needs. So that's an important lesson. If you don't allow enough tokens, your completion will be cut short. So you'll actually get less text back from OpenAI. So now you might be thinking, well, OK, a token is a chunk of text. But so what? Why do I need to think about limiting it? And the best way to make sure that your max token setting isn't causing you problems is to check the object you get with your response. If you see the finish reason of length, that is a bad sign. That means the text that you're getting back has been cut short. And if you see a finish reason of stop, that is a good sign. That means that OpenAI actually got to the end of the process and it's given you all of the text that it wanted to give you. So you might be thinking, OK, so a token is a chunk of text. But why does that matter? Well, there are some good reasons for knowing about tokens and being able to limit them. Each token incurs a charge and it takes time to process. So that gives you an incentive. If you limit the number of tokens, you can keep costs down and keep performance up. And that's really important, of course, for when you run out of free credit and if you're creating a production app, and that app scales to millions and millions of users. And why shouldn't it? Now, there's something really important about max tokens that we need to understand. Max tokens does not help us control how concise a text is. As we saw in our app, we get an incomplete response when the token count is low, not a more concise one. So as a tool to control how verbose or how expressive OpenAI is, max tokens is useless. And that begs the question, how should I use it? And the answer is, we should set it high enough to allow a full response from OpenAI. So you might just have to do a little bit of experimentation with that each time and just making sure the text that you get back from the API is not cut short. So how can we control the length of text we get from OpenAI? Well, we do that with prompt design. Good prompt design is everything. And good prompt design is the best way to ensure that the text we get from OpenAI is the length we want. Now, I actually think that the text that we got back when we had max tokens set to 60 was just a little bit too long. So as we go through this project and we learn more about prompt design, we will come back and just do a little bit of refactoring here. But for now, I want to keep up the momentum, keep moving forwards. So let's start tackling our next API call, which is to generate a full synopsis from our one sentence movie idea. When you're ready for that, let's move on. OK, so we've got the one sentence idea from the user. Now let's make it into a professional synopsis for our movie. So I'm going to come down here and set up a function called fetch synopsis. And of course, this will be an async function. Now eventually, when the app's finished, we'll be displaying the synopsis to our user in this container, which will pop up when the finished movie pitch is ready to display. And the synopsis will actually be right down here at the bottom. Now, if we have a look over in the HTML, this is the section here, which is actually going to contain all of the output. And it's this paragraph down here, output text, which will contain the synopsis. So at the moment, this whole container is hidden by default. But if we go over to index.css and we just uncomment that one line there, now that container has appeared at the bottom. So in this new function that I've set up, fetch synopsis, what I'm going to do is take control of this paragraph that we've got right here with the ID of output text. And then we'll be able to display our synopsis right there just to check it's working. So inside the fetch synopsis function, we want to say document.getElementByID. And the ID is outputtext. And we'll set the inner text to whatever text we get back from the API. And we can just borrow that line of code right there. Because in every case, the text that we get back from OpenAI comes in an object. And it is always actually in the same place. Now, I'm going to call fetch synopsis from right here inside this event listener. And I'll put it right underneath where we get the first reply to our initial idea. And I'll pass in the user input because that will also be needed to get the synopsis. And of course, let's take that in as a parameter right here. And again, I'll just call it outline. OK, now it's time for a big challenge for you. And I'm just going to paste it right in here. So I want you to set up an API call with model, prompt, and max tokens properties. The prompt should ask for a synopsis for a movie based on the outline supplied by the user, which of course, we're bringing in with this parameter outline. And just remember, when writing prompts, try to be specific, descriptive, and as detailed as possible about the desired outcome. Try to avoid fluffy and imprecise descriptions. So don't use phrases like sort of or roughly. Now, we said before that we can control the length of the output with good prompt design. Now, you could experiment here with asking for a particular word count. Say, give me a synopsis of 150 words. But I just want to warn you that that's likely to be quite imprecise. So don't be frustrated if the length of the text you get is not what you want. We will be dealing with that shortly. OK, there is quite a lot to do here, but you've got all of the skills you need to do this. So pause now and get it sorted. OK, so hopefully you managed to get a really good synopsis. Let's have a look together. Inside here, I am going to set up a const called response. And then we'll await an openAI create completion. And we need to pass that an object with our model prompt and max tokens. So the model is text da Vinci 003. I'll put my prompt in backticks so we can use the outline. And I'm just going to say, generate an engaging, professional, and marketable movie synopsis based on the following idea. And lastly, we need max tokens. Because remember, max tokens will actually default to 16. And we don't really need that comment there anymore. So I will say, max tokens. And I'm going to go for something pretty big because I don't want our text to get cut off. So I'll say 700, and we can always reduce that later. OK, let's hit Save and give it a try. So for my idea, I'm going with this. A madman develops a machine to control all humans. And only intelligent animals can save the human race. Although, would they really want to? Anyway, let's give that a try and see what we get. So we're going to get our first message from the API. Again, we've got this pretty long response. And we're waiting now for the synopsis. And I'm just going to scroll down because it's just going to appear right here inside this box. OK, and there we are. We get our synopsis. And the results we get back are fine. Now, I'm just going to copy and paste them because actually, you'll probably find that if you pause, you won't be able to scroll the mini browser. So let me just paste them right here inside this file. Now, the first thing that we can say is that that is pretty impressive. We've got a long body of text, very much as if it was written by a human. So pretty happy with that. And although it's long, it does finish naturally. I don't think it's actually been cut short by having too few tokens. I think 700 is plenty. But I do wonder if it's as good as it could be. Now, our prompt instruction that I've written here is fairly good. But it's actually quite hard to describe what exactly you want a longer passage of text to be like in detail. And as we've talked about several times, it's also quite hard to control the length. This is arguably quite long. Now, I've just counted the words here and it comes in at just over 160. That's actually not bad, but we can't really rely on that. Also, I can't help feeling that the story it's laid out is just a little bit woolly. It would be nice if it was just a little bit tighter, a little bit more concise. If we're going to grab the movie studio's attention, we need it to be really sharp. So what I want to look at next is how we can help OpenAI understand more about what we want. So far, we've been giving OpenAI these simple oneline instructions. What I want to try next is to include one or more examples actually inside the prompt to give OpenAI a push in the right direction, so it's going to give us more of what we want. So let's take a look at that next. So far, when writing prompts, we've just provided one or more sentences asking for what we want. You'll often hear this referred to as the zeroshot approach. And what that means is it's just an instruction. Now, the zeroshot approach works really well for many simple requests. But when we work with more complex requests, we run a greater risk of getting back results that don't really fulfill our needs. So we might get completions which are offtopic or too long, or perhaps the format is just not what we want. Now, I've got an example of that happening right here. Let me introduce you to Advertify, which is an app I've made to generate copy for advertising purposes. It's pretty simple. The user can input a product name, give a description, and a target market. They hit generate copy, and they get a bunch of text they can use in their adverts. And if we just have a quick look at the code, there's nothing unfamiliar going on here. We're bringing in the user inputs right here from the inputs and this text area, and then down here, we've got this oneline prompt, and it's a basic instruction, and it's just using the product name, product description, and the product target. So the instruction is just create 50 words of advertising copy for this product, which can be described as the product description and aimed at the product target market. And we might just tidy up that last back tick and bring it all onto one line. Let's see this in action, but before we do that, I'm just going to log out the response. Now I'm going to go for the same vegan fish cream example that we've got in the placeholder text. And I'll hit generate copy, and let's see what we get. And okay, it's working, but look, it's given me an ordered list. We've got one, two, three, four, five, and six. And it looks a little bit longer than 50 words, but if we open up the console, we can actually see that we've got this, and I'll just paste it in here. It says finish reason of length. And as I'm sure you'll recall, that means that OpenAI has actually cut us short. And if you look, number six is not complete. It just says, grab your vegan dash. And at that point we know that OpenAI was trying to give us more. Now I've tried this several times before recording with various prompts, and OpenAI really doesn't work that well with word counts. The five words or less that we used in a previous challenge does seem to work okay, but longer word counts often fail. Now, interestingly, OpenAI does know how long a tweet is. If we were to change this prompt and ask for a tweet, it will nearly always deliver something that's tweet length and throw in some hashtags as well. Okay, let's try something else and see if we can make this Advertify app work a little bit better. We're going to have a look at the fewshot approach. The fewshot approach is where we add one or more examples to our prompt. We do this because it helps the AI understand what we want. This is great for more complex tasks. So let's refactor our prompt. Firstly, I'm going to remove all of the variables and actually rewrite this instruction. So I've said user product name, a product description, and a target market to create advertising copy for a product. At the moment, we're not doing anything with the user input, so that's just gonna give us something really, really random. But what we're gonna do next is lay out an example. So staying inside the backticks, I'll come down onto a new line and here's my example. So I've got a product name, which is Flask Tie, a product description, a tie with a pouch to hold liquids and a straw to drink through, and the product target market is Office Workers. Now I've also put some advertising copy right here and that advertising copy is about 54 words long, which is the kind of length I'm looking for. Now, before this is going to work, we need to do a couple of things. Firstly, we need to bring back our variables and we're going to lay them out in the same style as our example. So in fact, I'm just going to copy and paste all of that text. And now let's bring in the user inputs. So Flask Tie, we can replace with product name. Product description, we can replace with product desk. And of course, target market will be product target. Now advertising copy, we are just going to leave that empty. So can you see what we've done here? We've given an example and then we've given a partly completed example. And this space right here is an invitation for OpenAI to go and complete. And if you'll remember, we're using the create completion endpoint. So OpenAI is just going to complete this block of text here in the same style as this block of text here. It's going to recognize that we want it to give us something like that, but tailored to the three variables that we've got right here. If we tested this right now, it would likely work. But there's one more thing that we can do to help the AI and to make our code more readable. What we're going to do is separate our instructions and context. Now OpenAI docs suggest that you can do this with three inverted commas or three hash symbols. I'm just going to go for the hash symbols. And I'm going to put one set right here after the instruction. And then I'll put another set down here in between my example and where we're asking OpenAI to do its thing. And so all that's doing is breaking up this body of text a little bit so OpenAI can see that it's dealing with different entities, an instruction, an example, and a request to go and complete. Okay, let's hit save and see if it works. So I'm going to put in the same vegan fish cream and I'll hit generate copy. Okay, let's check out the copy we've got. So firstly, we haven't got this ordered list. There's no numbers one to six. What we've got is a body of advertising text and that is what I wanted. Secondly, I've just checked the word count of this and it's actually coming in as 63 words. Now the example that I put here is actually 54 words. So there's only nine words between the two and that is pretty good. And if we have a look down in the console, we can see that the finished reason is stop. That makes sense because this is very much the end of the sentence and the end of the text so OpenAI has given us everything that it wanted to give us. So the example is doing a really good job of allowing us to show OpenAI what it is we want. Now remember, this is called the few shot and we can add one or more examples. We should remember however, that longer prompts cost more so you don't wanna go too far. But let's go ahead and add one more example and actually this is something that you're going to do as a challenge. So let's get straight onto that in the next script. Okay, so here's your challenge. Add a second example here, be sure to make it similar in design to the first. Now I've put that because we don't want to confuse the AI. The examples that we give should be in a similar format and they should be consistent else we're just creating confusion. Now I've also put here, remember to use separators. So that is these three triple hashtags and you can use them in exactly the same way to separate out your example from the request to create. Now one thing I want you to remember here is that the law of diminishing returns applies. Adding more and more examples will not guarantee better results. You might see great results or you might not. Also before you do this challenge, I just want to say that I'm aware that some people following this course might be at a disadvantage here because English is not their first language or perhaps language is just something they struggle with. So I've added a file up here called product.md and that has got an example that you can use. So you can copy the text and just stick to practicing the layout and the syntax of the prompt. Okay, pause now, get this challenge sorted and I'll see you back here in just a minute. Okay, hopefully you got that working. So I'm just going to come in here and I'm going to copy all of the texts that I've got right here and let's just put our second example underneath the first. First thing that I want to do is separate out the examples. So we'll borrow these hashtags. Now I'm just going to take the product name and that will be solar swim, product description. It's this crazy swimming costume with solar cells to charge your phone while you're on the beach or what have you. And we've got the target market, which is of course young adults who'll spend their money on anything. Finally, we've got the advertising copy and that is approximately the same length and style as the advertising copy we've got right here. So that should do the trick. Let's just remove that space and we'll hit save. And let's give it a go and I'll hit generate copy. And okay, again, that has been pretty successful. The word count is coming in at just under 50 words. Well, our examples were 54 words and 53 words respectively. So that is pretty good. And also I think we're getting pretty decent copy. Now it can be really, really hard to judge because open AI is not going to give you the same completion for the same prompt each time. So it's really hard to judge when you refactor a prompt exactly how much difference is made. But I think two examples is a pretty good amount for this few shot approach. And it's definitely had a beneficial effect on this app. So what we need to do now is go back to the function that generates our synopsis and see if the few shot approach will improve it. When you're ready for that, let's move on. Okay, so we have this synopsis fetch request and perhaps we can tighten up the synopsis we get back and improve it by adding an example or two with a few shot approach. Now, when you do this, you're welcome to use your own examples. In fact, I would encourage you to do that. If you're going to be working with open AI, then it's important to practice creating prompts. However, I have put an example synopsis up here in synopsis.md, which you can use if you'd like to when you complete this challenge. So your challenge is to refactor this prompt to use one or more examples, just like we did in the previous script. And remember to separate out the instruction from the example with either the triple hash or the triple inverted comma. Check back if you need to remind yourself of the syntax and I'll see you back here in just a moment. Okay, hopefully you managed to do that just fine. So I'm going to come in here and we're going to rework this prompt and I'm going to replace this with the following. Generate an engaging professional and marketable movie synopsis based on an outline. Now I'll come down onto a new line. I'm going to add my hashtag separator and then I'll add my example. And my example needs an outline and a synopsis. And I'm going to use the examples I prepared earlier. This is the outline and then we'll also take the synopsis. And I've made sure that this example synopsis is the kind of thing that I want to get back from OpenAI. Now, underneath that we'll add another separator and then it's outline. And this is where we need to bring in the user generated outline. And then the synopsis, which of course we can leave blank. Okay, let's hit save. And I'm just going to pop over to the CSS and I'll just uncomment this one one more time. So we'll see the synopsis appear here and let's put in a one sentence idea. So I've gone for a madman develops a machine to control all humans and only intelligent animals can save the human race. There we're getting our long initial response. And then down here, let's just wait for the synopsis. And there it is. Okay, the first thing I want to do is check the word count. And the good news is, it's almost exactly the same length as this example. There's less than 10 words in it. Now also, and this is subjective, but if you read through it, I think this is a pretty good synopsis. To me, it seems tight. It tells the whole story. It packs a lot of detail in. There's not too much fluff. So I'm pretty happy with that. And again, as always with OpenAI, it's really hard to make a comparison. But I think adding that example to the prompt has made a big difference. Now, of course, the synopsis makes this prompt quite big. And if we were to add several examples, we could end up burning tokens here unnecessarily. So if we were gonna take this to production, maybe we would run a bunch of tests with multiple synopses and then reduce the number to the minimum needed. Now, before we move on, I have got one more challenge for you. And I'm just going to come up here and paste it in. Okay, so the initial response that we get back when we first put our one line concept into the text area is pretty long. It doesn't actually break the layout, but it does just stretch things a bit too much. And I think it's all a bit unnecessary. What I prefer is to see something like this. So we've got a good personalized response, but it's just not that long. There's just enough there to make it clear that OpenAI has understood our request and is giving us a conversational response which references the one line input from the user. So here's a challenge for you. I want you to refactor this prompt to use examples of an outline and an enthusiastic response. Be sure to keep the length of your examples reasonably short, say 20 words or so. So we just don't want it to be massive like this one. Now again, it's great if you can work on this on your own, but I have put a file up here called outline.md and that has got some examples in it that you can use, but do write your own if you're able. Okay, pause now, get this challenge sorted and I'll see you back here in just a minute. Okay, hopefully you got that working just fine. So I'm going to come in here and I'm just going to change up this initial instruction. I'm saying generate a short message to enthusiastically say an outline sounds interesting and that you need some minutes to think about it. Now I'm not going to instruct it to mention something from the outline like we did before, mention one aspect of the sentence. What I want to do instead is show it how to do that with examples. So let's come in here and put the separator. And now I'm going to go over to outline.md and I'm going to bring in the examples that I've got right here. So I've got three examples and each one has got an outline and a message. So let's just label them clearly. And I'm going to separate each example with a separator. Okay, now let's come down onto a new line. We'll add another separator and then we'll say outline and message. And of course message will leave blank because that is what OpenAI is going to do for us. And outline will just be the outline that we've got coming in here as a parameter. Okay, let's hit save and I'm going to use the same example as I used before. And let's see what we get. But before we do that, let's actually just get rid of the output box that was at the bottom. And also just momentarily, I'm going to comment out this fetch synopsis function. So the app will only go so far as to call this function. I'll need to add my outline again and let's hit go. Okay, there we're getting a much shorter response. Wow, that's incredible. A madman and his machine? I'm feeling inspired already. Let me take a few moments to ponder it. So a much shorter completion, just like our examples, but still clearly referring to the outline I put in. Okay, now the app is working much more smoothly. But before we move on, there's just one little thing I want to tidy up. We've got a global variable up here. And actually we're only using it right here. So my bad for declaring it globally, I'm just going to move it down here inside that function. And I think that is better practice and a little bit neater. Now, next up in our app, we need to create an iconic title for our movie. But I think this would be a good moment for me to say a quick word about the architecture of this app. So let's just take literally a couple of minutes to talk about that. I just wanted to say a word about the architecture of this app. As always with code, there are loads of ways you could do this. So I want to say right now that the way I'm doing it here is definitely not the one best way. And in an app this size, actually it's not going to matter very much at all. But there are some things which we could do here which we're not doing and I wanted to give you an explanation as to why. So firstly, on the code reusability front, we are building objects for each of our calls to the API. And as you can see, before this app is finished, we will have done that multiple times. Now, would there be a way to set up one function to do the fetching and pass it an object with the prompt, max tokens, et cetera, every time we need it? Probably yes. And that would make our code much more reusable and less repetitive. But the reason I'm not doing that is just to let the AI take center stage and not get bogged down in JavaScript. Once you've got the basics of working with open AI, as you will by the end of this project, that is the time to start looking at ways of saving time, saving work and being more concise with the code. And likewise, normally I like to have my functions as much as possible just doing one thing. And we're breaking that rule here because we've got a function that's doing the fetching and it's also rendering out to the DOM. Now, we could potentially set up one render function and then have these fetch functions just return the response. And then when all of the API calls have completed, the render function could render out the results. It would be doable. We could store all of our completions in an object and have a function that checks if all of the fetch requests have responded successfully before triggering the render. But again, that would involve a whole lot more JavaScript when in this course, I really want to just focus on the AI. Now, at the end of the course, if you would like to refactor the code, that is not a bad idea at all. In fact, it's a really good idea. But I just wanted to give you a quick explanation now of why I chose to do it like this. Okay, with that done, let's go ahead and use open AI to generate a movie title that's so iconic, it's going to reverberate down through cinema history for the next 500 years. When you're ready for that, let's move on. We need a title for our movie. So let's go ahead and set up a function to do that. Now, I'm going to call this function fetch title. And as you know, by now, that is going to be an async function. Now, if we have a quick look over at index.html, we've got an element here, it's actually an H1. And that is where the title of our movie is going to be rendered. So back inside this function then, let's just deal with that first. Now we're going to generate this title from a synopsis. So we need to take that in as a parameter. I want to call this function from inside this fetch synopsis functions. So as soon as this response comes back with a synopsis, we want to send that synopsis straight back to open AI to get us a title. So I'll call the function right here. Now we need to pass in the synopsis and we've got that right here. But instead of writing out this code several times, why don't we actually set that up as a const called synopsis? And now we can use synopsis here and we'll also pass it in to the fetch title function. Okay, you should be pretty good at this by now. So I'm going to leave the rest of this function up to you. Just bear with me while I paste in your challenge. So I want you to write a prompt asking for a title based on a synopsis. If you'd like to, you can specify that the title should be gripping or alluring. And I say that just to remind you that being descriptive in prompt writing is a good thing. Secondly, remember that we need to add the model property and of course we should give it some max tokens. The default max tokens of 16 might be enough. That really depends on how long you think a movie title should be. I mean, some movie titles are really, really short. The Stephen King classic is, for example. Some are kind of medium to long, Pirates of the Caribbean, The Curse of the Black Pearl. And then some are quite ridiculous. Night of the Day of the Dawn of the Son of the Bride of the Return of the Revenge of the Terror of the Attack of the Evil Mutant Alien FleshEating Hellbound Zombified Living Dead Part Two. And that film is real by the way, but it is a spoof film. Okay, I'll leave that up to you. Set some max tokens that you think are gonna cover the kind of title length that you want for your film. When I do this, I'm probably gonna set it to something like 10. Okay, pause now and go ahead and do this. Okay, so hopefully you managed to do that just fine. So I will come in here and set up my response. And then we will await the create completion call. And then inside the object, we need the model. We need the prompt. And my prompt is just going to be, generate a catchy movie title for this synopsis. And as for max tokens, I'm actually going to go for 15, just to give myself plenty of leeway if it does end up generating a really long title. Okay, let's hit save. And then down here, we've still got this display div visible. Remember in index.css, we've just commented out in a display none, so eventually it will be hidden and then we'll just show it at the end when all of the API calls have been completed and the finished product is ready to show. But for now, let's just watch when our synopsis and title appear. So I'm going to put in my one sentence movie idea. And this is quite a long one. A time traveler goes back to the 1980s to prevent a catastrophic event, but falls in love with someone from the past and must choose between saving the world or staying with them. Let's hit send. There we've got our first bit of feedback. Here comes the synopsis and here is our title. 1985 forever, James Mason's time traveling love story. Now I'm a little bit concerned here that we've got a quotation mark at the beginning, but not at the end. And if we just count up how many words we've got here and we bear in mind that these are quite long words, I'm just a little bit worried that my max tokens of 15 might not be enough. So I'm actually just going to err on the side of caution and whack that up to about 25. Let's try that again and see what happens. Okay, interesting. This time it's actually given us a much shorter title. So we don't actually know if the max tokens would have made a difference or not in that first attempt. I should have logged out the response. Then I would have been able to actually see the reason for finish. Was it stop or was it length? But nevermind, we've got a title and it's a good title. Or is it? That is rather a matter of taste and to some extent a matter of culture. Consider this. In the English speaking world, this spooky Bruce Willis film is called The Sixth Sense and millions and millions of people have seen it. In China, it's actually called He's a Ghost, which is just a much more direct name. And without wanting to give away spoilers, it's actually a title that gives away key points of the plot. So ideas for titles will vary. Now we know that we can adjust the kind of outcomes we get from OpenAI with how we phrase our prompts. And we also know that we can give examples and that will also tweak the kind of responses we get. But now I want to introduce a new property, which is called temperature. And we're just going to add that property right here to the object that we pass to OpenAI. Before we dive in and start using temperature, let's just have a quick bit of theory about what it does. So we know that OpenAI works by predicting the likelihood of one language chunk or token following another. So if we had a phrase like the dog, well, OpenAI could complete that as the dog was sleeping, the dog was hungry, the dog bit me. There are millions and millions of possibilities. What temperature does is it controls how often the model outputs a less likely token. So what it's doing is giving us some control over whether our completions are safe and predictable on the one hand or more creative and varied on the other hand. Now temperature is set from zero to one in increments of 0.01, although you are going to find it really hard to see the difference with small changes. Now it defaults to one, so all of the API requests we've made so far in our app have used the default temperature of one. Now lower temperatures, for example, at zero, will use fewer less likely tokens. So the completions will be less creative and more predictable. And this is absolutely great if you're looking for factual responses. Now higher temperatures will use more less likely tokens. So in doing that, you're going to get more variety and more creativity. Now I'm wondering if my titles are a bit too wacky. So I'm going to come in here and I'll actually set the temperature to 0.7. And let's save that and give it a try. So I'm using the same outline as before. And here we are. We've got a title back, The Power of Love, Billy Biggs Time Travel Adventure. OK, that is a pretty good title. Now I have to reiterate, it's very, very hard to compare completions because there's so much variety anyway. I could experiment more. I could take this down to zero. But to be honest, I'm actually happy with what I've got right here. So I'm going to leave temperature alone. But why don't you have a play around with it and see how you like the results, see if you get a big difference or just a small difference. And remember how we're generating our title. We're actually using the synopsis. And the synopsis was generated with a temperature setting of one by default. So if you lower the temperature here in Fetch Title and you're still getting something which is too weird for you, try actually changing the temperature property in this function right here. So there's no right or wrong way to do this. It's just a matter of taste. And of course, it's important to know about the temperature property and how it works. Now we've come a long way. There's only two more things that we need to do with this app. We need to get the cast. And we need to generate an image. So when you're ready for that, let's move on. In our finished product, we're going to have a starstudded cast so movie industry insiders can better visualize our story. And this is going to give us a great chance to look at text extraction using OpenAI. If we have a look at our Fetch Synopsis prompt, in this example, we have actually got the actor's names in brackets after each character. But we're not always seeing that in the output. In this screenshot we are, we've got Jeff Bridges and Emily Blunt. But in this screenshot, we're actually not getting that at all. We've got the characters, but no names for suggested actors. Now that's probably being a bit inconsistent because we're not specifically asking for it. Although we've got it in the example, we haven't got it in the instruction. So here's a very quick mini challenge for you. Ask for actors' names in brackets after each character. And I've just put here, you could also suggest that OpenAI thinks of actors that would particularly suit the role. OK, pause now and quickly sort out that challenge. OK, hopefully you got that to work just fine. I'm just going to add a sentence onto the end of the instruction. The synopsis should include actors' names in brackets after each character. Choose actors that would be ideal for the role. OK, let's save that and see if it's worked. So I'll put an idea in here and we'll press Send. And there we are. We've got our synopsis. And we can indeed see that we're getting the actors' names in brackets after each character. Now I've tested this and it is very consistent. OK, the next thing that we need then is a function so we can extract the stars' names from this text so they can be listed out right here just above our synopsis. You could do that with vanilla JavaScript, but it's actually going to be easier to let OpenAI do that for us. So I'm going to come down here then. And after we've generated the synopsis, at the same time as we ask for the title, I'm going to call another function called FetchStars. And of course, we'll pass in the synopsis. Now I'll come down here. And because you've done this lots and lots of times, I'm going to do all of the heavy lifting here and set up that function minus the prompt. Now we should set some max tokens here. I think we don't need very many. 30 will be more than enough. And I'm not actually going to set a temperature here. I've tested this. And it really doesn't make much difference at all. So we might as well leave it at its default. Now we can see on the screenshot that we want the stars right here. Let's just check the HTML. And we've got this H2 element with the ID of outputstars. So that is where we're going to render our stars. And of course, this function needs a parameter. OK, now it's time for a challenge. And I'm going to paste it in right here just above the prompt. I want you to use OpenAI to extract the names in brackets from our synopsis. Now I've left that challenge wide open because I think we've done enough prompt engineering by now for you to be able to figure out how to do this. If you feel ready for that, pause now. Go ahead, get it sorted. It's always best to try and figure things out on your own as much as possible. But if you'd like a few more pointers, or if you've tried it and you've got stuck, I have put a hint file up here. It's called hint.md. And it's just got a couple of pointers that will push you in the right direction. OK, pause now. Get this challenge sorted. Do some experimentation. Take all the time you need. And I'll see you back here when you've got it working and we'll have a look together. OK, hopefully you got that working just fine. So I'm going to come in here and I'm going to start off with a simple instruction. Extract the names in brackets from the synopsis. That should do the trick because OpenAI is more than capable of recognizing names and brackets. But what I do want to do, though, is add an example. So I'm going to come down here onto a new line and I'll use a divider. So that will be the triple hashtag. And now I'm actually going to take the example that we've got up here. It's this Top Gun one. So we can borrow all of that. And I'll just paste it in here. Now we've got all of the names in brackets. So let's just show OpenAI what it is we're looking for. So I'll say names and then a colon. And then I'll just list out all of the names from the brackets. So Tom Cruise, Val Kilmer, and Kelly McGillis. And if we have a look back at the slide, we're actually getting a comma separated list here. And that is actually what we want. So those commas are there for a reason. Now let's come onto a new line again and I'll use a divider. And now let's just put synopsis colon. And this is where we want to pass it, the synopsis that we're bringing in right here. And then underneath that, names. And we'll leave that blank because that is where OpenAI is going to do its thing. Okay, let's hit save and we'll see if it's working. I'll paste an idea in and hit send. And let's check out our results. So we've got a title and there we are. We have got our stars, Al Pacino, Kevin Hart, and Dwayne Johnson, what a lineup. That I think will just be a really, really cool film. And we've also got a decent synopsis. Now the final thing that we need to finish this app off is an image. But before we can do that, we need to learn about generating images with OpenAI. That's a little bit different to everything we've done so far. So let's investigate that next. Okay, let's take a look at generating images with OpenAI. So you've probably seen OpenAI's image generation tool, DALI, and if you haven't, you really should take a moment to go and play with it. This link is of course clickable. It will take you straight there. It really, really is a lot of fun. Now, as well as the DALI playground, we can access the OpenAI image API, which allows us to generate images in our application. And I'm actually building one such application right here. It's a really simple, fun game. All you have to do is describe a famous painting without saying the name of the painting or the name of the artist. So if I wanted to generate an image of the most famous painting in the world, I could say something like, a 16th century woman with long brown hair standing in front of a green vista with cloudy skies. She's looking at the viewer with a faint smile on her lips. Looking at the viewer with a faint smile on her lips. And if I give that description to OpenAI, hopefully it's going to give me a good likeness of the Mona Lisa, which is going to appear right here inside this picture frame. Now, all of these CSS and HTML for this app is already done. We just need to finish off the JavaScript and there's nothing unfamiliar happening here. The user is going to input their image description right here, click create. There's an event listener on that button and it is going to call generate image, generate image is going to call the OpenAI image API. So let's go ahead and set up a response right here. And up until now, this is where we've been using the create completion endpoint. And now we want to use the create image endpoint. Now, just like with create completion, we need to pass an object with a set of properties, but actually these are not the same properties as we've used before with create completion. We actually don't need to specify a model. We don't need to give it max tokens or temperature. Let's just have a quick look at the properties that we do need. So first up, we need a prompt. This will be a description of the image and we'll go into more detail about prompt writing for images in just a moment. The second property is N and N just stands for number and it controls the number of images we get back from OpenAI. Now we can pass it an integer between one and 10. So the maximum number of images we can get in one go is 10 and it will actually default to one. So strictly speaking, I didn't need to add it here, but it's really, really important that you know it's there because in the future you might want to work with multiple images. Next up, we've got size and size takes in a string and that string is going to hold the size in pixels of the image we want. We've actually got three choices here. We can have 256 by 256, 512 by 512 or 1024 by 1024. Now the default here is the big one 1024 by 1024. And remember, bigger images cost more credit. So what you don't want to do is just always leave that at the default, take the biggest images and then resize it to something much smaller with CSS. That's really uneconomical. So just be careful to go for the image size that you want. Today we'll be going for the smaller image. And the last property we've got here is the response format and that is also a string. And the string can either be URL or B64 underscore JSON. So what this is giving us is the format of the completion. If it gives us a URL, we can just use that URL as a source inside an image element. And actually this will default to URL. And when you're doing the challenges, I recommend that you use URL. But there's a bit more we need to say about response format. Firstly, you need to be aware that OpenAI image URLs only last for one hour. So if you want to keep an image, you need to download it. As I'm recording this and my images need to last longer than an hour, I'm actually going to use this B64 underscore JSON method. And this is going to give me an encoded PNG image so I don't need to rely on OpenAI's URLs. Now, if you've never worked with a base 64 encoded image before, all it is is a massive chunk of code which the browser can interpret as an image. This is one that I've just pasted into VS code and it is absolutely huge. If you try and log this out in Scrimba, you'll likely actually crash the editor. You can search online for base 64 image to PNG conversions and you'll find plenty of sites where you can just paste in all of this code and it will just give you an image. But what's also important to know is that you can add a little bit of code just before the source in the image tag to tell the browser to expect a base 64 encoded image. And we're going to see that in just a moment. A quick word about prompt design. Prompt writing for images is actually less complex than the prompt writing we've done for text so far. All we really need to do is describe what we want in detail in a maximum of 1000 characters. Now, the more detailed the description, the more likely you are to get back the results that you want. So consider this, if you ask for a white dog, that's a bad description. You're not giving any detail at all and you're exerting no control over what you're going to get back. If on the other hand, you ask for a closeup studio photographic portrait of an old English sheepdog, well, that is a good description and then you can really start to imagine what you're going to get back and you'll actually find with that level of detail, it's quite easy to exert control over the images you get back from OpenAI. Let's get the rest of this coded out and then we can actually see it all in action. So I'm going to come in here and the first thing that we need is a prompt and that prompt is going to be whatever we've brought in here as a parameter which is whatever the user has inputted into the box here. Next, we need N and we only want one image today so I could leave out N, it does default to one but as I said, I just want to keep reminding you that it is there for when you need it. Next, we need size and that one is a string and I'm going for the smallest option which is 256 by 256 and that is just a lowercase X right there in the middle. Now lastly, we want the response format and again, this is a string and I'm just going to set it to URL. Now that is the code that it's best for you to use when doing these challenges. You're going to get back a URL and I've already set up this image element right here. The source is using the URL and therefore the image is going to appear right here. As I said before, I can't do that. I actually need to use base64json so that's B64 underscore JSON and that does mean that I won't be taking the URL because actually this is not going to give us a URL. It's actually going to give us this base64 encoded image so I'm going to change URL to B64 underscore JSON. Now I would log that out to show you so you could see the full response but actually the base64 encoded JSON is so big as I think I said before, it actually crashed the browser so I won't do that but feel free to experiment. Now there's one last thing that I need to do to make this work. You're not going to need to do it if you're using the URL format but I'm just going to paste a little bit of code right in here and what this code does is it just tells the browser that what's coming up is actually a data version of an image. It's a PNG image and the data type is base64. Without that code right there, this will not work. So let's save it and see if we can describe the most famous picture of all time. So I'm putting in here a 16th century woman with long brown hair standing in front of a green vista with cloudy skies. She's looking at the viewer with a faint smile on her lips. I've got the original ready down here for comparison. Let's hit create. And there we are and that is actually not bad at all. In fact, I think it's pretty hard to tell which one is the original and which one is my creation. That is a good likeness of the Mona Lisa. Now in a way I've been quite lucky here. You do have to work quite a bit with image prompts if you've got a really exact idea of what you want and it does all just come down to being descriptive and being detailed and you'll be really surprised actually by how much open AI knows about styles. You can talk about impressionism or the style of Matisse or Picasso. You can talk about different lights, shades and hues. You can talk about anime and manga. Just go into as much detail about the image as you want to. Now I'm going to leave you to play with this. Hopefully you can describe a few more famous paintings. You might even do better than I've done. When you're ready, go back to the app. We're going to put these image generating skills to good use, perhaps not quite in the way you'd expect us to actually. All will be revealed in the next scrim. We need an iconic image to complement our synopsis and title. These ones from Jaws and Star Wars are really, really cool. And in an ideal world, we too could have cover art that featured our starstudded cast and had the title of the film right there blazing across the cover. Now, unfortunately, OpenAI is not going to generate recognizable famous faces for us. And also, it can't reliably add text to images. So what we're going to do instead is focus on illustrating aspects of the plot. We can still create some funky images. And the app is going to end up looking something like that. Now we know how to get images. And what we could do quite easily is just come down here and add an input field to the app and have our users write an image prompt. But the whole point of AI is that it's labor saving. So what we want to do is actually use the title and synopsis to generate an image prompt for us. And that is a key power of AI. You can use what you've generated to generate something else. So it's a kind of chain reaction. Now, we're going to use two functions to do this. The first function is going to generate the prompt. And the second function is going to use that prompt to generate an image. So I'm going to come down here and set up a new function called fetch image prompt. Now, fetch image prompt should take in both the title and the synopsis. And I'm going to call this function from the fetch title function that we've got right here. Fetch title takes in the synopsis. And it gets us the title. And so what I think we'll do to keep things a bit tidy is actually set up the title on a const right here, because we're actually going to use the title twice. We'll use it to update the DOM. And now we'll use it when we call the fetch image prompt function. So we'll pass in title and synopsis. OK, now before we generate any images, we should actually check in the console what prompt we get back to make sure it's good. So for now, I'm just going to log out the prompt that this function generates. And now, of course, we need to build the AI call. And again, I'm going to do most of the heavy lifting before I set you the challenge for the prompt writing. So we're using the same model. I've left the prompt blank for now. And I've set max tokens to 100, which should be easily enough. And now let me paste in a challenge. OK, so this is your challenge. I want you to write a prompt that will generate an image prompt that we can use to get the artwork for our movie idea. OpenAI has no knowledge of our characters, of course. So the image prompt needs descriptions, not names. What do I mean by that? Well, if we have a character in the synopsis called Katie, and maybe she's the protagonist, and our image prompt says Katie is standing in front of a burning building, well, of course, the image API has no idea who Katie is, no idea what she looks like. So what we need is a description of Katie and what she's doing. So maybe how tall she is, what she's wearing. Perhaps she's carrying an axe or some other weapon and is standing in front of a burning building. I've set this as quite an open challenge, and deliberately so. I want you to do some experimentation here. But I will say that you might find it best to use examples. So I have put some examples up here in this examples.md file. But do, of course, feel free to write your own examples or go online and find some synopses and titles of your favorite films. And perhaps you can use them as well. OK, pause now, get this challenge sorted, and I'll see you back here in just a moment. OK, hopefully you managed to do that just fine. So let's come in here and start working on the prompt. I'm going to start off with an instruction. I'm going to say, give a short description of an image which could be used to advertise a movie based on the title and synopsis. Now, I'm going to add to that, the description should be rich in visual detail, but contain no names. Now, this second sentence may or may not be effective. OpenAI prefers to be told what to do, not told what not to do. But I've had reasonable success with this, so I'm going to phrase it like that. Now, I'm going to add some examples. And I actually put three examples in the examples.md file, but I'm only going to use two. I think two is enough. So first, I'll use the separator, and now I'll paste in my examples. So let's just tidy that up and add any more separators that we need. Remember, what we want to do is separate the instruction from the examples and actually separate each example as well. And of course, the important part is to include our title, our synopsis, and leave OpenAI a space to create the image prompt. OK, now, just as I was doing that, I realized that up here, I've asked for a short description of an image which could be used to advertise a movie based on title and synopsis. And then down here, I've used image description as two words. Image description hyphenated and image description hyphenated. Now, OpenAI can probably cope with that kind of typo, but it might be worth just looking out for that kind of thing and just making sure we are consistent. So I'm just going to change both of those to two words. Now, I'm going to put temperature to 0.8. And I'm doing that because I found that some of my image prompts were just a little bit too wacky when it was left at the default one, which is the most creative setting. Now, it's really hard to tell with temperature whether you're making a big difference or not sometimes. It might just have been that the ideas that OpenAI gave me at that time were just particularly strange. Now, it can be really hard to tell sometimes if small temperature changes are making much difference. It could just be that the examples I got before happened to be particularly strange. But that said, I've tested this a few times, and 0.8 seems to be about right. OK, so let's hit Save, and we can test it and see what we get. So I'll press Send. And now let's open up the console because that is where our image prompt is going to appear. And there we are, a colorful image of a raccoon, elephant, alligator, and squirrel standing in the middle of a city street back to back facing menacingly towards the viewer. Around them, robotic and alien forces march in the background, weapons drawn. In the sky above, a giant robot claw looms ominously. Now, I think that is a really, really nice image prompt. Now, if you approach this challenge differently and you've got different results which you're still happy with, that is absolutely fine. There is no one right answer to this. OK, we've got an image prompt. So in the next scrim, let's go ahead and use it to generate an image. When you're ready, let's move on. OK, so it's time to get this image into place. We've generated a good prompt. So now I'm going to set up a new function called fetch image URL. And of course, that needs to be an async function. Now, to fetch the URL, it's going to need a prompt. And we generated the prompt. And we've got it back in this response right here. So it's going to take in a parameter. And I'm just going to call that parameter image prompt. Now, if we have a look at the HTML, we can see that down here inside the output section, we've got this div. And it's got the class of output image container and the ID of output image container. Well, that is, of course, where we're going to render the image. So let's come back to our function. And I'm just going to add one line of code right here. Now, the source of that image is actually going to be the URL that we get back from this fetch request to the API. So I'm actually going to leave that empty for now, because that is going to form part of your challenge. But before we get onto that challenge, there's just a couple more bits to do. I want to call this function from inside this fetch image prompt function. So I'll do that right here. And we need to pass in our image prompt. Well, we've got that right here. So let's just take that and paste it in there. And I'm just going to delete this console.log. Now, the other thing that I'm going to do is just comment this line of code once again. So that is just the display none on the output container. So we'll be able to see the image right there. OK, now bear with me while I paste in your challenge. So this is your challenge. Use the image prompt to generate an image. The image should be 512 by 512 pixels in size. We only want one image, and we want to get a URL back from OpenAI. Think about what properties you need to put in the object that you're going to pass to OpenAI. Now, one word of warning. If you find a lot of garbled text in your images, you could specifically request an image with no text. And what I'm talking about is this. Sometimes, OpenAI gives you images which have got this garbled, meaningless text. And this image has also got a really dodgy white border. So there's literally nothing to like about it. So you could just try adding to your prompt a sentence which says, I don't want any text in the image. OK, pause now. If you need to flip back to the scrim before last to refresh your memory, go for it. Get this challenge sorted, and I'll see you back here in just a minute. OK, hopefully you managed to do that just fine. So I'm going to come in here, and I'll set up my response. And we're going to await OpenAI.createImage. And we know we need to pass it an object. And the first thing we'll put in that object is a prompt. And the prompt is going to be this image prompt parameter we've got here. But I do want to add a sentence to try and avoid this garbled text on the image. So I'm actually going to put this in curly braces. We'll have the dollar sign, and we'll wrap it in backticks. And I'm just going to add there should be no text in this image. OK, next I'm going to add an n property. And we don't really need to do that. Again, just a reminder for you, we only want one image. And that is what we'll get by default. Now, for the image size, we wanted 512 by 512. And the response format will default to URL anyway. But let's just write it out. And all we need to do now, then, is deal with what we get back. And we're going to put it right here inside the source for this image. And in fact, I'm just going to delete this challenge text, so we've got a little bit more space. So the image element is already inside backticks. So I can come in here with the dollar sign and the curly braces. And it will be response.data.data. And I'll just close the mini browser, because this is getting kind of long. And we actually want the element at position zero. And from the object store there, we want the URL. Now, when you tried that, it should have worked perfectly. As I said before, I can't use this format in the Scrimba recorder, because the image will have disappeared by the time you actually get to watch this. So I'm going to make a couple of changes here. I'm going to change this to B64 underscore Jason. And I'm also going to go for a slightly smaller image, because actually, the encoded images are quite big. It's a little bit overwhelming for the mini browser, which can actually crash sometimes if you give it too much data. So I'm just going to go for the 256. A little bit more to do. I just need to tell the browser that this image is coming in a data format. And I also need to update this. It won't be a URL property. It will be B64 underscore Jason. OK, let's hit Save. And let's put a one sentence outline right here. And it's one we've seen before. It's the time traveler going back to the 80s and falling in love. Let's hit Send. OK, and we have our image. Now, the first thing to mention is this is a little bit small, because I've gone for a smaller image size. So I'm just going to make a little tweak to the CSS. I'm going to come in here where we've got the output image container, and we're looking for an image within that container. And I'm just going to change max width to width. OK, so everything is working. We've got a title. We've got an image. We've got some stars, and we've got a synopsis. So I think what we need to do now is put that to display none. And we just need to make a few little updates to the UX. Now, if you recall, what we want to happen at the end is that we get this view pitch button, which will then display whatever OpenAI has given us. So what I'm going to do is come down here into this function, and we're going to add some more logic right here. And the first bit of logic is just going to take this container that we've got here, which is currently holding the text area where we write and then this loading SVG, and it's just going to replace it with this pink button. So let's go ahead and put that right in here. Now, the next thing we need to do is actually wire up this button. So I'm going to set up an event listener right here. And what we'll do then is we'll set the setup container to display none. So that's the entirety of this section right here. So that is this entire white container you see in the mini browser. So let's take control of that container, and then we'll access its style and display, and we'll set that to none. Next, we want to take the output container, which is this one right here, and that is the one which is set to display none by default in the CSS. And we want that set to display flex. Now, while we're here, I think we should actually quickly update the message that we get from the movie boss at the very end. He should make some outrageous claim about his own ability and ask for some money. So we've already selected the movie boss text. So let's set the inner text of that to the following. He's going to say, this idea is so good, I'm jealous. It's going to make you rich for sure. Remember, I want 10%. OK, now for the moment of truth, let's see the whole thing in action. I'm going to say a pizza delivery rider infiltrates an organized crime group. Let's hit Save and cross our fingers. So we get the first message. We get the personalized response. Now we've got the View Pitch button, and we're getting an error. I've made a really silly mistake. You can probably see what it is. Just pause now and debug that. OK, so you probably noticed I've actually forgotten to put the inverted commas around these two. Let's try that again. There's the first message. And here is the View Pitch button. And there we are. We have got our image. We have got pizza delivery man, the Loire Vitale story. OK, that sounds pretty classy. It's John Travolta and Uma Thurman. That's a pretty good mix if you like Quentin Tarantino films. And let's check out the synopsis. And actually, that synopsis is pretty cool. I would go and see that movie. OK, so everything is working. So let's just take one more scrim to recap what we've done with this project and where you can go from here. Congratulations on finishing Movie Pitch. Now you've got all of the foundations you need to work with the OpenAI API in your apps. Let's just quickly recap what we've studied. So we set up the OpenAI API. And you can see that right here on lines 1 to 11. And we used the text DaVinci 003 model with the Create Completions endpoint. And you can see that down here on lines 25 and 26. Now we started off using the zeroshot approach. So we made a simple prompt request to the API with just a oneline instruction. We then upgraded that to the fewshot approach with multiple examples. And we can see that right here, lines 27 down to 39. And we used the fewshot approach to demonstrate to the API what sort of completion we were looking for. We also used the max tokens property. And you can see that right here. We did that to ensure the model had enough tokens to answer the query successfully. And of course, we also used the temperature setting. And we can see that down here on line 70. And that alters how daring our completions were. Higher temperatures allowed OpenAI's virtual imagination to go into overdrive. And lower temperatures kept things safer and more predictable. Now finally, if we come down here, we can see that we have got a slightly different endpoint in use right here. It's the create image endpoint. And we used it to generate images from text prompts. So wow, that was quite a lot. Now it's always a good idea to make a project your own. And there are multiple ways that you could take this project to the next level. Here are just a few ideas. So in terms of the JavaScript, at the moment, we're making a lot of API calls to the same endpoint. I wonder if you could find a way to think dry, do not repeat yourself, and have one function do the calling so we don't repeat the same line of code like this one that you see right here multiple times. So that would just involve a bit of neat refactoring. Now in terms of the AI, if you want to get anywhere in Hollywood or Bollywood, you're going to need a script. And the logical next step of this project is to have OpenAI create a script for your movie. Now that is a little bit tricky in terms of the amount of tokens you'd have to use. So you would have to break the process up into chunks. But it is doable. And then you could perhaps use the Create Image endpoint to create more detailed character sketches. Or you could tailor the app to a more specific genre, so have it create specifically manga or romcoms. Now if you take these three ideas and sell the script, I want 5%. Now ultimately, the best way to consolidate your learning is to delete all of the code and start again from scratch. You can use it to practice your HTML, CSS, JavaScript, and your knowledge of the OpenAI API. Now of course, you wouldn't need to do it exactly the same way. You would do it as you think best and just justify the changes you make. So quite a lot of potential work to do there. But with the skills you've got, all of this is possible. Now whatever you do, make sure you take a break now and consolidate what you've learned before moving on to the next project. It's official. Chatbots are taking over the world. Loads of sites have them. And since the arrival of the more advanced chat GPT models, like text DaVinci 003, and now GPT 4, the model we're about to use, chatbots are going to skyrocket even more. And as they are one of the most common uses for AI in web dev, this course wouldn't be complete without one. So we're going to build a chatbot using GPT 4, the latest OpenAI model at the time of recording. And it looks like this. Know It All does exactly what it says. It answers your questions and converses with you at a human level. You can ask it anything and it will do its best to answer. Now I need to say a quick word about GPT 4. At the time of recording, you have to join a waiting list to get your hands on the GPT 4 API. Now I'm sure that will change in time. But if you haven't got GPT 4 API access now, you can click on this slide. It's a link which will take you to the waiting list sign up page. Now if you don't have access to the GPT 4 API yet, no problem. Everything we do will work with the GPT 3.5 Turbo model. And this is also a very, very capable model. So just take a note of that right now if you don't have the GPT 4 API yet. And then wherever in the course we use GPT 4, you can use GPT 3.5 Turbo instead. And then when you get the GPT 4 API access, you can just swap out GPT 3.5 Turbo for GPT 4 as I'll be using in this project. So what exactly are we going to study? Well, we'll take our knowledge from the previous project and we're going to add in the chatbot specific syntax used by GPT 4 and the GPT 3.5 Turbo models. The syntax is exactly the same and it will work with both models just fine. We'll also look at how you can instruct the chatbot to behave in a particular way or have a specific personality. And then we'll look at something called presence penalty, which can control how likely the chatbot is to talk about new topics. And we'll also look at something called frequency penalty, which can control how repetitive the chatbot is in its choice of words and phrases. And finally, we'll change direction and look at how we can store our conversation in a database so it persists even if a user reloads the page or closes their browser. Now, I just want to give you a quick warning again that at the moment our API key is visible on the front end. So be sure to keep it safe, don't share it, and make sure you ignore the end file if you're publishing to a repo. OK, that's enough chat from me. I've already got some HTML and CSS prepared for this project. So let's take a look at that and then get this chatbot working. Let's take a look at the code we've already got. We've got a form component right here and a button, which inside a form component will act as a submit button. Now, this div up here, this is where the conversation unfolds. And in it, we've got this one hard coded message, how can I help you? And that is the message that we can see right here. Now, over in index.js, we've got exactly the same setup as before. These lines of code right here are the open AI setup, just like they were in the previous project. And remember, in terms of the API key that we're bringing in from this.m file, we will be looking at deploying projects with API keys hidden towards the end of the course. And always be sure not to expose your API key when you're deploying projects, sharing them, or publishing them to public repositories. Now here, we've taken control of the chatbot conversation div. And we can see that right here in the HTML. This is the ID right here. So it's referring to this div. And that is the div where the conversation takes place. Now, we've got that stored in this const. And I've declared that globally, as we'll need to use it in multiple functions. Now, moving on down, we've got this event listener. And it is listening out for any submit event. And that will be triggered by a user clicking on the button or pressing Enter to submit the form. When a submit event is detected, we build a new element with create element. We then add the necessary CSS classes and append it to the chatbot conversation div. Then we populate it with whatever the user has inputted. And at that point, we can go ahead and clear the input field. Now, this line of code right here just moves the dialogue down so the latest messages are always in view. Else, as the conversation grows, you'd keep having to scroll down manually. Now, this is quite a neat way of doing it. But you could also use scroll into view. But I found when you're running a mini browser in a big browser, that can sometimes fail. So I went for this approach. Finally, we have this render typewriter text function. As the name suggests, it will give a typewriter effect to the AI's text as it is rendered. Now, there are loads of ways to do typewriter text. Some you can do just with CSS. But I've gone for a JavaScript approach. And what happens in this function is, again, we create a new element. We add the necessary CSS classes. This one includes a blinking cursor class, which would just give a nice blinking cursor effect to the text as it's rendered. And in fact, you can see the CSS for that right at the end of the CSS file. We've got the animation right there. The speed at which each individual character is rendered is controlled by this set interval. And at the moment, I've got it set to 50 milliseconds. And you can, of course, change that if you would prefer a different speed. Now, we won't be working much with the rest of the CSS in this file. But do pause and check it out if you would like to. There's nothing particularly special going on. But we have got a bit of CSS grid up here where we deal with the chatbot header. So the CSS grid is just used for this layout up here. OK, so that is what we have so far. Let's move on and take an overview of how the AI is going to work in this project. It's a little bit different to what we've done so far. So let's look at that next. OK, let's take an overview of how the GPT4 model works with chatbots. So we're going to use one function to make requests to the API. And in a conversation, we'll use that function multiple times. But we need to think about what we send in the prompt because there's actually a big problem for chatbots that we have to overcome. And I want to illustrate that for you right here. So what I've got here is a simple call to the API using the create completion endpoint and the text DaVinci 003 model, which is the model we used in the previous project. And I chose it here because it should look pretty familiar by now. And I can use it to illustrate the point that I want to make about chatbots without getting caught up in new syntax we haven't studied yet. Now, I'm going to come in here to this prompt. And I'm going to ask a question. Where were the 2001 Wimbledon tennis championships held? OK, let's call that function. And we'll hit Save and open up the console. And we get the answer. The 2001 Wimbledon tennis championships were held at the All England Lawn Tennis and Croquet Club in Wimbledon, London, England. OK, so it's a correct answer. Now let's ask it who won that year. And I'll hit Save. And it tells us the Philadelphia 76ers won the 1982 NBA championship. And this is what's called a hallucination. The AI makes up a linguistically plausible answer when it doesn't know the right answer. And we'll talk more about hallucinations later in this course. Now, it figures that it doesn't know the answer. The question who won it that year only makes sense in the context of the previous question, which had the keywords Wimbledon and 2001. And that exposes a big problem for chatbots, which is that models have no memory of past completions. And that means that all relevant information must be sent with each API request. So I need to refactor my second question to include all of the information that the model needs to know, i.e. who won Wimbledon in 2001. Let's run that. And it says Goran Ivanicevic won Wimbledon in 2001. And that is true, with the only problem being that it's assumed I was talking about the men's championships. The women's championships were won by Venus Williams that year, and she beat the Belgian Justine Ennan in three sets. So there's just a little example of the biases picked up by AI models as they're fed data from the open internet. OK, so when it comes to chatbots, in order for the model to interact properly so the conversation flows and remains logical, the model needs to know the context of the conversation. And we achieve that by sending the conversation as it exists so far with each request. So let's look at a diagram of how the main AI business logic is going to work with this chatbot. So the first thing that we'll need to do is store the conversation in an array. And that is what's represented here by this box. And this does have to be an array. The GPT4 model and the endpoint we're going to be using have much stricter rules on syntax than the text DaVinci 003 model and the Create Chat completion endpoint. Now, once we've set up this array, we'll use some special OpenAI syntax to instruct the chatbot and tell it how we want it to behave. And then we'll store that instruction in an object at the first index of the array. And I've just put the curly braces either side of instructions just to make it clear that that will be an object and that this is an array of objects. So now we've got the instructions at the first index of the array. We will take some input from the user. And the first thing to do with that input is to render it to the DOM. Then we'll use some OpenAI syntax to format it. And again, we'll save it in an object in conversation array. Once that's done, we're going to send the entire conversation array off to the OpenAI API. And what we'll get back, of course, is the response. And what we'll need to do then is render the completion to the DOM and then place the completion in an object with the correct syntax that we'll be discussing shortly. And then we'll be adding it to conversation array. So right now, conversation array has got three objects. We've got the instructions, the user's input, and the first response from the API. And then as you can imagine, this process continues. We take the user's reply. We render it to the DOM. We place it in an object. We store that object in conversation array. And then we send it off to the API. We get back our response. And we render it to the DOM, place it in an object, and then store the object in conversation array. And that process continues with the user's response. And we just keep repeating as needed. And this can go on and on until you reach the token limit. But the token limit with the GPT4 model is extremely generous. So it would have to be a very, very long conversation, indeed, to reach that limit. OK, that's the overview. Let's get to work. OK, so we need a place to store our conversation. This is going to be the single source of truth, as it were. The place where everything we ask of openAI and everything it replies to us is stored. So I'm going to come in here. And I'm going to set up a const called conversation array. And in fact, I'll abbreviate array to just R. And remember, this is going to store an array because the API needs to have this conversation as an array of objects. So let's look at the first object that we need. It's the object that holds the instruction. This is where we tell the chatbot how we want it to behave. Now, this instruction object consists of two key value pairs. And that will be the same for all of the objects in this array. They're going to follow a similar pattern. The first key will be role. And this should correspond to the value system. This is just telling openAI that what comes next is the instruction and not part of the conversation. The second key will be content. And this should correspond to a string with an instruction. And this is your chance to influence how the AI behaves and responds. So I'm going to come in here. And I'll set up the first object in this array. So the first key will be role. And we'll set that to the string system. And the second key will be content. And this will hold our instruction. And for now, I'm going to put something fairly mainstream. I've gone for you are a highly knowledgeable assistant. That is always happy to help. Now, later on in the project, when the chatbot is up and running, we will actually revisit this instruction and have some fun with it. But for now, let's keep things simple. OK, so we've got the conversation array and the instruction object set up. Now, let's deal with the user's inputted text, which we'll also be adding to this conversation array. So let's come on to that next. OK, so we have the instructions object in the array. And now we need to add the user's input. So in index.js, we've got this event listener listening out for a submit. And when it detects the submit, the anonymous function takes the user's input and saves it to this const user input. So what we need to do is take that incoming text and put it into an object and push it to conversation array. Now, we've already seen how the chatgpt model wants its objects. And we've got an example of it right here. We've got an object with two key value pairs, role and content. And what we're going to do next is actually very, very similar. The object is going to have two key value pairs. The first key will be role. And this should correspond to the value user. The second key will be content. And this should correspond to a string holding whatever the user has inputted. So right here inside the event listener's anonymous function, I have written you a challenge. I want you to push an object holding the user's input to conversation array. And then just log out conversation array to check it's worked. Now, I'm going to leave this slide right here so you know exactly what that object should hold. OK, pause now, get that sorted, and I'll see you back here in just a moment. OK, so hopefully, you managed to do that just fine. So I'm going to come in here, and I'm going to say conversation array, and I'll use.push. And we're going to push an object. And we know the first key will be role. And that will have the value of the string user. The second key will be content. And that will hold whatever object the user has inputted. OK, now let's log out conversation array and see if it's worked. So I'll open up the console, and I'll ask a question. And there we are. We are logging out conversation array. And you can see that we've got two objects. The first object has got the role of system and the content with our instruction. The second object has got the role of user and the content, what is your name, which is the question that I'm going to use. What is your name, which is the question that I just asked. OK, so that is perfect. We have got the conversation array growing. We've got the instructions object. We've got the user's inputted text. And the next step of the process is to send it all off to the OpenAI API. So in the next screen, let's look at how we actually use the Create Chat Completions endpoint, which is just a little bit different to the previous endpoints we've used. OK, so we've got our conversation so far, and it consists of two objects, the instructions object and the user input object. So let's work on sending them off to the API. So back in index.js then, when the event listener detects a submit, let's call a function called fetch reply. And I'll come down here and set the function up. And because this function will be called fetch, and because this function will be calling the API, this will be an async function. Next, let's store the response to a const and await the API call. Now, so far at this point, we've been using the create completion and create image endpoints. But now, we're going to use something different. Let's head over to the API docs and see what we can find. And this slide is, of course, a link to those docs. What I'm looking at here is the API reference for create chat completion. And there's a ton of info on this page, and we will be investigating it more soon. But I want to focus in on the code example they give us right here, and specifically this. It says openai.createchatcompletion. OK, let's go back to the code, and we're going to add create chat completion right here. And just like with the other endpoints, we're going to pass it an object. So in the next grim, let's head back to the docs and investigate what information this endpoint needs from us. OK, so let's complete the object we're going to send to the API. If we go back to the docs, we can see that two properties are required. They are the model and messages. Now, so far for the model, we've been using text da Vinci 003, which is a very capable model. But now we're going to use GPT4. GPT4 is the newest and most impressive openai model yet. It makes huge improvements on its predecessors, and that is why everyone's been talking about it. Now, if you're looking at these docs and thinking, wait there, it says GPT 3.5 Turbo here. Yes, the GPT4 model is fresh out at the time of recording, and these docs need updating. So if you click through to these docs, you might find they look a bit different. Now, let's just click through to this endpoint compatibility table. We can see that GPT4 is listed under chat completions. So this is going to work just fine. Now, for the messages property, slightly confusingly, the example they've given here is a bit of a simplification. So I'm going to ignore that and click through to chat format. Here we can see the format that it wants, and hopefully that looks familiar. It's an array with objects. And each object has got two key value pairs, role and content. And this is exactly what we have in conversation array. We've got an array of objects, and each object has got two key value pairs with role and content. And if we look closer here, we can see that the first object has the role of system, and the content is an instruction. So in the object we sent to the API, our messages property just needs to hold conversation array. So just to recap, the object we sent to the API will have two properties, model, which is GPT4, and messages, which should hold conversation array. And you can think of messages as being a replacement for the prompt property that we used in the previous project. OK, so let's do this as a challenge. And we're working here inside fetch reply and inside the object that we're going to send to the API. I want you to give this object a model property of GPT4, give it a messages property, which should hold our conversation array, and then ask a question, hit Send, and log out the response to see if it works. Now, just before you do that, you might have noticed that I'm not nudging you to include a max tokens property. In the past, we found that max tokens defaulted to 16. That was when we were using the text DaVinci 003 model on the create chat completions end point. Things are different. Because we're building up the conversation and sending it with every request, trying to define a single max tokens figure is impossible. And in fact, the default with GPT4 is much higher anyway, so it's not going to cause a problem. OK, code up this object, and then we'll have a look together. OK, so hopefully you got that working just fine. So this should be quite straightforward. The model is GPT4, and of course, that is in a string. And the message is our conversation array. Let's just come down here and log out the response. And I'll hit Save, and I'm going to ask it a question. I've asked, what is the capital of Tunisia? And we've got a response, and I'm just going to copy that response and paste it into the editor so we can see it really clearly. We've got loads of info here, just like before, but the important bit is right here with the content. The capital of Tunisia is Tunis. So we have successfully made our first request to the GPT4 model. OK, next we need to use this response in two ways. We need to update the DOM, and we need to update the conversation array. So let's look at that next. Next, we need to update the DOM, and we need to update conversation array. So we're going to go straight into a challenge, and I've got the challenge right here. We're working inside the fetch reply function. So I want you to pass the completion to the render typewriter text function so it updates the DOM. And we've got the render typewriter text function right here, and it takes in a parameter. Now once you've done that, push an object to conversation array. This object should have a similar format to the object we pushed in line 21, but the role should hold the string assistant, and the content should hold the completion we get back from the API. Let's just have a quick look at line 21. Here it is. It's actually line 22. We're pushing this object. It's got the role of user, and the content is the user input dot value. So the object that we pushed to conversation array is going to have a very similar format. Now once you've done that, you can log out conversation array to check it's working. And of course, you should see the DOM updated by the render typewriter text function. Now I've given you a big hint here. To save yourself some time and work, have a close look at the response before tackling number two. And I've actually left the response that we logged out in the last scrim right here, because that will help you. OK. Pause now, get this challenge sorted, and I'll see you back here in just a moment. OK, hopefully you got that working just fine. So I'm going to come in here, and I'll call the render typewriter text function. And what do we need to pass it? Well, if we come down to this response, we can see that we've got the actual completion right here. And we can access that by saying response dot data. And we want what we've got at the zero index of that array. And then we want to access the content in that object. So quite a convoluted line of code. But let's just write all of that in here. So it's response dot data dot choices. Then we want the element at position zero. Then we want to access the message property. And then the content. OK, next let's push an object to conversation array. Now luckily, and it's what I was getting at with this hint, I don't actually need to build an object. If we have a look again down at the response, let's see what we've got right here. Well, we've got an object, and it's got two key value pairs, role and content. The role is assistant, and the content is the completion. So that is exactly what we want already formatted. So I hope you noticed that, and you didn't extract the text and then write the code to build a new object. OK, so back up here, we're taking almost everything that we've got here. But because we want the whole object, we'll take away the dot content. And now let's just log out conversation array. I'll hit Save, and let's ask a question. What is the currency of Peru? And we've got an answer. The currency of Peru is the Peruvian soul. That's great. Fantastic answer. And if we open up the console and have a look, we can see conversation array. And it's got three objects in it, the instruction, the user inputted text, and now the object with the completion we've just got back from the API. OK, so our chatbot is essentially working. Now, I just want to ask it a couple of questions. What I'm keen to see is, is it keeping the context of the conversation? Does it have a memory that it's getting from our conversation array? So I'm going to ask it for pi. OK, and it's given me a very long answer. All I really wanted was the number 3.14159. But that's good, because now I can check its memory. I'm going to say, give it to me to three decimal places. And what's interesting to see here is, does it understand it to be pi, which we were just talking about? If it does, it's got a memory, and it's keeping the context of the conversation. Let's find out. And there we are, pi to three decimal places is 3.142. So it has a memory, and that is exactly what we want from a chatbot. OK, so we've pretty much nailed the chatbot, or at least its basics. So let's take a look at a few more tweaks we can make to really level up our chatbot skills. Now, one danger you can run into with chatbots is that they can get repetitive. No one likes a conversation with someone that just says the same thing over and over. So let's take a look at how we can deal with that next. At the moment, we're only using two properties when we make a request to the API. We're using the model and the messages. And they are both required. Now, in the previous project, we used max tokens, which, as I said, is not so helpful here. And we also use temperature, which you're welcome to add if you think your chatbot is not performing optimally. You can just go ahead and add it right here. And just like with the DaVinci model, it defaults to one. Now, I don't think we need to change the temperature on this, so I'm actually going to delete that. What I do want to look at, though, is two settings, frequency penalty and presence penalty. And what they do is offer some control over how repetitive our output is, because we want the language to sound natural, and we don't want to find ourselves saying, can you stop saying that? Now, we're going to look at these two settings together as they are similar and they're easy to confuse. So let's take presence penalty first. Presence penalty will be a number from minus two to two, and you can change it in increments of 0.01. It defaults to zero, so that is what we're using now because it's unset. At higher numbers, presence penalty increases the model's likelihood of talking about new topics. So what does that mean in the real world? Well, let's imagine a conversation between two friends, and this conversation is taking place at low presence penalty. So one friend says, hey, give me some good news. And the other friend says, Manchester United won six nil. It was the best game ever. I've never seen Real Madrid fans looking so unhappy. Manchester United are the best. Let me tell you about the game in detail. Hmm, we all know someone like that, right? Now, what would happen to this conversation if we could flip it to a high presence penalty? The first friend says, hey, give me some good news. And the reply is, well, my team won on Saturday. My investments are doing well. My brother's out of hospital, the sun's shining, and I'm getting married next June. So you can see that instead of obsessing over Manchester United, they're actually talking about more topics. Okay, let's compare that to frequency penalty. The settings are very similar. It's a number from minus two to two, and you can change it in increments of 0.01. It also defaults to zero. At higher numbers, it decreases the model's likelihood of repeating the exact same phrases. Okay, let's again imagine a conversation between two friends, and this will take place at low frequency penalty. So the first friend says, hey, how was your week? And the reply is, I went to a literally unbelievable party. There were literally millions of people trying to get in. Brad Pitt was there, and I spent literally the whole evening with him. Me and Brad are literally best friends now. And we've all met that person, right? The one who says literally a lot, or basically, or you know what I mean. Okay, if we repeat that at a high frequency penalty, hey, how was your week? I went to an amazing party. There were literally thousands of people trying to get in. Brad Pitt was there, and I spent the whole evening with him. Me and Brad are best friends now. Same unbelievable story, but the word literally has been used only once. So the frequency penalty will allow the word to be used, but it will stop it being overused. Okay, that's the theory. In the next scrim, let's get practical. Okay, so the conversation we looked at in the previous scrim was of course very contrived. Actually, presence penalty is a very subtle setting, and it can be hard to see it in action. In fact, it's only really noticeable when you're generating large amounts of text. So rather than watch me churning out loads of text, you're going to have to do some investigation on your own. So I'm going to come in here, and I'm going to set presence penalty to zero. And that is, of course, its default setting. And now I want you to spend some time experimenting. So you could just have a general chat with the chatbot at different presence penalty settings, but it's likely better if you ask it something that will generate a long answer. It's only then that you'll really see presence penalty in action. So here are some ideas. If you're a glass half full kind of person, you could ask it to tell you what is great and wonderful about the world. And if you're a glass half empty person, you could ask it to tell you what is wrong and terrible about the world. Either way, use the same prompt two or more times, but with different presence penalty settings and see what you get. And if you don't notice too much, don't worry. This is a subtle setting. And at least now you know it's there. So if you do have problems in this area down the line, you'll know what to do. Okay, take some time to do that now. And then when you're ready in the next grim, we'll look at frequency penalty. Okay, let's do some experimentation together with frequency penalty. And let's remind ourselves first what frequency penalty is supposed to do. We use a high frequency penalty to decrease the model's chances of repeating the same phrases. So what I'm going to do is set you a challenge which will involve generating some text which is likely to have some repeated words and phrases. Now, because we're going to generate a lot of text and it will actually be pretty painful to watch the render typewriter text function trundle through all of it, I've actually commented out that function call right here and I've replaced it with this console.log so we can just log out the completion. Now I've also put a file up here called output.md and we can paste our completions in there and save them for comparison. So here is your challenge. I want you to set the frequency penalty to zero. Give the chatbot this query, generate 20 ways to say you can't buy that because you're broke. Paste the results into output.md and then repeat that process with frequency penalty set to two. Once you've done that, you can examine the differences between the two outputs and see what frequency penalty is doing. Now I've just put a warning here, do not set frequency penalty to minus two and I've said that because it actually breaks things quite spectacularly, it will just churn out the same word again and again and again until all of the tokens it's allowed have been used up and in fact, it will probably crash the mini browser. Okay, pause now, have a go at that challenge. I'm going to go through the same process and we'll have a look at the two completions and compare them. Okay, so hopefully you managed to do that just fine. So I'm just going to quickly go through that process and paste my completions into output.md. Okay, so I've got my two outputs and if we compare them side by side, well, they both start okay. So we've got at frequency penalty zero, your financial situation doesn't permit you to make that purchase and then unfortunately, your funds are insufficient for buying that item. Both of those are fine, both great English. Let's have a look at the first two that we've got when frequency penalty was set to two. Unfortunately, your current financial situation doesn't allow for that purchase. It seems your wallet is feeling a bit light to acquire that item. Okay, a little bit quirky, but no problem. Now let's come to the very end because it's the last part that's going to be most difficult when the frequency penalty is high. Now number 20 here was with your financial setting, it's infeasible to add that to your possessions. Well, that's okay, it's correct English. It's a little bit strange. Now down here, we've got monetary scarcity dictates frugality and discipline despite passions give merit where it's due, nullifying all impulses towards luxury, at least for now. I can't decide if that's poetry or gibberish. I think it's just gibberish. And actually, if we look a little bit further up, we can see that by setting the frequency penalty high, we've really caused open AI problems because this is the first time that we're actually seeing bad English. We're actually getting completions that don't seem like they come from a human. Take for example, number 17. It's apparent after analyzing existing revenue streams, financing the stream can simply just about only be squeezed from another worldly dimension entirely. Don't know what that means. Or number 18, empty pockets rarely find footing when chasing certain ambitions. In our material world, might as well try catching stardust instead. Again, kind of poetic, but these weird little mistakes as well, there's a comma there, but no space. Now, one other thing I want to show you is this. I'm just going to highlight every instance of the word financial. Now we can see if I scroll down, that word appears seven times, but five of those times are when frequency penalty was set to zero and only two of them were when it was set to two. So that shows us that the frequency penalty is penalizing the word financial. It's only letting it appear twice. And when we're talking about money, it is quite an important word. Let's do the same thing with the word purchase. Again, the word purchase is used seven times, five times with frequency penalty at zero and just twice with frequency penalty set to two. So we can really see why the model is struggling to generate new language. Okay, so what is the bottom line? Well, here is my general advice for presence penalty and frequency penalty. And I'm going to express this as a flow chart. Firstly, ask yourself, is there a problem? If the answer is no, do nothing. If the answer is yes, then my advice is this. Don't go over one for either setting. Don't go under one for either setting. You can experiment outside of those parameters, but I would recommend staying inside them else you run the risk of getting some pretty strange results. And lastly, it's all about making small changes and testing. If you just make a small change and test, you'll very quickly find the settings which work for you and get you the results you want. So what are we going to do in this app? Well, I'm going to leave presence penalty at zero. I don't think it's doing too much for us anyway. And frequency penalty, I'm going to put to 0.3. And I'm going to do that just because I've had to play around with this for a long time. And I think that is where we get the best results. You of course are free to differ. Okay, so the chatbot is working pretty well. So next, I want to go back to where we started to this instruction right here and have a bit of fun with it and just see how we can alter the chatbots personality and why that might be useful. When you're ready for that, let's move on. Let's have some fun and change the chatbots personality. So here's a challenge for you. Update the content properties value to change the chatbots personality. So you just need to alter this string right here. You could ask it to be cheeky or funny or talk in rhyme or go for something practical. Maybe you want a shorter answer or even a one word answer. I'll leave that up to you. Pause now and have a go. Okay, hopefully you managed to get a good effect. Now I'm British, we love sarcasm. So I'm going to make this a sarcastic chatbot. And let me ask it something. And there we are, it's had a pretty good effect. It says you should definitely go to the most boring place on earth, your local park. Why explore beautiful beaches, breathtaking mountain landscapes or exotic cities with rich culture when you could just sit on a bench and watch the grass grow. Sounds like a dream vacation. Okay, that's pretty sarcastic and I like it. But actually there is a more serious side to this as well because we can actually get the chatbot to behave exactly as we want it to. Perhaps you want a chatbot that's going to interact with children, perhaps you want a chatbot that's going to interact with people for whom English or whichever language you're working in is not their first language. So you might want it to simplify the language a little bit. And perhaps you don't want such long answers. We have actually got some quite long answers so far. So I might change this to keep the answers short. And I'll ask it a big question. What is quantum computing? And look at that, the most concise definition you'll ever get, advanced computing using quantum bits. Okay, so that's good to know. You can control the chatbots personality and it really is that simple. Okay, so we're pretty much done with the mechanics of this chatbot. What I want to do next is use a Google Firebase database so we can persist the chats even when we refresh and reload. Let's do that next. Okay, so we're going to add a Google Firebase database to this project so we can persist the chat. Now, what does that mean? Well, let's check our specific aims. We want to persist the chat so that a user can pick up where they left off after a refresh or reload. So the conversation will be stored in the database and the user can close the page, turn off their laptop and come back and continue the conversation at a later date. We want to give the user the ability to reset the chat so they should be able to delete the conversation and start a fresh conversation from the beginning. So how is this going to work? Let's take a look at a diagram. At the moment, all of our code is on the front end and we've got an array holding our conversation. We send that off to open AI and it sends us back a response. What we're going to do now is remove the array where we store the conversation on the front end. We're going to create a database and we're going to store the conversation array in that database. When we need it, we'll bring it down to the front end and use it when we make calls to the open AI API. To start this off, we need a Google Firebase account and we need to set up a database. So let's start doing that next. We need a Google Firebase account. So head over to the homepage and actually the image on this slide is a link. Click on it and it's going to take you straight there. When you're there, click on Get Started and that is going to take you to a signup page. And once you've entered your details and set up your account, you'll end up on a page looking like this. Click Create Project and here we need to give the project a name. I'm going for know it all dash open AI but of course you can call yours whatever you want. And now it's going to ask us if we want Google Analytics and for this project, I'm going to turn it off. Now hit Create Project and it will take a few moments to set things up. And when it's ready, you can just click Continue. Now we've created our project, go over to Build and then from the dropdown menu, select Realtime Database. Click Next, click Create Database. But if you're interested in finding out a little bit about the different types of database that Firebase offers, then do click this link and that will give you a little bit more info. For this project, we're using the Realtime Database which is the easier one to work with. So create database and then in the popup, select the server that's nearest to you. I'm in the UK, so my nearest is Belgium. Click Next and it's going to offer us locked mode or test mode. I'm going for test mode and that does mean that anybody with my database reference will be able to view, edit or delete any of my data. Now that's fine for the purposes of this course. By the time this gets published, I will have locked this down. I recommend starting in test mode. It's just a little bit easier to not have to deal with the security rules at the beginning. You can always read up on that later and make some changes to the security rules if necessary. And there we are, we have our database. And the most important thing here is this URL. That is the URL we'll be using to communicate with our database. So let me just zoom in on that. There it is. I'm going to click here to copy it and then in our code, I'm just going to create a little comment here and paste it in. And we'll use that in the next grim when we start adding the Firebase setup to this code right here in index.js. We need to add the Firebase dependency. So I'm going to come over here to the threedot menu. And when I click on that, a menu appears and you can't actually see that in the recording, but I'm going to select add dependency and a popup appears and I'm just going to type in Firebase. And I'll click add and we can see that the Firebase dependency has appeared over here on the lefthand side. Now, if you're working locally, you can install this via MPM and you can check out the Firebase docs on that right here. Again, this screenshot is a link just click it to go straight through to those docs. And by the way, you can also use a CDN to work with Firebase if you prefer to do it that way. Okay, now we've got the Firebase dependency setup. We need to import some methods. So I'm going to come right up here to the top and we'll say import and then inside curly braces, I'm going to say initialize app. And we're importing that from Firebase slash app. Next, I'm going to import get database and ref. And these are coming from Firebase slash database. Okay, that's all the methods that we need. Now I'm going to come down here and set up a const called app settings. And this is going to hold an object and it's going to have one key value pair. Database URL is the key and the value will be a string containing the URL that we got when we set up the database. And we can delete that comment now. Underneath app settings, I'm going to set up another const and this one will be database. And here we'll use one of the methods that we've just imported, get database. Next, I'm going to come down here and set up a const app. And now we will use one of the methods that we just imported. So I'm going to say initialize app and I'll pass in the app settings. Now we need a const database. And again, we'll use one of the methods we just imported, get database. And we'll pass in app. And finally, we'll set up a const called conversation in database. And I've just shortened database to DB. And we'll set that equals to ref and we'll pass in database. So what we've just done there is set up a const called conversation in DB and it stores a reference to our database. And that's really important because now conversation in DB is going to be our single source of truth for the conversation we have with the chatbot. Okay, those are all the basic settings we need for now. We will add a few more methods later as we go on. Now, before we do anything else, we do need to make a few changes to the HTML and CSS. I want to add a clear button right here. So I'm going to go over to index.html and I'll come in here and just paste in the clear button. And I'll hit save and of course we've broken the CSS. So let's head over to index.css and we need to make a couple of changes. So firstly, I'm going to come down here and paste in some CSS for the new button. And as soon as I do that, you can see that things start to change over here. Now I've used the selector clearbtn and if we have a quick look back at index.html, the button has already got that class added. And the most important part of this CSS is actually this property right here, grid area clearbtn. And we're using that because the layout up here uses CSS grid. So if we come up here to the chatbot header selector, this is where we're setting display grid and we've got the grid template areas right here. And I'm going to change this dot for clearbtn. And now things are starting to come into alignment. So clearbtn is the grid area that we've got on the clearbtn selector. Now we've still got a flat edge to this button and that's because at the moment down on line 137, we are currently selecting all buttons and we've got styles set up for this submit button that we've got right here. And the submit button has a border left of zero. Now, if we go back to index.html, we've got the submit button right here and it's actually already got a CSS class of submitbtn. So how about we come over here and we replace this with the more specific selector. Okay, that has had the effect of giving us a full rounded button, which is what we want right here. Now all we need to do is bring this into alignment by coming up to the support ID selector and taking this property right here, text align right and changing it to center. Okay, now we've got it looking how we want it and we're good to move on. Next, we need to figure out how to push our user's input to the database. Right here, we're pushing our user's input to conversation array, but now we want to store the user's input in the database. That is going to be the single source of truth. So let's use Firebase's push method to push it to the database instead. To get access to the push method, we need to add it to the list of imports. So I'll come up here and add it in right there. And then back down in the event listener, let's come in here and we're going to delete conversation array. And now we've got push and that is no longer the standard JavaScript array method. That is now the Firebase push method. And what we need to pass it is the reference of the database we want to add to. And the reference to that database we've got up here, it is conversation in DB. So we'll say push and then in brackets, conversation in DB, a comma, and then the object which we want to push. And that is of course, exactly the same object. And it's as easy as that. So now let's go ahead and type something in here and we'll hit send. And let's take a look at what we've got in the database. And there we are, it has appeared. If you log into your real time database, you'll pretty much instantly see exactly what we've just pushed to the database. So we've got the content, hello, know it all, and the role of user. So that is our object. Now doing that leaves conversation array looking pretty redundant. We don't want to store our conversation here anymore, but that leaves us with an issue. We've got this instruction object and we need to keep it somewhere. But storing it in the database with the rest of the conversation has actually got two disadvantages. Number one, if we ever want to edit the instruction to change the chatbots personality or behavior, we'll actually have to edit the database directly. And number two, when we want to clear the conversation, we're going to get caught up in JavaScript spaghetti making sure we don't delete the instruction, but do delete everything else. So we'll be making work for ourselves. The solution that I think is best is that we keep the instruction right here in index.js where it's easily accessible and easily editable, and we just add it to the array we're sending to OpenAI with each request. So what I'm going to do is change conversation array to instruction object. And now it's an object, let's delete the square brackets. And when we update fetch reply, we'll add this to the array we send off to the OpenAI API. And actually updating fetch reply is what we need to come on to next. The next thing we need to do is make some changes to fetch reply. We know we need to send the entire conversation with every request to the OpenAI API. But now that the conversation's stored on the database, we need to fetch it before we can include it with our request. And with Firebase, we do that with the get method. But before we can use the get method, we need to add it to our list of imports. So let's come up here and we'll just put it after push. Now get is a method, so we'll give it the brackets and we're going to pass in the reference to our database, just like we did with the push method. Now we're going to chain a then and here we'll have a callback function and that function gets a parameter snapshot. And snapshot is the data in our database as it exists at this time. Now inside the body of this function, let's be safe and use an if just to make sure a snapshot exists. So if, for example, writing to the database right here in the event listener failed, then we would want to know about that. So let's just say else, no data available. Because if what we send to the OpenAI API is not the array of objects it expects, it's going to give us an error anyway. So it's a good idea to check what's going on here. Now in here, we can make our request to the OpenAI API. So let's just cut and paste all of this code. And let's just format that nicely. Now we're actually not quite ready to call the API. So I'm actually going to comment out all of that code. And let's just come in here and log out snapshot. Now remember, we've already got something in our database. So there should be something for us to log out. So rather than write anything more in here and add to the database, I'm just going to call the fetch reply function. Now when I hit save, let's just see what we log out. Well, there we are. We can see the contents of our database. We've got an object. It's got a content key with hello know it all and a role key with user. But at the beginning, it's also got a Firebase identifier of some kind. And we don't want to be sending that to OpenAI. It would just really confuse it. All we want is an array of objects. So let's check what the Firebase docs say. And they talk about this val method and it says here, it extracts a JavaScript value from a data snapshot. Well, that is exactly what we want to do. Now, if you'd like to read up more about val in the Firebase docs, this screenshot is a link. So just click on it and it will take you straight there. So I'm going to come in here and where we've got snapshot, I'm going to call val. And let's just save and see what happens. Okay, so on its own, it doesn't make any difference. We've still got Firebase's identifier and then our object. So let's just check something else we can use from MDN. And again, this screenshot is a link to this page on MDN. Now object.values, let's see what it says here. It says the object.values static method returns an array of a given object's own innumerable string keyed property values. Okay, that sounds like a bit of a mouthful, but the important thing here is it returns an array and an array is what we want. So let's just experiment with it and see what we get. So I'll come in here and say objects.values and then in brackets, we'll have our snapshot on which we've called the val method. Okay, let's hit save. And look at that, down in the console, we have got an array and it's got our object in it. And that is the exact correct format that we need to send to the OpenAI API. But we're not quite ready to send it yet. We need to think about the instruction. So let's do a little bit more work in fetch reply in the next scrim. We need to get the array that we can see in the console off to the OpenAI API. So I'm going to come in here and resurrect the old conversation array. And we'll use it to store the array we get back from the database. And then if I uncomment all of this code, we're now going to send conversation array off to the OpenAI API with each request. Now to check it's working, I'm going to log out the response and I'll just take this opportunity to delete the console.log and also the function called to fetch reply that we were just using as a test. And now we can give this a try, but wait a second, we've actually forgotten about our instruction object. We need that in the conversation that we send to the OpenAI API. So we need to include it in conversation array. And in fact, you can do that as a challenge. So I'm just going to come in here and paste the challenge right there because that is where you'll need to write some code. And I've just said, add instruction object to the front of conversation array. But I've put a warning here, you're going to get an error when you do this. Try to debug it. Okay, we're already logging out the response. So pause now, get this challenge sorted and I'll see you back here in just a minute. Okay, hopefully you got it working. So we can do this with unshift. So we'll say conversation array dot unshift and then we'll pass in the instruction object. Okay, now let's hit save and we'll give it a try. Oh, but we're getting an error. And I did warn you that you would. Now it's saying unexpected reserved word. So what could be the problem here? Well, we're using await here and we can only use await inside an async function. And that's what we're doing, isn't it? We've got async right here. Well, actually no, because this await is inside the callback function that we've got right here. So that is the function that should be async. So let's delete async from here and add it in right here. Okay, let's save and try again. And there we are, it's working. So the final thing that we need to do with this function is uncomment these two lines of code right here. And in the next grim, let's see what changes we need to make to them. This line of code right here, which sends the completion to render typewriter text is absolutely fine. We don't need to change that. But here, we don't want to update conversation array. Remember, conversation array is now here and it just holds the array we get back from the database when we use this get method. Our single source of truth is on the database. So what we actually need to be doing right here is updating the database. So here is a challenge for you. I want you to add the completion to the database and then ask the chatbot something to check it's working. Now to complete this challenge, you can look back at the code we've already written. And remember, this code here gives us an object which is already formatted in the way we need it. Okay, pause now, get this challenge done and I'll see you back here and we'll have a look together. Okay, hopefully you managed to do that just fine. So we need to use the push method and we need to pass it the database ref, which is conversation in DB. And then we give it the object we want to push and we can do that with this code right here. Okay, let's delete this line of code and we'll save and give it a try. I'll ask it a simple question and it's given me a reply, it's looking good. Let's just see if we've successfully updated the database. So this is what we originally had in the database. We've made a few changes to it since then and now look, we've got my question, what is a Frisbee and the answer, a flying disk. So the code that we just wrote right here is definitely working. Okay, so things are going well, but we've got a bit of an issue. If I just refresh the mini browser, in the database, we have got a whole conversation that I don't know about as a user, it's not shown here. And that's a problem because if I now start typing a question, I'm not aware that I'm actually continuing a conversation and in fact, I might well have forgotten what I wrote before. So that means we've got two more jobs to do. We need to render the existing conversation out when the app loads so the user can see what they've already said. And we also need to wire up this start over button so that a user can reset the conversation whenever they want. So let's start on the first task of rendering out the conversation on load next. We need a function that will render out the conversation as it exists on the database when the app loads so the user can see what has already been discussed. Now I've set you a challenge to do this and it might seem like a big challenge, but actually this function is just going to recycle bits of code we've already written and it's a good opportunity for you to get some more practice with Firebase. So the challenge is this, create a function called render conversation from DB, which will render any existing conversation in the database. Now this function should be called when the app loads. Now take all the time you need to do this. There are a few things to think about and if you need any extra help, I've created a file up here called hint.md and it's got some tips in there that will really help you. Now if you don't want any help, pause now and get started. I'm going to open hint.md so those watching on YouTube can pause it and read it. Okay, good luck with the challenge and when you're ready, I'll show you my way. Okay, hopefully you managed to do that just fine. So let's come down here and set up this function. Now the first thing that we need this function to do is to get the conversation from the database. So we'll use the get method and we'll pass in the reference to the database. We'll chain on then and we know the callback function here will be an async function and it will have the snapshot as a parameter. Inside the body of this function, let's use an if to make sure the snapshot exists because if there is no snapshot, if there's nothing in the database, this function need not do anything. And now we want to get that snapshot as an array of objects and we can do that by saying object.values and we'll pass in snapshot and we'll need the val method. Now we can iterate over that array using a for each and we'll represent each item in the database with database object, which I'll shorten to dbobj. Now for each database object, we want to create a new speech bubble and we can do that with document.createElement and we'll create a div. Now we'll need to add some classes to that div and that goes with every speech bubble is just speech. Now the second CSS class is dependent on whether it's a human or the AI that's speaking. So I'm going to open up the back ticks and both classes start with speech dash and now I'll use dollar sign and curly braces and in here we're going to use a ternary. So I'm going to say database object.role and I'm going to ask if that is triple equals to user. If it is, this CSS class should be speech dash human. And if it's not, it should be speech dash AI. Okay, now we need to append that speech bubble to chatbot conversation, which we took control of right up here on line 23 and apologies in hint.md. I think I said it was line 22, but I'm sure you found it. So let's say chatbot conversation dot append child and the element we want to append is new speech bubble. And lastly, we need to add the text to the speech bubble. Now I did say in hint.md, make sure that a malicious user can't use this to input JavaScript. So hopefully you didn't use inner HTML to do this. Text content is secure because anything which is inputted is just going to be passed as text. It cannot be passed as HTML with script tags and with executable JavaScript inside them. Lastly, we just want to move the conversation down so the latest message is always in view and we've done that several times in the app with scroll top and we're setting chatbot conversation dot scroll top equal to chatbot conversation dot scroll height. Now to check it's working, all we need to do is call it. And if we call it right there, it should run every time the app loads. Let's hit save and see what happens. And there we are, our conversation is rendered. So everything that we've got in the database, we can see it all right here is now rendered automatically as soon as we load. And we can see that the ternary has been successful because we've got different speech bubbles depending on whether it's the AI or the human that has spoken. Okay, that's a really good job. The final task then is to wire up this start over button. Let's do that in the next screen. Okay, so at the moment when the page loads, the conversation as it exists in the database renders automatically. What we want to do now is wire up this start over button so we can clear the conversation. So I'm going to come in here just above the last function we wrote, although it doesn't really matter where we write this code. And I'm going to say document dot get element by ID. And I want to take control of this button. So if we just check in the HTML quickly, we've got that button right here and it's got clear dash BTN. Now I'll add an event listener to listen out for clicks. Now, when a click is detected, we've got a very easy way of clearing the database. And to do that, we use the remove method. Now, of course, before we can use the remove method, we need to import it. So let's add it to the list right here. And all we need to do with remove, and you can probably guess this by now, is pass it the database reference. Now that is going to clear the database, but obviously it's not going to update the HTML. So let's add one more line and I'll say chatbotconversation dot innerHTML is equals two. And then we just need our hard coded message that goes right at the beginning of any conversation. And we can get that from the HTML. Here it is right here. So let's copy it and then we'll paste it here in inverted commas. And I'm just going to bring it all onto one line. And this is a safe use of innerHTML because this is hard coded HTML. The user has no way to access that or update it in any way. Okay, let's give it a try. So I'm going to come over here and I'll just click the start over button and look, everything disappears. We're back to our single hard coded message. Now let's check the database. I've just taken a screenshot of it. So that is how it looked before and now it's empty. So this is working. And if I come in here and I'll actually ask the chatbot, is this a new conversation? And it says yes. I'm going to ask it a couple of questions. Now let's refresh. The conversation is there. It seems to be working. Let's ask it if I've already asked it some questions. And it replies yes. What did I ask you? New convo, how are you questions? Well, yeah, that's a fair summary. I did ask it, is this a new conversation and how are you? And remember, it's giving us very short answers because we did leave the instruction right up here as you're an assistant that gives very short answers. Maybe let's just change that back to, you are a helpful assistant. What have I asked you today? And there we are, it knows everything. I'm going to click start over to clear the conversation. I'll try again, what have I asked you today? And look, it's telling us that it cannot recall any past interactions with us from separate sessions. And that proves that this is working. The start over button is doing exactly what we want it to. Okay, so that is a really good job. Congratulations on getting to the end of this project. Let's just take one last grim to recap what we've done and talk about where we go from here. Congratulations on finishing the knowitall chatbot app. Now you have got the foundations you need to build any human language capable chatbot that you want. Let's just recap what we've studied. We have used the GPT4 model and the create chat completions endpoint. We've given our chatbot a personality via the instruction object. We've seen how we can use the presence penalty setting to encourage the chatbot to talk about new topics. And we've used frequency penalty to control how repetitive the chatbot is when it selects words and phrases to use. We saved our conversation as a single source of truth in a database so the conversation can be persisted and the user can come back to it and pick it up at a later date. Now it's always a good idea to really make a project your own. So why don't you give the bot a specific function? So you could tweak the AI so that it has a specific purpose. This tech doesn't have to be a general purpose chatbot. It could be a coding expert, a poetry generator, an academic assistant. You could use the instruction object to train it to provide text in the specific style and format you need for writing reports at work, for example. The only limit is your imagination. And once you've done that, you could change the style and theme so it matches the chatbot's new purpose. And of course, it's always a good idea to build again from scratch. And if you don't need practice with the HTML, CSS, and JavaScript, you could just rebuild the API specific parts to really get that syntax to stick in your brain. It would also be good to add some error handling. We haven't really looked at error handling in this course. We're focusing very much on the AI. But whenever you're working with APIs, error handling is more than a good idea. It's essential really if you're taking anything to production. So do do a bit of research on that. But whatever you do, remember to take a break. Take a bit of time to consolidate what you've learned before moving on to the next project. Okay, so we are going to enter the fantastic world of fine tuning. And I've put here making AI models work for us. So what exactly do I mean by that? Well, in earlier projects, we have used two types of prompt. We've used the zero shot where we just give an instruction or ask a question. We've also used the few shot approach where we give an instruction and we use examples to demonstrate what we're looking for. Now those work fine for our purposes, but they have two big drawbacks. Firstly, prompts have size limits. We're limited in how much we can include in a prompt. Secondly, larger prompts use more tokens, so will be expensive when scaled. But there's actually a bigger problem than that. OpenAI's models have been trained on text openly available on the internet. Now that's great for when you want to use them for creativity, general Q&A, like we've been doing with our chatbot, translation and many other general tasks as well. But what it's not good for is answering questions that are specific to your circumstances. So consider this, you have a company and that company has specific policies and systems. So you have your own opening hours, you might have shipping fees, a returns policy, of course you'll have contact details and many other things besides. Now imagine you ask a chatbot like the one we just made, a specific question about your company. What are you going to get back? Well, you're going to get hallucinations. And what are hallucinations? Well, if the AI doesn't know the answer, it gives you a linguistically plausible incorrect answer. So it basically goes into the world of fantasy. Now, although they're improving, AI models are not that good at saying, I don't know. Remember, what these models do is predict the likelihood of a token or language chunk coming next. And this is one of the biggest problems with AI when working with facts. So if I ask what my company's opening hours are, it will likely say something like 9 a.m. to 5 p.m. and close on Sundays, just because that is a plausible answer. Now, fine tuning can help with this problem. By uploading your own dataset, you can give the model the information it needs to answer questions specific to your situation. So let's go. And we're going to start by thinking about how we can convert our chatbot, which we still have all of the code for right here, into a finely tuned support bot for my new company. So let's check that out next. Okay, so for this finetuned chatbot, we're going to use a lot of the same HTML and CSS as the previous project. But before we get to work on the AI, let's just make some quick changes to the design so it better fits our purpose. Up here, I've got the new We Wing It logo, which is just dronelogo. So let's just swap that out right here. And because that logo is slightly smaller, we need to just make a quick change to the CSS. Right here on line 54, the logo has got a width of 45 pixels. I'm going to change that to a width of 50 pixels. Okay, that's looking better. Now back in index.html, we need to change the name to We Wing It drones. And the subheader will now be delivery support chat. And to give us a little bit of extra room in the header, I'm just going to bring this down to ID. And let's just update that. It doesn't do anything, it's just for aesthetics. Now I'm also going to change this one hardcoded message. So now it's going to say, how can I help you with your We Wing It drone delivery? And lastly, we should come up here and change the title, even though it doesn't make any difference in the mini browser. Okay, let's save that. And there we are, everything is looking fine. Now the rest of the CSS and HTML is going to stay the same, but under the hood, we're going to completely change the way we use open AI. So this is We Wing It, an accidentprone drone delivery company with some very unhappy customers. Now We Wing It needs a way to communicate with their customers quickly and easily, so they want an AI support bot. Let's just check what exactly we're trying to achieve here. We want a chat bot with the ability to answer questions specific to our company. If the chat bot doesn't know the answer, rather than hallucinate, it should advise the user to email or phone. Okay, let's crack on. Now we know what we're doing and why we're doing it. Let's take a highlevel overview of the AI process to get an idea of how we're going to do it. So how do we get this highly tuned chat bot off the ground? Let's have a quick look at the whole process. Firstly, we need data. This is just like the examples we used in the fewshot prompts, but it's going to be much longer and specifically formatted. So we'll be using the CSV format or the commaseparated values format. It also works in JSON format, but basically at this point, we need to make sure the chat bot has got all of the information it needs to do its job properly. Now secondly, as the finetune process takes a long time to run, we don't want errors. So let's use OpenAI's data preparation tool to process our data so the format is correct and it won't get rejected. Now this is a CLI or command line interface tool, and if you haven't used the command line before, don't worry, we'll go through it step by step. The third thing we need to do is actually upload our data to OpenAI and tell it to make our finetuned model. Again, we do this with a CLI tool. It's very quick to get it going, but it can take a long, long time for the request to be processed. Now when that process ends, we will have our own special endpoint, which we can then use with our chat bot. But at that point, we'll need to think about the changes we need to make to our existing code, because to use the new model, we'll have to make some adjustments. All of the code we've got at the moment is very specific to the GPT4 model we've been using. We'll have to change things up a little bit here. Okay, so that's how it works. Let's make a start on the data. Okay, so we need some data to finetune our model, so let's deal with that next. And the first question is, how much data do you need to finetune a model? Well, the advice from OpenAI is you should provide at least a few hundred highquality examples ideally vetted by human experts. And what's more, OpenAI says increasing the number of examples is usually the best and most reliable way of improving performance. So if you were setting this up in the real world, you would basically grab all of your company's support tickets and customer service emails and anything else relevant that you can lay your hands on. And then as OpenAI recommends, you would have a human check it for accuracy and relevance. Now for the app we're building today, we're going to use a relatively small amount of data, but the principle is exactly the same. That is what I want to show you, the principle of how to finetune data. Now how we format that data is really important. OpenAI wants the data to be in JSONL format and the docs give us an example. Now JSONL is data formatted with JSON on each line. Each line has to be valid JSON in its own right and each line must end with a new line character. Now, if you haven't used JSONL before, don't worry. We won't be writing it ourselves. We'll be using a special tool to create it. So we'll actually be working with much simpler CSV or comma separated values data and we'll let OpenAI's tool do the heavy lifting. Now as well as wanting JSONL format, OpenAI gives us some further criteria for the format of our data. It wants each prompt to end with a separator to show where the prompt ends and the completion begins. It wants each completion to start with a white space and it wants each completion to end with a stop sequence to inform the model where the completion ends. Now the stop sequence is something that we haven't needed yet, but we will talk about it when we get to that point in the project and everything will become clear. To be honest, all of this sounds like a bit of a pain and actually in their example data, they didn't even show us exactly what they wanted with the stop sequences and the separator. So we don't even have that to help us. But good news is right here. You can use our CLI data preparation tool to easily convert your data into this format. So we're going to let that tool do everything for us. So don't worry at all if that looks intimidating. Okay, in the next grim, let's take a look at the data we'll be using. Okay, let's take a look at the data we're going to use. Now writing JSON out by hand is a pain. So I've organized this in a spreadsheet using comma separated values or CSV. All we've got here is two columns, prompt and completion. Each prompt is a question from a customer and each completion is an answer from customer service. So we've just got two columns. I think we can work with that. And I've got 38 pairs of prompts and completions. Ideally, I would have 10 times that number or even more. But this small dataset is going to allow us to see the principle of how fine tuning works. Now I've also pasted the CSV data into this file right here. And it doesn't look like it's formatted, but actually CSV formatting is really simple. We've got the two column headings right here. And then each line contains one prompt completion pair. So this is the prompt right here and this is the completion after the comma. Now I just want to draw your attention to the last few pairs because here I've done something which is just a little bit more complex. It's a little bit tricky to see. So I'm just going to take this last prompt completion pair and space it out a little bit. Now, instead of just having one question and one answer, the prompt actually consists of several parts. The first part is a summary. Then we've actually got a short conversation. So we've got something that the customer has asked and then we've got the agent's reply. And then the customer has responded to the agent. And then we finish with the agent and a space and then we get the completion at the very end. So basically all of that is the prompt and that is the completion. Now I've done that just to show you that these prompts don't have to just be one question and one answer. They can actually involve lots and lots of dialogue. So if you have got a lot of customer service data, you can format it in this way with the completion just being the final answer to the query and that is going to really help train your chatbot. Now you don't need to stick to one data style. As long as you're working with prompts and completions, you can mix as I've done here, some of the prompts will be one question and some of the prompts will actually be whole conversations. Okay, let me just put this data back as it was. Now, as we're going to be working with this data in the terminal, you need to download it or have some data of your own in a similar format. You can download it just by clicking on this slide and that's going to take you through to a Google Sheets version which you can save a local copy of or you can come down here to this cog icon and when you click on that, it will bring up a menu and click download a zip which will give you a zipped folder and when you unzip it, you'll see all of the files from this project. The one you're interested in of course is the We Wing It Data CSV. So I'm going to take that file and save it in my apps folder in a file called We Wing It and there we are. There is my data ready for the next step. Okay, we've got some work to do with this data before we can actually upload it and fine tune a model. We are going to use OpenAI's data preparation tool to do that for us but before we can do that, we need to set up our command line interface environment. So next, let's open up the terminal and tackle that. For the next part of the project, we're going to be using the terminal or command prompt. If you're new to using the terminal, it can look pretty intimidating. It's actually not that bad and to help you along, I've created a file over here called terminalcommands.md and I've pasted in all of the commands we're using in this scrim so you can refer to that quickly and easily if you need to. Now to open the command prompt in Windows, you can use the Windows key plus S and enter CMD in the search field. On MacBook, it should be right there in the launcher and of course in either case, you could use the terminal in VS Code. Now I've got the terminal open and the OpenAI tool that we're going to use requires Python 3. Now if you've already got Python 3 installed, then you're good to go. If you've never used Python before and you're starting to freak out, don't worry, we're not coding in Python. It's just needed in the background to run OpenAI's tools. Now you can check to see if you've got Python 3 installed with this command. So it's just Python 3 dash dash version and when I hit enter, it tells me that I've got Python 3.11.1. Now if you haven't got Python installed, you can click on this slide and that is going to take you to the official Python site or if you're on Mac, you can use Homebrew or if you're on Linux, you can probably get Python from your distros repository. Check their docs for details. Now in a moment, we're also going to need the pip package manager. If you have Python installed, you probably already have pip and you can check by doing pip dash dash version and that tells me that I have got pip installed. If for some reason you haven't got pip installed, this command right here, python3 dash m, ensure pip dash dash upgrade will install the latest version. Okay, now we should be good to go. So let's install the openAI CLI using pip install dash dash upgrade openAI. That should install the openAI tool and you can check that by running the openAI command and that should give you a list of commands that are specific to this openAI tool. Now there's one more thing I want to do. The fine tune model we create is going to be specific to us, only we can use it. So the openAI tool will need our API key. We don't need that to prepare the data, but we will need it before we upload. So we might as well do it right now. You can add the openAI key with this command. It's basically export and then everything that we've got in this mth.js file in this line of code right here. But be sure to swap out this colon for an equals. Now when you've done that, press enter and you're not going to get any special acknowledgement that your key's been accepted. Just assume it has been. And now with all of that setup complete, we're ready to work with the data separation tool. So let's get stuck into that next. Okay, so we've got the openAI CLI up and running and we've given it our API key. Now let's use it to prepare our data. And again, I've listed the CLI commands that I'm going to use in this file right here. So our first task is to cd to the folder where we've stored our data. Now I've saved my data in a folder called wewingit in apps, which is in documents. So let's head back to the terminal and I'm going to say cd documents slash apps slash wewingit. CD here stands for change directory. And of course you'll need to navigate to wherever it is you saved your data file. Okay, having navigated to the correct folder, we can start our finetune preparation. So we do that with this command. And what we're doing here is telling the terminal to use the openAI finetunes tool to prepare our data. Now this F flag is going to identify the file of data that we want to prepare. Now my data was called wewingitdata.csv. So what I need to do is just put that right there on the end. Now at this point, you might run into a problem. The first few times I did this, it worked fine. And then suddenly at this point, I started to hit an error. Well, that's the nature of working with new technology. Things can change. And the error I got was this, missing pandas. Well, if openAI wants pandas, openAI gets pandas. So what you need to do then is say pip install openAI pandas. Once you've done that, hit enter, let it do its thing. And then we can try the data preparation task once again. So this is the exact same command. This time it works and it gives us some information. It knows our file is formatted as a CSV file. It also complains about the number of prompt completion pairs we're using and says, in general, we recommend having at least a few hundred examples. Well, we know that already, but this is for demonstration purposes. So it's absolutely fine. Now I'm going to save you reading the rest of this text here because actually it's telling us something we already know. We talked about the format of our data and all of the features that we need. And we know that we need a separator to inform the model when the prompt ends and the completion begins. We know that each completion needs to start with a single white space. And we know that each completion should end with a stop sequence to inform the model when the completion has ended. But this tool is great because it's basically going to do everything for us. So down at the bottom, it's already told us that it's necessary to convert this to JSON L. That's absolutely fine. It's recommending us to add a suffix separator to all of our prompts. Well, let's say yes to that. Now it's recommending adding a suffix ending of a new line character to all of the completions. Let's say yes to that and press enter. And now it's recommending us to add a white space character to the beginning of all of the completions. Again, let's say yes. Now we just need to confirm that we're ready to proceed and it goes through its process and it tells us that it has created this file for us. And it invites us to take a look, which is a really good idea. If you go back to your folder, wherever you stored your data, you should see this prepared JSON L file waiting for you. Now, just so we can see this data, I'm going to copy it into a file in the editor. And there we are. And you can see that it's added the prompt key and completion key to each pair. It's added white space before the start of every completion. It's added a new line character at the end of every completion. And of course it's added a separator at the end of every prompt. And we could have done all of that by hand and just used the tool to check, but wow, what a lot of boring work that would have been. Okay, next we need to fine tune our data. So let's come on to that in the next scrim. Okay, let's get to work on the fine tune. At the end of the data preparation process, we got this instruction. So let's just break down what that is telling us to do. Firstly, we're using the open AI CLI tool to access the API. Then we need to use this fine tunes dot create command. And we use the T flag here to introduce our training data. And we save that in a file called we dash wing it dash data underscore prepared dot JSON L. Our training data, which has been prepared into the JSON L format. Now we need to add just a little bit more to the end. The docs tell us that we can use an M flag to specify the base model to use. And we're going to use the DaVinci model. If we leave that blank, it will actually default to the older query model. Okay, let's put that command in the terminal. And when I press enter, we get this message. Now open AI needs to enqueue this and it takes some time. The quickest for me has been a few minutes. The longest was literally all night. Now down here, it says stream interrupted, client disconnected. That is really common. It doesn't mean that the fine tune process has stopped. It just means that open AI is no longer giving us live updates. And what we can do is take this command here, paste it and run it. And that will reconnect us to the information stream. And eventually we'll get this message with an emoji that tells us we have been successful. And then what we see down here is actually our very own fine tuned model. It's everything from DaVinci right up until the date. Now that's pretty cool, but before we can use that model in our app, we need to make some changes to our JavaScript. So let's go ahead and do that next. Okay, so to use this model, we need to update our existing code. The first thing to remember here is that we have fine tuned a DaVinci base model. And at the time of recording, you can't fine tune GPT4. That means that we need to get rid of loads of this specific format. Now I've divided this process into several parts and we'll have challenges for them over several scrims. The first task is to change this array, which is holding the conversation. DaVinci models just need a string holding the conversation and we won't be using an instruction. Here are two mini challenges to do just that. Firstly, change conversation array to conversation string. And you can initialize it to an empty string. Now I've just put a warning here. Think about how this is going to work. Is there any other change you need to make? I'm just going to let you think about that. Once you've done that, you can come down here and update conversation string with just the user's input. So instead of pushing this whole object, all we actually want to push is whatever the user has inputted. Then you can just log out conversation string to check it's working. And I've disabled the fetch reply function as at the moment, we just get an error if we sent off the string instead of the array of objects to the current endpoint that we've got in our code right here. Okay, pause now, take all the time you need and I'll see you back here in just a moment. Okay, hopefully you managed to do that just fine. So let's just delete everything we've got here. We're going to make this conversation string. I'm just using the abbreviation STR and we'll initialize that to an empty string. Next, we need to come down here and update conversation string with just the user's input. So again, we need to convert conversation array to conversation string. And instead of push, we can just use plus equals. And let's log out conversation string and hit save. Okay, I'll just type something in the box and look, we get an error and it's a type error assignment to constant variable. Well, I did warn you here, think about how this is going to work. Is there any other change you need to make? Well, yes, we need to change this const for a let. Let's try again. And there we are, it is working. So that is two of the jobs off the list. Let's move on to what happens inside the fetch reply function. Next up, we need to change the end point. Create chat completion is specific to models like GPT4. We are going back to the completions endpoint which uses create completion. So I'm just going to come in here and delete chat. Now for your challenge, I want you to swap out the model GPT4 for your fine tuned model. And you can get the model name from right at the end of the fine tune process. Now, if you've closed your terminal window, don't worry, log into open AI, go to the playground, come up here to where we've got model and just click this down arrow. That will bring up a list of all the regular models and also all of the fine tune models that you have created. Now for the second part of the challenge, our fine tune model needs a property called prompt not messages. So you just need to swap out messages for prompt. And you also need to update this reference to conversation array. Conversation array does not exist anymore. Once you've done that, you can run a test by logging out the response. Now, before you do the challenge, I'm just going to uncomment this fetch reply function call. And I have already commented out these two lines of code. When we've got the endpoint working properly, we'll uncomment them and make some adjustments. Okay, pause now, get this challenge sorted and I'll see you back here in just a moment. Okay, hopefully you managed to do that just fine. So I'm going to come down here and I'm going to just grab the name of my fine tune model and I've got it right here. And I just need to paste it right here. Next, this messages property needs to become prompt and conversation array needs to become conversation string. Okay, let's log out the response and see what we get. And I'm just going to hit save and ask a random question in here. Let's open up the console and see what we get. And there we are, we've got a response. And the response is a little bit strange. I'm just going to copy it into the editor so you can see it clearly. The response to my question, how are you today? Was bizarrely this, it sounds like you've been on a real roller coaster, I'm sorry. Well, that's okay. We've still got quite a long way to go before we get this fine tune chatbot working. So it doesn't matter at all that it's giving us quite a random answer. The important thing is we're managing to make an API call to this new model and we're getting a response. So in the next grim, let's get to work on these two lines of code right here. We need to make a few more changes to the JavaScript before we can do some testing. So at the moment in these two commented lines of code, we are pushing to the old conversation array and we're also sending our completion to render typewriter text. Those two lines of code are not going to work anymore. We need to make some changes. So firstly, let's uncomment this line of code and we'll change this to conversation string. And instead of using push, we will of course use plus equals. And what is it that we need to update conversation string with? Well, I've just pasted in the response that we got up here and it's pretty similar to the responses we were getting when we were using the chat GPT models. Now, if we look down here, this one ends with messages. Messages doesn't exist. We've got choices, but the completion is actually stored in text. So let's just change message for text. And of course we don't need these brackets. And now that we're updating conversation string, just with the completion, we can actually reuse this code here because we also need to send the completion to render typewriter text. So let's just paste that in here and now it should work. So I'm just going to delete a couple of console.logs and let's delete all of this ugly code as well and give it a test. And okay, it's working. And it said, I've got some lovely things for sale in my shop. I hope you like. Now the reason why it's finished on like with no punctuation is because now we're using a DaVinci model again, we need to set max tokens at the moment it is set to 16 by default. Let's set it to something much higher. And I'm just going to try that again. Wow, and now it's gone a little bit crazy. All I said was, hey there. And it's given us all of this stuff, which is sort of related to the data we uploaded. It certainly got the email address right. And it's given us some rather random phone numbers, although this one is not too far away from the one we told it about. And then it starts going on about using cookies on the website. Well, this is more than just hallucination. It's actually gone a bit crazy. And what's worse is that it still didn't finish on a complete sentence. So if we whacked max tokens up to something much higher, like a thousand, I've got a feeling that this crazy chatbot would go on and on. But heyho, at least it's working. The basic mechanics are fine. So in the next grim, let's start figuring out what's going wrong. So we have the finetune chatbot working or sort of, because actually it's giving us nonsensical gibberish. So let's just check the settings we have. The presence penalty and frequency penalty aren't doing anything very much. They're almost at their default. So I'm not too worried about them. Now with open AI, when the completions are a bit weird, the first thing we might think of is temperature. Remember, temperature controls how daring the model will be. How creative, how inventive. Now we don't want creative, we want factual. So let's put the temperature down to zero. And let's see if that has had any effect. Okay, that completion is like something from a horror movie. Now the chatbot is claiming to be a human. It's begging us not to kill it. And it's just saying, please, please, please. And then creating a word so long it actually breaks our CSS. So I'm just going to refresh to get rid of that, because it's just a bit scary. Okay, so the temperature hasn't magically solved the problem, but I think a low temperature will be useful. So I'm going to leave it there. Now, before we do anything else, let's go back to the criteria for the format of our data. The first one that we had says, each prompt ends with a separator to inform the model when the prompt ends and the completion begins. And actually when we were prepping the data, it told us that it would use this arrow as the separator. So what we need to do is add the separator to the end of our prompt. And I've got the JSONL file right here. And we can see that each prompt does indeed end with a space and an arrow separator. Now we're not adding those separators to our conversation string. And I think that might be causing part of the problem. So in fact, I'm just going to log out conversation string right here. And let's be brave and just see if the chatbot is going to scare us again. And it's done the same creepy thing again. Now let's just have a quick look down in the console. And what we can see there, if I just cut and paste it into the editor, is that we're just getting one continuous line of text. So hey, and I'm not a robot, look like they're being said by the same speaker. But in fact, that was our prompt. And this is the beginning of the completion. So we have got problems there. And that brings us to a challenge. And your challenge is this. I want you to add the arrow separator to the end of our prompt as it is added to conversation string. And I've put here a space before the arrow separator because that is how it's formatted in our JSONL data. Okay, I'm going to leave the console.log that I just added. So when you're done with this, you can just type something into the chatbot and give it a test. Pause now, get that sorted, and I'll see you back here in just a minute. Okay, hopefully you managed to do that just fine. So I want to come down here and put this in backticks. And now we'll use the dollar sign and the curly braces so we can access the user input.value. I'm going to start with a space and I'm going to end with a space and an arrow separator. Okay, let's give that a try. Okay, so we've seen an improvement there. This is by no means perfect, but at least it's working a little bit. Now it feels like we're talking to a representative of the We Wing It company. It's got the email address. It's still giving us a load of hallucination and weirdness. And if we look down in the console, we have at least got our separator between hey, which was our prompt, and odia, which was the beginning of the completion. So we've made some small progress here. But what else could be going wrong? Let's just go back to our data formatting criteria. So we've sorted the first one. Secondly, it says each completion should start with a single white space. Well, if we have a look down in the console, that appears to be true. Or is it? I think the next thing that we should do is come down here to fetch reply and where we're updating conversation string, with the completion, we should explicitly add a white space at the beginning. So again, I'm going to put this in back ticks, use the dollar sign and curly braces to access the completion from the response. And now I'll add my white space at the beginning. Okay, let's hit save and try that one more time. And that's not really made any improvement. I think that's just as crazy as it was before. But we have now got that white space there for sure. We know it's there. We've ruled that out as a potential cause of problems. So let's go back to the data criteria and look at the third one. Each completion should end with a stop sequence to inform the model when the completion ends. Well, we're back with the mysterious stop sequence. We keep coming across it and we've never really looked at it in detail. So why don't we deal with that next? Let's take a look at the stop sequence, a way of stopping the model from generating tokens. So what exactly is it and how does it work? Well, it's an optional setting that tells the API to stop generating tokens at a given point. The completion will never contain the stop sequence and the stop sequence is an array. And that's because we can have multiple stops in a stop sequence. Now all of that is quite theoretical. So let's just jump straight into an example and you'll see what I mean. Right here, I've got a basic API request set up using the text DaVinci 003 model. And I'm just asking it to list some great books to read on the topic of coding. So I will uncomment this function call and hit save and let's open up the console and see what we get. And there we are, we get 10 good book recommendations. Now I'm going to come in here underneath the max tokens and I'm going to add a stop sequence and the property that we want is just stop. The stop sequence is an array and each individual stop will be a string. Now we can have up to four stops in this array. I'm only going to use one. And if we look at the formatting of what's in the console, each list item starts with a number and a dot. So it's one dot, code the hidden language of computer hardware and software by Charles Petzold, two dot, Python crash course by Eric Maths. So the stop that I put in here just to experiment with is going to be two dots. Let's see what happens when I press save. We get a list with only one item. So what happened here? Well, if you'll remember, the completion will not contain the stop sequence. So the model was prevented from writing two dots. And because it won't write a stop sequence, at that point, the completion is cut off. So if we wanted a list of five books, we could change this to six dot. And there we are, we have got a list of five books. Now you can use anything as a stop. You can put whatever characters in there you might find in your completion, but you just need to know what you're doing with it. Obviously, if you put something like the letter A, well, that's going to cause all sorts of problems. There we are. The completion has actually been cut off on the fourth letter of the book title, which was obviously an A. Now, when we're working with chat bots, it's very common to use the new line value as a stop sequence. Why? Because often you want the bot to give you one paragraph as an answer, and then you want the user to respond. If you don't use a stop sequence with a chat bot, you run the risk of the chat bot answering and asking questions and having a conversation with itself that becomes ever more bizarre and illogical until it hits the token limit and that stops it. Now with the general purpose chat bot we built with GPT4, we didn't need to add a stop property because GPT4 has actually got that specific syntax, the object with the role and the content properties. DaVinci and other models don't have that, so we can use the new line character as a stop to stop the bot from continuing the conversation on its own. Okay, let's go back to the app and add a stop property. Okay, so if we refer back to our slide, we can see that each completion should end with a stop sequence to inform the model when the completion ends. And when we were preparing our data, we actually selected this option to add a suffix ending of a new line character, which is backslash N. So as we add completions to our conversation, we need to include this new line character at the end as a stop sequence. Now we're also using this arrow as a separator, and when experimenting with this, I sometimes found arrows popping up in completions and some cases where the chat bot seemed to just get into a conversation with itself, asking itself questions and giving itself answers. So we're actually going to add the arrow as a stop sequence as well, because we never want the model to generate an arrow. Okay, so here is your challenge and it comes in two parts. Firstly, I want you to add the new line character, which is backslash N and the separator as a stop sequence. And actually, you don't need to have a space in the stop sequence. Secondly, I've said here, I want you to add something to the line below where we update conversation string. So that is this line of code right here. I'm going to leave you to think about that. What would it be good to add to that line? Okay, pause now, get that challenge sorted, and I'll see you back here in just a moment. Okay, so hopefully you managed to do that just fine. So let's come in here and we'll add our stop. And we know that that will be an array and it will be an array of strings. So the first string is going to be the new line character, and the second string will be the arrow separator. And of course, it doesn't matter what order they go in. Now, the second challenge, add something to the line below where we update conversation string. Well, if we refer back to this slide, we're going to add a suffix ending of backslash N to all completions. So this is the completion that we're getting back. Let's just add a new line character to the end of that. And what that is going to do is make it really clear to the model when it reads conversation string that it has reached the end of a completion because that is what the new line character will tell it. And likewise, the arrow separator that we're adding to the stop sequence here, which we're already adding to conversation string right here in the event listeners anonymous function, that will tell the model that it's reached the end of a prompt. Okay, let's hit save and see if it's worked. So I'll start again just by saying, hey. All right, and I'm still getting a pretty strange response here. Now I'm going to ask a question from our data. And if we have a quick look at the data, we can see that the customer support team has a phone number. So I'm going to ask it, what is your phone number? Okay, and look, we're getting a much better quality response. It knows the phone number. It is still hallucinating. We're only here from nine to six 30 weekdays. That is not actually true, but the answer is much shorter and it's much more manageable. Now I'm going to ask it a question it can't know the answer to because it simply isn't in the data. I'll ask it, who is your CEO? Okay, interesting. It completely makes up a person, Richard Quing. So for some reason, the large language model has decided that Richard Quing is the most probable name for a CEO of our company. And it's also made up an email address for him but using atwewingit.com. So again, it's a hallucination, but it's an interesting one. So I think where we are at the moment is that this chatbot works, it can see its data, it's giving us short manageable answers, which makes sense, but we're still getting too many hallucinations it is not accurate. Now we are going to get hallucinations because as I said at the beginning, we're not really using enough data. We've got enough data to show the principle, but not enough data for a full production ready app. But that said, there is one last really important aspect of fine tuning that I want to tell you about and it is an epochs. What does that mean? Well, when we were building the model, you might have noticed that as the model was fine tuning, it completed various epochs. And in this case, it completed four epochs. Well, maybe four epochs just wasn't enough. So in the next grim, let's talk about what epochs are, how we can get more of them and how that is going to help us. So let's check that out next. Let's talk about an epochs and I've put here the number of data cycles. So what exactly are an epochs? Well, the N stands for number. So it's the number of epochs. And what that basically means is that it controls the number of times open AI will cycle through the training dataset when fine tuning the model. Now it defaults to four and that is likely fine for larger datasets with hundreds or thousands of prompt completion pairs. But for smaller datasets, it isn't enough. The disadvantage of using higher numbers and therefore more cycles through the training data is that it costs more to do the fine tune with a small dataset like ours, it's still going to be pretty cheap. The other thing to think about is that if you're using a massive dataset, it will take a long time. Now an epochs is something we set when we send the training data to be fine tuned. We can't set it from within our app. So what we need to do is go back to the CLI and we're going to add this to the end of the fine tuning command. It's just dash dash N underscore epochs and then the number of epochs you want. And I'm going to go for 16. You don't have to go for 16, but I got much improved results using 16. You could go higher, but just remember it costs more each time. So that is the same command we used before. Here's the N epochs added on the end. And so the final command will look like this. It's everything we did before plus this N epochs on the end with the double dash. So here is a challenge for you. Use open AI CLI tool to build a new finetuned model with N epochs set to 16. You don't need to prepare the data again. That command is all you need. Once you've gone through that process, you will get a new model name and you can slide it in right here, replacing the old model. Once you've done that, go ahead and test it by asking it about weaving its phone number or email or anything else from the data that we've got right here. Go ahead and do that. It won't take very long at all to set up. It might take quite a while for the API to actually do the finetuning process. So make yourself a cup of tea and we'll have a look at this together when it's done. Okay, hopefully you got some good results with that. So I'm going to come over to the terminal. There is the command to create the fine tune. I'm going to add the N epoch 16 on the end and press enter. And then eventually having gone through its process, it's completed all of these epochs down to 16. And finally, I have got my new model and here it is right here. So let's go ahead and swap the old model for the new model. Okay, let's hit save and do some testing. So I'm going to start off asking about the phone number. Okay, and that is a really nice answer. It's got the phone number correct and it's also said, we prefer it if you only phone us in an emergency. Now that's really interesting because if we look at the data, we mentioned the phone number several times. If we come down to line 19, well, we can see here, we've got the straightforward question, what is your phone number? And if I just move the mini browser out of the way, it does actually say, we prefer it if people only phone us in an emergency and it gives exactly the same times as the chatbot told us right here. So the chatbot is successfully using our data and that is really, really good. And it's also suggested they email us instead. Okay, now I want to ask it something it can't know the answer to, who is your CEO? In the previous scrim, it hallucinated an answer to this. So let's see what it does. Who is your CEO? And it says, I'm sorry, I don't know the names of the people working here. Now that's really interesting. I've made sure to include in this data, various I don't know statements. If we have a look right here on line 18, for example, what material are your drones made of? Completion, I don't know the answer to that question. Check out line 24, who is your press officer? I'm sorry, I don't know the names of members of staff. And again, on line 32, we have, are your drones recyclable? I'm really sorry, I don't know the answer to that question. Now traditionally, AI is not very good at saying, I don't know, it prefers to hallucinate. But we've effectively given it permission to do that. So do bear that in mind when working with chatbot data, it's good to teach it to say, I don't know, as this will help stop it hallucinating. Let's just ask a few more questions. So I'm going to say, how much is insurance? And it's telling me it's three pounds 75 per delivery. And if I come up onto line eight, well, there we are. It says it's a flat rate of three pounds 75 per drone order. So again, that is pretty good. Let's ask it something a little bit more random. I've said your drone crashed into my house. Okay, it says, please contact us by email and we'll arrange to pay for any damage. I'm going to say, I want compensation. And there we are. It's giving a very logical rational answer. Now it's not being particularly humane. It's not being touchy feely. It's not being that conversational. And that, to be honest, is a shortcoming of our data. We just need much, much more data. And we need much more of this kind of data that we've got at the bottom, where we've got these entire conversations playing out before we get to the completion, because that is the quality data. That's what really helps the chatbot communicate well. So although I think we've done very well here, you will find limitations with this chatbot quite quickly. You're going to come across idiosyncrasies and hallucinations. But as I've said many times, if you wanted to take this to production, you would need a lot more data. But what we've done here has really proved the principle. It's working really well on a really small data set, so that is really good. And of course, your results might differ. Perhaps you're using your own data, or perhaps the model has simply been updated somewhat between the time that this was recorded and the time that you're actually building this app. And that is the nature of working with this new frontier in technology. Now the next thing that I want to do is take this app and deploy it live on the internet with the API key safely hidden. That's quite a process. We've got quite a bit to do. So let's make a start next. Okay, so our mission is to deploy our support bot, but keep the API key hidden. And we're going to do this using Netlify. So right now, if we were just to deploy this as a frontend only project, the API key would be visible right there on every user's machine and easily obtainable from the DevTools network tab. Let's just zoom in. Oops, there it is. Our API key is there for all to see. And that is what we need to avoid. What we want is for you to be able to share the projects you've made with OpenAI without fear of your key being compromised. And actually, the process we're about to go through can be used anytime you need to keep an API key hidden. It's not specific to OpenAI in any way. So how are we going to do this? Well, let's take a highlevel overview. Now, at the moment, we've got a frontend project, and what we've been doing is storing the API key on the frontend, making a request to the OpenAI API, and getting back a completion that we use on the frontend. But now, we're going to stop doing that and do something completely different. We are going to send our request to a Netlify serverless function. The serverless function will have access to the API key from a secure store. The serverless function will then make the call to the OpenAI API. The API will pass the response back to the serverless function, and the serverless function will pass the response back to the frontend, and we will use the completion from that response. But most importantly, this API key in its special store is never visible to the frontend, so it won't be visible in the DevTools Network tab. Now, there are a few prerequisites that I'll just mention quickly. You are going to need a free Netlify account. That's really easy to get. I'm also going to assume you have a basic knowledge of GitHub, because we will be deploying to Netlify directly from GitHub. If you know how to publish a project to a GitHub repo, that is enough. We'll also be making fetch requests, and if you've followed this course so far, you won't have any problem with that. We won't really be doing anything more sophisticated than we've already done, and you'll also need VS Code or a similar editor. Okay, that's the overview. Let's put it into practice. So firstly, we need to get our project stored locally. So come down here and click on this cog icon, and that will bring up a menu. Click Download as Zip, and then unzip that folder and save it somewhere sensible. I've saved mine in this folder called wewingit underscore deploy, and it's inside another folder called apps. Now, let's open that in VS Code. Now, before we forget, and to save us from running into an error later, I want to open up index.js and come onto line two. We won't be importing the API key like this anymore. So let's go ahead and delete that import statement. If you skip this step, when we deploy the project to Netlify, you will get an error and it will be a bit frustrating. So best to remember to do it now. Next, we need to open up the terminal, and I'll zoom in on it, and I'm going to run npm install, and this will download the dependencies defined in the package.json file, and it will generate a node modules folder with the installed modules. The dependency that we've been using is the OpenAI API, and we still need it when we deploy to Netlify. Okay, we run that, we let it do its thing, and then we can see that the node modules folder has appeared. The next step is to publish to GitHub. So you're welcome to do this in whichever way you're comfortable with. I've got the GitHub and pull requests extension installed, so I will go with this icon right here, and I'm going to come down here and click on publish to GitHub. I'm going to publish mine to a private repository. Now it's asking me here which files should be included in the repository. Really important to ignore the m.js file, so I'll untick that. Now you could also delete it. It doesn't matter if you delete it or ignore it, just make sure you don't include it in your repo. If you choose to delete it, make sure you've got a copy of it somewhere handy as we will be using it again shortly. Okay, I'll click okay. I'll let VS Code do its thing, and then when the process is completed, we can see that we've got the gitignore file and m.js is in that file. So that is exactly what we want. And of course at this point, you can log into GitHub and check it's worked if you want to be sure. Okay, so we've got the project stored in a repo. We've either ignored or deleted the API key. Next, we need to set up a Netlify account. Okay, next up, we need a Netlify account. It's really easy to do, click on this slide, and it's gonna take you through to the signup page, which looks like this. Now I'm going to sign up with GitHub, and then once it's done its thing, it will take me to the dashboard. And from here, I'm going to select add new site. I'll import an existing project, and it will invite me to connect to a git provider. I'll choose GitHub, and it's going to say no repositories found. And that's absolutely fine. Click configure Netlify on GitHub, and this will open a popup page and you can scroll down and either select all repositories or select repositories individually. I'm going to do that one, and I'll choose the repository. I'll just type in we wing it underscore deploy because that's the name of my repo, and click save. And we'll be taken to a settings page. And on here, we can leave everything as it is and just scroll down to the bottom and click on deploy site. As the site deploys, we will see site deploy in progress right here and then eventually published. So now the site is live, we can click on open production deploy, and that is going to actually take us to our site. We've got our own funky Netlify URL right here, and let's see if it works. So we can just come down here, type in a question, press send, and we'll wait and nothing happens. Let's open up dev tools and we've got plenty of errors. Well, that isn't actually a problem. We've only done the first stage and actually it would be really weird if it worked. It doesn't have an API key right now. So our next step is to get our API key to Netlify, and we're going to store it in a Netlify environment variable. Let's come on to that next. What we need to do now is give Netlify the API key. Now, if you remember the function that we'll be using to actually do the heavy lifting of making the API call, we'll have special access to our API key. So on the Netlify site, come over here to site settings, and we're going to click on environment variables, and we can click add a variable, and then I'm going to select add a single variable. That takes us to this page, and we need to add a key and a value. So let's take a look at our code right here because this is essentially the same information. This is going to be our key, and the actual API key will be the value. So back on this form then, we'll have open AI API key in here, and the actual API key stored here as the value. Then click create variable, and we can see that the open AI API key has been added. So what we've done here is we've stored our API key in a Netlify environment variable. So now, if we scroll up and we launch our site, again, we can try asking a question, but we're going to have the same result. We'll still get errors. And that figures, Netlify has the API key, but we're not doing anything with it in the code. And anyway, at the moment, all we've got is our frontend code, and if it did work, the API key would still be visible in DevTools. The fact that we've stored it in an environment variable doesn't mean you can't then display it on the frontend. You actually can. That is why we need the serverless function to make the API call away from the frontend. Okay, so what we need to do next then is work on the serverless function. But in order to set up a serverless function, we need to get handson with the terminal again and install Netlify's CLI or command line interface. So let's do that next. Okay, so to integrate a serverless function, we need the Netlify CLI. So let's come back to the terminal, and just to help out, I've created a document here called terminalcommands.md, and I've just listed the terminal commands that we'll be using in this scrim. So we're going to come in here with the first command, which is install netlifyclig. So I'm installing this globally. That's what the dash G means. So we can hit enter, and eventually you'll get shown something like this. Now, if we just type netlify and hit enter, we'll get taken to a list of Netlify commands. And the one that we're really interested in is init for initialize. And we're going to use that to configure our site. So down here, let's say netlify init. And now it's going to ask us a series of questions. The first one is connect this directory to an existing Netlify site. Now we've got two options here, and we can just move up or down with the arrow keys. But the first option is the one we want, so I'll just press enter. Now it's asking us if we want to use the current git remote origin, and it's correctly identified the we wing it underscore deploy repository that I'm using. So that's fine, click enter. And there we are, it says directory linked. That is what we want to see. And if we go back to the full screen of VS code, we've now got a new.netlify folder. Okay, in the next grim, we'll use the CLI to give us the boilerplate for a serverless function. Okay, let's get to work on a serverless function. The Netlify CLI is going to give us some boilerplate. So in the terminal, let's say netlify functions colon create. We'll hit enter, and we're going to get some options. The first one is for an edge function. We don't want that, so we can use the arrow key to come down to serverless function, press enter. And it's going to ask us a few questions. The language that we want is JavaScript, so hit enter. Now it's telling us to pick a template. And we've got some interesting options here, including this one or fetch, which talks about APIs. But because we've already got the open AI dependency, and we're not really dealing with authentication, I'm going to go with this basic hello world function, and we're going to adapt it from there. So let's hit enter. Now it's asking me for a name. The default is hello world. We definitely don't want that. I'm going to go for fetch AI, which I think is a reasonably descriptive name. And the function is instantly created. And now if we look at the whole of VS code, we've got a new folder here, netlify. Let's click into it. Here's our function, fetch AI. Let's click on that. And there we are. That is the boilerplate for our serverless function. And at the moment it's basically returning hello subject. Well, we can see from this line of code right here that unless we put a query string parameter in, the subject will be world. So this should return hello world. And if we open up a browser and we actually navigate to.netlify slash functions slash the name of our function, and I have actually put that right here in this terminal commands MD file. And when we go to that URL, what we get is this message, hello world. So we're seeing the object returned from that serverless function. And so this URL is now my endpoint. This is what I'm going to call from the front end instead of calling the open AI API directly. So next we need to go back to the editor and make some changes to index.js. We want to make it so that instead of calling the open AI API directly, we make a fetch request to this new endpoint. So let's do that next. Okay, so we need to completely change this fetch reply function. So instead of calling the open AI API directly from the front end, now we're going to call the netlify serverless function. Now I'm going to set you a challenge to update fetch reply right here in the Scrimba editor. But when we're done, we're going to need to paste this into VS code and push it to GitHub to trigger a redeploy. First, we need a URL to call. And we've already seen the URL, we looked at it before, and I've got it right here in the URL bar. So I'm just going to come in here and paste it in a const called URL. And now it's time for a challenge. And it is quite a long challenge. And this is where I'm assuming you have some knowledge of fetch requests. But do take all of the time you need and feel free to search online if you need to. When I do this, I'm going to use async await, but you don't have to, you can use a fetch request and chain then statements if you prefer to do it that way. Also, I'm just going to comment these two lines of code. When we've got this working in the console, we can uncomment these and update them as needed. But we'll do that after we've got the serverless function working properly. Okay, let's check out the challenge. So I want you to make a fetch request to the URL that we've got saved here using the following details. The method should be POST. In the headers, the content type should be text slash plane. And the body should hold conversation string, which remember we have got stored right here. Now, once you've made the fetch request, you can save the response to a const and log it out. Then copy and paste the updated fetch reply function to VS code and delete any unnecessary code from index.js. Push the changes to GitHub to trigger a redeploy. And remember that will take some seconds, maybe as long as a minute. And then navigate to your Netlify site, put something in the text input. It doesn't actually matter what because it's not going to be rendered and it's not going to be sent to open AI. Hit send and then what you should see in the console is hello world. And that will show that from the front end, we're successfully managing to access the serverless function and get whatever it returns. Okay, quite a lot to do. So pause now, take all of the time you need and we'll have a look together in just a moment. Okay, so hopefully you managed to do that just fine. So I'm going to come down here and I'm going to set up a new const response and I'm going to await a fetch request. The first thing a fetch request needs is a URL. Well, we've got that saved right here. And now we need to pass it an object. So we've got all of the details here. Firstly, the method should be post and then we need some headers and this will hold an object and it's going to have a single key value pair. The key will be content type and the value will be text slash plane. Lastly, it needs a body and the body will hold conversation string. Okay, then I'm going to come underneath that and set up a const called data and we will await the response JSON. And let's just log out data. Now we've got plenty of code here that we don't need. All of this code down here, we can safely delete but I'm just going to comment it out because we will need to copy and paste it later. And likewise, right up at the top, all of this code that brings in open AI, we no longer need and I will comment it out. Then I'm going to copy the fetch reply function and I'm going to bring it over to VS code and I'm going to paste it in. And of course, I haven't copied over the code I commented out apart from these two lines which we will be using here in the future. Now staying in VS code, up at the top, we've got these lines of code here, we can delete them. When you've done that, you can save, push it to GitHub and that will trigger a Netlify redeploy. And remember, that will take a few seconds, maybe even a couple of minutes. When it's done, navigate to our site, send a message and then what you see in the console should be the object with hello world. And that will show you that the frontend code has successfully communicated with the serverless function. And that means we need to get to work on this serverless function, next. I've just pasted this serverless function into the Scrimba editor. And again, when we're done, it will need to go back into VS code so it can be pushed to GitHub to trigger a redeploy. Now, before we do anything else, let's delete all of the code we don't need. Okay, that is the bare bones of the serverless function. Now, what we've got at the end might look a little bit unfamiliar. Don't worry about that. That is just exporting this handler function to make it available to the rest of the app. And that syntax there is actually from Node.js so it's not familiar to frontend developers. The next thing to do is bring in open AI. And remember, we used npm install to load in the dependencies we need so we have access to them across our project, including in this serverless function. Now, the code we need to do that is code we're very familiar with. In fact, we just commented it out in index.js. So let's head over to index.js and let's uncomment that. Now, we will not be importing our API key from this mf.js file. Remember, we've ignored that or deleted it. It's not available on Netlify. So let's delete that line of code. Now, the API key that we've got stored in the Netlify environment variable is available to us in this serverless function. And we can access it using process.env and then the name of the API key. So luckily, we don't need to change this line of code at all because this is going to fetch our API key from the environment variable we set up earlier. So now we've bought in open AI, we need to actually use it down here inside the try block. So let's head back to index.js and we'll take the second block of code that we've got commented out right here. That is looking pretty good, but we need to do something with conversation string. We don't have access to conversation string in this file and we can't just import it as this serverless function will not be part of the front end code. Remember, we're actually calling this serverless function using a fetch request. And we've got the fetch request right here and conversation string is being sent in the body of the fetch request. Now back in fetch AI, we're taking in an event parameter. And from that event parameter, we can actually access the body. So I'm going to come down here and replace conversation string with event.body. So now when the fetch request comes in, the conversation string will be in the body, fetch AI takes in the event parameter and accesses conversation string, which will be stored in the body of the event object. Okay, I think that's looking good. We can't test it yet, because first we need to deal with what we actually get back from the API. We've got this return down here, so we need to do something with that. Let's come onto it in the next screen. Now we need to deal with what the serverless function gets back from the open AI API. Now that's no problem. We've worked with the API long enough by now to know that what we get back is a response object and that we need the data from it. So working down here, inside the return statement, and we've got the body, we're calling JSON.stringify and then in here, I've pasted a challenge for you. I want you to add a key value pair. The key should be reply and the value should be response.data. Now, once you've done that, paste the code into fetchai.js in VS Code and push it to GitHub to redeploy and see what gets logged out when you test. Okay, pause now, get that sorted, and I'll see you back here in just a moment. Okay, so all I need to do is come in here and add the key value pair. So the key is reply and the value is response.data. Okay, let's copy that and I'm going to paste it over here in VS Code and I've just formatted it a little bit more neatly and obviously without the challenge text. Now, I'll go through the process. We've gone through several times. I'm going to push it all up to GitHub and wait for the redeploy. When the redeploy is complete, I'm going to navigate to the site. I'm just going to say, hey, and then what we see in the console is the data from the response object and look what we've got here, a completion. Hi there, how can I help you? Okay, that is big progress. We have just got one more step to take where we take this completion and get it rendered. Let's do that next. Okay, so the last thing that we need to do is deal with these two lines of code. So let's go straight into a challenge. I want you to update the two commented lines of code to get this working. Once you've done that, you can push to GitHub to redeploy and then test. Now, when this was purely a frontend project, we were taking our completion from the response. Now we've got the response stored in data. So it's just a question of working out where you get the completion from within the data object. Okay, pause now, get it sorted and we'll have a look together in just a moment. Okay, hopefully you got that working just fine. Now, these lines of code are almost correct, but now we have this data object and what we want is stored in the reply. So I'm going to delete response. And let's go for data.reply.choices and everything else can stay the same. Okay, let's copy that over to VS Code and I'll just update these two lines right here. Then we'll push to GitHub to trigger a redeploy and I'll ask the chatbot a question. We get back a completion, it's looking good, it's working and best of all, when we open up the network tab, there is absolutely no sign of our API key. So there we are. We have successfully deployed our open AI project to the live internet with the API key hidden. And now you can safely share your projects and use them in your portfolio without fear of your API key being compromised. Now you might be thinking, wait there, anyone can just access my serverless function. They can write their own fetch request to the URL, which of course is available on the front end. And then from there, they'll be able to use and abuse my open AI API key. Well, they can't, this endpoint is only going to accept fetch requests from its own domain. Now we can prove that right here in Scrimba because if we want to make a fetch request to this endpoint right here from inside a scrim, that should not be possible because scrims are hosted on scrimba.com and this endpoint is using my custom Netlify URL. Okay, let's hit save and I'm going to open up the console and I'll just say, hey, and there we are, we get an error. We are not able to make a fetch request to that URL from within Scrimba.com. And you can repeat that test from your own domain, check out the dev tools and look in detail at the error you get. Now, of course, if for some reason we wanted other domains to be able to access our endpoint, we would be able to do that with the cause policy. Cause is the cross origin resource sharing and it is quite a big topic, but if it's completely unfamiliar to you, I do recommend you read up on it at some point because it is really important when you start working with APIs and endpoints. Okay, we are done with this project. Let's just take one more scrim to recap what we've studied and talk about where you go next. I just want to say massive congratulations on finishing this course. You now have the AI foundations you need to tackle almost any web dev related AI task. Let's just have a quick look back at what we've studied. So we looked at how to use the open AI API. We used various models, including the GPT4 model, text DaVinci 003 and our own fine tuned model. We looked at prompt engineering, including the zero shot approach, the few shot approach and the specific object syntax needed for the create chat completions endpoints. We worked on building chatbots and on fine tuning a model using our own data. So the chatbot gave us answers specific to our circumstances. And of course, along the way, we've had loads of challenges. So where next for you and AI? Well, it would be great to fine tune with more data. You could get a fine tuned bot production ready. Now that will require you to find and organize a lot of data. Also keep an eye on the AI scene. There is a lot going on with text and images as we've seen here, but also with voice and of course with video. And in terms of how you use that in your personal projects, I recommend you find a need for AI and then build an app. So work on something that you've identified as a problem that needs solving and use AI to solve it. The best portfolio projects are unique to you. Now, whatever you do, why not head over to Scrimba's Discord server and go to the Today I Did channel. This screenshot is actually a link. It will take you straight there. And you can let everybody know that you finished the course and tell them how you got on. And when it comes to stretch goals in your own projects, why not share them in the I Built This channel. Show off your work and get great feedback from your peers. And lastly, do let me know how you got on with the course and show me what projects you've built. You can catch me right here on Twitter. And all that remains to be said is thank you very much for completing this course and I wish you all the very best.
