With timestamps:

00:00 - a little bit about myself before we
00:01 - start i'm from argentina uh this is my
00:04 - first conference outside south america
00:06 - as both as a speaker and as an attendee
00:08 - so i'm pretty excited about that
00:11 - i work coding and researching different
00:12 - things at movies
00:14 - we heard a booth so i talked with some
00:17 - of the guys here uh if you want to find
00:19 - me on the internet those are my github
00:20 - and trio profiles
00:23 - so if you're wondering a little bit
00:25 - about movie we are a software
00:26 - consultancy firm
00:28 - we are based in north in texas uruguay
00:31 - argentina and colombia
00:33 - we work on a lot of different
00:34 - technologies languages we build a lot of
00:37 - different things from mobile apps to
00:39 - websites and everything
00:41 - we have our first node.js react.js
00:44 - apprenticeship on austin
00:47 - we do that periodically so if you are
00:48 - from austin or you know someone from
00:50 - austin that might be interested in
00:51 - learning nobgs or reijs
00:54 - just check our site and see how that's
00:56 - going um that's our trio
00:59 - so before we start i would like to make
01:02 - one disclaimer uh this was originally a
01:05 - one hour long talk because a broad topic
01:07 - and i need to feed them in 25 minutes so
01:10 - bear with me on some things
01:11 - i hopefully will get through everything
01:14 - um
01:15 - so a lot of people ask me today why not
01:18 - js for machine learning especially
01:20 - because there are a lot of languages
01:22 - that are well known for being performed
01:25 - are
01:25 - they have better libraries for data data
01:28 - science
01:30 - you know python has tensorflow cycle
01:32 - learn air has a lot of
01:34 - math libraries as well as math lab
01:39 - uh my answer like personally was that
01:42 - after es6 came out i'm using javascript
01:44 - for everything
01:45 - uh all my prototypes and everything is
01:47 - javascript but if you want to you know
01:50 - talk to your boss about using javascript
01:52 - for machine learning you should go with
01:54 - this explanation
01:57 - we do web development
01:59 - so it makes sense for us to use node
02:01 - because we already using node we know
02:03 - like the ecosystem package management
02:06 - management manager and everything
02:08 - so
02:09 - it makes sense it was easier to import
02:11 - people because they already know es6
02:13 - they already know javascript
02:15 - and they already know everything but the
02:17 - framework and a little bit about the
02:19 - theory so you don't need to explain like
02:22 - tensorflow distributed computing or
02:24 - extra things that you don't really need
02:26 - for the case we all love javascript
02:28 - because we are all here at javascript
02:30 - conference
02:31 - node.js has a lot of modules that are
02:34 - great for different things like
02:37 - normalizing your data
02:39 - working without output
02:40 - a lot of just general things we have
02:42 - module for everything so that's great i
02:44 - don't know
02:45 - um the first one the right tool for the
02:47 - job well for me work because i'm not
02:50 - ingesting the amount of data that
02:52 - facebook or instagram are using or
02:55 - ingesting so for me makes sense uh it's
02:58 - working now in production some of my
02:59 - products uh i had recommendation systems
03:02 - and a lot of things working
03:04 - they're fine don't have any cpu issues
03:08 - much so
03:09 - that's pretty cool
03:12 - so yeah what's uh machine learning
03:14 - anyway
03:14 - i was trying to find like a definition
03:16 - of something to put in here and i found
03:18 - this quote from arthur samuels he's like
03:21 - a pioneer in
03:23 - artificial intelligence intelligence
03:25 - he said that it's a field of study that
03:28 - gives the computer the ability to learn
03:30 - without being explicit program
03:32 - and this code is interesting to me
03:34 - because it draws like the difference
03:36 - between what's called weak ai and strong
03:39 - ai
03:41 - weaker guides you can think of that as
03:43 - when you code like a
03:45 - program to play chess or checkers
03:48 - as a user you get the impression that
03:49 - the computer is actually thinking of how
03:51 - to beat you or he the computer will
03:53 - learn how to play but actually it's just
03:55 - following a set of steps that you
03:57 - pre-programmed like trying out like
03:59 - foreign forcing or whatever
04:01 - uh but strong ai it's when you program
04:04 - the mechanism for the computer to learn
04:06 - to do something
04:07 - um it's up to the computer what they do
04:10 - next so that's kind of of what's
04:13 - interesting
04:14 - and hard about machine learning uh i'm
04:17 - linking here to
04:19 - a paper for from arthur samuels about
04:23 - game theory and ai and that's a pretty
04:26 - good article about the difference
04:28 - between artificial intelligence and
04:29 - machine learning so if you want to check
04:31 - those out
04:33 - there are different kinds of learning
04:36 - the first one is on supervised learning
04:39 - this type of learning is when you have a
04:41 - bunch of data that you know nothing
04:43 - about that maybe you're mining from your
04:45 - application from other places and you
04:47 - just want to try to figure out a pattern
04:50 - or some way to grow that data for
04:52 - example maybe you have a bunch of data
04:54 - from your users like their weight and
04:57 - their head and you just
04:59 - move things around and then you figure
05:01 - hey so
05:02 - all these guys are
05:04 - size s for t-shirts these are guys this
05:07 - old set of guys are medium and so forth
05:10 - so that's kind of
05:11 - what this kind of learning is about
05:14 - it's pretty
05:15 - commonly used for
05:17 - data clustering and data mining
05:21 - and then we have supervised learning
05:23 - which is more early use which is
05:25 - basically learning by examples you will
05:27 - have some a set of data
05:30 - label data so you know what that data
05:32 - means and you're going to feed that data
05:34 - into your model and your model is going
05:36 - to try to figure out a function
05:39 - that can learn
05:41 - different patterns about that data so
05:44 - basically
05:45 - that will help you to solve a lot of
05:47 - different problems some problems that
05:49 - supervised learning solve
05:51 - its classification problems which is
05:54 - basically you taking an input and you
05:56 - need to classify that input among a set
05:59 - of labels for example an input being a
06:01 - picture and a set of labels being that's
06:04 - a hot dog it's not a hotdog
06:08 - our regression problems when you try to
06:10 - predict some continuous numeric output
06:13 - based on
06:14 - an input for example trying to figure
06:16 - out the price of a house based on the
06:19 - size of the house or
06:21 - trying to break your score in a test
06:23 - base based on your previous scores uh
06:26 - that's a regression problem
06:29 - there are a lot of different types of
06:31 - algorithms there are
06:33 - there's so and those are pretty popular
06:35 - but there are a bunch more uh i have
06:38 - been using all these in javascript uh
06:40 - for the first four uh key near network
06:43 - neighbors uh
06:45 - classification regression trees support
06:47 - vector machines name bias there is this
06:49 - package called machine underscore
06:51 - learning they have implementations of
06:53 - all of them so if you want to just play
06:55 - around you can download the package and
06:57 - throw inputs and see what happens if you
06:59 - are interested in how the algorithm
07:01 - works you can just go to the source and
07:03 - check the files they are
07:05 - fairly small and it's pretty interesting
07:08 - to just research and check how those
07:11 - are built and that paper it's about
07:14 - like a brief description of everyone
07:18 - every single algorithm i think there are
07:20 - more than those
07:22 - and why would you use one over the other
07:25 - and just like use cases in general
07:28 - just pretty cool
07:30 - so
07:32 - i'm time uh
07:34 - i'm going to try to do a brief
07:36 - introduction to neural networks in
07:38 - javascript
07:40 - i'm going to use this library for the
07:43 - examples it's called synaptic.js how
07:45 - many of you have heard about synaptic.js
07:49 - how many of you are using synaptic.js
07:51 - like
07:53 - okay
07:54 - so
07:55 - this library uh it's
07:57 - it's for me it's great it works on both
08:00 - the browser and node.js it's what's
08:02 - created by this guy
08:04 - juan casala which is from argentina
08:07 - he has a lot of examples uh pretty
08:11 - wiki with a lot of examples everything's
08:14 - pretty well documented so just getting
08:16 - there was
08:17 - just a brief getting from trying to
08:20 - learn machine learning in python and i
08:22 - was racing issues like asking super new
08:25 - questions and they're well super polite
08:27 - like yeah you need to do that or they
08:30 - were explaining everything to me i was
08:32 - pregnant so go and give him stars or
08:35 - asking asking questions okay there is
08:38 - another library called knit taptic which
08:40 - started as a fork of synaptic then they
08:43 - built their own thing using back
08:45 - propagation and no revolution so this is
08:48 - more for
08:50 - kind of gaming things
08:52 - it's more performant but if you just
08:54 - want to play around with things i would
08:56 - recommend start with synaptic and then
08:57 - move to knit a big
08:59 - so
09:01 - thing uh
09:03 - every time i think of neural networks
09:05 - that's the picture that everyone that
09:07 - has in his mind like about notes and
09:10 - arrows and
09:11 - that doesn't really explain what they do
09:14 - but what they basically do is uh split
09:17 - data into groups so what you're trying
09:20 - to get the computer to do with a neural
09:22 - network is he computer could you split
09:24 - this data in different groups it's like
09:27 - sure i splits the data into groups and
09:30 - then you have a new entry point of your
09:32 - data and you can just see where it fits
09:35 - and that's like the classification of
09:37 - the regulation problem
09:40 - so starting from like the basic unit of
09:44 - a neural network
09:46 - obviously it's a neuron
09:47 - i won't draw the parallelism between
09:50 - this kind of neuron and the real neurons
09:52 - because i really don't know how neurons
09:55 - actually work but i don't know i know
09:56 - how this works so
09:58 - uh
09:59 - yeah a neuron basically is composed of
10:01 - different parts we have these arrows
10:04 - coming into the neuron which is the
10:06 - red dot right there
10:08 - these these arrows are called
10:10 - synapses
10:12 - uh they could be
10:14 - an output from another neuron or just
10:16 - your input every single one of these
10:18 - synapses or the arrows
10:20 - are a value which is your input and they
10:23 - are all multiplied by a weight that way
10:26 - at first it's random
10:28 - and the whole
10:30 - thing of machine learning especially on
10:32 - neural network it's just finding the
10:34 - weights
10:34 - uh that makes sense for your data to get
10:37 - the input the output that you want uh
10:40 - then
10:41 - all neural networks besides their inputs
10:44 - they have one extra input called a bias
10:47 - which is always one and it has its own
10:49 - weight
10:50 - uh and this bias what it does is ensure
10:54 - that even
10:55 - if all the inputs are zero you're still
10:58 - going to get some kind of output
11:00 - that's why that's a one
11:02 - it has also random weight assigned to
11:04 - that
11:05 - then the neuron has a
11:07 - an activation an activation function
11:09 - that we're going to see a couple of
11:10 - slides uh after this one
11:12 - and then you have the synapses going out
11:15 - of the neuron that could be your final
11:17 - output or it could be the input of the
11:19 - next one
11:21 - so
11:22 - the activation functions uh what they do
11:25 - basically is grabbing all the inputs
11:27 - from the neuron they
11:30 - add them together the inputs of the
11:32 - neuron are basically your inputs or the
11:34 - input of the of the or the output of the
11:36 - previous neuron multiplied by the weight
11:40 - and the activation function grabs all
11:42 - the inputs add them together and then
11:44 - use that as an input for
11:46 - for itself
11:48 - synaptic.js by default uses logistic
11:50 - sigmoid functions
11:52 - which basically
11:54 - maps every value that you use as an
11:57 - input between
11:58 - 0 and 1
12:00 - that's basically what it does if you
12:02 - want to know more about like the theory
12:04 - uh i linked some papers there and some
12:09 - an answer in quora that was pretty well
12:12 - explained
12:13 - there are some more uh activation
12:14 - functions i'm playing with rectified
12:17 - linear unit now a lot i didn't use the
12:20 - other two and i know neptopic has even
12:23 - more but
12:25 - sigma is
12:27 - pretty popular and you don't really need
12:29 - to change that
12:30 - unless you you notice that your use case
12:33 - is not like working so you can play
12:35 - around with the functions and trying to
12:37 - see what it's better for your use case
12:40 - but starting with sigma it's totally
12:42 - fine
12:44 - so
12:45 - a neuron can perform these four
12:47 - different operations
12:49 - they can project they can activate they
12:51 - can propagate and then gate project is
12:54 - basically
12:55 - setting a synapse between two neurons
12:57 - that's the red one it's projected into
13:00 - the blue one
13:01 - they can activate that's when they grab
13:03 - all the inputs add them together and
13:06 - call the sigmoid function with that
13:08 - value
13:09 - uh they can propagate which is the
13:12 - actual training of the of the network
13:15 - that's when you
13:16 - uh you said uh you get an output from
13:19 - the neuron
13:20 - that wasn't what you expected so you
13:22 - propagate back the error based on what
13:25 - you really expect though and that's
13:27 - going to change the weights of all the
13:29 - synapses
13:30 - back uh like on the uh to the first
13:33 - input neuron and that's what's called
13:36 - back propagation and that's how the
13:38 - network learns
13:39 - and finally they can gate that's an
13:41 - operation that's not pretty common it's
13:43 - just for one kind of neural network that
13:46 - if we have time we're going to see later
13:49 - but yeah
13:51 - this is a small example on synaptic how
13:54 - you can create this neural network based
13:56 - on two neurons
13:57 - we create two neurons a and b we project
13:59 - a to b
14:01 - that's the synapses between those and
14:03 - then b has an output then we activate a
14:06 - with 0.5
14:09 - then when we do that
14:11 - a sends through the synapse 0.5 times
14:15 - random weight first and then we activate
14:17 - b
14:18 - b grabs that
14:19 - output from a
14:20 - performs the sigma activation function
14:23 - and we get like a lot of random data
14:25 - because we are not really doing anything
14:28 - so if we wanted to train
14:30 - uh b to output zero every time that a
14:34 - activates with one this is how we do it
14:37 - uh we
14:39 - do the same thing that before create the
14:40 - two neurons we project one two to the
14:43 - other one we set up a learning rate that
14:45 - will be important for all propagation
14:48 - and then we have a for loop
14:50 - uh that that's our training session this
14:53 - training session has 20 000
14:56 - iterations
14:58 - and on each iterations does the same
15:00 - thing because a activate one
15:03 - then activates b we don't care what b is
15:06 - at this moment
15:07 - like what the output of b is
15:09 - because we are going to propagate that
15:12 - error so we say every time we activate a
15:15 - a uh you should propagate b with zero so
15:18 - what we are saying here is every time
15:20 - that a is one b should be zero and the
15:23 - propagate will change the way it's like
15:26 - 20 000 times until we get closer and
15:28 - closer to zero so when the training
15:30 - session session ends we activate a we
15:32 - want and we get that something pretty
15:35 - close to zero that you could do a math
15:37 - from and that's
15:39 - the training of your first two neuron
15:41 - based uh neural network
15:45 - uh obviously on real neural networks you
15:48 - will need like a bunch of different
15:50 - neurons and layers and this is all our
15:53 - real neural network looks uh you can see
15:56 - on like why um who why you should use a
16:00 - number of
16:01 - uh hidden layers and how many neutrons
16:04 - uh the hidden layers should have based
16:06 - on those there is a paper right there
16:09 - and a
16:10 - stack exchange link that explains that
16:13 - very well
16:15 - so let's
16:17 - build a small neural network
16:20 - as an example uh you are i guess all
16:23 - familiar with the xor operation uh what
16:26 - it does basically it takes two inputs if
16:29 - the two inputs are
16:30 - equal uh they output zero they are
16:33 - different they output one so we are
16:35 - going to train a neural network to
16:36 - perform this operation
16:39 - so based on this uh on this picture we
16:43 - can say okay so we have two inputs
16:47 - on our vector so it's going to be two
16:49 - neurons on the input layer
16:52 - we have one output on our vector so it's
16:56 - going to be one neuron on the output
16:58 - layer
16:59 - and then
17:00 - we usually
17:01 - have at least one
17:03 - hidden layer
17:04 - you could not have any but that's just
17:06 - for super simple classification problems
17:09 - and at that point you might not need
17:11 - neural networks i mean you can use them
17:13 - but
17:14 - it defeats the problems but yeah let's
17:16 - use one hand layer and the rule of thumb
17:19 - for selecting how many neurons should be
17:21 - in the handle layer and the hidden layer
17:23 - it's basically the mean between the
17:26 - neurons on the output
17:28 - layer and the input layer so in this
17:30 - case two
17:32 - so
17:33 - uh going on synaptic again
17:36 - we import the layer object the number of
17:38 - in the network object we create
17:41 - all three layers
17:43 - uh ignore that three right there there
17:45 - should be a two
17:46 - what yeah
17:48 - then we project one
17:51 - layer to the other one
17:52 - uh
17:54 - the input layer to the
17:55 - height and layer the hidden layer to the
17:57 - output layer
17:58 - and then we create the network using the
18:00 - network structure
18:02 - so we have network it looks something
18:04 - like this to when it runs from the input
18:07 - one of the on the output and or hidden
18:09 - layer
18:11 - and this is how our training looks
18:13 - we are going to use a learning rate of
18:15 - 0.3 this is like my starting point for
18:18 - every learning rate then i tweak that i
18:20 - play around with that until i get the
18:22 - output that i want and that's
18:24 - kind of how it works
18:26 - unless you have a phd and you know what
18:27 - you're doing
18:28 - but that's how i do it
18:30 - uh
18:31 - well again we have training session of
18:34 - uh 20 000
18:37 - iterations and we say okay every time
18:39 - that we activate our network with zero
18:42 - and zero i'm expecting a zero
18:45 - zero one i'm expecting a one and so for
18:47 - it and that will loop uh twenty thousand
18:50 - times
18:51 - and now if i can get
18:55 - console torque i can do
18:56 - [Music]
18:59 - uh this is
19:01 - for example uh how it works it's pretty
19:03 - fast uh it's a small example and you can
19:06 - just
19:08 - play around with that like saying 101
19:12 - and i will put someone uh yeah i will
19:15 - upload the code for for the examples uh
19:17 - later on my github
19:19 - but yeah this is how you can build a
19:21 - simple neural network uh one thing that
19:23 - you might notice is that all the inputs
19:26 - are zero numbers between zero and one so
19:29 - if you want to use like real data that
19:31 - you probably want to
19:33 - you need to perform normalization that
19:35 - means
19:36 - all your data should be mapped into
19:38 - values between zero and one uh i have an
19:41 - example for that but i had to put that
19:43 - up so go to synaptic they have a great
19:46 - guide about data normalization
19:48 - um
19:50 - yeah
19:51 - there are some other types of
19:53 - networks
19:54 - uh we have convolutional neural networks
19:57 - these are
19:58 - mostly used for image recognition
20:00 - uh because if you try to use a regular
20:03 - neural network like
20:05 - this one
20:06 - uh
20:07 - like with an image you will split the
20:09 - image into pixels but then
20:11 - uh the neural network won't have the
20:13 - context to make sense of those pixels
20:16 - because it needs a special con
20:19 - spatial context
20:21 - so
20:22 - that's why what this uh neural network
20:24 - brings into into the table you can see
20:27 - that link
20:28 - explains everything pretty well uh this
20:31 - is based on how the animals actually
20:34 - see pictures so it's pretty interesting
20:36 - you are into those things then you have
20:39 - recurrent neural networks these are
20:41 - neural networks that uh feeds themselves
20:44 - with their their own output so this is
20:48 - for trying to find patterns on sequence
20:51 - of data
20:52 - uh basically
20:54 - uh if you are like trying to teach the
20:57 - neural network the alphabet you will
20:58 - need to know which letter was before the
21:01 - one that you are doing now and that's
21:03 - that loop does that that gives that
21:06 - little context
21:08 - then you have uh
21:09 - another type of
21:12 - neural network that it's still recurrent
21:14 - but this one keeps uh it just that gate
21:17 - function on the neurons and it keeps uh
21:20 - track of things that happen long time
21:22 - ago this is used when you want to teach
21:25 - like a neural network to write like
21:27 - harry potter you can make the network
21:29 - like read all the books like 20 000
21:32 - times and then it's going to remember
21:34 - yeah so every
21:36 - five
21:37 - characters i use a space i say hurry
21:40 - this many times
21:41 - between each word and there are a lot of
21:44 - cool experiments with that you can just
21:46 - go into that league and see there are a
21:47 - lot of people doing little
21:49 - interesting things with this kind of
21:51 - network
21:53 - so i have a couple of final notes
21:57 - this is like my main note uh i
22:00 - experiment a lot with node.js and just
22:03 - with every model that i found and i
22:06 - really like to encourage people to do
22:07 - the same because i didn't know that we
22:10 - had so many modules for machine learning
22:12 - until i started like the nickname on npm
22:16 - so
22:17 - i think uh everyone should just go and
22:19 - play around with synaptic and you know
22:21 - make your co-workers aware that that
22:23 - exists because maybe they want to learn
22:25 - about this and they don't want to learn
22:27 - python or math or tensorflow
22:31 - so
22:31 - now it's pretty cool uh and also there
22:34 - is a quote that i heard on the ireland
22:36 - conf a couple of weeks ago saying that
22:38 - if you are employing people give you
22:41 - should give them time to experiment
22:43 - because otherwise they are going to
22:44 - experience production you don't want
22:46 - that so
22:48 - that's another note
22:49 - um
22:51 - i know i there are people that's
22:53 - concerned about the performance of
22:55 - javascript
22:56 - in all these things
22:58 - so my answer is check gpujs hamsterjs
23:02 - gpujs
23:03 - self-explanatory
23:05 - you can access the cpu the gpu with
23:08 - javascript
23:09 - that solve a lot of my problems with
23:12 - like huge amounts of training data i
23:15 - just connect the gpu and you said
23:17 - if you don't have gpu you can use
23:19 - hamster.js which is a native trading for
23:23 - node.js
23:24 - that's also solved a couple of problems
23:26 - for me
23:27 - and
23:28 - yeah if you want to play around with
23:30 - neural networks and you don't know what
23:31 - to do there are libraries for iot like
23:33 - journey five cylon and you can use that
23:36 - as inputs on your neural network and
23:38 - start doing like weird things
23:40 - you know
23:42 - and there is robot.js which is
23:43 - automation for the desktop on node.js
23:46 - you know you can just feed a recurring
23:50 - network with the movements of your mouse
23:52 - and try to regret your session i mean
23:54 - you can experiment with a lot of modules
23:56 - so that's pretty cool
23:58 - and
24:00 - like last thing
24:02 - i have a lot of meetings with clients
24:04 - that they say yeah i want this
24:07 - machine learning algorithm to predict uh
24:09 - stock trades or you know to be rich or
24:12 - something it's like yeah that's not how
24:15 - it works i mean your pro when you're
24:17 - working machine learning your product is
24:19 - not
24:19 - actually the
24:21 - machine learning algorithm because
24:22 - everything is open source
24:24 - there are papers for everything if you
24:26 - want to build a neural network that
24:28 - learns to play music there is a paper
24:29 - for that there is at least a hundred
24:31 - guys that implemented that in different
24:33 - ways
24:34 - so the product is the data if you don't
24:36 - have data then they will it's just not
24:39 - going to work and it's going to work
24:41 - poorly so if you are working on machine
24:44 - learning things
24:45 - your real product it's the
24:47 - the data
24:48 - uh my last advice follow this guy he
24:51 - posts a lot of things for machine
24:53 - learning for normal people like me so i
24:56 - understand that so you can understand
24:58 - that too so thank you for having me

Cleaned transcript:

a little bit about myself before we start i'm from argentina uh this is my first conference outside south america as both as a speaker and as an attendee so i'm pretty excited about that i work coding and researching different things at movies we heard a booth so i talked with some of the guys here uh if you want to find me on the internet those are my github and trio profiles so if you're wondering a little bit about movie we are a software consultancy firm we are based in north in texas uruguay argentina and colombia we work on a lot of different technologies languages we build a lot of different things from mobile apps to websites and everything we have our first node.js react.js apprenticeship on austin we do that periodically so if you are from austin or you know someone from austin that might be interested in learning nobgs or reijs just check our site and see how that's going um that's our trio so before we start i would like to make one disclaimer uh this was originally a one hour long talk because a broad topic and i need to feed them in 25 minutes so bear with me on some things i hopefully will get through everything um so a lot of people ask me today why not js for machine learning especially because there are a lot of languages that are well known for being performed are they have better libraries for data data science you know python has tensorflow cycle learn air has a lot of math libraries as well as math lab uh my answer like personally was that after es6 came out i'm using javascript for everything uh all my prototypes and everything is javascript but if you want to you know talk to your boss about using javascript for machine learning you should go with this explanation we do web development so it makes sense for us to use node because we already using node we know like the ecosystem package management management manager and everything so it makes sense it was easier to import people because they already know es6 they already know javascript and they already know everything but the framework and a little bit about the theory so you don't need to explain like tensorflow distributed computing or extra things that you don't really need for the case we all love javascript because we are all here at javascript conference node.js has a lot of modules that are great for different things like normalizing your data working without output a lot of just general things we have module for everything so that's great i don't know um the first one the right tool for the job well for me work because i'm not ingesting the amount of data that facebook or instagram are using or ingesting so for me makes sense uh it's working now in production some of my products uh i had recommendation systems and a lot of things working they're fine don't have any cpu issues much so that's pretty cool so yeah what's uh machine learning anyway i was trying to find like a definition of something to put in here and i found this quote from arthur samuels he's like a pioneer in artificial intelligence intelligence he said that it's a field of study that gives the computer the ability to learn without being explicit program and this code is interesting to me because it draws like the difference between what's called weak ai and strong ai weaker guides you can think of that as when you code like a program to play chess or checkers as a user you get the impression that the computer is actually thinking of how to beat you or he the computer will learn how to play but actually it's just following a set of steps that you preprogrammed like trying out like foreign forcing or whatever uh but strong ai it's when you program the mechanism for the computer to learn to do something um it's up to the computer what they do next so that's kind of of what's interesting and hard about machine learning uh i'm linking here to a paper for from arthur samuels about game theory and ai and that's a pretty good article about the difference between artificial intelligence and machine learning so if you want to check those out there are different kinds of learning the first one is on supervised learning this type of learning is when you have a bunch of data that you know nothing about that maybe you're mining from your application from other places and you just want to try to figure out a pattern or some way to grow that data for example maybe you have a bunch of data from your users like their weight and their head and you just move things around and then you figure hey so all these guys are size s for tshirts these are guys this old set of guys are medium and so forth so that's kind of what this kind of learning is about it's pretty commonly used for data clustering and data mining and then we have supervised learning which is more early use which is basically learning by examples you will have some a set of data label data so you know what that data means and you're going to feed that data into your model and your model is going to try to figure out a function that can learn different patterns about that data so basically that will help you to solve a lot of different problems some problems that supervised learning solve its classification problems which is basically you taking an input and you need to classify that input among a set of labels for example an input being a picture and a set of labels being that's a hot dog it's not a hotdog our regression problems when you try to predict some continuous numeric output based on an input for example trying to figure out the price of a house based on the size of the house or trying to break your score in a test base based on your previous scores uh that's a regression problem there are a lot of different types of algorithms there are there's so and those are pretty popular but there are a bunch more uh i have been using all these in javascript uh for the first four uh key near network neighbors uh classification regression trees support vector machines name bias there is this package called machine underscore learning they have implementations of all of them so if you want to just play around you can download the package and throw inputs and see what happens if you are interested in how the algorithm works you can just go to the source and check the files they are fairly small and it's pretty interesting to just research and check how those are built and that paper it's about like a brief description of everyone every single algorithm i think there are more than those and why would you use one over the other and just like use cases in general just pretty cool so i'm time uh i'm going to try to do a brief introduction to neural networks in javascript i'm going to use this library for the examples it's called synaptic.js how many of you have heard about synaptic.js how many of you are using synaptic.js like okay so this library uh it's it's for me it's great it works on both the browser and node.js it's what's created by this guy juan casala which is from argentina he has a lot of examples uh pretty wiki with a lot of examples everything's pretty well documented so just getting there was just a brief getting from trying to learn machine learning in python and i was racing issues like asking super new questions and they're well super polite like yeah you need to do that or they were explaining everything to me i was pregnant so go and give him stars or asking asking questions okay there is another library called knit taptic which started as a fork of synaptic then they built their own thing using back propagation and no revolution so this is more for kind of gaming things it's more performant but if you just want to play around with things i would recommend start with synaptic and then move to knit a big so thing uh every time i think of neural networks that's the picture that everyone that has in his mind like about notes and arrows and that doesn't really explain what they do but what they basically do is uh split data into groups so what you're trying to get the computer to do with a neural network is he computer could you split this data in different groups it's like sure i splits the data into groups and then you have a new entry point of your data and you can just see where it fits and that's like the classification of the regulation problem so starting from like the basic unit of a neural network obviously it's a neuron i won't draw the parallelism between this kind of neuron and the real neurons because i really don't know how neurons actually work but i don't know i know how this works so uh yeah a neuron basically is composed of different parts we have these arrows coming into the neuron which is the red dot right there these these arrows are called synapses uh they could be an output from another neuron or just your input every single one of these synapses or the arrows are a value which is your input and they are all multiplied by a weight that way at first it's random and the whole thing of machine learning especially on neural network it's just finding the weights uh that makes sense for your data to get the input the output that you want uh then all neural networks besides their inputs they have one extra input called a bias which is always one and it has its own weight uh and this bias what it does is ensure that even if all the inputs are zero you're still going to get some kind of output that's why that's a one it has also random weight assigned to that then the neuron has a an activation an activation function that we're going to see a couple of slides uh after this one and then you have the synapses going out of the neuron that could be your final output or it could be the input of the next one so the activation functions uh what they do basically is grabbing all the inputs from the neuron they add them together the inputs of the neuron are basically your inputs or the input of the of the or the output of the previous neuron multiplied by the weight and the activation function grabs all the inputs add them together and then use that as an input for for itself synaptic.js by default uses logistic sigmoid functions which basically maps every value that you use as an input between 0 and 1 that's basically what it does if you want to know more about like the theory uh i linked some papers there and some an answer in quora that was pretty well explained there are some more uh activation functions i'm playing with rectified linear unit now a lot i didn't use the other two and i know neptopic has even more but sigma is pretty popular and you don't really need to change that unless you you notice that your use case is not like working so you can play around with the functions and trying to see what it's better for your use case but starting with sigma it's totally fine so a neuron can perform these four different operations they can project they can activate they can propagate and then gate project is basically setting a synapse between two neurons that's the red one it's projected into the blue one they can activate that's when they grab all the inputs add them together and call the sigmoid function with that value uh they can propagate which is the actual training of the of the network that's when you uh you said uh you get an output from the neuron that wasn't what you expected so you propagate back the error based on what you really expect though and that's going to change the weights of all the synapses back uh like on the uh to the first input neuron and that's what's called back propagation and that's how the network learns and finally they can gate that's an operation that's not pretty common it's just for one kind of neural network that if we have time we're going to see later but yeah this is a small example on synaptic how you can create this neural network based on two neurons we create two neurons a and b we project a to b that's the synapses between those and then b has an output then we activate a with 0.5 then when we do that a sends through the synapse 0.5 times random weight first and then we activate b b grabs that output from a performs the sigma activation function and we get like a lot of random data because we are not really doing anything so if we wanted to train uh b to output zero every time that a activates with one this is how we do it uh we do the same thing that before create the two neurons we project one two to the other one we set up a learning rate that will be important for all propagation and then we have a for loop uh that that's our training session this training session has 20 000 iterations and on each iterations does the same thing because a activate one then activates b we don't care what b is at this moment like what the output of b is because we are going to propagate that error so we say every time we activate a a uh you should propagate b with zero so what we are saying here is every time that a is one b should be zero and the propagate will change the way it's like 20 000 times until we get closer and closer to zero so when the training session session ends we activate a we want and we get that something pretty close to zero that you could do a math from and that's the training of your first two neuron based uh neural network uh obviously on real neural networks you will need like a bunch of different neurons and layers and this is all our real neural network looks uh you can see on like why um who why you should use a number of uh hidden layers and how many neutrons uh the hidden layers should have based on those there is a paper right there and a stack exchange link that explains that very well so let's build a small neural network as an example uh you are i guess all familiar with the xor operation uh what it does basically it takes two inputs if the two inputs are equal uh they output zero they are different they output one so we are going to train a neural network to perform this operation so based on this uh on this picture we can say okay so we have two inputs on our vector so it's going to be two neurons on the input layer we have one output on our vector so it's going to be one neuron on the output layer and then we usually have at least one hidden layer you could not have any but that's just for super simple classification problems and at that point you might not need neural networks i mean you can use them but it defeats the problems but yeah let's use one hand layer and the rule of thumb for selecting how many neurons should be in the handle layer and the hidden layer it's basically the mean between the neurons on the output layer and the input layer so in this case two so uh going on synaptic again we import the layer object the number of in the network object we create all three layers uh ignore that three right there there should be a two what yeah then we project one layer to the other one uh the input layer to the height and layer the hidden layer to the output layer and then we create the network using the network structure so we have network it looks something like this to when it runs from the input one of the on the output and or hidden layer and this is how our training looks we are going to use a learning rate of 0.3 this is like my starting point for every learning rate then i tweak that i play around with that until i get the output that i want and that's kind of how it works unless you have a phd and you know what you're doing but that's how i do it uh well again we have training session of uh 20 000 iterations and we say okay every time that we activate our network with zero and zero i'm expecting a zero zero one i'm expecting a one and so for it and that will loop uh twenty thousand times and now if i can get console torque i can do uh this is for example uh how it works it's pretty fast uh it's a small example and you can just play around with that like saying 101 and i will put someone uh yeah i will upload the code for for the examples uh later on my github but yeah this is how you can build a simple neural network uh one thing that you might notice is that all the inputs are zero numbers between zero and one so if you want to use like real data that you probably want to you need to perform normalization that means all your data should be mapped into values between zero and one uh i have an example for that but i had to put that up so go to synaptic they have a great guide about data normalization um yeah there are some other types of networks uh we have convolutional neural networks these are mostly used for image recognition uh because if you try to use a regular neural network like this one uh like with an image you will split the image into pixels but then uh the neural network won't have the context to make sense of those pixels because it needs a special con spatial context so that's why what this uh neural network brings into into the table you can see that link explains everything pretty well uh this is based on how the animals actually see pictures so it's pretty interesting you are into those things then you have recurrent neural networks these are neural networks that uh feeds themselves with their their own output so this is for trying to find patterns on sequence of data uh basically uh if you are like trying to teach the neural network the alphabet you will need to know which letter was before the one that you are doing now and that's that loop does that that gives that little context then you have uh another type of neural network that it's still recurrent but this one keeps uh it just that gate function on the neurons and it keeps uh track of things that happen long time ago this is used when you want to teach like a neural network to write like harry potter you can make the network like read all the books like 20 000 times and then it's going to remember yeah so every five characters i use a space i say hurry this many times between each word and there are a lot of cool experiments with that you can just go into that league and see there are a lot of people doing little interesting things with this kind of network so i have a couple of final notes this is like my main note uh i experiment a lot with node.js and just with every model that i found and i really like to encourage people to do the same because i didn't know that we had so many modules for machine learning until i started like the nickname on npm so i think uh everyone should just go and play around with synaptic and you know make your coworkers aware that that exists because maybe they want to learn about this and they don't want to learn python or math or tensorflow so now it's pretty cool uh and also there is a quote that i heard on the ireland conf a couple of weeks ago saying that if you are employing people give you should give them time to experiment because otherwise they are going to experience production you don't want that so that's another note um i know i there are people that's concerned about the performance of javascript in all these things so my answer is check gpujs hamsterjs gpujs selfexplanatory you can access the cpu the gpu with javascript that solve a lot of my problems with like huge amounts of training data i just connect the gpu and you said if you don't have gpu you can use hamster.js which is a native trading for node.js that's also solved a couple of problems for me and yeah if you want to play around with neural networks and you don't know what to do there are libraries for iot like journey five cylon and you can use that as inputs on your neural network and start doing like weird things you know and there is robot.js which is automation for the desktop on node.js you know you can just feed a recurring network with the movements of your mouse and try to regret your session i mean you can experiment with a lot of modules so that's pretty cool and like last thing i have a lot of meetings with clients that they say yeah i want this machine learning algorithm to predict uh stock trades or you know to be rich or something it's like yeah that's not how it works i mean your pro when you're working machine learning your product is not actually the machine learning algorithm because everything is open source there are papers for everything if you want to build a neural network that learns to play music there is a paper for that there is at least a hundred guys that implemented that in different ways so the product is the data if you don't have data then they will it's just not going to work and it's going to work poorly so if you are working on machine learning things your real product it's the the data uh my last advice follow this guy he posts a lot of things for machine learning for normal people like me so i understand that so you can understand that too so thank you for having me
