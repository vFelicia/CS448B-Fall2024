With timestamps:

00:00 - you've probably read in the news that
00:01 - deep learning is the secret recipe
00:03 - behind many exciting developments and
00:05 - has made many of our world's dreams and
00:07 - perhaps also nightmares come true
00:09 - who would have thought that deep minds
00:11 - alphago could be at least a doll in a
00:13 - boat game which boasts in more possible
00:14 - moves than there are atoms in the entire
00:16 - universe
00:17 - a lot of people including me never saw
00:19 - it coming it seemed impossible but it's
00:22 - here now deep learning is everywhere
00:24 - it's beating physicians are diagnosing
00:26 - cancer it's responsible for translating
00:28 - web pages in a matter of mere seconds to
00:30 - the autonomous vehicles by weimo and
00:32 - tesla
00:34 - hi my name is jason and welcome to this
00:36 - course in deep learning where you'll
00:38 - learn everything you need to get started
00:39 - with deep learning in python how to
00:41 - build remarkable algorithms capable of
00:43 - solving complex problems that weren't
00:45 - possible just a few decades ago we'll
00:47 - talk about what deep learning is and the
00:49 - difference between artificial
00:51 - intelligence and machine learning i'll
00:53 - introduce new networks what they are and
00:55 - just how essential they are to deep
00:56 - learning you're going to learn about how
00:58 - deep learning models train and learn and
01:01 - the various types of learning associated
01:03 - supervised unsupervised and
01:04 - reinforcement learning we're going to
01:06 - talk about loss functions optimizers the
01:08 - grading descent algorithm the different
01:10 - types of neural network architectures
01:12 - and the various steps involved in deep
01:14 - learning
01:15 - this entire course is centered on the
01:17 - notion of deep learning but what is it
01:20 - deep learning is a subset of machine
01:22 - learning which in turn is a subset of
01:24 - artificial intelligence which involves
01:26 - more traditional methods to learn
01:27 - representations directly from data
01:30 - machine learning involves teaching
01:32 - computers to recognize patterns in data
01:34 - in the same way as our brains do so as
01:37 - humans it's easy for us to distinguish
01:39 - between a cat and a dog but it's much
01:41 - more difficult to teach a machine to do
01:43 - this and we'll talk more about this
01:44 - later on in this course before i do that
01:47 - i want to give you a sense of the
01:48 - amazing successes of deep learning in
01:50 - the past
01:51 - in 1997 gary kasparov the most
01:54 - successful champion in the history of
01:55 - chess lost to ibm's deep blue one of the
01:59 - first computer or artificial systems
02:01 - it was the first defeat of a reigning
02:03 - world chess champion by a computer
02:05 - in 2011 ibm's watson competed in game
02:09 - show jeopardy against his champions brad
02:12 - rotter and ken jennings and won the
02:14 - first prize a million dollars
02:16 - in 2015 alphago a deep learning computer
02:19 - program created by google's deepmind
02:21 - division defeated lisa dole an 18 time
02:24 - world champion at go a game of google
02:27 - more times complex than chess
02:29 - but deep learning can do more than just
02:31 - betas at boat games it finds
02:33 - applications anywhere from self-driving
02:35 - vehicles to fake news detection to even
02:37 - predicting earthquakes
02:39 - these were astonishing moments not only
02:41 - because machines beat humans at their
02:43 - own games but because of the endless
02:45 - possibilities that they opened up
02:47 - what followed such events have been a
02:49 - series of striking breakthroughs in
02:51 - artificial intelligence machine learning
02:53 - and yes deep learning
02:55 - to put it simply deep learning is a
02:57 - machine learning technique that learns
02:59 - features and tasks directly from data by
03:01 - running inputs through a biologically
03:03 - inspired neural network architecture
03:06 - these neural networks contain a number
03:08 - of hidden layers through which data is
03:10 - processed allowing for the machine to go
03:12 - deep in its learning making connections
03:14 - and weighing input for the best results
03:16 - we'll go over neural networks in the
03:18 - next video so why deep learning
03:21 - the problem with traditional machine
03:22 - learning algorithms is that no matter
03:24 - how complex they get they'll always be
03:27 - machine like they need a lot of domain
03:29 - expertise human intervention and are
03:31 - only capable of what they're designed
03:33 - for
03:34 - for example if i show you the image of a
03:36 - face you will automatically recognize
03:38 - it's a face
03:39 - but how would a computer know what this
03:40 - is well if we follow traditional machine
03:43 - learning we'd have to manually and
03:45 - painstakingly define to a computer what
03:47 - it faces for example it has eyes ears
03:50 - and mouth but now how do you define an
03:52 - eye or a mouth to a computer well if you
03:54 - look at an eye the corners are at some
03:57 - angle they're definitely not 90 degrees
03:59 - they're definitely not zero degrees
04:01 - there's some angle in between so we
04:03 - could work with that and train our
04:04 - classifier to recognize these kinds of
04:06 - lines in certain orientations
04:09 - this is complicated
04:11 - for ei practitioners and the rest of the
04:12 - world that's where deep learning holds a
04:14 - bit of promise
04:16 - the key idea in deep learning is that
04:17 - you can learn these features just from
04:19 - raw data so i can feed a bunch of images
04:22 - or faces to my deep learning algorithm
04:24 - and it's going to develop some kind of
04:26 - hierarchical representation of detecting
04:28 - lines and edges and then using these
04:30 - lines and edges to detect eyes and a
04:32 - mouth and composing it together to
04:34 - ultimately detect the face
04:36 - as it turns out the underlying
04:38 - algorithms for training these models
04:40 - have existed for quite a long time
04:42 - so why has deep learning gaining
04:44 - popularity many decades later
04:46 - well for one data has become much more
04:49 - pervasive we're living in the age of big
04:51 - data and these algorithms require
04:53 - massive amounts of data to effectively
04:55 - be implemented
04:57 - second we have hardware and architecture
04:59 - that are capable of handling the vast
05:01 - amount of data and computational power
05:04 - that these algorithms require hardware
05:06 - that simply wasn't available a few
05:08 - decades ago
05:09 - third building and deploying these
05:11 - algorithms models as i call is extremely
05:14 - streamlined with the increasing
05:15 - popularity of open source software like
05:18 - tensorflow and pytorch
05:23 - deep learning models refer to the
05:25 - training of things called neural
05:26 - networks
05:27 - neural networks form the basis of deep
05:29 - learning a sub-field of machine learning
05:31 - where algorithms are inspired by the
05:33 - structure of the human brain
05:35 - just like neurons make up the brain the
05:37 - fundamental building blocks of a neural
05:39 - network is also a neuron
05:42 - neural networks take in data they train
05:44 - themselves to recognize patterns in this
05:45 - data and predict outputs for a new set
05:48 - of similar data
05:50 - in a new network information propagates
05:52 - through three central components that
05:54 - form the basis of every neural network
05:56 - architecture the input layer the output
05:59 - layer and several hidden layers between
06:01 - the two
06:03 - in the next video we'll go over the
06:04 - learning process of a neural network
06:09 - the learning process of a neural network
06:11 - can be broken into two main processes
06:13 - forward propagation and back propagation
06:16 - full propagation is the propagation of
06:18 - information from the input layer to the
06:20 - output layer we can define our input
06:23 - layer as several neurons x1 through xn
06:26 - these neurons connect to the neurons of
06:28 - the next layer through channels and they
06:31 - are assigned numerical values called
06:32 - weights the inputs are multiplied to the
06:35 - weights and their sum is sent as input
06:38 - to the neurons in the hidden layer where
06:40 - each neuron in turn is associated to a
06:42 - numerical value called the bias which is
06:45 - then added to the input sum
06:48 - this weighted sum is then passed through
06:50 - a non-linear function called the
06:51 - activation function which essentially
06:54 - decides if that particular neuron can
06:55 - contribute to the next layer
06:58 - in the output layer it's basically a
06:59 - form of probability the neuron with the
07:01 - highest value determines what the output
07:03 - finally is
07:05 - so let's go over a few terms
07:07 - the weight of a neuron tells us how
07:09 - important the neuron is the higher the
07:11 - value the more important it is in the
07:13 - relationship the bias is like the new on
07:16 - having an opinion to the relationship it
07:18 - serves to shift the activation function
07:20 - to the right or to the left if you have
07:22 - had some experience with high school
07:24 - math you should know that adding a
07:26 - scalar value to a function shifts a
07:28 - graph either to the left or to the right
07:31 - and this is exactly what the bias does
07:32 - it shifts the activation function to the
07:34 - right or to the left that propagation is
07:37 - almost like for propagation except in
07:39 - the reverse direction information here
07:41 - is passed from the output layer to the
07:43 - hidden layers not the input layer
07:46 - but what information gets passed on from
07:48 - the output layer isn't the output layer
07:51 - supposed to be the final layer where we
07:52 - get the final output
07:55 - well
07:56 - yes but no bad propagation is the reason
07:59 - why new networks is so powerful it is
08:02 - the reason why new networks can learn by
08:04 - themselves
08:05 - in the last step before propagation a
08:07 - new network spits out a prediction
08:10 - this prediction could have two
08:11 - possibilities either right or wrong
08:14 - in bad propagation the new network
08:16 - evaluates its own performance and checks
08:18 - if it is right or wrong if it is wrong
08:20 - the network uses something called a loss
08:22 - function to quantify the deviation from
08:25 - the expected output and it is this
08:27 - information that's sent back to the
08:28 - hidden layers for the weights and biases
08:31 - to be adjusted so that the network's
08:32 - accuracy level increases let's visualize
08:36 - the training process with a real example
08:38 - let's suppose we have a data set this
08:40 - dataset gives us the weight of a vehicle
08:43 - and the number of goods carried by that
08:44 - vehicle and also tells us if those
08:46 - vehicles are cars or trucks
08:49 - we want to go through this data trade a
08:51 - new networks to predict cars or trucks
08:53 - based on their weights and goods to
08:55 - start off let's initialize the neural
08:56 - network by giving it random weights and
08:58 - biases these can be anything we really
09:01 - don't care what these values are as long
09:02 - as they're there
09:04 - in the first entry of a data set we have
09:06 - vehicle weight equal to a value which in
09:08 - this case is 15 and good as two
09:12 - according to this it's a car
09:14 - we now start moving these input
09:16 - dimensions through the newer network so
09:18 - basically what we want to do is take
09:19 - both the inputs multiply them by their
09:21 - weight and add a bias
09:23 - and this is where the magic happens we
09:25 - run this weighted sum through an
09:28 - activation function
09:29 - okay now let's say that the output of
09:31 - this activation function is 0.001
09:35 - this again is multiplied by the weight
09:37 - and added to the bias and finally in the
09:40 - output layer we have a guess
09:42 - now according to this neural network the
09:44 - type of vehicle with weight 15 and goods
09:46 - 2 has a greater probability of being a
09:48 - truck of course this is not true and a
09:50 - new network knows this so we use back
09:53 - propagation we're going to quantify the
09:54 - difference between the expected result
09:57 - and the predicted output using a loss
09:59 - function in bank propagation we're going
10:00 - to go backwards and adjust our initial
10:02 - rates and biases remember that during
10:04 - the initialization of the neural network
10:06 - we chose completely random weight and
10:08 - biases
10:09 - well during back propagation these
10:11 - values will be adjusted to better fit
10:13 - the prediction model
10:16 - okay so that was one iteration through
10:18 - the first piece of the data set in the
10:20 - second entry we have vehicle weight 34
10:22 - and goods 67.
10:24 - we're going to use the same process as
10:26 - before multiply the input with the
10:28 - weight and add a box pass this result
10:30 - into an activation function and repeat
10:32 - till the output layer check the error
10:34 - difference and employ back propagation
10:36 - to adjust the weights in the biases your
10:38 - new network will continue doing this
10:40 - repeated processor for propagation
10:42 - calculating the error and then back
10:44 - propagation for as many entries there
10:46 - are in this data set
10:48 - the more data you give the newer network
10:50 - the better it will be at predicting the
10:52 - right output but there's a tradeoff
10:54 - because too much data and you'll end up
10:56 - with a problem like overfitting which
10:58 - i'll discuss later in this course but
11:00 - that's essentially how a new network
11:01 - works you feed input the network
11:04 - initializes with random weights and
11:05 - biases that are adjusted each time
11:07 - during back propagation
11:09 - until the network's gone through all
11:11 - your data and is now able to make
11:12 - predictions
11:14 - this learning algorithm can be
11:16 - summarized as follows first we
11:18 - initialize the network with random
11:19 - values for the network's parameters or
11:22 - the weights in the biases we take a set
11:24 - of input data and pass them through the
11:26 - network we compare these predictions
11:29 - obtained with the values of the expected
11:31 - labels and calculate the loss using a
11:34 - loss function
11:35 - we perform back propagation in order to
11:37 - propagate this loss to each and every
11:39 - weight and bias
11:40 - we use this propagated information to
11:43 - update the weights and biases of neural
11:44 - network with the gradient descent
11:46 - algorithm in such a way that the total
11:48 - loss is reduced and a better model is
11:50 - obtained
11:52 - the last step is continue iterating the
11:54 - previous steps until we consider that we
11:56 - have a good enough model
12:02 - in this section we're going to talk
12:03 - about the most common terminologies used
12:05 - in deep learning today
12:07 - let's start off with the activation
12:08 - function
12:10 - the activation function serves to
12:11 - introduce something called non-linearity
12:14 - into the network and also decides
12:16 - whether a particular neuron can
12:17 - contribute to the next layer
12:20 - but how do you decide if the neuron can
12:22 - fire or activate
12:24 - well we had a couple of ideas which led
12:26 - to the creation of different activation
12:28 - functions
12:29 - the first idea we had is how about we
12:31 - activate the neuron if it is above a
12:33 - certain value or threshold if it is less
12:36 - than the threshold don't activate it
12:39 - activation function a is equal to
12:41 - activated if y is greater than some
12:43 - threshold else it's not
12:45 - this is essentially a step function its
12:47 - output is 1 or activated when value is
12:50 - greater than 0. its output is activated
12:52 - when value is greater than some
12:54 - threshold and outputs not activated
12:56 - otherwise
12:58 - great so this makes an activation
12:59 - function for a neuron no confusions life
13:02 - is perfect except there are some
13:04 - drawbacks with this to understand about
13:06 - it think about the following
13:08 - think about a case where you want to
13:10 - classify multiple such neurons into
13:12 - classes say class 1 class 2 class 3 etc
13:16 - what will happen if more than one neuron
13:18 - is activated all these neurons will
13:21 - output a one
13:22 - well how do you decide
13:24 - now how do you decide which class it
13:26 - belongs to it's complicated right you
13:28 - would want the network to activate only
13:30 - one neuron and the other should be zero
13:33 - only then you would be able to say it
13:34 - was classified properly
13:36 - in real practice however it is harder to
13:38 - train and converge it this way it would
13:40 - be better if the activation was not
13:42 - binary and instead some probable value
13:44 - like 75 activated or 16 activated
13:48 - there's a 75 chance that it belongs to
13:50 - class 2 etc
13:53 - then if more than one neuron activates
13:55 - you could find which neuron fires based
13:57 - on which has the highest probability
13:59 - okay maybe you'll ask yourself i want
14:02 - something to give me a more analog value
14:04 - rather than just saying activated or not
14:06 - activated something other than in binary
14:09 - and maybe you would have thought about a
14:10 - linear function a straight line function
14:12 - where the activation is proportional to
14:14 - the input by a value called the slope of
14:16 - the line
14:17 - this way it gives us a range of
14:19 - activations so it isn't binary
14:21 - activation we can definitely connect a
14:23 - few neurons together and if more than
14:25 - one fires we could take the maximum
14:27 - value and decide based on that so that
14:29 - is okay too and what is the problem with
14:31 - this
14:32 - well if you are familiar with gradient
14:34 - descent which i'll come to in just a bit
14:36 - you'll notice that the derivative of a
14:38 - linear function is a constant
14:40 - makes sense because the slope isn't
14:41 - changing at any point
14:43 - for a function f of x is equal to mx
14:45 - plus c the derivative is m this means
14:48 - that the gradient has no relationship
14:50 - whatsoever with x this also means that
14:52 - during back propagation the adjustments
14:54 - made to the weights and the biases
14:56 - aren't dependent on x at all and this is
14:59 - not a good thing additionally think
15:00 - about if you have connected layers no
15:03 - matter how many layers you have if all
15:06 - of them are linear in nature the
15:08 - activation function of the final layer
15:10 - is nothing but just a linear function of
15:12 - the input of the first layer
15:14 - pause for a bit and think about it
15:16 - this means that the entire neural
15:17 - network of dozens of layers can be
15:19 - replaced by a single layer remember a
15:22 - combination of linear functions in the
15:24 - linear manner is still another linear
15:26 - function
15:27 - and this is terrible because we've just
15:29 - lost the ability to stack layers this
15:31 - way no matter how much we stack the
15:34 - whole network is still equivalent to a
15:35 - single layer with single activation next
15:38 - we have a sigmoid function and if you've
15:40 - ever watched a video on activation
15:41 - functions this is the kind of function
15:43 - used in the examples a sigmoid function
15:46 - is defined as a if x is equal to 1 over
15:48 - 1 plus e to the negative x
15:50 - well this looks smooth and kind of like
15:52 - a step function what are its benefits
15:54 - think about it for a moment
15:56 - well first things first it has
15:57 - non-linear nature combinations of this
16:00 - function are also non-linear great so
16:02 - now we can stack layers
16:04 - what about non-binary activations yes
16:06 - that too this function outputs an analog
16:09 - activation unlike the step function and
16:11 - also has a smooth gradient an advantage
16:13 - of this activation function is that
16:15 - unlike the linear function the output of
16:17 - this function is going to be in the
16:19 - range zero to one inclusive compared to
16:21 - the negative infinity to infinity of the
16:24 - latter
16:25 - so we have activations bound in a range
16:27 - and this won't blow up the activations
16:30 - and this is great and sigmoid functions
16:32 - are one of the most widely used
16:33 - activation functions today
16:36 - but life isn't always rosy and sigmoids
16:38 - two tend to have the share of
16:39 - disadvantages
16:41 - if you look closely between x is equal
16:43 - to negative two and x is equal to two
16:45 - the y values are very steep any small
16:48 - changes in values of x in that region
16:50 - will cause values of y to change
16:52 - drastically
16:53 - also towards either end of the function
16:55 - the y values tend to respond very less
16:58 - to changes in x
16:59 - the gradient at those regions is going
17:01 - to be really really small
17:03 - almost zero and it gives rise to the
17:05 - vanishing gradient problem which just
17:07 - says that if the input to the activation
17:09 - function is either large or small the
17:11 - sigmoids are going to squish that down
17:13 - to a value between zero and one and the
17:16 - gradient of this function becomes really
17:18 - small and you'll see why when we talk
17:20 - about gradient descent this is a huge
17:23 - problem another activation function that
17:25 - is used is the tan h function
17:27 - this looks very similar to sigmoid in
17:29 - fact
17:30 - mathematically this is what's known as a
17:32 - shifted sigmoid function
17:35 - okay so like the sigmoid it has
17:37 - characteristics that we discussed above
17:39 - it is nonlinear nature so we can stack
17:42 - layers it is bound to arrange from
17:44 - negative one to one so there's no
17:46 - worrying about the activations blowing
17:47 - up
17:49 - the derivative of the tanning function
17:50 - however is steeper than that of the
17:52 - sigmoid so deciding between the sigmoid
17:54 - and the tanh will really depend on your
17:56 - requirement of the gradient strength
17:58 - like sigmoid tanh is also a very popular
18:00 - and widely used activation function
18:03 - and yes like the sigmoid tanh does have
18:05 - a vanishing gradient problem the
18:06 - rectified linear unit or the value
18:08 - function is defined as a of x is equal
18:10 - to the max from 0 to x at first look
18:13 - this would look like a linear function
18:15 - right the graph is linear in the
18:16 - positive axis
18:18 - let me tell you rather is in fact
18:20 - non-linear nature and combinations of
18:23 - relu are also non-linear great so this
18:25 - means that we can stack layers however
18:28 - unlike the previous two functions that
18:30 - we discussed is not bounded the range of
18:32 - the relu is from zero to infinity this
18:35 - means there is a chance of blowing up
18:37 - the activation
18:39 - another point i would like to discuss
18:41 - here is sparsity of inactivation imagine
18:45 - a big neural network with lots of
18:46 - neurons using a sigmoid or a tanning
18:49 - will cause almost all the neurons to
18:51 - fire in an analog way
18:53 - this means that almost all activations
18:55 - will be processed to describe the
18:57 - network's output in other words the
18:59 - activation would be dense
19:01 - and this is costly ideally we want only
19:04 - a few neurons in the network to activate
19:06 - and thereby making the activations pass
19:08 - and efficient
19:10 - here's where the relu comes in imagine a
19:13 - network with randomly initialized
19:15 - weights and almost 50 percent of the
19:17 - network yields zero activation because
19:20 - of the characteristic relu it outputs
19:22 - zero for negative values of x
19:25 - this means that only 50 percent of the
19:27 - neurons fire sparse activation making
19:30 - the network lighter but when life gives
19:32 - you an apple it comes with a little worm
19:34 - inside
19:35 - because of that horizontal line in relu
19:37 - for negative values of x the gradient is
19:40 - zero in that region which means that
19:42 - during back propagation the weights will
19:44 - not get adjusted during descent this
19:47 - means that those neurons which go into
19:49 - that state will stop responding to
19:50 - variations in the error simply because
19:53 - the gradient is zero nothing changes
19:56 - this is called the dying value problem
19:58 - this problem can cause several neurons
20:00 - to just die and not respond thus making
20:02 - a substantial part of the network
20:04 - passive rather than what we want out of
20:08 - there are workarounds for this one way
20:09 - especially is to simply make the
20:11 - horizontal line into a non-horizontal
20:13 - component by adding a slope usually the
20:16 - slope is around 0.001
20:18 - and this this new version of the relu is
20:21 - called leaky value the main idea is that
20:24 - the gradient should never be zero one
20:26 - major advantage of the relu is the fact
20:28 - that it's less computationally expensive
20:30 - than functions like tannage and sigmoid
20:32 - because it involves simpler mathematical
20:34 - operations
20:36 - this is a really good point to consider
20:38 - when you were designing your own deep
20:39 - neural networks great so now the
20:41 - question is which activation function to
20:43 - use
20:44 - because of the advantages that relu
20:46 - offers does this mean that you should
20:47 - use reload for everything you do
20:49 - or could you consider sigmoid and tan h
20:52 - well both
20:54 - when you know the function that you're
20:55 - trying to approximate has certain
20:57 - characteristics you should choose an
20:59 - activation function with which will
21:01 - approximate the function faster leading
21:03 - to faster training processes
21:05 - for example a sigmoid function works
21:07 - well for binary classification problems
21:10 - because approximating our classifier
21:12 - functions as combinations of the sigmoid
21:14 - is easier than maybe the relu this will
21:17 - lead to faster training processes and
21:19 - larger convergence
21:20 - you can use your own custom functions
21:22 - too if you don't know the nature of the
21:23 - function you're trying to learn i would
21:25 - suggest you start with relu and then
21:27 - work backwards from there
21:29 - before we move on to the next section i
21:31 - want to talk about why we use non-linear
21:34 - activation functions as opposed to
21:35 - linear ones
21:37 - if you recall in my definition of
21:39 - activation functions i mentioned that
21:41 - activation functions serve to introduce
21:43 - something called non-linearity in the
21:45 - network for all intensive purposes
21:47 - introducing non-linearity simply means
21:49 - that your activation function must be
21:51 - non-linear that is not a straight line
21:54 - mathematically linear functions are
21:56 - polynomials of degree 1 that when
21:58 - graphed in the x y plane are straight
22:00 - lines inclined to the x-axis at a
22:02 - certain value we call this the slope of
22:05 - the line
22:06 - non-linear functions are polynomials of
22:08 - degree greater than one
22:10 - and when graphed they don't form
22:11 - straight lines rather than more curved
22:14 - if we use linear activation functions to
22:16 - model our data then no matter how many
22:18 - hidden layers our network has it will
22:20 - always become equivalent to having a
22:22 - single layer network and in deep
22:24 - learning we want to be able to model
22:26 - every type of data without being
22:28 - restricted as would be the case should
22:30 - we use linear functions
22:35 - we discussed previously in the learning
22:36 - process of neural networks that we
22:38 - started with random weight and biases
22:40 - the neural network makes a prediction
22:42 - this prediction is compared against the
22:44 - expected output and the weights and
22:46 - biases are adjusted accordingly
22:48 - well loss functions are the reason that
22:50 - we're able to calculate that difference
22:52 - really simply a loss function is a way
22:54 - to quantify the deviation of the
22:56 - predicted output by the neural network
22:58 - to the expected output it's as simple as
23:00 - that nothing mo nothing less
23:03 - there are plenty of loss functions out
23:04 - there for example under regression we
23:06 - have squared error loss absolute ever
23:08 - loss in huber loss in binary
23:10 - classification we have binary cross
23:12 - entropy and hinge loss in multi-class
23:15 - classification problems we have the
23:17 - multi-class cross entropy and the
23:19 - callback liability divergence loss and
23:21 - so on
23:23 - the choice of the best function really
23:24 - depends on what kind of project you're
23:26 - working on different projects require
23:28 - different loss functions
23:30 - now i don't want to talk any further
23:31 - loss functions right now we'll do this
23:33 - under the optimization section because
23:35 - that's really where most functions are
23:37 - utilized
23:41 - in the previous section we dealt with
23:42 - loss functions which are mathematical
23:44 - ways of measuring how wrong predictions
23:46 - made by neural network are
23:48 - during the training process we tweak and
23:51 - change the parameters or the weights of
23:53 - the model
23:54 - to try and minimize that loss function
23:56 - and make our predictions as correct and
23:58 - optimized as possible
23:59 - but how exactly do you do that how do
24:02 - you change the parameters of your model
24:04 - by how much and when
24:06 - we have the ingredients how do we make
24:07 - the cake
24:09 - this is where optimizers come in they
24:11 - tied together the loss function and
24:13 - model parameters or the weight and
24:15 - biases by updating the network in
24:17 - response to the output of the loss
24:19 - function
24:21 - in simpler terms optimizers shape and
24:23 - mold your model into more accurate
24:25 - models by adjusting the weights and the
24:27 - biases
24:28 - the loss function is its guide it tells
24:31 - the optimizer whether it's moving in the
24:33 - right or the wrong direction
24:35 - to understand this better
24:37 - imagine did you have just killed mount
24:38 - everest and now you decide to descend
24:40 - the mountain blindfolded it's impossible
24:43 - to know which direction to go in you
24:45 - could either go up which is away from
24:47 - your goal or go down which is towards
24:49 - your goal but to begin you would start
24:51 - taking steps using your feet you'll be
24:54 - able to gauge whether you're going up or
24:56 - down
24:57 - in this analogy you resemble the neural
25:00 - network going down your goal is trying
25:03 - to minimize the error and your feet are
25:06 - resemblance of the loss functions they
25:08 - measure whether you're going in the
25:09 - right way or the wrong way
25:11 - similarly it's impossible to know what
25:13 - your model's weights should be right
25:14 - from the start but with some trial and
25:16 - error based on the loss function you
25:18 - could end up getting there eventually
25:21 - we now come to grading descent often
25:23 - called the grand daddy of optimizers
25:26 - grading descent is an iterative
25:28 - algorithm that starts up at a random
25:30 - point in the loss function and travels
25:32 - down its slope in steps until it reaches
25:34 - the lowest point or the minimum of the
25:36 - function
25:37 - it is the most popular optimizer we use
25:39 - nowadays it's fast robust and flexible
25:43 - and here's how it works
25:45 - first we calculated what a small change
25:47 - in each individual weight would do to
25:49 - the loss function
25:50 - we adjust each individual weight based
25:53 - on its gradient that is take a small
25:55 - step in the determined direction
25:57 - the last step is to repeat the first and
25:59 - the second step until the loss function
26:01 - gets as low as possible i want to talk
26:03 - about this notion of a gradient the
26:05 - gradient of a function is the vector of
26:08 - the partial derivatives with respect to
26:10 - all independent variables the gradient
26:12 - always points in the direction of the
26:14 - steepest increase in the function
26:16 - suppose we have a graph like so with
26:19 - loss on the y-axis and the value of the
26:21 - weight on the x-axis
26:22 - we have a little data point here that
26:24 - corresponds to the randomly initialized
26:26 - weight to minimize this loss that is to
26:29 - get this data point to the minimum of
26:31 - the function
26:32 - we need to take the negative gradient
26:34 - since we want to find the steepest
26:35 - decrease in function
26:37 - this process happens iteratively through
26:39 - the losses as minimized as possible
26:42 - and that's grading descent in a nutshell
26:44 - when dealing with high dimensional data
26:46 - sets that is a lot of variables it's
26:48 - possible you'll find yourself in an area
26:50 - where it seems like you've reached the
26:51 - lowest possible value for your loss
26:53 - function but in reality it's just a
26:56 - local minimum
26:57 - to avoid getting stuck in a local minima
27:00 - we make sure we use the proper learning
27:02 - rate
27:03 - changing our weights too fast by adding
27:05 - or subtracting too much that is taking
27:07 - steps that are too large or too small
27:10 - can hinder your ability to minimize the
27:12 - loss function
27:14 - we don't want to make a jump so large
27:16 - that we skip over the optimal value for
27:18 - a given weight to make sure this doesn't
27:20 - happen we use a variable called the
27:22 - learning rate
27:23 - this thing is usually just a small
27:25 - number like 0.001
27:27 - that we multiply the gradients by to
27:29 - scale them this ensures that any changes
27:32 - we make to our weights are pretty small
27:34 - in math talk taking steps that are too
27:36 - large can mean that the algorithm will
27:38 - never converge to an optimum at the same
27:40 - time we don't want to take steps that
27:42 - are too small because then we might
27:44 - never end up with the right values for
27:46 - our weights in math talk steps that are
27:48 - too small might lead to our optimizer
27:50 - converging on a local minimum for the
27:52 - loss function but never the absolute
27:54 - minimum
27:55 - for a simple summary just remember that
27:57 - the learning rate ensures that we change
27:59 - our weight at the right pace not making
28:01 - any changes that are too big or too
28:03 - small
28:05 - instead of calculating the gradients for
28:07 - all your training examples on every part
28:09 - of the gradient descent it's sometimes
28:11 - more efficient to only use a subset of
28:13 - the training examples each time
28:15 - stochastic gradient descent is an
28:17 - implementation that either uses batches
28:19 - of examples at a time or random examples
28:22 - on each pass
28:23 - stochastic gradient descent uses the
28:25 - concept of momentum momentum accumulates
28:28 - gradients of the past steps to dictate
28:30 - what might happen in the next steps also
28:33 - because we don't include the entire
28:34 - training set
28:36 - sjd is less computationally expensive
28:38 - it's difficult to overstate how popular
28:41 - gradient descent really is back
28:42 - propagation is basically gradient
28:44 - descent implemented on a network there
28:46 - are other types of optimizers based on
28:48 - gradient descent that are used today ad
28:50 - grad adapts the learning rate
28:52 - specifically to individual features
28:55 - that means that some of the weights in
28:56 - your data set will have different
28:58 - learning rates than others
29:00 - this works really well for sparse data
29:02 - sets where a lot of input examples are
29:04 - missing
29:05 - at a grad has a major issue though the
29:07 - adaptive learning rate tends to get
29:09 - really really small over time rms prop
29:12 - is a special version of adegrad
29:14 - developed by professor jeffrey hinton
29:16 - instead of letting all the gradients
29:18 - accumulate for momentum it accumulates
29:20 - gradients in a fixed window
29:23 - rms prop is similar to add a prop which
29:25 - is another optimizer that seeks to solve
29:28 - some of the issues that atograd leaves
29:29 - open atom stands for adaptive moment
29:32 - estimation and is another way of using
29:34 - past gradients to calculate the carbon
29:36 - gradient atom also utilizes the concept
29:39 - of momentum which is basically our way
29:41 - of telling the neural network whether we
29:42 - want past changes to affect the new
29:44 - change by adding fractions of the
29:46 - previous gradients to the current one
29:49 - this optimizer has become pretty
29:50 - widespread and is practically accepted
29:53 - for use in training new networks
29:56 - it's easy to get lost in the complexity
29:58 - of some of these new optimizers just
30:00 - remember that they all have the same
30:01 - goal minimizing the loss function and
30:04 - trial and error will get you there
30:09 - you may have heard me referring to the
30:11 - words parameters quite a bit and often
30:14 - this word is confused with the term
30:16 - hyperparameters
30:17 - in this video i'm going to outline the
30:19 - basic difference between the two a model
30:22 - parameter is a variable that is internal
30:25 - to the new network and whose values can
30:27 - be estimated from the data itself
30:29 - they are required by the model when
30:31 - making predictions these values define
30:34 - the skill of the model on your problem
30:36 - they can be estimated directly from the
30:38 - data
30:39 - and are often not manually set by the
30:41 - practitioner
30:43 - and oftentimes when you save your model
30:46 - you are essentially saving your model's
30:48 - parameters parameters are key to machine
30:51 - learning algorithms and examples of
30:53 - these include the weights and the biases
30:56 - a hyper parameter is a configuration
30:58 - that is external to the model and whose
31:01 - value cannot be estimated from data
31:03 - there's no way that we can find the best
31:05 - value for a model hyper parameter on a
31:07 - given problem we may use rules of thumb
31:10 - copy values used in other problems or
31:12 - search for the best value by trial and
31:14 - error
31:16 - when a machine learning algorithm is
31:17 - tuned for a specific problem such as
31:19 - when you're using grid search or random
31:21 - search then you are in fact tuning the
31:23 - hyper parameters of the model in order
31:25 - to discover the parameters that result
31:28 - in more skillful predictions
31:30 - model hyper parameters are often
31:32 - referred to as parameters which can make
31:34 - things confusing so a good rule of thumb
31:36 - to overcome this confusion is as follows
31:39 - if you have to specify a parameter
31:41 - manually then it is probably a hyper
31:44 - parameter
31:45 - parameters are inherent to the model
31:46 - itself some examples of hyper parameters
31:49 - include the learning rate for training
31:50 - on your network the c in sigma
31:53 - hyper parameters for sport vector
31:55 - machines and the k and k newest
31:57 - neighbors
32:01 - we need terminologies like epochs batch
32:04 - size and iterations only when the data
32:06 - is too big which happens all the time in
32:08 - machine learning and when we can't pass
32:10 - all this data to the computer at once so
32:13 - to overcome this problem we need to
32:15 - divide the data set into smaller chunks
32:17 - give it to our computer one by one and
32:19 - update the weights of the new network at
32:21 - the end of every step to fit it into the
32:24 - data given
32:25 - one epoch is when an entire data set is
32:28 - passed forward and backward through the
32:30 - network once
32:32 - in a majority of deep learning models we
32:33 - use more than one epoch i know it does
32:36 - make sense in the beginning why do we
32:37 - need to pass the entire data set many
32:39 - times through the same neural network
32:41 - passing the entire data set through the
32:43 - network only once is trying to read the
32:46 - entire lyrics of a song once he won't be
32:48 - able to remember the entire song
32:50 - immediately you have to reread the
32:52 - lyrics a couple more times before you
32:54 - can say you know the song by memory
32:56 - the same is true with the neural network
32:58 - we pass the data set multiple times
33:00 - through the neural network so it's able
33:02 - to generalize better
33:04 - gradient descent is an iterative process
33:07 - and updating the parameters and back
33:08 - propagation in a single pass or one
33:11 - epoch is not enough as the number of
33:13 - epochs increases the more the parameters
33:15 - are adjusted leading to a better
33:17 - performing model
33:19 - but too many epochs could spell disaster
33:21 - and lead to something called overfitting
33:23 - where a model has essentially memorized
33:24 - the patterns in the training data and
33:26 - performs terribly on data it's never
33:28 - seen before
33:31 - so what is the right number of epochs
33:33 - unfortunately there is no right answer
33:35 - the answer is different for different
33:37 - data sets sometimes your data set can
33:40 - include millions of examples passing
33:42 - this entire data set at once becomes
33:44 - extremely difficult so what we do
33:46 - instead is divide the data set into a
33:49 - number of batches rather than passing
33:51 - the entire dataset once the total number
33:54 - of training examples present in a single
33:56 - batch is called a batch size iterations
33:59 - is the number of batches needed to
34:00 - complete one epoch
34:03 - note the number of batches is equal to
34:06 - the number of iterations for one epoch
34:08 - let's say that we have a data set of 34
34:10 - 000 training examples if we divide the
34:13 - data set into batches of 500 then it
34:16 - will take 68 iterations to complete one
34:19 - epoch
34:22 - well i hope that gives you some kind of
34:23 - sense about the very basic terminologies
34:26 - used in deep learning before we move on
34:28 - i do want to mention this and you will
34:30 - see this a lot in deep learning you'll
34:32 - often have a bunch of different choices
34:34 - to make how many hidden layers should i
34:36 - choose or which activation function must
34:38 - i use and where and to be honest there
34:40 - are no clear-cut guidelines as to what
34:42 - your choice should always be
34:44 - that's a fun part about deep learning
34:46 - it's extremely difficult to know in the
34:48 - beginning what's the right combination
34:50 - to use for your project what works for
34:52 - me might not work for you and a
34:55 - suggestion from my end would be that you
34:57 - dabble along with material shown try
34:59 - various combinations and see what works
35:01 - for you best
35:02 - ultimately that's a learning process pun
35:04 - intended throughout this course i'll
35:06 - give you quite a bit of intuition as to
35:08 - what's popular so that when it comes to
35:10 - building a deep learning project you
35:12 - won't find yourself lost
35:16 - in this section we're going to talk
35:18 - about the different types of learning
35:19 - which are machine learning concepts but
35:21 - i extended to deep learning as well
35:24 - in this course we'll go over supervised
35:26 - learning unsupervised learning and
35:28 - reinforcement learning
35:31 - supervised learning is the most common
35:33 - sub branch machine learning today
35:35 - typically if you're new to machine
35:37 - learning your journey will begin with
35:38 - supervised learning algorithms
35:41 - let's explore what these are
35:43 - supervised machine learning algorithms
35:45 - are designed to learn by example the
35:47 - name supervised learning originates from
35:49 - the idea that training this type of
35:51 - algorithm is almost like there's a human
35:53 - supervising the whole process
35:56 - in supervised learning we train our
35:57 - models on well-labeled data
36:00 - each example is a pair consisting of an
36:02 - input object which is typically a vector
36:05 - and a desired output value also called a
36:07 - supervisory signal
36:09 - during training a supervised learning
36:11 - algorithm will search for patterns in
36:13 - the data that correlate with the desired
36:15 - outputs
36:16 - after training it will take in new
36:18 - unseen inputs and will determine which
36:21 - label the new inputs will be classified
36:23 - as based on prior training data the
36:26 - objective of a supervised learning model
36:28 - is to predict the correct label for
36:30 - newly presented input data at its most
36:32 - basic form a supervised learning
36:35 - algorithm can simply be written as y is
36:37 - equal f x
36:38 - where y is the predicted output that is
36:40 - determined by a mapping function that
36:42 - assigns a class to an input value x the
36:45 - function used to connect input features
36:47 - to a predicted output is created by the
36:49 - machine learning model during training
36:52 - supervised learning can be split into
36:53 - two subcategories classification and
36:56 - regression
36:57 - during training a classification
36:59 - algorithm will be given data points with
37:01 - an assigned category the job of a
37:03 - classification algorithm is then to take
37:05 - this input value and assigned to a class
37:08 - of category that it fits into based on
37:10 - the training data provided
37:12 - the most common example of
37:14 - classification is determining if an
37:16 - email is spam or not with two classes to
37:19 - choose from spam or not spam this
37:21 - problem is called a binary
37:23 - classification problem
37:25 - the algorithm will be given training
37:26 - data with emails that are both spam and
37:28 - not spam and the model will find the
37:31 - features within the data that correlate
37:33 - to either class and create a mapping
37:35 - function
37:37 - then when provided with an unseen email
37:39 - the model will use this function to
37:41 - determine whether or not the email is
37:43 - pam
37:44 - an example of a classification problem
37:46 - would be the mnist handwritten digits
37:48 - dataset where the inputs are images of
37:50 - handwritten digits pixel data and the
37:53 - output is the class label for what digit
37:55 - the image represents that is numbers
37:57 - zero to nine there are numerous
37:59 - algorithms to solve classification
38:00 - problems each which depends on the data
38:03 - and the situation here are a few popular
38:05 - classification algorithms
38:07 - linear classifiers support vector
38:10 - machines decision trees k-nearest
38:12 - neighbors and drellum forest regression
38:15 - is a predictive statistical process
38:17 - where the model attempts to find the
38:19 - important relationship between dependent
38:21 - and independent variables
38:23 - the goal of a regression algorithm is to
38:25 - predict a continuous number such as
38:28 - sales income and tax scores
38:30 - the equation for a basic linear
38:32 - regression can be written as follows
38:34 - where x and 5 represents the features of
38:36 - the data and w of i and b are parameters
38:39 - which are developed during training for
38:41 - simple linear regression models with
38:43 - only one feature in the data the formula
38:45 - looks like this where w is the slope x
38:48 - is the single feature and b is a
38:50 - y-intercept
38:52 - familiar
38:53 - for simple regression problems such as
38:55 - this the model's predictions are
38:56 - represented by the line of best fit
38:59 - for models using two features a plane is
39:01 - used and for models with more than two
39:03 - features a hyperplane is used
39:06 - imagine we want to determine a student's
39:08 - test grade based on how many hours they
39:10 - study the week of the test
39:12 - let's say the plot data with line of
39:14 - best fit looks like this
39:16 - there is a clear positive correlation
39:18 - between our studied the independent
39:20 - variable and the student's final test
39:22 - goals the dependent variable a line of
39:25 - best fit can be drawn through the data
39:26 - points to show the model's predictions
39:29 - when given new input
39:31 - say we wanted to know how well a student
39:33 - would do with five hours of study
39:36 - we can use the line of best fit to
39:37 - predict the test call based on other
39:40 - students performances another example of
39:43 - regression problem would be the boston
39:44 - house prices data set where the input of
39:47 - variables that describe a neighborhood
39:49 - and the output is a house price in
39:51 - dollars
39:52 - there are many different types of
39:53 - regression algorithms three most common
39:56 - are linear regression lasso regression
39:58 - and multivariate regression supervised
40:00 - learning finds applications in
40:02 - classification and regression problems
40:04 - like
40:05 - bioinformatics such as fingerprint iris
40:08 - and face recognition in smartphones
40:10 - object recognition spam detection and
40:13 - speech recognition
40:17 - unsupervised learning is a branch of
40:19 - machine learning that is used to
40:20 - manifest underlying patterns and data
40:22 - and is often used in exploratory data
40:25 - analysis
40:26 - unlike supervised learning unsupervised
40:29 - learning does not use label data but
40:31 - instead focuses on the data's features
40:34 - label training data has a corresponding
40:36 - output for each input
40:39 - the goal of an unsupervised learning
40:41 - algorithm is to analyze data and find
40:43 - important features in that data
40:46 - unsupervised learning will often find
40:48 - subgroups or hidden patterns within the
40:50 - dataset that a human observer might not
40:52 - pick up on and this is extremely useful
40:54 - as we'll soon find out
40:57 - unsupervised learning can be of two
40:59 - types clustering and association
41:02 - clustering is the simplest and among the
41:04 - most common applications of unsupervised
41:06 - learning it is the process of grouping
41:08 - the given data into different clusters
41:10 - or groups
41:12 - classes will contain data points that
41:14 - are as similar as possible to each other
41:16 - and as dissimilar as possible to data
41:18 - points in other clusters
41:20 - clustering helps find underlying
41:22 - patterns within the data that may not be
41:24 - noticeable through a human observer
41:27 - it can be broken down into partitional
41:29 - clustering and hierarchical clustering
41:31 - partitional clustering refers to a set
41:34 - of clustering algorithms where each data
41:36 - point in a data set can belong to only
41:38 - one cluster
41:40 - hierarchical clustering finds clusters
41:42 - by a system of hierarchies
41:44 - every data point can belong to multiple
41:46 - clusters some classes will contain
41:49 - smaller clusters within it
41:51 - this hierarchy system can be organized
41:53 - as a tree diagram
41:55 - some of the more commonly used
41:57 - clustering algorithms are the k-means
41:59 - expectation
42:01 - and the hierarchical cluster analysis of
42:03 - the aca
42:05 - association on the other hand attempts
42:07 - to find relationships between different
42:09 - entities the classic example of
42:11 - association rules is market basket
42:14 - analysis this means using a database of
42:18 - transactions in the supermarket to find
42:20 - items that are frequently bought
42:22 - together for example a person who buys
42:24 - potatoes and burgers usually buys beer
42:27 - for example the person who buys tomatoes
42:29 - and pizza cheese
42:31 - might want to bring pizza bread and so
42:33 - on
42:34 - unsupervised learning finds applications
42:36 - almost everywhere for example airbnb
42:39 - which helps host stays and experiences
42:41 - and connect people all over the world
42:43 - this application uses unsupervised
42:45 - learning algorithms where a potential
42:47 - client
42:48 - queries their requirements and airbnb
42:50 - learns these patterns and recommends
42:52 - stays and experiences which fall under
42:54 - the same group of cluster example a
42:56 - person looking for houses in san
42:58 - francisco might not be interested in
43:00 - finding houses in boston amazon also
43:02 - uses unsupervised learning to learn the
43:04 - customers purchases and recommend
43:06 - products which are frequently bought
43:08 - together which is an example of
43:10 - association rule mining
43:12 - credit card fraud detection is another
43:15 - unsupervised learning algorithm that
43:17 - learns the various patterns of a user
43:19 - and and their usage of a credit card if
43:22 - the card is used in parts that do not
43:23 - match the behavior an alarm is generated
43:26 - which could possibly be marked as fraud
43:28 - and in some cases your bank might call
43:30 - you to confirm whether it was you using
43:32 - the card or not
43:34 - reinforcement learning is a type of
43:36 - machine learning technique that enables
43:38 - an agent to learn in an interactive
43:40 - environment by trial and error using
43:42 - feedback from its own actions and
43:44 - experiences
43:46 - like supervised learning it uses mapping
43:48 - between the input and the output but
43:50 - unlike supervised learning where
43:52 - feedback provided to the agent is a
43:54 - correct set of actions for performing a
43:56 - task
43:57 - reinforcement learning uses rewards and
43:59 - punishments as signals for positive and
44:02 - negative behavior
44:03 - when you compare with unsupervised
44:05 - learning reinforcement learning is
44:07 - different in terms of its goals
44:09 - while the goal in unsupervised learning
44:11 - is to find similarities and differences
44:13 - between data points
44:15 - in reinforcement learning the goal is to
44:17 - find a suitable action model that would
44:19 - maximize the total cumulative reward of
44:21 - the agent
44:23 - reinforcement learning refers to
44:24 - goal-oriented algorithms which learn how
44:27 - to attain a complex objective or goal or
44:30 - how to maximize along a particular
44:32 - dimension over many steps
44:35 - for example they can maximize the points
44:37 - of one in a game over many moves
44:40 - reinforcement learning algorithms can
44:42 - start from a blank slate and under the
44:44 - right conditions achieve superhuman
44:46 - performance
44:47 - like a pet incentivized by scolding and
44:49 - treats these algorithms are penalized
44:52 - when they make the wrong decisions and
44:54 - rewarded when they make the right ones
44:56 - this is reinforcement
44:59 - reinforcement learning is usually
45:01 - modeled as a markov decision process
45:03 - although other frameworks like queue
45:05 - learning are used
45:07 - some key terms that describe the
45:09 - elements of a reinforcement learning
45:11 - problem are the environment which is the
45:13 - physical world in which the agent
45:15 - operates
45:16 - the state represents the current
45:18 - situation of the agent
45:20 - reward is a feedback received from the
45:23 - environment
45:24 - policy sometimes is the method to map
45:27 - the agent state to the agent's actions
45:30 - and finally value is the future reward
45:33 - that an agent will receive by taking an
45:36 - action in a particular state
45:39 - a reinforcement learning problem can be
45:41 - best explained through games let's take
45:43 - the game of pac-man where the goal of
45:45 - the agent or pac-man is to eat the food
45:48 - in the grid while avoiding the ghosts on
45:50 - its way
45:51 - the grid world is the interactive
45:53 - environment for the agent
45:55 - pac-man receives a reward for eating
45:57 - food and punishment if it gets killed by
45:59 - the ghost that is it loses the game
46:02 - the states are the location of pac-man
46:04 - in the grid world and the total
46:06 - cumulative reward is pac-man winning the
46:09 - game
46:10 - reinforcement learning finds
46:11 - applications in robotics business
46:13 - strategy planning traffic light control
46:16 - web system configuration and aircraft
46:19 - and robot motion control
46:24 - a central problem in deep learning is
46:26 - how to make an algorithm that will
46:27 - perform well not just in training data
46:30 - but also on new inputs
46:32 - one of the most common challenges you'll
46:34 - face when training models is a problem
46:36 - of overfitting a situation where your
46:39 - model performs exceptionally well on
46:41 - training data but not in testing data
46:44 - see i have a data set graphed on the xy
46:46 - plane like so
46:48 - now i want to construct a model that
46:50 - would best fit the data set
46:52 - what i could do is draw a line of some
46:54 - random slope in intercept now evidently
46:57 - this isn't the best model and in fact
46:59 - this is called underfitting because it
47:01 - doesn't fit the model well in fact it
47:03 - underestimates the data set
47:06 - instead what we could do is draw a line
47:08 - that looks something like this now this
47:10 - really fits our model the best
47:12 - but this is overfitting
47:15 - remember that while training we show our
47:17 - networks and training data and once
47:18 - that's done
47:20 - we'd expect it to be almost close to
47:22 - perfect
47:23 - the problem with this graph is that
47:25 - although it is probably the best line of
47:27 - fit for this graph it is the best line
47:29 - of fit only if you're considering your
47:31 - trading data
47:33 - what your network is done in this graph
47:35 - is memorize the patterns between the
47:36 - trading data
47:38 - and won't give accurate predictions at
47:40 - all on data it's never seen before
47:43 - and this makes sense because instead of
47:45 - memorizing patterns generally to perform
47:47 - well on both training as well as new
47:49 - testing data
47:51 - our network in fact has memorized the
47:53 - patterns only on the training data so it
47:55 - obviously won't perform well on new data
47:58 - it's never seen before
48:00 - this is the problem of overfitting it
48:02 - fitted too much
48:04 - and by the way this would be the more
48:06 - accurate kind of fitting it's not
48:08 - perfect but it'll do well on both
48:10 - training as well as new testing data
48:12 - with sizeable accuracy
48:15 - there are a couple of ways to tackle
48:16 - overfitting
48:18 - the most interesting type of
48:19 - regularization is dropout it produces
48:22 - very good results and is consequently
48:25 - the most frequently used regularization
48:27 - technique in the field of deep learning
48:30 - to understand dropout let's say that we
48:32 - have a new network with two hidden
48:33 - layers
48:34 - what dropout does is that at every
48:36 - iteration it randomly selects some nodes
48:39 - and removes them along with their
48:41 - incoming and outgoing connections as
48:43 - shown
48:44 - so each iteration has a different set of
48:46 - nodes and this results in a different
48:49 - set of outputs
48:50 - so why do these models perform better
48:53 - these models usually perform better than
48:55 - a single model as they capture more
48:57 - randomness and memorizes less of the
49:00 - training data and hence will be folks to
49:02 - generalize better and build a more
49:04 - robust predictive model
49:08 - sometimes the best way to make a deep
49:10 - learning model generalized better is to
49:12 - train it on mode data
49:14 - in practice the amount of data we have
49:16 - is limited and one way to get around
49:18 - this problem is to create fake data and
49:21 - add it to the training set
49:23 - for some deep learning tasks it is
49:25 - reasonably straightforward to create new
49:26 - fake data
49:28 - this approach is easiest for
49:29 - classification a classifier needs to
49:31 - take complicated high dimensional input
49:34 - x and summarize it with the category
49:36 - identity y
49:38 - this means that the main task facing a
49:40 - classifier is to be invariant to a wide
49:42 - variety of transformations
49:44 - we can generate new xy pairs easily just
49:47 - by applying transformations on the xy
49:50 - inputs in our training set
49:52 - dataset augmentation has been a
49:53 - particularly effective technique for a
49:55 - specific classification problem object
49:58 - recognition
49:59 - images are high dimensional and include
50:01 - an enormous range of factors of
50:03 - variation many of which can easily be
50:06 - simulated
50:07 - operations like translating the training
50:09 - images a few pixels in each direction
50:12 - can often greatly improve generalization
50:15 - many other operations such as rotating
50:17 - the image or scaling the image have also
50:19 - proved quite effective
50:22 - you must be careful not to apply
50:23 - transformation that would change the
50:25 - correct class for example in optical
50:27 - character recognition tasks that require
50:30 - recognizing the difference between a b
50:32 - and a d and the difference between a six
50:34 - and a nine
50:36 - horizontal flips and 180 degree
50:38 - rotations are not appropriate ways of
50:40 - augmenting data sets with these tasks
50:43 - when training large models with
50:45 - sufficient representational capacity to
50:47 - overfit the task
50:48 - we often observe that the training error
50:50 - decreases steadily over time but the
50:52 - error validation set begins to rise
50:55 - again
50:56 - this means we can obtain a model with
50:58 - better validation set error and thus
51:00 - hopefully better test that error by
51:02 - stopping training at the point where the
51:04 - error in the validation set starts to
51:06 - increase
51:07 - this strategy is known as early stopping
51:10 - it is probably the most commonly used
51:12 - form of regularization in deploying
51:14 - today its popularity is due to both its
51:17 - effectiveness and its simplicity
51:20 - in this section i'm going to introduce
51:22 - the three most common types of neural
51:24 - network architectures today
51:26 - fully connected v4 with new networks
51:28 - recurring neural networks and
51:30 - convolutional neural networks
51:36 - the first type of new network
51:37 - architecture we're going to discuss is a
51:39 - fully connected feed forward neural
51:40 - network
51:42 - by fully connected i mean that each
51:44 - neuron in the preceding layer is
51:46 - connected to every neuron in the
51:47 - subsequent layer without any connection
51:50 - backwards there are no cycles or loops
51:52 - in the connections in the network
51:54 - as i mentioned previously each neuron in
51:57 - a neural network contains an activation
51:59 - function that changes the output of a
52:01 - neuron when given its input
52:03 - there are several types of activation
52:05 - functions that can change this input to
52:07 - output relationship to make a neuron
52:09 - behave in a variety of ways
52:11 - some of the most well-known activation
52:13 - functions are the linear function which
52:15 - is a straight line that essentially
52:17 - multiplies the input by a constant value
52:20 - the sigmoid function that ranges from
52:21 - zero to one the hyperbolic tangent of
52:24 - the tanning function ranges from
52:26 - negative one to positive one and the
52:28 - rectified linear unit or the relu
52:30 - function which is a piecewise function
52:32 - that outputs a zero if the input is less
52:34 - than a certain value or a linear
52:37 - multiple if the input is greater than a
52:39 - certain value
52:40 - each type of activation function has its
52:42 - pros and cons so we use them in various
52:44 - layers in the deep neural network based
52:47 - on the problem each is designed to solve
52:49 - in addition the last three activation
52:51 - functions we refer to as non-linear
52:53 - functions because the output is not a
52:56 - linear multiple of the input
52:58 - non-linearity is what allows deep neural
53:00 - networks to model complex functions
53:04 - using everything we've learned so far we
53:06 - can create a wide variety of fully
53:08 - connected feed for neural networks
53:10 - we can create networks with various
53:12 - inputs various outputs various hidden
53:15 - layers neurons per hidden layer and a
53:17 - variety of activation functions
53:19 - these numerous combinations allow us to
53:21 - create a variety of powerful deep neural
53:24 - networks that can solve the wide array
53:26 - of problems the more neurons we add to
53:29 - each hidden layer the wider the network
53:31 - becomes
53:32 - in addition the more hidden layers we
53:34 - add the deeper the network becomes
53:37 - however each neuron we add increases the
53:40 - complexity and thus the computational
53:42 - resource necessary to train a new
53:44 - network increases
53:46 - this increasing complexity isn't linear
53:48 - in the number of neurons we add so it
53:51 - leads to an explosion in complexity and
53:53 - training time for large neural networks
53:56 - that's the trade-off you need to
53:57 - consider when you are building deep
53:59 - neural networks
54:02 - all the new networks we've discussed so
54:03 - far are known as feed forward neural
54:05 - networks they take in a fixed sized
54:08 - input and give you a fixed size output
54:10 - that's all it does
54:11 - and that's what we expect neural
54:13 - networks to do take in an input and give
54:15 - a sizeable output
54:17 - but as it turns out these plane of
54:19 - vanilla neural networks aren't able to
54:21 - model every single problem that we have
54:23 - today
54:24 - to better understand this use this
54:26 - analogy suppose i show you the picture
54:28 - of a ball a round spherical ball that
54:31 - was moving in space in some direction
54:33 - i've just taken a photo of the ball or a
54:35 - snapshot of the ball at some time t now
54:37 - i want you to predict the next position
54:39 - of the ball in say two or three seconds
54:42 - you're probably not going to give me an
54:44 - accurate answer
54:45 - now let's look at another example
54:48 - suppose i walk up to you and say the
54:49 - word
54:50 - dog you will never understand my
54:53 - statement because well it doesn't make
54:54 - sense there are trilling combinations
54:57 - solely using the word dog and among
54:59 - these trillion combinations i'm
55:01 - expecting you to now guess what i'm
55:03 - trying to tell
55:04 - you what these two examples have in
55:07 - common is that it doesn't make sense it
55:09 - doesn't in the first case i'm expecting
55:12 - you to predict the next position in time
55:14 - and in the second i'm expecting you to
55:16 - understand what i mean by dog these two
55:18 - examples cannot be understood and
55:20 - interpreted unless some information
55:22 - about the pass was supplied
55:25 - now in the first example if i give you
55:26 - the previous position states of the ball
55:28 - and now ask you to predict the future
55:30 - trajectory of the ball you're going to
55:32 - be able to do this accurately
55:34 - and in the second case if i give you a
55:36 - full sentence saying i have a dog
55:39 - this makes sense because now you
55:41 - understand that out of the trillion
55:43 - possible combinations involving a dog my
55:46 - original intent was for you to
55:48 - understand that i have a dog
55:51 - why did i give you this example how does
55:53 - this apply to neural networks
55:55 - in the introduction i said vanilla
55:57 - neural networks can't model every single
55:59 - situation or problem that we have and
56:02 - the biggest problem it turns out is that
56:04 - plain vanilla feed forward neural
56:06 - networks cannot model sequential data
56:09 - sequential data is data in a sequence
56:12 - for example a sentence is a sequence of
56:14 - what a ball moving in space is a
56:17 - sequence of all its position states
56:19 - in the sentence that i've shown you you
56:21 - understood each word based off your
56:23 - understanding of the previous words
56:26 - this is called sequential memory you
56:28 - were able to understand the data point
56:29 - in the sequence by your memory of the
56:31 - previous data point in that sequence
56:34 - traditional neural networks can't do
56:36 - this and it seems like a major
56:37 - shortcoming
56:39 - one of the disadvantages of modelling
56:41 - sequences with traditional neural
56:43 - networks is the fact that they don't
56:44 - share parameters across time
56:47 - let us take for example these two
56:48 - sentences on tuesday it was raining and
56:51 - it was raining on tuesday
56:53 - these sentences mean the same thing
56:55 - although the details are in different
56:57 - parts of the sequence
57:00 - actually when we feed these sentences
57:02 - into a field for new network for a
57:03 - prediction task
57:05 - the model will assign different weights
57:07 - to on tuesday and it was raining at each
57:10 - moment in time
57:11 - things we learn about the sequence won't
57:13 - transfer if they appear at different
57:15 - points in the sequence
57:18 - sharing parameters gives the network the
57:20 - ability to look for a given feature
57:22 - everywhere in the sequence rather than
57:24 - just in a certain area
57:27 - that's the model sequences we need a
57:29 - specific learning framework able to deal
57:31 - with variable lens sequences maintain
57:34 - sequence order and to keep track of
57:36 - long-term dependencies rather than
57:38 - cutting input data too short
57:40 - and finally to share parameters across
57:42 - the sequence so as to not reload things
57:46 - and that's where recurrent neural
57:48 - networks come in
57:49 - rnns are a type of new network
57:51 - architecture that use something called a
57:53 - feedback loop in the hidden layer
57:56 - unlike feed forward new networks the
57:58 - recurrent neural network or rnn can
58:01 - operate effectively on sequences of data
58:04 - with variable input length
58:06 - this is how an rnn is usually
58:08 - represented
58:09 - this little loop here is called the
58:11 - feedback loop
58:13 - sometimes you may find the rnns depicted
58:15 - over time like this
58:17 - the first path represents the network in
58:19 - the first time step
58:21 - the hidden node h1 uses the input x1 to
58:24 - produce output y1
58:26 - this is exactly what we've seen with
58:28 - basic feed forward new networks however
58:31 - at the second time step the hidden node
58:33 - at the current time step h2 uses both
58:35 - the new input x2 as well as the state
58:38 - from the previous time step h1 as input
58:41 - to make new predictions
58:44 - this means that a current neural network
58:46 - uses knowledge of its previous states as
58:48 - input for its current prediction
58:50 - and we can repeat this process for an
58:52 - arbitrary number of steps allowing for
58:54 - the network to propagate information by
58:56 - its hidden state throughout time
58:59 - this is almost like giving a neural
59:00 - network a short-term memory
59:03 - they have this abstract concept of
59:05 - sequential memory and because of this
59:07 - we're able to model certain areas of
59:10 - data sequential data that standalone
59:12 - neural networks aren't able to model
59:16 - recurrent neural networks remember their
59:18 - past and their decisions are influenced
59:20 - by what it has learned from the past
59:23 - basic feed-forward networks remember
59:25 - things too but they remember things they
59:27 - learned during training for example an
59:29 - image classifier learns what a three
59:31 - looks like during training and then uses
59:33 - that knowledge to classify things in
59:35 - production so how do we train an rnn
59:39 - well it is almost the same as training a
59:41 - basic fully connected feed forward
59:42 - network except that the back propagation
59:45 - algorithm is applied for every sequence
59:47 - data point rather than the entire
59:49 - sequence
59:51 - this algorithm is sometimes called the
59:52 - back propagation through time algorithm
59:54 - or the btt algorithm
59:58 - to really understand how this works
60:00 - imagine we're creating a recurring new
60:02 - network to predict the next letter a
60:03 - person is likely to type based on the
60:06 - previous letters they've already typed
60:08 - the letter that a user just typed is
60:10 - quite important to predicting the new
60:12 - letter
60:13 - however all the previous letters are
60:15 - also very important to this prediction
60:17 - as well
60:19 - at the first time step say the user
60:21 - typed the letter f so a network might
60:24 - predict that the next letter is in e
60:26 - based on all of the previous training
60:27 - examples that included the word fe
60:30 - at the next time step the user types the
60:32 - letter r so our network uses both the
60:35 - new letter r plus the state of the first
60:37 - hidden neuron in order to compute the
60:40 - next prediction l
60:42 - the network predicts this because of the
60:44 - high frequency of occurrences in the
60:46 - word fel in our training data set
60:49 - adding the letter a might predict the
60:51 - letter t adding an n would predict the
60:53 - letter k which would match the word i
60:56 - use in tender to type which is frank
60:58 - there however is an issue with rnn's
61:00 - known as short-term memory short-term
61:02 - memory is caused by the infamous
61:04 - vanishing and exploding gradient
61:06 - problems
61:07 - as the rnn processes more words it has
61:10 - trouble retaining information from
61:11 - previous steps kind of like our memory
61:14 - if you're given a long sequence of
61:16 - numbers like pi and you tried reading
61:18 - them out you're probably going to forget
61:19 - the initial few digits right
61:22 - short-term memory and the vanishing
61:24 - gradient is due to the nature of back
61:26 - propagation the algorithm used to train
61:28 - and optimize neural networks
61:31 - after the forward propagation or the
61:33 - pass
61:34 - the network compares this prediction to
61:36 - the ground truth using a loss function
61:38 - which outputs an error value
61:41 - an estimate of how poorly the network is
61:43 - performing
61:44 - the network uses that error value to
61:46 - perform back propagation which
61:48 - calculates the gradients for each node
61:50 - in the network the gradient is the value
61:52 - used to adjust the network's internal
61:54 - weights allowing for the network to
61:56 - learn
61:57 - the bigger the gradient the bigger the
61:58 - adjustments are and vice versa
62:01 - here's where the problem lies when
62:03 - performing back propagation each node in
62:06 - a layer calculates its gradient with
62:08 - respect to the effects of the gradient
62:10 - in the layer before it
62:11 - so if the adjustment to the layers
62:13 - before it is small
62:14 - then the adjustments to the current
62:16 - layer will be even smaller
62:19 - and this causes gradients to
62:20 - exponentially shrink as it back
62:22 - propagates down the earlier layers fail
62:24 - to do any learning as the internal
62:26 - weights are barely being adjusted due to
62:28 - extremely small gradients and that is
62:31 - the vanishing gradient problem
62:33 - let's see how this applies to recover
62:35 - new networks you can think of each time
62:37 - step in a recurrent neural network as a
62:39 - layer to train recon neural network you
62:42 - use an application of back propagation
62:44 - called back propagation through time
62:47 - the gradient values will exponentially
62:49 - shrink as the back propagates through
62:51 - each time step
62:52 - again the gradient is used to make
62:54 - adjustments in the new network weights
62:56 - thus allowing it to learn
62:58 - small gradients means small adjustments
63:01 - and this causes the early layers not
63:03 - alone
63:04 - because of the vanishing gradient the
63:06 - rnn doesn't learn the long range
63:08 - dependencies across time steps
63:11 - this means that in a sequence it was
63:12 - raining on tuesday there is a
63:14 - possibility that the words it and was
63:17 - are not considered when trying to
63:19 - predict the user's intention
63:22 - the network then has to make the best
63:23 - guess with on tuesday and that's pretty
63:26 - ambiguous and would be difficult even
63:28 - for a human so not being able to learn
63:30 - on earlier time steps causes the network
63:32 - to have a short-term memory
63:35 - we can combat the short-term memory of
63:37 - an rnn by using two variants of
63:39 - recurrent neural networks
63:41 - gated rnns and long short-term memory
63:44 - rnns also known as lcms both these
63:47 - variants function just like rns but
63:49 - they're capable of learning long-term
63:51 - dependencies using mechanisms called
63:53 - gates
63:54 - these gates are different tensor
63:56 - operations that learn information that
63:59 - can learn what information to add or
64:01 - remove to the hidden state or the
64:02 - feedback loop
64:04 - the main difference between a gated rnn
64:06 - and an lscm is that the gated rnn has
64:10 - two gates to control its memory an
64:12 - update gate and reset gate while an lsem
64:15 - has three gates an input gate an output
64:18 - gate and a forget gate rnns work well
64:21 - for applications that involve sequences
64:23 - of data that change over time
64:25 - these applications include natural
64:27 - language processing sentiment
64:29 - classification dna sequence
64:31 - classification speech recognition and
64:34 - language translation
64:36 - a convolutional neural network or cnn
64:38 - for short is a type of deep neural
64:40 - network architecture designed for
64:42 - specific tasks like image classification
64:45 - cnns were inspired by the organization
64:48 - of neurons in the visual cortex of the
64:50 - animal brain
64:51 - as a result they provide some very
64:54 - interesting features that are useful for
64:55 - processing certain types of data like
64:58 - images audio and video
65:00 - like a fully connected neural network a
65:02 - cnn is composed of an input layer an
65:05 - output layer and several hidden layers
65:07 - between the two
65:09 - cnns derive their names from the type of
65:11 - hidden layers it consists of
65:13 - the hidden layers of a cnn typically
65:15 - consist of convolutional layers pooling
65:17 - layers fully connected layers and
65:20 - normalization layers this means that
65:22 - instead of traditional activation
65:24 - functions we use in feed-forward neural
65:26 - networks convolution and pooling
65:28 - functions are used instead more often
65:30 - than not the input of a cnn is typically
65:32 - a two-dimensional array of neurons which
65:35 - correspond to the pixels of an image for
65:37 - example if you're doing image
65:39 - classification
65:40 - the output layer is typically
65:42 - one-dimensional convolution is a
65:44 - technique that allows us to extract
65:46 - visual features from a 2d array in small
65:49 - chunks each neuron in a convolution
65:51 - layer is responsible for a small cluster
65:54 - of neurons in the preceding layer
65:56 - the bounding box that determines the
65:58 - cluster of neurons is called a filter
66:00 - also called a kernel
66:02 - conceptually you can think of it as a
66:04 - filter moving across an image and
66:06 - performing a mathematical operation on
66:08 - individual regions of the image it then
66:10 - sends its result to the corresponding
66:12 - neuron in the convolution layer
66:15 - mathematically a convolution of two
66:17 - functions f and g is defined as follows
66:20 - which is in fact the dot product of the
66:22 - input function and the kernel function
66:25 - pooling also known as sub sampling or
66:27 - down sampling is the next step in a
66:29 - convolutional neural network its
66:32 - objective is to further reduce the
66:33 - number of neurons necessary in
66:35 - subsequent layers of the network while
66:38 - still retaining the most important
66:39 - information
66:41 - there are two different types of pooling
66:42 - that can be performed max pulling and
66:45 - min pooling
66:46 - as the name suggests max pooling is
66:48 - based on picking up the maximum value
66:51 - from the selected region and min pooling
66:53 - is based on picking up the minimum value
66:55 - from that region
66:57 - when we put all these techniques
66:58 - together we get an architecture for a
67:00 - deep neural network quite different from
67:03 - a fully connected neural network for
67:04 - image classification where cnns are used
67:07 - heavily we first take an input image
67:09 - which is a two-dimensional matrix of
67:11 - pixels typically with three color
67:13 - channels red green and blue
67:16 - next we use a convolution layer with
67:18 - multiple filters to create a
67:20 - two-dimensional feature matrix as the
67:22 - output for each filter
67:24 - we then pool the results to produce a
67:26 - downsample feature matrix for each
67:28 - filter in the convolution layer
67:31 - next we typically repeat the convolution
67:33 - and pooling steps multiple times using
67:36 - previous features as input
67:39 - then we add a few fully connected hidden
67:41 - layers to help classify the image
67:43 - and finally we produce a classification
67:46 - prediction in the output layer
67:48 - convolutional neural networks are used
67:51 - heavily in the field of computer vision
67:53 - and work well for a variety of tasks
67:55 - including image recognition image
67:57 - processing image segmentation video
68:00 - analysis and natural language processing
68:05 - in this section i'm going to discuss the
68:07 - 5 steps that are common in every deep
68:09 - learning project that you build
68:12 - these can be extended to include various
68:13 - other aspects but at its very core they
68:16 - are very fundamentally five steps
68:19 - data is at the core of what deep
68:21 - learning is all about
68:23 - your model will only be as powerful as
68:25 - the data you bring
68:27 - which brings me to the first step
68:29 - gathering your data
68:30 - the choice of data and how much data you
68:33 - would require entirely depends on the
68:35 - problem you're trying to solve
68:36 - picking the right data is key and i
68:38 - can't stress how important this part is
68:41 - bad data implies a bad model
68:45 - a good rule of thumb is to make
68:46 - assumptions about the data you require
68:48 - and be careful to record these
68:50 - assumptions so that you can test them
68:52 - later if needed data comes in a variety
68:54 - of sizes for example iris flaw data set
68:57 - contains about 150 images in the total
69:00 - set
69:01 - gmail smart reply has around 238 million
69:05 - examples in its running sets and google
69:07 - translate reportedly has trillions of
69:10 - data points
69:12 - when you're choosing a data set there's
69:14 - no one-size-fits-all but the general
69:16 - rule of thumb is that the amount of data
69:18 - you need for a well-performing model
69:20 - should be 10 times the number of
69:21 - parameters in that model
69:23 - however this may differ from time to
69:25 - time depending on the type of model
69:27 - you're building for example in
69:29 - regression analysis you should use
69:31 - around 10 examples per predictor
69:33 - variable
69:34 - for image classification the minimum you
69:37 - should have is around a thousand images
69:38 - per class that you're trying to classify
69:41 - while quantity of data matters quality
69:43 - matters too
69:45 - there's no use having a lot of data if
69:48 - it's bad data
69:49 - there are certain aspects of quality
69:51 - that tend to correspond to
69:52 - well-performing models
69:54 - one aspect is reliability
69:57 - reliability refers to the degree in
69:59 - which you can trust your data a model
70:01 - train on a reliable data set is more
70:04 - likely to yield useful predictions than
70:06 - model trained on unreliable data
70:09 - how common are label errors if your data
70:12 - is labeled by humans sometimes there may
70:14 - be mistakes are your features noisy is
70:17 - it completely accurate
70:19 - some noise is all right you'll never be
70:21 - able to purge your data of all the noise
70:23 - there are many other factors that
70:24 - determine quality for the purpose of
70:27 - this video though i'm not going to talk
70:28 - about the remaining although if you're
70:30 - interested i'll leave them in the show
70:32 - notes below lucky for us there are
70:34 - plenty of resources on the web that
70:36 - offer good data sets for free here are a
70:38 - few sites where you can begin your
70:40 - dataset search the uci machine learning
70:42 - repository maintains around 500
70:45 - extremely well maintained data sets that
70:47 - you can use in your deep learning
70:48 - projects
70:49 - kaggle's another one you'll love how
70:51 - detailed that data sets are they give
70:53 - you info on the features data types
70:56 - number of records and so on you can use
70:59 - a kernel too and you won't have to
71:00 - download the data set
71:02 - google's dataset search is still in beta
71:05 - but is one of the most amazing sites
71:07 - that you can find today
71:09 - reddit2 is a great place to request for
71:11 - data sets you want but again there is a
71:13 - chance of not being properly organized
71:15 - create your own data set that will work
71:18 - too you can use web scrapers like
71:19 - beautiful soup to get your required data
71:22 - for the data set
71:25 - after you have selected your data set
71:27 - you now need to think of how you're
71:28 - going to use this data
71:30 - there are some common pre-processing
71:32 - steps that you should follow
71:34 - first splitting the data set into
71:36 - subsets
71:38 - in general we usually split a data set
71:40 - into three parts training testing and
71:42 - validating sets
71:44 - we train our modules with the training
71:46 - set evaluated on the validation set and
71:49 - finally once it's ready to use test it
71:51 - one last time on the testing data set
71:54 - now it is reasonable to ask the
71:56 - following question
71:57 - why not have two sets training and
71:59 - testing in that way the process will be
72:02 - much simpler just train the model on the
72:04 - training data and test it on the testing
72:06 - data
72:08 - the answer to that is developing a model
72:10 - involves tuning its configuration in
72:13 - other words choosing certain values for
72:15 - the hyper parameters or the weight and
72:17 - biases this tuning is done with the
72:19 - feedback received from the validation
72:21 - set and is in essence a form of learning
72:24 - it turns out we just can't split the
72:26 - data set randomly do that and you'll get
72:28 - random results there has to be some kind
72:31 - of logic to split the data set
72:33 - essentially what you want is for all
72:35 - three sets the training testing and
72:37 - validation sets to be very similar to
72:40 - each other and to eliminate skewing as
72:42 - much as possible
72:44 - this mainly depends on two things
72:46 - first the total number of samples in
72:48 - your data and second or the actual model
72:51 - you're trying to train
72:53 - models with very few hyper parameters
72:55 - will be very easy to validate in tune so
72:58 - you can probably reduce the size of your
73:00 - validation set
73:01 - but if your model has many hyper
73:03 - parameters
73:04 - you would want to have a large
73:05 - validation set as well as consider cross
73:08 - validation
73:09 - also if you happen to have a model with
73:11 - no hyper parameters whatsoever or ones
73:14 - that cannot be easily tuned you probably
73:16 - don't need a validation set
73:18 - all in all like many other things in
73:20 - machine learning and deep learning the
73:22 - train test validation split ratio is
73:24 - also quite specific to your use case and
73:27 - it gets easier to make judgment as you
73:29 - train and build more and more models so
73:32 - here's a quick note on cross validation
73:35 - usually you'd want to split your data
73:37 - set into two the train and the test
73:40 - after this you keep aside the test set
73:42 - and randomly choose some percentage of
73:45 - the training set to be the actual train
73:47 - set and the remaining to be the
73:49 - validation set
73:51 - the model is then iteratively trained
73:53 - and validated on these different sets
73:56 - there are multiple ways to do this and
73:58 - this is commonly known as cross
74:00 - validation
74:01 - basically you use your training set to
74:03 - generate multiple splits of the train
74:06 - and validation set cross validation
74:08 - avoids overfitting and is getting more
74:10 - and more popular with k fold cross
74:12 - validation being the most popular method
74:15 - additionally if you're working on time
74:17 - series data a frequent technique is to
74:19 - split the data by time for example if
74:22 - you have a dataset with 40 days of data
74:24 - you can train your data from days 1 to
74:26 - 39 and evaluate your model on the data
74:30 - from day 40.
74:32 - for systems like this the training data
74:34 - is older than the serving data so this
74:37 - technique ensures your validation set
74:39 - mirrors the lag between training and
74:41 - serving
74:43 - however keep in mind that time-based
74:44 - splits work best with very very large
74:47 - data sets such as those with tens of
74:49 - millions of examples the second method
74:52 - that we have in pre-processing is
74:53 - formatting the data set that you've
74:55 - picked might not be in the right format
74:57 - that you like for example the data might
74:59 - be in the form of a database but you'd
75:02 - like it as a csv file vice versa
75:05 - of course there are a couple of ways to
75:06 - do this and you can google them if you'd
75:08 - like dealing with missing data is one of
75:10 - the most challenging steps in the
75:12 - gathering of data for your deep learning
75:14 - projects unless you're extremely lucky
75:16 - to land with the perfect data set which
75:18 - is quite rare
75:20 - dealing with missing data will probably
75:21 - take a significant chunk of your time
75:24 - it is quite common in real world
75:26 - problems to miss some values of our data
75:28 - samples
75:29 - this may be due to errors on the data
75:31 - collection blank spaces on surveys
75:34 - measurements not applicable and so on
75:36 - missing values are typically represented
75:38 - with the nan or the null indicators
75:42 - the problem with this is that most
75:44 - algorithms can't handle these kind of
75:46 - missing values so we need to take care
75:48 - of them before feeding data to our
75:49 - models
75:50 - there are a couple of ways to deal with
75:52 - them
75:53 - one is eliminating the samples of the
75:55 - features with missing values the
75:57 - downside of code is that you risk to
75:59 - delete relevant information
76:01 - the second step is to impute the missing
76:03 - values
76:04 - a common way is to set the missing
76:06 - values as the mean value for the rest of
76:08 - the samples but of course there are
76:10 - other ways to deal with specific data
76:11 - sets be smart as handling missing data
76:14 - in the wrong way can spell disasters
76:16 - sometimes you may have too much data
76:18 - that what you require more data can
76:20 - result in larger computational and
76:22 - memory requirements
76:24 - in cases like this it's best practice to
76:27 - use a small sample of the data set it
76:29 - will be faster and ultimately an
76:31 - increase in time for you to explore and
76:33 - prototype solutions
76:35 - in most real world data sets you're
76:37 - going to come across imbalanced data
76:39 - that is classification data that has
76:41 - skewed class proportions leading to the
76:44 - rise of a minority class and a majority
76:46 - class if we train a model on data like
76:49 - this a model will only spend time
76:52 - learning about the majority class and a
76:54 - lot less time on the minority class
76:57 - and hence a model will ultimately be
76:59 - biased to the majority class and so in
77:02 - cases like this we usually use a process
77:04 - called down sampling and up weighting
77:07 - which is essentially reducing majority
77:09 - cost by some factor and adding example
77:11 - weights of that factor to the down
77:13 - sample class
77:14 - for example if we down sample the
77:16 - majority cost by a factor of 10 then the
77:19 - example weighted we add to that class
77:21 - should be 10. it may seem odd to add
77:23 - example weights after down sampling
77:26 - what is its purpose well there are a
77:28 - couple of reasons
77:30 - at least a faster convergence during
77:32 - training we see the minority class more
77:34 - often which helps the model converge
77:37 - faster
77:38 - by consolidating the majority class into
77:40 - fewer examples with larger weights we
77:42 - spend less disk space storing them
77:45 - operating ensures their model is still
77:47 - calibrated we add operating after down
77:50 - sampling so as to keep the data set in
77:52 - similar proportion
77:54 - these processes essentially help a model
77:57 - see more of the minority costs rather
77:59 - than just solely the majority class
78:02 - this helps our model perform better in
78:04 - real world situations feature scaling is
78:06 - a crucial step in the pre-processing
78:08 - phase as the majority of deep learning
78:11 - algorithms perform much better when
78:13 - dealing with features that are on the
78:15 - same scale
78:16 - the most common techniques are
78:18 - normalization which refers to the
78:20 - rescaling of features to a range between
78:22 - 0 and 1
78:24 - which in fact is a special case of min
78:26 - max scaling
78:27 - to normalize that data we need to apply
78:30 - min max scaling to each feature column
78:33 - standardization consists of centering
78:36 - the field at mean 0 with standard
78:38 - deviation 1 so that the feature columns
78:41 - have the same parameters as a standard
78:43 - normal distribution that is 0 mean and
78:46 - unit variance
78:49 - this makes it much easier for the
78:50 - learning algorithms to learn the weights
78:52 - of the parameters in addition it keeps
78:55 - useful information about outliers and
78:58 - makes the algorithms less sensitive to
79:00 - them
79:01 - once our data has been prepared we now
79:04 - feed this into our network to trade
79:06 - we've discussed the learning process of
79:08 - a neural network in the previous module
79:10 - so if you are unsure i'd advise you to
79:12 - watch that module first
79:14 - but essentially once a data has been fed
79:17 - forward propagation occurs and the
79:19 - losses compared against the loss
79:21 - function and the parameters are adjusted
79:23 - based on this loss incurred again
79:25 - nothing too different from what we
79:27 - discussed previously
79:30 - your model has successfully trained
79:32 - congratulations
79:33 - now we need to test how good our model
79:35 - is using the validation set that we had
79:38 - set aside earlier
79:39 - the evaluation process allows us to test
79:42 - a model against data it has never seen
79:44 - before and this is meant to be
79:46 - representative of how good the model
79:48 - might perform in the real world
79:51 - after the evaluation process there's a
79:53 - high chance that your model could be
79:55 - optimized further
79:56 - remember we started with random weights
79:58 - and biases and these were fine-tuned
80:00 - during back propagation
80:03 - well in quite a few cases bad
80:04 - propagation won't get it right the first
80:06 - time and that's okay there are a few
80:09 - ways to optimize your model further
80:12 - tuning hyper parameters is a good way of
80:14 - optimizing your model's performance one
80:17 - way to do this is by showing the model
80:19 - the entire data set multiple times
80:21 - that is by increasing the number of
80:23 - epochs
80:24 - this has sometimes shown to improve
80:26 - accuracy in other ways by adjusting the
80:28 - learning rate we talked about what the
80:30 - learning rate was in the previous module
80:32 - so if you don't know what the learning
80:33 - rate is i do advise you to check out the
80:35 - previous module
80:37 - but essentially the learning rate
80:39 - defines how far we shift the line during
80:41 - each step based on information from the
80:43 - previous training step in back
80:44 - propagation
80:46 - these values all play a role in how
80:48 - accurate a model can become and how long
80:51 - the training takes
80:52 - for complex models initial conditions
80:54 - can play a significant role in
80:56 - determining the outcome of training
80:58 - there are many considerations at this
81:00 - phase of training and it's important you
81:02 - define what makes a model good enough
81:05 - otherwise you might find yourself
81:06 - tweaking parameters for a long long time
81:09 - the adjustment of these hyper parameters
81:11 - remains a bit of an art and is more of
81:13 - an experimental process that heavily
81:15 - depends on the specifics of your data
81:17 - set model and training process
81:20 - you will develop this as you go more and
81:22 - more into deep learning so don't worry
81:23 - too much about this now
81:26 - one of the more common problems you will
81:28 - encounter is when your model performs
81:30 - well on training data but performs
81:32 - terribly on data it's never seen before
81:34 - this is the problem of overfitting
81:37 - this happens when the model learns a
81:39 - pattern specific to the training data
81:41 - set that aren't relevant to other unseen
81:43 - data
81:45 - there are two ways to avoid this
81:46 - overfitting
81:48 - getting more data and regularization
81:52 - getting more data is usually the best
81:53 - solution a model trainer mode data will
81:56 - naturally generalize better
81:59 - reducing the model size by reducing the
82:02 - number of learnable parameters in the
82:03 - model and with it its learning capacity
82:06 - is another way
82:08 - however by lowering the capacity of the
82:10 - network you force it to learn patterns
82:12 - that matter or then minimize the loss
82:15 - on the other hand reducing the network's
82:17 - capacity too much will lead to
82:19 - underfitting the model will not be able
82:22 - to learn the relevant patterns in the
82:23 - trained data
82:24 - unfortunately there are no magical
82:26 - formulas to determine this balance it
82:28 - must be tested and evaluated by setting
82:31 - different number of parameters and
82:33 - observing its performance the second
82:35 - method to addressing overfitting is by
82:37 - applying weight regularization to the
82:39 - model
82:41 - a common way to achieve this is to
82:42 - constraint the complexity of the network
82:45 - by forcing its weights to take only
82:47 - small values
82:48 - regularizing the distribution of weight
82:50 - values
82:52 - this is done by adding to the loss
82:54 - function of the network a cost
82:56 - associated with having larger weights
82:58 - and this cost comes in two ways l1
83:01 - regularization at the cost with regards
83:04 - to the absolute value of the weight
83:06 - coefficient or the l1 norm of the
83:08 - weights
83:09 - l2 regularization adds a cost with
83:12 - regards to the squared value of the
83:14 - weight's coefficient that is the l2 norm
83:17 - of the weight
83:19 - another way of reducing overfitting is
83:21 - by augmenting data
83:23 - for a model to perform well or
83:24 - satisfactory we need to have a lot of
83:26 - data
83:27 - we've just have just already but
83:29 - typically if you're working with images
83:31 - there's always a chance that your model
83:33 - won't perform as well as you'd like it
83:35 - no matter how much data you have
83:38 - in cases like this when you have limited
83:40 - data sets
83:41 - data augmentation is a good way of
83:43 - increasing your data set without really
83:46 - increasing it we artificially augment
83:49 - our data or in this case images so that
83:52 - we get more data from already existing
83:54 - data so what kind of augmentations are
83:56 - we talking about well anything from
83:59 - flipping the image of the y-axis
84:01 - flipping over the x-axis applying blur
84:03 - to even zooming on in the image
84:06 - what this does is that it shows your
84:07 - model more than what meets the eye it
84:10 - exposes your model to more of the
84:12 - existing data so that in testing it will
84:15 - automatically perform better because it
84:17 - has seen images represented in almost
84:19 - every single form
84:21 - finally the last method we're going to
84:23 - talk about is dropout dropout is a
84:25 - technique used in deep learning that
84:27 - randomly drops out units or neurons in
84:30 - the network
84:31 - simply put dropout refers to the
84:33 - ignoring of neurons during the training
84:35 - phase of a randomly chosen set of
84:37 - neurons
84:39 - by ignoring i mean that these units are
84:41 - not considered during a particular
84:43 - forward or backward pass
84:46 - so why do we need dropout at all
84:48 - why do we need to shut down parts of a
84:50 - neural network a fully connected layer
84:53 - occupies most of the parameters and
84:55 - hence neurons develop a co-dependency
84:58 - amongst each other during training
85:00 - which curbs the individual power of each
85:02 - neuron and which ultimately leads to
85:04 - overfitting of the training data
85:06 - so drop out a good way of reducing
85:08 - overfitting
85:12 - i hope that this introductory course has
85:14 - helped you develop a good intuition of
85:16 - deep learning as a whole
85:18 - of course we've only just scraped the
85:19 - surface there's a whole new world out
85:21 - there
85:22 - if you like this course please consider
85:24 - liking and subscribing it really helps
85:26 - me make courses like this
85:28 - i have a couple of videos on computer
85:30 - vision with opencv that i will be
85:32 - releasing in a couple of weeks so stay
85:34 - tuned for that
85:35 - in the meantime good luck

Cleaned transcript:

you've probably read in the news that deep learning is the secret recipe behind many exciting developments and has made many of our world's dreams and perhaps also nightmares come true who would have thought that deep minds alphago could be at least a doll in a boat game which boasts in more possible moves than there are atoms in the entire universe a lot of people including me never saw it coming it seemed impossible but it's here now deep learning is everywhere it's beating physicians are diagnosing cancer it's responsible for translating web pages in a matter of mere seconds to the autonomous vehicles by weimo and tesla hi my name is jason and welcome to this course in deep learning where you'll learn everything you need to get started with deep learning in python how to build remarkable algorithms capable of solving complex problems that weren't possible just a few decades ago we'll talk about what deep learning is and the difference between artificial intelligence and machine learning i'll introduce new networks what they are and just how essential they are to deep learning you're going to learn about how deep learning models train and learn and the various types of learning associated supervised unsupervised and reinforcement learning we're going to talk about loss functions optimizers the grading descent algorithm the different types of neural network architectures and the various steps involved in deep learning this entire course is centered on the notion of deep learning but what is it deep learning is a subset of machine learning which in turn is a subset of artificial intelligence which involves more traditional methods to learn representations directly from data machine learning involves teaching computers to recognize patterns in data in the same way as our brains do so as humans it's easy for us to distinguish between a cat and a dog but it's much more difficult to teach a machine to do this and we'll talk more about this later on in this course before i do that i want to give you a sense of the amazing successes of deep learning in the past in 1997 gary kasparov the most successful champion in the history of chess lost to ibm's deep blue one of the first computer or artificial systems it was the first defeat of a reigning world chess champion by a computer in 2011 ibm's watson competed in game show jeopardy against his champions brad rotter and ken jennings and won the first prize a million dollars in 2015 alphago a deep learning computer program created by google's deepmind division defeated lisa dole an 18 time world champion at go a game of google more times complex than chess but deep learning can do more than just betas at boat games it finds applications anywhere from selfdriving vehicles to fake news detection to even predicting earthquakes these were astonishing moments not only because machines beat humans at their own games but because of the endless possibilities that they opened up what followed such events have been a series of striking breakthroughs in artificial intelligence machine learning and yes deep learning to put it simply deep learning is a machine learning technique that learns features and tasks directly from data by running inputs through a biologically inspired neural network architecture these neural networks contain a number of hidden layers through which data is processed allowing for the machine to go deep in its learning making connections and weighing input for the best results we'll go over neural networks in the next video so why deep learning the problem with traditional machine learning algorithms is that no matter how complex they get they'll always be machine like they need a lot of domain expertise human intervention and are only capable of what they're designed for for example if i show you the image of a face you will automatically recognize it's a face but how would a computer know what this is well if we follow traditional machine learning we'd have to manually and painstakingly define to a computer what it faces for example it has eyes ears and mouth but now how do you define an eye or a mouth to a computer well if you look at an eye the corners are at some angle they're definitely not 90 degrees they're definitely not zero degrees there's some angle in between so we could work with that and train our classifier to recognize these kinds of lines in certain orientations this is complicated for ei practitioners and the rest of the world that's where deep learning holds a bit of promise the key idea in deep learning is that you can learn these features just from raw data so i can feed a bunch of images or faces to my deep learning algorithm and it's going to develop some kind of hierarchical representation of detecting lines and edges and then using these lines and edges to detect eyes and a mouth and composing it together to ultimately detect the face as it turns out the underlying algorithms for training these models have existed for quite a long time so why has deep learning gaining popularity many decades later well for one data has become much more pervasive we're living in the age of big data and these algorithms require massive amounts of data to effectively be implemented second we have hardware and architecture that are capable of handling the vast amount of data and computational power that these algorithms require hardware that simply wasn't available a few decades ago third building and deploying these algorithms models as i call is extremely streamlined with the increasing popularity of open source software like tensorflow and pytorch deep learning models refer to the training of things called neural networks neural networks form the basis of deep learning a subfield of machine learning where algorithms are inspired by the structure of the human brain just like neurons make up the brain the fundamental building blocks of a neural network is also a neuron neural networks take in data they train themselves to recognize patterns in this data and predict outputs for a new set of similar data in a new network information propagates through three central components that form the basis of every neural network architecture the input layer the output layer and several hidden layers between the two in the next video we'll go over the learning process of a neural network the learning process of a neural network can be broken into two main processes forward propagation and back propagation full propagation is the propagation of information from the input layer to the output layer we can define our input layer as several neurons x1 through xn these neurons connect to the neurons of the next layer through channels and they are assigned numerical values called weights the inputs are multiplied to the weights and their sum is sent as input to the neurons in the hidden layer where each neuron in turn is associated to a numerical value called the bias which is then added to the input sum this weighted sum is then passed through a nonlinear function called the activation function which essentially decides if that particular neuron can contribute to the next layer in the output layer it's basically a form of probability the neuron with the highest value determines what the output finally is so let's go over a few terms the weight of a neuron tells us how important the neuron is the higher the value the more important it is in the relationship the bias is like the new on having an opinion to the relationship it serves to shift the activation function to the right or to the left if you have had some experience with high school math you should know that adding a scalar value to a function shifts a graph either to the left or to the right and this is exactly what the bias does it shifts the activation function to the right or to the left that propagation is almost like for propagation except in the reverse direction information here is passed from the output layer to the hidden layers not the input layer but what information gets passed on from the output layer isn't the output layer supposed to be the final layer where we get the final output well yes but no bad propagation is the reason why new networks is so powerful it is the reason why new networks can learn by themselves in the last step before propagation a new network spits out a prediction this prediction could have two possibilities either right or wrong in bad propagation the new network evaluates its own performance and checks if it is right or wrong if it is wrong the network uses something called a loss function to quantify the deviation from the expected output and it is this information that's sent back to the hidden layers for the weights and biases to be adjusted so that the network's accuracy level increases let's visualize the training process with a real example let's suppose we have a data set this dataset gives us the weight of a vehicle and the number of goods carried by that vehicle and also tells us if those vehicles are cars or trucks we want to go through this data trade a new networks to predict cars or trucks based on their weights and goods to start off let's initialize the neural network by giving it random weights and biases these can be anything we really don't care what these values are as long as they're there in the first entry of a data set we have vehicle weight equal to a value which in this case is 15 and good as two according to this it's a car we now start moving these input dimensions through the newer network so basically what we want to do is take both the inputs multiply them by their weight and add a bias and this is where the magic happens we run this weighted sum through an activation function okay now let's say that the output of this activation function is 0.001 this again is multiplied by the weight and added to the bias and finally in the output layer we have a guess now according to this neural network the type of vehicle with weight 15 and goods 2 has a greater probability of being a truck of course this is not true and a new network knows this so we use back propagation we're going to quantify the difference between the expected result and the predicted output using a loss function in bank propagation we're going to go backwards and adjust our initial rates and biases remember that during the initialization of the neural network we chose completely random weight and biases well during back propagation these values will be adjusted to better fit the prediction model okay so that was one iteration through the first piece of the data set in the second entry we have vehicle weight 34 and goods 67. we're going to use the same process as before multiply the input with the weight and add a box pass this result into an activation function and repeat till the output layer check the error difference and employ back propagation to adjust the weights in the biases your new network will continue doing this repeated processor for propagation calculating the error and then back propagation for as many entries there are in this data set the more data you give the newer network the better it will be at predicting the right output but there's a tradeoff because too much data and you'll end up with a problem like overfitting which i'll discuss later in this course but that's essentially how a new network works you feed input the network initializes with random weights and biases that are adjusted each time during back propagation until the network's gone through all your data and is now able to make predictions this learning algorithm can be summarized as follows first we initialize the network with random values for the network's parameters or the weights in the biases we take a set of input data and pass them through the network we compare these predictions obtained with the values of the expected labels and calculate the loss using a loss function we perform back propagation in order to propagate this loss to each and every weight and bias we use this propagated information to update the weights and biases of neural network with the gradient descent algorithm in such a way that the total loss is reduced and a better model is obtained the last step is continue iterating the previous steps until we consider that we have a good enough model in this section we're going to talk about the most common terminologies used in deep learning today let's start off with the activation function the activation function serves to introduce something called nonlinearity into the network and also decides whether a particular neuron can contribute to the next layer but how do you decide if the neuron can fire or activate well we had a couple of ideas which led to the creation of different activation functions the first idea we had is how about we activate the neuron if it is above a certain value or threshold if it is less than the threshold don't activate it activation function a is equal to activated if y is greater than some threshold else it's not this is essentially a step function its output is 1 or activated when value is greater than 0. its output is activated when value is greater than some threshold and outputs not activated otherwise great so this makes an activation function for a neuron no confusions life is perfect except there are some drawbacks with this to understand about it think about the following think about a case where you want to classify multiple such neurons into classes say class 1 class 2 class 3 etc what will happen if more than one neuron is activated all these neurons will output a one well how do you decide now how do you decide which class it belongs to it's complicated right you would want the network to activate only one neuron and the other should be zero only then you would be able to say it was classified properly in real practice however it is harder to train and converge it this way it would be better if the activation was not binary and instead some probable value like 75 activated or 16 activated there's a 75 chance that it belongs to class 2 etc then if more than one neuron activates you could find which neuron fires based on which has the highest probability okay maybe you'll ask yourself i want something to give me a more analog value rather than just saying activated or not activated something other than in binary and maybe you would have thought about a linear function a straight line function where the activation is proportional to the input by a value called the slope of the line this way it gives us a range of activations so it isn't binary activation we can definitely connect a few neurons together and if more than one fires we could take the maximum value and decide based on that so that is okay too and what is the problem with this well if you are familiar with gradient descent which i'll come to in just a bit you'll notice that the derivative of a linear function is a constant makes sense because the slope isn't changing at any point for a function f of x is equal to mx plus c the derivative is m this means that the gradient has no relationship whatsoever with x this also means that during back propagation the adjustments made to the weights and the biases aren't dependent on x at all and this is not a good thing additionally think about if you have connected layers no matter how many layers you have if all of them are linear in nature the activation function of the final layer is nothing but just a linear function of the input of the first layer pause for a bit and think about it this means that the entire neural network of dozens of layers can be replaced by a single layer remember a combination of linear functions in the linear manner is still another linear function and this is terrible because we've just lost the ability to stack layers this way no matter how much we stack the whole network is still equivalent to a single layer with single activation next we have a sigmoid function and if you've ever watched a video on activation functions this is the kind of function used in the examples a sigmoid function is defined as a if x is equal to 1 over 1 plus e to the negative x well this looks smooth and kind of like a step function what are its benefits think about it for a moment well first things first it has nonlinear nature combinations of this function are also nonlinear great so now we can stack layers what about nonbinary activations yes that too this function outputs an analog activation unlike the step function and also has a smooth gradient an advantage of this activation function is that unlike the linear function the output of this function is going to be in the range zero to one inclusive compared to the negative infinity to infinity of the latter so we have activations bound in a range and this won't blow up the activations and this is great and sigmoid functions are one of the most widely used activation functions today but life isn't always rosy and sigmoids two tend to have the share of disadvantages if you look closely between x is equal to negative two and x is equal to two the y values are very steep any small changes in values of x in that region will cause values of y to change drastically also towards either end of the function the y values tend to respond very less to changes in x the gradient at those regions is going to be really really small almost zero and it gives rise to the vanishing gradient problem which just says that if the input to the activation function is either large or small the sigmoids are going to squish that down to a value between zero and one and the gradient of this function becomes really small and you'll see why when we talk about gradient descent this is a huge problem another activation function that is used is the tan h function this looks very similar to sigmoid in fact mathematically this is what's known as a shifted sigmoid function okay so like the sigmoid it has characteristics that we discussed above it is nonlinear nature so we can stack layers it is bound to arrange from negative one to one so there's no worrying about the activations blowing up the derivative of the tanning function however is steeper than that of the sigmoid so deciding between the sigmoid and the tanh will really depend on your requirement of the gradient strength like sigmoid tanh is also a very popular and widely used activation function and yes like the sigmoid tanh does have a vanishing gradient problem the rectified linear unit or the value function is defined as a of x is equal to the max from 0 to x at first look this would look like a linear function right the graph is linear in the positive axis let me tell you rather is in fact nonlinear nature and combinations of relu are also nonlinear great so this means that we can stack layers however unlike the previous two functions that we discussed is not bounded the range of the relu is from zero to infinity this means there is a chance of blowing up the activation another point i would like to discuss here is sparsity of inactivation imagine a big neural network with lots of neurons using a sigmoid or a tanning will cause almost all the neurons to fire in an analog way this means that almost all activations will be processed to describe the network's output in other words the activation would be dense and this is costly ideally we want only a few neurons in the network to activate and thereby making the activations pass and efficient here's where the relu comes in imagine a network with randomly initialized weights and almost 50 percent of the network yields zero activation because of the characteristic relu it outputs zero for negative values of x this means that only 50 percent of the neurons fire sparse activation making the network lighter but when life gives you an apple it comes with a little worm inside because of that horizontal line in relu for negative values of x the gradient is zero in that region which means that during back propagation the weights will not get adjusted during descent this means that those neurons which go into that state will stop responding to variations in the error simply because the gradient is zero nothing changes this is called the dying value problem this problem can cause several neurons to just die and not respond thus making a substantial part of the network passive rather than what we want out of there are workarounds for this one way especially is to simply make the horizontal line into a nonhorizontal component by adding a slope usually the slope is around 0.001 and this this new version of the relu is called leaky value the main idea is that the gradient should never be zero one major advantage of the relu is the fact that it's less computationally expensive than functions like tannage and sigmoid because it involves simpler mathematical operations this is a really good point to consider when you were designing your own deep neural networks great so now the question is which activation function to use because of the advantages that relu offers does this mean that you should use reload for everything you do or could you consider sigmoid and tan h well both when you know the function that you're trying to approximate has certain characteristics you should choose an activation function with which will approximate the function faster leading to faster training processes for example a sigmoid function works well for binary classification problems because approximating our classifier functions as combinations of the sigmoid is easier than maybe the relu this will lead to faster training processes and larger convergence you can use your own custom functions too if you don't know the nature of the function you're trying to learn i would suggest you start with relu and then work backwards from there before we move on to the next section i want to talk about why we use nonlinear activation functions as opposed to linear ones if you recall in my definition of activation functions i mentioned that activation functions serve to introduce something called nonlinearity in the network for all intensive purposes introducing nonlinearity simply means that your activation function must be nonlinear that is not a straight line mathematically linear functions are polynomials of degree 1 that when graphed in the x y plane are straight lines inclined to the xaxis at a certain value we call this the slope of the line nonlinear functions are polynomials of degree greater than one and when graphed they don't form straight lines rather than more curved if we use linear activation functions to model our data then no matter how many hidden layers our network has it will always become equivalent to having a single layer network and in deep learning we want to be able to model every type of data without being restricted as would be the case should we use linear functions we discussed previously in the learning process of neural networks that we started with random weight and biases the neural network makes a prediction this prediction is compared against the expected output and the weights and biases are adjusted accordingly well loss functions are the reason that we're able to calculate that difference really simply a loss function is a way to quantify the deviation of the predicted output by the neural network to the expected output it's as simple as that nothing mo nothing less there are plenty of loss functions out there for example under regression we have squared error loss absolute ever loss in huber loss in binary classification we have binary cross entropy and hinge loss in multiclass classification problems we have the multiclass cross entropy and the callback liability divergence loss and so on the choice of the best function really depends on what kind of project you're working on different projects require different loss functions now i don't want to talk any further loss functions right now we'll do this under the optimization section because that's really where most functions are utilized in the previous section we dealt with loss functions which are mathematical ways of measuring how wrong predictions made by neural network are during the training process we tweak and change the parameters or the weights of the model to try and minimize that loss function and make our predictions as correct and optimized as possible but how exactly do you do that how do you change the parameters of your model by how much and when we have the ingredients how do we make the cake this is where optimizers come in they tied together the loss function and model parameters or the weight and biases by updating the network in response to the output of the loss function in simpler terms optimizers shape and mold your model into more accurate models by adjusting the weights and the biases the loss function is its guide it tells the optimizer whether it's moving in the right or the wrong direction to understand this better imagine did you have just killed mount everest and now you decide to descend the mountain blindfolded it's impossible to know which direction to go in you could either go up which is away from your goal or go down which is towards your goal but to begin you would start taking steps using your feet you'll be able to gauge whether you're going up or down in this analogy you resemble the neural network going down your goal is trying to minimize the error and your feet are resemblance of the loss functions they measure whether you're going in the right way or the wrong way similarly it's impossible to know what your model's weights should be right from the start but with some trial and error based on the loss function you could end up getting there eventually we now come to grading descent often called the grand daddy of optimizers grading descent is an iterative algorithm that starts up at a random point in the loss function and travels down its slope in steps until it reaches the lowest point or the minimum of the function it is the most popular optimizer we use nowadays it's fast robust and flexible and here's how it works first we calculated what a small change in each individual weight would do to the loss function we adjust each individual weight based on its gradient that is take a small step in the determined direction the last step is to repeat the first and the second step until the loss function gets as low as possible i want to talk about this notion of a gradient the gradient of a function is the vector of the partial derivatives with respect to all independent variables the gradient always points in the direction of the steepest increase in the function suppose we have a graph like so with loss on the yaxis and the value of the weight on the xaxis we have a little data point here that corresponds to the randomly initialized weight to minimize this loss that is to get this data point to the minimum of the function we need to take the negative gradient since we want to find the steepest decrease in function this process happens iteratively through the losses as minimized as possible and that's grading descent in a nutshell when dealing with high dimensional data sets that is a lot of variables it's possible you'll find yourself in an area where it seems like you've reached the lowest possible value for your loss function but in reality it's just a local minimum to avoid getting stuck in a local minima we make sure we use the proper learning rate changing our weights too fast by adding or subtracting too much that is taking steps that are too large or too small can hinder your ability to minimize the loss function we don't want to make a jump so large that we skip over the optimal value for a given weight to make sure this doesn't happen we use a variable called the learning rate this thing is usually just a small number like 0.001 that we multiply the gradients by to scale them this ensures that any changes we make to our weights are pretty small in math talk taking steps that are too large can mean that the algorithm will never converge to an optimum at the same time we don't want to take steps that are too small because then we might never end up with the right values for our weights in math talk steps that are too small might lead to our optimizer converging on a local minimum for the loss function but never the absolute minimum for a simple summary just remember that the learning rate ensures that we change our weight at the right pace not making any changes that are too big or too small instead of calculating the gradients for all your training examples on every part of the gradient descent it's sometimes more efficient to only use a subset of the training examples each time stochastic gradient descent is an implementation that either uses batches of examples at a time or random examples on each pass stochastic gradient descent uses the concept of momentum momentum accumulates gradients of the past steps to dictate what might happen in the next steps also because we don't include the entire training set sjd is less computationally expensive it's difficult to overstate how popular gradient descent really is back propagation is basically gradient descent implemented on a network there are other types of optimizers based on gradient descent that are used today ad grad adapts the learning rate specifically to individual features that means that some of the weights in your data set will have different learning rates than others this works really well for sparse data sets where a lot of input examples are missing at a grad has a major issue though the adaptive learning rate tends to get really really small over time rms prop is a special version of adegrad developed by professor jeffrey hinton instead of letting all the gradients accumulate for momentum it accumulates gradients in a fixed window rms prop is similar to add a prop which is another optimizer that seeks to solve some of the issues that atograd leaves open atom stands for adaptive moment estimation and is another way of using past gradients to calculate the carbon gradient atom also utilizes the concept of momentum which is basically our way of telling the neural network whether we want past changes to affect the new change by adding fractions of the previous gradients to the current one this optimizer has become pretty widespread and is practically accepted for use in training new networks it's easy to get lost in the complexity of some of these new optimizers just remember that they all have the same goal minimizing the loss function and trial and error will get you there you may have heard me referring to the words parameters quite a bit and often this word is confused with the term hyperparameters in this video i'm going to outline the basic difference between the two a model parameter is a variable that is internal to the new network and whose values can be estimated from the data itself they are required by the model when making predictions these values define the skill of the model on your problem they can be estimated directly from the data and are often not manually set by the practitioner and oftentimes when you save your model you are essentially saving your model's parameters parameters are key to machine learning algorithms and examples of these include the weights and the biases a hyper parameter is a configuration that is external to the model and whose value cannot be estimated from data there's no way that we can find the best value for a model hyper parameter on a given problem we may use rules of thumb copy values used in other problems or search for the best value by trial and error when a machine learning algorithm is tuned for a specific problem such as when you're using grid search or random search then you are in fact tuning the hyper parameters of the model in order to discover the parameters that result in more skillful predictions model hyper parameters are often referred to as parameters which can make things confusing so a good rule of thumb to overcome this confusion is as follows if you have to specify a parameter manually then it is probably a hyper parameter parameters are inherent to the model itself some examples of hyper parameters include the learning rate for training on your network the c in sigma hyper parameters for sport vector machines and the k and k newest neighbors we need terminologies like epochs batch size and iterations only when the data is too big which happens all the time in machine learning and when we can't pass all this data to the computer at once so to overcome this problem we need to divide the data set into smaller chunks give it to our computer one by one and update the weights of the new network at the end of every step to fit it into the data given one epoch is when an entire data set is passed forward and backward through the network once in a majority of deep learning models we use more than one epoch i know it does make sense in the beginning why do we need to pass the entire data set many times through the same neural network passing the entire data set through the network only once is trying to read the entire lyrics of a song once he won't be able to remember the entire song immediately you have to reread the lyrics a couple more times before you can say you know the song by memory the same is true with the neural network we pass the data set multiple times through the neural network so it's able to generalize better gradient descent is an iterative process and updating the parameters and back propagation in a single pass or one epoch is not enough as the number of epochs increases the more the parameters are adjusted leading to a better performing model but too many epochs could spell disaster and lead to something called overfitting where a model has essentially memorized the patterns in the training data and performs terribly on data it's never seen before so what is the right number of epochs unfortunately there is no right answer the answer is different for different data sets sometimes your data set can include millions of examples passing this entire data set at once becomes extremely difficult so what we do instead is divide the data set into a number of batches rather than passing the entire dataset once the total number of training examples present in a single batch is called a batch size iterations is the number of batches needed to complete one epoch note the number of batches is equal to the number of iterations for one epoch let's say that we have a data set of 34 000 training examples if we divide the data set into batches of 500 then it will take 68 iterations to complete one epoch well i hope that gives you some kind of sense about the very basic terminologies used in deep learning before we move on i do want to mention this and you will see this a lot in deep learning you'll often have a bunch of different choices to make how many hidden layers should i choose or which activation function must i use and where and to be honest there are no clearcut guidelines as to what your choice should always be that's a fun part about deep learning it's extremely difficult to know in the beginning what's the right combination to use for your project what works for me might not work for you and a suggestion from my end would be that you dabble along with material shown try various combinations and see what works for you best ultimately that's a learning process pun intended throughout this course i'll give you quite a bit of intuition as to what's popular so that when it comes to building a deep learning project you won't find yourself lost in this section we're going to talk about the different types of learning which are machine learning concepts but i extended to deep learning as well in this course we'll go over supervised learning unsupervised learning and reinforcement learning supervised learning is the most common sub branch machine learning today typically if you're new to machine learning your journey will begin with supervised learning algorithms let's explore what these are supervised machine learning algorithms are designed to learn by example the name supervised learning originates from the idea that training this type of algorithm is almost like there's a human supervising the whole process in supervised learning we train our models on welllabeled data each example is a pair consisting of an input object which is typically a vector and a desired output value also called a supervisory signal during training a supervised learning algorithm will search for patterns in the data that correlate with the desired outputs after training it will take in new unseen inputs and will determine which label the new inputs will be classified as based on prior training data the objective of a supervised learning model is to predict the correct label for newly presented input data at its most basic form a supervised learning algorithm can simply be written as y is equal f x where y is the predicted output that is determined by a mapping function that assigns a class to an input value x the function used to connect input features to a predicted output is created by the machine learning model during training supervised learning can be split into two subcategories classification and regression during training a classification algorithm will be given data points with an assigned category the job of a classification algorithm is then to take this input value and assigned to a class of category that it fits into based on the training data provided the most common example of classification is determining if an email is spam or not with two classes to choose from spam or not spam this problem is called a binary classification problem the algorithm will be given training data with emails that are both spam and not spam and the model will find the features within the data that correlate to either class and create a mapping function then when provided with an unseen email the model will use this function to determine whether or not the email is pam an example of a classification problem would be the mnist handwritten digits dataset where the inputs are images of handwritten digits pixel data and the output is the class label for what digit the image represents that is numbers zero to nine there are numerous algorithms to solve classification problems each which depends on the data and the situation here are a few popular classification algorithms linear classifiers support vector machines decision trees knearest neighbors and drellum forest regression is a predictive statistical process where the model attempts to find the important relationship between dependent and independent variables the goal of a regression algorithm is to predict a continuous number such as sales income and tax scores the equation for a basic linear regression can be written as follows where x and 5 represents the features of the data and w of i and b are parameters which are developed during training for simple linear regression models with only one feature in the data the formula looks like this where w is the slope x is the single feature and b is a yintercept familiar for simple regression problems such as this the model's predictions are represented by the line of best fit for models using two features a plane is used and for models with more than two features a hyperplane is used imagine we want to determine a student's test grade based on how many hours they study the week of the test let's say the plot data with line of best fit looks like this there is a clear positive correlation between our studied the independent variable and the student's final test goals the dependent variable a line of best fit can be drawn through the data points to show the model's predictions when given new input say we wanted to know how well a student would do with five hours of study we can use the line of best fit to predict the test call based on other students performances another example of regression problem would be the boston house prices data set where the input of variables that describe a neighborhood and the output is a house price in dollars there are many different types of regression algorithms three most common are linear regression lasso regression and multivariate regression supervised learning finds applications in classification and regression problems like bioinformatics such as fingerprint iris and face recognition in smartphones object recognition spam detection and speech recognition unsupervised learning is a branch of machine learning that is used to manifest underlying patterns and data and is often used in exploratory data analysis unlike supervised learning unsupervised learning does not use label data but instead focuses on the data's features label training data has a corresponding output for each input the goal of an unsupervised learning algorithm is to analyze data and find important features in that data unsupervised learning will often find subgroups or hidden patterns within the dataset that a human observer might not pick up on and this is extremely useful as we'll soon find out unsupervised learning can be of two types clustering and association clustering is the simplest and among the most common applications of unsupervised learning it is the process of grouping the given data into different clusters or groups classes will contain data points that are as similar as possible to each other and as dissimilar as possible to data points in other clusters clustering helps find underlying patterns within the data that may not be noticeable through a human observer it can be broken down into partitional clustering and hierarchical clustering partitional clustering refers to a set of clustering algorithms where each data point in a data set can belong to only one cluster hierarchical clustering finds clusters by a system of hierarchies every data point can belong to multiple clusters some classes will contain smaller clusters within it this hierarchy system can be organized as a tree diagram some of the more commonly used clustering algorithms are the kmeans expectation and the hierarchical cluster analysis of the aca association on the other hand attempts to find relationships between different entities the classic example of association rules is market basket analysis this means using a database of transactions in the supermarket to find items that are frequently bought together for example a person who buys potatoes and burgers usually buys beer for example the person who buys tomatoes and pizza cheese might want to bring pizza bread and so on unsupervised learning finds applications almost everywhere for example airbnb which helps host stays and experiences and connect people all over the world this application uses unsupervised learning algorithms where a potential client queries their requirements and airbnb learns these patterns and recommends stays and experiences which fall under the same group of cluster example a person looking for houses in san francisco might not be interested in finding houses in boston amazon also uses unsupervised learning to learn the customers purchases and recommend products which are frequently bought together which is an example of association rule mining credit card fraud detection is another unsupervised learning algorithm that learns the various patterns of a user and and their usage of a credit card if the card is used in parts that do not match the behavior an alarm is generated which could possibly be marked as fraud and in some cases your bank might call you to confirm whether it was you using the card or not reinforcement learning is a type of machine learning technique that enables an agent to learn in an interactive environment by trial and error using feedback from its own actions and experiences like supervised learning it uses mapping between the input and the output but unlike supervised learning where feedback provided to the agent is a correct set of actions for performing a task reinforcement learning uses rewards and punishments as signals for positive and negative behavior when you compare with unsupervised learning reinforcement learning is different in terms of its goals while the goal in unsupervised learning is to find similarities and differences between data points in reinforcement learning the goal is to find a suitable action model that would maximize the total cumulative reward of the agent reinforcement learning refers to goaloriented algorithms which learn how to attain a complex objective or goal or how to maximize along a particular dimension over many steps for example they can maximize the points of one in a game over many moves reinforcement learning algorithms can start from a blank slate and under the right conditions achieve superhuman performance like a pet incentivized by scolding and treats these algorithms are penalized when they make the wrong decisions and rewarded when they make the right ones this is reinforcement reinforcement learning is usually modeled as a markov decision process although other frameworks like queue learning are used some key terms that describe the elements of a reinforcement learning problem are the environment which is the physical world in which the agent operates the state represents the current situation of the agent reward is a feedback received from the environment policy sometimes is the method to map the agent state to the agent's actions and finally value is the future reward that an agent will receive by taking an action in a particular state a reinforcement learning problem can be best explained through games let's take the game of pacman where the goal of the agent or pacman is to eat the food in the grid while avoiding the ghosts on its way the grid world is the interactive environment for the agent pacman receives a reward for eating food and punishment if it gets killed by the ghost that is it loses the game the states are the location of pacman in the grid world and the total cumulative reward is pacman winning the game reinforcement learning finds applications in robotics business strategy planning traffic light control web system configuration and aircraft and robot motion control a central problem in deep learning is how to make an algorithm that will perform well not just in training data but also on new inputs one of the most common challenges you'll face when training models is a problem of overfitting a situation where your model performs exceptionally well on training data but not in testing data see i have a data set graphed on the xy plane like so now i want to construct a model that would best fit the data set what i could do is draw a line of some random slope in intercept now evidently this isn't the best model and in fact this is called underfitting because it doesn't fit the model well in fact it underestimates the data set instead what we could do is draw a line that looks something like this now this really fits our model the best but this is overfitting remember that while training we show our networks and training data and once that's done we'd expect it to be almost close to perfect the problem with this graph is that although it is probably the best line of fit for this graph it is the best line of fit only if you're considering your trading data what your network is done in this graph is memorize the patterns between the trading data and won't give accurate predictions at all on data it's never seen before and this makes sense because instead of memorizing patterns generally to perform well on both training as well as new testing data our network in fact has memorized the patterns only on the training data so it obviously won't perform well on new data it's never seen before this is the problem of overfitting it fitted too much and by the way this would be the more accurate kind of fitting it's not perfect but it'll do well on both training as well as new testing data with sizeable accuracy there are a couple of ways to tackle overfitting the most interesting type of regularization is dropout it produces very good results and is consequently the most frequently used regularization technique in the field of deep learning to understand dropout let's say that we have a new network with two hidden layers what dropout does is that at every iteration it randomly selects some nodes and removes them along with their incoming and outgoing connections as shown so each iteration has a different set of nodes and this results in a different set of outputs so why do these models perform better these models usually perform better than a single model as they capture more randomness and memorizes less of the training data and hence will be folks to generalize better and build a more robust predictive model sometimes the best way to make a deep learning model generalized better is to train it on mode data in practice the amount of data we have is limited and one way to get around this problem is to create fake data and add it to the training set for some deep learning tasks it is reasonably straightforward to create new fake data this approach is easiest for classification a classifier needs to take complicated high dimensional input x and summarize it with the category identity y this means that the main task facing a classifier is to be invariant to a wide variety of transformations we can generate new xy pairs easily just by applying transformations on the xy inputs in our training set dataset augmentation has been a particularly effective technique for a specific classification problem object recognition images are high dimensional and include an enormous range of factors of variation many of which can easily be simulated operations like translating the training images a few pixels in each direction can often greatly improve generalization many other operations such as rotating the image or scaling the image have also proved quite effective you must be careful not to apply transformation that would change the correct class for example in optical character recognition tasks that require recognizing the difference between a b and a d and the difference between a six and a nine horizontal flips and 180 degree rotations are not appropriate ways of augmenting data sets with these tasks when training large models with sufficient representational capacity to overfit the task we often observe that the training error decreases steadily over time but the error validation set begins to rise again this means we can obtain a model with better validation set error and thus hopefully better test that error by stopping training at the point where the error in the validation set starts to increase this strategy is known as early stopping it is probably the most commonly used form of regularization in deploying today its popularity is due to both its effectiveness and its simplicity in this section i'm going to introduce the three most common types of neural network architectures today fully connected v4 with new networks recurring neural networks and convolutional neural networks the first type of new network architecture we're going to discuss is a fully connected feed forward neural network by fully connected i mean that each neuron in the preceding layer is connected to every neuron in the subsequent layer without any connection backwards there are no cycles or loops in the connections in the network as i mentioned previously each neuron in a neural network contains an activation function that changes the output of a neuron when given its input there are several types of activation functions that can change this input to output relationship to make a neuron behave in a variety of ways some of the most wellknown activation functions are the linear function which is a straight line that essentially multiplies the input by a constant value the sigmoid function that ranges from zero to one the hyperbolic tangent of the tanning function ranges from negative one to positive one and the rectified linear unit or the relu function which is a piecewise function that outputs a zero if the input is less than a certain value or a linear multiple if the input is greater than a certain value each type of activation function has its pros and cons so we use them in various layers in the deep neural network based on the problem each is designed to solve in addition the last three activation functions we refer to as nonlinear functions because the output is not a linear multiple of the input nonlinearity is what allows deep neural networks to model complex functions using everything we've learned so far we can create a wide variety of fully connected feed for neural networks we can create networks with various inputs various outputs various hidden layers neurons per hidden layer and a variety of activation functions these numerous combinations allow us to create a variety of powerful deep neural networks that can solve the wide array of problems the more neurons we add to each hidden layer the wider the network becomes in addition the more hidden layers we add the deeper the network becomes however each neuron we add increases the complexity and thus the computational resource necessary to train a new network increases this increasing complexity isn't linear in the number of neurons we add so it leads to an explosion in complexity and training time for large neural networks that's the tradeoff you need to consider when you are building deep neural networks all the new networks we've discussed so far are known as feed forward neural networks they take in a fixed sized input and give you a fixed size output that's all it does and that's what we expect neural networks to do take in an input and give a sizeable output but as it turns out these plane of vanilla neural networks aren't able to model every single problem that we have today to better understand this use this analogy suppose i show you the picture of a ball a round spherical ball that was moving in space in some direction i've just taken a photo of the ball or a snapshot of the ball at some time t now i want you to predict the next position of the ball in say two or three seconds you're probably not going to give me an accurate answer now let's look at another example suppose i walk up to you and say the word dog you will never understand my statement because well it doesn't make sense there are trilling combinations solely using the word dog and among these trillion combinations i'm expecting you to now guess what i'm trying to tell you what these two examples have in common is that it doesn't make sense it doesn't in the first case i'm expecting you to predict the next position in time and in the second i'm expecting you to understand what i mean by dog these two examples cannot be understood and interpreted unless some information about the pass was supplied now in the first example if i give you the previous position states of the ball and now ask you to predict the future trajectory of the ball you're going to be able to do this accurately and in the second case if i give you a full sentence saying i have a dog this makes sense because now you understand that out of the trillion possible combinations involving a dog my original intent was for you to understand that i have a dog why did i give you this example how does this apply to neural networks in the introduction i said vanilla neural networks can't model every single situation or problem that we have and the biggest problem it turns out is that plain vanilla feed forward neural networks cannot model sequential data sequential data is data in a sequence for example a sentence is a sequence of what a ball moving in space is a sequence of all its position states in the sentence that i've shown you you understood each word based off your understanding of the previous words this is called sequential memory you were able to understand the data point in the sequence by your memory of the previous data point in that sequence traditional neural networks can't do this and it seems like a major shortcoming one of the disadvantages of modelling sequences with traditional neural networks is the fact that they don't share parameters across time let us take for example these two sentences on tuesday it was raining and it was raining on tuesday these sentences mean the same thing although the details are in different parts of the sequence actually when we feed these sentences into a field for new network for a prediction task the model will assign different weights to on tuesday and it was raining at each moment in time things we learn about the sequence won't transfer if they appear at different points in the sequence sharing parameters gives the network the ability to look for a given feature everywhere in the sequence rather than just in a certain area that's the model sequences we need a specific learning framework able to deal with variable lens sequences maintain sequence order and to keep track of longterm dependencies rather than cutting input data too short and finally to share parameters across the sequence so as to not reload things and that's where recurrent neural networks come in rnns are a type of new network architecture that use something called a feedback loop in the hidden layer unlike feed forward new networks the recurrent neural network or rnn can operate effectively on sequences of data with variable input length this is how an rnn is usually represented this little loop here is called the feedback loop sometimes you may find the rnns depicted over time like this the first path represents the network in the first time step the hidden node h1 uses the input x1 to produce output y1 this is exactly what we've seen with basic feed forward new networks however at the second time step the hidden node at the current time step h2 uses both the new input x2 as well as the state from the previous time step h1 as input to make new predictions this means that a current neural network uses knowledge of its previous states as input for its current prediction and we can repeat this process for an arbitrary number of steps allowing for the network to propagate information by its hidden state throughout time this is almost like giving a neural network a shortterm memory they have this abstract concept of sequential memory and because of this we're able to model certain areas of data sequential data that standalone neural networks aren't able to model recurrent neural networks remember their past and their decisions are influenced by what it has learned from the past basic feedforward networks remember things too but they remember things they learned during training for example an image classifier learns what a three looks like during training and then uses that knowledge to classify things in production so how do we train an rnn well it is almost the same as training a basic fully connected feed forward network except that the back propagation algorithm is applied for every sequence data point rather than the entire sequence this algorithm is sometimes called the back propagation through time algorithm or the btt algorithm to really understand how this works imagine we're creating a recurring new network to predict the next letter a person is likely to type based on the previous letters they've already typed the letter that a user just typed is quite important to predicting the new letter however all the previous letters are also very important to this prediction as well at the first time step say the user typed the letter f so a network might predict that the next letter is in e based on all of the previous training examples that included the word fe at the next time step the user types the letter r so our network uses both the new letter r plus the state of the first hidden neuron in order to compute the next prediction l the network predicts this because of the high frequency of occurrences in the word fel in our training data set adding the letter a might predict the letter t adding an n would predict the letter k which would match the word i use in tender to type which is frank there however is an issue with rnn's known as shortterm memory shortterm memory is caused by the infamous vanishing and exploding gradient problems as the rnn processes more words it has trouble retaining information from previous steps kind of like our memory if you're given a long sequence of numbers like pi and you tried reading them out you're probably going to forget the initial few digits right shortterm memory and the vanishing gradient is due to the nature of back propagation the algorithm used to train and optimize neural networks after the forward propagation or the pass the network compares this prediction to the ground truth using a loss function which outputs an error value an estimate of how poorly the network is performing the network uses that error value to perform back propagation which calculates the gradients for each node in the network the gradient is the value used to adjust the network's internal weights allowing for the network to learn the bigger the gradient the bigger the adjustments are and vice versa here's where the problem lies when performing back propagation each node in a layer calculates its gradient with respect to the effects of the gradient in the layer before it so if the adjustment to the layers before it is small then the adjustments to the current layer will be even smaller and this causes gradients to exponentially shrink as it back propagates down the earlier layers fail to do any learning as the internal weights are barely being adjusted due to extremely small gradients and that is the vanishing gradient problem let's see how this applies to recover new networks you can think of each time step in a recurrent neural network as a layer to train recon neural network you use an application of back propagation called back propagation through time the gradient values will exponentially shrink as the back propagates through each time step again the gradient is used to make adjustments in the new network weights thus allowing it to learn small gradients means small adjustments and this causes the early layers not alone because of the vanishing gradient the rnn doesn't learn the long range dependencies across time steps this means that in a sequence it was raining on tuesday there is a possibility that the words it and was are not considered when trying to predict the user's intention the network then has to make the best guess with on tuesday and that's pretty ambiguous and would be difficult even for a human so not being able to learn on earlier time steps causes the network to have a shortterm memory we can combat the shortterm memory of an rnn by using two variants of recurrent neural networks gated rnns and long shortterm memory rnns also known as lcms both these variants function just like rns but they're capable of learning longterm dependencies using mechanisms called gates these gates are different tensor operations that learn information that can learn what information to add or remove to the hidden state or the feedback loop the main difference between a gated rnn and an lscm is that the gated rnn has two gates to control its memory an update gate and reset gate while an lsem has three gates an input gate an output gate and a forget gate rnns work well for applications that involve sequences of data that change over time these applications include natural language processing sentiment classification dna sequence classification speech recognition and language translation a convolutional neural network or cnn for short is a type of deep neural network architecture designed for specific tasks like image classification cnns were inspired by the organization of neurons in the visual cortex of the animal brain as a result they provide some very interesting features that are useful for processing certain types of data like images audio and video like a fully connected neural network a cnn is composed of an input layer an output layer and several hidden layers between the two cnns derive their names from the type of hidden layers it consists of the hidden layers of a cnn typically consist of convolutional layers pooling layers fully connected layers and normalization layers this means that instead of traditional activation functions we use in feedforward neural networks convolution and pooling functions are used instead more often than not the input of a cnn is typically a twodimensional array of neurons which correspond to the pixels of an image for example if you're doing image classification the output layer is typically onedimensional convolution is a technique that allows us to extract visual features from a 2d array in small chunks each neuron in a convolution layer is responsible for a small cluster of neurons in the preceding layer the bounding box that determines the cluster of neurons is called a filter also called a kernel conceptually you can think of it as a filter moving across an image and performing a mathematical operation on individual regions of the image it then sends its result to the corresponding neuron in the convolution layer mathematically a convolution of two functions f and g is defined as follows which is in fact the dot product of the input function and the kernel function pooling also known as sub sampling or down sampling is the next step in a convolutional neural network its objective is to further reduce the number of neurons necessary in subsequent layers of the network while still retaining the most important information there are two different types of pooling that can be performed max pulling and min pooling as the name suggests max pooling is based on picking up the maximum value from the selected region and min pooling is based on picking up the minimum value from that region when we put all these techniques together we get an architecture for a deep neural network quite different from a fully connected neural network for image classification where cnns are used heavily we first take an input image which is a twodimensional matrix of pixels typically with three color channels red green and blue next we use a convolution layer with multiple filters to create a twodimensional feature matrix as the output for each filter we then pool the results to produce a downsample feature matrix for each filter in the convolution layer next we typically repeat the convolution and pooling steps multiple times using previous features as input then we add a few fully connected hidden layers to help classify the image and finally we produce a classification prediction in the output layer convolutional neural networks are used heavily in the field of computer vision and work well for a variety of tasks including image recognition image processing image segmentation video analysis and natural language processing in this section i'm going to discuss the 5 steps that are common in every deep learning project that you build these can be extended to include various other aspects but at its very core they are very fundamentally five steps data is at the core of what deep learning is all about your model will only be as powerful as the data you bring which brings me to the first step gathering your data the choice of data and how much data you would require entirely depends on the problem you're trying to solve picking the right data is key and i can't stress how important this part is bad data implies a bad model a good rule of thumb is to make assumptions about the data you require and be careful to record these assumptions so that you can test them later if needed data comes in a variety of sizes for example iris flaw data set contains about 150 images in the total set gmail smart reply has around 238 million examples in its running sets and google translate reportedly has trillions of data points when you're choosing a data set there's no onesizefitsall but the general rule of thumb is that the amount of data you need for a wellperforming model should be 10 times the number of parameters in that model however this may differ from time to time depending on the type of model you're building for example in regression analysis you should use around 10 examples per predictor variable for image classification the minimum you should have is around a thousand images per class that you're trying to classify while quantity of data matters quality matters too there's no use having a lot of data if it's bad data there are certain aspects of quality that tend to correspond to wellperforming models one aspect is reliability reliability refers to the degree in which you can trust your data a model train on a reliable data set is more likely to yield useful predictions than model trained on unreliable data how common are label errors if your data is labeled by humans sometimes there may be mistakes are your features noisy is it completely accurate some noise is all right you'll never be able to purge your data of all the noise there are many other factors that determine quality for the purpose of this video though i'm not going to talk about the remaining although if you're interested i'll leave them in the show notes below lucky for us there are plenty of resources on the web that offer good data sets for free here are a few sites where you can begin your dataset search the uci machine learning repository maintains around 500 extremely well maintained data sets that you can use in your deep learning projects kaggle's another one you'll love how detailed that data sets are they give you info on the features data types number of records and so on you can use a kernel too and you won't have to download the data set google's dataset search is still in beta but is one of the most amazing sites that you can find today reddit2 is a great place to request for data sets you want but again there is a chance of not being properly organized create your own data set that will work too you can use web scrapers like beautiful soup to get your required data for the data set after you have selected your data set you now need to think of how you're going to use this data there are some common preprocessing steps that you should follow first splitting the data set into subsets in general we usually split a data set into three parts training testing and validating sets we train our modules with the training set evaluated on the validation set and finally once it's ready to use test it one last time on the testing data set now it is reasonable to ask the following question why not have two sets training and testing in that way the process will be much simpler just train the model on the training data and test it on the testing data the answer to that is developing a model involves tuning its configuration in other words choosing certain values for the hyper parameters or the weight and biases this tuning is done with the feedback received from the validation set and is in essence a form of learning it turns out we just can't split the data set randomly do that and you'll get random results there has to be some kind of logic to split the data set essentially what you want is for all three sets the training testing and validation sets to be very similar to each other and to eliminate skewing as much as possible this mainly depends on two things first the total number of samples in your data and second or the actual model you're trying to train models with very few hyper parameters will be very easy to validate in tune so you can probably reduce the size of your validation set but if your model has many hyper parameters you would want to have a large validation set as well as consider cross validation also if you happen to have a model with no hyper parameters whatsoever or ones that cannot be easily tuned you probably don't need a validation set all in all like many other things in machine learning and deep learning the train test validation split ratio is also quite specific to your use case and it gets easier to make judgment as you train and build more and more models so here's a quick note on cross validation usually you'd want to split your data set into two the train and the test after this you keep aside the test set and randomly choose some percentage of the training set to be the actual train set and the remaining to be the validation set the model is then iteratively trained and validated on these different sets there are multiple ways to do this and this is commonly known as cross validation basically you use your training set to generate multiple splits of the train and validation set cross validation avoids overfitting and is getting more and more popular with k fold cross validation being the most popular method additionally if you're working on time series data a frequent technique is to split the data by time for example if you have a dataset with 40 days of data you can train your data from days 1 to 39 and evaluate your model on the data from day 40. for systems like this the training data is older than the serving data so this technique ensures your validation set mirrors the lag between training and serving however keep in mind that timebased splits work best with very very large data sets such as those with tens of millions of examples the second method that we have in preprocessing is formatting the data set that you've picked might not be in the right format that you like for example the data might be in the form of a database but you'd like it as a csv file vice versa of course there are a couple of ways to do this and you can google them if you'd like dealing with missing data is one of the most challenging steps in the gathering of data for your deep learning projects unless you're extremely lucky to land with the perfect data set which is quite rare dealing with missing data will probably take a significant chunk of your time it is quite common in real world problems to miss some values of our data samples this may be due to errors on the data collection blank spaces on surveys measurements not applicable and so on missing values are typically represented with the nan or the null indicators the problem with this is that most algorithms can't handle these kind of missing values so we need to take care of them before feeding data to our models there are a couple of ways to deal with them one is eliminating the samples of the features with missing values the downside of code is that you risk to delete relevant information the second step is to impute the missing values a common way is to set the missing values as the mean value for the rest of the samples but of course there are other ways to deal with specific data sets be smart as handling missing data in the wrong way can spell disasters sometimes you may have too much data that what you require more data can result in larger computational and memory requirements in cases like this it's best practice to use a small sample of the data set it will be faster and ultimately an increase in time for you to explore and prototype solutions in most real world data sets you're going to come across imbalanced data that is classification data that has skewed class proportions leading to the rise of a minority class and a majority class if we train a model on data like this a model will only spend time learning about the majority class and a lot less time on the minority class and hence a model will ultimately be biased to the majority class and so in cases like this we usually use a process called down sampling and up weighting which is essentially reducing majority cost by some factor and adding example weights of that factor to the down sample class for example if we down sample the majority cost by a factor of 10 then the example weighted we add to that class should be 10. it may seem odd to add example weights after down sampling what is its purpose well there are a couple of reasons at least a faster convergence during training we see the minority class more often which helps the model converge faster by consolidating the majority class into fewer examples with larger weights we spend less disk space storing them operating ensures their model is still calibrated we add operating after down sampling so as to keep the data set in similar proportion these processes essentially help a model see more of the minority costs rather than just solely the majority class this helps our model perform better in real world situations feature scaling is a crucial step in the preprocessing phase as the majority of deep learning algorithms perform much better when dealing with features that are on the same scale the most common techniques are normalization which refers to the rescaling of features to a range between 0 and 1 which in fact is a special case of min max scaling to normalize that data we need to apply min max scaling to each feature column standardization consists of centering the field at mean 0 with standard deviation 1 so that the feature columns have the same parameters as a standard normal distribution that is 0 mean and unit variance this makes it much easier for the learning algorithms to learn the weights of the parameters in addition it keeps useful information about outliers and makes the algorithms less sensitive to them once our data has been prepared we now feed this into our network to trade we've discussed the learning process of a neural network in the previous module so if you are unsure i'd advise you to watch that module first but essentially once a data has been fed forward propagation occurs and the losses compared against the loss function and the parameters are adjusted based on this loss incurred again nothing too different from what we discussed previously your model has successfully trained congratulations now we need to test how good our model is using the validation set that we had set aside earlier the evaluation process allows us to test a model against data it has never seen before and this is meant to be representative of how good the model might perform in the real world after the evaluation process there's a high chance that your model could be optimized further remember we started with random weights and biases and these were finetuned during back propagation well in quite a few cases bad propagation won't get it right the first time and that's okay there are a few ways to optimize your model further tuning hyper parameters is a good way of optimizing your model's performance one way to do this is by showing the model the entire data set multiple times that is by increasing the number of epochs this has sometimes shown to improve accuracy in other ways by adjusting the learning rate we talked about what the learning rate was in the previous module so if you don't know what the learning rate is i do advise you to check out the previous module but essentially the learning rate defines how far we shift the line during each step based on information from the previous training step in back propagation these values all play a role in how accurate a model can become and how long the training takes for complex models initial conditions can play a significant role in determining the outcome of training there are many considerations at this phase of training and it's important you define what makes a model good enough otherwise you might find yourself tweaking parameters for a long long time the adjustment of these hyper parameters remains a bit of an art and is more of an experimental process that heavily depends on the specifics of your data set model and training process you will develop this as you go more and more into deep learning so don't worry too much about this now one of the more common problems you will encounter is when your model performs well on training data but performs terribly on data it's never seen before this is the problem of overfitting this happens when the model learns a pattern specific to the training data set that aren't relevant to other unseen data there are two ways to avoid this overfitting getting more data and regularization getting more data is usually the best solution a model trainer mode data will naturally generalize better reducing the model size by reducing the number of learnable parameters in the model and with it its learning capacity is another way however by lowering the capacity of the network you force it to learn patterns that matter or then minimize the loss on the other hand reducing the network's capacity too much will lead to underfitting the model will not be able to learn the relevant patterns in the trained data unfortunately there are no magical formulas to determine this balance it must be tested and evaluated by setting different number of parameters and observing its performance the second method to addressing overfitting is by applying weight regularization to the model a common way to achieve this is to constraint the complexity of the network by forcing its weights to take only small values regularizing the distribution of weight values this is done by adding to the loss function of the network a cost associated with having larger weights and this cost comes in two ways l1 regularization at the cost with regards to the absolute value of the weight coefficient or the l1 norm of the weights l2 regularization adds a cost with regards to the squared value of the weight's coefficient that is the l2 norm of the weight another way of reducing overfitting is by augmenting data for a model to perform well or satisfactory we need to have a lot of data we've just have just already but typically if you're working with images there's always a chance that your model won't perform as well as you'd like it no matter how much data you have in cases like this when you have limited data sets data augmentation is a good way of increasing your data set without really increasing it we artificially augment our data or in this case images so that we get more data from already existing data so what kind of augmentations are we talking about well anything from flipping the image of the yaxis flipping over the xaxis applying blur to even zooming on in the image what this does is that it shows your model more than what meets the eye it exposes your model to more of the existing data so that in testing it will automatically perform better because it has seen images represented in almost every single form finally the last method we're going to talk about is dropout dropout is a technique used in deep learning that randomly drops out units or neurons in the network simply put dropout refers to the ignoring of neurons during the training phase of a randomly chosen set of neurons by ignoring i mean that these units are not considered during a particular forward or backward pass so why do we need dropout at all why do we need to shut down parts of a neural network a fully connected layer occupies most of the parameters and hence neurons develop a codependency amongst each other during training which curbs the individual power of each neuron and which ultimately leads to overfitting of the training data so drop out a good way of reducing overfitting i hope that this introductory course has helped you develop a good intuition of deep learning as a whole of course we've only just scraped the surface there's a whole new world out there if you like this course please consider liking and subscribing it really helps me make courses like this i have a couple of videos on computer vision with opencv that i will be releasing in a couple of weeks so stay tuned for that in the meantime good luck
