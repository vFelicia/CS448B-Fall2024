With timestamps:

00:00 - apache cassandra is an open source nosql
00:03 - distributed database
00:05 - this beginner's course has four modules
00:08 - the first three modules cover the basics
00:11 - of apache cassandra data modeling the
00:14 - final module covers practical
00:16 - applications of cassandra
00:18 - if you want a broad overview of nosql
00:21 - databases you can check out the previous
00:23 - nosql database course on our channel
00:26 - this module will cover the basics of
00:28 - apache cassandra data modeling upon
00:31 - completion of this module you will be
00:32 - able to differentiate between the
00:34 - relational databases and an apache
00:36 - standard database
00:38 - recognize the basic terminologies used
00:40 - in cassandra tables
00:42 - explore cql fundamentals
00:44 - identify how partitions primary keys and
00:47 - clustering columns are used in cassandra
00:49 - tables
00:50 - observe the concept of replication and
00:52 - consistency
00:54 - upon completing this lesson you will be
00:56 - able to understand how relational data
00:58 - modeling concepts compare to those used
01:00 - when data modeling for apache cassandra
01:03 - now you will learn about some
01:05 - differences between relational databases
01:07 - and apache cassandra
01:08 - the cassandra data model follows
01:10 - cassandra modeling methodology while the
01:12 - relational database follows the sample
01:14 - relational model methodology
01:16 - in cassandra modeling we think about the
01:18 - application needs when building the data
01:20 - model
01:21 - since queries are the driving force for
01:23 - this model those queries must be
01:25 - conceptualized before data entry whereas
01:27 - in a relational model entities are the
01:30 - driving factor
01:31 - in relational modeling primary keys are
01:34 - used to ensure uniqueness of the
01:35 - entities
01:36 - in cassandra modeling primary keys take
01:39 - on an additional significance
01:41 - particularly in determining performance
01:43 - on a large scale
01:45 - in order for us to provide high
01:46 - performance and availability on a large
01:48 - scale cassandra uses a distributed
01:51 - architecture
01:52 - cassandra by default does not support
01:54 - asset transactions or joins but uses
01:57 - denormalization instead
01:59 - cassandra does not enforce referential
02:01 - integrity across tables
02:03 - here's a sample of a relational data
02:05 - model
02:06 - this is one of many types of relational
02:08 - data models
02:09 - this model produces conceptual logical
02:12 - and physical data models including steps
02:14 - for normalization and optimizing the
02:16 - data model
02:17 - note that queries aren't considered
02:19 - until the very late stages of the
02:20 - process
02:22 - let's now look at the apache cassandra
02:24 - data modeling approach
02:26 - here we still have conceptual logical
02:28 - and physical data models but the
02:29 - application workflow and inquiries are
02:32 - considered earlier in the process
02:34 - this analysis is incorporated early into
02:36 - the logical data modeling phase rather
02:38 - than waiting until the end of the
02:40 - process
02:42 - let's look at the difference between the
02:43 - modeling approaches
02:45 - in the relational approach we first
02:47 - think about the type of data create a
02:49 - data model based on the data and then
02:51 - think about our application last
02:53 - the developer can then make queries to
02:55 - retrieve this information
02:58 - in the approach followed by cassandra
03:00 - the process is reversed
03:02 - here the application of the data must be
03:04 - decided up front in other words what
03:06 - queries are going to be performed
03:09 - that will dictate how your logical model
03:11 - will take shape
03:12 - once your queries and data model are
03:14 - complete you can load the data into your
03:16 - tables
03:17 - relational databases are asset compliant
03:20 - by default
03:21 - acid is used to describe the
03:22 - transactional behavior in a relational
03:24 - database and stands for atomicity
03:26 - consistency isolation and durability
03:30 - atomicity refers to the integrity of the
03:32 - database transaction either all the
03:34 - statements in a transaction will succeed
03:36 - or none of them will
03:38 - consistency means that the data within a
03:40 - database should comply with certain
03:42 - rules
03:43 - transactions cannot leave the database
03:45 - in an inconsistent state
03:46 - the new database state must satisfy
03:49 - integrity constraints
03:51 - isolation refers to the ability to
03:53 - process multiple transactions at the
03:54 - same time without interference
03:57 - durability refers to the ability of a
03:59 - completed transaction persisting even
04:01 - after a subsequent failure
04:04 - apache cassandra does not support asset
04:06 - properties like most relational
04:08 - databases
04:09 - in database management both commit and
04:11 - rollback can easily be achieved using
04:13 - transaction logging when a single server
04:15 - is involved
04:16 - but when the data is spread across
04:18 - servers in distributed databases like
04:19 - cassandra the situation becomes tricky
04:22 - the locking and coordination required to
04:24 - ensure these two-phase semantics of acid
04:27 - can cause severe performance penalties
04:30 - however cassandra does provide atomicity
04:32 - isolation and durability when performing
04:34 - read and write commands on a single row
04:37 - additionally it also provides something
04:39 - called tunable consistency which can be
04:41 - adjusted depending upon the number of
04:42 - nodes which have to agree to execute a
04:44 - particular read or write command
04:47 - to understand tunable consistency let's
04:50 - look at the cap theorem first
04:52 - given by eric brewer this theorem talks
04:54 - about the trade-offs in implementing a
04:56 - distributed database
04:58 - all distributed databases are primarily
05:00 - tolerant to partitions hence the choice
05:02 - is really between consistency and
05:04 - availability
05:05 - cassandra by default is an ap system
05:08 - which means it chooses availability and
05:10 - partition tolerance while sacrificing
05:12 - consistency
05:13 - however it does provide the option to
05:15 - make the system more consistent by
05:17 - sacrificing availability this is
05:19 - referred to as tunable consistency
05:21 - wherein the consistency of the database
05:23 - can be adjusted or tuned by adjusting
05:25 - the number of nodes which have to return
05:27 - a read or write request
05:29 - cassandra primarily operates under the
05:31 - ap model because all data eventually
05:34 - becomes consistent
05:35 - one more point of difference between
05:36 - cassandra and relational databases is
05:38 - the support of joins
05:40 - to recall joins are a statement often
05:42 - used by relational databases to combine
05:44 - data
05:45 - a join statement facilitates the query
05:47 - through the joining of data from
05:49 - different tables
05:50 - take a look at the example below the
05:52 - join statement combines results from two
05:54 - tables namely videos which stores the
05:57 - details of videos and comments which
05:59 - stores the comments made by different
06:00 - users on the videos this is done by the
06:03 - matching of video id as specified in
06:06 - both the tables the database matches the
06:08 - video id for each video and generates a
06:10 - result
06:12 - this is the output of the join command
06:14 - the select statement in the given
06:16 - transaction selects the column specified
06:18 - from the output of the join command and
06:20 - the where statement is the filtering
06:22 - criteria
06:23 - therefore the output of this transaction
06:25 - only provides the comments of the video
06:27 - titled interstellar
06:29 - the basic requirement for a join command
06:31 - to work is that the results from two
06:33 - combined tables should be stored at one
06:35 - location
06:36 - however in distributed databases the
06:38 - different partitions of data are stored
06:40 - on different nodes hence joins would
06:43 - have a negative impact on the latency as
06:45 - the scale of the databases increases and
06:47 - the random distributions become more
06:49 - prevalent
06:51 - unlike relational databases cassandra
06:53 - does not support joints but uses
06:55 - denormalization instead
06:57 - denormalization ensures that all the
06:59 - required information is confined to the
07:01 - appropriate table without having to
07:03 - perform joins
07:04 - organizing data like this can duplicate
07:06 - data in multiple tables which is cost
07:08 - efficient and allows for faster
07:10 - retrieval of said data while using
07:12 - cassandra to organize the data the
07:14 - developer can decide upfront what tables
07:16 - they want their data to be in
07:19 - in this example we have two scenarios in
07:21 - which we want to access comments
07:23 - first a video view where all the
07:26 - comments for a particular video can be
07:27 - viewed and a second a user view that
07:30 - displays comments made by a particular
07:32 - user
07:33 - to support this two tables must be
07:35 - created
07:36 - the first table comments by video
07:38 - supports the ability to access comments
07:40 - for a specific video
07:42 - here video title is used as a partition
07:44 - key
07:45 - the comment id helps in defining a
07:47 - unique row in the database for each
07:49 - comment
07:50 - the second table comments by user
07:52 - supports the same data but has a
07:54 - different partition key
07:55 - here the user id is used as the
07:57 - partition key
07:59 - note that these two tables reflect the
08:01 - common naming convention we have for
08:02 - cassandra tables we append by something
08:05 - to the table name to convey some idea of
08:07 - the query that the table is designed to
08:10 - support
08:11 - joins rely on referential integrity
08:13 - constraints to combine data
08:16 - let's look at this point of difference
08:18 - between relational data modeling and
08:19 - cassandra data modeling
08:21 - ri refers to the relationship between
08:23 - tables
08:24 - a value in one table requires the same
08:27 - value to exist in another table
08:29 - if there is a user in the user by email
08:32 - table then the user must also exist in
08:34 - the users table and vice versa this
08:37 - referential integrity is required
08:39 - between tables such that a user is
08:41 - either represented in both tables or
08:43 - none of them and also requires for a
08:44 - join to work properly
08:46 - cassandra does not enforce referential
08:48 - integrity because this capability
08:50 - requires a read before a write
08:53 - it is not considered an issue that has
08:54 - to be fixed on the cassandra side
08:57 - although it is a feature that improves
08:59 - performance and accessibility it is
09:00 - considered a developer prerogative
09:03 - referential integrity can be enforced in
09:05 - an application design but that means
09:07 - more work for the developers
09:10 - upon completing this topic you will be
09:13 - able to
09:15 - differentiate between the terminologies
09:17 - used in apache cassandra tables
09:21 - understand the different data types
09:23 - available in cassandra
09:25 - cassandra uses database terms that have
09:29 - specific connotations in the context of
09:31 - data modeling
09:33 - the key terms are data model key space
09:36 - table and partition
09:38 - a data model is an abstract model for
09:41 - organizing elements of the data
09:44 - data models vary depending on the type
09:46 - capability and purpose of the database
09:50 - the type of the data model depends on
09:52 - the capability of the database
09:55 - for cassandra the data model is based on
09:58 - the queries that you may want to perform
10:01 - a key space is the outermost logical
10:03 - container of tables
10:06 - it stores tables and replication data
10:09 - in other words it's a container for
10:11 - replication
10:13 - a table is a combination of rows and
10:15 - columns
10:16 - the tables are contained within the k
10:19 - space
10:20 - as a columnar database cassandra stores
10:24 - data based on partitions
10:26 - partition is a raw rows of data that are
10:29 - stored on a node in the data table based
10:32 - on the partitioning strategy
10:35 - each row in the partition consists of
10:38 - key and value pairs
10:40 - cassandra stores and retrieves data
10:43 - based on partitions
10:45 - a partition is a terminology where you
10:48 - can make or break the data model
10:52 - a primary key is the most important part
10:54 - of the data model
10:56 - it does two things
10:58 - first it guarantees the uniqueness of
11:00 - the data and second it defines the
11:03 - placement of the record in the cluster
11:05 - this allows for easy access to the data
11:08 - in the model
11:10 - a partition key is the first part of the
11:12 - primary key it determines where in the
11:15 - cluster your data will be stored on
11:18 - which node it will be stored
11:20 - a partition key is a left hand column in
11:23 - the graphic indicating one two or three
11:27 - respectively
11:28 - inside each partition there are rows and
11:31 - columns
11:32 - columns are stored locally and referred
11:34 - to as cells in the table
11:37 - a row is contained inside a partition
11:40 - there can be more rows and they are
11:42 - stored together on a partition
11:45 - clustering columns define the order of
11:48 - data within a given partition
11:50 - the default sorting is in the ascending
11:54 - order
11:57 - upon completing this lesson you will be
11:58 - able to explore the basic cql concepts
12:01 - within an apache cassandra database
12:04 - cassandra query language or cql is a
12:07 - nosql language also known as not only
12:10 - sql
12:11 - let's learn about some basic cql
12:13 - concepts including
12:14 - building blocks such as key spaces
12:16 - tables and the primary keys as well as
12:19 - basic cql commands such as select
12:22 - truncate alter table and source
12:25 - the key space is a top level container
12:27 - in apache cassandra to organize a
12:29 - related set of tables
12:31 - it is very similar to a relational
12:33 - databases schema
12:34 - cassandra key space defines replication
12:37 - settings that describe how many copies
12:39 - of a given piece of data are being
12:41 - stored within a cluster
12:43 - once you've created a key space in cqlsh
12:46 - you may want to select the key space
12:47 - that will be used for your subsequent
12:49 - command
12:50 - you can enter a use command followed by
12:52 - the name of the key space
12:54 - if you do not select a key space with
12:56 - the use command you'll have to include
12:57 - the name of the key space whenever you
12:59 - reference the name of a table
13:01 - this helps cassandra interpret your
13:03 - request
13:05 - let's take a quick look at the syntax
13:07 - for creating a table
13:09 - the code here shows the statements for
13:10 - creating two different tables
13:12 - remember the key spaces contain tables
13:15 - and tables contain our data
13:17 - note that every cassandra table has a
13:19 - primary key
13:21 - inside each key space is the table the
13:23 - primary key clause is a unique
13:25 - identification for each row within the
13:27 - table
13:28 - cassandra's primary keys uniquely
13:30 - identify rows of data
13:32 - in this example we see a uuid used as a
13:35 - partition key to uniquely identify each
13:38 - user in our users table
13:40 - however a partition key alone is not
13:42 - enough to ensure uniqueness of a row
13:45 - it is the primary key clause as a whole
13:47 - that determines uniqueness of a row
13:49 - within a partition
13:52 - the cql select command is used to read
13:54 - data from cassandra tables
13:56 - selecting all the rows of the table
13:58 - using an asterisk notation causes
14:00 - cassandra to perform a full table scan
14:03 - this could be very expensive on a table
14:04 - with a large number of rows
14:07 - instead use the limit keyword to get a
14:09 - subset of the rows
14:11 - cassandra also supports a paging
14:13 - mechanism which indicates specific
14:15 - columns we desire to read by name or
14:18 - uses the asterisks to indicate all
14:20 - columns
14:21 - cassandra also supports a couple of
14:23 - built-in aggregation functions such as
14:25 - the count function
14:26 - for counts on larger tables you may want
14:28 - to consider using the spark part of disk
14:31 - analytics
14:32 - another command you'll find useful is
14:34 - the truncate command
14:36 - this command causes all rows to be
14:37 - deleted from a table while leaving the
14:39 - schema in place
14:41 - this command should obviously be used
14:43 - with care as the data is removed from
14:44 - persistent storage and you would have to
14:46 - restore it from a backup to retrieve it
14:49 - truncate sends a jmx command to all
14:51 - nodes to delete ss tables that hold the
14:53 - data
14:54 - if any of these nodes are down the
14:56 - command will fail while working with
14:58 - cassandra you'll most likely encounter
15:00 - situations when you'll need to make
15:02 - changes to your table
15:04 - this is where the altered table command
15:06 - comes into play
15:07 - you can change the data type of a column
15:09 - add columns drop columns rename columns
15:13 - and change table properties but you
15:15 - cannot change the columns in a primary
15:17 - key clause
15:18 - here's an example code for adding and
15:20 - dropping columns from a table
15:22 - the last command is the source command
15:24 - which allows you to execute a set of cql
15:26 - statements from a file
15:28 - to execute a source command you'll want
15:30 - to put the file name in single quotes
15:33 - cqlsh will output the results of each
15:35 - command sequentially as it executes
15:39 - upon completing this topic you will be
15:42 - able to
15:44 - learn about the basic concepts of
15:46 - partitions table partitions partition
15:50 - ordering partition distribution and
15:53 - partition storage
15:56 - to truly understand the data modeling
15:58 - you must master partitioning concepts
16:02 - partitions give you an indication of
16:05 - where your data is in your data model as
16:08 - well as in the cluster
16:10 - we will go through some of these
16:12 - concepts in depth
16:14 - partitions partition keys composite
16:17 - partition keys and clustering columns
16:22 - here are two commands that can be
16:24 - executed on a relational database as
16:28 - well as on apache cassandra
16:30 - in a relational world these queries
16:32 - should work on a video table because of
16:35 - joins
16:37 - here common queries are title equals and
16:41 - added date is less than
16:44 - if you run this in a cassandra database
16:47 - the output appears as an invalid request
16:50 - this is because the partition keys
16:52 - absent from the equality clause of the
16:55 - search expression these commands fail
16:58 - because joins are not supported
17:01 - you've already seen the structure of the
17:03 - storage engine in lesson two
17:07 - this diagram demonstrates how the data
17:10 - is translated from the storage engine
17:13 - into a cql table
17:16 - when used programmatically that is when
17:19 - we select data from the table we see it
17:22 - in rows and columns just like we use in
17:25 - our application
17:27 - determination of the partition key is a
17:30 - critical step
17:32 - if you look at this diagram closely we
17:35 - have the primary key as the id
17:39 - there is just one column in the primary
17:41 - key
17:42 - the partition key is always the first
17:45 - value in the primary key
17:50 - this module will cover the basics of a
17:53 - cassandra data model
17:55 - upon completion of this module you will
17:58 - be able to explore the concept of
18:02 - denormalization
18:04 - and investigate collection counters and
18:07 - user defined data types
18:12 - upon completing this topic you will be
18:14 - able to
18:16 - explain how joints are used in
18:18 - relational data modeling
18:21 - relay the concept of denormalization
18:24 - used by cassandra
18:28 - you have already learned that the
18:30 - relational data model supports joins and
18:33 - apache cassandra supports denormalized
18:36 - tables
18:38 - here you see three tables named videos
18:41 - users and comments
18:43 - each table has unique information in it
18:47 - this is a typical relational example
18:51 - we're going to make all these work
18:53 - together by running a join query to see
18:56 - how they can be used in relation to each
18:59 - other
19:01 - join query joins two tables to work as
19:05 - one
19:06 - let's assume that you want to find some
19:09 - comments for a particular movie
19:12 - the first table titled videos store
19:14 - details of videos and the second table
19:17 - comment stores comments made by users on
19:21 - all the videos
19:23 - we need to find a particular entry which
19:26 - has all the data collected from both the
19:28 - tables
19:30 - let's say we are searching for a comment
19:32 - from the videos table
19:34 - how do we access the comment from the
19:37 - comment table if you just have the video
19:40 - with us
19:42 - this is possible using the join query
19:46 - they are often used by relational
19:48 - databases to organize data
19:51 - the two tables shown previously are
19:54 - joined and this is how the output looks
19:57 - like
19:59 - the tables are merged with the help of
20:01 - the video id specified in both the
20:04 - tables
20:05 - that is how relational databases work
20:10 - in the code the title is interstellar
20:14 - the select statement in the given
20:17 - transaction selects the columns
20:20 - specified from the output of the join
20:22 - command and the bear statement specifies
20:25 - which values to select from the
20:28 - specified column
20:31 - now this can be used to do a join on two
20:34 - tables and get comments for that one
20:38 - title
20:39 - this is how
20:40 - rdbms or relational databases are
20:44 - designed
20:45 - in the output of this transaction we
20:48 - will get the comments of the video
20:50 - titled interstellar
20:53 - that is how the relational databases
20:55 - were designed to be used having multiple
20:59 - tables and then using queries to merge
21:02 - and bring outputs
21:05 - you've just learned how the relational
21:08 - data model works with joins
21:11 - now it's time to see how cassandra works
21:13 - without joins
21:16 - let's look at these two tables closely
21:19 - they look similar but the primary key is
21:22 - different for both
21:24 - in the first table comments by video the
21:27 - video title is the partition key and the
21:29 - comment id is the clustering column
21:33 - and in the second table comments by user
21:36 - the user id is the partition key and the
21:39 - comment id is used as a clustering
21:42 - column
21:43 - these are the denormalized tables used
21:46 - in cassandra of the same joins that were
21:50 - used in the relational data models
21:53 - upon completing this topic
21:56 - you will be able to learn how collection
21:58 - columns and collection types
22:01 - are considered
22:03 - understand how to create and use udts
22:07 - learn about counters
22:10 - you will now learn about some additional
22:13 - data types supported by apache cassandra
22:16 - for efficiency
22:18 - these data types are collections
22:21 - counters and user-defined data types or
22:25 - udts
22:28 - these data types simplify table design
22:32 - optimize table functionality
22:34 - store data more efficiently and might
22:37 - even change the design of the table
22:40 - completely
22:43 - now you're going to learn about
22:45 - collections
22:47 - cassandra provides collection types to
22:50 - group and store data together in a
22:53 - single column
22:55 - take for example a relational database
22:58 - or a cassandra database to understand
23:01 - this concept
23:02 - consider that in a relational database
23:05 - you have to group multiple email
23:07 - addresses of a user meaning there will
23:10 - be a many to one joint relationship
23:13 - between a user table and an email table
23:17 - but cassandra awards joins between two
23:20 - tables by storing the user's email
23:23 - addresses in a collection column in the
23:26 - user table
23:27 - each collection specifies the data type
23:30 - of the data held
23:33 - the collection data structure allows you
23:36 - to store multiple values in each cell
23:39 - while retaining the value
23:42 - all collections are multi-valued columns
23:46 - the maximum number of elements in a
23:48 - collection is 64 000 which in practice
23:52 - can be dozens or even hundreds
23:55 - collections are designed to store a
23:58 - small amount of data
24:00 - the maximum size of an element is 64 kb
24:05 - which in practice is much smaller than
24:07 - the number of elements in collection
24:10 - they are also retrieved in their
24:13 - entirety
24:14 - a collection column cannot be part of a
24:17 - primary key a partition key or
24:20 - clustering columns
24:23 - you cannot nest a collection inside of
24:26 - another collection
24:28 - if you want to nest a collection inside
24:31 - another
24:32 - then you have to use the keyword frozen
24:36 - more on this later
24:38 - there are three collection types in
24:41 - total
24:42 - set
24:43 - list
24:44 - and map respectively
24:46 - let's begin with the first collection
24:49 - type set
24:51 - it is a list that stores the typed
24:53 - collection of unique values
24:56 - it is stored on order but it gives back
24:59 - the data in a sorted order
25:03 - let's look at an example of set
25:06 - here a user's table is created with an
25:09 - id first name last name and a set of
25:12 - email addresses
25:14 - the data type is represented in angle
25:17 - brackets so in this example text data
25:21 - type is used
25:23 - after that the values are inserted into
25:26 - the table
25:27 - here there are two email addresses
25:30 - because it is common for a single user
25:33 - to have two email addresses
25:37 - list is like a set it groups and stores
25:40 - values in a single cell
25:43 - the difference between list and set is
25:46 - that in a list the values does not have
25:49 - to be unique and there can be duplicates
25:53 - lists are stored in a particular order
25:56 - and are accessed by an index
26:00 - here's an example of how the users table
26:03 - from the previous slide is altered to
26:06 - insert a list of frequent destinations
26:11 - first the table is added to list of text
26:14 - data types
26:16 - then the user cas one two three is
26:19 - updated and berlin london and paris are
26:23 - added to the list
26:25 - list uses the set command to enter the
26:28 - values for the respective collection
26:30 - type
26:32 - map is the last of our collection types
26:36 - it allows us to enter values which are
26:39 - in relation ship with one another in a
26:42 - key value format
26:45 - map is ordered by unique keys
26:48 - both the key and the value have a
26:50 - designated data type
26:53 - map uses the set command to enter the
26:56 - values for the map collection type
27:00 - let's look at an example here
27:03 - here the table users have again been
27:05 - altered with a map called to do with the
27:08 - data types of timestamp and text
27:12 - then the user cast 123 is updated and a
27:16 - map to-do list was added
27:19 - the to-do list is a set of dates and
27:22 - to-do items
27:24 - in this case the user
27:27 - cas123 is going to create a database on
27:30 - january 1st
27:32 - load and test the database the next day
27:34 - and then a month after move the database
27:37 - to production
27:39 - here's another collection type frozen
27:43 - if you want to nest collections inside
27:46 - other collections you will have to use
27:48 - the frozen keyword
27:51 - without using frozen you cannot nest a
27:54 - collection inside another collection
27:57 - frozen allows us to serialize multiple
28:00 - components into a single value
28:04 - essentially frozen will turn that value
28:06 - into a blob
28:09 - if you want to update a single value in
28:11 - the collection you can't use the frozen
28:14 - keyword
28:15 - now it's time to explore user defined
28:18 - types popularly called udt
28:23 - udt allows you to group together similar
28:26 - fields of data that are both named and
28:30 - typed
28:32 - udts can include any supported data
28:35 - types
28:36 - including other collections and other
28:38 - udts
28:40 - multiple data fields can be attached
28:43 - each named and typed to a single column
28:46 - udts allow embedding of more complex
28:49 - data within a single column adding
28:52 - flexibility to your table and to your
28:54 - data model
28:56 - let's look at some examples
28:59 - here a udt is created which has street
29:03 - city
29:04 - zip code and a set of phone numbers
29:07 - now all these fields can make up a
29:09 - single address
29:12 - but in a relational world it is possible
29:14 - for several users to have the same
29:17 - address
29:18 - so a new udt or full name is created
29:22 - which has the user's first name and last
29:24 - name now let's look at how the udt is
29:28 - used in a table
29:30 - first a users table is created which
29:33 - includes name
29:35 - set of direct reports and a map of
29:38 - addresses in the name column the full
29:42 - name udt is used
29:45 - after that a set collection type with
29:47 - full name udt is used to create a direct
29:52 - reports column and finally address udt
29:56 - is used in a map collection
29:59 - in relational databases to find a query
30:02 - you will have to first join these two
30:04 - tables together but in cassandra's
30:07 - denormalization
30:09 - technique the address is embedded in a
30:11 - single cell of the user's table
30:15 - a counter is a data type where you can
30:17 - have a column that stores a 64-bit
30:20 - signed integer
30:23 - they could be incremented or decremented
30:25 - and the values are changed using the
30:27 - update command
30:29 - counters need to have a specially
30:31 - dedicated table that can only have
30:34 - primary key and counter columns
30:38 - counters can lead to the duplicate data
30:41 - in multiple tables
30:43 - you can have more than one counter
30:45 - column in your table
30:47 - let's say we own a range that
30:49 - specializes in cows and we want to count
30:52 - the moo each cow makes
30:55 - we're going to create a table called moo
30:57 - count
30:58 - it has the column name cow name which is
31:01 - also the primary key of the table and
31:04 - the moo count is the counter column
31:07 - now to add to a moo count we will use
31:10 - the update command to update the mu
31:12 - counts table each time we hear a move
31:17 - in this example we updated the mu count
31:19 - to increment the value by 8 where the
31:22 - cow name is betsy
31:32 - upon completion of this module you will
31:35 - be able to learn about the different
31:38 - models in the data stacks modeling
31:41 - methodology
31:44 - upon completing this topic you will be
31:47 - able to understand the purpose of
31:49 - conceptual data model
31:51 - and learn about the cardinality
31:54 - attribute keys er model and relationship
31:57 - keys
31:59 - let's begin with the conceptual data
32:02 - model
32:03 - it is responsible for determining the
32:06 - attributes of all the objects in your
32:08 - domain and analyzing how they are
32:12 - related
32:14 - a conceptual model defines what a model
32:18 - should contain
32:19 - for example users comments videos
32:23 - ratings etc
32:25 - the purpose of a conceptual data model
32:28 - is to identify what type of data is
32:32 - contained in the model itself
32:35 - from here you can extrapolate the
32:37 - essential objects
32:39 - and constraints that the data types
32:42 - would additionally require
32:46 - it is important that you capture all
32:49 - different views of your model while
32:51 - conceptualizing your data model
32:54 - collaboration with other departments and
32:56 - experts is the key
32:59 - as a database architect it is critical
33:02 - that your solutions serve your business
33:05 - needs
33:07 - you will not always be an expert on the
33:09 - business domain and therefore it is
33:12 - important to interact with non-technical
33:14 - people to get hold of the details
33:18 - both the technical and non-technical
33:20 - members can share review and agree upon
33:24 - the conceptual data model
33:27 - business experts can then verify and
33:30 - ensure that the conceptual data model is
33:32 - correct and everyone is on the same page
33:36 - collaboration between different
33:38 - technical roles is also immensely
33:41 - critical
33:42 - apache cassandra requires database
33:45 - administrators and developers to work
33:48 - closely with each other
33:50 - the conceptual model is a way of
33:53 - bringing together the technical and
33:56 - non-technical people to the same table
34:01 - the conceptual data model offers an
34:04 - abstract view of your domain
34:06 - it is generally independent of
34:09 - technology meaning that it does not
34:11 - specify
34:13 - what technology to use for example sql
34:17 - or nosql
34:19 - at this point it is not specific to any
34:21 - database either
34:25 - the term cardinality often refers to the
34:28 - relationship between two entities in a
34:31 - given data model
34:33 - or in other words how many instances of
34:36 - an entity are related to an instances of
34:38 - another entity
34:40 - cardinality tells how many times an
34:43 - entity can or must participate in a
34:46 - relationship
34:47 - other possibilities are one to n
34:50 - and one is to one
34:53 - here the relationship between the video
34:55 - entity and the actor entity is the
34:58 - character name in the video and the
35:00 - actors who played those characters
35:03 - here m and n are the videos and actors
35:06 - respectively
35:08 - the videos feature actors
35:10 - each video will have several actors and
35:13 - each actor can participate in several
35:16 - videos
35:19 - attribute types are the fields that
35:21 - store properties about an entity or
35:24 - relationship
35:25 - attributes or fields where we wish to
35:28 - define our entity
35:30 - for example we may want to store the
35:33 - title and description of each video
35:36 - actors of names which we can further
35:39 - break down into other attributes like
35:42 - first name and last name respectively
35:46 - key attributes uniquely identify an
35:48 - entity
35:50 - two videos can have the same name or
35:52 - description but the id is unique to a
35:55 - single video
35:57 - and the names are the unique key for
35:59 - actors
36:04 - let's look at the entity relationship
36:07 - model
36:08 - here we have an entity type the
36:11 - relationship types and attribute types
36:14 - and first we need to identify the entity
36:16 - types
36:18 - there are different entity types and
36:20 - videos
36:22 - these are things you're looking for to
36:24 - find information
36:26 - attribute types will look at actors and
36:29 - define their first and last names
36:31 - respectively
36:33 - the relationship type looks at the
36:35 - relationship between the video and the
36:38 - actor
36:41 - attributes can have multiple values and
36:44 - we call these not surprisingly
36:46 - multi-valued attributes
36:49 - multi-valued attributes store multiple
36:52 - values per attribute
36:54 - one can refer to these attributes with a
36:57 - double oval diagram around the attribute
37:00 - types
37:02 - for example one video can be an example
37:06 - for more than one genre
37:10 - now you will learn about the three
37:11 - relationship keys one to one one to many
37:15 - and many to many respectively
37:18 - in the one-to-one relationship we use
37:21 - either of the participating entity keys
37:25 - in this example a movie can have one
37:28 - defined date for its first showing
37:31 - so this is an example of an one-to-one
37:34 - key relationship
37:36 - one to many relationships uses the
37:39 - entity key on the many relationship side
37:43 - here we have one user but we can upload
37:46 - multiple videos
37:49 - many-to-many relationship combines both
37:52 - the entity keys
37:54 - here many actors can be part of multiple
37:57 - videos
38:00 - weak entity types are entities that
38:02 - cannot exist without a corresponding
38:05 - strong entity on the other side
38:08 - for example if the video vanishes or
38:12 - disappears then the encoding will
38:15 - disappear too
38:22 - upon completing this topic you will be
38:25 - able to
38:26 - learn the concept of workflow and access
38:29 - patterns
38:32 - you will learn about application
38:34 - workflow which describes how users will
38:38 - navigate through the application
38:41 - it also helps in determining which
38:44 - queries will perform against an apache
38:47 - cassandra database
38:50 - every application has a particular
38:53 - workflow
38:55 - access patterns help us to determine how
38:58 - the data is accessed
39:00 - we also get to know what queries to run
39:03 - first
39:05 - for example when a user logs into a
39:08 - website what are they going to click
39:12 - what data are they going to use
39:14 - what data are they going to read
39:17 - how will a user go through a website
39:20 - application
39:21 - the workflow and access patterns must be
39:24 - thought through before building the
39:27 - application
39:28 - think about how the application should
39:31 - work and how users should use it
39:35 - for example you may want one query to
39:38 - run when a user logs into the site
39:41 - this query will perhaps find the user
39:44 - with a specified email address
39:47 - once the user is in the site you may
39:50 - want to run the second query to show the
39:53 - latest videos uploaded to the site
39:57 - you may want to
39:58 - run a third query to verify the user's
40:02 - login for them to access their account
40:04 - details is indeed working or not
40:08 - a fourth query can be used to find
40:10 - videos uploaded by users with a known id
40:15 - a fifth query can help you find a video
40:19 - with a specified video id
40:22 - these are just examples of popular
40:24 - queries one can build into an
40:27 - application
40:31 - upon completing this topic you will be
40:35 - able to
40:36 - understand the mapping rules
40:39 - explore a modeling approach to see how
40:42 - mapping rules are applied
40:44 - learn the concept of chaboko diagrams
40:48 - understand the logical and physical udt
40:52 - diagrams
40:53 - and learn about the four data modeling
40:56 - principles
40:59 - at this point we have covered conceptual
41:02 - data model and the application
41:04 - workflow next we will look at how to
41:07 - make a logical data model
41:10 - to do this we have to map our conceptual
41:13 - model to a logical model
41:16 - now you will go through some basic rules
41:18 - for query driven methodology
41:21 - the mapping rules ensure that a logical
41:24 - data model is correct
41:26 - each query has a corresponding table
41:30 - the tables are designed to allow queries
41:34 - to execute properly
41:36 - the tables will return the data in the
41:39 - correct order
41:42 - here are the five mapping rules to guide
41:45 - a query driven
41:46 - transition mapping rule 1
41:50 - identify the entities and their
41:52 - relationships
41:54 - mapping rule 2
41:56 - identify the equality search attributes
42:01 - mapping rule 3 identify the inequality
42:05 - search attributes
42:07 - mapping rule 4 identify the ordering
42:11 - attributes and
42:14 - mapping rule 5 identify the key
42:17 - attributes
42:19 - create a table schema from the
42:21 - conceptual data model for each query
42:25 - apply the mapping rules in order
42:29 - for this schema we're looking at videos
42:31 - by user upload
42:34 - therefore
42:35 - mapping rule 1 identify your entities
42:38 - and relationships
42:40 - in this case we have a table named
42:43 - videos uploaded by users
42:46 - there is a relationship between two
42:48 - uploads with users being one entity and
42:51 - the videos being the other
42:54 - mapping rule 2
42:56 - identify your equality search attributes
43:00 - in this case clearly equality is the
43:03 - user id
43:05 - mapping rule 3 identify the inequality
43:09 - search attributes
43:11 - in this case it is the uploaded
43:13 - timestamp
43:15 - mapping rule 4 identify the ordering
43:19 - attribute
43:20 - mapping rule 5
43:22 - identify the key attributes needed to
43:26 - guarantee uniqueness and minimality
43:30 - here video id guarantees uniqueness
43:35 - we have the conceptual data model on the
43:37 - left and the access patterns on the
43:40 - right
43:41 - it is important to remember that our
43:44 - application workflow had access patterns
43:47 - based on the queries
43:49 - which we wish to execute on our apache
43:53 - cassandra database
43:55 - this is an important part of the
43:57 - application workflow
43:59 - to get a logical data model we combine
44:02 - the results of the conceptual data model
44:05 - and the access patterns after combining
44:09 - them some mapping rules and patterns are
44:13 - applied
44:14 - this logical model is described using a
44:18 - chiboko diagram a diagram named after
44:22 - artem chaboco who devised his method of
44:26 - mapping to a logical data model
44:30 - chaboco diagrams graphical
44:33 - representations of cassandra database
44:35 - schema design
44:38 - they document the logical and physical
44:40 - data models respectively
44:43 - you already know your entities that is
44:45 - the video and the key attributes
44:48 - at the bottom query one is to find a
44:51 - video with a specified video id
44:55 - now if you look at the chiboko diagram
44:58 - we can see that q1 leads into a table
45:01 - which can satisfy this query we will get
45:04 - into that in more detail but first we
45:07 - have to understand some of the
45:10 - terminologies used here
45:13 - first and foremost this diagram looks
45:15 - very similar to an application workflow
45:19 - we have a queries which lead us into the
45:22 - tables
45:23 - we have the table diagrams which are
45:26 - similar to the tables we have in our
45:28 - cassandra database
45:31 - we have the query list at the bottom
45:34 - right
45:35 - as you know queries are the king and the
45:37 - queen while designing the cassandra data
45:40 - model
45:41 - and lastly we have our udt's which stand
45:45 - for user-defined data types
45:48 - these are a lot like cassandra tables
45:51 - but without a primary key
45:53 - and these can be nested into other
45:56 - cassandra
45:58 - tables on a logical level the cheboko
46:02 - diagram shows the column names and
46:04 - properties
46:06 - on a physical level cheboko shows the
46:09 - column data type
46:11 - in order to move from the logical tables
46:14 - to the physical tables we just need to
46:16 - add column data type followed by a bit
46:20 - of optimization if needed
46:23 - here is an example of how a table is
46:26 - broken down by column name data type and
46:29 - type of column
46:31 - as you can see in our table we have some
46:34 - abstract column names on the left and
46:36 - the corresponding descriptions on the
46:39 - right
46:40 - note that the notation on the left
46:44 - corresponds to the description on the
46:46 - right that is column 7 which is a list
46:50 - is in square brackets
46:52 - similarly column 8 a set is enclosed in
46:56 - curly brackets
46:58 - also a map like column 9 is in angular
47:02 - brackets and so on
47:05 - however the key thing to notice here is
47:08 - that all these columns are followed by
47:10 - their cql types which we have previously
47:14 - studied
47:15 - it is the physical types that makes the
47:19 - table physical
47:21 - we can now write our create table
47:23 - commands using these column names and
47:27 - their respective types into our database
47:31 - also note that column one is a partition
47:34 - key which is represented in the table as
47:38 - k
47:39 - similarly column 2 and 3 are clustering
47:42 - columns which are represented by c
47:46 - followed by an upward arrow or downward
47:49 - arrow representing the ascending order
47:52 - or descending order respectively
47:55 - a static column is specified by s
47:58 - secondary index column as idx and the
48:01 - counter column as a plus plus
48:06 - here you will learn about logical udt
48:09 - diagrams in detail
48:11 - udt is a data type and udts can be
48:15 - nested within one another
48:18 - for example there is an encoding column
48:21 - in the videos table
48:23 - this column represents our data is
48:25 - stored in the database
48:28 - there is more information about the data
48:31 - than just simple text
48:34 - hence it is encapsulated and it's stored
48:36 - in the encoding udt
48:39 - it has attributes of encoding height
48:41 - width and bit rates
48:44 - the same is true for actor name which
48:47 - encapsulates the first name and last
48:50 - name of the actor and stores it into the
48:52 - udt column
48:54 - if we want to have a single column to
48:56 - store both first and last name or have
48:59 - them stored differently it is important
49:02 - to remember that udt can be very helpful
49:06 - in such cases
49:08 - here is the same diagram but with the
49:11 - cql types explicitly specified
49:16 - is a representation of the physical
49:18 - layer udt diagrams
49:21 - please notice how real cql types have
49:24 - been specified for each column
49:29 - this is a classic example of a logical
49:32 - data model in the form of a detailed
49:34 - chiboko diagram for the killer video
49:36 - domain
49:38 - take a moment and carefully observe this
49:40 - diagram for all the concepts that we
49:43 - have covered so far
49:45 - we have queries which access our access
49:48 - patterns are tables with different kinds
49:51 - of data an application workflow that
49:54 - links these queries to the tables and
49:56 - are encoding udt defined
50:01 - here are the four main principles of
50:04 - cassandra data modeling
50:06 - know your data
50:08 - know your queries
50:10 - nest data and duplicate data
50:14 - we will look at each of these principles
50:16 - in more detail
50:20 - understanding the data is key to a
50:22 - successful implementation of a database
50:26 - a proper understanding of data provides
50:29 - the understanding to organize the data
50:32 - as well
50:33 - as we have seen space and time are often
50:36 - are primary concerns while data modeling
50:40 - in cassandra we sacrifice space in order
50:44 - to reduce time
50:46 - hence organization of data is of utmost
50:49 - importance
50:50 - the conceptual data model provides us
50:53 - with the data capturing information
50:56 - once we define what is to be stored we
51:00 - move on to create a database
51:03 - this not only helps in successful
51:05 - organization but also helps in
51:07 - preserving the properties of the data
51:10 - that is required
51:12 - part of understanding the data requires
51:15 - understanding a primary key as well
51:19 - to recall primary keys take care of two
51:22 - essential things uniqueness and
51:25 - satisfying queries
51:27 - entity and relationship keys affect the
51:30 - table primary keys
51:33 - primary key uniquely identifies a row
51:36 - entity and a relationship
51:39 - let's look at the following tables for
51:42 - example
51:43 - the video tables as video id as its
51:46 - primary key
51:48 - here the primary key makes no attempt at
51:50 - organizing the data so as to satisfy
51:53 - queries at later stages
51:56 - on the other hand the table videos by
51:59 - user as a primary key along with
52:02 - clustering columns which specify
52:04 - ordering in order to help with queries
52:07 - that result in returning the results in
52:10 - a specified order
52:13 - the relationship cardinalities as well
52:16 - as the data cardinality are very
52:18 - important when it comes to data modeling
52:22 - data cardinality helps in selecting an
52:25 - efficient partition key whereas the
52:28 - relationship cardinality helps in
52:30 - satisfying the queries
52:34 - queries are the center of data modeling
52:36 - in cassandra and captured by the
52:39 - application workflow models
52:42 - each entity is created with queries in
52:46 - mind
52:47 - in cassandra it's always better to
52:50 - reduce the number of partitions that are
52:52 - to be read for a query result
52:55 - table schema also changes if the queries
52:59 - are changed
53:01 - to achieve this it is extremely
53:03 - important to clearly understand the kind
53:06 - of data required out of the read
53:09 - operations
53:11 - single partition per query is the most
53:14 - efficient access pattern
53:17 - entities in cassandra are designed in
53:19 - such a way that queries can generate the
53:22 - results with the minimum number of reads
53:26 - ideally the data should be organized in
53:29 - such a way that each query can retrieve
53:31 - the results from a single partitioned
53:34 - search
53:36 - partition can be single row or multi-row
53:39 - respectively
53:40 - in this example if we want to query for
53:43 - a particular partition we can give the
53:46 - partition key
53:47 - then the node having the partition is
53:50 - determined and the data is retrieved by
53:53 - the node and returned to us
53:55 - no additional filtering or searching the
53:58 - entire cluster is required
54:01 - however there are cases when multiple
54:04 - partitions have to be accessed to
54:06 - retrieve the results and although this
54:09 - is a less efficient way of doing things
54:11 - in cassandra it is acceptable to some
54:14 - extent as long as these kind of queries
54:17 - are limited in number
54:20 - however the strict anti-pattern that has
54:23 - to be avoided at all costs is asking the
54:27 - database to perform a linear search
54:30 - through all the partitions of all the
54:32 - tables to fetch the result for a
54:34 - particular query
54:36 - since most of the data is stored in
54:38 - distributed nodes it is highly
54:41 - inefficient to go through all the data
54:43 - stored at multiple locations and it
54:46 - defeats the whole purpose of having a
54:48 - horizontally scaled database
54:54 - after completing this topic you will be
54:58 - able to learn the concept of data
55:01 - nesting and explore the three ways of
55:05 - nesting data
55:08 - until now we've talked about creating a
55:10 - logical data model by combining the
55:13 - results of a conceptual data model and
55:17 - accessing the pattern of application
55:19 - workflow by applying mapping rules and
55:22 - patterns
55:24 - let's now create a logical data model
55:27 - containing tables for each query
55:30 - capturing entities and relationships
55:33 - from the conceptual model
55:36 - the working rule with apache cassandra
55:39 - is to group as much data as possible on
55:42 - disk
55:43 - this grouping of data is referred to as
55:47 - nesting of data and is the main data
55:50 - modeling technique for a cassandra data
55:53 - model
55:54 - nesting helps in organizing multiple
55:57 - data entities into a single partition
56:01 - and it also helps support partition per
56:04 - query during data access
56:07 - in cassandra queries are the king and
56:10 - the queen
56:11 - schemas are often driven through these
56:14 - application queries
56:17 - there are three main ways of nesting
56:19 - data in cassandra
56:22 - these are clustering columns collection
56:25 - columns and user defined type
56:28 - columns
56:30 - clustering columns are the primary data
56:33 - nesting mechanisms
56:35 - look at the actors by video table on the
56:38 - right
56:39 - we apply a query to filter by video id
56:43 - but we nest the actors by their actor
56:46 - name and the character name using
56:48 - clustering columns
56:51 - the partition key identifies the entity
56:54 - that other entities nest into while the
56:57 - clustering columns identify the nested
57:00 - entities
57:02 - when a table has multiple clustering
57:04 - column the data is stored in a nested
57:08 - sorted order
57:09 - multiple clustering columns implement
57:13 - multi-level nesting
57:15 - in the given example actors are
57:18 - identified by videos using clustering
57:21 - columns
57:24 - udts are a secondary mechanism for
57:27 - nesting data
57:29 - usually this occurs when there is a
57:32 - one-to-one relationship
57:35 - you also have lists of maps
57:37 - or your choice of user-defined types
57:40 - within a single column
57:42 - working with udts is much easier than
57:45 - working with multiple collection columns
57:49 - for example we turn videos into a video
57:52 - type udt with id title description and
57:56 - other parameters
57:58 - then we put all those videos in a
58:01 - collection column in the videos by user
58:04 - table
58:05 - table videos by user tests all videos as
58:09 - a collection in the videos column with
58:12 - the video underscore type type
58:17 - partition per query and data nesting may
58:21 - result in data duplication
58:24 - in relational data models the tables are
58:27 - joined as we read the data
58:29 - in cassandra however instead of wasting
58:32 - time on reads the data is taken and
58:35 - returned to the client application
58:38 - for example we have three tables that
58:41 - represent the video's object videos by
58:44 - actor videos by genre and videos by tag
58:49 - this duplicates data across tables
58:52 - partitions and rows
58:54 - that means every time we want to store a
58:57 - new video we have to write the record
59:00 - into each one of these tables to keep
59:03 - them in sync
59:07 - upon completing this topic you will be
59:10 - able to
59:12 - learn how to use data types in a table
59:16 - understand the ways to load data into
59:18 - the table
59:20 - and explore some of the features of cql
59:24 - copy
59:25 - here's our data modeling methodology
59:28 - again
59:29 - we've created our conceptual model and
59:32 - our application workflow
59:34 - we've also applied the mapping rules
59:37 - created a logical data model
59:40 - and now it's time for us to create our
59:42 - physical data model
59:45 - to make a physical model add data types
59:48 - to each column of a logical model
59:52 - physical models also
59:54 - optimize efficiency and performance
59:58 - ensuring that the partition sizes won't
60:01 - grow too large
60:03 - and will return the queries quickly
60:06 - here we have a logical version of
60:09 - commons by user and a physical version
60:12 - of comments by user
60:14 - as you can see the physical version of
60:17 - cql data types like
60:21 - user id is uuid
60:24 - posted timestamp is of time stamp data
60:27 - type
60:28 - video id is of time uuid data type
60:33 - comment title and type is of text
60:36 - respectively
60:38 - tags is a set of text
60:40 - and lastly
60:42 - preview thumbnails is defined as a map
60:47 - now we will use these data types in
60:49 - creating a table
60:53 - we already saw how to use the data types
60:56 - in the table in theory
60:59 - to better understand the physical data
61:02 - model it's time to demonstrate it with
61:04 - an example
61:06 - creating a table could cause a lot of
61:09 - problems if the tables aren't configured
61:12 - correctly
61:14 - the first challenge is creating a
61:16 - partition that is way too large
61:19 - there are many other challenges that are
61:22 - related to optimization
61:25 - now we have data types which we have
61:27 - added in the previous topic
61:29 - now let's create a table using the cql
61:32 - create table statement
61:35 - in this example we've created a table of
61:38 - comments by user
61:41 - the field in the table are everything
61:43 - that we saw on the previous slide in the
61:45 - physical version of the table the user
61:48 - id is uuid
61:50 - and the posted timestamp is a timestamp
61:53 - and all the other data types that we saw
61:56 - earlier
61:59 - once the table is created the user may
62:02 - want to insert data into it
62:05 - there can be two types of users
62:08 - a new user and an existing user
62:11 - if it's a new application then users
62:14 - will use your site and insert the data
62:16 - for you
62:18 - but if you're migrating from an existing
62:20 - database then it may take a few ways to
62:23 - get the data into the table
62:26 - first is the copy command which can
62:29 - leverage cql copy in cql and has a value
62:33 - of inserting approximately 2 million
62:37 - values into the database
62:41 - second is the ss table loader which is
62:44 - used when migrating from one cluster to
62:47 - another
62:49 - and lastly there is spark for data
62:51 - loading which has a lot of options but
62:54 - does not require a working knowledge of
62:57 - spark
63:00 - here are some basic sql copy features
63:04 - copy 2 exports data from a table to a
63:08 - csv file
63:10 - copy from imports data to a table from a
63:14 - csv file in sql copy the process
63:18 - verifies the primary key and updates the
63:21 - existing records accordingly
63:24 - if the header is false it specifies that
63:27 - the fields are imported in a
63:29 - deterministic
63:30 - order
63:31 - when column names are specified in the
63:34 - table then the fields are imported in
63:36 - that order
63:38 - if any field is missing or empty it is
63:41 - set to null
63:43 - source cannot have more fields than the
63:45 - target table but surely can have less
63:48 - number of fields than the target table
63:52 - so let's look at an example of basic
63:54 - data loading with cql copy
63:57 - this is a good method to use and to get
64:00 - started
64:02 - we will import the data in column 1
64:05 - column 2 and column 3 from the csv file
64:09 - table 1 data.csv
64:12 - when header is equal to true
64:26 - welcome to module 4 cassandra practical
64:30 - application
64:31 - we're going to help you start your own
64:34 - cassandra as a service db on a cloud
64:37 - provider of your choice
64:40 - cassandra as a service is named astrodb
64:43 - and is provided by datastax
64:47 - upon completion of this module you will
64:50 - be able to
64:52 - create your own astrodb instance
64:55 - and perform the workshops
65:02 - upon completing this lesson you will be
65:05 - able to perform the workshop related to
65:07 - this course
65:12 - earlier we learnt about different apis
65:15 - to connect to our astrodb like the
65:17 - document api graphql api and the rest
65:21 - api
65:23 - all of these apis are part of a project
65:26 - called stargate
65:28 - stargate is built into astrodb for your
65:32 - convenience
65:34 - if you would like to learn more about
65:36 - what stargate is how it works how you
65:39 - can get involved in the open source
65:42 - project please go to stargate.io
65:45 - for more information
65:47 - for an in-depth understanding of how it
65:50 - all works and how you can utilize target
65:53 - your advantage
65:54 - view the workshop video on youtube
65:57 - created by the data stacks developer
66:00 - advocates
66:02 - scan the qr code to go to the video or
66:06 - copy the link and paste it in your
66:08 - preferred browser
66:11 - another option for your astra db is to
66:14 - build a to do app using the rest api
66:17 - python and node.js respectively
66:21 - this is an example react to do
66:24 - application built using a datastax astra
66:27 - fee free tier database
66:31 - for an in-depth understanding of how it
66:33 - all works
66:34 - view the workshop video on youtube
66:37 - created by the datastax developer
66:40 - advocates
66:42 - scan the qr code to go to the video or
66:45 - copy the link and paste it in your
66:47 - preferred browser
66:50 - in this example we discover a fully
66:52 - reactive version of the spring pet
66:55 - clinic application using spring web flux
67:00 - this example is considered an
67:02 - intermediate level application for
67:05 - developers to build on astra db
67:08 - for an in-depth understanding of how it
67:10 - all works view the workshop video on
67:13 - youtube created by the data stacks
67:15 - developer advocates
67:18 - scan the qr code to go to the video or
67:22 - copy the link and paste it in your
67:24 - preferred browser
67:27 - earlier in this course we carried out
67:29 - some basic data modeling to understand
67:32 - how cassandra can best be built and
67:36 - deployed
67:37 - if you wanted to do a deeper dive into a
67:40 - more advanced data modeling for your app
67:43 - we also have a workshop available for
67:46 - you to view on youtube from the data
67:49 - stacks developer advocates
67:52 - once again scan the qr code to go to the
67:55 - video or copy the link and paste it in
67:58 - your preferred browser
68:01 - throughout this course we had mentioned
68:03 - using spark for specific functions on
68:06 - astra db including data loading and
68:08 - connecting
68:10 - we also wanted to provide you with a
68:13 - workshop for you to view if you would
68:16 - like to get a more in-depth
68:17 - understanding of how it all works and
68:20 - how you can utilize spark to your
68:22 - advantage on youtube again created by
68:25 - the developer advocates of data stacks
68:29 - scan the qr code to go to the video or
68:32 - feel free to copy the link and paste it
68:35 - in your preferred browser

Cleaned transcript:

apache cassandra is an open source nosql distributed database this beginner's course has four modules the first three modules cover the basics of apache cassandra data modeling the final module covers practical applications of cassandra if you want a broad overview of nosql databases you can check out the previous nosql database course on our channel this module will cover the basics of apache cassandra data modeling upon completion of this module you will be able to differentiate between the relational databases and an apache standard database recognize the basic terminologies used in cassandra tables explore cql fundamentals identify how partitions primary keys and clustering columns are used in cassandra tables observe the concept of replication and consistency upon completing this lesson you will be able to understand how relational data modeling concepts compare to those used when data modeling for apache cassandra now you will learn about some differences between relational databases and apache cassandra the cassandra data model follows cassandra modeling methodology while the relational database follows the sample relational model methodology in cassandra modeling we think about the application needs when building the data model since queries are the driving force for this model those queries must be conceptualized before data entry whereas in a relational model entities are the driving factor in relational modeling primary keys are used to ensure uniqueness of the entities in cassandra modeling primary keys take on an additional significance particularly in determining performance on a large scale in order for us to provide high performance and availability on a large scale cassandra uses a distributed architecture cassandra by default does not support asset transactions or joins but uses denormalization instead cassandra does not enforce referential integrity across tables here's a sample of a relational data model this is one of many types of relational data models this model produces conceptual logical and physical data models including steps for normalization and optimizing the data model note that queries aren't considered until the very late stages of the process let's now look at the apache cassandra data modeling approach here we still have conceptual logical and physical data models but the application workflow and inquiries are considered earlier in the process this analysis is incorporated early into the logical data modeling phase rather than waiting until the end of the process let's look at the difference between the modeling approaches in the relational approach we first think about the type of data create a data model based on the data and then think about our application last the developer can then make queries to retrieve this information in the approach followed by cassandra the process is reversed here the application of the data must be decided up front in other words what queries are going to be performed that will dictate how your logical model will take shape once your queries and data model are complete you can load the data into your tables relational databases are asset compliant by default acid is used to describe the transactional behavior in a relational database and stands for atomicity consistency isolation and durability atomicity refers to the integrity of the database transaction either all the statements in a transaction will succeed or none of them will consistency means that the data within a database should comply with certain rules transactions cannot leave the database in an inconsistent state the new database state must satisfy integrity constraints isolation refers to the ability to process multiple transactions at the same time without interference durability refers to the ability of a completed transaction persisting even after a subsequent failure apache cassandra does not support asset properties like most relational databases in database management both commit and rollback can easily be achieved using transaction logging when a single server is involved but when the data is spread across servers in distributed databases like cassandra the situation becomes tricky the locking and coordination required to ensure these twophase semantics of acid can cause severe performance penalties however cassandra does provide atomicity isolation and durability when performing read and write commands on a single row additionally it also provides something called tunable consistency which can be adjusted depending upon the number of nodes which have to agree to execute a particular read or write command to understand tunable consistency let's look at the cap theorem first given by eric brewer this theorem talks about the tradeoffs in implementing a distributed database all distributed databases are primarily tolerant to partitions hence the choice is really between consistency and availability cassandra by default is an ap system which means it chooses availability and partition tolerance while sacrificing consistency however it does provide the option to make the system more consistent by sacrificing availability this is referred to as tunable consistency wherein the consistency of the database can be adjusted or tuned by adjusting the number of nodes which have to return a read or write request cassandra primarily operates under the ap model because all data eventually becomes consistent one more point of difference between cassandra and relational databases is the support of joins to recall joins are a statement often used by relational databases to combine data a join statement facilitates the query through the joining of data from different tables take a look at the example below the join statement combines results from two tables namely videos which stores the details of videos and comments which stores the comments made by different users on the videos this is done by the matching of video id as specified in both the tables the database matches the video id for each video and generates a result this is the output of the join command the select statement in the given transaction selects the column specified from the output of the join command and the where statement is the filtering criteria therefore the output of this transaction only provides the comments of the video titled interstellar the basic requirement for a join command to work is that the results from two combined tables should be stored at one location however in distributed databases the different partitions of data are stored on different nodes hence joins would have a negative impact on the latency as the scale of the databases increases and the random distributions become more prevalent unlike relational databases cassandra does not support joints but uses denormalization instead denormalization ensures that all the required information is confined to the appropriate table without having to perform joins organizing data like this can duplicate data in multiple tables which is cost efficient and allows for faster retrieval of said data while using cassandra to organize the data the developer can decide upfront what tables they want their data to be in in this example we have two scenarios in which we want to access comments first a video view where all the comments for a particular video can be viewed and a second a user view that displays comments made by a particular user to support this two tables must be created the first table comments by video supports the ability to access comments for a specific video here video title is used as a partition key the comment id helps in defining a unique row in the database for each comment the second table comments by user supports the same data but has a different partition key here the user id is used as the partition key note that these two tables reflect the common naming convention we have for cassandra tables we append by something to the table name to convey some idea of the query that the table is designed to support joins rely on referential integrity constraints to combine data let's look at this point of difference between relational data modeling and cassandra data modeling ri refers to the relationship between tables a value in one table requires the same value to exist in another table if there is a user in the user by email table then the user must also exist in the users table and vice versa this referential integrity is required between tables such that a user is either represented in both tables or none of them and also requires for a join to work properly cassandra does not enforce referential integrity because this capability requires a read before a write it is not considered an issue that has to be fixed on the cassandra side although it is a feature that improves performance and accessibility it is considered a developer prerogative referential integrity can be enforced in an application design but that means more work for the developers upon completing this topic you will be able to differentiate between the terminologies used in apache cassandra tables understand the different data types available in cassandra cassandra uses database terms that have specific connotations in the context of data modeling the key terms are data model key space table and partition a data model is an abstract model for organizing elements of the data data models vary depending on the type capability and purpose of the database the type of the data model depends on the capability of the database for cassandra the data model is based on the queries that you may want to perform a key space is the outermost logical container of tables it stores tables and replication data in other words it's a container for replication a table is a combination of rows and columns the tables are contained within the k space as a columnar database cassandra stores data based on partitions partition is a raw rows of data that are stored on a node in the data table based on the partitioning strategy each row in the partition consists of key and value pairs cassandra stores and retrieves data based on partitions a partition is a terminology where you can make or break the data model a primary key is the most important part of the data model it does two things first it guarantees the uniqueness of the data and second it defines the placement of the record in the cluster this allows for easy access to the data in the model a partition key is the first part of the primary key it determines where in the cluster your data will be stored on which node it will be stored a partition key is a left hand column in the graphic indicating one two or three respectively inside each partition there are rows and columns columns are stored locally and referred to as cells in the table a row is contained inside a partition there can be more rows and they are stored together on a partition clustering columns define the order of data within a given partition the default sorting is in the ascending order upon completing this lesson you will be able to explore the basic cql concepts within an apache cassandra database cassandra query language or cql is a nosql language also known as not only sql let's learn about some basic cql concepts including building blocks such as key spaces tables and the primary keys as well as basic cql commands such as select truncate alter table and source the key space is a top level container in apache cassandra to organize a related set of tables it is very similar to a relational databases schema cassandra key space defines replication settings that describe how many copies of a given piece of data are being stored within a cluster once you've created a key space in cqlsh you may want to select the key space that will be used for your subsequent command you can enter a use command followed by the name of the key space if you do not select a key space with the use command you'll have to include the name of the key space whenever you reference the name of a table this helps cassandra interpret your request let's take a quick look at the syntax for creating a table the code here shows the statements for creating two different tables remember the key spaces contain tables and tables contain our data note that every cassandra table has a primary key inside each key space is the table the primary key clause is a unique identification for each row within the table cassandra's primary keys uniquely identify rows of data in this example we see a uuid used as a partition key to uniquely identify each user in our users table however a partition key alone is not enough to ensure uniqueness of a row it is the primary key clause as a whole that determines uniqueness of a row within a partition the cql select command is used to read data from cassandra tables selecting all the rows of the table using an asterisk notation causes cassandra to perform a full table scan this could be very expensive on a table with a large number of rows instead use the limit keyword to get a subset of the rows cassandra also supports a paging mechanism which indicates specific columns we desire to read by name or uses the asterisks to indicate all columns cassandra also supports a couple of builtin aggregation functions such as the count function for counts on larger tables you may want to consider using the spark part of disk analytics another command you'll find useful is the truncate command this command causes all rows to be deleted from a table while leaving the schema in place this command should obviously be used with care as the data is removed from persistent storage and you would have to restore it from a backup to retrieve it truncate sends a jmx command to all nodes to delete ss tables that hold the data if any of these nodes are down the command will fail while working with cassandra you'll most likely encounter situations when you'll need to make changes to your table this is where the altered table command comes into play you can change the data type of a column add columns drop columns rename columns and change table properties but you cannot change the columns in a primary key clause here's an example code for adding and dropping columns from a table the last command is the source command which allows you to execute a set of cql statements from a file to execute a source command you'll want to put the file name in single quotes cqlsh will output the results of each command sequentially as it executes upon completing this topic you will be able to learn about the basic concepts of partitions table partitions partition ordering partition distribution and partition storage to truly understand the data modeling you must master partitioning concepts partitions give you an indication of where your data is in your data model as well as in the cluster we will go through some of these concepts in depth partitions partition keys composite partition keys and clustering columns here are two commands that can be executed on a relational database as well as on apache cassandra in a relational world these queries should work on a video table because of joins here common queries are title equals and added date is less than if you run this in a cassandra database the output appears as an invalid request this is because the partition keys absent from the equality clause of the search expression these commands fail because joins are not supported you've already seen the structure of the storage engine in lesson two this diagram demonstrates how the data is translated from the storage engine into a cql table when used programmatically that is when we select data from the table we see it in rows and columns just like we use in our application determination of the partition key is a critical step if you look at this diagram closely we have the primary key as the id there is just one column in the primary key the partition key is always the first value in the primary key this module will cover the basics of a cassandra data model upon completion of this module you will be able to explore the concept of denormalization and investigate collection counters and user defined data types upon completing this topic you will be able to explain how joints are used in relational data modeling relay the concept of denormalization used by cassandra you have already learned that the relational data model supports joins and apache cassandra supports denormalized tables here you see three tables named videos users and comments each table has unique information in it this is a typical relational example we're going to make all these work together by running a join query to see how they can be used in relation to each other join query joins two tables to work as one let's assume that you want to find some comments for a particular movie the first table titled videos store details of videos and the second table comment stores comments made by users on all the videos we need to find a particular entry which has all the data collected from both the tables let's say we are searching for a comment from the videos table how do we access the comment from the comment table if you just have the video with us this is possible using the join query they are often used by relational databases to organize data the two tables shown previously are joined and this is how the output looks like the tables are merged with the help of the video id specified in both the tables that is how relational databases work in the code the title is interstellar the select statement in the given transaction selects the columns specified from the output of the join command and the bear statement specifies which values to select from the specified column now this can be used to do a join on two tables and get comments for that one title this is how rdbms or relational databases are designed in the output of this transaction we will get the comments of the video titled interstellar that is how the relational databases were designed to be used having multiple tables and then using queries to merge and bring outputs you've just learned how the relational data model works with joins now it's time to see how cassandra works without joins let's look at these two tables closely they look similar but the primary key is different for both in the first table comments by video the video title is the partition key and the comment id is the clustering column and in the second table comments by user the user id is the partition key and the comment id is used as a clustering column these are the denormalized tables used in cassandra of the same joins that were used in the relational data models upon completing this topic you will be able to learn how collection columns and collection types are considered understand how to create and use udts learn about counters you will now learn about some additional data types supported by apache cassandra for efficiency these data types are collections counters and userdefined data types or udts these data types simplify table design optimize table functionality store data more efficiently and might even change the design of the table completely now you're going to learn about collections cassandra provides collection types to group and store data together in a single column take for example a relational database or a cassandra database to understand this concept consider that in a relational database you have to group multiple email addresses of a user meaning there will be a many to one joint relationship between a user table and an email table but cassandra awards joins between two tables by storing the user's email addresses in a collection column in the user table each collection specifies the data type of the data held the collection data structure allows you to store multiple values in each cell while retaining the value all collections are multivalued columns the maximum number of elements in a collection is 64 000 which in practice can be dozens or even hundreds collections are designed to store a small amount of data the maximum size of an element is 64 kb which in practice is much smaller than the number of elements in collection they are also retrieved in their entirety a collection column cannot be part of a primary key a partition key or clustering columns you cannot nest a collection inside of another collection if you want to nest a collection inside another then you have to use the keyword frozen more on this later there are three collection types in total set list and map respectively let's begin with the first collection type set it is a list that stores the typed collection of unique values it is stored on order but it gives back the data in a sorted order let's look at an example of set here a user's table is created with an id first name last name and a set of email addresses the data type is represented in angle brackets so in this example text data type is used after that the values are inserted into the table here there are two email addresses because it is common for a single user to have two email addresses list is like a set it groups and stores values in a single cell the difference between list and set is that in a list the values does not have to be unique and there can be duplicates lists are stored in a particular order and are accessed by an index here's an example of how the users table from the previous slide is altered to insert a list of frequent destinations first the table is added to list of text data types then the user cas one two three is updated and berlin london and paris are added to the list list uses the set command to enter the values for the respective collection type map is the last of our collection types it allows us to enter values which are in relation ship with one another in a key value format map is ordered by unique keys both the key and the value have a designated data type map uses the set command to enter the values for the map collection type let's look at an example here here the table users have again been altered with a map called to do with the data types of timestamp and text then the user cast 123 is updated and a map todo list was added the todo list is a set of dates and todo items in this case the user cas123 is going to create a database on january 1st load and test the database the next day and then a month after move the database to production here's another collection type frozen if you want to nest collections inside other collections you will have to use the frozen keyword without using frozen you cannot nest a collection inside another collection frozen allows us to serialize multiple components into a single value essentially frozen will turn that value into a blob if you want to update a single value in the collection you can't use the frozen keyword now it's time to explore user defined types popularly called udt udt allows you to group together similar fields of data that are both named and typed udts can include any supported data types including other collections and other udts multiple data fields can be attached each named and typed to a single column udts allow embedding of more complex data within a single column adding flexibility to your table and to your data model let's look at some examples here a udt is created which has street city zip code and a set of phone numbers now all these fields can make up a single address but in a relational world it is possible for several users to have the same address so a new udt or full name is created which has the user's first name and last name now let's look at how the udt is used in a table first a users table is created which includes name set of direct reports and a map of addresses in the name column the full name udt is used after that a set collection type with full name udt is used to create a direct reports column and finally address udt is used in a map collection in relational databases to find a query you will have to first join these two tables together but in cassandra's denormalization technique the address is embedded in a single cell of the user's table a counter is a data type where you can have a column that stores a 64bit signed integer they could be incremented or decremented and the values are changed using the update command counters need to have a specially dedicated table that can only have primary key and counter columns counters can lead to the duplicate data in multiple tables you can have more than one counter column in your table let's say we own a range that specializes in cows and we want to count the moo each cow makes we're going to create a table called moo count it has the column name cow name which is also the primary key of the table and the moo count is the counter column now to add to a moo count we will use the update command to update the mu counts table each time we hear a move in this example we updated the mu count to increment the value by 8 where the cow name is betsy upon completion of this module you will be able to learn about the different models in the data stacks modeling methodology upon completing this topic you will be able to understand the purpose of conceptual data model and learn about the cardinality attribute keys er model and relationship keys let's begin with the conceptual data model it is responsible for determining the attributes of all the objects in your domain and analyzing how they are related a conceptual model defines what a model should contain for example users comments videos ratings etc the purpose of a conceptual data model is to identify what type of data is contained in the model itself from here you can extrapolate the essential objects and constraints that the data types would additionally require it is important that you capture all different views of your model while conceptualizing your data model collaboration with other departments and experts is the key as a database architect it is critical that your solutions serve your business needs you will not always be an expert on the business domain and therefore it is important to interact with nontechnical people to get hold of the details both the technical and nontechnical members can share review and agree upon the conceptual data model business experts can then verify and ensure that the conceptual data model is correct and everyone is on the same page collaboration between different technical roles is also immensely critical apache cassandra requires database administrators and developers to work closely with each other the conceptual model is a way of bringing together the technical and nontechnical people to the same table the conceptual data model offers an abstract view of your domain it is generally independent of technology meaning that it does not specify what technology to use for example sql or nosql at this point it is not specific to any database either the term cardinality often refers to the relationship between two entities in a given data model or in other words how many instances of an entity are related to an instances of another entity cardinality tells how many times an entity can or must participate in a relationship other possibilities are one to n and one is to one here the relationship between the video entity and the actor entity is the character name in the video and the actors who played those characters here m and n are the videos and actors respectively the videos feature actors each video will have several actors and each actor can participate in several videos attribute types are the fields that store properties about an entity or relationship attributes or fields where we wish to define our entity for example we may want to store the title and description of each video actors of names which we can further break down into other attributes like first name and last name respectively key attributes uniquely identify an entity two videos can have the same name or description but the id is unique to a single video and the names are the unique key for actors let's look at the entity relationship model here we have an entity type the relationship types and attribute types and first we need to identify the entity types there are different entity types and videos these are things you're looking for to find information attribute types will look at actors and define their first and last names respectively the relationship type looks at the relationship between the video and the actor attributes can have multiple values and we call these not surprisingly multivalued attributes multivalued attributes store multiple values per attribute one can refer to these attributes with a double oval diagram around the attribute types for example one video can be an example for more than one genre now you will learn about the three relationship keys one to one one to many and many to many respectively in the onetoone relationship we use either of the participating entity keys in this example a movie can have one defined date for its first showing so this is an example of an onetoone key relationship one to many relationships uses the entity key on the many relationship side here we have one user but we can upload multiple videos manytomany relationship combines both the entity keys here many actors can be part of multiple videos weak entity types are entities that cannot exist without a corresponding strong entity on the other side for example if the video vanishes or disappears then the encoding will disappear too upon completing this topic you will be able to learn the concept of workflow and access patterns you will learn about application workflow which describes how users will navigate through the application it also helps in determining which queries will perform against an apache cassandra database every application has a particular workflow access patterns help us to determine how the data is accessed we also get to know what queries to run first for example when a user logs into a website what are they going to click what data are they going to use what data are they going to read how will a user go through a website application the workflow and access patterns must be thought through before building the application think about how the application should work and how users should use it for example you may want one query to run when a user logs into the site this query will perhaps find the user with a specified email address once the user is in the site you may want to run the second query to show the latest videos uploaded to the site you may want to run a third query to verify the user's login for them to access their account details is indeed working or not a fourth query can be used to find videos uploaded by users with a known id a fifth query can help you find a video with a specified video id these are just examples of popular queries one can build into an application upon completing this topic you will be able to understand the mapping rules explore a modeling approach to see how mapping rules are applied learn the concept of chaboko diagrams understand the logical and physical udt diagrams and learn about the four data modeling principles at this point we have covered conceptual data model and the application workflow next we will look at how to make a logical data model to do this we have to map our conceptual model to a logical model now you will go through some basic rules for query driven methodology the mapping rules ensure that a logical data model is correct each query has a corresponding table the tables are designed to allow queries to execute properly the tables will return the data in the correct order here are the five mapping rules to guide a query driven transition mapping rule 1 identify the entities and their relationships mapping rule 2 identify the equality search attributes mapping rule 3 identify the inequality search attributes mapping rule 4 identify the ordering attributes and mapping rule 5 identify the key attributes create a table schema from the conceptual data model for each query apply the mapping rules in order for this schema we're looking at videos by user upload therefore mapping rule 1 identify your entities and relationships in this case we have a table named videos uploaded by users there is a relationship between two uploads with users being one entity and the videos being the other mapping rule 2 identify your equality search attributes in this case clearly equality is the user id mapping rule 3 identify the inequality search attributes in this case it is the uploaded timestamp mapping rule 4 identify the ordering attribute mapping rule 5 identify the key attributes needed to guarantee uniqueness and minimality here video id guarantees uniqueness we have the conceptual data model on the left and the access patterns on the right it is important to remember that our application workflow had access patterns based on the queries which we wish to execute on our apache cassandra database this is an important part of the application workflow to get a logical data model we combine the results of the conceptual data model and the access patterns after combining them some mapping rules and patterns are applied this logical model is described using a chiboko diagram a diagram named after artem chaboco who devised his method of mapping to a logical data model chaboco diagrams graphical representations of cassandra database schema design they document the logical and physical data models respectively you already know your entities that is the video and the key attributes at the bottom query one is to find a video with a specified video id now if you look at the chiboko diagram we can see that q1 leads into a table which can satisfy this query we will get into that in more detail but first we have to understand some of the terminologies used here first and foremost this diagram looks very similar to an application workflow we have a queries which lead us into the tables we have the table diagrams which are similar to the tables we have in our cassandra database we have the query list at the bottom right as you know queries are the king and the queen while designing the cassandra data model and lastly we have our udt's which stand for userdefined data types these are a lot like cassandra tables but without a primary key and these can be nested into other cassandra tables on a logical level the cheboko diagram shows the column names and properties on a physical level cheboko shows the column data type in order to move from the logical tables to the physical tables we just need to add column data type followed by a bit of optimization if needed here is an example of how a table is broken down by column name data type and type of column as you can see in our table we have some abstract column names on the left and the corresponding descriptions on the right note that the notation on the left corresponds to the description on the right that is column 7 which is a list is in square brackets similarly column 8 a set is enclosed in curly brackets also a map like column 9 is in angular brackets and so on however the key thing to notice here is that all these columns are followed by their cql types which we have previously studied it is the physical types that makes the table physical we can now write our create table commands using these column names and their respective types into our database also note that column one is a partition key which is represented in the table as k similarly column 2 and 3 are clustering columns which are represented by c followed by an upward arrow or downward arrow representing the ascending order or descending order respectively a static column is specified by s secondary index column as idx and the counter column as a plus plus here you will learn about logical udt diagrams in detail udt is a data type and udts can be nested within one another for example there is an encoding column in the videos table this column represents our data is stored in the database there is more information about the data than just simple text hence it is encapsulated and it's stored in the encoding udt it has attributes of encoding height width and bit rates the same is true for actor name which encapsulates the first name and last name of the actor and stores it into the udt column if we want to have a single column to store both first and last name or have them stored differently it is important to remember that udt can be very helpful in such cases here is the same diagram but with the cql types explicitly specified is a representation of the physical layer udt diagrams please notice how real cql types have been specified for each column this is a classic example of a logical data model in the form of a detailed chiboko diagram for the killer video domain take a moment and carefully observe this diagram for all the concepts that we have covered so far we have queries which access our access patterns are tables with different kinds of data an application workflow that links these queries to the tables and are encoding udt defined here are the four main principles of cassandra data modeling know your data know your queries nest data and duplicate data we will look at each of these principles in more detail understanding the data is key to a successful implementation of a database a proper understanding of data provides the understanding to organize the data as well as we have seen space and time are often are primary concerns while data modeling in cassandra we sacrifice space in order to reduce time hence organization of data is of utmost importance the conceptual data model provides us with the data capturing information once we define what is to be stored we move on to create a database this not only helps in successful organization but also helps in preserving the properties of the data that is required part of understanding the data requires understanding a primary key as well to recall primary keys take care of two essential things uniqueness and satisfying queries entity and relationship keys affect the table primary keys primary key uniquely identifies a row entity and a relationship let's look at the following tables for example the video tables as video id as its primary key here the primary key makes no attempt at organizing the data so as to satisfy queries at later stages on the other hand the table videos by user as a primary key along with clustering columns which specify ordering in order to help with queries that result in returning the results in a specified order the relationship cardinalities as well as the data cardinality are very important when it comes to data modeling data cardinality helps in selecting an efficient partition key whereas the relationship cardinality helps in satisfying the queries queries are the center of data modeling in cassandra and captured by the application workflow models each entity is created with queries in mind in cassandra it's always better to reduce the number of partitions that are to be read for a query result table schema also changes if the queries are changed to achieve this it is extremely important to clearly understand the kind of data required out of the read operations single partition per query is the most efficient access pattern entities in cassandra are designed in such a way that queries can generate the results with the minimum number of reads ideally the data should be organized in such a way that each query can retrieve the results from a single partitioned search partition can be single row or multirow respectively in this example if we want to query for a particular partition we can give the partition key then the node having the partition is determined and the data is retrieved by the node and returned to us no additional filtering or searching the entire cluster is required however there are cases when multiple partitions have to be accessed to retrieve the results and although this is a less efficient way of doing things in cassandra it is acceptable to some extent as long as these kind of queries are limited in number however the strict antipattern that has to be avoided at all costs is asking the database to perform a linear search through all the partitions of all the tables to fetch the result for a particular query since most of the data is stored in distributed nodes it is highly inefficient to go through all the data stored at multiple locations and it defeats the whole purpose of having a horizontally scaled database after completing this topic you will be able to learn the concept of data nesting and explore the three ways of nesting data until now we've talked about creating a logical data model by combining the results of a conceptual data model and accessing the pattern of application workflow by applying mapping rules and patterns let's now create a logical data model containing tables for each query capturing entities and relationships from the conceptual model the working rule with apache cassandra is to group as much data as possible on disk this grouping of data is referred to as nesting of data and is the main data modeling technique for a cassandra data model nesting helps in organizing multiple data entities into a single partition and it also helps support partition per query during data access in cassandra queries are the king and the queen schemas are often driven through these application queries there are three main ways of nesting data in cassandra these are clustering columns collection columns and user defined type columns clustering columns are the primary data nesting mechanisms look at the actors by video table on the right we apply a query to filter by video id but we nest the actors by their actor name and the character name using clustering columns the partition key identifies the entity that other entities nest into while the clustering columns identify the nested entities when a table has multiple clustering column the data is stored in a nested sorted order multiple clustering columns implement multilevel nesting in the given example actors are identified by videos using clustering columns udts are a secondary mechanism for nesting data usually this occurs when there is a onetoone relationship you also have lists of maps or your choice of userdefined types within a single column working with udts is much easier than working with multiple collection columns for example we turn videos into a video type udt with id title description and other parameters then we put all those videos in a collection column in the videos by user table table videos by user tests all videos as a collection in the videos column with the video underscore type type partition per query and data nesting may result in data duplication in relational data models the tables are joined as we read the data in cassandra however instead of wasting time on reads the data is taken and returned to the client application for example we have three tables that represent the video's object videos by actor videos by genre and videos by tag this duplicates data across tables partitions and rows that means every time we want to store a new video we have to write the record into each one of these tables to keep them in sync upon completing this topic you will be able to learn how to use data types in a table understand the ways to load data into the table and explore some of the features of cql copy here's our data modeling methodology again we've created our conceptual model and our application workflow we've also applied the mapping rules created a logical data model and now it's time for us to create our physical data model to make a physical model add data types to each column of a logical model physical models also optimize efficiency and performance ensuring that the partition sizes won't grow too large and will return the queries quickly here we have a logical version of commons by user and a physical version of comments by user as you can see the physical version of cql data types like user id is uuid posted timestamp is of time stamp data type video id is of time uuid data type comment title and type is of text respectively tags is a set of text and lastly preview thumbnails is defined as a map now we will use these data types in creating a table we already saw how to use the data types in the table in theory to better understand the physical data model it's time to demonstrate it with an example creating a table could cause a lot of problems if the tables aren't configured correctly the first challenge is creating a partition that is way too large there are many other challenges that are related to optimization now we have data types which we have added in the previous topic now let's create a table using the cql create table statement in this example we've created a table of comments by user the field in the table are everything that we saw on the previous slide in the physical version of the table the user id is uuid and the posted timestamp is a timestamp and all the other data types that we saw earlier once the table is created the user may want to insert data into it there can be two types of users a new user and an existing user if it's a new application then users will use your site and insert the data for you but if you're migrating from an existing database then it may take a few ways to get the data into the table first is the copy command which can leverage cql copy in cql and has a value of inserting approximately 2 million values into the database second is the ss table loader which is used when migrating from one cluster to another and lastly there is spark for data loading which has a lot of options but does not require a working knowledge of spark here are some basic sql copy features copy 2 exports data from a table to a csv file copy from imports data to a table from a csv file in sql copy the process verifies the primary key and updates the existing records accordingly if the header is false it specifies that the fields are imported in a deterministic order when column names are specified in the table then the fields are imported in that order if any field is missing or empty it is set to null source cannot have more fields than the target table but surely can have less number of fields than the target table so let's look at an example of basic data loading with cql copy this is a good method to use and to get started we will import the data in column 1 column 2 and column 3 from the csv file table 1 data.csv when header is equal to true welcome to module 4 cassandra practical application we're going to help you start your own cassandra as a service db on a cloud provider of your choice cassandra as a service is named astrodb and is provided by datastax upon completion of this module you will be able to create your own astrodb instance and perform the workshops upon completing this lesson you will be able to perform the workshop related to this course earlier we learnt about different apis to connect to our astrodb like the document api graphql api and the rest api all of these apis are part of a project called stargate stargate is built into astrodb for your convenience if you would like to learn more about what stargate is how it works how you can get involved in the open source project please go to stargate.io for more information for an indepth understanding of how it all works and how you can utilize target your advantage view the workshop video on youtube created by the data stacks developer advocates scan the qr code to go to the video or copy the link and paste it in your preferred browser another option for your astra db is to build a to do app using the rest api python and node.js respectively this is an example react to do application built using a datastax astra fee free tier database for an indepth understanding of how it all works view the workshop video on youtube created by the datastax developer advocates scan the qr code to go to the video or copy the link and paste it in your preferred browser in this example we discover a fully reactive version of the spring pet clinic application using spring web flux this example is considered an intermediate level application for developers to build on astra db for an indepth understanding of how it all works view the workshop video on youtube created by the data stacks developer advocates scan the qr code to go to the video or copy the link and paste it in your preferred browser earlier in this course we carried out some basic data modeling to understand how cassandra can best be built and deployed if you wanted to do a deeper dive into a more advanced data modeling for your app we also have a workshop available for you to view on youtube from the data stacks developer advocates once again scan the qr code to go to the video or copy the link and paste it in your preferred browser throughout this course we had mentioned using spark for specific functions on astra db including data loading and connecting we also wanted to provide you with a workshop for you to view if you would like to get a more indepth understanding of how it all works and how you can utilize spark to your advantage on youtube again created by the developer advocates of data stacks scan the qr code to go to the video or feel free to copy the link and paste it in your preferred browser
