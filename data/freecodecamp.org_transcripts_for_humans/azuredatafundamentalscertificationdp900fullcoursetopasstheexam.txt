With timestamps:

00:00 - hey this is andrew brown your cloud
00:01 - instructor at exam pro and i'm bringing
00:03 - you another complete study course and
00:04 - this time it's the azure data
00:05 - fundamentals certification made
00:07 - available to you on free code camp so
00:09 - this course is designed to help you pass
00:11 - exam and achieve microsoft issued
00:12 - certification and the way we're going to
00:14 - do that is by going through lots of
00:15 - lecture content getting some hands-on
00:17 - experience with follow-alongs and on the
00:19 - daily exam i have these really great
00:20 - cheat sheets that are going to help you
00:22 - uh pass for sure so the great thing is
00:24 - at the end of it you'll be able to get
00:26 - that certification and show that on your
00:28 - resume or linkedin that you have the
00:29 - azure knowledge so you can go get that
00:31 - data job or that promotion you've been
00:33 - looking for just to introduce myself i'm
00:36 - previously the cto of multiple edtech
00:37 - companies 15 years experience with five
00:39 - years specializing in the cloud i'm an
00:41 - aws community hero and i've published
00:43 - many many
00:44 - free cloud certification courses and i
00:47 - love star trek and coconut water i just
00:49 - want to take a moment here to tell you
00:50 - that this video cloud certification
00:52 - course is made possible by viewers like
00:54 - you and i appreciate your support and
00:56 - thank you if you want to help support
00:59 - more free cloud courses just like this
01:01 - one the best ways to buy your extra
01:02 - study materials at exam pro dot co
01:04 - forward slash dp hyphen 900 to get study
01:07 - notes flash cards quizlets deliverable
01:09 - cheat sheets practice exams
01:11 - you can ask questions and get some
01:13 - support from our cloud engineers and
01:15 - just so you know if you want to keep up
01:16 - to date with upcoming courses you can
01:18 - follow me on twitter at andrew brown
01:20 - that's uh and what you can do is once
01:22 - you do that you can tell me you passed
01:24 - the exam you can tell me what you'd like
01:25 - to see next because a lot of times the
01:27 - next course i make is based on the
01:28 - feedback i get from you so let's jump
01:30 - into the course now
01:34 - [Music]
01:37 - hey this is andrew brown from exam pro
01:39 - and we are at the start of our journey
01:40 - asking the most important question first
01:42 - which is what is the dp900 so the azure
01:44 - data fundamental certification is for
01:47 - those seeking a data related role such
01:48 - as data analysts data engineer or data
01:51 - scientist and the certification will
01:53 - demonstrate a person can define and
01:54 - understand coordinated concepts hadoop
01:56 - workloads apache spark workloads mssql
01:59 - databases nosql databases data lakes
02:01 - data warehouses elts and big data
02:04 - analytics and more the certification is
02:06 - generally referred to by its course code
02:08 - the dp900 and it's the natural path for
02:10 - the azure data engineer or the azure
02:13 - data analyst certifications this is an
02:16 - easy course to pass and great for those
02:18 - new to cloud or data related technology
02:20 - so let's look at our big old road map
02:23 - here and what i'm going to do is pull
02:25 - out my laser pointer so you can see
02:26 - where i am and we'll get some lines in
02:28 - here because i want to show you some of
02:30 - the paths you can take with the
02:31 - certification so we're over here with
02:33 - the dp900 and this is at the
02:35 - fundamentals level right and so a lot of
02:37 - times what people will do is they will
02:39 - have the az-900 oh they'll take that
02:41 - first if they're 100 new to cloud so i
02:43 - see a lot of people they start here and
02:45 - if you're if you've never used azure
02:47 - before it's a great starting point and
02:49 - then they'll move on to the dp900 or
02:51 - sometimes what they'll do is they'll
02:52 - move on to the ai 900 and then onto the
02:55 - db dp900 or they'll take the dp900 and
02:58 - then the ai fundamentals but the reason
03:00 - you want to take this data fundamentals
03:02 - certification is primarily because you
03:05 - you're very likely want to take the data
03:07 - engineer certification or the data
03:09 - analyst certification which is at the
03:10 - associate track so this one it's
03:12 - basically uh the dp900 but actually
03:15 - knowing how to implement everything and
03:18 - then the data analyst is really focused
03:20 - on power bi so just knowing how to use
03:22 - uh power bi uh to uh to its maximum
03:25 - extent okay
03:27 - now a lot of people that are going for
03:29 - the data scientist or ai engineer track
03:31 - will take the dp900 because you do need
03:33 - foundational knowledge about data to
03:36 - understand these roles
03:38 - and so you know it's just if you are
03:40 - going for the data scientist you
03:42 - probably want to add the dp900 to your
03:44 - track but if you've taken the az900 and
03:46 - the az-104 you might want to skip over
03:49 - this it's just up to you but yeah
03:51 - there's a lot of ways that you can go
03:52 - about this okay
03:54 - and we'll just move on uh here so how
03:56 - long to study to pass the dp900 so if
03:58 - you have one year experience with azure
04:00 - you're looking at five hours of study if
04:02 - you've passed the az 900 you're looking
04:04 - at 10 hours if you're new to cloud
04:06 - you're looking at 15 hours of study for
04:08 - new people i'm saying 30 minutes a day
04:11 - for 14 days the course content itself is
04:14 - not very long but the thing is you have
04:15 - to factor in that you have to do um
04:18 - you have to do practice exams and you
04:20 - also uh just have to put that knowledge
04:22 - into practice by using the console so
04:25 - even though you know the course
04:26 - content's not long there's additional
04:28 - time you have to do there where would
04:30 - you take this exam well you can do it in
04:32 - in-person test center or online from the
04:33 - convenience of your own home so there
04:35 - are two options you got psi and pearson
04:38 - vue and uh they both actually do in
04:40 - person and online and so just so you
04:43 - know if you ever heard the term proctor
04:45 - a lot of times we talk about online
04:46 - exams being proctored exams because
04:48 - there's a person or supervisor that's
04:50 - watching you take the exam online okay
04:53 - so what does it take to pass this exam
04:55 - well you got to watch the lecture videos
04:57 - you got to do hands-ons and follow
04:59 - alongs and it helps to do paid practice
05:02 - exams that simulate the real exam this
05:04 - is pretty easy certification so you
05:06 - could probably get away with um
05:08 - [Music]
05:13 - okay so what i did was i just googled
05:14 - dp900 exam guide and so i made it to the
05:17 - microsoft site if you scroll on down
05:18 - you're looking for that skills measure
05:20 - download that pdf open it up it will be
05:22 - over here now it's very common to see
05:24 - this red text here azure loves to update
05:26 - their exams even once a month they'll do
05:28 - this to me and people ask me are your
05:30 - exams out to date i'm like no they're
05:31 - just making my new changes if they do
05:33 - make major changes what they'll do is
05:35 - actually release a new version they'll
05:36 - call it the dp901 that's when you should
05:39 - be concerned about changes uh but uh
05:42 - yeah they do it frequently so what
05:43 - you'll do is if you get if you see red
05:45 - text you gotta scroll down to the real
05:46 - section because this is the old one and
05:48 - this is the new one where they make
05:49 - minor changes okay
05:51 - and what we'll do is work our way down
05:53 - here and and take a look here so
05:55 - describe core data concepts so batch and
05:58 - streaming relational data we have data
06:00 - linux concepts so visualizations bi
06:03 - types of bar like types of charts
06:05 - they're talking specifically about power
06:07 - bi because there's a lot of different
06:08 - kinds of visualizations describe
06:10 - analytic techniques so descriptive
06:11 - diagnostic predictive prescription
06:13 - cognitive most places don't describe
06:16 - cognitive so they add that additional
06:17 - one there elt and etl so um uh extract
06:22 - elt is more common for cloud so that's
06:24 - the one you really want to understand
06:26 - describe concepts of data processing
06:29 - onto how to work with relational data so
06:31 - describe what a relational workload is
06:32 - describe the structures within a
06:34 - database if you've ever worked with any
06:35 - kind of database you already know them
06:36 - tables indexes views columns et cetera
06:39 - we have described relational data
06:41 - services so they do p-a-s-i-a-s and
06:43 - s-a-s they're specifically talking about
06:46 - in that comparison you'll see that like
06:48 - in the az 900 uh those three though i
06:51 - think they might have removed them as of
06:52 - recent but um they're specifically
06:55 - talking about the azure sql family so
06:57 - down below here you have azure sql and
06:59 - underneath it has a bunch of
07:01 - variants so like sql database sql manage
07:04 - instance and virtual machines and these
07:05 - sit
07:06 - in the pas and iis and you have to know
07:08 - that okay
07:10 - describe azure synapse analytics
07:11 - describe database for postgres mario by
07:14 - sql that's the open source sql databases
07:17 - uh so then we have relational data so uh
07:20 - provisioning deployment of relational
07:21 - data
07:22 - deployment with the portal azure
07:24 - resource templates powershell cli you
07:27 - know what i don't ever see these on the
07:28 - exam you know but um you know they they
07:31 - have this in here okay
07:33 - identify security components now they
07:34 - say firewall they're actually talking
07:36 - about when you have a database because
07:38 - there's actually azure firewall but then
07:40 - there's um a a server firewall built
07:43 - into azure sql and that's what they're
07:45 - talking about there authentication like
07:46 - how to connect because there's a few
07:48 - different ways you can do that
07:49 - connectivity from on-prem to azure vnets
07:52 - etc identity query tools so azure data
07:54 - studio sql studio management sql sql cmd
07:59 - utilities and things like that describe
08:00 - query techniques for using uh sql so
08:03 - compare ddls with dmls there's actually
08:06 - a lot more types of um
08:08 - data language files for sql so we do
08:11 - them all just because it's you know it's
08:14 - the proper way to do it but they only
08:15 - care about these two
08:16 - query relational data in sql database uh
08:19 - azure disk postgres azure database for
08:21 - mysql we'll go on down to this section
08:23 - here non-relational data workloads so
08:25 - describe the non-relational data
08:26 - describe non-relational and nosql data
08:28 - recommend correct data stored determine
08:31 - when to use non-relational data and
08:32 - they're all talking about um
08:35 - because like non-relational data is
08:36 - mostly cosmodb a cosmodb like has a
08:39 - bunch of different sql engines there so
08:40 - that's going to help you understand that
08:41 - like graph and document key value store
08:44 - things like that describe non-relational
08:46 - data so we have table blob files cosmodb
08:50 - identify basic management tasks for
08:52 - non-relational data so provisioning
08:54 - deployment of non-relational data
08:55 - services describe method of deployment
08:57 - in uh azure portal et cetera et cetera
09:00 - again i don't see these a lot on the
09:02 - exam so i don't know why this is in here
09:04 - identify data security components um so
09:07 - it's the same thing as before it's just
09:08 - for non-relational data basic
09:10 - connectivity issues v-nets etc identify
09:12 - management tools for non-relational data
09:15 - um
09:16 - describe analytical workloads so
09:18 - transactional data the difference
09:19 - between transactional analytical and
09:21 - we're talking about olap and oltp okay
09:25 - difference between batch and real time
09:27 - uh warehouses data warehouse solutions
09:30 - describe modern data warehouses so here
09:32 - we're talking about data bricks hadoop
09:34 - systems
09:35 - synapses is kind of like a data lake
09:37 - house and then actually azure data lake
09:40 - and it's storage medium
09:42 - down below we have data ingestion
09:44 - so loading data azure data factory um hd
09:47 - insights databricks etc and then down
09:50 - with below we have a whole section on
09:51 - power bi what i'm surprised is they
09:53 - don't have much like on azure um
09:56 - streaming analytics and event hub
09:58 - because a lot of these have to consume
09:59 - from there so that's something i think
10:01 - that should be on the example they don't
10:02 - have it in here but yeah it's uh it's
10:04 - not a super hard exam it's mostly just
10:06 - describe describe identify see so you're
10:08 - all going to be in great shape and i
10:09 - hope that helps you out and we're on to
10:11 - the actual course now
10:12 - [Music]
10:16 - hey this is andrew brown from exam pro
10:18 - and we are taking a quick look at all
10:20 - the core data related azure services
10:21 - that we are likely to encounter through
10:23 - this course so let's get to it the first
10:25 - starting with azure storage accounts and
10:26 - this is an umbrella service for various
10:28 - storage types such as tables files and
10:29 - blobs we have azure blob storage and
10:32 - this is a
10:33 - data store that stores things as objects
10:36 - instead of files and the advantage here
10:37 - is that you get distributed storage
10:39 - these objects can span multiple machines
10:41 - for unstructured data you have azure
10:43 - tables which is a key value no school
10:45 - data store more like a database but it's
10:47 - under azure storage accounts and it's
10:49 - intended for its simpler projects you
10:51 - have azure files and this is a managed
10:53 - file share for nfs or smb so if you need
10:56 - a file share or file system that you
10:57 - need to mount to multiple virtual
10:59 - machines or workstation
11:00 - this is what you would use you have
11:02 - azure storage explorer this is a
11:05 - standalone application you download to
11:06 - your windows linux or mac machine that
11:09 - easily allows you to explore
11:11 - the various services above then you have
11:14 - azure synapse
11:15 - analytics this is a data warehouse and
11:17 - unified analytics platform the service
11:19 - used to be called something like azure
11:21 - warehouse but they added analytics on
11:23 - top of it kind of making it into a lake
11:25 - house service and so that's what it is
11:27 - now we have cosmodb this is a fully
11:30 - managed nosql database service that can
11:32 - host various nosql engines such as azure
11:35 - tables documents key value and graph
11:37 - when you use cosmodb it's going to have
11:39 - a core mode and that pretty much is its
11:42 - documents engine so a lot of times when
11:44 - we talk about cosmodb we just think of
11:46 - it as a documents
11:48 - documents database but it can actually
11:50 - have a variety underneath you have azure
11:53 - data lake store generation two we won't
11:55 - talk about gen 1 because it's just not
11:57 - really in use anymore but this is a
11:58 - centralized data repository for big data
12:00 - blob storage designed for vast amounts
12:02 - of data it actually is just azure blob
12:04 - storage with an additional layer of
12:06 - management you have azure data analytics
12:09 - this is a big data as a service you can
12:11 - write usql to return data from your
12:14 - azure data lake then you have azure data
12:16 - box which isn't really covering the exam
12:18 - but i'm including here because i think
12:20 - it's a great addition so you can import
12:22 - and export terabytes of data via hard
12:24 - drive you mail into the azure data
12:26 - center onto our next page here we have
12:29 - sql server for azure virtual machines
12:31 - this is when you need an sql server
12:33 - where you're migrating an existing sql
12:35 - from your on-premise data center onto
12:37 - azure but you can't afford to make any
12:39 - changes so you're literally taking the
12:41 - vm and lifting and then shifting it onto
12:43 - azure but you get to have access to the
12:46 - virtual machine underneath so you can
12:48 - control the os access
12:50 - layer and also if you already have an
12:53 - existing license it's a great solution
12:54 - for that as well if you are doing a lift
12:56 - and shift but you don't need to manage
12:58 - the virtual machine and you want azure
13:00 - to do all the work for you you have sql
13:02 - managed instances then you have azure
13:04 - sql which is the fully managed mssql
13:07 - database then you have azure databases
13:09 - for mariodb postgres and mysql you have
13:12 - azure cache for redis now this is an
13:15 - in-memory data store for returning data
13:16 - extremely fast but is also extremely
13:18 - volatile and this isn't covered on the
13:21 - exam but i like to include it because i
13:23 - think it's just one of the data services
13:25 - that's important you have microsoft
13:27 - office 365 sharepoint not really covered
13:29 - on the exam but you will hear it
13:31 - mentioned throughout
13:33 - the course content and you know i think
13:35 - that if you haven't had exposure to you
13:37 - should know what it is it is a shared
13:38 - file system for organizations the
13:40 - company owns all the files and applies
13:42 - fine-grained role-based access controls
13:44 - you have azure data bricks this is a
13:46 - third-party provider partnered with
13:48 - azure specializing in
13:50 - apache spark to provide very fast etl
13:52 - jobs as well as ml and streaming
13:55 - you have microsoft power bi this is a
13:58 - business intelligence tool used to
13:59 - create dashboards and interactive
14:01 - reports to empower business decisions we
14:03 - have hdinsights this is a fully managed
14:05 - hadoop system that can run many open
14:07 - source big data engines for doing data
14:09 - transformations for streaming etl elt
14:13 - we have azure data studio this is an ide
14:16 - that looks very much like visual studio
14:19 - code but designed around data related
14:20 - tasks across platforms similar to sss
14:24 - ssis
14:26 - but broader data workloads you have
14:28 - azure data factory a managed etl elt
14:31 - pipeline builder easily build
14:33 - transformation pipelines via a web
14:34 - interface within azure and then you have
14:37 - sql
14:38 - server integration services ssis it's a
14:41 - standalone windows app to prepare data
14:42 - for sql workloads via transformation
14:44 - pipelines there's probably a bunch of
14:46 - other little services or tools that we
14:49 - don't have in this list but don't worry
14:50 - we'll cover them throughout the course
14:52 - just remember these ones that we went
14:53 - over here today
14:59 - hey this is andrew brown from exam pro
15:00 - and we're taking a look at the types of
15:02 - cloud computing for azure data related
15:04 - services starting at the top of our
15:06 - pyramid is software as a service and
15:08 - it's a product that is run and managed
15:09 - by a service provider so you do not
15:11 - worry about how the service is
15:12 - maintained it just works and remains
15:14 - available and this is specifically
15:15 - designed for customers so uh it's not
15:18 - particularly azure services but it'll be
15:20 - like microsoft based services like power
15:22 - bi or the office 365 suite is going to
15:26 - be software as a service going down to
15:28 - platform as a service this focuses on
15:30 - deployment and management of your apps
15:32 - so you do not worry about provisioning
15:34 - and configuring or understanding the
15:35 - hardware or os layer and this is
15:38 - specifically for developers so we would
15:39 - put hdinsights azure sql cosmodb
15:43 - managed sql
15:45 - all right and at the bottom we have
15:47 - infrastructure as a service these are
15:48 - the basic building blocks for cloud it
15:50 - it provides access to networking
15:52 - features computers and data storage
15:53 - space you do not worry about the it
15:56 - staff data centers and hardware and
15:58 - underneath here we would have this would
15:59 - be for admins but we'd have azure disks
16:01 - virtual machines sql vms and honestly
16:05 - you know like when you look at aws and
16:07 - azure
16:08 - these kind of
16:09 - categories are defined a little bit
16:11 - differently so
16:12 - you know like manage sql i would
16:14 - probably put that infrastructure as a
16:15 - service but
16:16 - azure says that it goes into the mid
16:18 - tier i really want to pull up a
16:20 - particular document here that i think is
16:21 - important
16:22 - because this is all about the azure sql
16:24 - family and they're specifically
16:26 - categorizing these in particular so you
16:28 - go down below here and when we're
16:30 - looking i have them on here but when you
16:32 - look here i have sql vm down below so
16:35 - that would be considered infrastructure
16:36 - as a service you have managed sql where
16:38 - they put in the middle but they
16:40 - categorize it as platform as a service
16:42 - and then you have it's covered up here
16:45 - we have azure sql database which is
16:46 - platform as a service
16:48 - i'm showing you this because they might
16:50 - ask you this question on the exam uh and
16:52 - so i just wanted to point that out to
16:54 - you there but there you go
16:56 - [Music]
17:00 - okay so let's take a quick look at three
17:02 - azure data roles that azure wants you to
17:05 - know about specifically related to data
17:07 - services when i say roles here i don't
17:09 - mean like azure permissions i actually
17:12 - mean like jobs that people would do
17:14 - within azure
17:16 - and let's just take a look at it here so
17:18 - the first one is database administrator
17:20 - this is somebody that would configure
17:21 - and maintain a database such as azure
17:24 - data services or sql servers and they
17:26 - would be responsible for database
17:28 - management management security or
17:30 - granting users access backups monitoring
17:32 - performance and common tools that they
17:34 - would use would be azure data studio sql
17:37 - management studio azure portal the cli
17:40 - the next role would be data engineer and
17:42 - that would be to design and implement
17:44 - data tasks related to the transfer and
17:46 - storage of big data responsibilities
17:48 - here would be database pipelines in
17:49 - process data ingestion storage prepare
17:52 - data for analytics prepare data for
17:54 - analytics processing and common tools
17:57 - that they would use would be azure
17:58 - synapses studio sql azure cli and the
18:02 - last role here we have is data analyst
18:05 - so this is analyzes business data to
18:07 - reveal important information so you have
18:09 - provide insights into data visual
18:11 - reporting modeling data for analysis
18:14 - combines data for visualization analysis
18:16 - common tools here are power bi desktop
18:19 - power bi portal power bi service and
18:22 - power bi report builder so i just want
18:25 - you to know that there's definitely a
18:26 - lot more roles than just these three
18:28 - here but this kind of helps you narrow
18:30 - down what this entire dp900 is focused
18:33 - on which are these three kind of rules
18:35 - here but what we'll do is we'll jump
18:37 - into these common tools and just talk
18:38 - about them a little bit more in detail
18:40 - uh next here okay
18:42 - [Music]
18:46 - okay so we're taking a look here at
18:48 - database administrator common tools the
18:50 - first being azure data studios this
18:52 - allows you to connect to azure sql
18:54 - data warehouses post postgres sql sql
18:57 - servers big data cluster on premise i
18:59 - say azure sql data warehouse it must be
19:01 - azure synopsis analytics i probably just
19:04 - wrote that incorrectly there but various
19:06 - libraries and extensions along with
19:07 - automation tools a graphical interface
19:09 - for managing on-premise and cloud-based
19:11 - data services runs on windows max and
19:14 - linux
19:15 - possible replacement for ssms but still
19:18 - lacks some of those features if you
19:20 - launch the service it looks a lot like
19:22 - visual studio code because it probably
19:24 - is but it's specifically for data
19:26 - related tasks so if you're used to
19:27 - visual studio code you're going to be at
19:29 - home with the service you have sql
19:31 - server management studio ssms and it's
19:34 - an automation tooling for running sql
19:36 - commands or common database operations
19:37 - it has a graphical interface for
19:39 - managing on-premise and cloud-based data
19:41 - services but it only runs on windows so
19:44 - if you're on mac or linux you're going
19:45 - to be using azure data studio
19:48 - and if you're on windows you might just
19:49 - have both of these installed because
19:51 - there's just some things you can do in
19:52 - ssms that are just a lot easier than
19:54 - azure data studio but it's more mature
19:57 - than azure data studio so you know it's
19:59 - just going to be the features are going
20:00 - to be a lot more richer in partic in
20:02 - particular for sql you have azure portal
20:05 - and cli so here you can manage sql
20:07 - database configuration so you can create
20:08 - delete resize the number of cores uh you
20:11 - can manage and provision other data
20:14 - azure data services automate the
20:15 - creating updating or modifying resources
20:17 - via the azure resource manager templates
20:20 - which is infrastructure as code so those
20:22 - are the three
20:24 - major ones that a database administrator
20:26 - is going to be working with
20:27 - [Music]
20:31 - now let's take a look at data
20:32 - engineering common tools so at the top
20:34 - we have azure synapses studio so you
20:36 - know azure synapses analytics when you
20:38 - click into it there you launch a studio
20:40 - and this allows you to manage things
20:42 - like your data factories
20:44 - your warehouses sql pools spark pools
20:48 - things like that you're gonna have to
20:49 - know really really no sql there's tsql
20:52 - usql
20:54 - synapsis sql there's all sorts of sqls
20:58 - within azure so it's definitely
20:59 - something you want to learn for the
21:00 - azure cli you'll have to be able to use
21:03 - it to then
21:05 - execute sql commands because once you
21:06 - connect to an sql server via the cli
21:10 - you're just going to be writing sql from
21:11 - there
21:12 - i added these they weren't in the common
21:14 - tools list prior but i just added them
21:16 - now because i thought they were useful
21:17 - so hd insights which would have for
21:19 - streaming data via apache kafka or
21:22 - apache spark or applying etl jobs via
21:24 - hive pig and apache spark as your data
21:26 - bricks also because here you could be
21:29 - creating an apache spark
21:31 - cluster and using that to do etls or
21:33 - streaming jobs to your data warehouses
21:34 - your data lakes and of course you'd be
21:36 - working with blob storage and data likes
21:39 - as well so they should be on the list
21:40 - here
21:41 - but just there you go
21:43 - [Music]
21:47 - all right taking a look here at data
21:48 - analysts for common tools we have power
21:50 - bi desktop this is a standalone
21:51 - application for data visualization you
21:53 - can do data modeling here connect to
21:55 - many data sources and create interactive
21:57 - reports then you have power bi portal or
22:00 - also known as the power bi service and
22:02 - really this is just intended for
22:03 - creating interactive dashboards you can
22:04 - definitely do other things here but this
22:06 - is what i know it for then you have
22:08 - power bi builder report
22:10 - or report builder this is another
22:12 - standalone application and this allows
22:14 - you to create paginated reports which
22:15 - are just printable reports definitely
22:17 - there are more tools
22:19 - than just these three for uh data
22:21 - analysts but this is what azure wants
22:23 - you to know uh so there you go
22:25 - [Music]
22:29 - hey this is andrew brown from exam pro
22:31 - and we are looking at the data overview
22:34 - so the idea here is that we're going to
22:36 - be covering a lot of fundamental
22:38 - not necessarily as your specific data
22:40 - related knowledge that you need to know
22:42 - to really understand how to use azure
22:44 - data related services uh so i'll just
22:46 - give you a quick overview here and then
22:47 - we'll dive deeper into all these things
22:49 - so the first thing is data so that's
22:50 - units of information you have data
22:52 - documents these are types of abstract
22:54 - groupings of data you have data sets
22:56 - these are unstructured logical grouping
22:57 - of data when you structure your data now
22:59 - it's called structured data and then you
23:02 - have data types these are single units
23:04 - of data that are intended to be used in
23:06 - a particular way then you have a bunch
23:08 - of loose concepts so you have batch and
23:10 - streaming so this is how do we move our
23:11 - data around we have relational
23:13 - non-relational
23:14 - databases or data so how do we access
23:17 - query and search our data you have data
23:19 - modeling how do we prepare and design
23:20 - our data schema versus schemas how do we
23:23 - structure our data for search data
23:25 - integrity and data corruption how do we
23:27 - trust our data normalization and
23:29 - denormalization how do we trade
23:31 - quality versus speed and i'm sure we
23:33 - cover a lot more than just this list i
23:35 - just didn't feel like
23:36 - being exhausted here and put every
23:38 - little thing here so many things that
23:40 - have the word data in it but let's jump
23:42 - into it
23:43 - [Music]
23:46 - so the first question we should be
23:48 - asking ourselves is what is data so data
23:51 - is units of information that could be in
23:53 - the form of numbers text machine code
23:55 - images videos audio or even in a
23:58 - physical form like handwriting maybe if
24:01 - you're in the future it's crystals i
24:02 - don't know and so just some images here
24:05 - or
24:06 - examples of graphics so here we have an
24:08 - image here is a bunch of binary code or
24:11 - machine code
24:12 - we have a book
24:14 - here we have uh
24:16 - like audio so we have like audio
24:18 - spectrum here uh and then you have
24:20 - mathematical formulas so i'm sure at
24:22 - this point you know what uh data is but
24:24 - just in case uh you know you need to
24:26 - just broaden your your thoughts of what
24:27 - data is it really
24:29 - could be everything including physical
24:30 - stuff
24:35 - so what is a data document a data
24:38 - document defines the collective form in
24:40 - which data exists so common types of
24:42 - data documents would be data sets which
24:45 - is a logical grouping of data databases
24:47 - which is structured data that can be
24:48 - quickly accessed and searched data
24:51 - stores this is unstructured or
24:52 - semi-structured data
24:54 - for housing data data warehouses
24:57 - structured or semi-structured data for
24:59 - creating reports and analytics notebooks
25:01 - data that is arranged in pages and
25:03 - designed for easy consumption so just to
25:06 - give you some examples if we're looking
25:07 - at data sets we might talk about the
25:09 - mnist data set
25:11 - for azure sql that would be a database
25:14 - for a data store such as a data lake we
25:16 - have azure data lake
25:18 - for a data warehouse we have azure
25:20 - synapsis analytics and for notebooks we
25:23 - might talk about jupyter notebooks but
25:25 - this could also include you know an
25:26 - actual handwritten notebook so there you
25:29 - go
25:29 - [Music]
25:34 - all right so what is a data set well a
25:36 - data set is a logical grouping of units
25:38 - of data that are generally closely
25:39 - related or share the same data structure
25:42 - and so i say data structure there but i
25:44 - just want you to know that just because
25:45 - something has a data structure doesn't
25:46 - mean it's structured data it can be
25:48 - semi-structured like json objects or xml
25:51 - files
25:52 - but generally data sets are unstructured
25:54 - or structured they are publicly there
25:56 - are publicly available data sets that
25:58 - are used for learning statistics data
26:00 - analytics and machine learning the most
26:02 - popular one being the msns database i
26:04 - can't tell you how many times i've seen
26:05 - this data set
26:07 - but it's images of handwritten digits
26:08 - used to test classification clustering
26:10 - and image processing algorithms commonly
26:12 - used when learning how to build computer
26:15 - vision ml models to translate
26:16 - handwritten text into digital text and
26:19 - here's an example of that data set there
26:21 - another
26:22 - very popular data set is the commons
26:24 - objects in context the coco data set i
26:27 - believe microsoft had a hand in this one
26:29 - and it's a data set which contains many
26:30 - common images in a json file a coco
26:33 - format that identify objects or segments
26:36 - within an image so there again it has
26:37 - json files that means that it's
26:39 - semi-structured not necessarily
26:40 - structured data here is an example of
26:43 - the data set you can see that there are
26:44 - images and they have borders drawn
26:47 - around things that are trying to
26:48 - identify in the images such as objects
26:50 - and segmentation
26:53 - recognizing context super pixel stuff
26:55 - segmentation no idea what that means uh
26:58 - 329 000 images and it has a lot of
27:00 - labels and a bunch of other things in
27:03 - this data set
27:04 - another interesting one would be the
27:06 - imdb review data set where it has 25 000
27:09 - highly polar movie reviews meaning
27:11 - people really like the movie or they
27:13 - really dislike them i pulled out an
27:14 - example here of pluto nash which at the
27:16 - time was highly uh liked and disliked
27:19 - there was a huge split between this
27:20 - movie mostly i think people disliked it
27:23 - but apparently it's kind of split this
27:25 - is great for customer seg
27:28 - sentiment analysis again you'll probably
27:29 - be using ml for this but the idea is to
27:32 - say did people like the movies like or
27:34 - did they like it hate it uh we're sad
27:36 - about it that's kind of like customer
27:38 - sentiment right how did they feel
27:40 - some other data sets we have the free
27:42 - music archive this is a data set of
27:44 - musical uh music
27:45 - tracks so you have a hundred thousand
27:48 - tracks across 163 genres you have the uh
27:52 - li i guess this is library but lib re
27:54 - speech a data set of a thousand hours of
27:57 - english speech you think they use an
27:58 - english word to describe that but no
28:00 - there are many more data sets online
28:03 - some are paid some you have to extract
28:05 - via an apis uh some you have to scrape
28:08 - the data yourself
28:09 - um but just taking a big look here
28:11 - of the full list we got crunchbase
28:13 - glassdoor
28:14 - fbi
28:16 - google trends data hub
28:18 - world health organization it's across
28:20 - the board right so
28:22 - you know there's a lot of things that
28:24 - you can get data online a lot of times
28:26 - you are just creating your own data sets
28:27 - but it's good to know that there's a lot
28:29 - of stuff out there
28:30 - [Music]
28:34 - so what is a data type it's a single
28:36 - unit of data that tells the compiler or
28:38 - interpreter so a computer program how
28:41 - data is intended to be used and the
28:43 - variety of data types will greatly vary
28:45 - based on the computer program a better
28:47 - way of thinking of it instead of saying
28:48 - computer program just say programming
28:50 - language because that's where you're
28:52 - going to be encountering data types
28:54 - so let's take a look at the most common
28:55 - data types the first being numeric data
28:58 - types these are data types involving
29:00 - mathematical numbers the most common
29:02 - being integer which is a whole number
29:04 - could be negative or positive a lot of
29:06 - programming languages will have a
29:08 - variety of these that have different
29:09 - ranges like int 32 into 64 what have you
29:13 - then you have floats or sometimes also
29:16 - known as decimals this is a number that
29:17 - has a decimal so 1.5 0.0 can be a
29:21 - negative number as well here it is
29:22 - example of an end and a float in python
29:26 - we'll take a look at text data types
29:28 - this is a data type that contains a
29:30 - readable or non-readable letters so
29:33 - there are characters so characters is a
29:34 - single letter so it could be a to z it
29:37 - could be a digit it could be a blank
29:39 - space punctuation special characters
29:42 - then you have a string and a string is a
29:45 - sequence of characters that can be word
29:47 - sentences or paragraphs they don't
29:48 - necessarily have to be words but you
29:50 - know that's usually what you're using
29:51 - them for so here's an example of a
29:53 - character and a string then you have
29:55 - composite data types and these contain
29:58 - cells of data that can be accessed via
29:59 - an index
30:00 - or a key so for example we have an array
30:03 - so that is a group of elements that
30:04 - contains the same data type that can be
30:06 - accessed via the index and position you
30:09 - have a hash commonly known as the
30:10 - dictionary and if you're using python
30:12 - you're probably known as the dictionary
30:13 - i know it's a hash because i like ruby
30:15 - so that's how i know it and it's a group
30:17 - of elements where a key can be used to
30:19 - retrieve a a value
30:21 - and the thing is is that composites are
30:23 - they overlap with data structures so
30:25 - when we talk about data structures you
30:27 - might say hey is an array in hash a data
30:29 - structure and yes it is but yes it's
30:31 - also a data type it just depends on the
30:33 - programming language and the constraints
30:35 - around those okay so i know those get a
30:37 - little bit confusing i just wanted to
30:38 - point that out but here's an example of
30:40 - again this is um python so we have an
30:43 - array and
30:45 - down below we have a python dictionary
30:47 - and if you use
30:48 - json or javascript yes uh you know a
30:50 - json object basically is a hash
30:53 - we've got one more here so we have
30:55 - binary data types this these are
30:56 - represented by a series of bits uh or
30:59 - bytes which either are zero or one so
31:02 - off and on so here's an example in
31:04 - python how to set up a byte um for
31:07 - billion values if we have true or false
31:10 - some languages represent billions as a
31:12 - zero or one so a lot of times when
31:14 - you're using like mysql true and false i
31:16 - think is zero and one in there sometimes
31:18 - it's a t or an f when you're using um
31:21 - a post sql it's going to be a t or an f
31:24 - uh and sometimes it's just true or false
31:26 - so in python it's just capital t true
31:28 - and actually i think they have a
31:29 - lowercase tree which is a different kind
31:30 - of object it's a bit confusing but i'm
31:32 - just saying there's some variants there
31:34 - then you have enumeration data types or
31:36 - sometimes known as enums and these are a
31:38 - group of constants unchangeable
31:40 - variables so for example diamond spade
31:42 - hearts and clubs they're all related
31:44 - because it's card groups
31:47 - so the idea here is the data type uh
31:49 - you know it could be also a data
31:51 - structure again it varies on the
31:52 - language just like composite types but
31:54 - here on the right hand side we have a
31:55 - shake and it's wrapped in a class and
31:58 - below this again this is python that's
32:00 - how you do in python and so the shape
32:02 - could be vanilla chocolate cookies and
32:03 - mint a lot of times the nums will map to
32:06 - an integer value or something so but not
32:09 - always the case um but yeah that's uh
32:12 - that type there and there you go that's
32:13 - the common data types
32:14 - [Music]
32:19 - okay so let's take a look at schema
32:20 - versus schema list so what is a schema a
32:23 - schema in terms of a database is a
32:24 - formal language which describes the
32:26 - structure of data a blueprint of a
32:28 - database and a schema can be
32:30 - can define many different data
32:31 - structures that serve different purposes
32:34 - of a database so
32:36 - different data structures in a
32:37 - relational database could be things like
32:38 - tables fields relationships views
32:40 - indexes packages procedures functions
32:42 - mxl schemas
32:44 - cues triggers types sequences
32:46 - materialized views cinnamons cinnamons
32:49 - synonyms can't say that word database
32:51 - links and directories gonna highly vary
32:54 - based on the database that you're using
32:56 - and i'm just going to show you an
32:57 - example of a schema so here is actually
33:00 - part of my schema for the xampro app and
33:03 - so this is a ruby on rail schema that
33:05 - defines the structure for a relational
33:07 - database and it's written in a dsl
33:09 - called ruby
33:10 - but the thing is is that this is going
33:12 - to highly vary uh based on again what
33:14 - you're using but just notice that you
33:16 - can see things like creating a table
33:18 - creating indexes creating columns for
33:20 - the database things like that
33:22 - adding extensions uh schema-less uh is
33:25 - just kind of it's still schema but the
33:28 - idea here is the primary cell of a
33:29 - database can accept many types so just
33:32 - going back here for a moment notice here
33:34 - that we have very particular notice here
33:37 - that we have very particular data types
33:38 - like integer string and stuff like that
33:40 - the idea here is with schema list that
33:43 - that data type is a lot more flexible
33:45 - and the idea there is it allows you to
33:46 - forego
33:47 - the upfront data modeling that you
33:49 - normally would have to do which is a lot
33:50 - of work and so that's one of the
33:52 - advantages of schema lists common
33:54 - stimulus databases would be key value
33:56 - document columns
33:58 - and then the subcategory of wide columns
34:00 - and graph and
34:02 - not a lot of information here but we
34:03 - will describe it in more detail when we
34:05 - talk about
34:06 - nosql databases okay
34:08 - [Music]
34:12 - all right let's talk about query and
34:14 - querying because those are terms that
34:16 - you're going to need to know because
34:17 - you're going to be doing quite a bit of
34:19 - it if you are going to have a career as
34:21 - a
34:21 - anything in data right like a data
34:23 - analyst so what is a query a query is a
34:25 - request for data results also known as
34:28 - reads
34:29 - uh or to perform operations such as
34:31 - inserting updating or deleting also
34:32 - known as writes and so a query can
34:35 - perform maintenance operations on the
34:36 - data and it's not always restricted to
34:38 - just working with the data that resides
34:40 - within the database and you'll see this
34:42 - where there's like there's commands to
34:45 - do analysis on your database and other
34:48 - things like that but here's an example
34:50 - of a query so what is a data result well
34:54 - results are data results is the results
34:57 - of the data returned from a query so
34:59 - here you generally will see tabular data
35:01 - that's usually what people want back but
35:03 - you know you can get back json or xml it
35:06 - really just depends on the database what
35:08 - is querying so this is the act of
35:09 - performing a query so the idea is that
35:12 - you write your query above and it's
35:14 - being sent as an as a request could be
35:16 - through an sdk cli the shell an api lots
35:20 - of ways for it to get there and then the
35:22 - idea is that the query is going to
35:23 - return the the data results so what is a
35:26 - query language that is a scripting
35:28 - language or programming language
35:29 - designed as the format to submit a
35:31 - request or actions to the database noble
35:34 - query languages is sql graph sql
35:37 - cousteau xpath gremlin and there's a lot
35:41 - of them but you know those are ones that
35:43 - stand out to me right now and just to
35:45 - note up here this is sql okay so i just
35:48 - didn't want to make a big old line this
35:50 - way here but this is sql here okay
35:53 - [Music]
35:57 - all right so let's compare batch and
35:59 - stream processing so batch processing is
36:01 - when you send batches a collection of
36:03 - data to be processed and batches are
36:05 - generally scheduled so you might say
36:07 - every day at 1 pm but you can also just
36:09 - queue up a batch whenever you feel like
36:10 - it batches are not real-time meaning
36:12 - that all the data is sent and then you
36:14 - wait until the batch is back to see the
36:16 - results batch processing is ideal for
36:19 - very large processing workloads batch
36:21 - processing is generally more cost
36:23 - efficient than stream processing and so
36:27 - here we just kind of have a
36:28 - representation so here we have our data
36:29 - we've broken into batches or collections
36:31 - we pass it to something like an etl
36:34 - engine
36:35 - and it will transfer the data and then
36:36 - we'll insert into our database data
36:38 - warehouse data store data lake house
36:41 - wherever you want to put it then we have
36:43 - stream processing so this is when you
36:45 - process data as soon as it arrives
36:47 - you'll have producers which will send
36:48 - data to a stream and consumers which
36:51 - will pull data from a stream a lot of
36:53 - times the stream will look like a
36:54 - pipeline and data can be held in that
36:56 - stream for a period of time so you have
36:58 - a better reusability of data if you need
37:01 - it for multiple
37:03 - consumers
37:05 - stream processing is good for real-time
37:06 - analytics real-time processing like
37:08 - streaming videos anything that has to do
37:10 - with real time if you need it right away
37:12 - it for that purpose it is much more
37:14 - expensive than batch processing um and
37:17 - here's a visual representation where we
37:18 - have bits of our data they go into our
37:20 - stream pipeline that can be held there
37:22 - for a while
37:23 - and sometimes
37:25 - minor operations will be performed on it
37:27 - but consumers will pull the data and do
37:28 - what they want with that data if we want
37:30 - to contextualize these things in terms
37:32 - of services on azure the idea is you'd
37:35 - have your data sources and you'd ingest
37:37 - them into something like azure stream
37:39 - analytics or i didn't really make this
37:41 - graphic very good but the idea is that
37:43 - you go into stream analytics or maybe
37:45 - you go into hdinsights or maybe you go
37:49 - into azure synapse analytics or one of
37:53 - these intermediate steps and then
37:54 - eventually you go to power bi to make
37:55 - your visualization reports
37:57 - for
37:58 - stream processing
38:00 - you could use event hub so event hub is
38:03 - a uh a single topic uh streaming service
38:07 - and you could ingest that into azure
38:08 - stream analytics
38:10 - um it's funny because this is the stream
38:12 - analytics icon this is actually the
38:13 - hadoop icon up here it's got it mixed up
38:16 - but anyway you go into stream analytics
38:19 - and you can insert that into cosmodb and
38:21 - then maybe pull cosmodb into power bi
38:24 - but yeah that is the difference between
38:26 - the two okay
38:29 - [Music]
38:33 - all right let's talk about relational
38:34 - data and this has to do with tables and
38:36 - relationships between other tables so
38:39 - let's talk about what a table is it's a
38:40 - logical grouping of rows and columns so
38:42 - an excel spreadsheet is actually tabular
38:45 - data tabular data just means the data
38:47 - that makes use of table data structures
38:50 - okay then you have views which look a
38:52 - lot like tables except they are the
38:54 - result set so a table when you do a
38:56 - query you're returning back data and
38:59 - you're storing that queries data in
39:01 - memory and basically it's a temporary or
39:04 - virtual table
39:05 - then you have materialized views and
39:07 - it's the same thing as a view except the
39:09 - difference here instead of being stored
39:11 - in memory it's stored on disk but again
39:13 - it's the results of a table so it's a
39:15 - virtual table
39:16 - then you have indexes and this is a copy
39:19 - of your data sorted by one or more
39:21 - multiple columns for faster reads at the
39:23 - cost of storage so think of it kind of
39:25 - like a virtual table but it does include
39:27 - all the columns and it's it's just so
39:29 - it's just to help you understand what
39:31 - order to retrieve data
39:34 - you have constraints these are rules
39:36 - applied to rights that can ensure data
39:38 - integrity so like if you have a database
39:40 - and you want to make sure that there are
39:42 - no duplicate records you put a
39:43 - constraint for no duplicates things like
39:45 - that
39:46 - you have triggers this is a function
39:47 - that is is a trigger on a specific
39:50 - database event this is really useful
39:51 - let's say after you insert a
39:54 - a column in your database you want to
39:56 - have a uuid you'd have a function that
39:58 - would generate out the uuid
40:01 - then you have primary keys so one or
40:03 - more multiple columns that uniquely
40:04 - identify a table in the row the most
40:06 - common primary key is id
40:09 - a foreign key so a column which holds
40:11 - the value of a primary key from another
40:13 - key to establish a relationship
40:15 - very commonly it's just the name of the
40:17 - other table with underscore id so
40:19 - relationship is when two tables have a
40:21 - reference to one another to join the
40:23 - data together i know this text is really
40:26 - boring but we're just going to cover so
40:27 - much about relational database tables so
40:29 - i didn't feel that we needed a visual
40:30 - here
40:31 - but let's keep moving forward here with
40:33 - relational data
40:38 - so for relational data we have tables
40:40 - and then the relationship between the
40:41 - tables let's talk about those
40:42 - relationships so relational databases
40:44 - establish relationships to other tables
40:46 - via foreign keys referencing another's
40:49 - table's primary key so looking at the
40:50 - example here uh you know this is the
40:53 - primary there's a little icon here that
40:55 - shows you that it's the primary key so
40:56 - this is the primary key
40:58 - and over here in another table it's
41:00 - referencing
41:01 - a foreign key so that's the foreign key
41:03 - that's the primary key okay and there
41:06 - are four
41:07 - types of relationships between
41:09 - relational databases
41:11 - and their tables the first is one to one
41:13 - so imagine a monkey has a banana or here
41:15 - we have a table called country and it
41:18 - has a capital it's one to one then you
41:20 - have one to many so a store has many
41:22 - customers or you could say a book has
41:24 - many pages notice that this denotes the
41:27 - many here then you have many to many so
41:29 - a project that has many tasks and tasks
41:32 - can belong to many projects or here a
41:35 - book can have many authors and an author
41:37 - can have many books so there's many to
41:38 - many and then last is a variant on the
41:41 - many to many so and it's via a join or
41:44 - junction table i just call them join
41:45 - tables so a student has many classes
41:48 - through enrollments and a class has many
41:50 - students
41:50 - uh through enrollments so here it's the
41:53 - same thing a book can have many authors
41:55 - an author can have any books but it's
41:57 - all through a library so you could say a
41:59 - book has many
42:00 - authors through a library and an author
42:02 - has many
42:03 - books through a library okay so there
42:05 - you go
42:06 - [Music]
42:10 - okay so we know that
42:13 - relational databases store tabular data
42:15 - but the thing is that data can also be
42:17 - stored
42:18 - either in a row oriented way or a column
42:21 - oriented way and let's just talk about
42:23 - the differences there and why we would
42:25 - do that so the first case we have row
42:27 - store the data is organized into rows
42:29 - this is great for traditional relational
42:31 - databases
42:32 - which are row stores
42:34 - good for general purpose databases
42:37 - suited for online transaction processing
42:39 - oltp we are going to come back to that
42:42 - term later on great when needing all
42:45 - possible columns in a row
42:47 - which is important during a query not
42:49 - the best at analytics or massive amounts
42:51 - of data all right we're looking at
42:52 - column store data is organized into
42:54 - columns it's faster at aggregating
42:56 - values for analytics so ideas imagine
42:59 - that you want to
43:01 - count how many cities there are for
43:03 - millions of records if it's organized by
43:05 - column like querying based on column or
43:07 - data stored together as columns a lot
43:09 - faster generally these are no sql stores
43:12 - or esco-like databases it's a bit
43:14 - confusing because
43:16 - uh you know like you would think tableau
43:17 - data is just relational databases but
43:20 - when you want to do column store they're
43:21 - basically nosql stores
43:23 - so the term's a bit fuzzy there it's
43:25 - great for vast amounts of data when
43:27 - we're talking about massive amounts
43:28 - we're talking millions and millions of
43:29 - records terabytes worth of data okay
43:32 - suited for online analytical processing
43:35 - oltp
43:36 - great when you only need a few columns
43:38 - so you don't need to get data from all
43:40 - the columns
43:42 - and there you go
43:43 - [Music]
43:47 - let's talk about database indexes which
43:49 - is a data structure that improves the
43:51 - speed of reads from the database table
43:53 - by storing the same or partial redundant
43:55 - data organized in a more efficient
43:58 - logical order and the logical order is
44:00 - commonly determined by one or more
44:01 - columns such as sort keys they're always
44:04 - called that a common data structure for
44:06 - an index is a balanced tree uh
44:09 - and it's short for b tree not to be
44:11 - confused with binary tree which is
44:12 - something else
44:13 - so you might see b tree and be like okay
44:15 - that's how it's doing that
44:17 - uh so here we just have kind of a visual
44:20 - imagine you have a table or a foot like
44:21 - that's for a phone book and you want to
44:23 - quickly find people based on the phone
44:25 - number because maybe you're trying to
44:26 - find them based on the starting number
44:28 - being 344 or something so the idea is
44:30 - you make an index and you say i want
44:32 - this to index by the phone number and so
44:34 - what it's going to do is change the
44:36 - order there so it might just pull the id
44:38 - or the number and reorder it and so now
44:40 - what you'll do is you'll use that index
44:42 - and that index will use as a reference
44:44 - to determine so it's not storing all the
44:45 - data but it will use that as a reference
44:48 - to the original table to quickly return
44:49 - your data
44:51 - and so here's a very easy way to create
44:53 - an index in postgres so we'd say create
44:55 - index and then we give it a unique name
44:56 - and we'd say
44:58 - let's make it index
44:59 - on the um for our addresses but just on
45:02 - the phone number so there you go
45:04 - [Music]
45:08 - let's take a look here at data integrity
45:10 - versus data corruption so data integrity
45:12 - is the maintenance insurance of data
45:13 - accuracy and consistency over its entire
45:16 - life cycle and it's often used as a
45:19 - proxy term for data quality now you
45:21 - might think it's data validation but
45:23 - it's just a prerequisite of data
45:25 - integrity because again data integrity
45:27 - is all about the entire life cycle
45:28 - making sure over all that it's going to
45:31 - stay consistent so validation is just
45:33 - one part of it the goal of data
45:34 - integrity is to ensure data is recorded
45:36 - exactly as intended data integrity is
45:39 - the opposite of data corruption so data
45:41 - corruption is the act or state of data
45:43 - not being in the intended state result
45:46 - in data loss or misinformation and data
45:48 - corruption occurs when unintended
45:49 - changes result when reading writing and
45:52 - so in the case when you're doing reads
45:53 - and writes maybe you have a hardware
45:54 - failure
45:55 - somebody just inputs the wrong data or
45:58 - someone intentionally is being malicious
46:00 - to corrupt your data or there's
46:02 - unforeseen side effects for operator
46:03 - operations via computer code so you
46:06 - wrote code and you didn't know that it
46:07 - was doing something that it wasn't
46:08 - supposed to be doing so how do we ensure
46:10 - data integrity well we have a
46:12 - well-defined and documented data
46:14 - modeling so data modeling if you know
46:16 - exactly how your data is supposed to be
46:17 - and it doesn't match the model there
46:19 - then you'll know logical constraints on
46:21 - your database items so we talked about
46:23 - that when we
46:24 - talked about all the types of relational
46:26 - data so constraints will keep that data
46:28 - integrity in place redundant and
46:29 - versions of your data to compare and
46:31 - restore
46:32 - so you have to be able to um not just
46:34 - validate your data but be able to bring
46:36 - it back to the state that it's supposed
46:37 - to be human analysis of the data so you
46:40 - know that's where data analysis will
46:42 - just check periodically uh hash
46:44 - functions to determine if changes have
46:45 - been tampered with you see this quite
46:47 - often when you're downloading uh open
46:49 - source software or software off like
46:51 - soft soft pedia where you can have an
46:53 - md5 hash to say did the thing i download
46:55 - match the thing that was expected
46:58 - uh principle of least privileges so
46:59 - limiting access to specific actions for
47:01 - specific user roles will mitigate uh
47:04 - problems that are unexpected with your
47:06 - data so all that stuff uh makes up data
47:08 - integrity okay
47:10 - [Music]
47:13 - okay it's time to compare normalized
47:15 - versus denormalized data so normalize is
47:17 - a schema designed to store non-redundant
47:20 - and consistent data whereas denormalize
47:23 - is a schema that combines data so that
47:25 - accessing data or querying it is very
47:27 - very fast so when we see tables and
47:30 - relationships like a relational table
47:32 - where everything is
47:33 - very discreetly
47:35 - organized this is normalized data and
47:38 - then on the right hand side where you
47:39 - could take all those tables on the
47:40 - right-hand side and make them one table
47:43 - this would be extremely efficient so the
47:46 - left-hand side for normalized data
47:47 - integrity is maintained little to no
47:49 - redundant data
47:51 - many tables optimize for storage of data
47:54 - on the right hand side we have data
47:55 - tegrity is not necessarily maintained or
47:57 - there's not good controls in place you
47:59 - have to do extra work to make sure it is
48:01 - in good shape redundant data is common
48:03 - fewer tables excessive data storage is
48:06 - less optimal now when you're using
48:07 - relational databases you can use both
48:09 - normalized and denormalized
48:12 - schemas and when you are using nosql
48:14 - it's a little bit harder but like
48:16 - there's cases where you can kind of
48:17 - model things like tables but generally
48:19 - data is denormalized
48:21 - in nosql so there's a bit more challenge
48:23 - with data integrity but the the upside
48:26 - is you get a lot more performance right
48:28 - so it's just way way faster at scale
48:33 - [Music]
48:35 - a pivot table is a table of statistics
48:37 - that summarizes the data of more
48:38 - extensive tables from a database
48:40 - spreadsheet or business intelligence
48:41 - tool and pivot tables are a technique in
48:44 - data processing they arrange or
48:46 - rearrange so pivot statistics in order
48:48 - to draw attention to useful information
48:50 - and this leads to finding figures and
48:52 - facts quickly making them integral to
48:55 - data analysis so when you're looking at
48:56 - microsoft excel it's very easy to create
48:59 - pivot tables think of a pivot table as
49:01 - an interactive report where you can
49:02 - quickly aggregate or group your data
49:05 - based on various factors so maybe you're
49:07 - grouping it by year month week or day
49:09 - some average min or max
49:11 - and so over here i have an example of a
49:13 - pivot table excel i got this from excel
49:15 - jet which they actually have really good
49:17 - information about
49:19 - pivot tables an example so if you think
49:21 - that you want to learn more about this i
49:22 - would go check out that resource but
49:24 - here you what you can see is that we
49:26 - have a table and notice that it has
49:28 - these little filters at the top right
49:30 - and so the idea is you can drop that
49:31 - down and say sort by date and and other
49:34 - stuff and what that you can do is create
49:37 - here is another pivot table here where
49:38 - we said okay let's sum the sales based
49:41 - on blue and green so this is a pivot
49:43 - table and then created another pivot
49:45 - table okay uh and so um you know it
49:48 - becomes very useful tool in excel all
49:50 - right
49:51 - and just one more thing pivot tables
49:53 - used to be a trademarked word owned by
49:55 - microsoft so a lot of times pivot tables
49:57 - were specifically just in excel or
49:59 - their uh was it their microsoft access
50:01 - database but now pivot tables is a
50:03 - unique term or a general term that
50:05 - everybody uses just for this kind of
50:07 - operations
50:08 - [Music]
50:12 - let's talk about data consistency and
50:14 - this is when data is being kept in two
50:16 - different places and whether the data
50:18 - exactly matches or does not match so
50:21 - when you have to duplicate data
50:23 - in many places and you need to keep them
50:25 - up to date to be exactly matching based
50:27 - on how the data is transmitted and
50:29 - service level the service levels of your
50:31 - cloud service provider they'll use these
50:33 - two terms and we'll hear strongly
50:34 - consistent and eventually consistent so
50:37 - strongly consistent means every time you
50:39 - request data so you query data you can
50:41 - expect consistent data be returned
50:44 - within x time so they might say within
50:46 - 10 milliseconds 100 milliseconds one
50:48 - second so the thing is we will never
50:51 - return to you old data but you will have
50:53 - to wait at least x amount of seconds for
50:55 - the query to return whatever that
50:57 - defined time is we talk about eventual
50:59 - consistency when you're when you request
51:01 - when you request data you may get back
51:04 - inconsistent data within x amount of
51:05 - periods so two seconds
51:07 - we are giving you whatever data is
51:09 - currently in the database you may get
51:11 - new data or old data but if you wait a
51:14 - little bit longer it will generally be
51:15 - up to date why would we have these two
51:17 - methods it just depends on your use case
51:18 - maybe you can tolerate some data to be
51:21 - inconsistent it's more important to get
51:23 - whatever data is available now and
51:24 - sometimes you need an absolute guarantee
51:27 - that the data is one to one okay so
51:29 - those are the two different ones
51:31 - [Music]
51:35 - so synchronous and asynchronous can
51:36 - refer to mechanisms of data
51:38 - transformation uh and data replications
51:41 - let's break these two down so
51:42 - synchronous is continuous streams i'm
51:45 - just going to mark that there continuous
51:47 - stream of data that is synchronized by a
51:49 - timer clock so you get a guarantee of
51:51 - time of when the data will be synced
51:54 - you can only access data once the
51:56 - transfer is complete you get guaranteed
51:58 - consistency of data returned it at the
52:00 - time of access slower access times so
52:03 - here is the data and if you're thinking
52:05 - about strongly consistent that is what
52:07 - this is it's this is going to be when
52:09 - things are strongly consistent then we
52:11 - have asynchronous so continuous stream
52:13 - of data separated by a start and stop
52:15 - bits no guarantee of time can access
52:17 - data anytime but may return older
52:19 - versions or empty placeholder
52:22 - faster access times no guarantee of
52:24 - consistency here it is so you see it's
52:26 - moving in bits right
52:28 - and so the idea is that we can access
52:30 - any time in between here to get maybe up
52:32 - to date data or not up to date data to
52:34 - solidify this let's put in some
52:36 - scenarios so a company has a primary
52:38 - database but they need to have a backup
52:39 - database in case their primary database
52:41 - fails the company cannot lose any data
52:43 - so everything must be in sync the
52:45 - database is not going to be accessed
52:47 - while it is standing by to act as a
52:50 - replacement so the reason why this works
52:52 - is that you know if you have a backup
52:54 - database you have to make sure all your
52:55 - data is one-to-one
52:57 - then on the other side here a company
52:58 - has a primary database but they want a
53:00 - read replica a copy of the database so
53:02 - their data analytics person can create
53:04 - computational intensive reports that do
53:06 - not impact the primary database it does
53:07 - not matter if the data is exactly
53:09 - one-to-one at the time of access because
53:11 - in this scenario it's like any time the
53:13 - database goes down
53:14 - you want it up to date to the second on
53:16 - this side it's like they might run
53:17 - reports once a day whatever so you know
53:20 - there's always new data coming in
53:22 - and b2 uh to burdensome to make sure
53:24 - that the data is always up to the second
53:26 - so there you go
53:31 - hey this is andrew brown from exam pro
53:33 - and we are looking at non-relational
53:34 - data and this is where we store data in
53:37 - a non-tabular form and will be optimized
53:39 - for different kinds of data structures
53:41 - so what kind of data structures well
53:43 - we're looking at key value stores so
53:45 - each value has a key design to scale
53:47 - only simple lookups then you have a
53:50 - document store so primary entity is
53:52 - json-like data structure called a
53:54 - document you have column restore
53:56 - sometimes this falls under relational
53:57 - databases but it is a non-relational
53:59 - data type or
54:01 - database so it has a table like
54:03 - structure but data is stored around
54:05 - columns instead of rows then you have
54:06 - the graph database where data is
54:08 - represented with nodes and structures
54:11 - where relationships really really do
54:12 - matter and so sometimes non-relational
54:14 - databases can be both a key value and
54:16 - document store like azure cosmo db or
54:18 - amazon dynamodb and the reason for that
54:21 - is that documents are actually a subset
54:23 - of key values which we'll talk about
54:25 - later when we get to that point
54:27 - [Music]
54:31 - hey it's andrew brown from exam pro and
54:32 - we're taking a look at data sources so a
54:34 - data source is where the data originates
54:38 - from so an analytics tool may be
54:40 - connected to various data sources to
54:42 - create a visualization report and a data
54:44 - source could be a data lake a data
54:46 - warehouse a data store a database a data
54:49 - requested on demand via an api endpoint
54:52 - from a web app
54:54 - and flat files such as excel or
54:56 - spreadsheet and so the example here is
54:58 - that we have a data source and somehow
55:01 - there has to be a connector between them
55:02 - and it's going to be consumed by either
55:04 - a warehouse an etl engine a data lake or
55:07 - bi tool those are common ones that need
55:09 - data sources
55:10 - so extracting data from data sources so
55:12 - a data tool like a business intelligence
55:15 - software would establish a connection to
55:16 - multiple data sources
55:18 - at the bi would extract data which could
55:21 - uh could be pulled data at the time of
55:23 - report or it could be pulled data on
55:25 - schedule or data could be streamed the
55:28 - mechanism for extracting data will vary
55:30 - per data source i just want you to know
55:31 - that because you know when you're using
55:33 - these services it does really vary on
55:35 - how it pulls the data so i just want you
55:37 - to understand there are a few different
55:38 - ways okay
55:39 - [Music]
55:43 - so a data store is a repository for
55:46 - persistently storing and managing
55:47 - collections of unstructured or
55:49 - semi-structured data so here i have kind
55:51 - of a visual where we have
55:53 - files going into some kind of store and
55:55 - a data store is a very very broad term
55:57 - so it's interchangeably used with
55:59 - databases though databases is
56:01 - technically a subset of a data store but
56:04 - generally a data store indicates working
56:06 - with unstructured or even
56:07 - semi-structured data so if somebody said
56:09 - a data store i'm thinking that it's
56:11 - either unstructured semi-structured okay
56:14 - a data store can be specialized in
56:15 - storing flat files
56:17 - emails maybe a database as we said it
56:21 - was a subset uh or designed to be
56:23 - distributed across many many machines
56:26 - or it could be a directory service okay
56:29 - so that's a data store
56:30 - [Music]
56:34 - so what is a database a database is a
56:36 - data store that stores semi-structured
56:38 - and structured data
56:40 - and but a better term i would say would
56:41 - be a databases more complex data store
56:44 - because it requires using formal design
56:46 - and modeling techniques databases can be
56:48 - generally categorized as either
56:49 - relational databases so this is
56:51 - structured data that strongly represents
56:53 - tabular data so tables roles and columns
56:55 - they're generally either row oriented or
56:58 - columnar oriented
57:00 - and when we talk about non-relational
57:02 - databases we're looking at
57:03 - semi-structured
57:04 - data that may or may not distantly
57:06 - resemble tabular data and i know that i
57:09 - put this one over on the relational side
57:12 - sometimes it ends up here or there just
57:14 - understand that that one kind of floats
57:16 - in between the two really depends on the
57:17 - technology underneath and so here is a
57:20 - pretty common way you get your sql you
57:22 - hit the database you get a table back
57:24 - right so the databases have a rich set
57:26 - of functionality specialized uh language
57:28 - to query so that is our sql here right
57:32 - we have specialized modeling strategies
57:33 - to optimize retrieval for different use
57:36 - cases more fine-tuned control over the
57:38 - transformations of the data into useful
57:39 - data structures or reports the thing
57:41 - here on the end normally a database
57:43 - infers someone is using a relational
57:45 - row-oriented data store so when somebody
57:48 - that's when someone says a database
57:49 - you're usually thinking like mysql sql
57:51 - postgres redb things like that okay
57:54 - [Music]
57:58 - so what is a data warehouse it's a
57:59 - relational data store designed for
58:00 - analytic workloads which is generally
58:02 - column oriented data store and again i'm
58:04 - going to make an emphasis here sometimes
58:06 - it's non-relational sometimes it's
58:07 - relational don't get too worried about
58:09 - that part okay companies will have
58:11 - terabytes and millions of rows of data
58:13 - and they need a fast way to be able to
58:15 - produce analytic reports that's how you
58:17 - know you need a data warehouse okay data
58:20 - warehouses are generally
58:22 - generally perform aggregations
58:24 - so aggregations is grouping of data
58:27 - to find a total or average data
58:29 - warehouses are optimized around columns
58:31 - since they need to quickly aggregate
58:33 - column data and so here is an example
58:36 - where we have our warehouse and
58:39 - the idea is that we're taking data in
58:41 - from like an unstructured source through
58:43 - an etl
58:44 - and then here we have an sql these are
58:46 - two different data sources we use sql to
58:49 - then get our results okay so data
58:51 - warehouses are generally designed to be
58:53 - hot hot meaning that the data will be
58:56 - returned very very fast even though they
58:59 - have vast amounts of data
59:01 - data warehouses are infrequently
59:03 - accessed meaning they aren't intended
59:04 - for real-time reporting but maybe once
59:06 - or twice a day or once a week to
59:08 - generate businesses and user reports now
59:10 - can it
59:11 - report extremely fast of course but it's
59:13 - not like at a at a per millisecond
59:15 - you're running it all day and keeping it
59:17 - all up to date that's more for a stream
59:19 - right a data warehouse needs to be needs
59:21 - to consume data from a relational
59:22 - database on a regular basis or you know
59:25 - through an etl data gets transformed
59:27 - input in there okay generally generally
59:30 - data warehouses are read only so you
59:32 - insert data and then you read it you're
59:33 - not using it for transactional data okay
59:37 - [Music]
59:41 - what is a data mart a data mart is a
59:42 - subset of a data warehouse a data mart
59:45 - will store generally under 100 gigabytes
59:47 - and has a single business focus and so a
59:50 - data mart allows different teams or
59:51 - departments to have control over their
59:53 - own data set for specific use cases so
59:55 - here we have a data warehouse and we are
59:58 - running queries to then put them in
60:00 - their own little data warehouses uh but
60:02 - the idea is that you know there are you
60:04 - know just smaller data sets that are
60:06 - more focused databars are generally
60:07 - designed to be read only because you're
60:08 - going to always have to pull data from
60:10 - the main data warehouse data marks also
60:12 - increase the frequency at which data can
60:14 - be accessed because of just smaller data
60:16 - sets
60:17 - and you don't have to worry about you
60:19 - know a huge cost because the larger the
60:22 - data set you have to query over the more
60:23 - expensive it gets right the cost of
60:25 - query is much much lower
60:27 - and so you might even see people
60:28 - accessing these a lot more frequently
60:31 - than they would a data warehouse so
60:32 - there you go
60:33 - [Music]
60:37 - so what is a data lake it is a
60:39 - centralized storage repository that
60:40 - holds vast amounts of raw data so big
60:42 - data in either semi-structured or
60:44 - unstructured format a data lake lets you
60:47 - store all your data without careful
60:48 - design or having to answer questions on
60:50 - the future use of the data so basically
60:52 - it's hoarding for data scientists here
60:55 - is kind of a visualization where you
60:56 - have a bunch of different data sources
60:58 - dropping into the data lake maybe you
61:00 - want to perform etls and put data back
61:02 - into the data lake and then you know you
61:04 - can extract out for reports ml all sorts
61:06 - of things we'll definitely cover data
61:07 - lakes when we get to the data lake
61:09 - section but uh this is the general
61:11 - overview here a daylight is commonly
61:12 - accessed for data workloads such as
61:14 - visualizations for bis tools real-time
61:16 - analytics machine learning on-premise
61:18 - data data lakes are great for data
61:20 - scientists but it's very hard to use
61:22 - data lakes for bi reporting so it's not
61:25 - that you can't do it it's just that
61:26 - there's additional steps and so there
61:27 - might be a different solution that might
61:29 - be a bit easier such as a data warehouse
61:31 - if data lakes are not well maintained
61:33 - they can become data swamps which is
61:35 - basically like data corruption right so
61:38 - yeah there you go
61:39 - [Music]
61:43 - what is a data lake house well a data
61:45 - lake house combines the best elements of
61:46 - a data lake and data warehouse and this
61:49 - isn't something that azure has an
61:51 - offering for right off the bat right now
61:53 - but you can definitely
61:54 - definitely believe that cloud service
61:56 - providers will have this in the future
61:57 - so i want you to know about this today
61:59 - even if it's not on your exam so data
62:01 - like houses compared to data warehouse
62:04 - can support video audio and text files
62:06 - support data science ml workloads have
62:08 - support for both streaming and etl
62:10 - work with many open source formats data
62:13 - will generally reside in a data lake or
62:14 - blob store so the thing with data lake
62:16 - or data warehouse is usually used
62:18 - proprietary formats so this is a lot
62:20 - more flexible data like houses compared
62:22 - to data lakes can perform bi tasks very
62:25 - well which is something data links
62:26 - cannot do much easier to set up and
62:28 - maintain has management features to
62:30 - avoid a data like becoming a data swamp
62:32 - right because data warehouses are
62:34 - very well
62:35 - data modeled and
62:37 - data lakes aren't
62:38 - so data lakes are kind of in between
62:41 - and uh data lakes and with a data lake
62:43 - house is going to be more performant
62:44 - than a data lake so where would you find
62:47 - a solution right now probably with data
62:49 - delta like which is
62:51 - a data bricks solution so it's
62:54 - apache data like i believe is open
62:56 - source and so if you wanted a managed
62:58 - version of it
62:59 - databricks has an offering for that and
63:01 - so they have this nice little graphic
63:02 - here where they show you that you know
63:04 - you combine the two and you get the best
63:05 - of both worlds so you know you'll see
63:07 - more data likes in the future so there
63:08 - you go
63:09 - [Music]
63:13 - hey it's andrew brown from exam pro we
63:15 - are looking at data structures so what
63:17 - is a data structure this is data that is
63:19 - organized in a specific storage format
63:21 - that enables easy access and
63:23 - modification and a data structure can
63:26 - store various data types so data can be
63:28 - abstractly described to have a degree of
63:30 - structure so we have unstructured a
63:32 - bunch of loose data that has no
63:33 - organization or possibly any relation
63:36 - semi-structured data that can be browse
63:38 - or search with limitations structured
63:40 - data that can be easily browsed or
63:42 - searched so when we look at unstructured
63:44 - data think of a bunch of loose files and
63:46 - this is just a screenshot from one of my
63:47 - folders here's a bunch of stuff or
63:49 - semi-structured data we have xml or
63:52 - structured data where it's like we're
63:53 - using a relational database so yeah
63:56 - there we go let's go drill down into
63:57 - these three types of abstractions
63:59 - [Music]
64:03 - so again what is unstructured data well
64:05 - it's just a bunch of loose data think of
64:07 - it as junk folder on your computer with
64:09 - a bunch of random files not optimized
64:10 - for search analysis or simply no
64:12 - relation between the various data so
64:14 - again there's a bunch of files in my uh
64:16 - one of my folders there and so when
64:18 - we're talking about microsoft azure
64:19 - services that store unstructured data we
64:21 - have sharepoint so shared documents for
64:23 - an organization
64:25 - azure blob storage so unstructured
64:27 - object data store azure files a
64:29 - mountable file system for storing
64:31 - unstructured files azure data lake for
64:33 - big data it's basically blob storage but
64:36 - for vast amounts of data and if i wanted
64:38 - to add a fifth one there you know like
64:39 - azure azure disks but that's more for
64:42 - virtual machines okay
64:44 - [Music]
64:48 - so let's take a look here at
64:49 - semi-structured data which basically has
64:51 - no schema and the data has some form of
64:53 - relationship it's easy to browse data to
64:54 - find related data and you can search
64:57 - data but there are limitations or or
64:59 - when you search you will pay a
65:00 - computative or operational cost a great
65:02 - way of thinking about this is think of a
65:04 - big box of legos and you have these lego
65:06 - pieces so it's not that there's not a
65:08 - schema defined there's a schema defined
65:10 - in the sense of the lego piece so it can
65:13 - connect and make relationships with
65:14 - other things that are compatible but
65:16 - overall the entire data as a whole does
65:19 - not have a schema right you don't have
65:21 - you're not upfronting and doing data
65:23 - modeling so just understand that's why i
65:25 - put the asterisk there so there is a
65:27 - schema for the data structures just not
65:29 - for everything in total totally here
65:31 - that's why it's called semi-structured
65:32 - for concrete semi-structured data
65:34 - structures we've got xml json
65:36 - avro and parquet i don't know if it's
65:38 - pronounced parquet but that's the way i
65:39 - say it and for azure and other services
65:41 - that store semi semi structured data we
65:44 - have azure tables which is a key value
65:45 - store azure cosmodb where its primary
65:48 - one is document of course it stores
65:50 - other types but when we're talking about
65:52 - semi-structure we're talking about a
65:53 - document store mongodb which is an open
65:56 - source document store and then we have
65:57 - apache cassandra which is a
66:00 - y column store database um there's no
66:03 - sql that's just an open source one okay
66:07 - [Music]
66:11 - all right so we're still on
66:12 - semi-structured data structures i just
66:13 - want to give them a little bit more
66:15 - attention because you know you might
66:17 - need to know some of the the guts to
66:18 - these semi-structures so what is
66:21 - semi-structured data semi-structured
66:23 - data is data that contains fields the
66:25 - fields don't have to be the same in
66:26 - every entity
66:28 - you only define the fields that you need
66:29 - on a per entity basis so common
66:32 - semi-structured data structures is
66:34 - javascript object notation json format
66:36 - used in json notation stored data in
66:39 - memory read and write from files apache
66:42 - apache optimize row columner format also
66:45 - known as orc organizes data into columns
66:48 - rather than rows so column or data
66:50 - source structure apache parquet this is
66:52 - another column or data store a parkit
66:54 - file contains rows and groups
66:56 - we have apache avro which is row based
66:59 - format each record contains a header
67:02 - that describes the structure of the data
67:04 - in the record you have also xml we're
67:06 - not going to go through all of these but
67:07 - i do want to drill down on some of these
67:09 - semi-structured data structures so you
67:11 - know how they internally work okay
67:17 - all right let's take a look at json so
67:18 - json stands for javascript object
67:20 - notation and it is a lightweight data
67:23 - interchange format it is easy for humans
67:25 - to read and write it is easy for
67:27 - machines to parse and generate and it is
67:29 - based on a subset of javascript so here
67:31 - is an example of json and json is built
67:34 - on two structures the first is a
67:36 - collection of names
67:38 - name value pairs in other languages this
67:41 - is realized as an object a record a
67:42 - struct a dictionary a hash table key
67:44 - list or associative array so if you've
67:46 - ever heard of those things before that's
67:48 - basically what it looks like the other
67:49 - part is an ordered list of values other
67:51 - languages might call them arrays vectors
67:54 - list or sequence just to point them out
67:56 - there is the collection and there is the
67:58 - ordered list and json is a text format
68:00 - so that it is completely language
68:02 - independent
68:04 - so it is
68:06 - used quite a bit these days
68:12 - all right let's take a look at apache
68:13 - org files which stands for optimize row
68:15 - columner it's a storage format for
68:17 - apache hadoop system so it is similar to
68:20 - rc files and parkit files and is the
68:22 - successor to rc files we're not going to
68:24 - cover rc files here but you know that's
68:26 - where these come from it was developed
68:28 - by facebook to support columnar reads
68:30 - predictive pushdowns and lazy reads is
68:32 - more storage efficient than rc files
68:34 - taking up 75 percent less space orc only
68:37 - supports hadoop hive and pig when we get
68:40 - to the hadoop section you'll understand
68:41 - what those are work performs better with
68:43 - hive than parquet files org files are
68:45 - organized into stripes of data so here
68:48 - is that example there the autonomy of an
68:50 - org file so the file footer stores the
68:52 - auxiliary information the list of
68:54 - stripes in the file the number of rows
68:56 - per stripe each column the data type is
68:59 - column level aggregate information so
69:01 - count min max
69:03 - the stripe footer contains a directory
69:04 - of stream locations
69:06 - and we have the road data which is used
69:09 - for table scans and the index table
69:11 - includes min max values for each column
69:14 - and the row positions for each columns
69:16 - and the default size of a stripe is 250
69:18 - megabytes with large stripe sizes enable
69:21 - large efficient reads for hdfs which is
69:25 - the hadoop file system which we'll talk
69:27 - about when we get to the hadoop section
69:28 - but if you want to
69:30 - just review this here and then take a
69:32 - look at this graphic and it'll make a
69:34 - hundred percent sense here okay
69:36 - [Music]
69:40 - so let's take a look here at parquet
69:42 - files so apache parquet is a column
69:44 - restore file format available to any
69:46 - project in the hadoop ecosystem so hive
69:49 - hbase mapreduce pig spark
69:51 - presto there's a huge list of them and
69:53 - not just here in the hadoop system but
69:56 - other other services azure even aws
69:58 - services work really well with parquet
70:00 - files so you know it's just becoming a
70:03 - very common format for columnar storage
70:06 - formats the parquet is built to support
70:08 - very efficient compression encoding
70:09 - schemes it uses the record shredding
70:11 - assembly algorithm that's why i don't go
70:14 - in detail like work here here about like
70:16 - talking about this data structure
70:17 - because it just gets complicated so i
70:20 - just want you to know that parquet files
70:22 - uh are more generally used in org files
70:24 - or have very particular use cases uh and
70:27 - you're gonna come across parket more
70:29 - when you're doing column or storage file
70:30 - formats okay
70:31 - [Music]
70:35 - let's take a look at avro so apache avro
70:38 - is a row-based format that provides rich
70:40 - data structures compact fast binary data
70:42 - format a container file to store
70:44 - persistent data remote procedure calls
70:47 - rpcs simple integration with dynamic
70:49 - languages avro provides functionality
70:50 - similar to systems such as thrift and
70:52 - protocol buffers here is the data
70:54 - structure so when would you be using
70:56 - avro over parquet well it's just when
70:58 - you uh when you have data like if you
71:00 - want to serialize your json into a more
71:02 - efficient format we're doing general
71:04 - queries right if you're doing analytics
71:07 - right columner based stuff you're doing
71:09 - it for um
71:10 - aggregation and stuff like that
71:12 - if you're just trying to kind of like
71:14 - simulate general general relational
71:16 - database structures for semi-structured
71:17 - or nosql databases you're going to want
71:19 - to use avro
71:21 - [Music]
71:25 - all right let's talk about uh structured
71:27 - data so structured data it has a scheme
71:29 - and data data has a relationship it's
71:31 - easy to browse to find related data it's
71:33 - easy to search data the most common
71:34 - structured data is tabular data
71:36 - representing the rows and columns right
71:38 - so examples here for azure would be its
71:40 - postgres azure
71:42 - data sql database for postgres for msql
71:46 - azure sql for mssql and azure synapse
71:49 - analytics which is the data warehouse
71:50 - service okay
71:56 - hey this is andrew brown from exam pro
71:57 - and we are looking at what is data
71:59 - mining so this is the extraction of
72:00 - patterns and knowledge from large
72:01 - amounts of data not to be confused with
72:03 - the extraction of data itself a lot of
72:05 - people think data mining means go on a
72:06 - website and start scraping that's not
72:08 - what it is uh cross industry standard
72:10 - process for data mining called chris dmm
72:12 - is defined into six phases there's a lot
72:14 - of way to define data mining but i just
72:16 - chose this one because i found that it's
72:18 - the easiest to understand so we got our
72:19 - big wheel here let's break through the
72:21 - six phases so business understanding
72:23 - right so here we are at the start of our
72:26 - journey over here business understanding
72:28 - so what does the business need then we
72:30 - have data understanding which is what do
72:32 - we have to do and what data do we need
72:35 - and you can see that you can work
72:36 - between back and forth before you move
72:37 - on to the next step
72:39 - we have data preparation so how do we
72:40 - organize the data for modeling then we
72:43 - have modeling so what modeling
72:45 - techniques should we apply over here
72:47 - evaluation so what which data model best
72:50 - meets the business objectives and on the
72:52 - end here we have deployment so
72:55 - how do people access the data so there
72:57 - you go
72:57 - [Music]
73:01 - let's take a look at different types of
73:03 - data mining methods and this will
73:04 - definitely not be an exhaustive list but
73:06 - it will give you a good idea what a data
73:08 - miner does so data mining methods or
73:10 - techniques is the way to find valid
73:12 - patterns and relationships in huge data
73:14 - sets so we have classification this is
73:15 - where you classify data into different
73:17 - classes
73:18 - you have clustering a division of
73:20 - information into groups of connected
73:21 - objects you have regression which is
73:24 - ident identify and analyze the
73:26 - relationships between variables because
73:28 - of the presence of other factors we have
73:29 - sequential so evaluating sequential data
73:32 - to discover sequential patterns
73:33 - association rules discover a link or two
73:36 - or more items find a hidden pattern in
73:38 - the data sets
73:39 - these common these common constraints
73:41 - math formulas are used to determine
73:43 - significant and interesting links so we
73:45 - got support so indication of how
73:48 - frequently the item set appears in the
73:49 - data set confidence indication of how
73:51 - often the rule has been found to be true
73:54 - lift indication of importance compared
73:56 - to other items conviction indication of
73:58 - the strength of the rule from the
74:00 - statistical independence for outer
74:02 - detection we have observation of data
74:04 - items in the data set which do not match
74:07 - an expected pattern or expected behavior
74:09 - we have prediction use a combination of
74:11 - data mining techniques such as trends
74:13 - clustering classification to predict
74:14 - future data
74:15 - not super important for you to remember
74:17 - all this but i'm just trying to like get
74:18 - you exposure to these terms and things
74:20 - so that as you see them more it'll make
74:22 - sense okay
74:23 - [Music]
74:27 - hey this is andrew brown from exam pro
74:29 - and we are looking at what is data
74:30 - wrangling so data wrangling is the
74:32 - process of transforming and mapping data
74:34 - from one raw data form into another
74:36 - format with the intent of making it more
74:37 - appropriate and valuable for a variety
74:40 - of downstream purposes such as analytics
74:42 - also known as data munging so there are
74:45 - six core steps behind data wrangling the
74:47 - first is discovery so understand what
74:49 - your data is about and keep in mind
74:52 - domain specific details about your data
74:53 - as you move through the other steps
74:55 - structuring so you need to organize your
74:57 - content into a structure that will be
74:59 - easier to work for our end results
75:02 - cleaning so remove outliers change null
75:04 - values remove duplicates remove special
75:06 - characters standardizing formatting
75:08 - enriching appending or enhancing
75:10 - collected data with relevant context
75:12 - obtained from additional sources
75:14 - validating authenticate the reliability
75:17 - quality and safety of the data and
75:19 - publishing so place your data in a data
75:21 - store so it can be used
75:24 - downstream so there you go
75:26 - [Music]
75:30 - hey this is andrew brown from exam pro
75:32 - and we're looking at what is data
75:33 - modeling but before we can answer that
75:34 - we should ask what is a data model so
75:36 - it's an abstract model that organizes
75:38 - elements of data and standardizes how
75:40 - they relate to one another and to the
75:42 - properties of real world entities a data
75:44 - model could be a relational database
75:45 - that contains many tables so here's
75:47 - actually an example of some data
75:49 - modeling i did which is for the exam pro
75:51 - platform if you ever open up power bi
75:54 - they have like a data modeling tab so it
75:55 - becomes very clear what it is but
75:56 - generally uh you know data models just
75:59 - look like a bunch of tables and
76:00 - relationships but it's going to vary
76:02 - based on what you're using a data model
76:04 - for so a dml could be conceptual so how
76:06 - daters represented the organizational
76:08 - level abstractly without
76:10 - concretely describing how it works
76:12 - within the software so people orders
76:14 - projects relationships logical so how
76:17 - data is presented in software tables
76:19 - columns
76:20 - object oriented classes physical so how
76:23 - data is physically stored so partitions
76:25 - cpus and table spaces so this one would
76:27 - probably be the the middle one here
76:29 - which is logical okay so you know this
76:31 - isn't just exactly how data modelling
76:33 - looks like there's all varieties the way
76:35 - data modeling or a data model can appear
76:37 - so what is data modeling a process used
76:40 - to
76:40 - define and analyze data requirements
76:43 - needed to support the business processes
76:45 - within the scope of the corresponding
76:46 - information systems and organizations so
76:49 - here uh we have our uh data modeling
76:52 - here so you can see that uh it's
76:55 - actually broken up kind of into three
76:56 - sections which maps up really well see
76:58 - where it says physical conceptual things
77:00 - like that it matches up to our three
77:02 - categories here conceptual logical
77:03 - physical so just take in mind that uh
77:06 - you know if you have data modeling you
77:07 - can move from a conceptual to a logical
77:10 - to a physical one all right and so there
77:12 - you go
77:16 - [Music]
77:17 - all right let's take a look at etl
77:18 - versus elt so etl intel is used when you
77:21 - want to move data from one location to
77:23 - another where the data store database
77:25 - have a different data structure so you
77:27 - need to transform the data for the
77:29 - target system a common use case would be
77:32 - mssql to cosmodb so this one is
77:34 - relational this one's nosql they just
77:36 - don't have the same data structures
77:38 - you'd have to do some kind of
77:38 - transformation and so here we have our
77:41 - visuals for etl and elt so let's talk
77:43 - about etl first which stands for extract
77:46 - transform and load so loads the data
77:48 - first into a staging server and then
77:50 - into a target system so even though it's
77:52 - not shown here we actually have an
77:54 - intermediate virtual machine or server
77:56 - that's being loaded temporarily into
77:58 - doing the transformations and then when
78:00 - it's done it's going to output it into
78:02 - its target system uh used for on-premise
78:05 - relational and structured data so it's
78:07 - very common for on-prem like this could
78:09 - be a migration strategy so they could be
78:11 - taking an sql database and just moving
78:13 - it to
78:14 - um a sql database on
78:17 - azure right and so there might be
78:19 - like they're the same type of database
78:20 - but there could be different versions of
78:22 - databases so the the feature sets
78:24 - slightly different so they do some
78:25 - transformations it's good for a small
78:28 - amount of data to be fair etl can be
78:30 - used for larger workloads but you know
78:32 - when we're comparing from elts it's
78:33 - generally smaller doesn't provide data
78:35 - lake support
78:37 - easy to implement
78:38 - mostly supports relational databases
78:40 - okay
78:42 - when we talk about extract load
78:43 - transform loads directly into the target
78:45 - system
78:46 - used for scalable cloud structures and
78:48 - unstructured data sources used for large
78:50 - amounts of data provides data like
78:52 - support requires specialized skills to
78:55 - implement and maintain supports for
78:57 - unstructured data readily available so
78:59 - you're going to see the elt is going to
79:01 - be the more common use case where we're
79:04 - dealing with cloud
79:06 - but it does require a little bit more
79:07 - knowledge where this one is just like if
79:09 - you know sql you're going to be in good
79:10 - shape okay so there you go
79:12 - [Music]
79:16 - hey it's andrew brown from exam pro and
79:18 - we are looking at what data analytics is
79:19 - so this is when you're concerned with
79:21 - examining transforming arranging data so
79:23 - you can extract useful information a
79:26 - person that does data analytics is
79:27 - called a data analyst and they commonly
79:30 - use tools such as sql business
79:32 - intelligence tools and spreadsheets if
79:34 - we look at the data and analytics
79:36 - workflow so you can understand their
79:37 - whole scope of their job they'll do data
79:40 - ingestion so getting data from multiple
79:41 - sources data cleaning and transformation
79:44 - so maybe they're using
79:46 - you know pandas and
79:49 - notebooks and things like that or sql
79:51 - commands
79:53 - dimensional reductions so they have to
79:54 - reduce the amount of data
79:56 - data and analysis
79:58 - which could be like statistics and
80:00 - things like that visualizations you use
80:02 - your bi tools or you are actually coding
80:04 - in dashboards and things like that so
80:07 - yeah that is data analytics
80:09 - [Music]
80:13 - all right let's talk about kpis here so
80:14 - key performance indicators are type of
80:16 - performance measurement that a company
80:18 - will use or their organization will use
80:20 - to determine performance over time so
80:23 - here's an example of a kpi for product
80:25 - revenue and the goal was 3.12 million
80:29 - but the company actually generated out
80:30 - 2.29 million so there's 26 percent under
80:33 - their goal kpi can evaluate the success
80:35 - of an organization or for a specific
80:37 - organization activity and there are two
80:39 - categories of measurement for kpis we
80:41 - have
80:42 - quantitative and these quantitative and
80:44 - qualitative or get easily confused
80:46 - because they're very similar name but
80:47 - quantitative the properties can be
80:49 - measured with a numerical result facts
80:51 - presented with a specific value so
80:54 - monthly revenue numbers of sign ups
80:55 - number of reports or defects so what
80:57 - we're looking up at here is quantitative
81:00 - okay and then we have qualitative so
81:03 - properties that are observed and can
81:05 - generally not be measured with the
81:06 - numerical results numerical or numeric
81:09 - or textual value that represents uh
81:11 - personal feelings tastes and opinions so
81:14 - maybe customer sentiment would be
81:15 - example there so that is kpis
81:17 - [Music]
81:21 - all right so we're taking a look here at
81:23 - data analytics techniques and this is
81:24 - something you 100 need to know for the
81:26 - exam so
81:27 - pay close attention here okay so the top
81:30 - of our list here we have descriptive
81:31 - analytics and the question we are
81:32 - answering here is what has happened uh
81:35 - so here we might have specialized
81:36 - metrics such as kpis or return
81:38 - investment or things that we're more
81:39 - familiar with like generating sales and
81:41 - financial reports
81:42 - at this stage we have a lot of data
81:44 - right it's very accurate comprehensive
81:46 - it's either live data and we can make
81:48 - very effective visualizations to
81:49 - understand our data
81:51 - because we have all that historical
81:52 - information but there's a lot more to it
81:55 - in terms of value and so we'll move down
81:58 - or out
81:59 - move out uh to see the other stuff that
82:02 - we can do here the next is diagnostic
82:03 - analytics so why did it happen it's
82:06 - supplemental to descriptive analytics we
82:08 - can drill down investigate descriptive
82:09 - metrics to determine root cause find and
82:11 - isolate anomalies into its own data set
82:13 - and apply statistical techniques we have
82:15 - predictive analytics so what will happen
82:18 - so we use historical data to predict
82:19 - trends or recurrence uh we use either
82:22 - statistical or machine learning
82:24 - techniques apply this is where a data
82:25 - scientist might get involved we use
82:27 - neural networks decision trees
82:28 - regression classification neural
82:29 - networks just means
82:31 - deep learning machine learning okay
82:33 - uh prescriptive analytics how can we
82:35 - make it happen so goes a step further
82:38 - than predictive and uses ml by ingesting
82:40 - hybrid data to predict future scenarios
82:42 - that are exploitable and then the last
82:44 - one is what if this happens
82:46 - and that's cognitive analysis so using
82:48 - analytics to draw patterns to create
82:50 - what if scenarios and what actions can
82:51 - be taken if the scenarios become a
82:53 - reality so there you go that is data
82:55 - analytic
82:56 - [Music]
83:00 - all right let's take a look here at
83:01 - microsoft onedrive so microsoft onedrive
83:03 - is a storage and storage synchronization
83:05 - service for files which reside in the
83:07 - cloud similar to products like dropbox
83:09 - google drive box and microsoft
83:11 - sharepoint so onedrive is intended for
83:13 - personal storage for a single individual
83:15 - you pay for different sizes of storage
83:17 - so you have five gigabytes which are
83:18 - free 100 gb 1tb and 6tb you do not worry
83:22 - about the underlying hardware the
83:23 - durability resilience fall tolerance
83:25 - availability that's what we call
83:26 - serverless technology here is a nice
83:28 - screenshot of what it looks like to use
83:30 - onedrive both in the browser and on your
83:32 - phone files can be shared easily to
83:34 - other users via a shareable link or a
83:36 - specific email that has onedrive account
83:39 - files are accessed via a web app or
83:41 - shared folders that hold a reference to
83:44 - file stored in the cloud so shared
83:45 - folders mean like you can literally use
83:47 - your file system and you'll have folders
83:49 - that are linked to it files can be
83:51 - synchronized so a copy resides in a
83:52 - local computer hard drive is copied to
83:54 - the cloud a file residing the cloud can
83:56 - be copied to a little computer hard
83:58 - drive
83:58 - copying occurs automatically when files
84:00 - are changed differences and files could
84:01 - result in conflicts and a user must
84:03 - choose which file to keep or how to
84:05 - resolve the conflict might have more
84:06 - options files can be versions so you can
84:08 - recover older versions of files older
84:10 - files may retain for 30 days and be
84:12 - automatically deleted so there you go
84:13 - that is onedrive
84:18 - let's take a look here at microsoft 365
84:21 - sharepoint so 365 sharepoint is a
84:23 - web-based collaborative platform that
84:25 - integrates into the microsoft office
84:26 - intended for document management and
84:28 - shared storage and here is a screenshot
84:30 - of my sharepoint uh and so there's the
84:32 - az-104 so the thing is is that it's
84:35 - basically
84:36 - one drive uh but for companies with a
84:39 - bunch of layers on top of it and it is
84:41 - extremely useful to use it's super super
84:44 - useful so like if you work in a company
84:46 - you should really be using sharepoint
84:47 - for sharing files so just growing down
84:50 - the feature list here so sharepoint
84:51 - sites because there's some concepts here
84:53 - just besides documents they have things
84:55 - like sites so data within sharepoint is
84:57 - organized around sites a site is a
84:59 - collaborative space for teams with the
85:01 - following components so document library
85:03 - pages web parts and more sharepoint
85:05 - doctor document library which will
85:07 - expand there and that's what the
85:08 - screenshot is on the right-hand side so
85:10 - a document library is a file storage and
85:11 - synchronization but designed for teams
85:13 - it is very similar to onedrive but files
85:14 - are owned by the company and not an
85:16 - individual you can apply robust
85:18 - permissions to access files within or
85:20 - outside your organization a site always
85:22 - has a default document library called
85:24 - documents
85:25 - and so there's a lot more going on in
85:27 - sharepoint but this is the most
85:29 - important feature of it and actually
85:31 - when you use onedrive when you install
85:32 - on your computer there's like a
85:33 - synchronization device it's the same
85:36 - thing so sharepoint most likely is using
85:38 - the onedrive technology underneath i
85:40 - don't know 100 but it's most likely the
85:42 - case so there you go
85:43 - [Music]
85:47 - hey this is andrew brown from exam pro
85:49 - and we are looking at data core concepts
85:51 - this one's four sheets long so let's
85:52 - jump into it so the first here is data
85:54 - which is units of information data
85:56 - documents types of abstract groupings of
85:57 - data data sets unstructured logical
85:59 - grouping of data data structures which
86:02 - has some form of structure and there's
86:04 - variance right so we have unstructured a
86:06 - bunch of loose data that has no
86:07 - organization or possible relation we're
86:09 - talking about flat files here various
86:10 - files that can reside in a file system
86:12 - semi-structured so that's data that can
86:14 - be borrowed or searched with limitations
86:15 - so csvs xml json parquet and so if we're
86:19 - talking about xml files the markup looks
86:20 - like html for json it's a text file
86:22 - that's composed of dictionaries and
86:24 - arrays rc files are storage formats
86:26 - designed for map reduce framework not
86:28 - something we covered in the uh lecture
86:30 - content but i just wanted to mention
86:31 - them there work so a columnar data
86:34 - structure 75 more efficient than rc
86:36 - files limited compatibility works very
86:39 - well with hive we have avro so a rose
86:43 - row-wise uh data structure for hadoop
86:46 - systems you have parkette a columnar
86:48 - data structure that has more support for
86:50 - hadoop systems than orc then we were
86:52 - talking about structured data so data
86:53 - that can be easily browsed or searched
86:55 - so tabular data and so tabular data is
86:57 - data that is arranged as tables think of
86:59 - spreadsheets
87:01 - data types how single units of data are
87:03 - intended to be used we're not going to
87:04 - go through the whole list they're not
87:05 - going to ask that on the exam but you
87:06 - should know your data types uh four
87:08 - types of roles that azure cares for you
87:11 - to know we have database administrators
87:12 - so configures and maintains databases
87:14 - data engineer design and implement data
87:16 - tasks really to transfer and storage of
87:18 - big data data analysts analyzes business
87:20 - data to reveal important information
87:22 - then we have our
87:24 - tiers of computing so we have software
87:26 - as a service a product that is run and
87:28 - managed by service provider platform as
87:30 - a service focus on the deployment
87:31 - management of your apps infrastructure
87:33 - as a service basic building blocks of
87:34 - cloud i.t provides access to networking
87:36 - computers data storage and space and
87:38 - remember that uh we're talking about sql
87:41 - it's going to be the sql vms on this
87:43 - layer and then here's going to be the
87:44 - managed sql and
87:48 - azure sql databases okay
87:51 - so we're on to the second page here so
87:53 - let's talk about data stores
87:54 - unstructured or semi-structured data
87:56 - for housing data a broad term that can
87:59 - encompass anything that stores data
88:00 - databases structured data that can be
88:02 - accessed quickly and search generally
88:04 - relative row-based tabular data for oltp
88:07 - data warehouses structured
88:08 - semi-structured data for creating
88:10 - reports and analytics column-based
88:11 - tabular data for olap data mart's a
88:13 - subset of data warehouse for specific
88:15 - business data tasks data lakes combines
88:17 - the best of data warehouses and data
88:19 - lakes notebooks data that is arranged in
88:21 - pages designed for easy consumption
88:23 - batching when you send batches a
88:24 - collection of data to be processed not
88:26 - real time streaming when the data is
88:28 - processed as soon as it arrives so it's
88:29 - real time relational data data that uses
88:32 - struct structure tabular data and has
88:34 - relationships between tables and in
88:36 - terms of relationships for relational
88:38 - relational stuff we have one to one so
88:39 - one to one so think a monkey has a
88:41 - banana one to many a store has many
88:43 - customers many to many a project has
88:45 - many tasks and tasks can belong to many
88:47 - projects
88:48 - a join table a student has many classes
88:50 - through enrollments the enrollments
88:52 - would be the joint table and a class has
88:53 - many students through enrollments then
88:55 - we're talking about row stores so or row
88:58 - wise
88:59 - data organizing rows optimize for oltp
89:02 - then you have column store or columner
89:04 - data organizing columns optimize for
89:06 - olap so analytics now we have indexes a
89:09 - data structure that improves the reads
89:10 - of databases this is also shows up under
89:13 - non-relational databases but i just
89:14 - threw it here just because we have pivot
89:16 - tables it is a table of statistics that
89:18 - summarizes the data of more extensive
89:19 - table from a database spreadsheet or bi
89:22 - tool
89:23 - now talking about non-relational data
89:25 - data that has semi-structured data
89:26 - associated with schema
89:28 - new school databases so we got key value
89:31 - each value has a key designed to scale
89:32 - only simple lookups
89:34 - i like to describe a simple dumb and not
89:36 - a lot of features
89:37 - we have document primary entities xml or
89:39 - json-like data structure called a
89:41 - document columner has a table like
89:44 - structure but the data is stored around
89:46 - columns instead of rows graph data is
89:48 - represented with nodes and structures
89:49 - where relationships matter okay
89:52 - uh we're on to the third page here so
89:54 - data modeling an abstract model that
89:55 - organizes elements of data and
89:57 - standardizes how they relate to one
89:59 - another in the real world entities
90:01 - schema a formal language to describe the
90:03 - structure of data used by databases and
90:05 - data stores during the data modeling
90:07 - phase schema is generally used for when
90:09 - upfront data modeling can be foregone
90:12 - foregone i did not write that right but
90:15 - because the schema is flexible normally
90:17 - used with no skill databases data
90:19 - integrity the maintenance and assurance
90:20 - of data accuracy and consistency over
90:22 - its entire lifecycle data corruption the
90:24 - act of data not being in the intended
90:26 - state will result in data loss
90:28 - or misinformation normalization a schema
90:31 - designed to store non-redundant
90:32 - inconsistent data denormalize a schema
90:35 - that combines data so that access to
90:37 - data is fast elts or etls transform data
90:41 - from one data store to another loads of
90:43 - data in an intermediate stage doesn't
90:45 - work does not work with data lakes e-l-t
90:48 - transformations done at the target data
90:50 - store uh works with data lakes more
90:51 - common in cloud services things of azure
90:54 - app analytics okay or azure synapse
90:56 - analytics where the data is loaded and
90:58 - done uh in the actual data warehouse or
91:01 - etc a query when a user requests data
91:03 - from a data store by using query
91:05 - language to return the data result data
91:07 - source data sources where data
91:08 - originates from
91:10 - so analytics and data warehouse tools
91:12 - may be connected to various data sources
91:14 - bi tools would have data sources as well
91:16 - data consistency when data being kept in
91:18 - two different places and whether the
91:20 - date that the data exactly matches or
91:22 - does not match strongly consistent every
91:24 - time you request data you can expect
91:26 - consistent data to be returned within a
91:28 - time eventually consistent when you
91:30 - request data you may get inconsistent
91:32 - data so like stale data synchronization
91:34 - continuous stream of data that is
91:35 - synchronized by a timer or clock so
91:37 - guarantee of time asynchronous
91:39 - a synchronization continuous stream of
91:41 - data
91:42 - separated by start and stop uh stop bits
91:46 - no guarantee of time and this is
91:47 - synchronization in terms of processing
91:49 - okay data mining the extraction of
91:52 - patterns and knowledge from large
91:53 - amounts of data not the extraction of
91:55 - data itself data wrangling the process
91:56 - of transforming mapping data from one
91:58 - raw data into from
92:00 - form into another format and we're on
92:02 - the last page here so data analytics
92:04 - data analytics is examining transforming
92:06 - arranging data so that you can extract
92:08 - and study useful information key
92:10 - performance indicators probably not
92:12 - talked about the exam but i threw it in
92:13 - here because it's just important to know
92:15 - type of performance measurement that a
92:17 - company organization to determine
92:18 - performance over time then in terms of
92:20 - the types of uh analytics that we can
92:23 - utilize we have descriptive analytics
92:25 - what happened so accurate comprehensive
92:26 - like data
92:28 - effective visualization so dashboards
92:29 - reports kpis roi that's when you have
92:32 - all the information diagnostic analytics
92:34 - why did it happen drill down to
92:36 - investigate root cause sometimes they
92:38 - call that root cause analysis we didn't
92:39 - talk about that in the course but that's
92:41 - what it is focus on a subset of
92:43 - descriptive and now an analytics subset
92:45 - so it's a subset of this one up here
92:48 - okay predictive analytics what will
92:50 - happen so use historical data with
92:52 - statistics and ml probably should
92:54 - highlight that in red there for you to
92:55 - generate trends or predictions
92:57 - predictive analytics what will happen
92:59 - use hybrid data with ml to predict
93:01 - future scenarios that are exploitable
93:04 - cognitive analysts what if this happens
93:06 - so use ml and nlp to determine what if
93:09 - scenarios to create plans if they happen
93:11 - these are all really similar but the
93:13 - thing is is that they just it's it's the
93:15 - lens you put on like the the reason why
93:16 - you're doing it okay
93:18 - then we talk about run drive so storage
93:20 - and uh storage synchronization service
93:21 - for a single user and then we have
93:23 - sharepoint storage and storage
93:24 - synchronization service for an
93:26 - organization there's a little bit more
93:27 - to that but that is it for data core
93:29 - concepts
93:30 - [Music]
93:34 - let's take a look at azure synapse
93:36 - analytics and this is a data warehouse
93:38 - and a unified analyst platform we're
93:40 - going to talk more about the latter
93:42 - because like you know what a data
93:43 - warehouse at this point it's just a
93:44 - column or store
93:46 - and so here is a visual of um the data
93:48 - analytics or data synapses studio so
93:52 - here you can see there's a query going
93:53 - on so we're just querying data but
93:55 - there's a lot we can do uh on the
93:57 - unified analytics platform so we can
93:58 - perform etl and elt processes
94:01 - in a code free a visual environment so
94:03 - you don't have to write any code using
94:05 - just data from more than 95 native
94:06 - connectors
94:08 - deeply integrated with apache spark uses
94:10 - tsql queries on both your data warehouse
94:13 - and spark engines just that's what we're
94:15 - looking at there is the tsql and
94:17 - supports multiple languages so tsql
94:19 - python
94:20 - scala spark sql and net and it's
94:23 - integrated with artificial intelligence
94:25 - so ai and business intelligence tools bi
94:28 - so we could use azure machine learning
94:30 - studio or azure cognito services
94:34 - or microsoft power bi just to get a
94:36 - better visual of the entire flow here on
94:38 - the left hand side you're ingesting data
94:41 - from sources
94:42 - all the data is going to be stored on a
94:44 - data lake
94:46 - storage gen 2 here
94:48 - at the top here we have the azure
94:50 - synapse analytics studios that's where
94:52 - you're going to be doing
94:54 - the interface you're going to be working
94:55 - with and then you're going to be able to
94:57 - output the various services and notice
94:58 - here that we have sql
95:00 - and apache spark which are the different
95:02 - runtime engines this really looks like
95:04 - to me
95:05 - a data lake
95:06 - or lake house which when i was talking
95:08 - about lake house i was like azure
95:09 - doesn't have an offering but now that
95:11 - i'm looking at the screenshot this
95:12 - definitely is a data lake house so yeah
95:16 - i guess azure synapse is a data lake
95:19 - house cool
95:20 - [Music]
95:24 - all right let's talk about synapse sql
95:26 - so snapchat scale is a distributed
95:27 - version of t sql designed for data
95:29 - warehouse workloads it extends tsql to
95:32 - address streaming and machine learning
95:34 - scenarios it uses built-in streaming
95:36 - capabilities to land data from uh
95:38 - load supposed to say load data from
95:41 - cloud data sources into sql tables
95:43 - integrate ai with sql by using ml models
95:46 - to score data using tsql predict
95:48 - function and offers both serverless and
95:50 - dedicated resource models so for the
95:52 - serverless side this is great for
95:54 - unpredictable workloads so unplanned or
95:55 - bursty workloads use
95:58 - use the always available or serverless
96:00 - sql endpoint are our options for
96:02 - predictable workloads we create
96:04 - dedicated sql pools to reserve
96:05 - processing power for data stored in sql
96:08 - tables so here we're talking about
96:10 - dedicated dedicated sql pools and others
96:12 - so let's talk about those very quickly i
96:15 - couldn't even be bothered to make a
96:16 - graphic for this because it's just too
96:18 - much work so i just pulled it right from
96:19 - the docs but dedicated sql pool is a
96:22 - query service over the over the data in
96:24 - your data warehouse the unit of scale is
96:26 - an abstraction of compute power that is
96:28 - known as a data warehouse unit dwd once
96:30 - your dedicated sql pool is created you
96:32 - can import big data with simple poly
96:35 - based tsql queries and then use the
96:37 - power of this distributed query engine
96:39 - to perform
96:41 - high performance analytics then there's
96:43 - the serverless sql pools which looks
96:44 - like this and serverless sql pool is a
96:47 - query service over the data in your data
96:49 - lake scaling done automatically to
96:51 - accommodate query resource requirements
96:53 - as as topology changes over time by
96:55 - adding removing nodes or failure it
96:57 - adapts to changes and makes sure your
97:00 - query has enough resources and finishes
97:02 - successfully they're not going to test
97:03 - you on these data pool things but you
97:05 - know i just figured we'd provide a
97:06 - little more context and some more uh
97:09 - language around here just to help
97:10 - solidify what the service is but there
97:12 - you go
97:13 - [Music]
97:17 - just a couple of things that i just want
97:18 - to give extra emphasis on which is
97:20 - apache spark and data lake with synapses
97:22 - so
97:23 - it's synapse but i just keep on saying
97:25 - synapses just get used to it
97:27 - azure synapse can deeply and seamlessly
97:29 - integrate with apache spark as you can
97:31 - see here in ml models with spark ml
97:33 - algorithms and azure ml integration for
97:35 - apache spark 2.4 with built-in support
97:38 - for linux foundation delta lake there's
97:39 - the apache spark 3. so i'm surprised
97:41 - they're not up to date yet at least when
97:42 - i wrote this simplified resources models
97:45 - that freeze your you from having to
97:47 - worry about managing clusters fast spark
97:49 - startup and aggressive auto scaling
97:51 - built-in supportfor.net for spark allow
97:53 - you to reuse c-sharp expertise in
97:55 - existing.net
97:57 - code with the spark application talking
97:59 - about data lake here azure synapse
98:01 - removes the traditional technology
98:03 - barriers using sql and spark together
98:05 - and you can seamlessly mix and match
98:07 - based on your needs and expertise tables
98:09 - defined on files in the data lake are
98:10 - seamlessly consumed by spark or hive sql
98:13 - and spark can directly explore and
98:15 - analyze parquet csv tsv json file stored
98:18 - in the data lake
98:20 - fast scalable data loading between spark
98:22 - and
98:23 - sql and spark databases so there you go
98:28 - [Music]
98:30 - so a data lake is a centralized data
98:32 - repository for unstructured and
98:33 - semi-structured data and a lake is
98:35 - intended to store vast amounts of data a
98:37 - daylight generally uses object blob or
98:39 - files as its storage medium and so the
98:41 - idea is you will collect data by putting
98:43 - putting various sources in there you'll
98:44 - do transformations so change your blend
98:46 - data into new semi-structures using etl
98:49 - or elt and put it right back in the data
98:51 - lake or we could distribute it by
98:53 - allowing access to data to various
98:55 - programs and apis or publish the data
98:57 - set to a meta catalog so analysts can
98:59 - quickly find useful data now if we want
99:01 - to have an azure data lake uh you know
99:04 - you're gonna have to create a data lake
99:05 - storage but the thing is is that gen one
99:07 - is no longer really intended to be used
99:09 - the only people that should be using it
99:10 - are the people that are still in use but
99:13 - we'll focus on gen 2. so gen 2 is the
99:16 - data lake storage for azure blob storage
99:18 - which has been extended to support big
99:20 - data analytic workloads designed to
99:22 - handle petabytes of data and hundreds of
99:24 - gigabytes of throughput in order to
99:25 - efficiently access the data data lake
99:27 - storage adds a hierarchical hierarchical
99:30 - namespace to the azure blob storage and
99:32 - this is what it looks like on the
99:33 - left-hand side so
99:35 - we have two ways of access through the
99:36 - blob endpoint
99:38 - w-a-s-b-s and the dfs endpoint abfs
99:42 - these are different drivers one is for
99:44 - object one is for file but they'll both
99:46 - get you in there and they're both
99:47 - compatible with hdfs which is hadoop and
99:50 - then the hierarchical namespace you get
99:52 - access control throttling timeout
99:53 - management performance and optimizations
99:56 - okay so there you go
99:57 - [Music]
100:02 - all right let's take a look here at
100:03 - polybase and this is a data
100:04 - virtualization feature of sql servers
100:07 - and specifically we're saying msql right
100:09 - microsoft sql polybase enables your sql
100:12 - server instance to query data with tsql
100:14 - directly from sql server oracle
100:16 - teradata mongodb hadoop clusters cosmodb
100:19 - without separately installing client
100:21 - connection software and so here's a
100:23 - fancy example to show you uh how great
100:26 - this tool is and polybase allows you to
100:28 - join data from an sql server instance
100:30 - with external data prior to polybase to
100:32 - join data to external data sources you
100:33 - can either transfer half your data so
100:35 - that all the data was in one location or
100:38 - query both sources of data then write
100:40 - custom query logic to join and integrate
100:42 - the data at the client level
100:44 - so there you go
100:45 - [Music]
100:49 - let's talk about how elts happen in um
100:52 - synapse analytics so you can perform
100:54 - elts uh in synapse sql and uh within the
100:59 - snaps analytics so the fastest and most
101:01 - scalable way to load data is through
101:03 - poly base external tables and copy
101:05 - statements that's why we talked about
101:06 - polybase so with polybase and copy
101:08 - statement you can access external data
101:10 - stored in azure blob storage data lake
101:12 - store
101:13 - via the ts sql language makes sense
101:16 - because if you can do data like store
101:18 - blob storage is the same thing so here
101:20 - is a graphic where you can see we are
101:22 - ingesting from uh sql into polybase into
101:25 - our data warehouse and then we can talk
101:27 - to our data lake and do a variety of
101:29 - other things so the basic steps for an
101:31 - etl are extract the source data into
101:33 - text files load the data into blob
101:35 - storage or azure data lake store prepare
101:37 - the data for loading load the data into
101:39 - into staging tables with polybase or
101:41 - copy command transform the data and
101:43 - insert the data into the production
101:45 - tables
101:46 - [Music]
101:50 - so azure data lakes analytics is an
101:52 - on-demand analytics job service that
101:53 - simplifies big data instead of deploying
101:55 - configuring and tuning hardware you
101:56 - write queries called usql to transform
101:59 - your data and extract valuable insights
102:02 - so the idea here is that by exporting
102:04 - approximately 2.8 billion rows of tcps
102:07 - ds store sales data 500 gigabytes into a
102:09 - csv it took less than seven seven
102:11 - minutes and importing a full terabyte of
102:13 - source uh took uh under with a connector
102:16 - took under less than six hours so the
102:17 - idea is that it's pretty darn fast and
102:20 - just to show you where it is it's over
102:21 - here in the middle so the idea is that
102:23 - this tool just lets you do run uh
102:26 - queries on your data lake okay
102:29 - let's talk about usql so usql is a
102:30 - structured query language included with
102:32 - data like analytics to perform queries
102:34 - on your data lake you can see there's a
102:36 - bunch of stuff in here like extract and
102:38 - stuff um and so usql can query and
102:40 - combine data from a variety of data
102:41 - sources including azure data lake
102:43 - storage blob storage sqldb data
102:46 - warehouse sql server instances running
102:48 - on azure vm you can install the azure
102:50 - data lake tools for visual studio to
102:52 - perform you sql jobs on your azure data
102:54 - lake i didn't see it in um
102:57 - azure data studio but it might be there
102:59 - too
103:00 - [Music]
103:04 - hey this is andrew brown from exam pro
103:06 - we're taking a look here at the azure
103:07 - synapse and data lake cheat sheet for
103:08 - the dp900 let's jump into it a data lake
103:11 - is a centralized data repository for
103:12 - unstructured and semi-structured data a
103:14 - data lake is intended to store vast
103:16 - amounts of data data likes generally use
103:17 - objects blobs or files as its storage
103:20 - medium for the azure data lake storage
103:23 - generation 2 this is an azure blob
103:25 - storage which has been extended to
103:27 - support big data analytics workloads it
103:29 - does this via its hierarchical namespace
103:31 - and what the oracle namespace gives you
103:33 - is acls throttle management performance
103:35 - optimizers you can access your data like
103:37 - via the wasb protocol blob or abfs which
103:42 - is a file system protocol azure synapse
103:45 - analytics is a data warehouse and
103:46 - unified analytics platform has two
103:48 - underlying transformation engines so we
103:49 - have esqel pools and spark pools synapse
103:52 - sql is tsql but designed to be
103:53 - distributed sql dedicated pools is
103:56 - reserve compute for processing
103:57 - serverless endpoints on-demand no
103:59 - guarantee of performance data stored on
104:02 - azure data lake store generation 2
104:05 - operations are performed within the
104:06 - azure synapse studio
104:09 - polybase enables your sql server
104:11 - instance to query data with tsql used to
104:13 - connect many relational database sources
104:15 - probably use with other services not
104:16 - just with azure snaps but there you go
104:23 - all right let's take a look at azure
104:25 - blob so blob storage is an object store
104:27 - that is optimized for storing massive
104:28 - amounts of unstructured data
104:30 - unstructured data is data that doesn't
104:32 - adhere to a particular data model or
104:33 - definition such as text or binary data
104:35 - as your blobs are composed of the
104:36 - following components so we have storage
104:38 - accounts which is a unique namespace in
104:40 - azure for your data you have containers
104:42 - which is similar to a folder and a file
104:43 - system and then the actual data being
104:45 - stored so azure storage supports three
104:48 - types of blobs we've got block blobs so
104:50 - these store text environment data made
104:52 - up of blocks of data that can be managed
104:53 - individually so we're up to 4.75
104:55 - terabytes
104:57 - we have a pen blobs these optimize for
104:59 - append operations ideal for scenarios
105:00 - such as logging data from virtual
105:02 - machines and we have page blobs these
105:04 - store random access files to up to eight
105:07 - terabytes in size and store virtual hard
105:09 - drives vhd files and serve as disks for
105:12 - azure virtual machines and there you go
105:18 - all right let's take a quick look here
105:19 - at azure files so azure files is a fully
105:22 - managed file share in the cloud and a
105:23 - file share is a centralized server for
105:25 - storage that allows for multiple
105:26 - connections it's like having one big
105:28 - shared drive that everyone
105:30 - you know virtual machines can work on at
105:32 - the same time so here's an example or a
105:34 - diagram of it so to connect to the file
105:36 - share you use a network protocol such as
105:37 - server message block smb or network file
105:40 - system nfs when a connection is
105:42 - established the files shares file system
105:45 - will be accessible in the in the
105:46 - specific directory within your own
105:48 - directory tree this is known as mounting
105:50 - so some use cases here completely
105:52 - replace your supplement your on-premise
105:54 - file servers
105:56 - nas drives a lift and shift of your
105:58 - on-prem storage to the cloud via classic
106:01 - lift or hybrid lift lift and shift means
106:03 - when you move workloads without
106:04 - re-architecting so importing local vms
106:06 - to the cloud a classic lift would be
106:08 - where both the application and its data
106:10 - are moved to azure a hybrid lift is
106:12 - where the application data is moved to
106:14 - azure files and the application
106:15 - continues to run on premise we have
106:17 - simplified the cloud deployment so
106:19 - shared application settings so multiple
106:21 - vms and developer workstations need to
106:23 - access the same config files or
106:25 - diagnostic share we have all vms logged
106:28 - to the file share developers can mount
106:30 - and debug all logs in a centralized
106:32 - place we can dev test and debug so
106:34 - quickly share tools for developers
106:36 - needed for local environments we can do
106:38 - containerization so you can have azure
106:40 - files to persist volumes for stateful uh
106:42 - containers super useful when you're
106:43 - working with containers why use azure
106:46 - files instead of setting up your own
106:48 - file share well shared access so already
106:50 - set up to work with the standard
106:51 - networking protocols it's fully managed
106:53 - so it's kept up to date with security
106:55 - patches designed to scale uh it has
106:57 - scripting tools to automate the
106:58 - management and creation of file uh files
107:01 - shared with azure api and powershell and
107:03 - it has resilience so it's built to be
107:05 - durable and always
107:06 - working so there you go
107:08 - [Music]
107:12 - hey this is andrew brown from exam pro
107:14 - and we're looking at azure account
107:16 - storage cheat sheet and this is a very
107:17 - short section so azure storage accounts
107:19 - an umbrella service for various forms of
107:21 - managed storage you have azure tables
107:23 - blob storage and files there's of course
107:25 - cued and some other things in there but
107:26 - these are the three that we care about
107:28 - azure blob storage object storage is
107:30 - distributed across many machines
107:32 - supports three types so we got blah blah
107:34 - so store text and binary data blocks of
107:36 - data can be managed individually up to
107:38 - 4.7 terabytes append blocks optimize for
107:40 - append operations ideal for logging page
107:43 - blobs store random access files up to 8
107:45 - terabytes in size azure files is a fully
107:48 - managed file share in the cloud to
107:49 - connect to the file share and network
107:51 - protocols used either smb or nfs azure
107:55 - storage explorer a standalone
107:56 - cross-platform app to access various
107:58 - storage formats within the azure storage
108:00 - accounts and there you go
108:06 - let's talk about business intelligence
108:07 - tools so bi is both a data analysis
108:10 - strategy and technology for business
108:11 - information the most popular bi tools
108:14 - are tableau microsoft power bi and
108:16 - amazon quick site we're going to
108:17 - obviously be focusing on power bi
108:19 - because that's what azure would like us
108:20 - to focus on
108:21 - bi helps organizations make data-driven
108:23 - decisions by combining business
108:25 - analytics data mining data visualization
108:27 - data tools infrastructure and best
108:29 - practices and there's the logo of the
108:31 - three so you know what it is and now
108:33 - we'll jump into power bi
108:34 - [Music]
108:38 - hey it's andrew brown from exam pro and
108:40 - we're taking a look at microsoft power
108:42 - bi which is a business intelligence tool
108:44 - for visualization business data and
108:46 - here's a screenshot of the power bi
108:47 - desktop and power bi can get a little
108:50 - bit confusing because they have a lot of
108:51 - things under the power bi name but i'll
108:53 - break them down here so it's nice and
108:55 - clear so the power bi desktop is a way
108:57 - to design and adjust reports the power
109:00 - bi mobile is a
109:01 - view reports on the go on your phone
109:03 - power bi service sometimes called the
109:05 - power bi portal is to access some
109:07 - modified reports in the cloud and power
109:08 - bi embedded is a way to embed power bi
109:11 - components into your applications and
109:14 - usually you need to get data into a
109:16 - power api and so this is one of the most
109:18 - powerful reasons why people like using
109:20 - it it's because it ingests with so many
109:22 - data sources so in here this is a
109:23 - desktop one you go in and you can go
109:26 - under azure and there's all like every
109:27 - azure service you'd ever want if you go
109:29 - the database tab there's a lot of
109:30 - database integrations for postgres mysql
109:33 - everything it's crazy
109:35 - and so power bi can directly integrate
109:36 - with azure services as you saw here i
109:38 - couldn't be bothered to make a graphic
109:40 - here but you know here you can see you
109:42 - can get things from hd insights sql
109:44 - databases
109:45 - account storage machine learning stream
109:47 - analytics event hubs things like that
109:50 - uh so just to compare the two because
109:52 - these are the most important services is
109:53 - the desktop and the service and they're
109:55 - very easy to get mixed up so power bi
109:57 - desktop is a dell is is a downloadable
109:59 - free windows application and installed
110:01 - on a local windows computer if you're on
110:03 - a mac you cannot use it sorry or linux
110:06 - either report uh it it can it has
110:08 - reports or sorry so the role that
110:11 - somebody would be using would be you
110:12 - would be a report designer and you'd use
110:15 - uh the desktop application to publish
110:17 - power bi reports to the power bi service
110:19 - okay and power bi service is a
110:21 - cloud-based service where users view and
110:23 - interact with reports users in power bi
110:26 - service can edit the reports and create
110:27 - visuals based on the existing data model
110:29 - and they can share and collaborate with
110:31 - co-workers so just looking at the
110:33 - overlapping services power bi desktop
110:35 - has many data sources transforming
110:37 - shaping and modeling measures calculated
110:39 - columns python themes rls creation
110:42 - then on the power bi server side you
110:44 - have some data sources that you can
110:46 - ingest
110:47 - dashboards that is the key thing for
110:49 - power bi services that you get
110:50 - dashboards you don't get that on the
110:52 - power bi desktop part apps and
110:54 - workspaces sharing data flow creation
110:56 - paginated reports rls management gateway
110:59 - connections paginate reports is actually
111:02 - with the
111:03 - builder which you have to download so
111:04 - i'm not sure why it's in there both you
111:07 - get reports visualization security
111:08 - filters bookmarks q a and r visuals but
111:12 - just
111:13 - make a note here that you use the power
111:16 - bi desktop to create reports and then
111:19 - they're get they can be used in power bi
111:21 - service to create dashboards okay
111:24 - [Music]
111:28 - let's talk about data visualizations and
111:29 - chart types and specifically power bi
111:31 - ones so power bi has many kinds of
111:33 - visualizations we'll cover the most
111:35 - common ones but you can see over here
111:37 - like look at all these little little
111:38 - squares that represents all different
111:40 - kinds of visualizations you can make and
111:42 - even with them they're highly
111:44 - configurable okay so let's go and look
111:46 - at bar and column charts so see how a
111:48 - set of variables changes across
111:50 - different categories we've all seen bar
111:51 - charts it supports stacked ones and
111:54 - bar charts stacked side by side or
111:56 - horizontal ones you know the you know
111:58 - what bar charts are line charts overall
112:00 - shape of an entire series of values so
112:02 - they're just lines
112:04 - uh we have a matrix so that is where you
112:06 - have a tabular structure that summarizes
112:08 - the data you have key influencers the
112:10 - major contributors to a selected result
112:12 - or value and that one kind of has a very
112:14 - cool looking visualization
112:16 - you have tree map charts of colored
112:18 - rectangles with size representing the
112:20 - relative value of each item
112:22 - we have scatter graphs of represent
112:24 - relationships between two numerical
112:26 - values so you have an x and y it's
112:27 - basically a bunch of dots on a graph you
112:29 - have bubble chart it's the same thing
112:31 - but the the dots now are bubbles and the
112:33 - larger the bubble can represent a third
112:35 - dimension
112:36 - you have dot plot charts and these are a
112:38 - little bit confusing to look at but they
112:40 - are basically bubble charts but you
112:42 - they're organized based on an x-axis so
112:44 - you're basically putting those into
112:45 - categories i'm always confused when i
112:48 - look at that one but that's just one and
112:50 - one more for us here is a field map so
112:52 - you have a geographic map where
112:53 - different areas can be filled
112:55 - so like here you have states of
112:57 - different colors that represent things
112:59 - it could be gradients
113:00 - there's a lot you can do with maps and
113:02 - geographical maps and that again is not
113:05 - all the data visualizations but the most
113:06 - common ones you'll come across
113:12 - all right let's take a look here at
113:13 - power bi embedded and honestly this
113:15 - probably won't show up on the exam it's
113:16 - just that when you look use azure and
113:18 - you type in power bi it shows up in the
113:19 - console and i was like what is this
113:21 - thing and i thought this was kind of
113:23 - interesting and i feel like it's it's
113:24 - relevant so that's why i have it in here
113:26 - so azure power bi embedded is a platform
113:28 - as a service analytics embedding
113:30 - solution that allows you to quickly
113:31 - embed visuals reports dashboards into an
113:33 - application for independent software
113:35 - vendors it enables you to visualize
113:37 - application data rather than building
113:39 - the service yourself for developers you
113:40 - embed reports and dashboards into an
113:42 - application for their customers to use
113:44 - azure power bi if you need a power bi
113:46 - pro user account you need to create an
113:48 - app workspace and you need to choose a
113:50 - capacity so either i guess it'd be like
113:52 - billing work via capacity based or
113:55 - hourly metric modes there you go
113:57 - [Music]
114:01 - let's take a look at power bi
114:02 - interactive reports and these and so
114:04 - basically when you're using power bi
114:06 - desktop and you can generate reports in
114:08 - the in the portal or service but uh the
114:10 - reports
114:12 - are interactive so if you're getting
114:13 - confusing like there's power bi reports
114:15 - and interact reports
114:16 - basically by default everything's
114:18 - interactive with power bi okay
114:20 - so here is an example of one i just
114:23 - downloaded the uh like the example one
114:25 - that uh microsoft provides
114:27 - and as you can see they uh like in the
114:30 - middle of it there's like a little knob
114:31 - so that kind of gives you an indication
114:33 - of interactivity or there's like other
114:34 - buttons here so like if you go here you
114:37 - can actually click between map and
114:38 - tabular i believe you can move this
114:40 - range around just get different
114:42 - information so they're highly
114:43 - interactive
114:44 - uh then they're extremely stylized as
114:46 - you can see you can make them look
114:47 - really really good a report can contain
114:49 - many pages and you can assemble reports
114:52 - as easy as choosing a visualization and
114:54 - dragging it out so you take that just
114:56 - drag it out where you want to go and
114:57 - customize it from there
114:59 - now underneath what you can do is you
115:01 - can see the underlying data so it's just
115:04 - like tabular data and all the tables and
115:06 - fields that populate it and you can and
115:09 - i think with like a pro version you can
115:10 - modify it and so you cannot do these
115:12 - with dashboards so we're using the power
115:14 - bi service you're not going to get
115:15 - access to this
115:17 - when you're looking at data modeling
115:18 - it's again in power bi desktop you can
115:21 - see the relationships between models and
115:23 - modify them and do things with them
115:24 - again you cannot do this with dashboards
115:27 - and that's a key thing you need to
115:28 - understand between the interactive
115:30 - reports and the dashboards
115:35 - let's take a quick look here at power bi
115:37 - service and also dashboards which is
115:39 - very important for this so power bi is a
115:41 - cloud-based service where users view and
115:43 - interact with reports and where they can
115:45 - create dashboards and so here is um a
115:48 - screenshot of me logged into power bi if
115:50 - you wanna know how to get there you go
115:52 - app.powerbi.com
115:54 - and uh if you already have you have to
115:56 - even if you have a microsoft account you
115:58 - have to fill in a form and then it
115:59 - activates the service and you can go and
116:02 - explore some dashboards and reports
116:04 - right off the bat for free so it's very
116:06 - easy to jump into
116:07 - one of the concepts that's very
116:08 - important with power bi service is
116:10 - dashboards and so before we talk about
116:12 - that let's talk about what a tile is a
116:13 - tile is a snapshot of data pinned into
116:15 - your dashboard so here's an example of
116:17 - data a tile can be created from a report
116:18 - a data set a different dashboard qa q
116:21 - and a box excel sql server reporting
116:24 - service ssrs
116:26 - and many many more looking at a
116:28 - dashboard is a single page often called
116:31 - a canvas that tells a story through a
116:34 - visualization so there it is the
116:35 - visualizations you can see on the
116:36 - dashboard are called tiles you can pin
116:39 - tiles to a dashboard from reports okay
116:42 - [Music]
116:47 - it is very very very important that we
116:49 - know the difference between reports and
116:50 - dashboards so this isn't going to be a
116:52 - fun slide but we'll have to go through
116:54 - it and work our way through so let's
116:55 - talk about the difference for
116:56 - capabilities they both have pages but
116:58 - dashboard has a single page and reports
117:00 - has multiple pages for data sources one
117:03 - or more reports
117:05 - you can have one or more reports and one
117:08 - or more data sets per dashboard
117:10 - and for report a single data set per
117:12 - dashboard for filtering you can't filter
117:14 - or slice for filtering out reports many
117:17 - different ways to filter highlight and
117:18 - slice
117:19 - you can set alerts for dashboards you
117:21 - cannot for reports for features you can
117:25 - set one dashboard as your featured
117:27 - dashboard reports there's no such thing
117:29 - as a featured report
117:31 - you can see the underlying data set
117:33 - tables and fields so absolutely not for
117:35 - dashboards absolutely yes for reports
117:36 - for customization new and for reports
117:39 - you got tons of customization so there
117:42 - you go
117:43 - [Music]
117:47 - all right let's talk about paginated
117:48 - reports which are reports designed to
117:50 - fit into page formats so they can be
117:52 - printed or shared the data
117:54 - display of all data are tables which can
117:56 - span multiple pages so rdls is an xml
117:59 - representation of an sql server
118:01 - reporting service so that's an ssrs
118:04 - report definition file a report
118:05 - definition contains data retrieval and
118:07 - layout information for report pattern
118:08 - reports are just a visualization of dot
118:11 - rdl files so power bi report builder is
118:14 - used to design pixel perfect
118:16 - remember that word pixel perfect they
118:18 - really use that a hundred times over
118:20 - passionate reports using power bi report
118:22 - builder it is a tool specifically
118:24 - designed for creation of patching
118:25 - reports so if you want to figure out how
118:27 - to download this power bi thing within
118:30 - your power bi service go to the top
118:32 - right corner go to download i don't know
118:34 - why that's animated but it worked out
118:36 - fine and so once you're in there
118:39 - uh you will download the file and you'll
118:41 - install this really old looking software
118:44 - but i guess the thing is is that this is
118:46 - a huge pain point for companies i guess
118:48 - and they really make a huge emphasis on
118:50 - it so i guess we need to know what it is
118:52 - [Music]
118:57 - hey this is andrew brown from exam pro
118:58 - and we are looking at the power bi cheat
119:00 - sheet for the dp900 let's jump into it
119:02 - so the first is business intelligence or
119:04 - bi which is both a data analysis
119:06 - strategy and technology for business
119:08 - information helps organizations make
119:10 - data driven decisions now we're talking
119:13 - about power bi so power bi desktop a
119:15 - desktop app to design interactive
119:16 - reports from various data sources can be
119:18 - published to the power bi service then
119:20 - you have the power bi service also known
119:22 - as the power bi portal a web app to view
119:24 - reports and create interactive
119:26 - shareable dashboards by pinning various
119:28 - data sets and reports visualizations you
119:31 - have power bi mobile a mobile web app to
119:33 - view reports on the go
119:34 - power bi report builder a windows app
119:37 - that builds pixel perfect printable
119:39 - reports used to build page data reports
119:42 - power bi embedded embed power bi
119:44 - visualizations into web apps interactive
119:46 - reports reports in power bi drag
119:48 - visualizations load data from many data
119:50 - sources both
119:51 - in desktop and and service meaning like
119:54 - you can do you can make reports in both
119:57 - power bi desktop and power bi service
119:59 - okay
120:00 - paginate reports pixel perfect printable
120:02 - report files uh tabular data laid out in
120:05 - page format dashboards build shareable
120:07 - dashboards by pinning various power bi
120:09 - visualizations a single page report
120:10 - basically designed for a screen only for
120:13 - the power bi service dashboard tiles or
120:16 - just tiles is a representation or
120:18 - represent a visualization that has been
120:20 - pinned to a dashboard it could be a
120:22 - bunch of other things but that's what
120:23 - the key thing it is visualization is a
120:25 - visualization uh is a chart or graph
120:27 - that's backed by it says my but you see
120:29 - by a data set and uh whoops we went to
120:32 - the next part but that's it for power bi
120:34 - [Music]
120:38 - all right let's take a look at
120:39 - structured query language which uh
120:41 - stands for sql it's designed to access
120:43 - maintain data for a relational database
120:44 - management system on rdbms
120:47 - we use sql to insert update delete view
120:50 - data from our data databases tables and
120:53 - sql can join many tables and include
120:55 - many functions to transform uh the final
120:57 - output of results on the right hand side
120:58 - that is a real query that i use in my
121:01 - postgres database to um
121:03 - grab exam sets so if you're on the xampp
121:05 - pro platform and you're doing a
121:07 - particular set of an exam this query
121:09 - gets that relative information and you
121:11 - can see that it's polling if you look
121:13 - down below here
121:15 - it's joining in tag information and then
121:17 - it has like sub queries and stuff so
121:18 - it's a very complex query and doing that
121:20 - formatting the sql syntax was
121:22 - standardized as iso 9075 that won't show
121:25 - up on your exam but it's good to know
121:27 - relational databases will mostly adhere
121:29 - to the standard while adding in their uh
121:31 - not one but own database specific
121:34 - features sql's highly transferable skill
121:36 - and we see sql being used in
121:38 - non-relational databases provide a
121:40 - popular and familiar querying tool so
121:42 - it's something you definitely want to
121:43 - know how to do
121:44 - [Music]
121:48 - all right let's compare olap to oltp so
121:52 - online transactional processing versus
121:54 - online analytical processing when we're
121:56 - talking about ltp we're generally using
121:58 - databases so databases is built to store
122:00 - current transactions and enables fast
122:02 - access to specific transactions for
122:03 - ongoing business processing so think of
122:06 - uh you know any kind of sql server and
122:09 - then on the right hand side we have data
122:11 - warehouses a data warehouse is built to
122:12 - store large quantities of historical
122:14 - data and enable fast complex queries
122:16 - across all the data so when we visually
122:18 - look at the oltp we have a bunch of
122:20 - small transactions that are evenly
122:23 - distributed so they look pretty similar
122:24 - in the read and writes and then for data
122:26 - warehouse we have very very few
122:30 - retransactions and uh we have large
122:33 - payloads i think that the arrows are
122:35 - supposed to be pointing this way but
122:37 - that's okay it's not a big deal
122:39 - so when we're talking about databases
122:40 - you have a single data source you have
122:42 - short transactions small and simple
122:44 - queries with an emphasis on rights many
122:45 - transactions late it's latency sensitive
122:48 - and you have small payloads on the olap
122:51 - side we have multiple data sources so
122:53 - you're ingesting data long transactions
122:55 - long and complex queries with an
122:56 - emphasis on reads fewer transactions or
122:59 - very few
123:00 - and throughput sensitive and large
123:02 - payloads the use case over here would be
123:04 - general purpose adding items to your
123:05 - shopping cart would be an example a use
123:07 - case on the analytics side would be
123:08 - generating reports so there you go
123:14 - [Music]
123:15 - all right let's take a look at some open
123:16 - source relational databases that we know
123:18 - we are going to definitely encounter on
123:19 - azure starting with mysql which was
123:21 - created by my school a b i believe
123:23 - that's like a switzerland or a swiss
123:25 - company and they they are required by
123:27 - sun microsystems and then they are
123:29 - required by oracle and mysql was or is
123:32 - an open source project uh so mysql is a
123:35 - pure relational database rdbms it is a
123:38 - simple database which makes it easy to
123:40 - set up using maintain has multiple
123:41 - storage engines so in odb and my ism
123:45 - when it says that there's multiple they
123:46 - just mean there's two because i don't
123:48 - know of any other than those two but
123:50 - it's the most popular relational
123:51 - database the reason why it's been around
123:53 - forever it was one of the
123:54 - earliest mysql databases that was open
123:57 - source which is a very important thing
123:59 - to note
124:00 - um you know and it's just very easy to
124:02 - use
124:03 - mario db is a fork of mysql by the
124:05 - original creators of my
124:07 - mysql ab after oracle was acquired my
124:10 - school
124:11 - required mysql there was a concern that
124:13 - oracle may change the open source
124:15 - licensing or stop future my school from
124:18 - being free to use if you know oracle
124:20 - they'll like to
124:21 - charge charge you for their stuff
124:23 - and oracle has their own database and so
124:25 - you know there was a lot of fear around
124:27 - that and the thing is is that when my
124:29 - sql ab sold their database to sun
124:32 - microsystems it it's because they
124:33 - trusted sun but then they didn't know
124:36 - that sun was going through a lot of
124:37 - financial difficulties and then
124:39 - literally a year later some was acquired
124:40 - by oracle
124:42 - it's just how it goes eh so
124:44 - they would have never sold to oracle
124:46 - originally that's why we have mario db
124:48 - um
124:49 - then we have postgres which evolved from
124:51 - ingress
124:52 - the ingress project at the university of
124:54 - california postgres is an object-related
124:57 - relational database so o-r-d-b-m-s
125:01 - it just has a single storage engine
125:03 - which i guess is the ingress engine
125:05 - right here and so it's the most advanced
125:08 - relational database it can support full
125:09 - text search table inheritance triggers
125:11 - rows data types request slot you know
125:13 - they say the most advanced i mean it's
125:15 - more advanced than mysql
125:17 - postgres is the database i love to use
125:19 - it's such a great service i don't
125:21 - understand how it's object relational i
125:23 - think it's just how they store the data
125:24 - underneath and that's what makes it so
125:26 - flexible but i can tell you that
125:28 - postgres is a lot easier to use like
125:30 - initially mysql the syntax is easier but
125:33 - postgres in terms of data modeling is a
125:35 - lot easier because you could just create
125:37 - columns you don't have to worry about
125:38 - them whereas like mysql you've got to
125:40 - fiddle around with the data types it's
125:41 - very frustrating or there's like serious
125:42 - limitations on rows so yeah that's the
125:45 - two there okay and if you want to deploy
125:47 - these on azure it's really simple uh you
125:49 - just type in the name you go mario or
125:52 - mysql or postgres and then you just do
125:54 - azure database for mariodb and you just
125:56 - launch a server okay
126:02 - let's talk about read replicas for azure
126:04 - databases here so a read replica is a
126:06 - copy of your database that is kept
126:08 - synced with your primary database and
126:10 - this additional database is used to
126:12 - improve read contention if you've never
126:14 - heard the word contention before it
126:16 - means heated disagreement uh so the idea
126:19 - is that if you have a lot of reads and
126:21 - it's and it's
126:23 - hurting the database you can offload
126:24 - those reads to your secondary database
126:27 - that's dedicated specifically for read
126:29 - operations so re-replicas can be applied
126:31 - to azure sql and the managed instances
126:34 - so i guess you can't do it with the
126:35 - azure vms
126:37 - uh like the virtual machines the sql vms
126:40 - that's what they're called you can have
126:42 - multiple rewrite for a database i can't
126:44 - remember what the range is maybe it's
126:45 - between two to six i don't think that
126:46 - matters for the exam and just kind of a
126:48 - visual you have your read and writes
126:50 - that go to your primary and then a lot
126:51 - of your reads go to your read replicas
126:54 - and a very common use case to have a
126:55 - re-replica is so that you can use it as
126:57 - an olap when you are a very small size
127:00 - i'm not going to talk about an exam but
127:02 - i just know from a practical standpoint
127:03 - that's something that i've done multiple
127:05 - times over the years so there you go
127:06 - [Music]
127:10 - all right let's take a look at scitis on
127:12 - azure and honestly i don't know if it's
127:14 - cetus situs sometimes i want to say
127:16 - citrus but
127:19 - i could not find a pronunciation for it
127:21 - so i'm going to call it situs and it is
127:23 - an open source postgres extension that
127:24 - transform postgres into a distributed
127:26 - database and so situs extends postgres
127:29 - to provide better support for database
127:31 - sharding real-time queries multi-tenancy
127:34 - which is super useful
127:35 - time time series workloads and if you go
127:38 - to azure postgres and you see hyperscale
127:41 - option it really is just using siteis so
127:44 - um this is a really really really really
127:46 - good service if you are using postgres
127:48 - it's one of the
127:49 - few reasons i would consider using azure
127:51 - for my database because i use postgres
127:53 - as my primary one this will absolutely
127:55 - not show up on the exam but i think it's
127:57 - very useful to know what the service is
127:59 - [Music]
128:03 - all right let's take a look at the azure
128:05 - sql family when we say sql we're talking
128:07 - about microsoft's version of sql
128:10 - and if you search sql you'll see a bunch
128:13 - of stuff here even more than this and it
128:15 - can get really confusing but you
128:16 - absolutely need to know this for the
128:17 - exam you need to know the difference
128:19 - between these three main services
128:21 - so at the top you have sql server on
128:23 - azure virtual machines or sql you'll
128:26 - just see like vm for desktop vm a lot
128:28 - when you need os level control and
128:30 - access when you need to lift and shift
128:32 - your workloads to the cloud when you
128:34 - have an existing sql license and you
128:36 - want to save money via the azure hybrid
128:38 - benefit this is when you're going to use
128:40 - that okay if you've never heard the term
128:42 - lift and shift the idea is that on your
128:44 - on-premise environment you're running a
128:46 - virtual machine that is your database
128:48 - and you can literally save a virtual
128:50 - image import that into azure and it runs
128:52 - exactly how it did
128:54 - on your on premise so you don't get a
128:56 - lot of the advantages of the cloud but
128:58 - the idea is it's the easiest way to get
128:59 - onto the cloud right
129:01 - the next option is sql managed instance
129:03 - this is when you have an existing
129:04 - database you want to modernize it's the
129:07 - broadest sql server engine compatibility
129:09 - highly available just disaster recovery
129:11 - automated backups ideal for most
129:12 - migrations to cloud so the thing is if
129:14 - you're going to do a lift and shift
129:17 - you can you can
129:19 - kind of go to esco manage so you
129:20 - probably have to do a transformation
129:21 - some kind of like etl job or something
129:23 - to go into here but the idea is that
129:25 - there's a lot of different versions of
129:28 - um
129:29 - of uh mssql depending on how old it is
129:32 - and stuff like that so if you aren't
129:34 - going to be bringing your license or
129:35 - need os level you really want to be
129:36 - using this one because you get all the
129:38 - built-in scalability stuff right
129:40 - the third one is azure sql database this
129:42 - is a fully managed sql database designed
129:44 - to be fault tolerant built in disaster
129:46 - recovery hive it's highly available
129:49 - designed to scale uh and it's the best
129:51 - option um but again you know if you have
129:54 - an older database maybe you could do a
129:55 - transformation to it
129:56 - uh and then underneath it has sql
129:58 - servers i thought this was a fourth
130:00 - option but really when you go to azure
130:02 - sql and launch it you actually have to
130:03 - create a server because you can have
130:05 - multiple sql servers associated to a
130:07 - database
130:08 - and there's things called like what's it
130:09 - called like elastic pool or something
130:11 - like that so
130:12 - that's just the underlying server for
130:13 - the azure sql database it's not a
130:15 - service in itself
130:17 - but it's just a component of that
130:18 - service okay even though you can go in
130:20 - the ui and see a list there makes it
130:22 - really really really confusing but yeah
130:24 - there you go
130:30 - so azure elastic pools is a feature of
130:32 - azure sql and it allows you to uh there
130:35 - are simple cost effective solutions for
130:37 - managing and scaling multiple databases
130:39 - that have varying and unpredictable
130:41 - usage demand so databases in elastic
130:44 - pool are on a single server and share a
130:46 - set number of resources at a set price
130:48 - elastic pools in azure sql enable sas
130:50 - developers to optimize the price
130:52 - performance of a group databases within
130:54 - a prescribed budget while delivering
130:55 - performance elasticity for each database
130:57 - why would somebody want to do this
130:59 - because like it doesn't seem like a good
131:00 - practice to put a bunch of databases on
131:02 - a single server it's more like you'd
131:04 - rather want a database to be distributed
131:06 - across
131:06 - servers so
131:08 - if you're running a sas product software
131:10 - as a service you'll have multi-tenancy
131:12 - meaning that each person has their own
131:14 - database there's different levels of
131:16 - tenancy but if you did give let's say
131:18 - you had a company and um
131:20 - or like you had five large clients and
131:22 - they use the same software and they're
131:23 - all varying sizes
131:25 - but they're not large enough to justify
131:27 - their own server this would be the
131:28 - service for you right where you are
131:30 - constantly spinning up databases per
131:32 - customer
131:33 - um but i don't think i would ever use
131:35 - this in practicality and i am a
131:36 - multi-tenant sas but uh it's nice that
131:38 - they provide that option so there you go
131:40 - [Music]
131:44 - hey this is andrew brown from exam pro
131:46 - and we are on to the relational database
131:48 - cheat sheet uh and so let's jump into it
131:51 - so structure query language sql designed
131:53 - to access and maintain data for a
131:55 - relational database management system
131:57 - online transaction processing keyword
131:59 - there is transaction so frequent and
132:01 - short queries for transactional
132:02 - information so databases any kind of
132:04 - generic workload or web app online
132:06 - analytical processing so complex queries
132:09 - for large databases to produce reports
132:11 - and analytics so think data warehouse
132:14 - on to the open source relational
132:16 - databases we got mysql a pure relational
132:18 - database easy to set up most popular
132:21 - open source relation relational database
132:23 - definitely something i started off with
132:24 - mariodb is a fork of mysql postgres is
132:27 - an object
132:28 - relational database now my favorite
132:31 - relational database to use is more
132:32 - advanced and well liked among developers
132:35 - read replicas is a duplicate of your
132:37 - database in sync with the main to help
132:39 - to reduce reads on your primary database
132:41 - now talking about azure sql it's an
132:43 - umbrella service for uh for different
132:45 - offerings of mssql databases hosting
132:47 - services so we have sql vm so for lift
132:51 - and shift when you want os access and
132:53 - control or you need to bring your own
132:55 - license for azure hybrid benefit
132:57 - manage sql for lift and shift when your
133:00 - broadest when you need the broadest
133:01 - amount of compatibility with sql
133:02 - versions mssql in particular you can use
133:05 - uh
133:06 - manage sql on on-premise by using azure
133:09 - arc it gives you many of the benefits of
133:11 - a fully managed database but it's not as
133:13 - good as the azure sql database which is
133:16 - a fully managed sql database has a few
133:18 - options here you can run it as a single
133:19 - server run it as a database which is a
133:21 - collection of servers run in an elastic
133:23 - pool so databases of different sizes
133:25 - residing on this on one server to save
133:29 - cost
133:30 - uh then we have connection policies so
133:32 - we have three modes we got default so
133:34 - choose proxy or default initially
133:35 - depending if the server is within or
133:37 - outside the azure network we have proxy
133:39 - outside the azure network proxy through
133:41 - a gateway
133:42 - it's important to remember to listen on
133:44 - port 1443 this might show up on your
133:46 - exam so remember this port 1443 when
133:49 - connecting via proxy mode through a
133:51 - gateway outside the azure network and
133:52 - then last is redirect redirected with
133:54 - the azure network and that's the
133:55 - recommended way to do it and i just want
133:57 - to point out for these three here i
133:59 - didn't write it in here but remember
134:01 - that this one here is for
134:02 - infrastructures code this one is for
134:04 - platform as a service and this one's for
134:05 - platform as a service
134:10 - okay all right let's talk about tsql
134:13 - which stands for transact sql and it's a
134:15 - set of programming extensions from
134:17 - sybase and microsoft that added feature
134:19 - several features to the structured query
134:21 - language if you've never heard of cybase
134:24 - i think the original company that
134:25 - actually made the microsoft sql database
134:28 - and then maybe microsoft bought them out
134:30 - or et cetera
134:31 - but there's a long history there so tsql
134:34 - expands on the sql standard to include
134:36 - procedural programming local variables
134:38 - very various support functions for
134:39 - string processing date processing
134:42 - mathematics changes to the delete and
134:43 - update statements for the microsoft sql
134:46 - servers there are five groups of sql
134:48 - commands and honestly there's five
134:50 - groups for regular sql
134:53 - actually normally they'll just say
134:55 - even for this exam they're only going to
134:56 - tell you of about the definition
134:58 - manipulation one but i'm going to tell
135:00 - you all of them because you should know
135:01 - all of them it really helps to know them
135:03 - so the first is data definition language
135:05 - so this is
135:06 - ddl used to define
135:09 - the database schema
135:10 - we have data query language dql used for
135:13 - performing queries on the data
135:16 - data manipulation language dml
135:19 - manipulation of the data in the database
135:21 - data control language dcl rights
135:23 - permissions and controls of the database
135:25 - transaction control language tcl
135:27 - transactions within the database and so
135:29 - now that we've covered what tsql is
135:32 - let's dive in and actually look at all
135:34 - these types of documents
135:39 - let's start off with the data definition
135:41 - language which is
135:42 - sql syntax commands for creating and
135:44 - modifying the database or database
135:45 - objects so tables index views store
135:48 - procedures functions and triggers the
135:50 - first is the create
135:52 - command here and so we can create a
135:53 - database or database object so here you
135:55 - would just say create whatever it is you
135:57 - want to create table database etc so
136:00 - here we're creating a table called users
136:01 - and we're providing those fields okay
136:04 - then we have alter so this alters the
136:06 - structure of an existing database so
136:08 - alter table whatever it is the table and
136:10 - then we can add a column if we want to
136:12 - drop the database that deletes all the
136:14 - objects from the database truncate would
136:16 - just delete the records within the
136:17 - database comments is just a comment
136:20 - and rename is if we want to rename a
136:23 - database object now this is what's
136:24 - interesting here where we have execute
136:26 - sp rename so when we said that tsql
136:29 - extends this is its own little special
136:32 - language because in other
136:34 - other sql
136:36 - variants it definitely does not look
136:37 - like that okay
136:38 - [Music]
136:42 - let's take a look at data manipulation
136:44 - language dml and so this is going to
136:46 - have to do with anything with
136:47 - manipulating data so the first thing is
136:49 - we have an insert command to be able to
136:51 - uh insert data so here you can see we
136:53 - say the values we want and if you're
136:55 - wondering how does it know what values
136:57 - it's going to be the order in which the
136:59 - column appears in the in the actual
137:01 - table that's how it knows that andrew
137:03 - goes under first name and email goes in
137:05 - like the email goes into the email field
137:07 - then for update this is uh when we want
137:09 - to update existing materials so we'll
137:11 - say update users then we'll have to say
137:14 - where so we want to match the id 8
137:16 - update user 8 and then we'll set the
137:18 - values that we want to change then we
137:20 - can delete a user very simple delete
137:21 - from users where id equals 6
137:24 - merge or upsert to insert or update
137:26 - records at the same time i don't see
137:28 - these in other languages so i think it
137:29 - might be a tsql specific thing
137:31 - it's kind of hard to show this because
137:33 - they're very large queries but if you
137:34 - need to do an insert and update at the
137:36 - exact same time you use this then you
137:38 - have call this allows you to call
137:40 - proceed or java sub programs basically
137:43 - functions so let's say you need to
137:44 - calculate a function uh to calculate the
137:46 - distance between toronto and chennai uh
137:49 - you could uh do that you have lock table
137:51 - and this is for concurrency control to
137:53 - ensure two people are not writing to the
137:55 - to the program at the same time okay
137:58 - [Music]
138:02 - all right let's take a look here at the
138:03 - data query language dql and the first
138:05 - thing we have is the select and by the
138:07 - way every if it's query that means it
138:09 - has everything to do with selecting data
138:11 - okay so here what we're going to do is
138:13 - select the these particular fields and
138:15 - do from users if we want to get all
138:17 - fields we can just do an asterisk here
138:19 - um but that's the idea is like i want
138:21 - these fields from this users you could
138:22 - even say like where and other stuff in
138:24 - there we have show so this describes
138:26 - what a table looks like most other
138:27 - languages like bicycle would just say
138:29 - like show and then the table name but
138:31 - here again we have that these kind of
138:33 - weird exec sp columns thing which again
138:36 - is a tsql specific thing
138:38 - and so show would describe what the
138:39 - table looks like so what columns is
138:41 - contained okay we have explained plan so
138:44 - returns the query plan of a microsoft
138:45 - azure synapse analytics sql statement
138:47 - without running the statement
138:49 - this thing is really complicated i could
138:51 - not show you an example there but you
138:53 - know that's what it does help is just
138:55 - like i want to understand more
138:56 - information about database objects so
138:58 - here you're asking more information
138:59 - about the user's table again this is the
139:02 - special tsql stuff
139:08 - all right let's take a look here at data
139:09 - control and this has to do with well
139:11 - control right so we have grant saying
139:14 - i have a i have a table called employees
139:16 - i'm only going to let the um
139:18 - ts goal user or the uh mssql user andrew
139:22 - only be able to select insert update and
139:23 - delete on then you have revoke and
139:25 - that's the opposite let's just say you
139:27 - know we don't want bacon to be able to
139:28 - delete anything on the employees table
139:30 - and that's pretty much it
139:34 - [Music]
139:36 - let's take a look here at transaction
139:37 - control language tcl so tcl commands are
139:40 - used to manage transactions in a
139:42 - database transactions is when you need
139:44 - multiple things to happen
139:46 - and if they don't all happen then you
139:48 - roll back on them okay so this is really
139:50 - important in finance where you have
139:51 - multiple people that have to be part of
139:53 - the purchasing decision
139:54 - and if all purchases don't follow
139:56 - through then you don't want to commit
139:58 - that transaction okay so we have command
140:01 - so set to permanently save any
140:03 - transaction to the database rollback
140:05 - restores the database to the last
140:07 - committed state save point used to
140:09 - temporarily save a transaction so that
140:10 - you can roll back to this uh to the
140:12 - point whenever necessary set
140:13 - transactions specify characteristics for
140:16 - the transaction
140:17 - [Music]
140:21 - all right just a quick review of all the
140:23 - sql documents we or syntax documents we
140:25 - just looked at and so we had ddl which
140:28 - is for defining dml which is for
140:30 - manipulating dql which is for querying
140:32 - dcl which is for controlling and tcl
140:34 - which is for transacting now i
140:36 - highlighted ddl and dml in red because
140:39 - these are what the exam will focus on
140:41 - and sometimes they simplify and they'll
140:42 - take things like that that go in um
140:45 - like select and they'll just put it in
140:46 - manipulation okay so these are the two
140:48 - main ones that you'd have to choose
140:50 - between but i wanted to show you all
140:52 - five of them because this is really what
140:53 - sql is based off of uh and so you know
140:56 - this is just the right way of looking at
140:58 - it this is not an exhaustive list of all
141:00 - the possible commands but it was what i
141:02 - could terry pick out that i recognized
141:03 - that i was familiar with okay
141:05 - [Music]
141:09 - hey this is andrew brown from exam pro
141:11 - and we are on to the tsql cheat sheet so
141:13 - transact sql is a set of programming
141:15 - extensions from sybase and microsoft
141:16 - that adds several features to the
141:18 - structured query language this is used
141:20 - for ms sql databases okay for mss google
141:24 - servers there are five groups of sql
141:25 - commands and so we have data definition
141:28 - language used to define the database
141:30 - schema data query language used for
141:32 - performing queries on data data
141:34 - manipulation language manipulation of
141:36 - data in the database data control
141:38 - language rights permissions and other
141:39 - controls of the database
141:41 - transaction control language tcl
141:44 - transactions within the database so on
141:46 - the exam
141:47 - they're going to ask you either this
141:51 - or this they don't bother with all of
141:53 - them but they're actually r5 okay and so
141:55 - that's all you need to know here
141:56 - [Music]
142:01 - let's talk about connectivity
142:02 - architecture so when a connection from a
142:04 - server to a azure sql database the
142:06 - client will connect to a gateway that
142:08 - listens on port
142:10 - 1443 and i want to remember that port
142:12 - number because it's an important part
142:13 - number that might show up on your exam
142:15 - okay so over here on the right-hand side
142:17 - we have a virtual machine connecting to
142:19 - various different sql servers
142:22 - and so the idea here is that
142:25 - based on the connection policy the
142:26 - gateway will grant traffic and route
142:27 - access to the appropriate database so we
142:29 - actually have three kinds of policies we
142:31 - got proxy so connections are proxy
142:33 - through a gateway increased latency and
142:35 - reduced throughput intended for
142:36 - workloads connecting from outside the
142:37 - azure network redirect which is mostly
142:40 - recommended establishes a direct
142:42 - connection reduced latency improved
142:43 - throughput intended for workloads
142:45 - connecting inside the azure network and
142:47 - default which is just going to default
142:49 - if you launch a vm outside the azure
142:51 - network it's going to use proxy if you
142:52 - launch it within the azure network it's
142:54 - going to
142:55 - use redirect so this thing this port
142:57 - 1443 is only important when you're doing
142:59 - proxy because if you're internal you
143:01 - don't need to go through that port
143:02 - there's no gateway to pass through
143:05 - but yeah that is just the different
143:07 - kinds of connection policies there
143:09 - [Music]
143:13 - all right so let's take a look here at
143:14 - ms sql database authentication so during
143:16 - the setup of your ms sql database you
143:18 - must select an authentication mode you
143:20 - got two options here windows
143:22 - authentication mode which enables both
143:24 - windows authentication and disables sql
143:26 - server authentication and mixed mode
143:28 - which enables both windows
143:29 - authentication and sql server
143:31 - authentication so if you were to remote
143:33 - into your windows machine and under your
143:35 - server properties look under security
143:37 - and server authentication there are
143:39 - those two options interestingly they
143:40 - don't call it mixed mode in the ui but
143:43 - that's what it is it's called mix mode
143:45 - so let's talk about what windows
143:46 - authentication and sql
143:49 - server authentication is so windows
143:51 - authentication which is
143:52 - the recommended way is specific windows
143:55 - users and groups group accounts are
143:57 - trusted to log into the sql server and
144:00 - very and this is the most secure and
144:01 - easy way to modify revoke privileges
144:04 - because you know if they're windows
144:06 - users that means that you can then
144:07 - manage them from azure active directory
144:09 - right
144:10 - then you have sql server authentication
144:12 - so we have a username and password which
144:14 - is set and stored on the primary
144:15 - database you cannot use a
144:18 - kerberos security protocol so that's one
144:20 - disadvantage login password must be
144:22 - passed over the network at the time of
144:24 - connection so that's an additional
144:25 - attack vector but this is an easier way
144:28 - to connect to the database from outsider
144:29 - domain or from a web-based interface so
144:31 - it's just going to be based on the uh
144:33 - the scenario that you're in but if you
144:34 - can just stick with windows
144:36 - authentication
144:37 - [Music]
144:41 - let's take a look here at network
144:42 - connectivity so for your sql database
144:44 - you need to choose either a public or
144:46 - private endpoint a public endpoint is
144:48 - reachable outside the azure network over
144:49 - the internet and you would use firewall
144:51 - rules to protect your database for
144:52 - private endpoints you're uh they're only
144:54 - reachable within the azure network or
144:56 - connecting
144:57 - uh or originating from inside the
144:59 - network so you would use azure private
145:01 - links to keep your traffic network
145:03 - within the azure network so here's just
145:05 - the two options you would see when you
145:07 - provision your database you'll just see
145:09 - here that you choose either public or
145:10 - private and you're setting firewall
145:12 - rules or you're creating private
145:13 - endpoints okay
145:15 - [Music]
145:18 - well let's take a look here at azure
145:20 - defender for sql which is a unified
145:22 - package for advanced sql security
145:24 - capabilities and what it does is
145:26 - vulnerability assessment and advanced
145:28 - threat protection so azure defender is
145:30 - available for azure sql the manage
145:32 - instance and synapse analytics and what
145:34 - it does is it discovers and classifies
145:36 - sensitive data or classify sensitive
145:39 - data surfacing and mitigating potential
145:41 - database vulnerabilities detecting
145:43 - anomalous activities and you can turn it
145:45 - on at any time and you just pay a
145:46 - monthly cost within the azure portal so
145:49 - there you go
145:54 - let's take a look at azure database
145:55 - firewall rules so
145:57 - azure databases are protected by server
145:59 - firewalls a server firewall is an
146:01 - internal firewall that resides on the
146:03 - database server all connections are
146:05 - rejected by default to the database so
146:07 - once your database is provisioned you
146:09 - have an option where you click server
146:11 - firewall and what you're going to do is
146:13 - configure it so you'll say
146:15 - here i allow azure so i give 0000
146:19 - and notice also the connection policy
146:20 - remember we talked about that before so
146:22 - you can set proxy or
146:23 - redirect
146:25 - and so there's that there and then if
146:27 - you wanted to do it via tsql you could
146:29 - as well this is allow only allow the
146:31 - server at
146:32 - zero point four at zero zero zero point
146:34 - four because you can specify your range
146:36 - there is azure firewalls which is uh a
146:39 - totally different service and then
146:40 - there's network security groups which it
146:43 - which is like a logical firewall around
146:45 - your uh vm or your i'm sorry around your
146:49 - nic cards and the subnet but this is the
146:52 - one we were talking about here which is
146:53 - the server firewalls for azure databases
146:55 - okay
146:56 - [Music]
147:00 - all right let's take a look here at
147:01 - always encrypted which is a feature that
147:03 - encrypts columns in the azure sql
147:04 - database or sql server so if you had a
147:07 - column like a credit card number and you
147:08 - wanted to always keep it encrypted you'd
147:10 - use always encrypted and so always
147:12 - encrypted uses two types of keys we have
147:14 - column encryption keys which are used to
147:16 - encrypt the data in an encrypted column
147:19 - and call them master keys a key uh
147:21 - protecting the key that encrypts one or
147:23 - more column encryption keys and you can
147:25 - always uh uh you can al you can apply
147:28 - always encrypted using tsql and so since
147:31 - there is a key that encrypts the key
147:32 - that is called envelope encryption and
147:34 - that is a great way of doing that i
147:35 - imagine that maybe it gets stored in eks
147:37 - or whatever the name of the service that
147:39 - azure calls for their uh encryption keys
147:46 - let's take a look here at role-based
147:48 - access control specifically for
147:49 - databases so role-based asset controls
147:52 - is when you apply roles to users to
147:53 - grant them fine grade
147:56 - actions for specific azure services and
147:58 - there's four in particular that we
148:00 - really do care about here uh to
148:01 - databases and this is and this will
148:03 - probably show up on your exam so you
148:05 - definitely need to know these we have
148:07 - sqldbcontributor so this role allows you
148:09 - to manage sql databases but don't access
148:11 - them can't manage their security related
148:13 - policies or their parent sql servers you
148:15 - have sql managed instance contributor
148:17 - this is so you can manage sql managed
148:19 - instances and required network
148:21 - configuration can't give access to
148:23 - others sql security manager manage the
148:26 - security related policies of sql servers
148:28 - and databases
148:29 - uh but not access to the sql servers and
148:32 - last is sql server contributor manage
148:34 - sql servers and databases but not access
148:38 - not have access to them those sql
148:40 - servers okay so our bacs you definitely
148:43 - want to know these four okay
148:44 - [Music]
148:48 - let's take a look here at transparent
148:50 - data encryption tde which encrypts data
148:52 - at rest for microsoft databases it can
148:54 - be applied to server sql uh or sql
148:57 - servers azure sql databases azure
148:59 - synapse analytics tde does real-time i o
149:02 - encryption and decryption of data logs
149:04 - and files encryption uses a database
149:06 - encryption key called a dek data
149:09 - database boot record stores the key for
149:11 - availability during recovery the d e key
149:14 - the d e k is a symmetric key so it's the
149:17 - same cryptographic key for both of the
149:19 - encryption uh of plain text and
149:20 - decryption of cipher text just to give
149:22 - you a visual there to help you out the
149:24 - idea is you have something that's plain
149:25 - text you use the same key to encrypt it
149:27 - and then the same key to decrypt it
149:30 - and that's just how that works so the
149:32 - steps to apply tdd to database create a
149:34 - database master key create a certificate
149:36 - to support the tde create the database
149:38 - encryption key enable tde on the
149:40 - database so that's just how you do it
149:42 - there within the azure portal and there
149:44 - you go
149:45 - [Music]
149:49 - before we start talking about what
149:50 - dynamic data masking is let's define
149:52 - what is data masking this is when a
149:54 - request for data is transformed to mask
149:56 - the sensitive data so imagine you have a
149:58 - credit card here and you do not want to
150:01 - expose that to particular users maybe
150:03 - there are part of your support team or
150:05 - or even the end user themselves and so
150:07 - the idea is that it's in the database
150:09 - it's stored in its raw format it's
150:11 - untransformed but what it'll do is pass
150:13 - through a masking service and that will
150:16 - have different rules on it which will
150:17 - then apply a particular filter so here
150:20 - we'll only show the last three so
150:21 - dynamic data masking which is a feature
150:23 - of a particular
150:27 - azure sql servers
150:29 - anyway can be applied to azure sql and
150:31 - manage instances and synapse um and so
150:34 - the idea is you would just turn this
150:35 - feature on and then you create a masking
150:38 - policy so you could say exclude a
150:40 - particular users for masking so like
150:42 - let's say you have like root users admin
150:44 - users that need to see all the data you
150:45 - can do that you make masking rules so
150:48 - what fields should be masked and then
150:49 - you have masking functions so how to
150:51 - apply the masking field so here this
150:53 - would be a masking function that would
150:55 - say okay only show the last three
150:57 - letters okay
151:02 - let's take a look at private links so
151:04 - azure private links allows you to
151:05 - establish secure connections between
151:06 - azure resources so traffic remains
151:08 - within the azure network so here's a big
151:10 - graphic of what that looks like and the
151:12 - idea is that you have a private link
151:14 - endpoint uh which is just a network
151:16 - interface that connects you privately
151:17 - and securely to a service powered by
151:19 - azure private link and private endpoints
151:22 - uses a private ip address for your vnet
151:24 - so many azure services by default work
151:26 - with private link and third party
151:28 - providers can be powered by private link
151:30 - as well private link service which
151:32 - allows you to connect your own workloads
151:33 - to private link you may need an azure
151:35 - standard internal load bouncer to
151:37 - associate with the link service but the
151:38 - idea here is that if you have your sql
151:40 - server
151:41 - right
151:42 - that way you can connect it to your
151:44 - on-premise
151:46 - loads or if you have an sql server on
151:48 - your on-premise and you want to do it to
151:50 - a vm that's what you're going to use
151:52 - okay
151:57 - hey this is andrew brown from exam pro
151:59 - we are on to database security for
152:01 - azure the dp900 so let's jump into it so
152:04 - mssql database authentication we have
152:06 - two modes when setting it up when you're
152:09 - remoting into a windows machine so you
152:10 - have windows authentication mode which
152:12 - enables windows authentication and
152:13 - disables sql server authentication and
152:16 - we have mix mode where enables both of
152:18 - these things what are these things well
152:20 - windows authentication is when you
152:22 - authenticate via windows users and sql
152:24 - server authentication is between the
152:25 - user and password you can connect from
152:27 - anywhere windows authentication is the
152:29 - recommended one because it's just more
152:31 - secure
152:32 - for network connectivity we have public
152:34 - endpoints so they're reachable outside
152:35 - the azure network over the internet you
152:37 - use server firewalls for production and
152:40 - you have private endpoints so only
152:42 - reachable with the azure network so use
152:43 - azure private links to keep traffic
152:45 - within the azure network azure defender
152:47 - sql a unified package for advanced sql
152:49 - server security capabilities for
152:51 - vulnerability assessment and advanced
152:53 - threat protection
152:54 - server firewall rules an internal
152:56 - firewall that resides on the database
152:57 - server all connections are rejected by
152:59 - default uh to the database always
153:01 - encrypted a feature that encrypts
153:03 - columns in an azure sql database or sql
153:06 - server
153:06 - role-based access uh
153:09 - controls for databases so these are
153:10 - roles you need to know the sql db
153:12 - contributor manages the sql database but
153:14 - not access them can can't manage their
153:16 - security-related policies or their
153:18 - parent sql servers sql manage instance
153:20 - contributor manage sql instances and
153:22 - require network configuration
153:24 - configuration can't
153:26 - can't give access to others sql security
153:29 - manager manage the security related
153:31 - policies of sql servers databases but
153:33 - not access to sql servers and the last
153:35 - is sql server contributor manage sql
153:38 - servers databases but not access to
153:41 - them to the sql servers okay you have
153:44 - transparent data encryption td encrypts
153:46 - data at rest for microsoft databases in
153:48 - many cases it's already turned on for
153:50 - you dynamic data masking you can choose
153:52 - your database columns that will be
153:54 - masked obscured for specific users azure
153:57 - private links allows you to establish
153:58 - secure connections between azure
153:59 - resources
154:00 - so traffic remains within the azure
154:02 - network i should have put one underneath
154:04 - but this is generally if you want to
154:05 - also connect in a hybrid connection okay
154:08 - so there you go
154:09 - [Music]
154:13 - let's take a look at what a key value
154:15 - store is so a key value store is a data
154:18 - store that is really dumb but it's super
154:21 - super fast okay and so they'll lack
154:23 - features that you would normally see in
154:24 - relational databases like relationships
154:26 - indexes aggregation transactions all
154:28 - sorts of things but you know there is a
154:31 - trade-off for that speed okay
154:33 - and so here is kind of a representation
154:35 - of a key value store which uh you have a
154:37 - key which is a unique you know key to
154:39 - identify the value and i'm representing
154:42 - the value as a bunch of ones and zeros
154:43 - because i want you to understand that
154:44 - there aren't really columns it's just
154:46 - key and value so the idea is that
154:48 - imagine that those ones and zeros
154:50 - actually represent
154:51 - a dictionary and that's usually what
154:53 - they are is it associative array hash
154:54 - dictionary underneath
154:56 - okay and so even though it looks like
154:58 - you know what i mean like if this was a
154:59 - relational database you know you could
155:01 - see these as kind of like columns and so
155:03 - if we kind of did that that's how a key
155:06 - value store can kind of mimic
155:08 - um you know a tabular data right
155:11 - but the thing is is that you know there
155:13 - is no consistency between the the rows
155:16 - hence it is schema-less but that's kind
155:18 - of a way to get tabular data from key
155:20 - values but due to their simple design
155:22 - they can scale well well beyond
155:23 - relational databases so relational
155:25 - databases it becomes very hard to shard
155:27 - them and do a bunch of other stuff with
155:28 - them but key value stores are super easy
155:30 - to scale but you know they come with a
155:33 - lot of extra engineering around them
155:35 - because of these missing features
155:40 - all right let's talk about document
155:42 - stores so document store is a no skill
155:43 - database that stores document as its
155:45 - primary data structure a document could
155:46 - be an xml but it's most commonly json or
155:49 - json like structure and documents
155:52 - stores are sub classes of key value
155:54 - stores so the components of a document
155:56 - store compared to relational database
155:58 - looks like this so the idea is that you
156:01 - have tables is now collections rows are
156:03 - documents columns or fields indexes are
156:05 - still the same name and when you do
156:06 - joins they're called embedding and
156:07 - linking so you know if a key value store
156:10 - can kind of store this why would you do
156:12 - it well there's just a lot more features
156:14 - around the documents itself and so you
156:17 - know how we saw key value store didn't
156:18 - have like it had like nothing like no
156:20 - functionality well document store brings
156:22 - a lot more of the functionality that
156:23 - you're used to in a relational database
156:25 - you know and so it makes things a little
156:27 - bit easier to work with okay
156:29 - [Music]
156:33 - all right let's take a quick look here
156:34 - at mongodb which is an open source
156:36 - document database which stores json-like
156:38 - documents and the primary data structure
156:39 - for mongodb is called a bson so a binary
156:42 - json is a subset of json so its data
156:44 - structure is very similar to json but
156:47 - it's designed to be both efficient and
156:49 - storage in both storage space and scan
156:51 - speed compared to json and bson has more
156:54 - data types than json has date times byte
156:56 - arrays regular expressions md5 binary
156:58 - data javascript code json's just strings
157:01 - integers and arrays it's very very
157:03 - simple but because it has all these new
157:05 - other data types and it's stored in this
157:06 - binary format it's not plain text it's
157:09 - actually binary data
157:10 - that's the one reason why it the storage
157:12 - space and the scan speed is so fast now
157:14 - if you did use javascript to perform an
157:17 - operation like say insert data this is
157:19 - what it would look like so you have kind
157:21 - of an idea that you're inserting items
157:22 - into a collection there okay
157:25 - just to list out some features of
157:26 - mongodb
157:27 - it supports searches against fields
157:29 - range queries regular expressions it
157:31 - supports primary and secondary indexes
157:33 - it's highly available it's it's high
157:35 - availability can be obtained via rep
157:37 - replica sets
157:38 - so replica to offload reads or access
157:40 - standby in case of failover momodube
157:42 - scales horizontally using sharding
157:44 - mongodb can run multiple servers via
157:46 - load balancing mongodb can be used as a
157:48 - file system which is called grid fs with
157:51 - with load balancing and data replication
157:53 - features over multiple machines uh for
157:55 - storing files mongodb provides three
157:56 - ways to perform aggregation uh grouping
157:59 - dat and aggregations just grouping data
158:01 - to return a query so aggregation
158:02 - pipeline map reduce single purpose
158:05 - aggregation
158:06 - mongodb supports fixed collections
158:08 - called capped collections i'm going to
158:10 - become claims to support multi-document
158:13 - asset transactions so mongodb when it
158:16 - first came out didn't do all this stuff
158:18 - and people complained about it i like it
158:20 - being very hard to scale but now it's a
158:21 - lot easier to use so you know mongodb is
158:24 - something that is uh more uh a more
158:27 - popular option nowadays than it was a
158:29 - few years ago so there you go
158:31 - [Music]
158:34 - all right let's take a look here at what
158:36 - a graph database is so graph database is
158:38 - a database composed of data structures
158:40 - that use vertices nodes or dots which
158:42 - form relationships to other vertices via
158:44 - edges arcs and lines so some use cases
158:46 - here fraud detection real-time
158:48 - recommendations engines master data
158:49 - management network and it operations
158:52 - identity and access management and
158:54 - there's a lot they're saying like it's
158:55 - really really good for that i am
158:57 - something i want to look into later
158:58 - traceability and manufacturing contact
159:00 - tracing
159:01 - data lineage for gdpr customer 360
159:05 - degree analysis like for marketing
159:07 - product recommendations social media
159:09 - graphing and feature engineering for ml
159:12 - so let's just kind of break down you
159:13 - know the little components here so what
159:15 - you'd have is a node and a node can
159:17 - contain data properties and then through
159:19 - that it would have a relationship
159:21 - through an edge and that relationship
159:23 - can have a direction and also data
159:25 - properties on it and so it's a lot more
159:28 - um
159:29 - verbose like in ter than a relational
159:32 - database and also just how it can point
159:33 - to stuff so uh super useful uh for
159:36 - particular use cases
159:38 - [Music]
159:42 - let's take a look here at azure tinker
159:44 - pop which is a graph computing framework
159:45 - for both graph databases oltps and graph
159:48 - analytic systems olaps so tinkerpop
159:52 - enables developers to use a vendor
159:53 - agnostic distributed framework to
159:55 - traverse query many different graph
159:58 - systems they'll always say traverse
159:59 - because there's so many it's a tree
160:01 - right
160:02 - so there's a lot of databases that this
160:04 - thing connects to and so here they all
160:06 - are but the ones i want to indicate to
160:08 - you that are important is amazon neptune
160:10 - cosmodb hadoop via spark neo4j which is
160:14 - one of the most popular
160:15 - graphing databases orient db and titan
160:18 - okay so the thing is is that this isn't
160:20 - a
160:21 - graph database it is a basically adapter
160:25 - to other graph databases and ticker pop
160:28 - includes a graph traversal language
160:29 - called gremlin which is the single
160:31 - language that can be used for all these
160:33 - graph systems so let's talk about
160:34 - gremlin gremlin is a graph traversal
160:36 - language for apache tinker pop and so it
160:39 - looks like this and sometimes uh you
160:41 - know like even without tinker pop i
160:42 - think this is with cosmodb that they'll
160:44 - support this language by default so you
160:46 - don't necessarily need to have tinker
160:48 - pop to work with some databases but it's
160:50 - great to have that service if you if or
160:52 - like the framework if you need it so
160:54 - gremlin is is designed to write once and
160:56 - run anywhere w-o-r-a gremlin traversal
160:59 - can be evaluated as a real-time query so
161:01 - lltb or a batch analytics query so over
161:04 - here it's just kind of showing you these
161:05 - are the oltps graph databases over here
161:08 - and then on the right-hand side we have
161:10 - olaps okay
161:11 - and so gremlin hosted language embedding
161:14 - means you can use your favorite
161:15 - programming language when you write
161:17 - gremlin okay so there you go
161:18 - [Music]
161:22 - hey this is andrew brown from exam pro
161:24 - and we are looking at azure tables which
161:26 - is a type of storage for nosql key value
161:28 - data store within the azure storage
161:30 - accounts azure table stores
161:32 - non-relational structured data with a
161:33 - schema-less design and there are two
161:35 - ways to interact with azure tables
161:37 - through the storage table storage api or
161:40 - microsoft azure storage explorer which i
161:42 - find is the easiest way to interact with
161:44 - it so just kind of looking at storage
161:47 - explorer there if you wanted to add an
161:48 - entry you'd have to provide a partition
161:51 - key which is a unique identify fire for
161:54 - the partition with a given table and a
161:56 - row key a unique identifier for an
161:58 - entity within a given partial a
161:59 - partition and so you have all your data
162:02 - types here so we see string boolean
162:03 - binary data type
162:05 - double uh
162:06 - guids
162:08 - 32 and 64. if we wanted a query you'd
162:11 - have to query along the partition and
162:13 - row key so you could also do some
162:15 - additional filtering here so just notice
162:16 - here that um you know you have your
162:18 - partition key you put your value like
162:20 - klingon and wharf and then this is not
162:23 - this is just additional properties you
162:24 - added
162:25 - a lot of time the way these key values
162:27 - work is that
162:28 - this will return the results like all
162:31 - the results and then server side and
162:33 - then client-side these will be filtered
162:35 - client-side i don't know if that's the
162:36 - case with azure table but that's
162:38 - generally how these things work and so
162:40 - there you go
162:40 - [Music]
162:44 - hey it's andrew brown from exam pro and
162:46 - we're looking at cosmodb which is a
162:48 - service for fully managed noaa school
162:49 - databases that are designed to scale
162:52 - and be highly performant so cosmodb
162:54 - supports different kinds of nosql
162:55 - database engines which you interact via
162:57 - an api so we have the coresql which is
163:00 - their document datastore their azure
163:02 - cosmodb api for mongodb their azure
163:05 - table
163:06 - and gremlin okay and this will be using
163:08 - uh probably tinker pop um so all of
163:11 - these nosql engines uh uh specify
163:14 - capacity so you can do provision
163:16 - throughput for pay for guarantee of
163:18 - capacity or serverless pay for what you
163:20 - use so if you are just playing around
163:22 - with the service you can go ahead and
163:23 - choose that serv
163:25 - serverless option and so a lot of times
163:27 - when people talk about cosmodb they're
163:28 - usually talking about coresql so if you
163:30 - say cosmodb it's usually document but
163:33 - understand that there's a bunch of stuff
163:35 - underneath it
163:36 - now if you want to start viewing data
163:38 - and making stuff and playing around with
163:39 - it you'd use the cosmo db explorer which
163:42 - is a web interface that you can find at
163:44 - cosmos.azure.com so after you made your
163:46 - cosm db cluster or container whatever
163:48 - they call it
163:49 - then you could go access your database
163:51 - so here we have the sql api
163:54 - and so that would be the document store
163:56 - and you could just see here that we have
163:58 - we've created a new item here for that
164:00 - data okay
164:02 - and so i just want to show you that if
164:03 - you drop down here you choose container
164:05 - or database so we create a new container
164:07 - um also if you are in um
164:10 - azure it looks like they just have it
164:12 - here under the data explorer tab so it's
164:13 - the same thing it's the cosmo db
164:15 - explorer just in line okay so you don't
164:17 - have to like go to that url you could
164:19 - just click into your um your it's called
164:22 - account cosmodb account and go to data
164:24 - explorer i just wanted to show you here
164:26 - like if you made a graph database that
164:27 - you can do everything through this
164:28 - explorer for all the different types the
164:30 - interface will change a bit so here we'd
164:31 - add a new vertex right and it's just
164:33 - slightly different okay
164:35 - [Music]
164:39 - all right so the thing about azure
164:40 - tables is that you can
164:42 - use it within either cosmodb
164:45 - okay or you can use it within account
164:47 - storage and the thing is is that um
164:50 - it's a really good comparison to look at
164:52 - these two things because this way we can
164:55 - really understand like how powerful
164:57 - cosmodb is all right
164:59 - so what we'll do is compare the two so
165:01 - over here when you have azure tables in
165:03 - account storage
165:04 - it's fast but it has no upper bounds of
165:06 - latency
165:07 - for azure cosmodb it's going to give you
165:09 - single digit millisecond latency for
165:11 - reason writes
165:12 - for throughputs it's variable throughput
165:15 - it's limited to 20 000 operations you
165:17 - get a guaranteed uh backed by an sla and
165:20 - no upper limits when you're using cosmo
165:22 - db for global distribution it's a single
165:24 - region
165:25 - and for cosmic db you have 30 plus
165:28 - regions for indexing you only get the
165:30 - primary index or partition and row no
165:32 - secondary indexes and then for
165:35 - cosmodb you get automatic and complete
165:36 - indexing in all properties no index
165:38 - management for green you get query
165:40 - execution uses index for primary key and
165:43 - scans otherwise and for uh cosmodb you
165:46 - get queries that can take advantage of
165:48 - automatic indexing on properties for
165:49 - fast query times for consistency we got
165:52 - strong with primary region and eventual
165:54 - with secondary regions and with uh
165:56 - cosmodb there's like five you know what
165:59 - i mean there's just uh the consistent
166:01 - levels are a lot more flexible okay
166:03 - for pricing it's consumption based and
166:05 - then for
166:06 - cosmodb you have consumption based or
166:08 - provision capacity for the slas it's
166:10 - 99.99 availability and here it it's
166:13 - backed by an sla but some conditions it
166:16 - does not apply okay so
166:18 - you know hopefully that shows you that
166:20 - cosmodb like is very performant is
166:22 - globally available
166:23 - single digit millisecond and i i really
166:25 - feel like this is to compete with um
166:27 - adabus um
166:29 - dynamodb because it sounds so
166:31 - similar to dynamodb but
166:33 - yeah there you go
166:38 - hey this is andrew brown from exam pro
166:41 - and we are on to the azure tables and
166:42 - cosmos db cheat sheet for the dp900 i
166:44 - want to point out something uh that i'm
166:47 - sure you already know about but in the
166:48 - course i spelt cosmos db without the s
166:51 - like everywhere and i'm not going to go
166:53 - back and fix that
166:55 - but i know i'm going to hear like
166:57 - never the end of it for like the next
166:59 - year okay
167:00 - so let's start at the top here azure
167:02 - tables it's a key value data store can
167:04 - be hosted on either azure storage
167:06 - account storage it is designed for a
167:08 - single region and single table can be
167:10 - hosted on cosmos db
167:12 - and when it's hosted here it's designed
167:14 - for scale across multiple regions
167:15 - cosmodb a fully managed nosql service
167:17 - that supports multiple nosql engines
167:20 - called apis why they didn't call them
167:22 - engines i don't know
167:23 - coresql api this is the default one it's
167:26 - a document database you can use sql to
167:28 - query documents and when people are
167:29 - talking about cosmodb that's what
167:31 - they're talking about the document
167:32 - database the default one okay
167:34 - graph apis a graph database you can use
167:38 - with gremlin to transfer traverse the
167:40 - nodes and edges mongodb api a mongodb
167:42 - database it is a document database
167:45 - tables ai is just as your table's key
167:47 - value but within cosmodb apache
167:49 - tinkerpop an open source framework to
167:52 - have an agnostic way to talk to many
167:54 - graph databases they probably won't ask
167:55 - you about tinker pop on the exam gremlin
167:57 - graph traversal language to traverse
167:59 - nodes and edges you definitely need to
168:01 - know what gremlin is and be used to
168:03 - seeing what it is like identify what it
168:04 - looks like mongodb an open source
168:07 - document database and the way it works
168:09 - is it has its own um data structure its
168:12 - document structure called bson which is
168:14 - binary json a storage and compute
168:16 - optimized version of json introduces new
168:19 - data types cosmo db explorer a web ui to
168:22 - view cosmos databases and there you go
168:24 - [Music]
168:28 - so what is apache hadoop well it's an
168:30 - open source framework for distributed
168:32 - processing of large data sets hadoop
168:34 - allows you to distribute large data sets
168:36 - across many servers and computing
168:38 - queries across many servers so htfs and
168:40 - mapreduce were the first features that
168:42 - were launched with hadoop way back in
168:44 - the day in version one and since then
168:46 - there's a lot more services now but the
168:48 - idea behind distributed processing is
168:50 - the idea is the idea is that your
168:52 - computer servers do not need to be on
168:54 - specialized hardware you can run them on
168:56 - common hardware and that's actually how
168:57 - google back in the day
168:59 - like uh there's a story over google they
169:01 - just kept on like adding like all random
169:02 - machines to build up their search engine
169:04 - didn't matter what it was and that
169:06 - eventually became the hadoop system so
169:08 - apache dupe framework has the following
169:10 - so hadoop common collections of common
169:12 - utilities and libraries that support
169:13 - other hadoop modules hadoop distributed
169:15 - file system a brazilian and redundant
169:17 - file storage distributed on clusters of
169:19 - common hardware hadoop mapreduce writes
169:22 - apps that can process multi-terabyte
169:25 - data in parallel on large clusters of
169:26 - common hardware hbase a distributed
169:28 - scalable big data store yarn manage
169:31 - resources nodes containers and perform
169:33 - scheduling hive used for generating
169:35 - reports using sql
169:37 - pig a high level scripting language to
169:39 - write complex data transformations if
169:41 - they sound like they do the same thing
169:43 - they absolutely do they're just slightly
169:44 - different hadoop can integrate with many
169:46 - other open source projects via the
169:47 - hadoop components and we're going to see
169:49 - a lot of those open source
169:51 - things here in a moment
169:56 - let's take a look here at apache kafka
169:58 - which is an open source streaming
170:00 - platform to create high performance data
170:02 - pipeline streaming analytics data
170:03 - integration and mission critical
170:05 - applications absolutely the number one
170:07 - streaming service though it is open
170:09 - source so you have to find a way to uh
170:11 - to host it kafka was originally
170:13 - developed by linkedin and open source in
170:14 - 2011. kafka was written in scala and
170:17 - java so to use kafka you're going to be
170:18 - writing java code
170:20 - here is just kind of a diagram of how it
170:22 - works so you got producers consumers and
170:24 - topics so in kafka data is stored in
170:27 - partitions on a cluster which can span
170:29 - multiple machines which makes it
170:30 - distributed computing producers publish
170:32 - messages in a key and value format using
170:35 - kafka producer api and consumers can
170:37 - listen for messages and consume using
170:39 - the
170:40 - the cough consumer api messages are
170:42 - organized into topics producers will
170:44 - push messages to topics and consumers
170:46 - will listen on topics so there you go
170:52 - so azure hd insights is a managed
170:54 - service to run popular open source
170:56 - analytics services so here's kind of a
170:58 - graphic of how it works and hdinsight
171:00 - supports the following frameworks apache
171:02 - hadoop which is it which is what it is
171:05 - it's the entire system apache spark
171:07 - kafka storm hive hbase and lap and also
171:11 - you can run our workloads hd insights
171:14 - has broad range of scenarios such as etl
171:17 - data warehousing machine learning
171:18 - internet of things just because you can
171:20 - put in so many things here and so
171:22 - you know hdinsights is just a managed
171:24 - version of hadoop and it just makes it
171:26 - really easy to to do stuff so you can
171:28 - consume stuff run these in clusters so i
171:30 - think these are each called a cluster
171:32 - for whatever you're running and they can
171:33 - go out out to somewhere now once you
171:36 - launch hdinsights
171:38 - you can use the apache ambari which is
171:40 - an open source hadoop management web
171:42 - portal to provision manage and monitor
171:44 - your hadoop clusters
171:46 - and so when you create a cluster you're
171:48 - going to get one by default it's just
171:49 - under here where it says cluster
171:51 - dashboard
171:52 - and the idea here is you'll see all the
171:54 - types like htf mapreduce all the stuff
171:56 - here and it just makes it really easy to
171:58 - interact with hadoop okay
172:00 - [Music]
172:05 - hey this is andrew brown from exam pro
172:07 - and welcome to the hadoop cheat sheet
172:09 - for the dp900 let's jump into it so
172:11 - apache hadoop is an open source
172:12 - framework for distributed processing of
172:14 - large data sets
172:15 - and underneath it has the hadoop
172:17 - distributed file system hdfs a resilient
172:19 - and redundant file storage that's
172:21 - distributed on clusters of common
172:23 - hardware you have mapreduce which writes
172:26 - apps that can uh process multi-terabytes
172:28 - data in parallel on large clusters of
172:30 - common hardware hbase a distributed
172:32 - scalable big data store yarn managed
172:34 - resources nodes containers perform
172:36 - scheduling
172:37 - hive used for generating reports using
172:39 - an esco language pig a high level
172:42 - scripting language to write complex data
172:43 - transformations apache spark can perform
172:46 - is 100 times faster in memory and 10
172:48 - times faster than disk than hadoop
172:50 - supports etl streaming and ml flows you
172:53 - can run that on hadoop i didn't just did
172:55 - not put it under the um
172:56 - i debated whether i should put it in in
172:58 - line or there but that's where i put it
173:00 - apache uh kafka a streaming pipeline
173:03 - analytics service hd insights a managed
173:06 - service to run popular open source
173:07 - analytics services it is fully managed
173:09 - hadoop system so it's just hadoop but
173:11 - managed by
173:14 - azure there so there you go that's the
173:15 - chi chi
173:16 - [Music]
173:20 - apache spark is an open source unified
173:22 - analytics engine for big data and
173:24 - machine learning and spark lets you run
173:25 - workloads much much faster than hadoop
173:27 - though you can run it in the hadoop
173:29 - system okay so 100 times faster in
173:31 - memory 10 times faster than disk
173:33 - and which is why spark is being
173:35 - described as lightning fast so when it
173:36 - says hadoop you know it's talking about
173:39 - hive and pig and the other things that
173:40 - usually come along with hadoop and so
173:43 - apache spark is a collection of
173:45 - libraries that work well together to
173:46 - form an analytics ecosystem so we have
173:49 - the spark core this is the underlying
173:50 - engine and api the api supports the
173:52 - following programming languages r sql
173:54 - python scala and java you have spark sql
173:58 - which introduces a data structure called
173:59 - a data frame not the same thing as a
174:01 - pandas data frame but they're just
174:02 - called the same thing okay which can be
174:04 - used with uh dsls to work with
174:07 - structures and semi-structured data you
174:09 - have spark streaming allows spark to
174:11 - ingest data from many streaming services
174:13 - so htfs flume kafka twitter
174:16 - kinesis
174:17 - you have graph x so distributed graph
174:20 - processing framework you have machine
174:22 - learning the mlib library
174:24 - and this is a distributed machine
174:25 - learning framework with common machine
174:26 - learning statistical algorithms
174:29 - the way you are going to interact with
174:31 - spark is through resilient distributed
174:33 - data set rdd which is a dsl to execute
174:35 - various parallel operations on the epoxy
174:37 - spark cluster so here are some common
174:39 - functions
174:40 - map filter distinct count min max mean
174:44 - paralyzed you get the idea and here's an
174:46 - example of rdd api so just notice here
174:50 - um i think basically all of these are
174:52 - are
174:53 - those functions right so
174:55 - that just makes it really easy to
174:57 - transform and work with data okay
175:02 - [Music]
175:03 - so databricks is a software company
175:05 - specializing in providing fully managed
175:07 - apache spark clusters and the company
175:09 - founders were the creators of the apache
175:10 - spark delta lake and ml flow
175:13 - open source projects and databricks has
175:15 - two main offerings the database platform
175:18 - and so dablex cloud based spark platform
175:21 - with an easy to use web ui
175:23 - where you can launch fully managed spark
175:25 - clusters launch notebooks to write code
175:26 - and interact with spark create
175:28 - workspaces to collaborate with team
175:29 - members and role-based access controls
175:31 - create jobs for etl or data analysis
175:34 - tasks that run immediately or on
175:35 - schedule create ml workflows and is
175:38 - available on all main cloud service
175:40 - providers aws azure and gcp they also
175:43 - have the database community edition
175:44 - which is a free version of the
175:46 - databricks platform for educational use
175:48 - create a free michael cluster that
175:50 - terminates after two hours when idle no
175:52 - workspace jobs or rbac so really just a
175:55 - subset of the one above so azure data
175:59 - bricks is a partnership between
176:00 - microsoft and databricks to offer the
176:02 - database platform within the azure
176:04 - portal running on azure compute services
176:06 - and it offers two environments
176:08 - workspaces and so basically this is just
176:11 - the azure database platform with
176:12 - integrations to azure data related
176:14 - services for building big pipelines so
176:16 - if you need to do batching you can use
176:18 - azure data factory streaming apache
176:20 - kafka event hub iot
176:22 - storage azure blob storage azure data
176:25 - like storage the other side of it is
176:27 - azure databricks sql analytics run sql
176:29 - queries on your data lake create
176:31 - multiple visualization types to explore
176:33 - your query results build and share your
176:36 - dashboards
176:37 - now if you want to launch a databricks
176:39 - workspace it's really easy all you got
176:41 - to do is create a workspace and choose
176:42 - your plan launch a workspace
176:45 - use sso to connect to it and start your
176:47 - database platform so if you go up here
176:49 - and this is in the azure portal with
176:50 - azure databricks we'll just choose which
176:51 - one we want and then we launch the
176:53 - workspace it'll sson and then you're in
176:56 - there okay so um you know basically if
176:59 - you launch it's not going to cost you
177:00 - any money so if you want to play around
177:01 - with it you can do that or if you're
177:03 - really antsy you can just go to the
177:05 - databricks website and try the community
177:07 - edition because there's risk there's no
177:09 - risk of spending any money when using
177:10 - that okay
177:11 - [Music]
177:15 - hey this is andrew brown from exam pro
177:17 - we're taking a look here at the apache
177:19 - spark and data bricks cheat sheet so
177:20 - let's jump into it so apache spark is an
177:22 - open source unified analytics engine for
177:24 - big data and machine learning it's a
177:26 - hundred times faster in memory than
177:27 - hadoop ten times faster than disk than
177:29 - hadoop can perform etl so batch
177:31 - streaming and ml workloads uh for the
177:33 - apache ecosystem it's composed of spark
177:36 - core which is the underlying engine api
177:37 - spark sql you use sql it also has a new
177:40 - data structure called a data frame to
177:42 - work with data spark streaming which is
177:44 - a way to ingest data from many streaming
177:46 - services graph x distributed graph
177:48 - processing framework mlib a distributed
177:51 - machine learning framework there's also
177:52 - rdd it's a domain specific dsl
177:55 - to
177:56 - execute various parallel operations on
177:58 - apache spark clusters then we're talking
178:00 - about data bricks here it's a software
178:01 - company specializing at providing fully
178:04 - managed apache spark clusters we have
178:06 - azure databricks a partnership between
178:08 - microsoft and database to offer the the
178:11 - database platform with within the azure
178:13 - portal running on azure compute services
178:16 - azure databricks offers two environments
178:17 - we have the databricks workspace so this
178:20 - is the databricks platform with
178:21 - integrations to azure data related
178:23 - services for building data pipelines if
178:25 - you went to azure or sorry if you went
178:28 - to databricks website and signed up it's
178:29 - the same portal okay azure database sql
178:33 - analytics run your query on your data
178:35 - lake and i believe that this is
178:37 - basically um azure synapse analytics so
178:40 - that's the engine that's in there okay
178:44 - [Music]
178:46 - take a look here at sql management
178:47 - studio also known as ssms which is an
178:50 - ide for managing any sql infrastructure
178:53 - if you take a good look at the
178:54 - screenshot you can see that allows us to
178:56 - work with databases write sql statements
178:59 - and that's pretty much it so access can
179:01 - access configure manage administer and
179:03 - develop all components of sql server
179:05 - azure sql database azure synapse
179:07 - analytics and it has a few components in
179:09 - it so we have object explorer view and
179:11 - manage all objects in one or more
179:13 - instances of sql server template
179:14 - explorer build and manage files
179:16 - for boilerplate text that you can use to
179:18 - speed the development of queries and
179:20 - scripts and this is deprecated but you
179:22 - might hear about it so i just mentioned
179:24 - it here and this is build projects used
179:26 - to manage administration items such as
179:28 - scripts and queries so not a complicated
179:30 - service but uh yeah there you go
179:32 - [Music]
179:37 - all right let's take a look here at
179:38 - server data tools ssdt which transforms
179:42 - data development by introducing
179:43 - ubiquitous declarative models that span
179:45 - all phases of development phases within
179:47 - visual studio so this is a visual studio
179:49 - tool like azure basically has like a
179:52 - tool for everything like it'll be the
179:53 - same tool but it will be repurposed for
179:55 - their different other tooling products
179:57 - and this one's for visual studio so i
179:59 - don't have a great internal screenshot
180:01 - but you can see that it works with um it
180:03 - has a few different components inside
180:05 - there and analysis reporting
180:06 - integrations
180:08 - so it uses
180:09 - sdt transact so tsql to build debug
180:13 - maintain refactor databases also
180:15 - provides a table designer for creating
180:17 - editing tables in either database
180:18 - projects connected database instances be
180:20 - able to view control data loaded files
180:23 - easy to publish to sql database or sql
180:25 - server has an object explorer offers a
180:28 - view of your database similar to sql ssm
180:31 - mess allows you to
180:33 - lightly duty database administration
180:35 - design work easily create edit rename
180:37 - delete tables stored procedures
180:38 - functions edit table data compare
180:41 - schemas execute queries by using
180:42 - contextual
180:43 - menu rights i don't think this is on the
180:45 - exam but because it's just one of these
180:47 - many services i figured we'd throw it in
180:49 - here
180:50 - just so that if you do see this
180:52 - initialism you know what it's for
180:53 - [Music]
180:57 - so azure data studio is a cross-platform
181:00 - database tool for professionals using
181:01 - on-premise and any data platforms for
181:03 - windows mac os and linux and here is a
181:05 - screenshot and
181:07 - if you recognize it it looks just like
181:09 - visual studio code right if you open up
181:11 - the extensions that's what you would see
181:13 - so query design and manage your database
181:14 - and data warehouses data azure data
181:16 - studio offers a modern experience with
181:18 - intelligence very similar to experience
181:20 - to visual studio code code snippets
181:22 - source control integration integral
181:24 - terminal built-in charting customizable
181:27 - dashboards jupiter notebooks connected
181:28 - to your data set and it has a
181:30 - marketplace of free extension so some i
181:32 - know that seemed very useful was sql
181:33 - database inspector so a great way of
181:36 - looking at your data cousteau extension
181:38 - for azure data studio postgres extension
181:41 - and many many many more so there you go
181:43 - [Music]
181:47 - so azure data factory is a managed
181:49 - service for etl elts and data
181:51 - integration create data driven workflows
181:52 - for orchestrating data movement and
181:54 - transferring data at scale and so if you
181:56 - see the image it makes it pretty clear
181:57 - what you can do with it there's a bit of
181:59 - a pipeline there so create pipelines to
182:00 - schedule data-driven workflows build
182:02 - complex etl processes that transfer data
182:04 - visually with data flows using compute
182:06 - services such as hdinsights hadoop data
182:08 - bricks sql database publish and
182:10 - transform data to data stores such as
182:12 - azure synapse analytics raw data can be
182:15 - organized into meaningful data stores
182:17 - and data lakes let's break down the
182:18 - components here so we just have a better
182:20 - idea how this thing works so pipelines
182:22 - is a logical grouping of activities that
182:23 - perform units of work activities is a
182:25 - processing step in the pipeline data
182:28 - sets or data structures within the data
182:29 - store link services define connections
182:31 - information for data sources to connect
182:33 - to the data factory data flows are logic
182:36 - to determine how data moves through the
182:38 - pipeline transform integration runtimes
182:40 - compute infrastructure used by data
182:42 - factory control flows orchestration of
182:45 - pipeline activities that include
182:47 - chaining activities and a sequence of
182:48 - branching you should know what
182:51 - control flows are and data flows are and
182:53 - there you go
182:57 - [Music]
182:58 - so microsoft sql server integration
183:00 - services ssis is a platform for building
183:03 - enterprise level data integrations data
183:04 - transformation solutions
183:06 - ssis can be used to automate sql
183:08 - database servers it can be
183:10 - used as an integration runtime for azure
183:12 - data factory you can perform the
183:13 - following tasks copy files download
183:15 - files loading data within data
183:16 - warehouses cleansing mining managing sql
183:19 - server objects managing sql server data
183:22 - you can perform etl for a variety of
183:23 - sources xml flat files relational data
183:25 - sources
183:26 - and ssis has built-in tasks and
183:29 - transformation graphical tools for
183:30 - building packages integration service
183:32 - catalogs databases
183:34 - where you store run
183:35 - and manage your packages
183:37 - it comes with a graphical interface to
183:39 - transform so you don't have to write any
183:40 - code and the designer is a graphical
183:42 - tool that you can use to create and
183:43 - maintain integration services packages
183:45 - just to show you what it looks like
183:47 - here it is so it allows you to drag out
183:48 - transformations and design different
183:50 - kinds of flows and control flows and
183:52 - data flows notice it has control flows
183:54 - and data flows similar to azure data
183:56 - factory uh but you can see this is the
183:58 - tool kind of azure data factory is like
184:00 - kind of like the web version of this but
184:02 - yeah there you go
184:03 - [Music]
184:07 - hey this is andrew brown from exam pro
184:09 - and we're taking a look here at etl and
184:11 - sql tools cheat sheet so let's jump into
184:13 - it the azure data factory is a managed
184:15 - service for etl elt and data integration
184:18 - jobs you can create data driven
184:20 - workflows for orchestrating data
184:21 - movement and transforming data at scale
184:23 - build elt pipelines visually without
184:25 - writing any code via web interfaces
184:28 - you have ssis the s the sql server
184:31 - integration services
184:32 - a platform for building enterprise level
184:34 - integration data flow solutions
184:36 - a low code tool for building etl
184:38 - pipelines very similar to data factory
184:40 - but existed 15 years prior mostly
184:42 - focused around sql no surprise there
184:44 - integrates with azure data factory so
184:46 - you can extend it to non-relational
184:48 - database workloads okay we have azure
184:50 - data studio an id similar to a very
184:53 - similar to visual studio code those
184:55 - cross-platform and works with sql and
184:57 - non-relational database data has many
184:59 - many many many extensions you have sql
185:01 - server management studio smss an id for
185:04 - managing any sql infrastructure that
185:06 - only works for windows more mature than
185:08 - data studio very similar in terms of
185:11 - parallel so we have like the modern
185:12 - version that's the web mode over this
185:14 - web that's both
185:16 - s like relational and non-relational and
185:18 - then the older one that's mature but it
185:20 - is a a windows app and does a lot of
185:22 - interesting stuff for sql you have sql
185:24 - server data tools ssdt this is a visual
185:27 - studio extension to work and design
185:29 - visually
185:30 - sql databases within visual studio so
185:32 - there you go we're all done
185:33 - [Music]
185:37 - hey this is andrew brown from exam pro
185:39 - and i wanted to show you how to go ahead
185:41 - and install power bi so what i've done
185:43 - is i'm on my windows machine and i
185:45 - pulled up the microsoft store and all i
185:48 - did was go in the top right corner and
185:49 - type in power bi this is the easiest way
185:52 - to install power bi you can download the
185:55 - application but i find this is just the
185:57 - best way to do it and we'll hit get or
186:00 - free to get started here
186:03 - um
186:04 - and it should now start downloading so
186:05 - we go in the top right corner
186:08 - you'll notice here that it is now
186:09 - downloading okay so we just have to wait
186:11 - for that to finish and i'll see you back
186:13 - here in a moment
186:16 - all right so after waiting a short while
186:18 - here it looks like power bi desktop is
186:20 - finished downloading what we can do is
186:21 - click into power bi and we can go ahead
186:24 - and launch this service
186:26 - and so we'll let that go ahead and
186:28 - launch
186:31 - and the initial time you launch it it
186:33 - does take a little bit of time there so
186:34 - i stopped and restarted the video but
186:36 - here you can see we are now inside of
186:38 - power bi
186:39 - uh and if we wanted to get started there
186:41 - are a few ways we can go ahead and do
186:42 - that they have some nice tutorials
186:44 - here's in the bottom right corner but
186:45 - what i want to do is just get something
186:47 - open so we can start looking at
186:49 - something so if you were to type in
186:50 - sample data sets
186:52 - for power bi microsoft has this nice
186:54 - page where you have a bunch of
186:55 - downloadable
186:56 - reports or things we can work with and
186:58 - so maybe we should give the first one a
187:00 - go so i'm going to go ahead and download
187:01 - this one here
187:03 - and if we go down below so we'll just
187:06 - take a look here
187:13 - so i think we just have to go ahead and
187:14 - click here
187:18 - and
187:19 - uh from here i just want to hit that
187:21 - download button so i'm just looking for
187:23 - it
187:29 - here it is
187:30 - okay great and so i'll go ahead and
187:32 - download that file notice that it's it's
187:34 - the
187:35 - power bi x that is the file that we are
187:37 - looking for
187:38 - and we'll go ahead and open this file
187:44 - so after any short while there the file
187:46 - did open it does seem to take a while
187:48 - for things to open but i'll just hit the
187:49 - x there
187:50 - they have a new modeling mode we'll just
187:52 - say not yet
187:53 - and so down below you can see i have a
187:55 - bunch of tabs here and this allows us to
187:57 - go ahead and explore this data but you
188:00 - can see things are extremely interactive
188:02 - here in
188:04 - power bi desktop so you can do all sorts
188:06 - of things
188:07 - uh here okay
188:12 - [Music]
188:16 - hey this is andrew brown from exam pro
188:18 - and in this follow along we're going to
188:19 - be looking all at azure sql so what i
188:22 - want you to do is go to the type here
188:23 - and type in sql and you'll notice you'll
188:25 - get the options like the databases
188:27 - managed instances and virtual machines
188:28 - those are the three under the azure sql
188:30 - tier but to make our lives a little bit
188:31 - easier we'll type in azure sql i find
188:34 - this really confusing and so i'm hoping
188:36 - that i can show you the easiest way to
188:37 - find it but if you go to azure sql here
188:39 - and add it this will actually now give
188:41 - you the option to choose between the
188:42 - three so you can make an informed
188:43 - decision and we'll work our way from the
188:45 - right to the left so sql virtual
188:47 - machines is great when you want os level
188:49 - access to the virtual machine uh when
188:51 - you're doing a lift and shift that means
188:52 - you're moving your
188:53 - sql server from on-prem to the cloud
188:56 - because you want to
188:58 - take advantage of the cloud and also if
189:01 - you want to bring your own license to
189:03 - take advantage of the azure hybrid
189:05 - benefit so if you drop down here you'll
189:07 - notice that you'll see byol and if we
189:10 - expand here it'll tell you all the
189:11 - details here as to why you'd want to use
189:13 - that i'm not going to spin one up
189:14 - because i'm going to show you the price
189:16 - okay so we go over to the price and we
189:17 - scroll on down we go to um page you go
189:20 - you're going to notice whether it's on
189:21 - or off with high uh hybrid benefit
189:22 - you're paying about a dollar to two per
189:24 - hour and so you know it's you're not
189:27 - going to learn that much by spending one
189:28 - up
189:29 - because we are going to spin up an sql
189:30 - server but at this stage it's not going
189:31 - to really matter it's they're all very
189:33 - similar so you know i just i don't want
189:35 - to spend money if you're a student there
189:37 - let's look at sql managed instances
189:38 - we'll expand that again this is for lift
189:40 - and shift meaning you're moving from
189:42 - on-premise to the cloud but here you
189:44 - don't have access to the os level
189:46 - you're not bringing your own license
189:48 - however the upside is it's a fully
189:50 - managed service meaning that it can
189:51 - scale very well it's going to have great
189:53 - backups and things like that and it
189:55 - comes in two flavors we have single
189:57 - instance in single instance as your arc
189:59 - now i didn't cover this in the course
190:00 - but azure arc allows you to um extend
190:03 - your i think it's the control plane it's
190:05 - either data plane or control plane but
190:06 - the control plane uh two different cloud
190:08 - providers and also to your on premise so
190:10 - the idea here is like you can use sql
190:12 - manage instances and ha like you can
190:14 - have all the benefits of here and launch
190:16 - the instance within your own
190:18 - infrastructure so um you know if you
190:20 - really need to keep that server on-prem
190:22 - then you can do that with azure arc but
190:23 - here's a single instance um you know
190:25 - it's just whatever there and so i'll
190:27 - show you the pricing on that one
190:29 - and so we'll go to manage instances
190:30 - we'll scroll on down here
190:32 - and the lows cost one here for pays you
190:34 - goes a dollar so it's still kind of
190:36 - expensive if you're a student
190:38 - again i don't know if it has a free tier
190:40 - and i i wouldn't imagine it would
190:42 - because that's not really a free tier
190:45 - kind of product but you know i just want
190:46 - you to know that that option's there
190:48 - we'll make our way now to uh sql
190:50 - databases
190:52 - and so
190:53 - we have a few options here we have
190:54 - single database elastic pool and
190:55 - database server
190:57 - okay so single database is you have it's
191:00 - a great fit for modern uh cloud
191:03 - cloud-borne applications that need fully
191:04 - managed database with predictable
191:06 - performance so it has hyperscale i mean
191:07 - that it can scale up to 100 terabytes it
191:10 - has serverless compute it's easy to
191:12 - manage you have elastic pools we covered
191:14 - this in in the lecture content this is
191:15 - where you might be like a multi-tenant
191:18 - sas and you have multiple databases one
191:20 - per customer and you want to save class
191:23 - so you actually have them all running on
191:24 - the same server and you have database
191:26 - server and those so this is just to
191:28 - manage a group of single databases and
191:30 - elastic pools it's a way of grouping
191:32 - stuff together and so today what i'm
191:34 - going to do is launch myself a single
191:36 - database
191:37 - okay
191:39 - and what we'll do is we'll just say
191:41 - azure
191:43 - sql or we'll say dp900
191:45 - dp900 azure sql
191:50 - just so i can see what i'm doing we'll
191:51 - go down below here and i'm just going to
191:53 - call this um
191:55 - my azure
191:57 - uh
191:58 - sql
192:00 - we will drop down here and select a
192:01 - server we'll have to create a new server
192:03 - so this will be my azure sql server
192:08 - and you might have to work to choose a
192:09 - name i'm going to do one two three
192:12 - four five six
192:14 - there we go okay for the login i'm going
192:17 - to do azure user
192:19 - for the password i'm going to do capital
192:20 - t testing one two three capital t
192:23 - testing one two three it's really nice
192:25 - sometimes they make you do four five six
192:27 - and then an
192:29 - exclamation mark but we have the shorter
192:30 - password there we're going to say okay
192:34 - do we want to use elastic pool no i
192:35 - don't think we need that today but let's
192:37 - expand the general purpose here i want
192:39 - to make sure that we have the lowest
192:40 - cost possible so notice
192:42 - to the right here it's 381 dollars if
192:44 - you're provision it you could also do
192:46 - serverless
192:48 - i've actually never used the serverless
192:50 - feature here but this would be really
192:52 - great if you're just trying to learn
192:54 - because it's going to be based on usage
192:56 - here i've definitely used serverless
192:58 - services just not this one in particular
193:00 - for sql but what i'm going to do is
193:01 - stick with provision i'm going to look
193:03 - for a basic plan
193:05 - and what we'll do is scroll this all the
193:06 - way to the left here
193:08 - and you know this is just if we forget
193:10 - to turn off our database so now it's
193:12 - five five dollars a a month and probably
193:15 - you know per hour it's you know
193:17 - fractions of a penny so we'll go here
193:18 - and hit apply and we'll scroll down as
193:21 - we have some options geo-redundant zone
193:23 - redundant locally redundant honestly we
193:25 - only need locally redundant so i'm going
193:27 - to go ahead and select that obviously
193:29 - you know if you're doing a production
193:30 - database you want geo-redundant because
193:32 - you're going to have um uh you know
193:34 - better like a better redundancy in terms
193:36 - of like if a region goes down you'll
193:38 - still have your database okay we're the
193:40 - next step to networking
193:42 - and here we either have public or
193:44 - private endpoint this is really
193:45 - important for uh security purposes right
193:48 - and we do cover this in the lecture
193:49 - content as well and so i think it would
193:51 - be a great idea to have that there let's
193:52 - go read about firewall rules allow
193:54 - services and resources to access server
193:56 - yes to communicate from all resources
193:57 - inside the azure boundary that may or
193:59 - may not be private subscription so allow
194:01 - azure services and resources to access
194:03 - this service yes
194:05 - uh add current ip address yeah because
194:07 - that's me so i'm going to say yes to
194:08 - both of those it's going to set up our
194:09 - firewall rules which is really nice
194:11 - we'll go next to security here we have
194:13 - azure defender for sql if you want that
194:15 - additional protection you turn that on
194:17 - it costs 15 a month and we'll go to the
194:19 - next tab here
194:21 - here we could um i guess let's see for
194:23 - backup so start with a blank database
194:25 - restore from a backup or select a sample
194:27 - i'll select a sample because that's nice
194:29 - to have some data i didn't know they had
194:30 - that there
194:32 - and so we'll go to next tags
194:34 - i didn't really cover tags in the course
194:36 - but they're actually really important
194:37 - especially when we're talking about data
194:38 - because you do want to catalog
194:40 - and and categorize all your stuff so
194:42 - tagging is very simple what we do is we
194:44 - have a name so we could say um
194:47 - workload and i'll just say like uh
194:49 - learning right
194:51 - but allows us to filter and find
194:53 - resources later down the road we'll go
194:55 - ahead and hit review create we can
194:56 - review all of our stuff see that the
194:58 - price is okay with us we'll go ahead and
195:01 - create and again we're not going to get
195:02 - billed five dollars by pressing that
195:03 - button
195:05 - because it's again
195:06 - build per hour
195:08 - um so you know we will be build uh some
195:10 - kind of sense i don't know if azure sql
195:12 - has a free tier probably does
195:15 - uh free tier let's see here
195:17 - free
195:18 - tier
195:20 - i don't see it but i'm not too worried
195:22 - about that um so you know again if
195:24 - you're really worried about any spend
195:25 - just uh watch and don't don't do it but
195:27 - i do recommend if you do have the
195:28 - pennies to go launch the stuff of
195:30 - yourself because you'll learn a lot more
195:31 - that way okay so i'll see you back here
195:33 - when this is done uh provisioning
195:36 - all right so after a short little wait
195:37 - there our server is ready so i'm gonna
195:39 - go ahead and hit click go to resources
195:41 - and what we can do on the left hand side
195:43 - is just do a little exploration so look
195:45 - at the top here where it says set server
195:46 - firewall remember from our
195:49 - our security part where if we had set
195:52 - um some default rules well here they are
195:54 - okay so here is my ip address that is
195:56 - allowing me to connect to it notice we
195:58 - have that connection policy where it's
195:59 - default proxy or redirect
196:02 - you know things like that so we'll go
196:04 - back uh one step here
196:06 - and i just want to show you down below
196:08 - that there are some features so notice
196:10 - that tde transparent data encryption is
196:12 - already turned on uh there's your azure
196:14 - defender for sql
196:16 - we have our dynamic data masking so on
196:18 - the left hand side we'll scroll on down
196:20 - and just take a look at some of the
196:21 - security so we go to tde and you can see
196:22 - it's already turned on for us which is
196:24 - great i'm not going to click on the
196:25 - security center because it's a pain to
196:27 - exit out but let's look at dynamic data
196:29 - masking so here what it's doing is it's
196:31 - suggesting
196:32 - uh fields that should be mass and we go
196:34 - ahead and just click add and add and
196:36 - then we can mask these domains so i just
196:38 - click that there and so now these
196:40 - domains are masked
196:41 - and it shows you a bit of the mass
196:43 - function as to what it would do to to
196:44 - mask it so it's as easy as that
196:47 - um data discovery classification this
196:49 - will be useful it's in preview right now
196:50 - it's not showing any information as of
196:53 - yet um
196:54 - but the great thing about this is that
196:56 - it could
196:57 - discover some information that you might
196:59 - care about that needs to be that's
197:00 - considered sensitive right so you might
197:02 - say this is confidential gdpr address
197:05 - here
197:06 - um information types let's see what we
197:07 - can do here so we'll say customer
197:09 - and
197:10 - we'll say credit card
197:12 - and we'll say highly confidential
197:15 - and i'll say add classification
197:18 - i haven't used the service before
197:19 - because i had never seen it before but
197:20 - um it looks like that's a great way to
197:23 - um you know keep track of that kind of
197:25 - stuff
197:26 - so that's cool
197:28 - uh auditing i don't think i ever do
197:30 - auditing but um
197:31 - you can turn
197:33 - azure sql auditing it will track logs
197:35 - and stuff like that so the thing is is
197:37 - that if someone were to get into your
197:38 - database and try to make changes or to
197:41 - corrupt your data stuff like that you'd
197:42 - want to see who was actually accessing
197:44 - those queries so definitely if you care
197:45 - about security you'd absolutely want to
197:47 - turn that on
197:49 - and then we'll just scroll back to the
197:50 - top here back go to our overview
197:52 - i want to show you that uh through
197:54 - connect we can connect to data studio or
197:56 - visual studio if you click this it's
197:57 - going to tell you you need to install it
198:00 - i don't have azure data yet so i'm going
198:01 - to go ahead and install it on this
198:02 - computer and we'll go down below and i'm
198:05 - on windows machine today so
198:07 - this is for windows mac and linux and so
198:10 - i will go ahead and use the
198:12 - user installer i guess
198:14 - and i will download that it's 100
198:16 - megabytes as that's going we'll make our
198:18 - way back
198:19 - to
198:20 - our main page here and i also want you
198:22 - to notice there's power bi so we will go
198:24 - and download this uh we can go download
198:26 - actually right now so if we were to
198:28 - click on um
198:30 - uh is it just get started yeah it's
198:32 - gonna download this my azure pb ids and
198:36 - we'll be able to open that file in power
198:37 - bi and you know we did power bi earlier
198:40 - in earlier follow along so it'll be easy
198:42 - to open i want you to know if you don't
198:44 - have time to do all the stuff because
198:45 - we're going to have this database up and
198:46 - running for a little a little longer
198:48 - than you expect so if you if you don't
198:50 - feel like you can do this all in one
198:51 - setting you can delete this database and
198:53 - always go ahead and spin it up again
198:55 - when you're ready to continue but on the
198:57 - left hand side they have a query editor
198:59 - here and we're just going to go log in
199:01 - so i said my password was testing with
199:03 - capital t one two three we'll hit okay
199:06 - and now that we're in i can see my
199:07 - tables on the left-hand side here
199:10 - uh we got views and stored procedures
199:11 - but let's say we wanted to execute a
199:13 - query so i'll just expand this i've
199:16 - never seen this database before so i'm
199:17 - just going to go with it but we'll type
199:19 - in select
199:20 - um oops
199:22 - and it's going to be very aggressive
199:23 - about autocompletes you got to be really
199:25 - careful here and i'm just going to say
199:26 - select asterisk which means all fields
199:29 - and we want to select it from sales lt
199:32 - dot customer
199:33 - and notice when it auto completes it has
199:35 - square back brackets around it that is
199:37 - just the thing that tsql does let's say
199:39 - you had a field called first name
199:41 - and you wanted to have it with a space
199:43 - and that's what brackets that you do so
199:45 - they're just filling that in case you
199:46 - have reserved words or um you know
199:48 - spaces but we don't actually have any
199:50 - spaces here so we just remove it like
199:51 - that
199:52 - sales lte is kind of like the container
199:54 - for all these um all these tables it's
199:56 - just a naming convention they have for
199:57 - their table names
199:59 - and so we'll go here select all
200:01 - and hit run and we should get some data
200:03 - back and so let's just perform a very
200:06 - simple join so a join will happen when
200:08 - you actually have
200:10 - a foreign key you can join on so here
200:11 - you can see this is the
200:13 - primary key customer doesn't seem to
200:15 - have a whole lot of information so let's
200:17 - go into products
200:19 - and this should have yeah here you see
200:21 - it says product category that is a
200:23 - foreign key so we can use that so i'm
200:25 - gonna go here and type in sales lt and
200:28 - we'll do product
200:31 - okay and then we'll do where
200:34 - or sorry we'll do a left join so we'll
200:35 - say left join
200:37 - and i want to left join on this product
200:40 - id so it would be left join with
200:43 - sales lt dot product
200:46 - uh category
200:48 - and we would say on
200:51 - and so we would say sales lt
200:54 - dot
200:54 - product and it's going to be
200:57 - uh
200:59 - what was it called it is
201:02 - product category okay so we say product
201:05 - category id
201:07 - equals and we'd say sales lt dot product
201:11 - category
201:13 - category
201:15 - dot id so we're saying we want to join
201:18 - where these match on here
201:20 - assuming joins working like every other
201:22 - language which it should and so i'm
201:24 - going to be very particular and just
201:25 - pull out some particular keys that i
201:26 - want so here i'm going to say
201:28 - i want the sales lt.product oops
201:34 - and i wonder if i can do this as on this
201:36 - like as
201:38 - prod
201:40 - or for p here because i'm going to go
201:41 - crazy if i keep on typing that a so
201:43 - we'll just try that um i think it should
201:45 - work um if we do that because then it'll
201:47 - just be easy to read here
201:49 - it's like a way of aliasing it sometimes
201:50 - you can don't even need to put the word
201:52 - as you just put p like this
201:54 - and for sales product we could do that
201:56 - as well so i took out the on so i'll
201:58 - just say like pc
202:00 - and we'll see if it lets us do that
202:04 - okay and so here we have product
202:08 - id
202:10 - and i want to get the product name
202:14 - looks like it's working
202:16 - and
202:17 - we want the category name i think so
202:19 - we'll drop this down
202:23 - i assume it must have a category it's
202:25 - taking it's time to load here but we'll
202:27 - just say pc.name
202:31 - as product name
202:35 - as name
202:37 - as id
202:38 - and we'll see if that lets us run that
202:43 - it's currently not available if the
202:44 - problem persists
202:45 - contact customer supports we'll hit run
202:46 - again
202:49 - i'm not sure why our server's having
202:51 - problems i didn't i didn't turn it off
202:53 - did i
202:54 - let's go back here and take a look
202:56 - it's online yeah it's in good shape here
202:59 - so we should be able to query it
203:01 - maybe just lost our connection so what
203:03 - i'm going to do is just click back here
203:05 - i'll type in
203:07 - testing123
203:10 - [Music]
203:14 - and i'll go ahead and close that there
203:15 - we'll make our way back
203:17 - go back in the query editor
203:23 - there we go
203:24 - i'm not sure i was giving us trouble
203:26 - i threw an error here um
203:30 - invalid id oh yeah it's not called that
203:32 - it's called uh product id
203:34 - there we go
203:36 - we'll run that
203:39 - invalid column name product id
203:42 - must be naming these wrong here
203:45 - capital d here
203:48 - uh is it the same thing down below here
203:50 - normally you know like when i name these
203:52 - i would have them all lowercase
203:53 - underscore but you know in the azure
203:55 - world they they like to do this quite a
203:56 - bit
203:57 - so we will try this
204:00 - um
204:04 - product id so it doesn't oh right it
204:07 - wouldn't be uh
204:09 - this would be
204:11 - product category sorry so this is
204:13 - product category id
204:17 - and yeah that should match
204:20 - there we go so we just did a join
204:21 - between the two tables so we have the
204:22 - name and the product name
204:24 - which is uh pretty good there our azure
204:26 - data studio looks like it's done
204:28 - downloading so i'm going to go ahead and
204:30 - install that so what i'm all i'm doing
204:31 - here is i'm just
204:33 - double clicking the file to open it up
204:41 - and we'll just give that there a second
204:43 - oh it was here the entire time we'll say
204:45 - i accept next next next
204:48 - uh
204:48 - we'll get a desktop icon sometimes i
204:50 - can't find them later registered data
204:52 - suit is an editor for sporto files new
204:55 - i like other other programs for that and
204:57 - we'll go ahead and install that
205:13 - doesn't take too long
205:17 - and we will launch the data studio
205:21 - and yeah if you've ever used visual
205:22 - studio code it's gonna look very
205:23 - familiar to you see
205:26 - um
205:27 - so don't like the light mode rather have
205:29 - it dark appearance can we change that to
205:31 - dark
205:32 - zen mode oh no oh no what did i do uh i
205:36 - don't know how to get out of that okay
205:37 - great maybe not do zen mode i just
205:38 - wanted to make it a dark mode uh not
205:41 - sure how to change that i guess it's not
205:43 - a big deal but what we'll do is we'll go
205:44 - over here the top and we can go ahead
205:47 - and add a new connection
205:48 - and so that's an mssql server
205:51 - what we'll need to do is i want to keep
205:53 - this open because um it's very hard to
205:55 - find the tables here but what we'll go
205:57 - here is go back and we'll get the server
205:59 - name so we'll copy that and then within
206:02 - visual azure data studio we'll paste in
206:04 - our server we have three options windows
206:06 - authentication sql login azure active
206:08 - directory i'm going to use sql login the
206:10 - password is azure user username is azure
206:12 - user the password is capital t testing
206:14 - 123. what i like will tell to remember
206:17 - is that if you drop it down it'll see if
206:18 - the database is there
206:20 - and there it is so my azure sql that's
206:23 - what we called it i'm going to go ahead
206:25 - and connect
206:27 - and if i expand it i should be able to
206:29 - see
206:30 - tables good and stuff like that
206:35 - very good
206:36 - a lot easier to work with than
206:38 - that preview editor there
206:40 - so what we can do i'm just going to go
206:42 - over here and grab our query
206:45 - copy this
206:47 - and what we'll do up here is we'll make
206:49 - a new query so i think you
206:52 - uh
206:53 - it's over here new query maybe it's over
206:54 - here as well let me
206:58 - see now i guess we just do it from there
207:01 - i'm just trying to figure out what's
207:02 - easiest way to go up there and do a new
207:04 - query i'm going to go ahead and paste
207:05 - that in notice we can change our
207:06 - connection i don't think a database is
207:08 - selected right now so we'll go ahead and
207:10 - connect to this database here
207:13 - we'll go ahead and run that
207:14 - and it's the same data okay uh there's
207:16 - the thing called notebooks which is kind
207:18 - of cool so i can go ahead and make a
207:20 - new notebook i think it's using jupyter
207:21 - notebooks underneath
207:23 - um but what we'll do can we save that
207:25 - query
207:26 - we sure can i'm not going to save that
207:28 - query today because i do not care but
207:30 - yeah it is jupiter notebooks great so
207:32 - what we'll do is go file new notebook
207:35 - and here i can write some text so
207:37 - this will
207:38 - return back
207:41 - both products
207:44 - and product categories
207:47 - okay
207:48 - and then what i can do here
207:50 - is add another piece of code
207:53 - we'll paste that in oops
207:56 - and i can even say like limit so i could
207:58 - say like
207:59 - limit to
208:01 - 20
208:02 - or maybe 10.
208:04 - let me go ahead and run that
208:07 - and we'll just connect on the server
208:09 - here just needs to connect
208:11 - okay
208:13 - and we spelt limit wrong so we'll just
208:15 - spell that again there
208:19 - limit
208:22 - i don't think i'm spelling it wrong but
208:24 - i'll just take it out because i don't
208:25 - feel like goofing around with that today
208:26 - and so here we have our records
208:29 - okay
208:30 - it should just be limit right
208:32 - l-i-m
208:35 - maybe it's not limited tsql give me two
208:36 - seconds tsql limit
208:41 - um
208:47 - oh
208:48 - they don't have a limit function i did
208:50 - not know that okay well that's kind of
208:51 - interesting they have things like where
208:53 - row is equal to like i'm just looking at
208:55 - it over here right so c areas use the
208:57 - top record so select top five
209:02 - you know
209:04 - top ten let me try that
209:07 - there we go so i guess it's just not
209:09 - limit so we have that in there which is
209:11 - kind of nice um and so let's say we
209:13 - wanted to narrow that down to something
209:15 - in particular uh if there's a way we
209:17 - could browse the data
209:20 - oops
209:21 - i mean that by accident but what we
209:23 - could do here is just take a look
209:25 - i wanted to see the um i'll do it over
209:27 - here i want to see the
209:29 - product category id
209:31 - because then maybe we could just do a
209:33 - like a where statement really quickly
209:35 - and so these are all mountain bikes when
209:37 - it's five so what we'll do
209:40 - is we'll make a where statement
209:42 - and we will say when pc product
209:46 - category id is equal to five
209:50 - uh and then we'll hit run
209:53 - okay and then up here we'll just say
209:54 - that you know show us
209:56 - oops
209:57 - uh can i edit this
210:00 - yup
210:01 - show us the top
210:03 - 10 mountain bikes
210:08 - it's not really saying like it's not
210:10 - this is not very useful information
210:11 - because it's not saying uh why it's the
210:13 - top ten it's just listing it there's no
210:15 - ordering here so maybe that would be
210:16 - also a good idea here so let's just take
210:18 - a look at our data structure here i mean
210:21 - we could do it in
210:22 - in here as well if we want to
210:24 - uh explore that way but i think it's a
210:27 - bit easier
210:28 - to do it here so let's look at the
210:29 - product
210:31 - um and see if there's like any
210:32 - definitive information that makes it
210:33 - interesting like list price
210:35 - so maybe that's something we could
210:37 - return here so we'd say um p dot list
210:39 - price
210:41 - as price
210:45 - and what we could do is order that so
210:47 - we'd say order by p dot list price
210:52 - descending
210:59 - there we go
211:00 - when you have to do that tutorial it's
211:01 - it's easier to forget so now it's
211:03 - listing it from descending prices uh
211:05 - that amount isn't very human readable i
211:07 - don't know if it's like 3000 whatever i
211:08 - wonder if we could use a function to do
211:11 - that if we're using ms or sorry like
211:13 - postgres it would just be like floor
211:15 - so maybe i could like floor the result
211:17 - here or round it let's just see if i can
211:19 - do that
211:20 - sometimes it's like round yeah there it
211:22 - is
211:23 - those are like built-in functions to
211:24 - most sql languages here
211:27 - and
211:28 - uh it needs two arguments what's the
211:30 - second oh probably what to round it to
211:32 - so we'll probably put a zero here
211:33 - because it's gonna be like rounded to
211:34 - what
211:36 - maybe it needs to be a one
211:39 - other languages you just put round in
211:41 - so i don't know what that is so what
211:42 - we'll do is look that up so we'll say
211:43 - tsql round
211:47 - okay just doing that off screen here
211:49 - and so it says number expression is an
211:51 - expression of the exact numeric
211:52 - approximation is the precision in which
211:55 - you want it to be
211:56 - if i can get a nice example so
211:59 - uh round's a number
212:02 - i just don't want it with zeros
212:06 - um
212:09 - so that will round it to
212:11 - two decimal points and then we have
212:12 - negative two so we'll go up the top here
212:15 - is the precision to numeric to be
212:17 - rounded length must be an expression of
212:19 - tiny intent when the length is a
212:20 - positive number is rounded to the number
212:22 - of decimal points specified with the
212:23 - length with a number
212:25 - uh it's on the left side of the decimal
212:27 - point oh okay that's kind of interesting
212:29 - so what we want to do is probably round
212:33 - the
212:33 - [Music]
212:34 - left or right side
212:36 - so either give it two there
212:39 - can't convert data type
212:41 - v and char oh its name you know what
212:43 - you're probably watching me be like
212:44 - andrew what the heck are you doing so
212:46 - it's supposed to be up here so we do
212:47 - round
212:50 - okay and i'll just say two
212:52 - and that should round it to two decimal
212:53 - points
212:54 - which is fine but i don't i really don't
212:56 - want anything there so i'll say zero
213:00 - and you know i probably would want that
213:01 - just as an integer there's probably a
213:02 - way to cast it so maybe there's like a
213:04 - cast function or two int
213:07 - so i don't see that so
213:09 - cast uh float
213:12 - to
213:12 - [Music]
213:13 - ant uh tsql
213:17 - yeah it's called cast that's what i
213:18 - thought
213:19 - and so it's as simple as that so what we
213:20 - can do
213:22 - and we probably don't need to round it
213:23 - because the cast will probably uh round
213:25 - it itself
213:27 - so we can say cast and
213:30 - say as int
213:34 - very good that looks really nice let's
213:36 - say we wanted to like concat something
213:38 - onto that
213:39 - uh again i don't know how to do concat
213:41 - and tsql so we'll look it up so we'll
213:42 - say tsql concat
213:44 - strings in postgres it's just a pipe
213:47 - oh it's a plus wow we get a plus i love
213:50 - that you don't usually get a plus so
213:52 - what i'll do here is i'll cast this as
213:54 - an end
213:54 - and then i'll cast this as
213:57 - i don't know what it is in this as
213:58 - varchar bar chart yes
214:01 - can i do that first it's still a number
214:03 - great and now that it is a a string we
214:05 - can do maybe string
214:08 - dollar sign like that we'll say run
214:10 - we'll let me do that yes very good and
214:12 - we'll give it
214:13 - also the usd
214:16 - there he goes that's kind of nice so now
214:17 - we kind of have a human readable human
214:20 - readable thing there okay
214:22 - so what i'll do is i'll just copy that
214:24 - over put it in my notebook
214:26 - and
214:27 - we'll just have that there
214:29 - i'll move that off screen
214:31 - and you know that's pretty much how you
214:32 - would create data with sql now now that
214:35 - we have a mysql database we can actually
214:37 - consume it with uh with a lot of other
214:39 - services like power bi or maybe some
214:42 - other services that we can think of but
214:44 - yeah how about we do that next let's try
214:46 - to load this data into power bi
214:48 - [Music]
214:52 - all right so now let's take a look at
214:54 - using power bi with our ms sql server
214:56 - here so in the bottom left corner you
214:58 - saw that i downloaded that file earlier
214:59 - so i'm going to go ahead and open that
215:00 - up if you don't have it just make your
215:02 - way over to power bi and download it
215:04 - again and all it's going to do is
215:05 - establish a connection to the data
215:07 - source now we didn't really need to
215:08 - download this file to do it it's not
215:10 - that hard to establish our own
215:11 - connection uh but it does save us one
215:14 - little step here okay so
215:16 - it's gonna load here just give it a
215:17 - moment
215:19 - it's gonna pop this up if you don't see
215:20 - this what you can do i'm just gonna go
215:22 - ahead and close this to show you how to
215:23 - make a manual connection so i'm going to
215:24 - go over to server name copy that go back
215:27 - and if we go into now you have this
215:30 - button here but we'll just go get data
215:32 - more so you have a more predictable way
215:34 - of loading data in
215:38 - and we'll go to database here we can go
215:40 - to azure and so we have azure sql
215:43 - database um which i think that's what
215:45 - we're using
215:47 - and so we'll go ahead and hit connect
215:52 - and so we'll put the server name in
215:53 - there
215:54 - and the server name was my azure
215:57 - sql
215:58 - we'll hit ok
215:59 - we'll give it a moment to connect and
216:01 - we'll get back to the same screen so
216:02 - here we can import the data that we want
216:04 - we'll say product product category um
216:07 - customer
216:09 - right customer address address
216:11 - the more we bring in the more it's going
216:13 - to be so you know what i'm just going to
216:14 - narrow it down to just product product
216:16 - category and customer you can transfer
216:18 - the data which is really nice i'm not
216:20 - going to get into that because that's a
216:20 - whole other thing but we'll go ahead and
216:22 - hit load and what this is going to do is
216:25 - it's going to start to import this data
216:29 - and we'll give it a moment
216:35 - so it's going to load the data in and
216:37 - then it's also going to detect
216:38 - relationships and do our data modeling
216:40 - so the data is now in if we go to here
216:42 - we'll be able to see these tables and
216:44 - explore that data in its raw format if
216:46 - we go over to this tab here we can see
216:48 - that it's detected a relationship the
216:50 - more tables the more relationships it
216:51 - will auto detect so we didn't want to
216:53 - make that too hard i want to go back
216:55 - over here to our report and i just want
216:58 - to show you how you can visualize some
216:59 - information so one of the easiest ones
217:00 - to do off the bat is card
217:03 - so i'm looking for card
217:05 - it's this one nope slicer
217:08 - card okay and so i'm just dragging this
217:10 - one out just there we go i just clicked
217:12 - it whatever you have to do to get there
217:14 - and this is great if you have a single
217:15 - field so i'm going to go drop this down
217:17 - and i'm going to find product by list
217:19 - price so i'm going to drag that onto the
217:21 - field here notice it went into there it
217:22 - only takes one input for the fields and
217:24 - there's my price if i want to change the
217:26 - look of it and go down here and there's
217:28 - all sorts of things we can change here
217:30 - like shadow
217:33 - you know other things that you can do
217:34 - for fun but you know there's a little
217:36 - bit of styling there
217:37 - and let's say i wanted to see that as a
217:40 - table
217:41 - so what i could do is just change that
217:43 - to table and now it's a table uh table
217:45 - display but this information is not very
217:47 - useful so what i want to do is drop this
217:49 - down and then
217:51 - drag out the name of the product
217:52 - category remember that these have a
217:54 - relationship it's established here so
217:55 - it's going to know how to slice that
217:57 - information to make it useful and so now
217:59 - we have a breakdown of
218:01 - price and name based on
218:03 - category right
218:05 - and i mean that's category which was a i
218:08 - don't know how to rename that very well
218:10 - yeah for this visual here we'll say
218:13 - uh you know product or so this would be
218:15 - category
218:18 - and for here we'll just say we'll just
218:19 - rename that to make it a bit easier
218:21 - we'll just say price
218:22 - there we go now let's say we want that
218:24 - as a graph so we just change that over
218:26 - to a graph so we can kind of see the
218:28 - difference in prices
218:29 - all sorts of things we just click around
218:31 - here to have some fun
218:34 - okay bar chart is going to be the most
218:35 - useful one i believe so yeah
218:38 - i don't know if there's anything else
218:38 - that's really fun here we have another
218:40 - breakdown kind of see like uh
218:42 - the cost there of all the stuff that
218:43 - they're selling
218:44 - okay i think that has the highest price
218:46 - is the bikes right because so the volume
218:48 - of sales is just the cost of each item
218:50 - right that they're selling uh you know
218:52 - so that's how you'd use power bi desktop
218:54 - with um
218:56 - uh with your sql server so yeah there we
218:58 - go
219:01 - so another thing that we can do since we
219:02 - have power bi and we connected it to our
219:04 - mysql server or our sql server no
219:06 - problem is we probably should try to
219:07 - publish
219:08 - it to
219:09 - the power bi service so that we can see
219:11 - how to make dashboards so what i'm going
219:13 - to do is go up the top here and type in
219:14 - app.powerbi.com
219:16 - if this is the first time you've ever
219:17 - gone here it's going to ask you to make
219:19 - an account even even if you have a
219:20 - microsoft account it's just another way
219:22 - of
219:22 - authorizing that there and so here you
219:24 - can see uh we can explore all sorts of
219:26 - data sets if we click into here these
219:28 - are just dashboards right
219:31 - and we'll go in here we can see all this
219:33 - kind of information which is uh really
219:35 - nice and we can go here and
219:37 - just select some information you can see
219:39 - it's very interactive which is really
219:40 - nice
219:41 - um and you know we can go ahead and
219:43 - create our own uh kind of things here if
219:45 - we were to publish a data center or do
219:47 - whatever but uh you know what i want to
219:50 - do
219:51 - is uh to connect some more data sources
219:53 - download power bi desktop we already
219:55 - have that installed so what i want to do
219:56 - is just publish and get something into
219:58 - here so i'm going to reopen up our file
220:01 - we had there earlier
220:03 - and establish a connection to our ms sql
220:06 - server
220:07 - we'll make a little port and we'll
220:09 - publish it and see how we can access it
220:10 - through the power bi service here okay
220:15 - so we'll give it a moment and all i want
220:17 - is product and product category
220:21 - once it decides to load here
220:23 - so we want product and product category
220:26 - and i will say load
220:32 - we'll give it a moment
220:39 - and now that is loaded what we'll do is
220:42 - again make ourselves a visual so i will
220:44 - drag out a card like we did last time
220:48 - and we'll go down to product here i will
220:50 - try price
220:52 - uh it's better probably better if it's a
220:53 - table it's a little bit more useful i
220:55 - think
220:56 - and we'll go here to product category
220:57 - and drag out the name
220:59 - again we'll rename these we'll just say
221:01 - rename price
221:04 - rename
221:08 - category
221:12 - and what we'll do there's a publish
221:14 - bubble go ahead and hit our publish
221:15 - button so do you want to save changes
221:16 - yes
221:18 - uh and we'll just save this whatever we
221:20 - want to call we'll just say my power bi
221:22 - report
221:24 - okay
221:28 - and
221:29 - uh so we will just have to put our email
221:32 - in here
221:40 - okay oops
221:43 - it's thinking we'll give it a second
221:50 - there we go so it's asking me to log
221:51 - into my account
221:55 - and we'll say my workspace sounds great
221:56 - to me
222:00 - and we'll give it a moment to
222:02 - publish that report
222:04 - great that report has now been published
222:06 - we'll make our way back to power bi
222:08 - and so it should be under our workspaces
222:10 - right
222:12 - all right so what we'll do is make our
222:13 - way over to our workspace on the
222:14 - left-hand side and you'll have to click
222:16 - on here it's not very clear and we have
222:18 - a data center report if we click into a
222:19 - report we have this error about missing
222:21 - credentials so what we'll do is make our
222:23 - way over to our data set
222:24 - and we'll just go to uh settings here
222:27 - and we might need to provide some
222:29 - credentials here again so let's edit the
222:31 - credentials
222:32 - and we'll just say azure user capital t
222:35 - testing one two three
222:37 - uh privacy level settings is private
222:41 - we'll sign in
222:43 - and you know what the pro
222:45 - i wonder if it'll give us access to user
222:46 - updates etc because it's from a a uh
222:50 - you know like remember how we had that
222:51 - firewall rule so it could be the
222:52 - firewall rules that is preventing it
222:55 - so by clicking on my report
222:57 - yeah it's still a problem so what i'll
222:58 - do is go over to my azure server set the
223:00 - firewall or rules here
223:02 - allow azure services to access this
223:04 - resource yes
223:06 - um
223:08 - so it's denying the rules
223:13 - so we'll have to figure that out there
223:16 - okay so it looks like actually let's go
223:18 - in the dataset and updating the
223:19 - credentials did work that message was
223:21 - just a bit of a false flag there i mean
223:22 - like it did need to reestablish
223:24 - connection but uh
223:25 - uh you know i thought i thought maybe i
223:27 - had to go digging around in our firewall
223:29 - settings but it since we have this
223:30 - turned on you know powerbi should be
223:32 - able to access it and it can so we have
223:34 - uh we're looking at a report right now
223:36 - within power bi but let's say we want to
223:38 - make a dashboard well how would you do
223:39 - that well you go ahead and you just go
223:40 - and hit the pin
223:42 - we'll say my dashboard
223:44 - we'll go ahead and pin
223:46 - that and uh i mean if we wanted to
223:49 - create a mobile layout we could do that
223:53 - okay so there it is for our mobile
223:55 - layout and we can go back to our web
223:56 - layout right
223:58 - and from there you know now that we have
224:00 - our dashboard we can go ahead and just
224:02 - you know share that with our team or put
224:04 - it in chat you know whatever it is but
224:06 - this is where the point where you would
224:07 - then hit the power bi pro where you have
224:09 - to upgrade but uh yeah it's as simple as
224:11 - that so yeah there you go
224:14 - [Music]
224:19 - so we saw azure data studio as a means
224:21 - to connector database but there's
224:23 - another tool which is um
224:24 - at sql server management studio i
224:26 - figured we should give that a go so what
224:28 - i'll do is go ahead and download that
224:30 - and it's 635 megabytes so you have to
224:33 - decide whether you want to download that
224:34 - once that's downloaded what we'll do is
224:35 - go ahead and install and give it uh give
224:37 - it a look okay
224:38 - all right so after a very long wait sss
224:41 - ms is done downloading and even in here
224:43 - it says that it downloads azure data
224:44 - studio with it so they really really
224:46 - want to use azure data studio which
224:48 - we've already used but you know it
224:49 - doesn't hurt to open up ssmss and give
224:52 - it a go if you can use it
224:56 - and so i'm just going to go ahead and
224:57 - install that there
224:59 - okay
225:02 - and it sounds great
225:07 - it's going to go
225:12 - there we go
225:14 - couldn't tell i hit install or not
225:17 - all right so waiting a little bit of
225:18 - time here that this finally installs
225:20 - we'll go ahead and close that and now
225:21 - what we'll do is go actually ahead and
225:23 - open that program
225:25 - ms
225:27 - i'm not sure where it installs into so
225:28 - i'll see you back here in a moment
225:30 - all right so i'm back and i found it
225:32 - it's under c program files microsoft sql
225:35 - server management studio 18 common seven
225:37 - id now uh you know your start menu you
225:40 - should be able to type in ssms and find
225:43 - it for whatever reason on my computer
225:44 - nothing ever shows up under the start
225:46 - menu and i can't be bothered to fix it
225:48 - but it's good to know where it actually
225:49 - installs to so now we know where it is i
225:51 - just double clicked it and now it is
225:52 - open and so here we can see
225:55 - a bunch of stuff and this will be a way
225:57 - for us to load this so what we'll need
225:58 - is our server name as per usual
226:01 - so i'm going to go back to azure here
226:03 - and i'm going to grab the server name
226:05 - we'll go back paste that in we'll choose
226:08 - sql server authentication we'll type in
226:10 - azure user we'll type in testing with a
226:13 - capital t one two three
226:15 - connect
226:16 - honestly i like this a lot better
226:18 - as your data studio i don't know like
226:20 - real apps just feel feel like they work
226:22 - a lot better you know and so we can go
226:24 - into our database here look at some
226:26 - tables
226:29 - okay give it a moment to load
226:31 - and we can go into like customer or
226:33 - maybe product
226:35 - okay uh there should be a way to
226:37 - visualize whoops like that so maybe go
226:39 - design mode here
226:46 - give it a second to open up
226:50 - yeah so we can kind of see like what
226:52 - columns there are and stuff in here
226:53 - which is really great but what i want to
226:55 - do is create a new query
226:58 - and i just want to show you that you can
226:59 - query it in here too just query it
227:01 - everywhere right and so uh from our
227:03 - azure data studio that i still have open
227:05 - from before what we can do is grab
227:07 - ourselves uh this code here
227:09 - and paste it on in and go ahead and
227:11 - execute that and we can see we get the
227:12 - results so you know this is a very very
227:14 - very very powerful tool um you know and
227:17 - it's just i could do spend all day
227:18 - trying to show you how to use this uh
227:20 - tool here but um yeah i mean this is all
227:22 - you need to know
227:23 - so yeah there you go
227:26 - so that is ssms
227:28 - and i'll just go ahead and close that
227:30 - and there we go
227:31 - [Music]
227:36 - hey it's andrew brown from exam pro and
227:37 - what we're going to look at here is
227:39 - storage accounts and specifically blob
227:40 - storage tables and maybe azure files
227:44 - and so what i want you to do is go all
227:45 - the way to the top here and by the way
227:47 - i'm just carrying this over from my
227:48 - mysql or sql tutorial so something to do
227:51 - while i'm waiting for this thing to
227:52 - install but what we'll do is type in uh
227:54 - account
227:55 - or storage account so we'll go here
227:58 - and oh yeah just hit cancel there that's
228:01 - for my other tutorial
228:03 - we'll hit add
228:05 - and we'll create a storage account
228:07 - okay and what we'll do is i'll say dp
228:11 - 900
228:13 - storage it will say ok
228:16 - and we'll name this storage account
228:17 - we'll say we'll call it
228:19 - my storage account
228:21 - notice we have the option between oops
228:23 - one two three four five six there we go
228:25 - oh
228:26 - somebody really really wants uh to have
228:29 - the same name as me nine eight seven six
228:30 - five four three two one because those
228:32 - are unique names right um oh it doesn't
228:35 - like that uh can only be contains must
228:37 - be three characters is now too long okay
228:39 - there we go and so they're uniquely
228:41 - identifiable across all accounts that
228:43 - kind of gets annoying between standard
228:44 - and premium if you choose it you get a
228:46 - few different options so with premium
228:48 - you get block blobs file shares page
228:50 - blob so we're going to stick with
228:51 - standard to save some money we change
228:53 - the redundancy we'll stick with local
228:55 - redundant zones because that's the most
228:56 - cost effective go over to advance see
228:58 - what else we want here
229:00 - um
229:02 - here we have the option to turn on uh
229:05 - uh
229:06 - higher or higher cool name space that's
229:08 - if we're doing data like gen storage 2
229:10 - which we're not doing at this point in
229:11 - time so i'll do is go ahead and hit
229:13 - review and create
229:16 - and we'll scroll down and say create
229:18 - okay
229:21 - and we'll just wait for that to create
229:23 - the storage account i'll see you back
229:25 - here in a moment all right so we waited
229:26 - a little bit of time here and our
229:27 - storage account is ready so i'll go in
229:29 - here and what we'll do is go to the
229:31 - left-hand side we can see we have some
229:32 - containers we wanted to make a file
229:34 - share it's really easy just hit the file
229:35 - share button and we'll name it so say my
229:38 - azure file share
229:40 - okay
229:42 - and we can pick how the tiers we want um
229:46 - transaction optimize seems okay to me we
229:48 - have to set a quota
229:50 - uh
229:51 - we'll say i don't know one gigabyte
229:52 - we're not really going to do anything
229:53 - real here so i'm just gonna hit create
229:56 - okay and so now we have our nice file
229:58 - share there if we click into it we can
230:00 - uh upload files so i need to go grab
230:03 - some kind of file off the internet and
230:05 - we'll just type in star trek here
230:07 - anything
230:09 - images as long as it's appropriate and
230:12 - yeah here's a photo so go ahead and save
230:13 - that save image as
230:16 - and we'll go to our
230:18 - downloads here
230:20 - and we'll just say
230:23 - kirk
230:24 - and spock
230:26 - okay
230:28 - and that is now downloaded we'll make
230:29 - our way back here i'll say upload
230:34 - what we can do is go to our downloads
230:36 - again
230:37 - kirk and spock we'll say open
230:39 - upload and there's the file right
230:42 - uh not super complicated so that's that
230:46 - um let's go back over to here into our
230:49 - storage accounts
230:50 - uh back here
230:52 - and i want you to show you uh storage
230:54 - explorer so you can go ahead and
230:56 - download search explorer if you haven't
230:58 - so you go here you download it hit
230:59 - download now i think i already have it
231:01 - installed so all i'll do is just click
231:02 - open
231:03 - and say open takes like two minutes to
231:05 - install
231:07 - yep i do
231:08 - and this is just an easier way like if
231:10 - you don't want to have to open up the
231:12 - portal and you want to be able to easily
231:13 - work with files it's another way you can
231:14 - do stuff so
231:20 - we'll give it a moment to open and so
231:21 - here we have our storage account we can
231:23 - drop it down and see what's under there
231:24 - because we have a bunch of disks that i
231:26 - need to go ahead and delete
231:27 - but if we go here to file share we have
231:30 - our file share here
231:32 - and you can just see it's just an
231:34 - alternative interface we can upload and
231:36 - do stuff there if i wanted to download
231:37 - that i could do that okay
231:39 - but let's go take a look at blob storage
231:42 - okay
231:44 - so go over to containers we'll make
231:46 - ourselves a new container we'll say
231:48 - my blob storage
231:50 - uh yeah it can be private that's fine
231:55 - we'll say create
231:57 - we'll click into our blob storage
231:59 - and we need to upload a file so again
232:01 - we'll upload the same file so we'll go
232:02 - here
232:03 - and we'll upload our kirk and spock file
232:06 - here
232:07 - and we'll upload it
232:08 - and there it is again
232:10 - and if we go over to
232:12 - our storage explorer and we go under
232:14 - blob containers
232:15 - and there
232:17 - you can see we have the data there so
232:18 - that's really really nice
232:20 - um
232:21 - so there's that
232:23 - we should probably take a look at
232:26 - tables
232:28 - so go back to my storage account
232:30 - left hand side we'll make ourselves a
232:31 - new table so we'll say my azure table
232:35 - say okay
232:37 - we click into this actually we can't
232:39 - this is kind of a pain you'd have to use
232:41 - the api stuff but this is where the
232:42 - explorer comes in handy so on the
232:44 - left-hand side we will look for tables
232:48 - and we'll say my azure table
232:52 - and uh we'll say add
232:55 - okay we have to set a partition key and
232:57 - a row key so our partition key would be
233:00 - something like
233:02 - we could say wharf
233:04 - and our category could be lieutenant
233:07 - commander
233:09 - okay insert
233:11 - and uh if we wanted to edit this we
233:14 - could add more property such as like
233:17 - um
233:19 - you know
233:21 - planet
233:23 - kronos
233:24 - whatever you want you got all sorts of
233:26 - types here but it's pretty darn simple
233:28 - as you can see not super complicated
233:30 - there
233:31 - um but yeah maybe we should learn how to
233:34 - use the azure tables via the cli so i'll
233:37 - be back here in a second and show you
233:38 - how to do that okay
233:39 - all right okay so i have the commands
233:41 - that we can do to uh do that for the cli
233:43 - so what i want you to do is click up
233:44 - here in the top right corner that is the
233:47 - azure cloud shell there and i want you
233:49 - to start in bash if it asks you to
233:51 - create a storage account go ahead and do
233:53 - that
233:54 - because it needs a volume so under here
233:56 - we'll actually make one this is the one
233:57 - that is for my developer environment
233:59 - so just hit yes and just wait for this
234:02 - to load sometimes it takes a little bit
234:04 - of time
234:05 - and if it's giving you a lot of trouble
234:07 - i'm going to hit restart
234:11 - doesn't usually give me that much
234:12 - trouble but
234:26 - there we go okay great it's i'm so used
234:28 - to waiting around in azure it's crazy
234:30 - but uh so what we can do is type in a z
234:32 - that is the cli command for az if we hit
234:34 - enter it should spit out some
234:35 - information
234:38 - yeah so it tells us all the things we
234:40 - can do there so what we want to do i'm
234:41 - typing clear here az storage identity
234:45 - insert hyphen t we want to put the name
234:47 - of our table table is called my azure
234:50 - table
234:51 - uh we need the
234:53 - container uh account name we'll need we
234:55 - don't need the container name so the
234:56 - account name is
234:58 - up here so i'm going to try to type it
235:00 - in here i'm going to go ahead and paste
235:01 - that in and now we can go ahead and
235:04 - insert
235:06 - so yeah
235:07 - yeah we'll go here to the hyphen e so e
235:09 - is going to be what we insert so we have
235:10 - partition key
235:12 - and this will be uh we'll say uh
235:16 - beverly
235:17 - and we'll put the row key as a commander
235:22 - and then we could say planet
235:25 - equals earth
235:28 - okay
235:29 - and if we hit enter
235:31 - i think we got it all right there give
235:33 - it a second
235:35 - uh it doesn't know what account name is
235:39 - let me just see if i spelt that wrong
235:42 - maybe it just doesn't like where it is
235:44 - so i'm going to just copy it out of here
235:46 - trying to make a bit clean by putting it
235:48 - there but um
235:50 - i will put it on the end here
235:54 - oops go ahead and paste that in there
236:00 - still doesn't like it um if it doesn't
236:02 - like that we could try giving it the
236:04 - container name
236:07 - it might just pick up the account name
236:08 - which is the account we're in um but i'm
236:10 - going to go here and find my container
236:14 - that's where i had this like working
236:15 - like a second ago and just decides not
236:17 - to work okay
236:22 - okay so that's fine so i already have a
236:24 - working version here on the side so what
236:26 - i'm going to do
236:28 - i'm just going to change some of the
236:29 - values here
236:33 - okay
236:36 - and so i have this here and i know this
236:38 - 100 works right so we have
236:41 - um
236:42 - a z a z sword storage identity identity
236:44 - insert insert
236:46 - um
236:48 - hyphen t for the table so maybe that was
236:50 - my problem is that i just didn't do
236:52 - hyphen t there so i'll just paste that
236:53 - in there i don't think container is used
236:55 - anymore but we'll hit enter
236:58 - so it doesn't like the container name so
237:00 - we'll just erase that out there
237:08 - and so there it has inserted the data so
237:09 - there we go so yeah i'll just show you
237:11 - like azure is a bit painful sometimes
237:13 - even when you have perfect instructions
237:14 - it still doesn't work properly we'll go
237:16 - ahead and hit refresh and there is the
237:18 - record in our table database
237:20 - so there you go
237:22 - uh and i might just leave these accounts
237:24 - open here because they might want to
237:25 - adjust them in another tutorial but of
237:27 - course we will clean them up at some
237:29 - point under the end of the course okay
237:34 - [Music]
237:35 - hey it's andrew brown from exam pro and
237:36 - in this uh follow along what we're going
237:38 - to do is take a look at cosmodb so what
237:41 - i want you to do is go to the top here
237:42 - and type in cosmodb
237:44 - and we'll go here and add
237:47 - a new cosmodb account and notice that we
237:49 - have some options we have sql
237:51 - mongodb cassandra gremlin azure table so
237:54 - what i want to do
237:56 - i'm going to be pretty crazy here i'm
237:58 - going to make a new core sql one
238:01 - and so we'll just say under here i'll
238:02 - say cosmo
238:04 - dp900
238:06 - cosmodb
238:08 - okay
238:09 - and i'm just gonna call this one cosmo
238:13 - db
238:14 - coresql
238:17 - and just to be more you can say dp900
238:19 - here i don't know if this one conflicts
238:21 - with other ones so we have provision
238:22 - throughput and serverless i'm going to
238:23 - choose serverless because um i don't
238:25 - need provision throughput
238:27 - and there is a free tier here but i'm
238:30 - going to go with serverless okay just so
238:32 - we don't have to worry about it we've
238:33 - got a lot of options here like global
238:34 - distribution et cetera like that
238:37 - i don't care about any of that stuff
238:40 - and so what i'm going to do is go ahead
238:41 - and hit review and create
238:45 - okay
238:48 - and we're going to repeat this process
238:50 - again and again so what i'm going to do
238:52 - is go back to cosmodb
238:55 - create a new account
238:56 - and we'll create a mongodb one now
238:59 - and we'll drop this down and we'll
239:00 - choose cosmodb
239:02 - i think i called it dp900 cosmodb so
239:05 - this would be dp900 cosmo
239:09 - db
239:11 - db the name is not available you just
239:13 - have to change until you get it i'm
239:14 - going to make this one serverless as
239:15 - well
239:16 - and we'll hit review and create
239:19 - so i just want to show you the variance
239:20 - of these ones okay
239:22 - so this is a little bit of annoying but
239:24 - what we have to do
239:25 - we'll add another one here
239:27 - this one will be azure table
239:29 - we'll drop it down
239:31 - we'll choose uh dp900 cosmodb
239:34 - we'll say cosmodb
239:37 - uh we'll say dp900 cosmo
239:40 - db
239:41 - um
239:43 - azure table
239:45 - serverless
239:47 - go ahead review and create
239:49 - and we'll create that
239:51 - and we'll go one more time we'll do a
239:53 - graph database okay oops
239:55 - doesn't want us to go back
239:58 - and we'll say gremlin
240:02 - and we'll choose
240:03 - our dp900
240:05 - dp900
240:07 - cosmo
240:09 - db
240:11 - gremlin
240:13 - okay serverless review and create
240:16 - and we'll go back to here and i'm just
240:19 - going to wait until these are all done
240:20 - so i'll see you back when we see all of
240:22 - our i think we made four four of our
240:24 - cosmo dbs okay
240:26 - all right so after waiting a little
240:27 - while here actually my gremlin database
240:29 - didn't create or my account didn't
240:30 - create so i had to make that twice but
240:32 - here they are and you can see this one
240:34 - is still creating but we can go and do
240:36 - some things while we're waiting first
240:37 - let's go check check out the core sql
240:40 - and so on the right hand side you'll see
240:41 - there's this quick start it's always a
240:43 - great way to get started so if we wanted
240:45 - to
240:46 - uh you know in azure we could start
240:48 - inserting data so if i was to go over to
240:51 - node.js and we say create items
240:53 - container
240:55 - this is what i'm the most familiar with
240:56 - now a lot of people go over to the
240:58 - explore data but i'm just curious as to
240:59 - what this stuff looks like so
241:01 - because i've never used the quick starts
241:02 - before so i'm just curious to see the
241:04 - quality of it so let's go ahead and
241:05 - create that items container
241:07 - right now i think it's just waiting to
241:09 - create so if i go over to that data
241:10 - explorer tab
241:12 - uh it's probably just going through that
241:14 - process of creating so it actually
241:15 - created us a to-do list which is nice
241:18 - okay and it set up a partition key
241:21 - and oh looks like it's done so now we
241:23 - can go ahead and download that
241:26 - application says npm install npm start
241:29 - it's kind of cool
241:32 - and i have it over here so i just got to
241:34 - unzip it
241:37 - okay so i'm just unzipping that here
241:41 - choosing winrar that's what i got
241:42 - installed and we'll just drag that on
241:45 - over here
241:47 - and i wonder
241:49 - we'll go up here
241:50 - now you may or may not be able to easily
241:52 - open this depending on how your setup of
241:54 - your computer is so if you do you can't
241:55 - just you can just follow along here i'm
241:57 - opening up visual studio code here you
241:59 - just can't see it's off screen
242:02 - but i just want to close some things in
242:04 - here
242:06 - okay
242:07 - and i'm just going to go open
242:10 - go here file
242:12 - open a folder
242:14 - and this is in my downloads
242:16 - so i'm going to navigate all the way to
242:18 - my downloads here so we'll say um
242:20 - [Music]
242:21 - this is users andrews
242:24 - if you want to see me i'm just writing
242:25 - this over here downloads
242:28 - sql and we'll say okay
242:33 - and what we'll do is open up a new
242:34 - terminal here
242:36 - and the instructions said that we should
242:38 - be able to do
242:40 - npm install npm starts we'll give that a
242:42 - go we'll say npm install
242:52 - i might as well take a look here at the
242:54 - code just curious what's going on here
242:56 - so it's actually using the azure's cosm
242:58 - yeah cosmodb client
243:00 - it has a config file over here which
243:02 - contains uh our configuration
243:04 - information looks like it has some
243:05 - initial items that it's going to insert
243:08 - so there's some initial data which looks
243:09 - kind of cool
243:11 - it sets up a partition key here we have
243:13 - an endpoint so
243:15 - what it does create the database if it
243:16 - does not exist
243:18 - read the database definition create the
243:20 - container if it does not exist
243:22 - read the container scale the container
243:27 - create family item
243:29 - query the container
243:32 - so there's a lot of examples here so
243:34 - here it just gives you some stuff so you
243:36 - know if you know you're coding and you
243:38 - want to go through this it's a great way
243:39 - to get started it looks like
243:41 - finished installing we'll do an npm
243:42 - start i don't know if i'll be able to
243:43 - actually view this
243:45 - because it's running through a
243:47 - machine oh so it just runs it okay i
243:49 - thought i was going to like uh
243:50 - output a um
243:52 - like a localhost 3000 you could view it
243:54 - so it actually just ran that stuff so
243:55 - create the to-do list reading created
243:57 - etc
243:59 - so yeah all of it's there so if you're
244:01 - familiar with that you can take a look
244:02 - but what we'll do now is make our way
244:04 - over to day explorer we can also go up
244:06 - to cosmos
244:08 - uh was it
244:10 - cosmos.azure.com is that what it is
244:13 - there we go this has a little bit more
244:15 - room i'm going to click on sign in
244:17 - it's the same thing as the date explorer
244:18 - just a little bit easier to work with
244:20 - subscription one
244:22 - and we are in sql here we'll drop it
244:24 - down now we actually have some data we
244:25 - can take a look at some good items
244:28 - check out items here
244:31 - and
244:32 - it should show us some right
244:37 - hmm
244:40 - go back here
244:46 - it says
244:48 - first name etc so you can see that it
244:50 - created the to-do list reading the
244:51 - database create the items reading
244:55 - oh completed with an error entity with
244:57 - the specified id does not exist in the
244:58 - system okay so it's expecting something
245:00 - to already exist there and that's why it
245:02 - couldn't run
245:04 - so maybe what we'll do is just go ahead
245:06 - and create what it wants okay
245:09 - so while we go ahead and create a new
245:10 - item so the partition key is called
245:12 - partition key that's a very uncreative
245:15 - name but when you create these records
245:17 - you always have to have the id and the
245:18 - partition key name so i'll go here and
245:21 - we'll type in partition key
245:24 - and i guess it could just be whatever we
245:25 - want
245:27 - um i'm just looking at the record here
245:29 - to see what they wrote
245:31 - so we just scroll up here
245:35 - we are looking for create item
245:39 - just scroll up here see if we can find
245:40 - it
245:46 - this is more like reading an item here
245:47 - i'm not necessarily creating
245:50 - partition key at sign country usa so
245:53 - um
245:55 - i guess we just say usa here
245:58 - say name
246:00 - oops
246:02 - andrew
246:04 - go see if he'll let us create that
246:07 - uh we'll go ahead and just copy this
246:09 - oops
246:11 - place with id so we'll just say one
246:13 - we'll say save
246:15 - okay and it added some additional data
246:16 - underscores ts is for uh for a time
246:19 - stamp okay
246:20 - um and now what we can do is go and look
246:23 - at that data so we'll hit more oh i
246:25 - guess we're exploring it right there
246:26 - okay so let's say we wanted to query
246:27 - that data we could create a few other
246:29 - records because that's not a lot of
246:30 - information right so what i'll do is
246:31 - just copy this we'll create a new item
246:33 - here
246:35 - uh i'm technically not the usa i'm in
246:37 - canada so we'll put in vaco here
246:42 - just copy that and we will save
246:45 - we'll create a new item
246:47 - uh they should have been different ids
246:49 - but whatever
246:51 - we'll say brazil
246:53 - put in roger
246:56 - and we will go ahead and save okay and
246:58 - so now let's say we wanted to query that
246:59 - data uh
247:01 - it should be
247:03 - these icons are very cryptic
247:05 - let's store procedure whoops
247:09 - um
247:12 - yeah i guess we could do it right here
247:16 - no
247:17 - usquery that's what i want okay so
247:20 - uh if we hit run
247:22 - execute query to return us all the data
247:25 - but let's say we wanted to only select a
247:26 - subset of data so we just do where
247:29 - um
247:30 - name
247:31 - equals andrew let's give that a go
247:35 - see if that works
247:36 - oops
247:39 - uh what if we do c dot name
247:44 - there we go so yeah it's as simple as
247:46 - writing queries not like any other kind
247:48 - of sql so
247:49 - that's pretty easy let's take a look at
247:52 - another one here so that was that let's
247:54 - close that off and we'll take a look at
247:55 - mongodb next
247:57 - all right let's take a look at mongodb
247:59 - so we'll go ahead and click into here
248:00 - make our way to quickstart and we have
248:02 - some options we have node.js so if you
248:04 - wrote javascript and use the connection
248:05 - girl that's one way of doing it i think
248:07 - mongodb might be the shell might be a
248:09 - fun way to do it so what i'll do
248:11 - so go over here i don't think i have
248:12 - this installed but it comes part of the
248:14 - server installation for mongodb so i'll
248:16 - go ahead
248:18 - if you would like to download the
248:19 - sell shepardly from server yeah that
248:21 - sounds better to me
248:23 - i don't think i need everything and so
248:25 - select the zip download which includes
248:28 - the
248:30 - okay
248:33 - and i guess we'll have to click on the
248:35 - computer community edition here
248:39 - oh
248:39 - windows
248:41 - we can do the zip or msi yeah i guess
248:44 - we'll just install the full thing it's
248:45 - not a big deal
248:47 - maybe i'll use it for something else
248:50 - but we'll go there it's only 200
248:51 - megabytes so we'll just wait for that to
248:53 - download
248:54 - shouldn't take too long here
248:57 - there we go
248:59 - and i just got my downloads over here
249:01 - we'll open that up
249:07 - sure we'll go ahead and hit next i agree
249:10 - next
249:11 - i guess we can customize it
249:15 - so maybe if we only wanted the client
249:18 - i'm going to just take everything
249:20 - um
249:21 - we'll say next that seems all fine to me
249:23 - we'll hit next install
249:31 - this process is on a mac or linux it's
249:33 - just going to be different process but
249:34 - yeah we'll go ahead and see this here
249:40 - all right so after waiting a very long
249:41 - time uh on that compass step it finally
249:44 - uh finished installing i guess it was
249:45 - doing a visual installer there it looks
249:48 - like it is opened
249:50 - and uh i guess we installed
249:52 - compass and it makes it really easy cool
249:54 - uh
249:55 - sure we'll say yes to everything there
249:57 - there's a little bit too much
249:57 - information
249:59 - but i guess what we can do is establish
250:00 - a connection and i guess i was thinking
250:02 - we just use the shell but this looks
250:04 - better so what we'll do is go ahead and
250:07 - take a look here so we will go grab our
250:09 - connection string
250:11 - and hopefully you'll just take it as
250:13 - that it looks like it's hold on here
250:17 - i think what we really want is this
250:20 - this is the string here connection
250:22 - string
250:23 - uh i guess we can grab it from here it's
250:25 - a bit easier
250:26 - we'll go back to our connection service
250:29 - here we'll hit connect
250:30 - fingers crossed it works first time
250:32 - yeah there we are we're in okay cool so
250:35 - uh i it's a new data like it's
250:37 - completely new so i guess we wouldn't
250:39 - have anything in it like a database or
250:40 - anything like that so we can do at the
250:43 - same time as we're doing it let's make
250:44 - our way over to cosmodb
250:47 - um oh i guess i closed it
250:49 - cosmos azure.com
250:52 - and we'll go down and flip over to our
250:54 - mongodb database
250:57 - and we don't we haven't created anything
250:58 - yet so we have to make a new database so
251:00 - i'll just say
251:01 - my database
251:02 - or my mongodb
251:05 - my collection can you think of a
251:07 - collection as like a table right
251:09 - uh shared key
251:11 - whatever you want uh my shared key
251:14 - again i'm just showing you how to insert
251:15 - stuff
251:17 - uh we don't need this sharded so we'll
251:19 - just say unsharded okay
251:22 - makes it a bit easier on us here
251:24 - i just want to see if that actually
251:25 - reflects over here in our manga db
251:27 - compass
251:29 - because it probably should
251:32 - we'll give it a moment there to create
251:33 - the database
251:35 - if it doesn't what we can do because
251:36 - there's a reload here right so let's hit
251:38 - reload until the data loads
251:43 - and
251:44 - yeah looks like we have it now so that's
251:46 - all good and i guess we have to
251:50 - click on this maybe click on this one
251:51 - again here to reconnect
251:54 - great now we have a database
251:56 - so we can go in here
251:58 - click on my collection
252:00 - we can add data let's hoping to execute
252:02 - a command oops
252:05 - and i'm just trying to see if it allows
252:07 - us to do that here in any way
252:09 - again i'm not that familiar with this
252:10 - tool
252:11 - but yeah i guess we can go create a
252:13 - document here we can make it through
252:14 - here so let's go to documents
252:16 - we'll add a new document
252:18 - and say one
252:20 - and
252:21 - i guess we should probably put our shirt
252:23 - key so my shared key
252:26 - say
252:27 - blue
252:30 - we'll just say name
252:32 - andrew
252:35 - and we will go ahead and save that
252:36 - document
252:38 - we'll go back here
252:40 - uh refresh
252:41 - we can see our document there we can
252:43 - edit it in here as well a lot nicer in
252:45 - this say we can add data we'll say
252:47 - document
252:50 - it's a little bit more cryptic
252:52 - i think i prefer to do it in there i'm
252:54 - again i'm not that familiar with this
252:55 - tool but what i really wanted to show
252:56 - you was the command shell so what we'll
252:59 - do
253:01 - oh let's just do shell up here
253:04 - oh that's nice okay i thought we were
253:06 - going to have to uh you know i thought
253:08 - we're going to have to paste this in so
253:09 - maybe we didn't have to install that all
253:11 - along but hey this looks pretty good to
253:13 - me right
253:14 - so what i have pulled up here is the
253:18 - api i had here a second ago at least
253:22 - say mongodb shell
253:24 - i definitely had it open here for us
253:26 - um because i wanted to show you the api
253:33 - uh so there's like right
253:36 - one
253:40 - okay here we go so i found the mongodb
253:43 - api here and if i just go back it's
253:45 - shell method so if you typed in
253:47 - mongodb.com for slash mail reference
253:48 - methods you can see all the kind of
253:50 - stuff that we can do
253:51 - and so let's just go ahead and see if we
253:53 - can insert something we only need to
253:55 - insert a single document and so here we
253:58 - kind of see the uh the stuff here yeah
254:00 - underscore id is the the main field
254:02 - there and what we'll do
254:05 - is scroll on down and we'll just go
254:07 - ahead and copy this command and see
254:10 - if we can get that to work okay
254:13 - so we'll go back
254:15 - over here give that a paste
254:18 - hit enter
254:21 - and we'll go back to our document and
254:22 - see if it's there
254:24 - there it is
254:25 - we will go back and check in our mongodb
254:27 - cluster here give it a refresh
254:31 - i want to see
254:35 - uh well it shows two records
254:40 - so what i'll do
254:42 - yeah it's not what i want so we'll try
254:44 - this again
254:50 - yeah i just started typing it six i
254:52 - already have an error
254:53 - and we want the my shared keys we'll say
254:56 - my shared key
255:00 - okay
255:02 - double quotations we'll say green
255:05 - and we'll just put in here name
255:09 - baco
255:11 - we'll say insert okay that record is
255:13 - there
255:14 - uh we'll give this a refresh here oops
255:18 - there it is there's the record as well
255:23 - it didn't insert the other one i wanted
255:25 - so we'll go back into our shell here
255:27 - we'll say db.products it's probably
255:29 - because i didn't i you know if you don't
255:31 - have the um
255:33 - partition keys just not going to insert
255:34 - it right i thought it would error out or
255:36 - say something that's what i was looking
255:37 - for so i'm going to say id we'll say 5
255:41 - i'm going to put my shared key
255:44 - we'll say red and we will put name here
255:48 - and we'll say roger
255:49 - we'll give that a go
255:53 - and if you don't type it right it will
255:54 - not work so we'll say insert one
255:58 - there we go we'll go back to our
255:59 - document here we'll give it a refresh
256:01 - here
256:05 - i'm having like no luck doing inserts
256:07 - today
256:09 - but you know you get the general idea
256:11 - right so we're not here to really teach
256:13 - you a mongodb tutorial but i just want
256:14 - to show you that you know you use the
256:16 - shell to insert data there's this tool
256:19 - here and what a document looks like so
256:21 - that pretty much covers the document
256:23 - mongodb so what we'll do
256:25 - let's go back here and let's take a look
256:27 - at azure tables alright so let's take a
256:29 - look at cosmo db azure table so click
256:31 - into it we'll make our way over to the
256:32 - data explorer
256:34 - we will say new table and i'll just say
256:36 - my new table
256:38 - say okay
256:42 - and we'll give it a moment to create
256:44 - this new table here
256:48 - as you can see we didn't have to make it
256:49 - a database or anything it was just a lot
256:51 - more straightforward here
256:55 - but it does seem to take a long time to
256:57 - make a table to be fair it is serverless
256:58 - so it could be uh having to provision
257:00 - something before it do something we'll
257:02 - go here and look at entities and we can
257:04 - go ahead and add ourselves
257:07 - uh a new entity here we go up the top
257:09 - here so we'll just say worf
257:12 - lieutenant commander
257:16 - planet
257:17 - kronos
257:22 - add entity and there it is
257:25 - okay
257:27 - go maybe over to the query builder
257:32 - and if we wanted to filter this out we
257:34 - could say partition key i guess we need
257:36 - another record for this to make sense so
257:38 - we'll say um
257:40 - data
257:41 - uh
257:43 - commander
257:46 - um okay he's not from earth but we'll
257:48 - just put earth
257:49 - we'll add that entity we'll add another
257:51 - one might as well
257:53 - we'll say um crusher
257:57 - commander
257:59 - earth
258:02 - and we can go here we just filter it out
258:04 - so we'll say crusher
258:07 - uh partition key yep that's right so it
258:09 - should be able to run that so run query
258:12 - okay gets that exact record so pretty
258:14 - simple not too complicated and so now
258:17 - we'll move on to gremlin
258:19 - all right so let's take a look at
258:20 - cosmodb gremlins so click into it and
258:22 - what i want you to do is go to quick
258:23 - starts we'll go to guided gremlin tour
258:26 - and we'll create ourselves a sales graph
258:28 - collection so we get a little bit more
258:29 - information that we normally would have
258:34 - give it a moment here
258:44 - and as there we go yep it's ready so we
258:46 - created the sample this downloaded a
258:48 - package that contains a console app as
258:50 - dot net the console app needs to be
258:51 - executed first to upload the sample data
258:54 - and the asp.net web app will allow you
258:55 - to visualize your data okay so i guess
258:57 - we have to go ahead and do that so we'll
258:58 - go ahead and download that
259:02 - and there it is
259:04 - i'm just going to unzip it here
259:07 - and we'll double click and do it
259:10 - and it doesn't look like much but i'll
259:13 - drag it out into my folder here
259:16 - whoops
259:18 - and we will go and click into this
259:22 - and
259:23 - i want to know how to use this so i
259:24 - guess we'll open this up in
259:27 - vs code
259:31 - okay let's give this a read
259:35 - sample application shows to interact
259:36 - with the cosmic db gremlin etc etc
259:39 - visual studio code 2015
259:41 - sample data has two projects uploading
259:43 - the sample data is done via the console
259:45 - application project included in the
259:46 - quick start
259:48 - run the console app
259:55 - all right so it's saying that we need to
259:57 - go to tools
259:59 - and we're going to double click this
260:01 - and luckily i have visual studio
260:04 - installed if you don't have install you
260:05 - don't have to go do this but we just
260:07 - want to see a nice looking gremlin graph
260:09 - in our cosmodb
260:12 - and i will go ahead and sign in
260:22 - and i'll use my example account
260:32 - and i guess it really wants me to choose
260:34 - some stuff but i'm fine let's just go
260:35 - ahead and start it
260:49 - uh the c sharp project
260:51 - is targeting.net which is not installed
260:53 - in this machine to proceed select the
260:54 - option below change the target
260:57 - you can change it sure
260:59 - as long as it runs you know
261:01 - i didn't feel like i was going to be
261:02 - doing.net today but we'll go ahead and
261:04 - open this up
261:05 - all that really matters is that we can
261:07 - run it projects loaded ready to use in
261:09 - the background and so we need to
261:11 - run this
261:14 - so it says over here
261:19 - so verify the settings if you download
261:21 - it etc okay if the console is run upload
261:23 - etc i just want to run it run the
261:25 - application f5 so i think it's time to
261:27 - hit f5 on my keyboard
261:29 - and so i have a mac keyboard so i'm not
261:31 - sure if that's going to work but what
261:32 - i'll do
261:34 - is i'll go to the top here oops
261:39 - i might need to install extra components
261:40 - so we'll go ahead and install whatever
261:42 - components it wants
261:44 - just click that over here in the right
261:46 - hand corner there
261:47 - it's probably because it needs a very
261:48 - particular version of net yeah it needs
261:50 - the.net desktop development here
261:53 - even though i'm not a dotnet developer i
261:55 - still know my way around ids pretty darn
261:57 - well
261:58 - so we'll go ahead and give that an
262:00 - install
262:04 - and it looks like it's at 848 megabytes
262:06 - i'll see you back here in a moment okay
262:11 - all right so after waiting a little
262:12 - while here it looks like it's installed
262:13 - and restarted and i think i can actually
262:16 - run the application now so we'll go at
262:18 - the top here
262:20 - and we will run let's say
262:26 - should be like a project run i guess we
262:27 - just hit the start button
262:29 - that works too right
262:32 - you.net fans that are watching you can
262:34 - make as much fun as you want to be okay
262:37 - uh and so you know we just want this to
262:39 - run so that it loads the data right
262:41 - that's all this thing does so it's going
262:42 - to build the project
262:50 - and it's complaining about virus and
262:52 - threat protection not really worried
262:53 - about that
262:55 - and it said it succeeded verified the
262:57 - database exists your collection etc
262:59 - uploading the graph now excellent
263:02 - there we go
263:06 - and
263:08 - we're just going to wait for it to
263:09 - upload those nodes and we're going to
263:11 - have a really really good example here
263:12 - right
263:19 - i'm not sure how many nodes there are in
263:20 - this
263:21 - could be a lot
263:26 - there we go oh now it's starting up
263:28 - edges great
263:31 - this is 469 nodes i wonder if there's
263:33 - going to be as many edges or less we'll
263:35 - see
263:39 - there we go so now it says graph
263:41 - uploaded you can now show this
263:43 - application
263:44 - okay sounds great so what we'll do and
263:47 - i'll just go ahead and close visual
263:49 - studio because i don't need it open
263:50 - anymore
263:51 - and
263:52 - what we'll do is open up the data
263:54 - explorer
263:56 - actually kind of prefer cosmodb so we'll
263:57 - type in cosmodb again
264:00 - and
264:01 - we will
264:03 - switch over to
264:06 - our gremlin database
264:08 - here and go into persons
264:12 - graph database
264:15 - you should be able to see a
264:16 - visualization without have well i guess
264:17 - we can just hit execute search query
264:24 - hmm
264:25 - maybe insert it in this one we'll go
264:27 - ahead and execute it over here
264:30 - there we go so now we have some records
264:32 - so cool
264:33 - all right we got that working and um so
264:36 - yeah this is like that gremlin language
264:38 - up here um i'm not that familiar with it
264:41 - so you know what i'm going to do i'm
264:42 - going to be right back and uh and learn
264:44 - it in two seconds okay
264:45 - all right so i'm back and i learned a
264:47 - little bit here and what i did was i
264:48 - went to the apache gremlin website for
264:51 - tinker pop or gremlin in particular so g
264:54 - dot v parentheses is when we want to get
264:56 - all records back if we put in a
264:58 - particular id here it should give us a
265:00 - particular result i also figured out as
265:01 - we move around here if you scroll into
265:03 - things like this
265:05 - and you click on there it will expand
265:07 - the next area it should there it goes
265:09 - it's a bit slow right
265:11 - but you know we have an id here so let's
265:12 - give it a go and see if it actually
265:14 - works so we'll do g v
265:16 - and
265:18 - let's see if we put that in there like
265:19 - this we'll give it uh maybe quotations
265:22 - they just have the number one so i'm not
265:24 - sure to tell you there and yeah so it
265:26 - grabbed that particular point there
265:28 - um it looks like we can grab different
265:30 - values so i'm assuming that these are
265:32 - values on it here so maybe we'll just do
265:34 - dot values here
265:37 - values
265:39 - uh well if we give a label here
265:43 - there we go service plan so yeah it's
265:44 - not that hard go back here execute the
265:48 - plan and then there's like commands like
265:50 - out e i assume that's the way of like
265:51 - you would select things in the region so
265:54 - um
265:55 - i don't know if like that is a
265:57 - definitive one but we'll just go ahead
265:59 - and put that in there and see what
266:00 - happens
266:04 - and we got nothing
266:07 - because you know we have relationships
266:09 - between other sources like edges
266:13 - and so maybe it's based on the label
266:15 - here so let's try this
266:21 - execute the plan
266:31 - yeah i'm not sure what it is
266:33 - but my point is is that we populated
266:35 - ourselves a graph you can tell that
266:36 - that's how you use the language if you
266:38 - really want to learn more you can go
266:39 - through the tutorial
266:41 - uh on how to create that information but
266:44 - for our purposes we just wanted to show
266:46 - that we could popular graph and play
266:47 - around with one there a bit okay
266:49 - so that is gremlin we did
266:53 - we did azure so that's all of them so
266:55 - we're all done here again i'm going to
266:57 - keep these around just in case i want to
266:58 - pull them into other services and at the
267:00 - end we will destroy all these resources
267:02 - okay
267:03 - [Music]
267:07 - hey this is andrew brown from exam pro
267:09 - we're going to learn how to use data
267:10 - factories to run a transformation job
267:12 - maybe between our sql and our blob
267:14 - storage because we do still have the
267:16 - setup from previous tutorials so i'll
267:18 - make my way over to factories you just
267:19 - type in data factory at the top here and
267:22 - you can see i have one there it's right
267:24 - now deleting because i'm not happy with
267:25 - it
267:26 - but what we'll do is create ourselves a
267:27 - new one so i'm going to make a new group
267:29 - here dp900 data factory 2
267:34 - okay
267:36 - and here i'll have to name it so we'll
267:37 - say dp900 data factory
267:42 - we'll go with version two it doesn't
267:44 - like it so i'm gonna put a two on the
267:45 - end there
267:46 - and we'll go to next i do not want to
267:49 - use git at all that was the reason i uh
267:51 - i restarted this was because it was
267:53 - complaining about that
267:54 - for networking it's going to have a
267:56 - public endpoint for we have the ability
267:58 - to encrypt our data which we do not care
267:59 - today i'll go ahead and hit review and
268:01 - create
268:03 - we'll go ahead and create arc data
268:04 - factory
268:06 - and we'll give it a moment here
268:11 - it doesn't take too long to set up a
268:12 - data factory so
268:16 - there it is so we'll go ahead and go to
268:17 - resource and the way we're going to
268:19 - actually access it i don't know why they
268:21 - have the button here but this is where
268:22 - it is it's author and monitor it's going
268:24 - to open up takes about a second and here
268:26 - we are so what we'll do is we want to
268:28 - create ourselves a new pipeline we'll
268:29 - make it easier on ourselves go to the
268:30 - left hand side hit the plus create a
268:32 - pipeline
268:34 - and we'll say sql to
268:36 - [Music]
268:37 - um sql to what
268:40 - it's a bit glitchy there sql to blob
268:44 - okay
268:45 - and we'll go ahead and save that um i
268:50 - think it uh
268:51 - does it auto save yeah it auto saves so
268:54 - what we'll do is go to the left hand
268:55 - side here go to link services click the
268:57 - plus we need sql we have an sql database
269:00 - so we'll go here we'll say
269:02 - continue we'll have a connection string
269:05 - we'll select our subscription our server
269:08 - name our database name our type is sql
269:10 - authentication azure user
269:14 - capital t testing one two three
269:17 - okay you can have always encrypted on
269:19 - there i'll go ahead and create that you
269:20 - can hit test connection if you're
269:21 - concerned that it's not working i think
269:23 - we can click into it hit text test
269:25 - connection there it's got a nice little
269:27 - green so that's all good we want to put
269:29 - this in our blob storage so i will add
269:32 - another one here
269:33 - we'll type in blob
269:35 - we will create that hit continue
269:39 - i don't care about the name but what
269:40 - we'll do is use a connection string to
269:42 - select it as well
269:44 - so we'll go down here subscription one
269:46 - it's my storage account
269:49 - and we will test the connection and it's
269:52 - all good we'll hit create
269:55 - cool so we'll now go back to our
269:57 - pipeline or our authoring here we need a
270:00 - data set um
270:02 - can we get that from sql we sure can
270:06 - we'll click on that we'll hit continue
270:09 - we'll select our linked service which is
270:11 - here
270:12 - we'll select the table we want so let's
270:14 - say we want to translate over the
270:15 - products
270:17 - and we will go hit ok
270:21 - all right and so now we have our data
270:23 - set
270:25 - okay and so we'll move on to the next
270:26 - step all right so let's go set up a
270:29 - transformation so what we'll do is drag
270:31 - out copy data and we will choose our
270:33 - source and we'll say our sql table and
270:36 - we're going to want to put that into a
270:37 - csv oops i did not create our data set
270:39 - here so we'll create a new data set go
270:42 - over to blob storage here
270:44 - hit continue choose csv format so you
270:47 - can do excel json all fun things
270:50 - we'll choose our link service which is
270:51 - blob storage uh this has to be the name
270:54 - of our blob storage i already forgot
270:57 - what it's called so we'll go up here
271:00 - make our way over to storage accounts
271:03 - on the left-hand side go into storage
271:04 - accounts we need to find that container
271:06 - name
271:07 - there it is
271:08 - go all the way back
271:10 - put that in there
271:12 - call that transform and we'll say
271:14 - products dot csv
271:16 - first row is header
271:18 - sure but we don't have any import scheme
271:20 - because like you could have
271:22 - one in there and say okay this is what
271:23 - the schema should be like have the
271:24 - headings in there so it knows what it
271:26 - needs to translate over to but uh it's
271:29 - going to be a one-to-one mapping so it's
271:30 - going to be something really simple here
271:33 - uh so this seems all fine to me notice
271:35 - we don't have any schema we could import
271:38 - one but there's nothing to import so go
271:40 - back to our copy data we'll choose
271:42 - delimited file and we'll take a look at
271:44 - mappings we'll import schemas it's going
271:46 - to import the schema from the table here
271:48 - are the column names here so as you can
271:51 - see there are no column names which is
271:52 - totally fine
271:54 - and uh what we can do
271:56 - is go ahead and let's go hit uh debug
272:02 - okay and so it's queued up
272:06 - and now what we'll do is go ahead and
272:07 - validate
272:10 - okay
272:15 - and it says it's been validated no
272:17 - issues as of yet
272:19 - we'll go hit publish
272:22 - um yep go ahead and publish it
272:27 - and so now it has actually ran the
272:29 - pipeline so what i want you to do is go
272:31 - over to your storage accounts
272:33 - give it a refresh
272:34 - go into my blob storage now we have a
272:36 - folder called transform we have a
272:38 - product called uh
272:39 - csv called products we'll go ahead and
272:41 - download that
272:44 - okay
272:47 - we can go ahead and open that up in
272:48 - excel
272:49 - and there you go so we just did a
272:51 - transformation it's not the most
272:53 - beautiful transformation but you can see
272:54 - that it's a very powerful tool we didn't
272:56 - have to write any any code whatsoever
272:58 - which is really nice so there you go
273:05 - hey it's andrew brown from exam pro and
273:07 - in this follow along we're going to take
273:08 - a look at a data bricks and sparks so
273:11 - what i want you to do is go to the top
273:12 - and type in azure data bricks we'll go
273:14 - ahead and hit add we'll create an azure
273:16 - databrick service
273:17 - and what we'll do is hit create dp900
273:21 - azure databricks
273:23 - okay
273:25 - within our workspace here what we'll do
273:27 - is choose
273:28 - dp900 azure databricks very uncreative
273:31 - of me but
273:33 - it works out fine
273:36 - and we have some options standard
273:38 - premium trial i'm going to stick with
273:39 - standard i do not want this to cost much
273:41 - whatsoever
273:43 - go ahead and create that environment
273:45 - there
273:48 - and i'll see you back here in a moment
273:49 - when it's deployed
273:51 - all right so um our databricks
273:53 - environment is ready so if you click
273:54 - launch workspace it does take a little
273:55 - bit time to launch so i'm not pressing
273:57 - it but we'll have our environment here
273:59 - and so uh you know there's stuff you can
274:01 - do here like create clusters and run
274:04 - jobs and notebooks but uh just so you
274:06 - don't spend any money i'm going to show
274:08 - you another way you can run this service
274:09 - which is totally uh totally won't cost
274:11 - you anything so we'll go to databricks
274:14 - and type in community edition
274:16 - if we go down below below here i already
274:18 - have a log if you sign up here
274:21 - it's exact same thing pretty much
274:24 - but there's no chance of you having
274:26 - unexpected spend so i prefer to do it
274:28 - this way even the cluster is shut off
274:30 - over a certain amount of time and so
274:32 - we'll click on the explore quick start
274:33 - tutorial here
274:35 - and i think it's shift
274:37 - enter if you want to the commands it's
274:38 - in the top right corner this little
274:39 - keyboard thing it tells you what you can
274:41 - press so run command and move to next
274:43 - cell that sounds like what we want to do
274:45 - so i'm going to hit
274:47 - shift enter
274:48 - shift so it says in the sidebar create a
274:50 - cluster etc but if we just keep on going
274:53 - here it should automatically create this
274:55 - one so automatically launch one without
274:57 - to clusters without prompting so we'll
274:58 - just hit attach and run okay
275:01 - and that's going to start up a cluster
275:03 - but as it's running we can just kind of
275:05 - take a look here and so you can see that
275:07 - we're running sql commands
275:09 - um and so it looks like we're it has
275:11 - we're loading a csv and then we're
275:14 - treating the csv with sql which is
275:16 - really nice so that's the thing where
275:17 - it's like you have data that's not
275:18 - necessarily in a database but you're
275:20 - able to run sql commands on csv files
275:23 - and things like that and then down below
275:25 - you can see uh it's using python to use
275:27 - spark to load and format the file
275:30 - um and uh and you can even visualize
275:32 - information in here so it automatically
275:34 - let us plot it and change some of the
275:36 - options here so we go here make a
275:37 - histogram
275:38 - quantile
275:40 - map that's useless area
275:43 - bars so it's very very useful tool but
275:46 - wait till that cluster spins up here
275:50 - takes a little bit of time
275:52 - if you want to monitor a cluster we can
275:54 - go on the left-hand side and just go to
275:56 - clusters here
275:58 - okay and you could leave this and and
276:00 - and totally not be worried about it you
276:02 - don't have to worry about it like
276:03 - spinning up and costing you money okay
276:06 - so i'd go here and give it a refresh
276:11 - and i don't see the cluster
276:17 - so i think it's just because i was
276:18 - terminating an old one and now it's it's
276:20 - waiting until it can determine uh start
276:22 - up a new one
276:24 - so
276:26 - it might not spin up but it's not really
276:28 - that oh there we go it's terminating
276:31 - great
276:35 - uh is it going now
276:39 - you can never tell these things
276:43 - is the free edition
276:44 - so but if i just go back here we can
276:47 - even look at the commands here cyborg
276:49 - create cluster quick start and database
276:51 - runtime drop down to l3 create a cluster
276:54 - um so is it going to create cluster now
276:58 - nope we'll cancel there yes
277:02 - and let's just follow it manually so i
277:04 - go to clusters create the cluster
277:07 - here it's saying l3 lts
277:12 - kind of gives you an idea how old their
277:14 - tutorial is
277:16 - lts
277:17 - but that's what it wants we should match
277:19 - it exactly
277:20 - sparking scala i doubt that it's for
277:24 - that one there
277:26 - and so
277:27 - we will just name our cluster quick
277:29 - start
277:31 - oops
277:34 - right after the name of what it asked
277:35 - for
277:36 - oops
277:40 - and we'll go ahead and create
277:48 - and we'll i guess wait till that starts
277:50 - up
277:52 - it could take about a few minutes to
277:53 - start up so what i'll do is i'll see you
277:55 - back here in a moment when it started
277:56 - okay
277:58 - all right so we had to wait there a
277:59 - little bit but uh now it looks like our
278:01 - cluster is running so what i want you to
278:03 - do is go back to that notebook we had
278:05 - and this time it should possibly we'll
278:07 - close the other one because that is our
278:09 - the one we don't want to use but we'll
278:11 - do shift enter shift enter shift enter
278:14 - um it's not would you like to launch a
278:17 - new cluster
278:18 - no i want to use the existing one maybe
278:20 - i can drop down and choose the cluster
278:22 - oh here we go um
278:25 - cancel hold on here
278:27 - quick start confirm
278:30 - there we go okay
278:32 - and we'll see if that executes the
278:33 - command
278:45 - now it's a bit slow but again this is a
278:47 - community edition and also imagine if
278:49 - you're dealing with massive amounts of
278:51 - data so it's not really surprised that
278:53 - it's not like super fast um
278:55 - but like you have to think of it at
278:57 - scale right and so it completed the job
278:59 - so that was pretty good
279:01 - we can go ahead and run this line again
279:02 - for fun
279:07 - okay
279:08 - so you get the idea here right
279:11 - so that's all i really wanted to show
279:12 - you here um just kind of a bit of
279:14 - exposure to data bricks there
279:16 - but you'd have to learn all about apache
279:18 - spark to really understand the stuff
279:20 - behind it
279:21 - um but uh yeah so what we'll do and
279:23 - again you don't have to shut it off it
279:25 - doesn't matter but what we'll do is just
279:26 - terminate and be be good about that
279:30 - also we'll delete this at the end all
279:32 - these follow alongs for the dp900 we'll
279:34 - definitely be sure to delete all of them
279:36 - but just for this one in particular i
279:37 - just want to get rid of it
279:39 - so i don't forget about it um yeah there
279:41 - you go that is
279:46 - databricks hey this is andrew brown from
279:49 - exam pro we're going to be looking at
279:50 - azure snaps analytics today in this
279:52 - follow along so what we'll do is go to
279:54 - the type here and type in synapse
279:57 - make our way over to analytics we'll go
279:58 - ahead and add ourselves a new workspace
280:02 - and i'll create a new one here this is
280:04 - the dp900 so we'll say dp900 um synapse
280:08 - workspace
280:10 - okay
280:12 - and we need to give it a managed
280:14 - resource group uh so
280:17 - i guess same thing dp900
280:21 - synapse
280:22 - workspace
280:24 - i'm just going to keep on naming it the
280:25 - same as long as i can have it for our
280:28 - subscription we'll have to create a new
280:30 - account because we do not have
280:33 - a gen 2 storage account i wonder if i
280:34 - could just make it in my other one nope
280:36 - so we'll have to just because i already
280:37 - have a storage account right on there so
280:39 - we'll say data lake storage
280:42 - okay
280:44 - probably wants it all lower case data
280:45 - lake storage
280:47 - dp900
280:49 - okay
280:51 - and uh we don't have a file system so
280:55 - dp900 data lake file system
281:00 - okay
281:01 - and notice here it says assign myself
281:03 - storage blob data contributor role for
281:05 - the data lake storage too if you
281:07 - remember uh from our
281:09 - lecture content about very important
281:12 - roles this is one of them and it's being
281:14 - auto assigned to us
281:16 - go here to security here it has this but
281:18 - i'm going to just say azure user and
281:20 - capital t testing one two three capital
281:22 - t testing one two three just so i can
281:24 - remember between our other ones enable
281:26 - double encryption that sounds good but
281:28 - not something we're doing today
281:31 - um we'll go to network this seems okay
281:33 - to me next review and create
281:43 - and just notice up here it says azure
281:44 - synapse workspace
281:46 - is 6.40 so it's not something we want to
281:49 - uh have lying around
281:51 - um this is per terabyte right but we're
281:52 - not going to be working with a lot of
281:53 - data here so it shouldn't be a big deal
281:55 - for us
281:56 - we'll go ahead and create that
282:07 - and uh yeah i'll see you back when this
282:09 - is all done okay
282:14 - all right so after waiting a little bit
282:15 - of time there our synapse analytics
282:17 - environment is ready what i want to show
282:19 - you on the left hand side is if you
282:20 - scroll on we'll actually got to go to
282:22 - the workspace first scrolling down you
282:24 - have your analytics pool so you have sql
282:26 - pools where you can run sql operations
282:27 - so you can go there and create those
282:29 - clusters these get pretty darn expensive
282:31 - so um
282:33 - it's not even showing the estimation
282:34 - there but um
282:35 - the thing is is that you know if you
282:37 - needed sql you could do that because
282:38 - remember there's two engines that you
282:40 - can use you can use the sql engine or
282:41 - the apache spark
282:43 - engine right so you create those pools
282:45 - sql pools first you can also create them
282:48 - within um the workspace but we'll go up
282:50 - to overview the workspace is the studio
282:52 - so we're going to open that up
282:54 - and once we get in here
282:57 - we can ingest data and do all sorts of
282:59 - things so it makes it very easy but
283:01 - let's actually just go back
283:04 - you can't exit out of that once you open
283:06 - that up so i'll go and reopen this up
283:08 - because they do have some test data we
283:09 - can use
283:10 - which is uh should not cost us much but
283:12 - if we go all the way over to learn here
283:15 - and we go to sample
283:16 - sample data we have some options here if
283:18 - you choose this one it'll create a pool
283:20 - but what i'm going to do is go to query
283:22 - with data with sql
283:24 - and it will give us some sample data so
283:26 - we click on this
283:28 - and what you can see here and this is
283:30 - just me trying to run it before
283:32 - but
283:33 - what you'll see here is
283:36 - we could use
283:37 - the synapse sql right
283:39 - and we're actually able to import files
283:41 - so here it's importing a parquet file
283:44 - from a um
283:45 - azure opens like it's a storage account
283:48 - by azure and it's just an open data set
283:50 - so here you can just kind of see that
283:52 - there's ways of querying it and what we
283:54 - would do is choose where we want to do
283:56 - it so we go built in we can publish that
283:58 - before we uh do that there and we'll go
284:00 - ahead and hit run and so you know it's
284:02 - going to pull that data
284:04 - it's going to take a little bit time to
284:05 - get it loaded in and before it runs it
284:07 - but it doesn't usually take too long and
284:09 - there is the data so
284:12 - yeah that gives you kind of an idea
284:14 - how azure synapse uh works
284:17 - um but yeah so if it was an sql here you
284:19 - just write your apache spark
284:22 - usual stuff there uh yeah and hopefully
284:24 - that gives you a good idea how that
284:25 - works i don't like to keep the service
284:27 - running it is can get very expensive
284:29 - very quickly so me being paranoid i'm
284:31 - going to make sure i terminate this one
284:33 - definitely early
284:35 - even right now
284:37 - um but yeah that is azure synapse
284:40 - analytics okay
284:42 - [Music]
284:46 - all right so we are at the end of our
284:48 - follow alongs and you could tell that
284:49 - they all kind of tied in with one
284:51 - another
284:52 - so what we'll do is we do some cleanup
284:54 - so from the left-hand side go to
284:56 - resource groups
284:57 - and and it's dp900 you can go ahead and
284:59 - delete so what i'm going to do is just
285:01 - click on each one here
285:02 - it could be from other ones but i'll go
285:04 - ahead and just type in like cosmodb
285:07 - again it's based on what you name them
285:09 - but this is the fastest way i find to
285:10 - clean up all my stuff
285:12 - and i'll go here and just keep on going
285:14 - down the list
285:17 - you know because mostly the things we
285:19 - used they're not going to have ongoing
285:20 - cost the one that will will definitely
285:23 - be like
285:24 - the sql server so that's something that
285:26 - we definitely need to terminate
285:29 - okay some of these might give us some
285:31 - trouble
285:32 - so i'm again i'm just running down the
285:33 - list here deleting as much as i
285:36 - can and then i'm going to see if there's
285:39 - any errors here but
285:40 - i think we already deleted this one but
285:42 - we'll give it another go here
285:50 - and yeah that should be all of them here
285:52 - if you get a failure you might have to
285:54 - investigate it so here it says failed to
285:56 - resource the client
285:58 - uh does not have permission yeah i do
286:01 - however that's tonight against the deny
286:03 - assignment so the idea is just you know
286:05 - wait for this to finish
286:07 - uh it could take a little bit of time
286:09 - and sometimes like it's deleted but it
286:11 - just takes time for this to vanish
286:14 - and another thing you can do is go to
286:16 - your resources and just make sure the
286:17 - resources aren't running anymore so i'm
286:19 - looking here
286:20 - and what's going to cost me money is
286:22 - this sql database that's running so
286:25 - you know i want to make sure that stuff
286:26 - is deleted
286:28 - so say yes
286:30 - you could also do it from here as well
286:32 - you should let the resource groups
286:33 - delete it because sometimes it gets
286:35 - confused
286:36 - but uh you know i just want it all gone
286:39 - i've got an ip here
286:41 - data factory storage account all these
286:44 - things i want them gone
286:47 - just say yes
286:50 - uh you know it doesn't show up here is
286:51 - disks i know i have a bunch of disks
286:54 - let's go over here no there's nothing
286:56 - okay good i thought i might have had
286:57 - like a bunch of disks you know why
286:58 - because i was looking at my explorer and
287:00 - probably just showing the old ones so
287:02 - yeah it takes a while for this to to
287:03 - happen but um you know i'll come back
287:05 - here and show you that i've deleted them
287:07 - all and that's what you should do too
287:08 - okay so i'll see you back here in one
287:10 - more wrap-up video and that's it all
287:12 - right so after waiting uh quite a little
287:14 - while here i just wanted to see if it
287:16 - was all cleaned up so i'm going to give
287:17 - it a nice refresh here looks like my
287:18 - resource groups are cleaned up if i make
287:21 - my way over to my resources here it
287:23 - doesn't look like there's much uh here
287:25 - remaining so
287:27 - and just check our notifications make
287:29 - sure everything is deleted so that's how
287:30 - we make sure that everything is cleaned
287:32 - up in our azure account
287:33 - and that's it for our follow alongs okay

Cleaned transcript:

hey this is andrew brown your cloud instructor at exam pro and i'm bringing you another complete study course and this time it's the azure data fundamentals certification made available to you on free code camp so this course is designed to help you pass exam and achieve microsoft issued certification and the way we're going to do that is by going through lots of lecture content getting some handson experience with followalongs and on the daily exam i have these really great cheat sheets that are going to help you uh pass for sure so the great thing is at the end of it you'll be able to get that certification and show that on your resume or linkedin that you have the azure knowledge so you can go get that data job or that promotion you've been looking for just to introduce myself i'm previously the cto of multiple edtech companies 15 years experience with five years specializing in the cloud i'm an aws community hero and i've published many many free cloud certification courses and i love star trek and coconut water i just want to take a moment here to tell you that this video cloud certification course is made possible by viewers like you and i appreciate your support and thank you if you want to help support more free cloud courses just like this one the best ways to buy your extra study materials at exam pro dot co forward slash dp hyphen 900 to get study notes flash cards quizlets deliverable cheat sheets practice exams you can ask questions and get some support from our cloud engineers and just so you know if you want to keep up to date with upcoming courses you can follow me on twitter at andrew brown that's uh and what you can do is once you do that you can tell me you passed the exam you can tell me what you'd like to see next because a lot of times the next course i make is based on the feedback i get from you so let's jump into the course now hey this is andrew brown from exam pro and we are at the start of our journey asking the most important question first which is what is the dp900 so the azure data fundamental certification is for those seeking a data related role such as data analysts data engineer or data scientist and the certification will demonstrate a person can define and understand coordinated concepts hadoop workloads apache spark workloads mssql databases nosql databases data lakes data warehouses elts and big data analytics and more the certification is generally referred to by its course code the dp900 and it's the natural path for the azure data engineer or the azure data analyst certifications this is an easy course to pass and great for those new to cloud or data related technology so let's look at our big old road map here and what i'm going to do is pull out my laser pointer so you can see where i am and we'll get some lines in here because i want to show you some of the paths you can take with the certification so we're over here with the dp900 and this is at the fundamentals level right and so a lot of times what people will do is they will have the az900 oh they'll take that first if they're 100 new to cloud so i see a lot of people they start here and if you're if you've never used azure before it's a great starting point and then they'll move on to the dp900 or sometimes what they'll do is they'll move on to the ai 900 and then onto the db dp900 or they'll take the dp900 and then the ai fundamentals but the reason you want to take this data fundamentals certification is primarily because you you're very likely want to take the data engineer certification or the data analyst certification which is at the associate track so this one it's basically uh the dp900 but actually knowing how to implement everything and then the data analyst is really focused on power bi so just knowing how to use uh power bi uh to uh to its maximum extent okay now a lot of people that are going for the data scientist or ai engineer track will take the dp900 because you do need foundational knowledge about data to understand these roles and so you know it's just if you are going for the data scientist you probably want to add the dp900 to your track but if you've taken the az900 and the az104 you might want to skip over this it's just up to you but yeah there's a lot of ways that you can go about this okay and we'll just move on uh here so how long to study to pass the dp900 so if you have one year experience with azure you're looking at five hours of study if you've passed the az 900 you're looking at 10 hours if you're new to cloud you're looking at 15 hours of study for new people i'm saying 30 minutes a day for 14 days the course content itself is not very long but the thing is you have to factor in that you have to do um you have to do practice exams and you also uh just have to put that knowledge into practice by using the console so even though you know the course content's not long there's additional time you have to do there where would you take this exam well you can do it in inperson test center or online from the convenience of your own home so there are two options you got psi and pearson vue and uh they both actually do in person and online and so just so you know if you ever heard the term proctor a lot of times we talk about online exams being proctored exams because there's a person or supervisor that's watching you take the exam online okay so what does it take to pass this exam well you got to watch the lecture videos you got to do handsons and follow alongs and it helps to do paid practice exams that simulate the real exam this is pretty easy certification so you could probably get away with um okay so what i did was i just googled dp900 exam guide and so i made it to the microsoft site if you scroll on down you're looking for that skills measure download that pdf open it up it will be over here now it's very common to see this red text here azure loves to update their exams even once a month they'll do this to me and people ask me are your exams out to date i'm like no they're just making my new changes if they do make major changes what they'll do is actually release a new version they'll call it the dp901 that's when you should be concerned about changes uh but uh yeah they do it frequently so what you'll do is if you get if you see red text you gotta scroll down to the real section because this is the old one and this is the new one where they make minor changes okay and what we'll do is work our way down here and and take a look here so describe core data concepts so batch and streaming relational data we have data linux concepts so visualizations bi types of bar like types of charts they're talking specifically about power bi because there's a lot of different kinds of visualizations describe analytic techniques so descriptive diagnostic predictive prescription cognitive most places don't describe cognitive so they add that additional one there elt and etl so um uh extract elt is more common for cloud so that's the one you really want to understand describe concepts of data processing onto how to work with relational data so describe what a relational workload is describe the structures within a database if you've ever worked with any kind of database you already know them tables indexes views columns et cetera we have described relational data services so they do pasias and sas they're specifically talking about in that comparison you'll see that like in the az 900 uh those three though i think they might have removed them as of recent but um they're specifically talking about the azure sql family so down below here you have azure sql and underneath it has a bunch of variants so like sql database sql manage instance and virtual machines and these sit in the pas and iis and you have to know that okay describe azure synapse analytics describe database for postgres mario by sql that's the open source sql databases uh so then we have relational data so uh provisioning deployment of relational data deployment with the portal azure resource templates powershell cli you know what i don't ever see these on the exam you know but um you know they they have this in here okay identify security components now they say firewall they're actually talking about when you have a database because there's actually azure firewall but then there's um a a server firewall built into azure sql and that's what they're talking about there authentication like how to connect because there's a few different ways you can do that connectivity from onprem to azure vnets etc identity query tools so azure data studio sql studio management sql sql cmd utilities and things like that describe query techniques for using uh sql so compare ddls with dmls there's actually a lot more types of um data language files for sql so we do them all just because it's you know it's the proper way to do it but they only care about these two query relational data in sql database uh azure disk postgres azure database for mysql we'll go on down to this section here nonrelational data workloads so describe the nonrelational data describe nonrelational and nosql data recommend correct data stored determine when to use nonrelational data and they're all talking about um because like nonrelational data is mostly cosmodb a cosmodb like has a bunch of different sql engines there so that's going to help you understand that like graph and document key value store things like that describe nonrelational data so we have table blob files cosmodb identify basic management tasks for nonrelational data so provisioning deployment of nonrelational data services describe method of deployment in uh azure portal et cetera et cetera again i don't see these a lot on the exam so i don't know why this is in here identify data security components um so it's the same thing as before it's just for nonrelational data basic connectivity issues vnets etc identify management tools for nonrelational data um describe analytical workloads so transactional data the difference between transactional analytical and we're talking about olap and oltp okay difference between batch and real time uh warehouses data warehouse solutions describe modern data warehouses so here we're talking about data bricks hadoop systems synapses is kind of like a data lake house and then actually azure data lake and it's storage medium down below we have data ingestion so loading data azure data factory um hd insights databricks etc and then down with below we have a whole section on power bi what i'm surprised is they don't have much like on azure um streaming analytics and event hub because a lot of these have to consume from there so that's something i think that should be on the example they don't have it in here but yeah it's uh it's not a super hard exam it's mostly just describe describe identify see so you're all going to be in great shape and i hope that helps you out and we're on to the actual course now hey this is andrew brown from exam pro and we are taking a quick look at all the core data related azure services that we are likely to encounter through this course so let's get to it the first starting with azure storage accounts and this is an umbrella service for various storage types such as tables files and blobs we have azure blob storage and this is a data store that stores things as objects instead of files and the advantage here is that you get distributed storage these objects can span multiple machines for unstructured data you have azure tables which is a key value no school data store more like a database but it's under azure storage accounts and it's intended for its simpler projects you have azure files and this is a managed file share for nfs or smb so if you need a file share or file system that you need to mount to multiple virtual machines or workstation this is what you would use you have azure storage explorer this is a standalone application you download to your windows linux or mac machine that easily allows you to explore the various services above then you have azure synapse analytics this is a data warehouse and unified analytics platform the service used to be called something like azure warehouse but they added analytics on top of it kind of making it into a lake house service and so that's what it is now we have cosmodb this is a fully managed nosql database service that can host various nosql engines such as azure tables documents key value and graph when you use cosmodb it's going to have a core mode and that pretty much is its documents engine so a lot of times when we talk about cosmodb we just think of it as a documents documents database but it can actually have a variety underneath you have azure data lake store generation two we won't talk about gen 1 because it's just not really in use anymore but this is a centralized data repository for big data blob storage designed for vast amounts of data it actually is just azure blob storage with an additional layer of management you have azure data analytics this is a big data as a service you can write usql to return data from your azure data lake then you have azure data box which isn't really covering the exam but i'm including here because i think it's a great addition so you can import and export terabytes of data via hard drive you mail into the azure data center onto our next page here we have sql server for azure virtual machines this is when you need an sql server where you're migrating an existing sql from your onpremise data center onto azure but you can't afford to make any changes so you're literally taking the vm and lifting and then shifting it onto azure but you get to have access to the virtual machine underneath so you can control the os access layer and also if you already have an existing license it's a great solution for that as well if you are doing a lift and shift but you don't need to manage the virtual machine and you want azure to do all the work for you you have sql managed instances then you have azure sql which is the fully managed mssql database then you have azure databases for mariodb postgres and mysql you have azure cache for redis now this is an inmemory data store for returning data extremely fast but is also extremely volatile and this isn't covered on the exam but i like to include it because i think it's just one of the data services that's important you have microsoft office 365 sharepoint not really covered on the exam but you will hear it mentioned throughout the course content and you know i think that if you haven't had exposure to you should know what it is it is a shared file system for organizations the company owns all the files and applies finegrained rolebased access controls you have azure data bricks this is a thirdparty provider partnered with azure specializing in apache spark to provide very fast etl jobs as well as ml and streaming you have microsoft power bi this is a business intelligence tool used to create dashboards and interactive reports to empower business decisions we have hdinsights this is a fully managed hadoop system that can run many open source big data engines for doing data transformations for streaming etl elt we have azure data studio this is an ide that looks very much like visual studio code but designed around data related tasks across platforms similar to sss ssis but broader data workloads you have azure data factory a managed etl elt pipeline builder easily build transformation pipelines via a web interface within azure and then you have sql server integration services ssis it's a standalone windows app to prepare data for sql workloads via transformation pipelines there's probably a bunch of other little services or tools that we don't have in this list but don't worry we'll cover them throughout the course just remember these ones that we went over here today hey this is andrew brown from exam pro and we're taking a look at the types of cloud computing for azure data related services starting at the top of our pyramid is software as a service and it's a product that is run and managed by a service provider so you do not worry about how the service is maintained it just works and remains available and this is specifically designed for customers so uh it's not particularly azure services but it'll be like microsoft based services like power bi or the office 365 suite is going to be software as a service going down to platform as a service this focuses on deployment and management of your apps so you do not worry about provisioning and configuring or understanding the hardware or os layer and this is specifically for developers so we would put hdinsights azure sql cosmodb managed sql all right and at the bottom we have infrastructure as a service these are the basic building blocks for cloud it it provides access to networking features computers and data storage space you do not worry about the it staff data centers and hardware and underneath here we would have this would be for admins but we'd have azure disks virtual machines sql vms and honestly you know like when you look at aws and azure these kind of categories are defined a little bit differently so you know like manage sql i would probably put that infrastructure as a service but azure says that it goes into the mid tier i really want to pull up a particular document here that i think is important because this is all about the azure sql family and they're specifically categorizing these in particular so you go down below here and when we're looking i have them on here but when you look here i have sql vm down below so that would be considered infrastructure as a service you have managed sql where they put in the middle but they categorize it as platform as a service and then you have it's covered up here we have azure sql database which is platform as a service i'm showing you this because they might ask you this question on the exam uh and so i just wanted to point that out to you there but there you go okay so let's take a quick look at three azure data roles that azure wants you to know about specifically related to data services when i say roles here i don't mean like azure permissions i actually mean like jobs that people would do within azure and let's just take a look at it here so the first one is database administrator this is somebody that would configure and maintain a database such as azure data services or sql servers and they would be responsible for database management management security or granting users access backups monitoring performance and common tools that they would use would be azure data studio sql management studio azure portal the cli the next role would be data engineer and that would be to design and implement data tasks related to the transfer and storage of big data responsibilities here would be database pipelines in process data ingestion storage prepare data for analytics prepare data for analytics processing and common tools that they would use would be azure synapses studio sql azure cli and the last role here we have is data analyst so this is analyzes business data to reveal important information so you have provide insights into data visual reporting modeling data for analysis combines data for visualization analysis common tools here are power bi desktop power bi portal power bi service and power bi report builder so i just want you to know that there's definitely a lot more roles than just these three here but this kind of helps you narrow down what this entire dp900 is focused on which are these three kind of rules here but what we'll do is we'll jump into these common tools and just talk about them a little bit more in detail uh next here okay okay so we're taking a look here at database administrator common tools the first being azure data studios this allows you to connect to azure sql data warehouses post postgres sql sql servers big data cluster on premise i say azure sql data warehouse it must be azure synopsis analytics i probably just wrote that incorrectly there but various libraries and extensions along with automation tools a graphical interface for managing onpremise and cloudbased data services runs on windows max and linux possible replacement for ssms but still lacks some of those features if you launch the service it looks a lot like visual studio code because it probably is but it's specifically for data related tasks so if you're used to visual studio code you're going to be at home with the service you have sql server management studio ssms and it's an automation tooling for running sql commands or common database operations it has a graphical interface for managing onpremise and cloudbased data services but it only runs on windows so if you're on mac or linux you're going to be using azure data studio and if you're on windows you might just have both of these installed because there's just some things you can do in ssms that are just a lot easier than azure data studio but it's more mature than azure data studio so you know it's just going to be the features are going to be a lot more richer in partic in particular for sql you have azure portal and cli so here you can manage sql database configuration so you can create delete resize the number of cores uh you can manage and provision other data azure data services automate the creating updating or modifying resources via the azure resource manager templates which is infrastructure as code so those are the three major ones that a database administrator is going to be working with now let's take a look at data engineering common tools so at the top we have azure synapses studio so you know azure synapses analytics when you click into it there you launch a studio and this allows you to manage things like your data factories your warehouses sql pools spark pools things like that you're gonna have to know really really no sql there's tsql usql synapsis sql there's all sorts of sqls within azure so it's definitely something you want to learn for the azure cli you'll have to be able to use it to then execute sql commands because once you connect to an sql server via the cli you're just going to be writing sql from there i added these they weren't in the common tools list prior but i just added them now because i thought they were useful so hd insights which would have for streaming data via apache kafka or apache spark or applying etl jobs via hive pig and apache spark as your data bricks also because here you could be creating an apache spark cluster and using that to do etls or streaming jobs to your data warehouses your data lakes and of course you'd be working with blob storage and data likes as well so they should be on the list here but just there you go all right taking a look here at data analysts for common tools we have power bi desktop this is a standalone application for data visualization you can do data modeling here connect to many data sources and create interactive reports then you have power bi portal or also known as the power bi service and really this is just intended for creating interactive dashboards you can definitely do other things here but this is what i know it for then you have power bi builder report or report builder this is another standalone application and this allows you to create paginated reports which are just printable reports definitely there are more tools than just these three for uh data analysts but this is what azure wants you to know uh so there you go hey this is andrew brown from exam pro and we are looking at the data overview so the idea here is that we're going to be covering a lot of fundamental not necessarily as your specific data related knowledge that you need to know to really understand how to use azure data related services uh so i'll just give you a quick overview here and then we'll dive deeper into all these things so the first thing is data so that's units of information you have data documents these are types of abstract groupings of data you have data sets these are unstructured logical grouping of data when you structure your data now it's called structured data and then you have data types these are single units of data that are intended to be used in a particular way then you have a bunch of loose concepts so you have batch and streaming so this is how do we move our data around we have relational nonrelational databases or data so how do we access query and search our data you have data modeling how do we prepare and design our data schema versus schemas how do we structure our data for search data integrity and data corruption how do we trust our data normalization and denormalization how do we trade quality versus speed and i'm sure we cover a lot more than just this list i just didn't feel like being exhausted here and put every little thing here so many things that have the word data in it but let's jump into it so the first question we should be asking ourselves is what is data so data is units of information that could be in the form of numbers text machine code images videos audio or even in a physical form like handwriting maybe if you're in the future it's crystals i don't know and so just some images here or examples of graphics so here we have an image here is a bunch of binary code or machine code we have a book here we have uh like audio so we have like audio spectrum here uh and then you have mathematical formulas so i'm sure at this point you know what uh data is but just in case uh you know you need to just broaden your your thoughts of what data is it really could be everything including physical stuff so what is a data document a data document defines the collective form in which data exists so common types of data documents would be data sets which is a logical grouping of data databases which is structured data that can be quickly accessed and searched data stores this is unstructured or semistructured data for housing data data warehouses structured or semistructured data for creating reports and analytics notebooks data that is arranged in pages and designed for easy consumption so just to give you some examples if we're looking at data sets we might talk about the mnist data set for azure sql that would be a database for a data store such as a data lake we have azure data lake for a data warehouse we have azure synapsis analytics and for notebooks we might talk about jupyter notebooks but this could also include you know an actual handwritten notebook so there you go all right so what is a data set well a data set is a logical grouping of units of data that are generally closely related or share the same data structure and so i say data structure there but i just want you to know that just because something has a data structure doesn't mean it's structured data it can be semistructured like json objects or xml files but generally data sets are unstructured or structured they are publicly there are publicly available data sets that are used for learning statistics data analytics and machine learning the most popular one being the msns database i can't tell you how many times i've seen this data set but it's images of handwritten digits used to test classification clustering and image processing algorithms commonly used when learning how to build computer vision ml models to translate handwritten text into digital text and here's an example of that data set there another very popular data set is the commons objects in context the coco data set i believe microsoft had a hand in this one and it's a data set which contains many common images in a json file a coco format that identify objects or segments within an image so there again it has json files that means that it's semistructured not necessarily structured data here is an example of the data set you can see that there are images and they have borders drawn around things that are trying to identify in the images such as objects and segmentation recognizing context super pixel stuff segmentation no idea what that means uh 329 000 images and it has a lot of labels and a bunch of other things in this data set another interesting one would be the imdb review data set where it has 25 000 highly polar movie reviews meaning people really like the movie or they really dislike them i pulled out an example here of pluto nash which at the time was highly uh liked and disliked there was a huge split between this movie mostly i think people disliked it but apparently it's kind of split this is great for customer seg sentiment analysis again you'll probably be using ml for this but the idea is to say did people like the movies like or did they like it hate it uh we're sad about it that's kind of like customer sentiment right how did they feel some other data sets we have the free music archive this is a data set of musical uh music tracks so you have a hundred thousand tracks across 163 genres you have the uh li i guess this is library but lib re speech a data set of a thousand hours of english speech you think they use an english word to describe that but no there are many more data sets online some are paid some you have to extract via an apis uh some you have to scrape the data yourself um but just taking a big look here of the full list we got crunchbase glassdoor fbi google trends data hub world health organization it's across the board right so you know there's a lot of things that you can get data online a lot of times you are just creating your own data sets but it's good to know that there's a lot of stuff out there so what is a data type it's a single unit of data that tells the compiler or interpreter so a computer program how data is intended to be used and the variety of data types will greatly vary based on the computer program a better way of thinking of it instead of saying computer program just say programming language because that's where you're going to be encountering data types so let's take a look at the most common data types the first being numeric data types these are data types involving mathematical numbers the most common being integer which is a whole number could be negative or positive a lot of programming languages will have a variety of these that have different ranges like int 32 into 64 what have you then you have floats or sometimes also known as decimals this is a number that has a decimal so 1.5 0.0 can be a negative number as well here it is example of an end and a float in python we'll take a look at text data types this is a data type that contains a readable or nonreadable letters so there are characters so characters is a single letter so it could be a to z it could be a digit it could be a blank space punctuation special characters then you have a string and a string is a sequence of characters that can be word sentences or paragraphs they don't necessarily have to be words but you know that's usually what you're using them for so here's an example of a character and a string then you have composite data types and these contain cells of data that can be accessed via an index or a key so for example we have an array so that is a group of elements that contains the same data type that can be accessed via the index and position you have a hash commonly known as the dictionary and if you're using python you're probably known as the dictionary i know it's a hash because i like ruby so that's how i know it and it's a group of elements where a key can be used to retrieve a a value and the thing is is that composites are they overlap with data structures so when we talk about data structures you might say hey is an array in hash a data structure and yes it is but yes it's also a data type it just depends on the programming language and the constraints around those okay so i know those get a little bit confusing i just wanted to point that out but here's an example of again this is um python so we have an array and down below we have a python dictionary and if you use json or javascript yes uh you know a json object basically is a hash we've got one more here so we have binary data types this these are represented by a series of bits uh or bytes which either are zero or one so off and on so here's an example in python how to set up a byte um for billion values if we have true or false some languages represent billions as a zero or one so a lot of times when you're using like mysql true and false i think is zero and one in there sometimes it's a t or an f when you're using um a post sql it's going to be a t or an f uh and sometimes it's just true or false so in python it's just capital t true and actually i think they have a lowercase tree which is a different kind of object it's a bit confusing but i'm just saying there's some variants there then you have enumeration data types or sometimes known as enums and these are a group of constants unchangeable variables so for example diamond spade hearts and clubs they're all related because it's card groups so the idea here is the data type uh you know it could be also a data structure again it varies on the language just like composite types but here on the right hand side we have a shake and it's wrapped in a class and below this again this is python that's how you do in python and so the shape could be vanilla chocolate cookies and mint a lot of times the nums will map to an integer value or something so but not always the case um but yeah that's uh that type there and there you go that's the common data types okay so let's take a look at schema versus schema list so what is a schema a schema in terms of a database is a formal language which describes the structure of data a blueprint of a database and a schema can be can define many different data structures that serve different purposes of a database so different data structures in a relational database could be things like tables fields relationships views indexes packages procedures functions mxl schemas cues triggers types sequences materialized views cinnamons cinnamons synonyms can't say that word database links and directories gonna highly vary based on the database that you're using and i'm just going to show you an example of a schema so here is actually part of my schema for the xampro app and so this is a ruby on rail schema that defines the structure for a relational database and it's written in a dsl called ruby but the thing is is that this is going to highly vary uh based on again what you're using but just notice that you can see things like creating a table creating indexes creating columns for the database things like that adding extensions uh schemaless uh is just kind of it's still schema but the idea here is the primary cell of a database can accept many types so just going back here for a moment notice here that we have very particular notice here that we have very particular data types like integer string and stuff like that the idea here is with schema list that that data type is a lot more flexible and the idea there is it allows you to forego the upfront data modeling that you normally would have to do which is a lot of work and so that's one of the advantages of schema lists common stimulus databases would be key value document columns and then the subcategory of wide columns and graph and not a lot of information here but we will describe it in more detail when we talk about nosql databases okay all right let's talk about query and querying because those are terms that you're going to need to know because you're going to be doing quite a bit of it if you are going to have a career as a anything in data right like a data analyst so what is a query a query is a request for data results also known as reads uh or to perform operations such as inserting updating or deleting also known as writes and so a query can perform maintenance operations on the data and it's not always restricted to just working with the data that resides within the database and you'll see this where there's like there's commands to do analysis on your database and other things like that but here's an example of a query so what is a data result well results are data results is the results of the data returned from a query so here you generally will see tabular data that's usually what people want back but you know you can get back json or xml it really just depends on the database what is querying so this is the act of performing a query so the idea is that you write your query above and it's being sent as an as a request could be through an sdk cli the shell an api lots of ways for it to get there and then the idea is that the query is going to return the the data results so what is a query language that is a scripting language or programming language designed as the format to submit a request or actions to the database noble query languages is sql graph sql cousteau xpath gremlin and there's a lot of them but you know those are ones that stand out to me right now and just to note up here this is sql okay so i just didn't want to make a big old line this way here but this is sql here okay all right so let's compare batch and stream processing so batch processing is when you send batches a collection of data to be processed and batches are generally scheduled so you might say every day at 1 pm but you can also just queue up a batch whenever you feel like it batches are not realtime meaning that all the data is sent and then you wait until the batch is back to see the results batch processing is ideal for very large processing workloads batch processing is generally more cost efficient than stream processing and so here we just kind of have a representation so here we have our data we've broken into batches or collections we pass it to something like an etl engine and it will transfer the data and then we'll insert into our database data warehouse data store data lake house wherever you want to put it then we have stream processing so this is when you process data as soon as it arrives you'll have producers which will send data to a stream and consumers which will pull data from a stream a lot of times the stream will look like a pipeline and data can be held in that stream for a period of time so you have a better reusability of data if you need it for multiple consumers stream processing is good for realtime analytics realtime processing like streaming videos anything that has to do with real time if you need it right away it for that purpose it is much more expensive than batch processing um and here's a visual representation where we have bits of our data they go into our stream pipeline that can be held there for a while and sometimes minor operations will be performed on it but consumers will pull the data and do what they want with that data if we want to contextualize these things in terms of services on azure the idea is you'd have your data sources and you'd ingest them into something like azure stream analytics or i didn't really make this graphic very good but the idea is that you go into stream analytics or maybe you go into hdinsights or maybe you go into azure synapse analytics or one of these intermediate steps and then eventually you go to power bi to make your visualization reports for stream processing you could use event hub so event hub is a uh a single topic uh streaming service and you could ingest that into azure stream analytics um it's funny because this is the stream analytics icon this is actually the hadoop icon up here it's got it mixed up but anyway you go into stream analytics and you can insert that into cosmodb and then maybe pull cosmodb into power bi but yeah that is the difference between the two okay all right let's talk about relational data and this has to do with tables and relationships between other tables so let's talk about what a table is it's a logical grouping of rows and columns so an excel spreadsheet is actually tabular data tabular data just means the data that makes use of table data structures okay then you have views which look a lot like tables except they are the result set so a table when you do a query you're returning back data and you're storing that queries data in memory and basically it's a temporary or virtual table then you have materialized views and it's the same thing as a view except the difference here instead of being stored in memory it's stored on disk but again it's the results of a table so it's a virtual table then you have indexes and this is a copy of your data sorted by one or more multiple columns for faster reads at the cost of storage so think of it kind of like a virtual table but it does include all the columns and it's it's just so it's just to help you understand what order to retrieve data you have constraints these are rules applied to rights that can ensure data integrity so like if you have a database and you want to make sure that there are no duplicate records you put a constraint for no duplicates things like that you have triggers this is a function that is is a trigger on a specific database event this is really useful let's say after you insert a a column in your database you want to have a uuid you'd have a function that would generate out the uuid then you have primary keys so one or more multiple columns that uniquely identify a table in the row the most common primary key is id a foreign key so a column which holds the value of a primary key from another key to establish a relationship very commonly it's just the name of the other table with underscore id so relationship is when two tables have a reference to one another to join the data together i know this text is really boring but we're just going to cover so much about relational database tables so i didn't feel that we needed a visual here but let's keep moving forward here with relational data so for relational data we have tables and then the relationship between the tables let's talk about those relationships so relational databases establish relationships to other tables via foreign keys referencing another's table's primary key so looking at the example here uh you know this is the primary there's a little icon here that shows you that it's the primary key so this is the primary key and over here in another table it's referencing a foreign key so that's the foreign key that's the primary key okay and there are four types of relationships between relational databases and their tables the first is one to one so imagine a monkey has a banana or here we have a table called country and it has a capital it's one to one then you have one to many so a store has many customers or you could say a book has many pages notice that this denotes the many here then you have many to many so a project that has many tasks and tasks can belong to many projects or here a book can have many authors and an author can have many books so there's many to many and then last is a variant on the many to many so and it's via a join or junction table i just call them join tables so a student has many classes through enrollments and a class has many students uh through enrollments so here it's the same thing a book can have many authors an author can have any books but it's all through a library so you could say a book has many authors through a library and an author has many books through a library okay so there you go okay so we know that relational databases store tabular data but the thing is that data can also be stored either in a row oriented way or a column oriented way and let's just talk about the differences there and why we would do that so the first case we have row store the data is organized into rows this is great for traditional relational databases which are row stores good for general purpose databases suited for online transaction processing oltp we are going to come back to that term later on great when needing all possible columns in a row which is important during a query not the best at analytics or massive amounts of data all right we're looking at column store data is organized into columns it's faster at aggregating values for analytics so ideas imagine that you want to count how many cities there are for millions of records if it's organized by column like querying based on column or data stored together as columns a lot faster generally these are no sql stores or escolike databases it's a bit confusing because uh you know like you would think tableau data is just relational databases but when you want to do column store they're basically nosql stores so the term's a bit fuzzy there it's great for vast amounts of data when we're talking about massive amounts we're talking millions and millions of records terabytes worth of data okay suited for online analytical processing oltp great when you only need a few columns so you don't need to get data from all the columns and there you go let's talk about database indexes which is a data structure that improves the speed of reads from the database table by storing the same or partial redundant data organized in a more efficient logical order and the logical order is commonly determined by one or more columns such as sort keys they're always called that a common data structure for an index is a balanced tree uh and it's short for b tree not to be confused with binary tree which is something else so you might see b tree and be like okay that's how it's doing that uh so here we just have kind of a visual imagine you have a table or a foot like that's for a phone book and you want to quickly find people based on the phone number because maybe you're trying to find them based on the starting number being 344 or something so the idea is you make an index and you say i want this to index by the phone number and so what it's going to do is change the order there so it might just pull the id or the number and reorder it and so now what you'll do is you'll use that index and that index will use as a reference to determine so it's not storing all the data but it will use that as a reference to the original table to quickly return your data and so here's a very easy way to create an index in postgres so we'd say create index and then we give it a unique name and we'd say let's make it index on the um for our addresses but just on the phone number so there you go let's take a look here at data integrity versus data corruption so data integrity is the maintenance insurance of data accuracy and consistency over its entire life cycle and it's often used as a proxy term for data quality now you might think it's data validation but it's just a prerequisite of data integrity because again data integrity is all about the entire life cycle making sure over all that it's going to stay consistent so validation is just one part of it the goal of data integrity is to ensure data is recorded exactly as intended data integrity is the opposite of data corruption so data corruption is the act or state of data not being in the intended state result in data loss or misinformation and data corruption occurs when unintended changes result when reading writing and so in the case when you're doing reads and writes maybe you have a hardware failure somebody just inputs the wrong data or someone intentionally is being malicious to corrupt your data or there's unforeseen side effects for operator operations via computer code so you wrote code and you didn't know that it was doing something that it wasn't supposed to be doing so how do we ensure data integrity well we have a welldefined and documented data modeling so data modeling if you know exactly how your data is supposed to be and it doesn't match the model there then you'll know logical constraints on your database items so we talked about that when we talked about all the types of relational data so constraints will keep that data integrity in place redundant and versions of your data to compare and restore so you have to be able to um not just validate your data but be able to bring it back to the state that it's supposed to be human analysis of the data so you know that's where data analysis will just check periodically uh hash functions to determine if changes have been tampered with you see this quite often when you're downloading uh open source software or software off like soft soft pedia where you can have an md5 hash to say did the thing i download match the thing that was expected uh principle of least privileges so limiting access to specific actions for specific user roles will mitigate uh problems that are unexpected with your data so all that stuff uh makes up data integrity okay okay it's time to compare normalized versus denormalized data so normalize is a schema designed to store nonredundant and consistent data whereas denormalize is a schema that combines data so that accessing data or querying it is very very fast so when we see tables and relationships like a relational table where everything is very discreetly organized this is normalized data and then on the right hand side where you could take all those tables on the righthand side and make them one table this would be extremely efficient so the lefthand side for normalized data integrity is maintained little to no redundant data many tables optimize for storage of data on the right hand side we have data tegrity is not necessarily maintained or there's not good controls in place you have to do extra work to make sure it is in good shape redundant data is common fewer tables excessive data storage is less optimal now when you're using relational databases you can use both normalized and denormalized schemas and when you are using nosql it's a little bit harder but like there's cases where you can kind of model things like tables but generally data is denormalized in nosql so there's a bit more challenge with data integrity but the the upside is you get a lot more performance right so it's just way way faster at scale a pivot table is a table of statistics that summarizes the data of more extensive tables from a database spreadsheet or business intelligence tool and pivot tables are a technique in data processing they arrange or rearrange so pivot statistics in order to draw attention to useful information and this leads to finding figures and facts quickly making them integral to data analysis so when you're looking at microsoft excel it's very easy to create pivot tables think of a pivot table as an interactive report where you can quickly aggregate or group your data based on various factors so maybe you're grouping it by year month week or day some average min or max and so over here i have an example of a pivot table excel i got this from excel jet which they actually have really good information about pivot tables an example so if you think that you want to learn more about this i would go check out that resource but here you what you can see is that we have a table and notice that it has these little filters at the top right and so the idea is you can drop that down and say sort by date and and other stuff and what that you can do is create here is another pivot table here where we said okay let's sum the sales based on blue and green so this is a pivot table and then created another pivot table okay uh and so um you know it becomes very useful tool in excel all right and just one more thing pivot tables used to be a trademarked word owned by microsoft so a lot of times pivot tables were specifically just in excel or their uh was it their microsoft access database but now pivot tables is a unique term or a general term that everybody uses just for this kind of operations let's talk about data consistency and this is when data is being kept in two different places and whether the data exactly matches or does not match so when you have to duplicate data in many places and you need to keep them up to date to be exactly matching based on how the data is transmitted and service level the service levels of your cloud service provider they'll use these two terms and we'll hear strongly consistent and eventually consistent so strongly consistent means every time you request data so you query data you can expect consistent data be returned within x time so they might say within 10 milliseconds 100 milliseconds one second so the thing is we will never return to you old data but you will have to wait at least x amount of seconds for the query to return whatever that defined time is we talk about eventual consistency when you're when you request when you request data you may get back inconsistent data within x amount of periods so two seconds we are giving you whatever data is currently in the database you may get new data or old data but if you wait a little bit longer it will generally be up to date why would we have these two methods it just depends on your use case maybe you can tolerate some data to be inconsistent it's more important to get whatever data is available now and sometimes you need an absolute guarantee that the data is one to one okay so those are the two different ones so synchronous and asynchronous can refer to mechanisms of data transformation uh and data replications let's break these two down so synchronous is continuous streams i'm just going to mark that there continuous stream of data that is synchronized by a timer clock so you get a guarantee of time of when the data will be synced you can only access data once the transfer is complete you get guaranteed consistency of data returned it at the time of access slower access times so here is the data and if you're thinking about strongly consistent that is what this is it's this is going to be when things are strongly consistent then we have asynchronous so continuous stream of data separated by a start and stop bits no guarantee of time can access data anytime but may return older versions or empty placeholder faster access times no guarantee of consistency here it is so you see it's moving in bits right and so the idea is that we can access any time in between here to get maybe up to date data or not up to date data to solidify this let's put in some scenarios so a company has a primary database but they need to have a backup database in case their primary database fails the company cannot lose any data so everything must be in sync the database is not going to be accessed while it is standing by to act as a replacement so the reason why this works is that you know if you have a backup database you have to make sure all your data is onetoone then on the other side here a company has a primary database but they want a read replica a copy of the database so their data analytics person can create computational intensive reports that do not impact the primary database it does not matter if the data is exactly onetoone at the time of access because in this scenario it's like any time the database goes down you want it up to date to the second on this side it's like they might run reports once a day whatever so you know there's always new data coming in and b2 uh to burdensome to make sure that the data is always up to the second so there you go hey this is andrew brown from exam pro and we are looking at nonrelational data and this is where we store data in a nontabular form and will be optimized for different kinds of data structures so what kind of data structures well we're looking at key value stores so each value has a key design to scale only simple lookups then you have a document store so primary entity is jsonlike data structure called a document you have column restore sometimes this falls under relational databases but it is a nonrelational data type or database so it has a table like structure but data is stored around columns instead of rows then you have the graph database where data is represented with nodes and structures where relationships really really do matter and so sometimes nonrelational databases can be both a key value and document store like azure cosmo db or amazon dynamodb and the reason for that is that documents are actually a subset of key values which we'll talk about later when we get to that point hey it's andrew brown from exam pro and we're taking a look at data sources so a data source is where the data originates from so an analytics tool may be connected to various data sources to create a visualization report and a data source could be a data lake a data warehouse a data store a database a data requested on demand via an api endpoint from a web app and flat files such as excel or spreadsheet and so the example here is that we have a data source and somehow there has to be a connector between them and it's going to be consumed by either a warehouse an etl engine a data lake or bi tool those are common ones that need data sources so extracting data from data sources so a data tool like a business intelligence software would establish a connection to multiple data sources at the bi would extract data which could uh could be pulled data at the time of report or it could be pulled data on schedule or data could be streamed the mechanism for extracting data will vary per data source i just want you to know that because you know when you're using these services it does really vary on how it pulls the data so i just want you to understand there are a few different ways okay so a data store is a repository for persistently storing and managing collections of unstructured or semistructured data so here i have kind of a visual where we have files going into some kind of store and a data store is a very very broad term so it's interchangeably used with databases though databases is technically a subset of a data store but generally a data store indicates working with unstructured or even semistructured data so if somebody said a data store i'm thinking that it's either unstructured semistructured okay a data store can be specialized in storing flat files emails maybe a database as we said it was a subset uh or designed to be distributed across many many machines or it could be a directory service okay so that's a data store so what is a database a database is a data store that stores semistructured and structured data and but a better term i would say would be a databases more complex data store because it requires using formal design and modeling techniques databases can be generally categorized as either relational databases so this is structured data that strongly represents tabular data so tables roles and columns they're generally either row oriented or columnar oriented and when we talk about nonrelational databases we're looking at semistructured data that may or may not distantly resemble tabular data and i know that i put this one over on the relational side sometimes it ends up here or there just understand that that one kind of floats in between the two really depends on the technology underneath and so here is a pretty common way you get your sql you hit the database you get a table back right so the databases have a rich set of functionality specialized uh language to query so that is our sql here right we have specialized modeling strategies to optimize retrieval for different use cases more finetuned control over the transformations of the data into useful data structures or reports the thing here on the end normally a database infers someone is using a relational roworiented data store so when somebody that's when someone says a database you're usually thinking like mysql sql postgres redb things like that okay so what is a data warehouse it's a relational data store designed for analytic workloads which is generally column oriented data store and again i'm going to make an emphasis here sometimes it's nonrelational sometimes it's relational don't get too worried about that part okay companies will have terabytes and millions of rows of data and they need a fast way to be able to produce analytic reports that's how you know you need a data warehouse okay data warehouses are generally generally perform aggregations so aggregations is grouping of data to find a total or average data warehouses are optimized around columns since they need to quickly aggregate column data and so here is an example where we have our warehouse and the idea is that we're taking data in from like an unstructured source through an etl and then here we have an sql these are two different data sources we use sql to then get our results okay so data warehouses are generally designed to be hot hot meaning that the data will be returned very very fast even though they have vast amounts of data data warehouses are infrequently accessed meaning they aren't intended for realtime reporting but maybe once or twice a day or once a week to generate businesses and user reports now can it report extremely fast of course but it's not like at a at a per millisecond you're running it all day and keeping it all up to date that's more for a stream right a data warehouse needs to be needs to consume data from a relational database on a regular basis or you know through an etl data gets transformed input in there okay generally generally data warehouses are read only so you insert data and then you read it you're not using it for transactional data okay what is a data mart a data mart is a subset of a data warehouse a data mart will store generally under 100 gigabytes and has a single business focus and so a data mart allows different teams or departments to have control over their own data set for specific use cases so here we have a data warehouse and we are running queries to then put them in their own little data warehouses uh but the idea is that you know there are you know just smaller data sets that are more focused databars are generally designed to be read only because you're going to always have to pull data from the main data warehouse data marks also increase the frequency at which data can be accessed because of just smaller data sets and you don't have to worry about you know a huge cost because the larger the data set you have to query over the more expensive it gets right the cost of query is much much lower and so you might even see people accessing these a lot more frequently than they would a data warehouse so there you go so what is a data lake it is a centralized storage repository that holds vast amounts of raw data so big data in either semistructured or unstructured format a data lake lets you store all your data without careful design or having to answer questions on the future use of the data so basically it's hoarding for data scientists here is kind of a visualization where you have a bunch of different data sources dropping into the data lake maybe you want to perform etls and put data back into the data lake and then you know you can extract out for reports ml all sorts of things we'll definitely cover data lakes when we get to the data lake section but uh this is the general overview here a daylight is commonly accessed for data workloads such as visualizations for bis tools realtime analytics machine learning onpremise data data lakes are great for data scientists but it's very hard to use data lakes for bi reporting so it's not that you can't do it it's just that there's additional steps and so there might be a different solution that might be a bit easier such as a data warehouse if data lakes are not well maintained they can become data swamps which is basically like data corruption right so yeah there you go what is a data lake house well a data lake house combines the best elements of a data lake and data warehouse and this isn't something that azure has an offering for right off the bat right now but you can definitely definitely believe that cloud service providers will have this in the future so i want you to know about this today even if it's not on your exam so data like houses compared to data warehouse can support video audio and text files support data science ml workloads have support for both streaming and etl work with many open source formats data will generally reside in a data lake or blob store so the thing with data lake or data warehouse is usually used proprietary formats so this is a lot more flexible data like houses compared to data lakes can perform bi tasks very well which is something data links cannot do much easier to set up and maintain has management features to avoid a data like becoming a data swamp right because data warehouses are very well data modeled and data lakes aren't so data lakes are kind of in between and uh data lakes and with a data lake house is going to be more performant than a data lake so where would you find a solution right now probably with data delta like which is a data bricks solution so it's apache data like i believe is open source and so if you wanted a managed version of it databricks has an offering for that and so they have this nice little graphic here where they show you that you know you combine the two and you get the best of both worlds so you know you'll see more data likes in the future so there you go hey it's andrew brown from exam pro we are looking at data structures so what is a data structure this is data that is organized in a specific storage format that enables easy access and modification and a data structure can store various data types so data can be abstractly described to have a degree of structure so we have unstructured a bunch of loose data that has no organization or possibly any relation semistructured data that can be browse or search with limitations structured data that can be easily browsed or searched so when we look at unstructured data think of a bunch of loose files and this is just a screenshot from one of my folders here's a bunch of stuff or semistructured data we have xml or structured data where it's like we're using a relational database so yeah there we go let's go drill down into these three types of abstractions so again what is unstructured data well it's just a bunch of loose data think of it as junk folder on your computer with a bunch of random files not optimized for search analysis or simply no relation between the various data so again there's a bunch of files in my uh one of my folders there and so when we're talking about microsoft azure services that store unstructured data we have sharepoint so shared documents for an organization azure blob storage so unstructured object data store azure files a mountable file system for storing unstructured files azure data lake for big data it's basically blob storage but for vast amounts of data and if i wanted to add a fifth one there you know like azure azure disks but that's more for virtual machines okay so let's take a look here at semistructured data which basically has no schema and the data has some form of relationship it's easy to browse data to find related data and you can search data but there are limitations or or when you search you will pay a computative or operational cost a great way of thinking about this is think of a big box of legos and you have these lego pieces so it's not that there's not a schema defined there's a schema defined in the sense of the lego piece so it can connect and make relationships with other things that are compatible but overall the entire data as a whole does not have a schema right you don't have you're not upfronting and doing data modeling so just understand that's why i put the asterisk there so there is a schema for the data structures just not for everything in total totally here that's why it's called semistructured for concrete semistructured data structures we've got xml json avro and parquet i don't know if it's pronounced parquet but that's the way i say it and for azure and other services that store semi semi structured data we have azure tables which is a key value store azure cosmodb where its primary one is document of course it stores other types but when we're talking about semistructure we're talking about a document store mongodb which is an open source document store and then we have apache cassandra which is a y column store database um there's no sql that's just an open source one okay all right so we're still on semistructured data structures i just want to give them a little bit more attention because you know you might need to know some of the the guts to these semistructures so what is semistructured data semistructured data is data that contains fields the fields don't have to be the same in every entity you only define the fields that you need on a per entity basis so common semistructured data structures is javascript object notation json format used in json notation stored data in memory read and write from files apache apache optimize row columner format also known as orc organizes data into columns rather than rows so column or data source structure apache parquet this is another column or data store a parkit file contains rows and groups we have apache avro which is row based format each record contains a header that describes the structure of the data in the record you have also xml we're not going to go through all of these but i do want to drill down on some of these semistructured data structures so you know how they internally work okay all right let's take a look at json so json stands for javascript object notation and it is a lightweight data interchange format it is easy for humans to read and write it is easy for machines to parse and generate and it is based on a subset of javascript so here is an example of json and json is built on two structures the first is a collection of names name value pairs in other languages this is realized as an object a record a struct a dictionary a hash table key list or associative array so if you've ever heard of those things before that's basically what it looks like the other part is an ordered list of values other languages might call them arrays vectors list or sequence just to point them out there is the collection and there is the ordered list and json is a text format so that it is completely language independent so it is used quite a bit these days all right let's take a look at apache org files which stands for optimize row columner it's a storage format for apache hadoop system so it is similar to rc files and parkit files and is the successor to rc files we're not going to cover rc files here but you know that's where these come from it was developed by facebook to support columnar reads predictive pushdowns and lazy reads is more storage efficient than rc files taking up 75 percent less space orc only supports hadoop hive and pig when we get to the hadoop section you'll understand what those are work performs better with hive than parquet files org files are organized into stripes of data so here is that example there the autonomy of an org file so the file footer stores the auxiliary information the list of stripes in the file the number of rows per stripe each column the data type is column level aggregate information so count min max the stripe footer contains a directory of stream locations and we have the road data which is used for table scans and the index table includes min max values for each column and the row positions for each columns and the default size of a stripe is 250 megabytes with large stripe sizes enable large efficient reads for hdfs which is the hadoop file system which we'll talk about when we get to the hadoop section but if you want to just review this here and then take a look at this graphic and it'll make a hundred percent sense here okay so let's take a look here at parquet files so apache parquet is a column restore file format available to any project in the hadoop ecosystem so hive hbase mapreduce pig spark presto there's a huge list of them and not just here in the hadoop system but other other services azure even aws services work really well with parquet files so you know it's just becoming a very common format for columnar storage formats the parquet is built to support very efficient compression encoding schemes it uses the record shredding assembly algorithm that's why i don't go in detail like work here here about like talking about this data structure because it just gets complicated so i just want you to know that parquet files uh are more generally used in org files or have very particular use cases uh and you're gonna come across parket more when you're doing column or storage file formats okay let's take a look at avro so apache avro is a rowbased format that provides rich data structures compact fast binary data format a container file to store persistent data remote procedure calls rpcs simple integration with dynamic languages avro provides functionality similar to systems such as thrift and protocol buffers here is the data structure so when would you be using avro over parquet well it's just when you uh when you have data like if you want to serialize your json into a more efficient format we're doing general queries right if you're doing analytics right columner based stuff you're doing it for um aggregation and stuff like that if you're just trying to kind of like simulate general general relational database structures for semistructured or nosql databases you're going to want to use avro all right let's talk about uh structured data so structured data it has a scheme and data data has a relationship it's easy to browse to find related data it's easy to search data the most common structured data is tabular data representing the rows and columns right so examples here for azure would be its postgres azure data sql database for postgres for msql azure sql for mssql and azure synapse analytics which is the data warehouse service okay hey this is andrew brown from exam pro and we are looking at what is data mining so this is the extraction of patterns and knowledge from large amounts of data not to be confused with the extraction of data itself a lot of people think data mining means go on a website and start scraping that's not what it is uh cross industry standard process for data mining called chris dmm is defined into six phases there's a lot of way to define data mining but i just chose this one because i found that it's the easiest to understand so we got our big wheel here let's break through the six phases so business understanding right so here we are at the start of our journey over here business understanding so what does the business need then we have data understanding which is what do we have to do and what data do we need and you can see that you can work between back and forth before you move on to the next step we have data preparation so how do we organize the data for modeling then we have modeling so what modeling techniques should we apply over here evaluation so what which data model best meets the business objectives and on the end here we have deployment so how do people access the data so there you go let's take a look at different types of data mining methods and this will definitely not be an exhaustive list but it will give you a good idea what a data miner does so data mining methods or techniques is the way to find valid patterns and relationships in huge data sets so we have classification this is where you classify data into different classes you have clustering a division of information into groups of connected objects you have regression which is ident identify and analyze the relationships between variables because of the presence of other factors we have sequential so evaluating sequential data to discover sequential patterns association rules discover a link or two or more items find a hidden pattern in the data sets these common these common constraints math formulas are used to determine significant and interesting links so we got support so indication of how frequently the item set appears in the data set confidence indication of how often the rule has been found to be true lift indication of importance compared to other items conviction indication of the strength of the rule from the statistical independence for outer detection we have observation of data items in the data set which do not match an expected pattern or expected behavior we have prediction use a combination of data mining techniques such as trends clustering classification to predict future data not super important for you to remember all this but i'm just trying to like get you exposure to these terms and things so that as you see them more it'll make sense okay hey this is andrew brown from exam pro and we are looking at what is data wrangling so data wrangling is the process of transforming and mapping data from one raw data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics also known as data munging so there are six core steps behind data wrangling the first is discovery so understand what your data is about and keep in mind domain specific details about your data as you move through the other steps structuring so you need to organize your content into a structure that will be easier to work for our end results cleaning so remove outliers change null values remove duplicates remove special characters standardizing formatting enriching appending or enhancing collected data with relevant context obtained from additional sources validating authenticate the reliability quality and safety of the data and publishing so place your data in a data store so it can be used downstream so there you go hey this is andrew brown from exam pro and we're looking at what is data modeling but before we can answer that we should ask what is a data model so it's an abstract model that organizes elements of data and standardizes how they relate to one another and to the properties of real world entities a data model could be a relational database that contains many tables so here's actually an example of some data modeling i did which is for the exam pro platform if you ever open up power bi they have like a data modeling tab so it becomes very clear what it is but generally uh you know data models just look like a bunch of tables and relationships but it's going to vary based on what you're using a data model for so a dml could be conceptual so how daters represented the organizational level abstractly without concretely describing how it works within the software so people orders projects relationships logical so how data is presented in software tables columns object oriented classes physical so how data is physically stored so partitions cpus and table spaces so this one would probably be the the middle one here which is logical okay so you know this isn't just exactly how data modelling looks like there's all varieties the way data modeling or a data model can appear so what is data modeling a process used to define and analyze data requirements needed to support the business processes within the scope of the corresponding information systems and organizations so here uh we have our uh data modeling here so you can see that uh it's actually broken up kind of into three sections which maps up really well see where it says physical conceptual things like that it matches up to our three categories here conceptual logical physical so just take in mind that uh you know if you have data modeling you can move from a conceptual to a logical to a physical one all right and so there you go all right let's take a look at etl versus elt so etl intel is used when you want to move data from one location to another where the data store database have a different data structure so you need to transform the data for the target system a common use case would be mssql to cosmodb so this one is relational this one's nosql they just don't have the same data structures you'd have to do some kind of transformation and so here we have our visuals for etl and elt so let's talk about etl first which stands for extract transform and load so loads the data first into a staging server and then into a target system so even though it's not shown here we actually have an intermediate virtual machine or server that's being loaded temporarily into doing the transformations and then when it's done it's going to output it into its target system uh used for onpremise relational and structured data so it's very common for onprem like this could be a migration strategy so they could be taking an sql database and just moving it to um a sql database on azure right and so there might be like they're the same type of database but there could be different versions of databases so the the feature sets slightly different so they do some transformations it's good for a small amount of data to be fair etl can be used for larger workloads but you know when we're comparing from elts it's generally smaller doesn't provide data lake support easy to implement mostly supports relational databases okay when we talk about extract load transform loads directly into the target system used for scalable cloud structures and unstructured data sources used for large amounts of data provides data like support requires specialized skills to implement and maintain supports for unstructured data readily available so you're going to see the elt is going to be the more common use case where we're dealing with cloud but it does require a little bit more knowledge where this one is just like if you know sql you're going to be in good shape okay so there you go hey it's andrew brown from exam pro and we are looking at what data analytics is so this is when you're concerned with examining transforming arranging data so you can extract useful information a person that does data analytics is called a data analyst and they commonly use tools such as sql business intelligence tools and spreadsheets if we look at the data and analytics workflow so you can understand their whole scope of their job they'll do data ingestion so getting data from multiple sources data cleaning and transformation so maybe they're using you know pandas and notebooks and things like that or sql commands dimensional reductions so they have to reduce the amount of data data and analysis which could be like statistics and things like that visualizations you use your bi tools or you are actually coding in dashboards and things like that so yeah that is data analytics all right let's talk about kpis here so key performance indicators are type of performance measurement that a company will use or their organization will use to determine performance over time so here's an example of a kpi for product revenue and the goal was 3.12 million but the company actually generated out 2.29 million so there's 26 percent under their goal kpi can evaluate the success of an organization or for a specific organization activity and there are two categories of measurement for kpis we have quantitative and these quantitative and qualitative or get easily confused because they're very similar name but quantitative the properties can be measured with a numerical result facts presented with a specific value so monthly revenue numbers of sign ups number of reports or defects so what we're looking up at here is quantitative okay and then we have qualitative so properties that are observed and can generally not be measured with the numerical results numerical or numeric or textual value that represents uh personal feelings tastes and opinions so maybe customer sentiment would be example there so that is kpis all right so we're taking a look here at data analytics techniques and this is something you 100 need to know for the exam so pay close attention here okay so the top of our list here we have descriptive analytics and the question we are answering here is what has happened uh so here we might have specialized metrics such as kpis or return investment or things that we're more familiar with like generating sales and financial reports at this stage we have a lot of data right it's very accurate comprehensive it's either live data and we can make very effective visualizations to understand our data because we have all that historical information but there's a lot more to it in terms of value and so we'll move down or out move out uh to see the other stuff that we can do here the next is diagnostic analytics so why did it happen it's supplemental to descriptive analytics we can drill down investigate descriptive metrics to determine root cause find and isolate anomalies into its own data set and apply statistical techniques we have predictive analytics so what will happen so we use historical data to predict trends or recurrence uh we use either statistical or machine learning techniques apply this is where a data scientist might get involved we use neural networks decision trees regression classification neural networks just means deep learning machine learning okay uh prescriptive analytics how can we make it happen so goes a step further than predictive and uses ml by ingesting hybrid data to predict future scenarios that are exploitable and then the last one is what if this happens and that's cognitive analysis so using analytics to draw patterns to create what if scenarios and what actions can be taken if the scenarios become a reality so there you go that is data analytic all right let's take a look here at microsoft onedrive so microsoft onedrive is a storage and storage synchronization service for files which reside in the cloud similar to products like dropbox google drive box and microsoft sharepoint so onedrive is intended for personal storage for a single individual you pay for different sizes of storage so you have five gigabytes which are free 100 gb 1tb and 6tb you do not worry about the underlying hardware the durability resilience fall tolerance availability that's what we call serverless technology here is a nice screenshot of what it looks like to use onedrive both in the browser and on your phone files can be shared easily to other users via a shareable link or a specific email that has onedrive account files are accessed via a web app or shared folders that hold a reference to file stored in the cloud so shared folders mean like you can literally use your file system and you'll have folders that are linked to it files can be synchronized so a copy resides in a local computer hard drive is copied to the cloud a file residing the cloud can be copied to a little computer hard drive copying occurs automatically when files are changed differences and files could result in conflicts and a user must choose which file to keep or how to resolve the conflict might have more options files can be versions so you can recover older versions of files older files may retain for 30 days and be automatically deleted so there you go that is onedrive let's take a look here at microsoft 365 sharepoint so 365 sharepoint is a webbased collaborative platform that integrates into the microsoft office intended for document management and shared storage and here is a screenshot of my sharepoint uh and so there's the az104 so the thing is is that it's basically one drive uh but for companies with a bunch of layers on top of it and it is extremely useful to use it's super super useful so like if you work in a company you should really be using sharepoint for sharing files so just growing down the feature list here so sharepoint sites because there's some concepts here just besides documents they have things like sites so data within sharepoint is organized around sites a site is a collaborative space for teams with the following components so document library pages web parts and more sharepoint doctor document library which will expand there and that's what the screenshot is on the righthand side so a document library is a file storage and synchronization but designed for teams it is very similar to onedrive but files are owned by the company and not an individual you can apply robust permissions to access files within or outside your organization a site always has a default document library called documents and so there's a lot more going on in sharepoint but this is the most important feature of it and actually when you use onedrive when you install on your computer there's like a synchronization device it's the same thing so sharepoint most likely is using the onedrive technology underneath i don't know 100 but it's most likely the case so there you go hey this is andrew brown from exam pro and we are looking at data core concepts this one's four sheets long so let's jump into it so the first here is data which is units of information data documents types of abstract groupings of data data sets unstructured logical grouping of data data structures which has some form of structure and there's variance right so we have unstructured a bunch of loose data that has no organization or possible relation we're talking about flat files here various files that can reside in a file system semistructured so that's data that can be borrowed or searched with limitations so csvs xml json parquet and so if we're talking about xml files the markup looks like html for json it's a text file that's composed of dictionaries and arrays rc files are storage formats designed for map reduce framework not something we covered in the uh lecture content but i just wanted to mention them there work so a columnar data structure 75 more efficient than rc files limited compatibility works very well with hive we have avro so a rose rowwise uh data structure for hadoop systems you have parkette a columnar data structure that has more support for hadoop systems than orc then we were talking about structured data so data that can be easily browsed or searched so tabular data and so tabular data is data that is arranged as tables think of spreadsheets data types how single units of data are intended to be used we're not going to go through the whole list they're not going to ask that on the exam but you should know your data types uh four types of roles that azure cares for you to know we have database administrators so configures and maintains databases data engineer design and implement data tasks really to transfer and storage of big data data analysts analyzes business data to reveal important information then we have our tiers of computing so we have software as a service a product that is run and managed by service provider platform as a service focus on the deployment management of your apps infrastructure as a service basic building blocks of cloud i.t provides access to networking computers data storage and space and remember that uh we're talking about sql it's going to be the sql vms on this layer and then here's going to be the managed sql and azure sql databases okay so we're on to the second page here so let's talk about data stores unstructured or semistructured data for housing data a broad term that can encompass anything that stores data databases structured data that can be accessed quickly and search generally relative rowbased tabular data for oltp data warehouses structured semistructured data for creating reports and analytics columnbased tabular data for olap data mart's a subset of data warehouse for specific business data tasks data lakes combines the best of data warehouses and data lakes notebooks data that is arranged in pages designed for easy consumption batching when you send batches a collection of data to be processed not real time streaming when the data is processed as soon as it arrives so it's real time relational data data that uses struct structure tabular data and has relationships between tables and in terms of relationships for relational relational stuff we have one to one so one to one so think a monkey has a banana one to many a store has many customers many to many a project has many tasks and tasks can belong to many projects a join table a student has many classes through enrollments the enrollments would be the joint table and a class has many students through enrollments then we're talking about row stores so or row wise data organizing rows optimize for oltp then you have column store or columner data organizing columns optimize for olap so analytics now we have indexes a data structure that improves the reads of databases this is also shows up under nonrelational databases but i just threw it here just because we have pivot tables it is a table of statistics that summarizes the data of more extensive table from a database spreadsheet or bi tool now talking about nonrelational data data that has semistructured data associated with schema new school databases so we got key value each value has a key designed to scale only simple lookups i like to describe a simple dumb and not a lot of features we have document primary entities xml or jsonlike data structure called a document columner has a table like structure but the data is stored around columns instead of rows graph data is represented with nodes and structures where relationships matter okay uh we're on to the third page here so data modeling an abstract model that organizes elements of data and standardizes how they relate to one another in the real world entities schema a formal language to describe the structure of data used by databases and data stores during the data modeling phase schema is generally used for when upfront data modeling can be foregone foregone i did not write that right but because the schema is flexible normally used with no skill databases data integrity the maintenance and assurance of data accuracy and consistency over its entire lifecycle data corruption the act of data not being in the intended state will result in data loss or misinformation normalization a schema designed to store nonredundant inconsistent data denormalize a schema that combines data so that access to data is fast elts or etls transform data from one data store to another loads of data in an intermediate stage doesn't work does not work with data lakes elt transformations done at the target data store uh works with data lakes more common in cloud services things of azure app analytics okay or azure synapse analytics where the data is loaded and done uh in the actual data warehouse or etc a query when a user requests data from a data store by using query language to return the data result data source data sources where data originates from so analytics and data warehouse tools may be connected to various data sources bi tools would have data sources as well data consistency when data being kept in two different places and whether the date that the data exactly matches or does not match strongly consistent every time you request data you can expect consistent data to be returned within a time eventually consistent when you request data you may get inconsistent data so like stale data synchronization continuous stream of data that is synchronized by a timer or clock so guarantee of time asynchronous a synchronization continuous stream of data separated by start and stop uh stop bits no guarantee of time and this is synchronization in terms of processing okay data mining the extraction of patterns and knowledge from large amounts of data not the extraction of data itself data wrangling the process of transforming mapping data from one raw data into from form into another format and we're on the last page here so data analytics data analytics is examining transforming arranging data so that you can extract and study useful information key performance indicators probably not talked about the exam but i threw it in here because it's just important to know type of performance measurement that a company organization to determine performance over time then in terms of the types of uh analytics that we can utilize we have descriptive analytics what happened so accurate comprehensive like data effective visualization so dashboards reports kpis roi that's when you have all the information diagnostic analytics why did it happen drill down to investigate root cause sometimes they call that root cause analysis we didn't talk about that in the course but that's what it is focus on a subset of descriptive and now an analytics subset so it's a subset of this one up here okay predictive analytics what will happen so use historical data with statistics and ml probably should highlight that in red there for you to generate trends or predictions predictive analytics what will happen use hybrid data with ml to predict future scenarios that are exploitable cognitive analysts what if this happens so use ml and nlp to determine what if scenarios to create plans if they happen these are all really similar but the thing is is that they just it's it's the lens you put on like the the reason why you're doing it okay then we talk about run drive so storage and uh storage synchronization service for a single user and then we have sharepoint storage and storage synchronization service for an organization there's a little bit more to that but that is it for data core concepts let's take a look at azure synapse analytics and this is a data warehouse and a unified analyst platform we're going to talk more about the latter because like you know what a data warehouse at this point it's just a column or store and so here is a visual of um the data analytics or data synapses studio so here you can see there's a query going on so we're just querying data but there's a lot we can do uh on the unified analytics platform so we can perform etl and elt processes in a code free a visual environment so you don't have to write any code using just data from more than 95 native connectors deeply integrated with apache spark uses tsql queries on both your data warehouse and spark engines just that's what we're looking at there is the tsql and supports multiple languages so tsql python scala spark sql and net and it's integrated with artificial intelligence so ai and business intelligence tools bi so we could use azure machine learning studio or azure cognito services or microsoft power bi just to get a better visual of the entire flow here on the left hand side you're ingesting data from sources all the data is going to be stored on a data lake storage gen 2 here at the top here we have the azure synapse analytics studios that's where you're going to be doing the interface you're going to be working with and then you're going to be able to output the various services and notice here that we have sql and apache spark which are the different runtime engines this really looks like to me a data lake or lake house which when i was talking about lake house i was like azure doesn't have an offering but now that i'm looking at the screenshot this definitely is a data lake house so yeah i guess azure synapse is a data lake house cool all right let's talk about synapse sql so snapchat scale is a distributed version of t sql designed for data warehouse workloads it extends tsql to address streaming and machine learning scenarios it uses builtin streaming capabilities to land data from uh load supposed to say load data from cloud data sources into sql tables integrate ai with sql by using ml models to score data using tsql predict function and offers both serverless and dedicated resource models so for the serverless side this is great for unpredictable workloads so unplanned or bursty workloads use use the always available or serverless sql endpoint are our options for predictable workloads we create dedicated sql pools to reserve processing power for data stored in sql tables so here we're talking about dedicated dedicated sql pools and others so let's talk about those very quickly i couldn't even be bothered to make a graphic for this because it's just too much work so i just pulled it right from the docs but dedicated sql pool is a query service over the over the data in your data warehouse the unit of scale is an abstraction of compute power that is known as a data warehouse unit dwd once your dedicated sql pool is created you can import big data with simple poly based tsql queries and then use the power of this distributed query engine to perform high performance analytics then there's the serverless sql pools which looks like this and serverless sql pool is a query service over the data in your data lake scaling done automatically to accommodate query resource requirements as as topology changes over time by adding removing nodes or failure it adapts to changes and makes sure your query has enough resources and finishes successfully they're not going to test you on these data pool things but you know i just figured we'd provide a little more context and some more uh language around here just to help solidify what the service is but there you go just a couple of things that i just want to give extra emphasis on which is apache spark and data lake with synapses so it's synapse but i just keep on saying synapses just get used to it azure synapse can deeply and seamlessly integrate with apache spark as you can see here in ml models with spark ml algorithms and azure ml integration for apache spark 2.4 with builtin support for linux foundation delta lake there's the apache spark 3. so i'm surprised they're not up to date yet at least when i wrote this simplified resources models that freeze your you from having to worry about managing clusters fast spark startup and aggressive auto scaling builtin supportfor.net for spark allow you to reuse csharp expertise in existing.net code with the spark application talking about data lake here azure synapse removes the traditional technology barriers using sql and spark together and you can seamlessly mix and match based on your needs and expertise tables defined on files in the data lake are seamlessly consumed by spark or hive sql and spark can directly explore and analyze parquet csv tsv json file stored in the data lake fast scalable data loading between spark and sql and spark databases so there you go so a data lake is a centralized data repository for unstructured and semistructured data and a lake is intended to store vast amounts of data a daylight generally uses object blob or files as its storage medium and so the idea is you will collect data by putting putting various sources in there you'll do transformations so change your blend data into new semistructures using etl or elt and put it right back in the data lake or we could distribute it by allowing access to data to various programs and apis or publish the data set to a meta catalog so analysts can quickly find useful data now if we want to have an azure data lake uh you know you're gonna have to create a data lake storage but the thing is is that gen one is no longer really intended to be used the only people that should be using it are the people that are still in use but we'll focus on gen 2. so gen 2 is the data lake storage for azure blob storage which has been extended to support big data analytic workloads designed to handle petabytes of data and hundreds of gigabytes of throughput in order to efficiently access the data data lake storage adds a hierarchical hierarchical namespace to the azure blob storage and this is what it looks like on the lefthand side so we have two ways of access through the blob endpoint wasbs and the dfs endpoint abfs these are different drivers one is for object one is for file but they'll both get you in there and they're both compatible with hdfs which is hadoop and then the hierarchical namespace you get access control throttling timeout management performance and optimizations okay so there you go all right let's take a look here at polybase and this is a data virtualization feature of sql servers and specifically we're saying msql right microsoft sql polybase enables your sql server instance to query data with tsql directly from sql server oracle teradata mongodb hadoop clusters cosmodb without separately installing client connection software and so here's a fancy example to show you uh how great this tool is and polybase allows you to join data from an sql server instance with external data prior to polybase to join data to external data sources you can either transfer half your data so that all the data was in one location or query both sources of data then write custom query logic to join and integrate the data at the client level so there you go let's talk about how elts happen in um synapse analytics so you can perform elts uh in synapse sql and uh within the snaps analytics so the fastest and most scalable way to load data is through poly base external tables and copy statements that's why we talked about polybase so with polybase and copy statement you can access external data stored in azure blob storage data lake store via the ts sql language makes sense because if you can do data like store blob storage is the same thing so here is a graphic where you can see we are ingesting from uh sql into polybase into our data warehouse and then we can talk to our data lake and do a variety of other things so the basic steps for an etl are extract the source data into text files load the data into blob storage or azure data lake store prepare the data for loading load the data into into staging tables with polybase or copy command transform the data and insert the data into the production tables so azure data lakes analytics is an ondemand analytics job service that simplifies big data instead of deploying configuring and tuning hardware you write queries called usql to transform your data and extract valuable insights so the idea here is that by exporting approximately 2.8 billion rows of tcps ds store sales data 500 gigabytes into a csv it took less than seven seven minutes and importing a full terabyte of source uh took uh under with a connector took under less than six hours so the idea is that it's pretty darn fast and just to show you where it is it's over here in the middle so the idea is that this tool just lets you do run uh queries on your data lake okay let's talk about usql so usql is a structured query language included with data like analytics to perform queries on your data lake you can see there's a bunch of stuff in here like extract and stuff um and so usql can query and combine data from a variety of data sources including azure data lake storage blob storage sqldb data warehouse sql server instances running on azure vm you can install the azure data lake tools for visual studio to perform you sql jobs on your azure data lake i didn't see it in um azure data studio but it might be there too hey this is andrew brown from exam pro we're taking a look here at the azure synapse and data lake cheat sheet for the dp900 let's jump into it a data lake is a centralized data repository for unstructured and semistructured data a data lake is intended to store vast amounts of data data likes generally use objects blobs or files as its storage medium for the azure data lake storage generation 2 this is an azure blob storage which has been extended to support big data analytics workloads it does this via its hierarchical namespace and what the oracle namespace gives you is acls throttle management performance optimizers you can access your data like via the wasb protocol blob or abfs which is a file system protocol azure synapse analytics is a data warehouse and unified analytics platform has two underlying transformation engines so we have esqel pools and spark pools synapse sql is tsql but designed to be distributed sql dedicated pools is reserve compute for processing serverless endpoints ondemand no guarantee of performance data stored on azure data lake store generation 2 operations are performed within the azure synapse studio polybase enables your sql server instance to query data with tsql used to connect many relational database sources probably use with other services not just with azure snaps but there you go all right let's take a look at azure blob so blob storage is an object store that is optimized for storing massive amounts of unstructured data unstructured data is data that doesn't adhere to a particular data model or definition such as text or binary data as your blobs are composed of the following components so we have storage accounts which is a unique namespace in azure for your data you have containers which is similar to a folder and a file system and then the actual data being stored so azure storage supports three types of blobs we've got block blobs so these store text environment data made up of blocks of data that can be managed individually so we're up to 4.75 terabytes we have a pen blobs these optimize for append operations ideal for scenarios such as logging data from virtual machines and we have page blobs these store random access files to up to eight terabytes in size and store virtual hard drives vhd files and serve as disks for azure virtual machines and there you go all right let's take a quick look here at azure files so azure files is a fully managed file share in the cloud and a file share is a centralized server for storage that allows for multiple connections it's like having one big shared drive that everyone you know virtual machines can work on at the same time so here's an example or a diagram of it so to connect to the file share you use a network protocol such as server message block smb or network file system nfs when a connection is established the files shares file system will be accessible in the in the specific directory within your own directory tree this is known as mounting so some use cases here completely replace your supplement your onpremise file servers nas drives a lift and shift of your onprem storage to the cloud via classic lift or hybrid lift lift and shift means when you move workloads without rearchitecting so importing local vms to the cloud a classic lift would be where both the application and its data are moved to azure a hybrid lift is where the application data is moved to azure files and the application continues to run on premise we have simplified the cloud deployment so shared application settings so multiple vms and developer workstations need to access the same config files or diagnostic share we have all vms logged to the file share developers can mount and debug all logs in a centralized place we can dev test and debug so quickly share tools for developers needed for local environments we can do containerization so you can have azure files to persist volumes for stateful uh containers super useful when you're working with containers why use azure files instead of setting up your own file share well shared access so already set up to work with the standard networking protocols it's fully managed so it's kept up to date with security patches designed to scale uh it has scripting tools to automate the management and creation of file uh files shared with azure api and powershell and it has resilience so it's built to be durable and always working so there you go hey this is andrew brown from exam pro and we're looking at azure account storage cheat sheet and this is a very short section so azure storage accounts an umbrella service for various forms of managed storage you have azure tables blob storage and files there's of course cued and some other things in there but these are the three that we care about azure blob storage object storage is distributed across many machines supports three types so we got blah blah so store text and binary data blocks of data can be managed individually up to 4.7 terabytes append blocks optimize for append operations ideal for logging page blobs store random access files up to 8 terabytes in size azure files is a fully managed file share in the cloud to connect to the file share and network protocols used either smb or nfs azure storage explorer a standalone crossplatform app to access various storage formats within the azure storage accounts and there you go let's talk about business intelligence tools so bi is both a data analysis strategy and technology for business information the most popular bi tools are tableau microsoft power bi and amazon quick site we're going to obviously be focusing on power bi because that's what azure would like us to focus on bi helps organizations make datadriven decisions by combining business analytics data mining data visualization data tools infrastructure and best practices and there's the logo of the three so you know what it is and now we'll jump into power bi hey it's andrew brown from exam pro and we're taking a look at microsoft power bi which is a business intelligence tool for visualization business data and here's a screenshot of the power bi desktop and power bi can get a little bit confusing because they have a lot of things under the power bi name but i'll break them down here so it's nice and clear so the power bi desktop is a way to design and adjust reports the power bi mobile is a view reports on the go on your phone power bi service sometimes called the power bi portal is to access some modified reports in the cloud and power bi embedded is a way to embed power bi components into your applications and usually you need to get data into a power api and so this is one of the most powerful reasons why people like using it it's because it ingests with so many data sources so in here this is a desktop one you go in and you can go under azure and there's all like every azure service you'd ever want if you go the database tab there's a lot of database integrations for postgres mysql everything it's crazy and so power bi can directly integrate with azure services as you saw here i couldn't be bothered to make a graphic here but you know here you can see you can get things from hd insights sql databases account storage machine learning stream analytics event hubs things like that uh so just to compare the two because these are the most important services is the desktop and the service and they're very easy to get mixed up so power bi desktop is a dell is is a downloadable free windows application and installed on a local windows computer if you're on a mac you cannot use it sorry or linux either report uh it it can it has reports or sorry so the role that somebody would be using would be you would be a report designer and you'd use uh the desktop application to publish power bi reports to the power bi service okay and power bi service is a cloudbased service where users view and interact with reports users in power bi service can edit the reports and create visuals based on the existing data model and they can share and collaborate with coworkers so just looking at the overlapping services power bi desktop has many data sources transforming shaping and modeling measures calculated columns python themes rls creation then on the power bi server side you have some data sources that you can ingest dashboards that is the key thing for power bi services that you get dashboards you don't get that on the power bi desktop part apps and workspaces sharing data flow creation paginated reports rls management gateway connections paginate reports is actually with the builder which you have to download so i'm not sure why it's in there both you get reports visualization security filters bookmarks q a and r visuals but just make a note here that you use the power bi desktop to create reports and then they're get they can be used in power bi service to create dashboards okay let's talk about data visualizations and chart types and specifically power bi ones so power bi has many kinds of visualizations we'll cover the most common ones but you can see over here like look at all these little little squares that represents all different kinds of visualizations you can make and even with them they're highly configurable okay so let's go and look at bar and column charts so see how a set of variables changes across different categories we've all seen bar charts it supports stacked ones and bar charts stacked side by side or horizontal ones you know the you know what bar charts are line charts overall shape of an entire series of values so they're just lines uh we have a matrix so that is where you have a tabular structure that summarizes the data you have key influencers the major contributors to a selected result or value and that one kind of has a very cool looking visualization you have tree map charts of colored rectangles with size representing the relative value of each item we have scatter graphs of represent relationships between two numerical values so you have an x and y it's basically a bunch of dots on a graph you have bubble chart it's the same thing but the the dots now are bubbles and the larger the bubble can represent a third dimension you have dot plot charts and these are a little bit confusing to look at but they are basically bubble charts but you they're organized based on an xaxis so you're basically putting those into categories i'm always confused when i look at that one but that's just one and one more for us here is a field map so you have a geographic map where different areas can be filled so like here you have states of different colors that represent things it could be gradients there's a lot you can do with maps and geographical maps and that again is not all the data visualizations but the most common ones you'll come across all right let's take a look here at power bi embedded and honestly this probably won't show up on the exam it's just that when you look use azure and you type in power bi it shows up in the console and i was like what is this thing and i thought this was kind of interesting and i feel like it's it's relevant so that's why i have it in here so azure power bi embedded is a platform as a service analytics embedding solution that allows you to quickly embed visuals reports dashboards into an application for independent software vendors it enables you to visualize application data rather than building the service yourself for developers you embed reports and dashboards into an application for their customers to use azure power bi if you need a power bi pro user account you need to create an app workspace and you need to choose a capacity so either i guess it'd be like billing work via capacity based or hourly metric modes there you go let's take a look at power bi interactive reports and these and so basically when you're using power bi desktop and you can generate reports in the in the portal or service but uh the reports are interactive so if you're getting confusing like there's power bi reports and interact reports basically by default everything's interactive with power bi okay so here is an example of one i just downloaded the uh like the example one that uh microsoft provides and as you can see they uh like in the middle of it there's like a little knob so that kind of gives you an indication of interactivity or there's like other buttons here so like if you go here you can actually click between map and tabular i believe you can move this range around just get different information so they're highly interactive uh then they're extremely stylized as you can see you can make them look really really good a report can contain many pages and you can assemble reports as easy as choosing a visualization and dragging it out so you take that just drag it out where you want to go and customize it from there now underneath what you can do is you can see the underlying data so it's just like tabular data and all the tables and fields that populate it and you can and i think with like a pro version you can modify it and so you cannot do these with dashboards so we're using the power bi service you're not going to get access to this when you're looking at data modeling it's again in power bi desktop you can see the relationships between models and modify them and do things with them again you cannot do this with dashboards and that's a key thing you need to understand between the interactive reports and the dashboards let's take a quick look here at power bi service and also dashboards which is very important for this so power bi is a cloudbased service where users view and interact with reports and where they can create dashboards and so here is um a screenshot of me logged into power bi if you wanna know how to get there you go app.powerbi.com and uh if you already have you have to even if you have a microsoft account you have to fill in a form and then it activates the service and you can go and explore some dashboards and reports right off the bat for free so it's very easy to jump into one of the concepts that's very important with power bi service is dashboards and so before we talk about that let's talk about what a tile is a tile is a snapshot of data pinned into your dashboard so here's an example of data a tile can be created from a report a data set a different dashboard qa q and a box excel sql server reporting service ssrs and many many more looking at a dashboard is a single page often called a canvas that tells a story through a visualization so there it is the visualizations you can see on the dashboard are called tiles you can pin tiles to a dashboard from reports okay it is very very very important that we know the difference between reports and dashboards so this isn't going to be a fun slide but we'll have to go through it and work our way through so let's talk about the difference for capabilities they both have pages but dashboard has a single page and reports has multiple pages for data sources one or more reports you can have one or more reports and one or more data sets per dashboard and for report a single data set per dashboard for filtering you can't filter or slice for filtering out reports many different ways to filter highlight and slice you can set alerts for dashboards you cannot for reports for features you can set one dashboard as your featured dashboard reports there's no such thing as a featured report you can see the underlying data set tables and fields so absolutely not for dashboards absolutely yes for reports for customization new and for reports you got tons of customization so there you go all right let's talk about paginated reports which are reports designed to fit into page formats so they can be printed or shared the data display of all data are tables which can span multiple pages so rdls is an xml representation of an sql server reporting service so that's an ssrs report definition file a report definition contains data retrieval and layout information for report pattern reports are just a visualization of dot rdl files so power bi report builder is used to design pixel perfect remember that word pixel perfect they really use that a hundred times over passionate reports using power bi report builder it is a tool specifically designed for creation of patching reports so if you want to figure out how to download this power bi thing within your power bi service go to the top right corner go to download i don't know why that's animated but it worked out fine and so once you're in there uh you will download the file and you'll install this really old looking software but i guess the thing is is that this is a huge pain point for companies i guess and they really make a huge emphasis on it so i guess we need to know what it is hey this is andrew brown from exam pro and we are looking at the power bi cheat sheet for the dp900 let's jump into it so the first is business intelligence or bi which is both a data analysis strategy and technology for business information helps organizations make data driven decisions now we're talking about power bi so power bi desktop a desktop app to design interactive reports from various data sources can be published to the power bi service then you have the power bi service also known as the power bi portal a web app to view reports and create interactive shareable dashboards by pinning various data sets and reports visualizations you have power bi mobile a mobile web app to view reports on the go power bi report builder a windows app that builds pixel perfect printable reports used to build page data reports power bi embedded embed power bi visualizations into web apps interactive reports reports in power bi drag visualizations load data from many data sources both in desktop and and service meaning like you can do you can make reports in both power bi desktop and power bi service okay paginate reports pixel perfect printable report files uh tabular data laid out in page format dashboards build shareable dashboards by pinning various power bi visualizations a single page report basically designed for a screen only for the power bi service dashboard tiles or just tiles is a representation or represent a visualization that has been pinned to a dashboard it could be a bunch of other things but that's what the key thing it is visualization is a visualization uh is a chart or graph that's backed by it says my but you see by a data set and uh whoops we went to the next part but that's it for power bi all right let's take a look at structured query language which uh stands for sql it's designed to access maintain data for a relational database management system on rdbms we use sql to insert update delete view data from our data databases tables and sql can join many tables and include many functions to transform uh the final output of results on the right hand side that is a real query that i use in my postgres database to um grab exam sets so if you're on the xampp pro platform and you're doing a particular set of an exam this query gets that relative information and you can see that it's polling if you look down below here it's joining in tag information and then it has like sub queries and stuff so it's a very complex query and doing that formatting the sql syntax was standardized as iso 9075 that won't show up on your exam but it's good to know relational databases will mostly adhere to the standard while adding in their uh not one but own database specific features sql's highly transferable skill and we see sql being used in nonrelational databases provide a popular and familiar querying tool so it's something you definitely want to know how to do all right let's compare olap to oltp so online transactional processing versus online analytical processing when we're talking about ltp we're generally using databases so databases is built to store current transactions and enables fast access to specific transactions for ongoing business processing so think of uh you know any kind of sql server and then on the right hand side we have data warehouses a data warehouse is built to store large quantities of historical data and enable fast complex queries across all the data so when we visually look at the oltp we have a bunch of small transactions that are evenly distributed so they look pretty similar in the read and writes and then for data warehouse we have very very few retransactions and uh we have large payloads i think that the arrows are supposed to be pointing this way but that's okay it's not a big deal so when we're talking about databases you have a single data source you have short transactions small and simple queries with an emphasis on rights many transactions late it's latency sensitive and you have small payloads on the olap side we have multiple data sources so you're ingesting data long transactions long and complex queries with an emphasis on reads fewer transactions or very few and throughput sensitive and large payloads the use case over here would be general purpose adding items to your shopping cart would be an example a use case on the analytics side would be generating reports so there you go all right let's take a look at some open source relational databases that we know we are going to definitely encounter on azure starting with mysql which was created by my school a b i believe that's like a switzerland or a swiss company and they they are required by sun microsystems and then they are required by oracle and mysql was or is an open source project uh so mysql is a pure relational database rdbms it is a simple database which makes it easy to set up using maintain has multiple storage engines so in odb and my ism when it says that there's multiple they just mean there's two because i don't know of any other than those two but it's the most popular relational database the reason why it's been around forever it was one of the earliest mysql databases that was open source which is a very important thing to note um you know and it's just very easy to use mario db is a fork of mysql by the original creators of my mysql ab after oracle was acquired my school required mysql there was a concern that oracle may change the open source licensing or stop future my school from being free to use if you know oracle they'll like to charge charge you for their stuff and oracle has their own database and so you know there was a lot of fear around that and the thing is is that when my sql ab sold their database to sun microsystems it it's because they trusted sun but then they didn't know that sun was going through a lot of financial difficulties and then literally a year later some was acquired by oracle it's just how it goes eh so they would have never sold to oracle originally that's why we have mario db um then we have postgres which evolved from ingress the ingress project at the university of california postgres is an objectrelated relational database so ordbms it just has a single storage engine which i guess is the ingress engine right here and so it's the most advanced relational database it can support full text search table inheritance triggers rows data types request slot you know they say the most advanced i mean it's more advanced than mysql postgres is the database i love to use it's such a great service i don't understand how it's object relational i think it's just how they store the data underneath and that's what makes it so flexible but i can tell you that postgres is a lot easier to use like initially mysql the syntax is easier but postgres in terms of data modeling is a lot easier because you could just create columns you don't have to worry about them whereas like mysql you've got to fiddle around with the data types it's very frustrating or there's like serious limitations on rows so yeah that's the two there okay and if you want to deploy these on azure it's really simple uh you just type in the name you go mario or mysql or postgres and then you just do azure database for mariodb and you just launch a server okay let's talk about read replicas for azure databases here so a read replica is a copy of your database that is kept synced with your primary database and this additional database is used to improve read contention if you've never heard the word contention before it means heated disagreement uh so the idea is that if you have a lot of reads and it's and it's hurting the database you can offload those reads to your secondary database that's dedicated specifically for read operations so rereplicas can be applied to azure sql and the managed instances so i guess you can't do it with the azure vms uh like the virtual machines the sql vms that's what they're called you can have multiple rewrite for a database i can't remember what the range is maybe it's between two to six i don't think that matters for the exam and just kind of a visual you have your read and writes that go to your primary and then a lot of your reads go to your read replicas and a very common use case to have a rereplica is so that you can use it as an olap when you are a very small size i'm not going to talk about an exam but i just know from a practical standpoint that's something that i've done multiple times over the years so there you go all right let's take a look at scitis on azure and honestly i don't know if it's cetus situs sometimes i want to say citrus but i could not find a pronunciation for it so i'm going to call it situs and it is an open source postgres extension that transform postgres into a distributed database and so situs extends postgres to provide better support for database sharding realtime queries multitenancy which is super useful time time series workloads and if you go to azure postgres and you see hyperscale option it really is just using siteis so um this is a really really really really good service if you are using postgres it's one of the few reasons i would consider using azure for my database because i use postgres as my primary one this will absolutely not show up on the exam but i think it's very useful to know what the service is all right let's take a look at the azure sql family when we say sql we're talking about microsoft's version of sql and if you search sql you'll see a bunch of stuff here even more than this and it can get really confusing but you absolutely need to know this for the exam you need to know the difference between these three main services so at the top you have sql server on azure virtual machines or sql you'll just see like vm for desktop vm a lot when you need os level control and access when you need to lift and shift your workloads to the cloud when you have an existing sql license and you want to save money via the azure hybrid benefit this is when you're going to use that okay if you've never heard the term lift and shift the idea is that on your onpremise environment you're running a virtual machine that is your database and you can literally save a virtual image import that into azure and it runs exactly how it did on your on premise so you don't get a lot of the advantages of the cloud but the idea is it's the easiest way to get onto the cloud right the next option is sql managed instance this is when you have an existing database you want to modernize it's the broadest sql server engine compatibility highly available just disaster recovery automated backups ideal for most migrations to cloud so the thing is if you're going to do a lift and shift you can you can kind of go to esco manage so you probably have to do a transformation some kind of like etl job or something to go into here but the idea is that there's a lot of different versions of um of uh mssql depending on how old it is and stuff like that so if you aren't going to be bringing your license or need os level you really want to be using this one because you get all the builtin scalability stuff right the third one is azure sql database this is a fully managed sql database designed to be fault tolerant built in disaster recovery hive it's highly available designed to scale uh and it's the best option um but again you know if you have an older database maybe you could do a transformation to it uh and then underneath it has sql servers i thought this was a fourth option but really when you go to azure sql and launch it you actually have to create a server because you can have multiple sql servers associated to a database and there's things called like what's it called like elastic pool or something like that so that's just the underlying server for the azure sql database it's not a service in itself but it's just a component of that service okay even though you can go in the ui and see a list there makes it really really really confusing but yeah there you go so azure elastic pools is a feature of azure sql and it allows you to uh there are simple cost effective solutions for managing and scaling multiple databases that have varying and unpredictable usage demand so databases in elastic pool are on a single server and share a set number of resources at a set price elastic pools in azure sql enable sas developers to optimize the price performance of a group databases within a prescribed budget while delivering performance elasticity for each database why would somebody want to do this because like it doesn't seem like a good practice to put a bunch of databases on a single server it's more like you'd rather want a database to be distributed across servers so if you're running a sas product software as a service you'll have multitenancy meaning that each person has their own database there's different levels of tenancy but if you did give let's say you had a company and um or like you had five large clients and they use the same software and they're all varying sizes but they're not large enough to justify their own server this would be the service for you right where you are constantly spinning up databases per customer um but i don't think i would ever use this in practicality and i am a multitenant sas but uh it's nice that they provide that option so there you go hey this is andrew brown from exam pro and we are on to the relational database cheat sheet uh and so let's jump into it so structure query language sql designed to access and maintain data for a relational database management system online transaction processing keyword there is transaction so frequent and short queries for transactional information so databases any kind of generic workload or web app online analytical processing so complex queries for large databases to produce reports and analytics so think data warehouse on to the open source relational databases we got mysql a pure relational database easy to set up most popular open source relation relational database definitely something i started off with mariodb is a fork of mysql postgres is an object relational database now my favorite relational database to use is more advanced and well liked among developers read replicas is a duplicate of your database in sync with the main to help to reduce reads on your primary database now talking about azure sql it's an umbrella service for uh for different offerings of mssql databases hosting services so we have sql vm so for lift and shift when you want os access and control or you need to bring your own license for azure hybrid benefit manage sql for lift and shift when your broadest when you need the broadest amount of compatibility with sql versions mssql in particular you can use uh manage sql on onpremise by using azure arc it gives you many of the benefits of a fully managed database but it's not as good as the azure sql database which is a fully managed sql database has a few options here you can run it as a single server run it as a database which is a collection of servers run in an elastic pool so databases of different sizes residing on this on one server to save cost uh then we have connection policies so we have three modes we got default so choose proxy or default initially depending if the server is within or outside the azure network we have proxy outside the azure network proxy through a gateway it's important to remember to listen on port 1443 this might show up on your exam so remember this port 1443 when connecting via proxy mode through a gateway outside the azure network and then last is redirect redirected with the azure network and that's the recommended way to do it and i just want to point out for these three here i didn't write it in here but remember that this one here is for infrastructures code this one is for platform as a service and this one's for platform as a service okay all right let's talk about tsql which stands for transact sql and it's a set of programming extensions from sybase and microsoft that added feature several features to the structured query language if you've never heard of cybase i think the original company that actually made the microsoft sql database and then maybe microsoft bought them out or et cetera but there's a long history there so tsql expands on the sql standard to include procedural programming local variables very various support functions for string processing date processing mathematics changes to the delete and update statements for the microsoft sql servers there are five groups of sql commands and honestly there's five groups for regular sql actually normally they'll just say even for this exam they're only going to tell you of about the definition manipulation one but i'm going to tell you all of them because you should know all of them it really helps to know them so the first is data definition language so this is ddl used to define the database schema we have data query language dql used for performing queries on the data data manipulation language dml manipulation of the data in the database data control language dcl rights permissions and controls of the database transaction control language tcl transactions within the database and so now that we've covered what tsql is let's dive in and actually look at all these types of documents let's start off with the data definition language which is sql syntax commands for creating and modifying the database or database objects so tables index views store procedures functions and triggers the first is the create command here and so we can create a database or database object so here you would just say create whatever it is you want to create table database etc so here we're creating a table called users and we're providing those fields okay then we have alter so this alters the structure of an existing database so alter table whatever it is the table and then we can add a column if we want to drop the database that deletes all the objects from the database truncate would just delete the records within the database comments is just a comment and rename is if we want to rename a database object now this is what's interesting here where we have execute sp rename so when we said that tsql extends this is its own little special language because in other other sql variants it definitely does not look like that okay let's take a look at data manipulation language dml and so this is going to have to do with anything with manipulating data so the first thing is we have an insert command to be able to uh insert data so here you can see we say the values we want and if you're wondering how does it know what values it's going to be the order in which the column appears in the in the actual table that's how it knows that andrew goes under first name and email goes in like the email goes into the email field then for update this is uh when we want to update existing materials so we'll say update users then we'll have to say where so we want to match the id 8 update user 8 and then we'll set the values that we want to change then we can delete a user very simple delete from users where id equals 6 merge or upsert to insert or update records at the same time i don't see these in other languages so i think it might be a tsql specific thing it's kind of hard to show this because they're very large queries but if you need to do an insert and update at the exact same time you use this then you have call this allows you to call proceed or java sub programs basically functions so let's say you need to calculate a function uh to calculate the distance between toronto and chennai uh you could uh do that you have lock table and this is for concurrency control to ensure two people are not writing to the to the program at the same time okay all right let's take a look here at the data query language dql and the first thing we have is the select and by the way every if it's query that means it has everything to do with selecting data okay so here what we're going to do is select the these particular fields and do from users if we want to get all fields we can just do an asterisk here um but that's the idea is like i want these fields from this users you could even say like where and other stuff in there we have show so this describes what a table looks like most other languages like bicycle would just say like show and then the table name but here again we have that these kind of weird exec sp columns thing which again is a tsql specific thing and so show would describe what the table looks like so what columns is contained okay we have explained plan so returns the query plan of a microsoft azure synapse analytics sql statement without running the statement this thing is really complicated i could not show you an example there but you know that's what it does help is just like i want to understand more information about database objects so here you're asking more information about the user's table again this is the special tsql stuff all right let's take a look here at data control and this has to do with well control right so we have grant saying i have a i have a table called employees i'm only going to let the um ts goal user or the uh mssql user andrew only be able to select insert update and delete on then you have revoke and that's the opposite let's just say you know we don't want bacon to be able to delete anything on the employees table and that's pretty much it let's take a look here at transaction control language tcl so tcl commands are used to manage transactions in a database transactions is when you need multiple things to happen and if they don't all happen then you roll back on them okay so this is really important in finance where you have multiple people that have to be part of the purchasing decision and if all purchases don't follow through then you don't want to commit that transaction okay so we have command so set to permanently save any transaction to the database rollback restores the database to the last committed state save point used to temporarily save a transaction so that you can roll back to this uh to the point whenever necessary set transactions specify characteristics for the transaction all right just a quick review of all the sql documents we or syntax documents we just looked at and so we had ddl which is for defining dml which is for manipulating dql which is for querying dcl which is for controlling and tcl which is for transacting now i highlighted ddl and dml in red because these are what the exam will focus on and sometimes they simplify and they'll take things like that that go in um like select and they'll just put it in manipulation okay so these are the two main ones that you'd have to choose between but i wanted to show you all five of them because this is really what sql is based off of uh and so you know this is just the right way of looking at it this is not an exhaustive list of all the possible commands but it was what i could terry pick out that i recognized that i was familiar with okay hey this is andrew brown from exam pro and we are on to the tsql cheat sheet so transact sql is a set of programming extensions from sybase and microsoft that adds several features to the structured query language this is used for ms sql databases okay for mss google servers there are five groups of sql commands and so we have data definition language used to define the database schema data query language used for performing queries on data data manipulation language manipulation of data in the database data control language rights permissions and other controls of the database transaction control language tcl transactions within the database so on the exam they're going to ask you either this or this they don't bother with all of them but they're actually r5 okay and so that's all you need to know here let's talk about connectivity architecture so when a connection from a server to a azure sql database the client will connect to a gateway that listens on port 1443 and i want to remember that port number because it's an important part number that might show up on your exam okay so over here on the righthand side we have a virtual machine connecting to various different sql servers and so the idea here is that based on the connection policy the gateway will grant traffic and route access to the appropriate database so we actually have three kinds of policies we got proxy so connections are proxy through a gateway increased latency and reduced throughput intended for workloads connecting from outside the azure network redirect which is mostly recommended establishes a direct connection reduced latency improved throughput intended for workloads connecting inside the azure network and default which is just going to default if you launch a vm outside the azure network it's going to use proxy if you launch it within the azure network it's going to use redirect so this thing this port 1443 is only important when you're doing proxy because if you're internal you don't need to go through that port there's no gateway to pass through but yeah that is just the different kinds of connection policies there all right so let's take a look here at ms sql database authentication so during the setup of your ms sql database you must select an authentication mode you got two options here windows authentication mode which enables both windows authentication and disables sql server authentication and mixed mode which enables both windows authentication and sql server authentication so if you were to remote into your windows machine and under your server properties look under security and server authentication there are those two options interestingly they don't call it mixed mode in the ui but that's what it is it's called mix mode so let's talk about what windows authentication and sql server authentication is so windows authentication which is the recommended way is specific windows users and groups group accounts are trusted to log into the sql server and very and this is the most secure and easy way to modify revoke privileges because you know if they're windows users that means that you can then manage them from azure active directory right then you have sql server authentication so we have a username and password which is set and stored on the primary database you cannot use a kerberos security protocol so that's one disadvantage login password must be passed over the network at the time of connection so that's an additional attack vector but this is an easier way to connect to the database from outsider domain or from a webbased interface so it's just going to be based on the uh the scenario that you're in but if you can just stick with windows authentication let's take a look here at network connectivity so for your sql database you need to choose either a public or private endpoint a public endpoint is reachable outside the azure network over the internet and you would use firewall rules to protect your database for private endpoints you're uh they're only reachable within the azure network or connecting uh or originating from inside the network so you would use azure private links to keep your traffic network within the azure network so here's just the two options you would see when you provision your database you'll just see here that you choose either public or private and you're setting firewall rules or you're creating private endpoints okay well let's take a look here at azure defender for sql which is a unified package for advanced sql security capabilities and what it does is vulnerability assessment and advanced threat protection so azure defender is available for azure sql the manage instance and synapse analytics and what it does is it discovers and classifies sensitive data or classify sensitive data surfacing and mitigating potential database vulnerabilities detecting anomalous activities and you can turn it on at any time and you just pay a monthly cost within the azure portal so there you go let's take a look at azure database firewall rules so azure databases are protected by server firewalls a server firewall is an internal firewall that resides on the database server all connections are rejected by default to the database so once your database is provisioned you have an option where you click server firewall and what you're going to do is configure it so you'll say here i allow azure so i give 0000 and notice also the connection policy remember we talked about that before so you can set proxy or redirect and so there's that there and then if you wanted to do it via tsql you could as well this is allow only allow the server at zero point four at zero zero zero point four because you can specify your range there is azure firewalls which is uh a totally different service and then there's network security groups which it which is like a logical firewall around your uh vm or your i'm sorry around your nic cards and the subnet but this is the one we were talking about here which is the server firewalls for azure databases okay all right let's take a look here at always encrypted which is a feature that encrypts columns in the azure sql database or sql server so if you had a column like a credit card number and you wanted to always keep it encrypted you'd use always encrypted and so always encrypted uses two types of keys we have column encryption keys which are used to encrypt the data in an encrypted column and call them master keys a key uh protecting the key that encrypts one or more column encryption keys and you can always uh uh you can al you can apply always encrypted using tsql and so since there is a key that encrypts the key that is called envelope encryption and that is a great way of doing that i imagine that maybe it gets stored in eks or whatever the name of the service that azure calls for their uh encryption keys let's take a look here at rolebased access control specifically for databases so rolebased asset controls is when you apply roles to users to grant them fine grade actions for specific azure services and there's four in particular that we really do care about here uh to databases and this is and this will probably show up on your exam so you definitely need to know these we have sqldbcontributor so this role allows you to manage sql databases but don't access them can't manage their security related policies or their parent sql servers you have sql managed instance contributor this is so you can manage sql managed instances and required network configuration can't give access to others sql security manager manage the security related policies of sql servers and databases uh but not access to the sql servers and last is sql server contributor manage sql servers and databases but not access not have access to them those sql servers okay so our bacs you definitely want to know these four okay let's take a look here at transparent data encryption tde which encrypts data at rest for microsoft databases it can be applied to server sql uh or sql servers azure sql databases azure synapse analytics tde does realtime i o encryption and decryption of data logs and files encryption uses a database encryption key called a dek data database boot record stores the key for availability during recovery the d e key the d e k is a symmetric key so it's the same cryptographic key for both of the encryption uh of plain text and decryption of cipher text just to give you a visual there to help you out the idea is you have something that's plain text you use the same key to encrypt it and then the same key to decrypt it and that's just how that works so the steps to apply tdd to database create a database master key create a certificate to support the tde create the database encryption key enable tde on the database so that's just how you do it there within the azure portal and there you go before we start talking about what dynamic data masking is let's define what is data masking this is when a request for data is transformed to mask the sensitive data so imagine you have a credit card here and you do not want to expose that to particular users maybe there are part of your support team or or even the end user themselves and so the idea is that it's in the database it's stored in its raw format it's untransformed but what it'll do is pass through a masking service and that will have different rules on it which will then apply a particular filter so here we'll only show the last three so dynamic data masking which is a feature of a particular azure sql servers anyway can be applied to azure sql and manage instances and synapse um and so the idea is you would just turn this feature on and then you create a masking policy so you could say exclude a particular users for masking so like let's say you have like root users admin users that need to see all the data you can do that you make masking rules so what fields should be masked and then you have masking functions so how to apply the masking field so here this would be a masking function that would say okay only show the last three letters okay let's take a look at private links so azure private links allows you to establish secure connections between azure resources so traffic remains within the azure network so here's a big graphic of what that looks like and the idea is that you have a private link endpoint uh which is just a network interface that connects you privately and securely to a service powered by azure private link and private endpoints uses a private ip address for your vnet so many azure services by default work with private link and third party providers can be powered by private link as well private link service which allows you to connect your own workloads to private link you may need an azure standard internal load bouncer to associate with the link service but the idea here is that if you have your sql server right that way you can connect it to your onpremise loads or if you have an sql server on your onpremise and you want to do it to a vm that's what you're going to use okay hey this is andrew brown from exam pro we are on to database security for azure the dp900 so let's jump into it so mssql database authentication we have two modes when setting it up when you're remoting into a windows machine so you have windows authentication mode which enables windows authentication and disables sql server authentication and we have mix mode where enables both of these things what are these things well windows authentication is when you authenticate via windows users and sql server authentication is between the user and password you can connect from anywhere windows authentication is the recommended one because it's just more secure for network connectivity we have public endpoints so they're reachable outside the azure network over the internet you use server firewalls for production and you have private endpoints so only reachable with the azure network so use azure private links to keep traffic within the azure network azure defender sql a unified package for advanced sql server security capabilities for vulnerability assessment and advanced threat protection server firewall rules an internal firewall that resides on the database server all connections are rejected by default uh to the database always encrypted a feature that encrypts columns in an azure sql database or sql server rolebased access uh controls for databases so these are roles you need to know the sql db contributor manages the sql database but not access them can can't manage their securityrelated policies or their parent sql servers sql manage instance contributor manage sql instances and require network configuration configuration can't can't give access to others sql security manager manage the security related policies of sql servers databases but not access to sql servers and the last is sql server contributor manage sql servers databases but not access to them to the sql servers okay you have transparent data encryption td encrypts data at rest for microsoft databases in many cases it's already turned on for you dynamic data masking you can choose your database columns that will be masked obscured for specific users azure private links allows you to establish secure connections between azure resources so traffic remains within the azure network i should have put one underneath but this is generally if you want to also connect in a hybrid connection okay so there you go let's take a look at what a key value store is so a key value store is a data store that is really dumb but it's super super fast okay and so they'll lack features that you would normally see in relational databases like relationships indexes aggregation transactions all sorts of things but you know there is a tradeoff for that speed okay and so here is kind of a representation of a key value store which uh you have a key which is a unique you know key to identify the value and i'm representing the value as a bunch of ones and zeros because i want you to understand that there aren't really columns it's just key and value so the idea is that imagine that those ones and zeros actually represent a dictionary and that's usually what they are is it associative array hash dictionary underneath okay and so even though it looks like you know what i mean like if this was a relational database you know you could see these as kind of like columns and so if we kind of did that that's how a key value store can kind of mimic um you know a tabular data right but the thing is is that you know there is no consistency between the the rows hence it is schemaless but that's kind of a way to get tabular data from key values but due to their simple design they can scale well well beyond relational databases so relational databases it becomes very hard to shard them and do a bunch of other stuff with them but key value stores are super easy to scale but you know they come with a lot of extra engineering around them because of these missing features all right let's talk about document stores so document store is a no skill database that stores document as its primary data structure a document could be an xml but it's most commonly json or json like structure and documents stores are sub classes of key value stores so the components of a document store compared to relational database looks like this so the idea is that you have tables is now collections rows are documents columns or fields indexes are still the same name and when you do joins they're called embedding and linking so you know if a key value store can kind of store this why would you do it well there's just a lot more features around the documents itself and so you know how we saw key value store didn't have like it had like nothing like no functionality well document store brings a lot more of the functionality that you're used to in a relational database you know and so it makes things a little bit easier to work with okay all right let's take a quick look here at mongodb which is an open source document database which stores jsonlike documents and the primary data structure for mongodb is called a bson so a binary json is a subset of json so its data structure is very similar to json but it's designed to be both efficient and storage in both storage space and scan speed compared to json and bson has more data types than json has date times byte arrays regular expressions md5 binary data javascript code json's just strings integers and arrays it's very very simple but because it has all these new other data types and it's stored in this binary format it's not plain text it's actually binary data that's the one reason why it the storage space and the scan speed is so fast now if you did use javascript to perform an operation like say insert data this is what it would look like so you have kind of an idea that you're inserting items into a collection there okay just to list out some features of mongodb it supports searches against fields range queries regular expressions it supports primary and secondary indexes it's highly available it's it's high availability can be obtained via rep replica sets so replica to offload reads or access standby in case of failover momodube scales horizontally using sharding mongodb can run multiple servers via load balancing mongodb can be used as a file system which is called grid fs with with load balancing and data replication features over multiple machines uh for storing files mongodb provides three ways to perform aggregation uh grouping dat and aggregations just grouping data to return a query so aggregation pipeline map reduce single purpose aggregation mongodb supports fixed collections called capped collections i'm going to become claims to support multidocument asset transactions so mongodb when it first came out didn't do all this stuff and people complained about it i like it being very hard to scale but now it's a lot easier to use so you know mongodb is something that is uh more uh a more popular option nowadays than it was a few years ago so there you go all right let's take a look here at what a graph database is so graph database is a database composed of data structures that use vertices nodes or dots which form relationships to other vertices via edges arcs and lines so some use cases here fraud detection realtime recommendations engines master data management network and it operations identity and access management and there's a lot they're saying like it's really really good for that i am something i want to look into later traceability and manufacturing contact tracing data lineage for gdpr customer 360 degree analysis like for marketing product recommendations social media graphing and feature engineering for ml so let's just kind of break down you know the little components here so what you'd have is a node and a node can contain data properties and then through that it would have a relationship through an edge and that relationship can have a direction and also data properties on it and so it's a lot more um verbose like in ter than a relational database and also just how it can point to stuff so uh super useful uh for particular use cases let's take a look here at azure tinker pop which is a graph computing framework for both graph databases oltps and graph analytic systems olaps so tinkerpop enables developers to use a vendor agnostic distributed framework to traverse query many different graph systems they'll always say traverse because there's so many it's a tree right so there's a lot of databases that this thing connects to and so here they all are but the ones i want to indicate to you that are important is amazon neptune cosmodb hadoop via spark neo4j which is one of the most popular graphing databases orient db and titan okay so the thing is is that this isn't a graph database it is a basically adapter to other graph databases and ticker pop includes a graph traversal language called gremlin which is the single language that can be used for all these graph systems so let's talk about gremlin gremlin is a graph traversal language for apache tinker pop and so it looks like this and sometimes uh you know like even without tinker pop i think this is with cosmodb that they'll support this language by default so you don't necessarily need to have tinker pop to work with some databases but it's great to have that service if you if or like the framework if you need it so gremlin is is designed to write once and run anywhere wora gremlin traversal can be evaluated as a realtime query so lltb or a batch analytics query so over here it's just kind of showing you these are the oltps graph databases over here and then on the righthand side we have olaps okay and so gremlin hosted language embedding means you can use your favorite programming language when you write gremlin okay so there you go hey this is andrew brown from exam pro and we are looking at azure tables which is a type of storage for nosql key value data store within the azure storage accounts azure table stores nonrelational structured data with a schemaless design and there are two ways to interact with azure tables through the storage table storage api or microsoft azure storage explorer which i find is the easiest way to interact with it so just kind of looking at storage explorer there if you wanted to add an entry you'd have to provide a partition key which is a unique identify fire for the partition with a given table and a row key a unique identifier for an entity within a given partial a partition and so you have all your data types here so we see string boolean binary data type double uh guids 32 and 64. if we wanted a query you'd have to query along the partition and row key so you could also do some additional filtering here so just notice here that um you know you have your partition key you put your value like klingon and wharf and then this is not this is just additional properties you added a lot of time the way these key values work is that this will return the results like all the results and then server side and then clientside these will be filtered clientside i don't know if that's the case with azure table but that's generally how these things work and so there you go hey it's andrew brown from exam pro and we're looking at cosmodb which is a service for fully managed noaa school databases that are designed to scale and be highly performant so cosmodb supports different kinds of nosql database engines which you interact via an api so we have the coresql which is their document datastore their azure cosmodb api for mongodb their azure table and gremlin okay and this will be using uh probably tinker pop um so all of these nosql engines uh uh specify capacity so you can do provision throughput for pay for guarantee of capacity or serverless pay for what you use so if you are just playing around with the service you can go ahead and choose that serv serverless option and so a lot of times when people talk about cosmodb they're usually talking about coresql so if you say cosmodb it's usually document but understand that there's a bunch of stuff underneath it now if you want to start viewing data and making stuff and playing around with it you'd use the cosmo db explorer which is a web interface that you can find at cosmos.azure.com so after you made your cosm db cluster or container whatever they call it then you could go access your database so here we have the sql api and so that would be the document store and you could just see here that we have we've created a new item here for that data okay and so i just want to show you that if you drop down here you choose container or database so we create a new container um also if you are in um azure it looks like they just have it here under the data explorer tab so it's the same thing it's the cosmo db explorer just in line okay so you don't have to like go to that url you could just click into your um your it's called account cosmodb account and go to data explorer i just wanted to show you here like if you made a graph database that you can do everything through this explorer for all the different types the interface will change a bit so here we'd add a new vertex right and it's just slightly different okay all right so the thing about azure tables is that you can use it within either cosmodb okay or you can use it within account storage and the thing is is that um it's a really good comparison to look at these two things because this way we can really understand like how powerful cosmodb is all right so what we'll do is compare the two so over here when you have azure tables in account storage it's fast but it has no upper bounds of latency for azure cosmodb it's going to give you single digit millisecond latency for reason writes for throughputs it's variable throughput it's limited to 20 000 operations you get a guaranteed uh backed by an sla and no upper limits when you're using cosmo db for global distribution it's a single region and for cosmic db you have 30 plus regions for indexing you only get the primary index or partition and row no secondary indexes and then for cosmodb you get automatic and complete indexing in all properties no index management for green you get query execution uses index for primary key and scans otherwise and for uh cosmodb you get queries that can take advantage of automatic indexing on properties for fast query times for consistency we got strong with primary region and eventual with secondary regions and with uh cosmodb there's like five you know what i mean there's just uh the consistent levels are a lot more flexible okay for pricing it's consumption based and then for cosmodb you have consumption based or provision capacity for the slas it's 99.99 availability and here it it's backed by an sla but some conditions it does not apply okay so you know hopefully that shows you that cosmodb like is very performant is globally available single digit millisecond and i i really feel like this is to compete with um adabus um dynamodb because it sounds so similar to dynamodb but yeah there you go hey this is andrew brown from exam pro and we are on to the azure tables and cosmos db cheat sheet for the dp900 i want to point out something uh that i'm sure you already know about but in the course i spelt cosmos db without the s like everywhere and i'm not going to go back and fix that but i know i'm going to hear like never the end of it for like the next year okay so let's start at the top here azure tables it's a key value data store can be hosted on either azure storage account storage it is designed for a single region and single table can be hosted on cosmos db and when it's hosted here it's designed for scale across multiple regions cosmodb a fully managed nosql service that supports multiple nosql engines called apis why they didn't call them engines i don't know coresql api this is the default one it's a document database you can use sql to query documents and when people are talking about cosmodb that's what they're talking about the document database the default one okay graph apis a graph database you can use with gremlin to transfer traverse the nodes and edges mongodb api a mongodb database it is a document database tables ai is just as your table's key value but within cosmodb apache tinkerpop an open source framework to have an agnostic way to talk to many graph databases they probably won't ask you about tinker pop on the exam gremlin graph traversal language to traverse nodes and edges you definitely need to know what gremlin is and be used to seeing what it is like identify what it looks like mongodb an open source document database and the way it works is it has its own um data structure its document structure called bson which is binary json a storage and compute optimized version of json introduces new data types cosmo db explorer a web ui to view cosmos databases and there you go so what is apache hadoop well it's an open source framework for distributed processing of large data sets hadoop allows you to distribute large data sets across many servers and computing queries across many servers so htfs and mapreduce were the first features that were launched with hadoop way back in the day in version one and since then there's a lot more services now but the idea behind distributed processing is the idea is the idea is that your computer servers do not need to be on specialized hardware you can run them on common hardware and that's actually how google back in the day like uh there's a story over google they just kept on like adding like all random machines to build up their search engine didn't matter what it was and that eventually became the hadoop system so apache dupe framework has the following so hadoop common collections of common utilities and libraries that support other hadoop modules hadoop distributed file system a brazilian and redundant file storage distributed on clusters of common hardware hadoop mapreduce writes apps that can process multiterabyte data in parallel on large clusters of common hardware hbase a distributed scalable big data store yarn manage resources nodes containers and perform scheduling hive used for generating reports using sql pig a high level scripting language to write complex data transformations if they sound like they do the same thing they absolutely do they're just slightly different hadoop can integrate with many other open source projects via the hadoop components and we're going to see a lot of those open source things here in a moment let's take a look here at apache kafka which is an open source streaming platform to create high performance data pipeline streaming analytics data integration and mission critical applications absolutely the number one streaming service though it is open source so you have to find a way to uh to host it kafka was originally developed by linkedin and open source in 2011. kafka was written in scala and java so to use kafka you're going to be writing java code here is just kind of a diagram of how it works so you got producers consumers and topics so in kafka data is stored in partitions on a cluster which can span multiple machines which makes it distributed computing producers publish messages in a key and value format using kafka producer api and consumers can listen for messages and consume using the the cough consumer api messages are organized into topics producers will push messages to topics and consumers will listen on topics so there you go so azure hd insights is a managed service to run popular open source analytics services so here's kind of a graphic of how it works and hdinsight supports the following frameworks apache hadoop which is it which is what it is it's the entire system apache spark kafka storm hive hbase and lap and also you can run our workloads hd insights has broad range of scenarios such as etl data warehousing machine learning internet of things just because you can put in so many things here and so you know hdinsights is just a managed version of hadoop and it just makes it really easy to to do stuff so you can consume stuff run these in clusters so i think these are each called a cluster for whatever you're running and they can go out out to somewhere now once you launch hdinsights you can use the apache ambari which is an open source hadoop management web portal to provision manage and monitor your hadoop clusters and so when you create a cluster you're going to get one by default it's just under here where it says cluster dashboard and the idea here is you'll see all the types like htf mapreduce all the stuff here and it just makes it really easy to interact with hadoop okay hey this is andrew brown from exam pro and welcome to the hadoop cheat sheet for the dp900 let's jump into it so apache hadoop is an open source framework for distributed processing of large data sets and underneath it has the hadoop distributed file system hdfs a resilient and redundant file storage that's distributed on clusters of common hardware you have mapreduce which writes apps that can uh process multiterabytes data in parallel on large clusters of common hardware hbase a distributed scalable big data store yarn managed resources nodes containers perform scheduling hive used for generating reports using an esco language pig a high level scripting language to write complex data transformations apache spark can perform is 100 times faster in memory and 10 times faster than disk than hadoop supports etl streaming and ml flows you can run that on hadoop i didn't just did not put it under the um i debated whether i should put it in in line or there but that's where i put it apache uh kafka a streaming pipeline analytics service hd insights a managed service to run popular open source analytics services it is fully managed hadoop system so it's just hadoop but managed by azure there so there you go that's the chi chi apache spark is an open source unified analytics engine for big data and machine learning and spark lets you run workloads much much faster than hadoop though you can run it in the hadoop system okay so 100 times faster in memory 10 times faster than disk and which is why spark is being described as lightning fast so when it says hadoop you know it's talking about hive and pig and the other things that usually come along with hadoop and so apache spark is a collection of libraries that work well together to form an analytics ecosystem so we have the spark core this is the underlying engine and api the api supports the following programming languages r sql python scala and java you have spark sql which introduces a data structure called a data frame not the same thing as a pandas data frame but they're just called the same thing okay which can be used with uh dsls to work with structures and semistructured data you have spark streaming allows spark to ingest data from many streaming services so htfs flume kafka twitter kinesis you have graph x so distributed graph processing framework you have machine learning the mlib library and this is a distributed machine learning framework with common machine learning statistical algorithms the way you are going to interact with spark is through resilient distributed data set rdd which is a dsl to execute various parallel operations on the epoxy spark cluster so here are some common functions map filter distinct count min max mean paralyzed you get the idea and here's an example of rdd api so just notice here um i think basically all of these are are those functions right so that just makes it really easy to transform and work with data okay so databricks is a software company specializing in providing fully managed apache spark clusters and the company founders were the creators of the apache spark delta lake and ml flow open source projects and databricks has two main offerings the database platform and so dablex cloud based spark platform with an easy to use web ui where you can launch fully managed spark clusters launch notebooks to write code and interact with spark create workspaces to collaborate with team members and rolebased access controls create jobs for etl or data analysis tasks that run immediately or on schedule create ml workflows and is available on all main cloud service providers aws azure and gcp they also have the database community edition which is a free version of the databricks platform for educational use create a free michael cluster that terminates after two hours when idle no workspace jobs or rbac so really just a subset of the one above so azure data bricks is a partnership between microsoft and databricks to offer the database platform within the azure portal running on azure compute services and it offers two environments workspaces and so basically this is just the azure database platform with integrations to azure data related services for building big pipelines so if you need to do batching you can use azure data factory streaming apache kafka event hub iot storage azure blob storage azure data like storage the other side of it is azure databricks sql analytics run sql queries on your data lake create multiple visualization types to explore your query results build and share your dashboards now if you want to launch a databricks workspace it's really easy all you got to do is create a workspace and choose your plan launch a workspace use sso to connect to it and start your database platform so if you go up here and this is in the azure portal with azure databricks we'll just choose which one we want and then we launch the workspace it'll sson and then you're in there okay so um you know basically if you launch it's not going to cost you any money so if you want to play around with it you can do that or if you're really antsy you can just go to the databricks website and try the community edition because there's risk there's no risk of spending any money when using that okay hey this is andrew brown from exam pro we're taking a look here at the apache spark and data bricks cheat sheet so let's jump into it so apache spark is an open source unified analytics engine for big data and machine learning it's a hundred times faster in memory than hadoop ten times faster than disk than hadoop can perform etl so batch streaming and ml workloads uh for the apache ecosystem it's composed of spark core which is the underlying engine api spark sql you use sql it also has a new data structure called a data frame to work with data spark streaming which is a way to ingest data from many streaming services graph x distributed graph processing framework mlib a distributed machine learning framework there's also rdd it's a domain specific dsl to execute various parallel operations on apache spark clusters then we're talking about data bricks here it's a software company specializing at providing fully managed apache spark clusters we have azure databricks a partnership between microsoft and database to offer the the database platform with within the azure portal running on azure compute services azure databricks offers two environments we have the databricks workspace so this is the databricks platform with integrations to azure data related services for building data pipelines if you went to azure or sorry if you went to databricks website and signed up it's the same portal okay azure database sql analytics run your query on your data lake and i believe that this is basically um azure synapse analytics so that's the engine that's in there okay take a look here at sql management studio also known as ssms which is an ide for managing any sql infrastructure if you take a good look at the screenshot you can see that allows us to work with databases write sql statements and that's pretty much it so access can access configure manage administer and develop all components of sql server azure sql database azure synapse analytics and it has a few components in it so we have object explorer view and manage all objects in one or more instances of sql server template explorer build and manage files for boilerplate text that you can use to speed the development of queries and scripts and this is deprecated but you might hear about it so i just mentioned it here and this is build projects used to manage administration items such as scripts and queries so not a complicated service but uh yeah there you go all right let's take a look here at server data tools ssdt which transforms data development by introducing ubiquitous declarative models that span all phases of development phases within visual studio so this is a visual studio tool like azure basically has like a tool for everything like it'll be the same tool but it will be repurposed for their different other tooling products and this one's for visual studio so i don't have a great internal screenshot but you can see that it works with um it has a few different components inside there and analysis reporting integrations so it uses sdt transact so tsql to build debug maintain refactor databases also provides a table designer for creating editing tables in either database projects connected database instances be able to view control data loaded files easy to publish to sql database or sql server has an object explorer offers a view of your database similar to sql ssm mess allows you to lightly duty database administration design work easily create edit rename delete tables stored procedures functions edit table data compare schemas execute queries by using contextual menu rights i don't think this is on the exam but because it's just one of these many services i figured we'd throw it in here just so that if you do see this initialism you know what it's for so azure data studio is a crossplatform database tool for professionals using onpremise and any data platforms for windows mac os and linux and here is a screenshot and if you recognize it it looks just like visual studio code right if you open up the extensions that's what you would see so query design and manage your database and data warehouses data azure data studio offers a modern experience with intelligence very similar to experience to visual studio code code snippets source control integration integral terminal builtin charting customizable dashboards jupiter notebooks connected to your data set and it has a marketplace of free extension so some i know that seemed very useful was sql database inspector so a great way of looking at your data cousteau extension for azure data studio postgres extension and many many many more so there you go so azure data factory is a managed service for etl elts and data integration create data driven workflows for orchestrating data movement and transferring data at scale and so if you see the image it makes it pretty clear what you can do with it there's a bit of a pipeline there so create pipelines to schedule datadriven workflows build complex etl processes that transfer data visually with data flows using compute services such as hdinsights hadoop data bricks sql database publish and transform data to data stores such as azure synapse analytics raw data can be organized into meaningful data stores and data lakes let's break down the components here so we just have a better idea how this thing works so pipelines is a logical grouping of activities that perform units of work activities is a processing step in the pipeline data sets or data structures within the data store link services define connections information for data sources to connect to the data factory data flows are logic to determine how data moves through the pipeline transform integration runtimes compute infrastructure used by data factory control flows orchestration of pipeline activities that include chaining activities and a sequence of branching you should know what control flows are and data flows are and there you go so microsoft sql server integration services ssis is a platform for building enterprise level data integrations data transformation solutions ssis can be used to automate sql database servers it can be used as an integration runtime for azure data factory you can perform the following tasks copy files download files loading data within data warehouses cleansing mining managing sql server objects managing sql server data you can perform etl for a variety of sources xml flat files relational data sources and ssis has builtin tasks and transformation graphical tools for building packages integration service catalogs databases where you store run and manage your packages it comes with a graphical interface to transform so you don't have to write any code and the designer is a graphical tool that you can use to create and maintain integration services packages just to show you what it looks like here it is so it allows you to drag out transformations and design different kinds of flows and control flows and data flows notice it has control flows and data flows similar to azure data factory uh but you can see this is the tool kind of azure data factory is like kind of like the web version of this but yeah there you go hey this is andrew brown from exam pro and we're taking a look here at etl and sql tools cheat sheet so let's jump into it the azure data factory is a managed service for etl elt and data integration jobs you can create data driven workflows for orchestrating data movement and transforming data at scale build elt pipelines visually without writing any code via web interfaces you have ssis the s the sql server integration services a platform for building enterprise level integration data flow solutions a low code tool for building etl pipelines very similar to data factory but existed 15 years prior mostly focused around sql no surprise there integrates with azure data factory so you can extend it to nonrelational database workloads okay we have azure data studio an id similar to a very similar to visual studio code those crossplatform and works with sql and nonrelational database data has many many many many extensions you have sql server management studio smss an id for managing any sql infrastructure that only works for windows more mature than data studio very similar in terms of parallel so we have like the modern version that's the web mode over this web that's both s like relational and nonrelational and then the older one that's mature but it is a a windows app and does a lot of interesting stuff for sql you have sql server data tools ssdt this is a visual studio extension to work and design visually sql databases within visual studio so there you go we're all done hey this is andrew brown from exam pro and i wanted to show you how to go ahead and install power bi so what i've done is i'm on my windows machine and i pulled up the microsoft store and all i did was go in the top right corner and type in power bi this is the easiest way to install power bi you can download the application but i find this is just the best way to do it and we'll hit get or free to get started here um and it should now start downloading so we go in the top right corner you'll notice here that it is now downloading okay so we just have to wait for that to finish and i'll see you back here in a moment all right so after waiting a short while here it looks like power bi desktop is finished downloading what we can do is click into power bi and we can go ahead and launch this service and so we'll let that go ahead and launch and the initial time you launch it it does take a little bit of time there so i stopped and restarted the video but here you can see we are now inside of power bi uh and if we wanted to get started there are a few ways we can go ahead and do that they have some nice tutorials here's in the bottom right corner but what i want to do is just get something open so we can start looking at something so if you were to type in sample data sets for power bi microsoft has this nice page where you have a bunch of downloadable reports or things we can work with and so maybe we should give the first one a go so i'm going to go ahead and download this one here and if we go down below so we'll just take a look here so i think we just have to go ahead and click here and uh from here i just want to hit that download button so i'm just looking for it here it is okay great and so i'll go ahead and download that file notice that it's it's the power bi x that is the file that we are looking for and we'll go ahead and open this file so after any short while there the file did open it does seem to take a while for things to open but i'll just hit the x there they have a new modeling mode we'll just say not yet and so down below you can see i have a bunch of tabs here and this allows us to go ahead and explore this data but you can see things are extremely interactive here in power bi desktop so you can do all sorts of things uh here okay hey this is andrew brown from exam pro and in this follow along we're going to be looking all at azure sql so what i want you to do is go to the type here and type in sql and you'll notice you'll get the options like the databases managed instances and virtual machines those are the three under the azure sql tier but to make our lives a little bit easier we'll type in azure sql i find this really confusing and so i'm hoping that i can show you the easiest way to find it but if you go to azure sql here and add it this will actually now give you the option to choose between the three so you can make an informed decision and we'll work our way from the right to the left so sql virtual machines is great when you want os level access to the virtual machine uh when you're doing a lift and shift that means you're moving your sql server from onprem to the cloud because you want to take advantage of the cloud and also if you want to bring your own license to take advantage of the azure hybrid benefit so if you drop down here you'll notice that you'll see byol and if we expand here it'll tell you all the details here as to why you'd want to use that i'm not going to spin one up because i'm going to show you the price okay so we go over to the price and we scroll on down we go to um page you go you're going to notice whether it's on or off with high uh hybrid benefit you're paying about a dollar to two per hour and so you know it's you're not going to learn that much by spending one up because we are going to spin up an sql server but at this stage it's not going to really matter it's they're all very similar so you know i just i don't want to spend money if you're a student there let's look at sql managed instances we'll expand that again this is for lift and shift meaning you're moving from onpremise to the cloud but here you don't have access to the os level you're not bringing your own license however the upside is it's a fully managed service meaning that it can scale very well it's going to have great backups and things like that and it comes in two flavors we have single instance in single instance as your arc now i didn't cover this in the course but azure arc allows you to um extend your i think it's the control plane it's either data plane or control plane but the control plane uh two different cloud providers and also to your on premise so the idea here is like you can use sql manage instances and ha like you can have all the benefits of here and launch the instance within your own infrastructure so um you know if you really need to keep that server onprem then you can do that with azure arc but here's a single instance um you know it's just whatever there and so i'll show you the pricing on that one and so we'll go to manage instances we'll scroll on down here and the lows cost one here for pays you goes a dollar so it's still kind of expensive if you're a student again i don't know if it has a free tier and i i wouldn't imagine it would because that's not really a free tier kind of product but you know i just want you to know that that option's there we'll make our way now to uh sql databases and so we have a few options here we have single database elastic pool and database server okay so single database is you have it's a great fit for modern uh cloud cloudborne applications that need fully managed database with predictable performance so it has hyperscale i mean that it can scale up to 100 terabytes it has serverless compute it's easy to manage you have elastic pools we covered this in in the lecture content this is where you might be like a multitenant sas and you have multiple databases one per customer and you want to save class so you actually have them all running on the same server and you have database server and those so this is just to manage a group of single databases and elastic pools it's a way of grouping stuff together and so today what i'm going to do is launch myself a single database okay and what we'll do is we'll just say azure sql or we'll say dp900 dp900 azure sql just so i can see what i'm doing we'll go down below here and i'm just going to call this um my azure uh sql we will drop down here and select a server we'll have to create a new server so this will be my azure sql server and you might have to work to choose a name i'm going to do one two three four five six there we go okay for the login i'm going to do azure user for the password i'm going to do capital t testing one two three capital t testing one two three it's really nice sometimes they make you do four five six and then an exclamation mark but we have the shorter password there we're going to say okay do we want to use elastic pool no i don't think we need that today but let's expand the general purpose here i want to make sure that we have the lowest cost possible so notice to the right here it's 381 dollars if you're provision it you could also do serverless i've actually never used the serverless feature here but this would be really great if you're just trying to learn because it's going to be based on usage here i've definitely used serverless services just not this one in particular for sql but what i'm going to do is stick with provision i'm going to look for a basic plan and what we'll do is scroll this all the way to the left here and you know this is just if we forget to turn off our database so now it's five five dollars a a month and probably you know per hour it's you know fractions of a penny so we'll go here and hit apply and we'll scroll down as we have some options georedundant zone redundant locally redundant honestly we only need locally redundant so i'm going to go ahead and select that obviously you know if you're doing a production database you want georedundant because you're going to have um uh you know better like a better redundancy in terms of like if a region goes down you'll still have your database okay we're the next step to networking and here we either have public or private endpoint this is really important for uh security purposes right and we do cover this in the lecture content as well and so i think it would be a great idea to have that there let's go read about firewall rules allow services and resources to access server yes to communicate from all resources inside the azure boundary that may or may not be private subscription so allow azure services and resources to access this service yes uh add current ip address yeah because that's me so i'm going to say yes to both of those it's going to set up our firewall rules which is really nice we'll go next to security here we have azure defender for sql if you want that additional protection you turn that on it costs 15 a month and we'll go to the next tab here here we could um i guess let's see for backup so start with a blank database restore from a backup or select a sample i'll select a sample because that's nice to have some data i didn't know they had that there and so we'll go to next tags i didn't really cover tags in the course but they're actually really important especially when we're talking about data because you do want to catalog and and categorize all your stuff so tagging is very simple what we do is we have a name so we could say um workload and i'll just say like uh learning right but allows us to filter and find resources later down the road we'll go ahead and hit review create we can review all of our stuff see that the price is okay with us we'll go ahead and create and again we're not going to get billed five dollars by pressing that button because it's again build per hour um so you know we will be build uh some kind of sense i don't know if azure sql has a free tier probably does uh free tier let's see here free tier i don't see it but i'm not too worried about that um so you know again if you're really worried about any spend just uh watch and don't don't do it but i do recommend if you do have the pennies to go launch the stuff of yourself because you'll learn a lot more that way okay so i'll see you back here when this is done uh provisioning all right so after a short little wait there our server is ready so i'm gonna go ahead and hit click go to resources and what we can do on the left hand side is just do a little exploration so look at the top here where it says set server firewall remember from our our security part where if we had set um some default rules well here they are okay so here is my ip address that is allowing me to connect to it notice we have that connection policy where it's default proxy or redirect you know things like that so we'll go back uh one step here and i just want to show you down below that there are some features so notice that tde transparent data encryption is already turned on uh there's your azure defender for sql we have our dynamic data masking so on the left hand side we'll scroll on down and just take a look at some of the security so we go to tde and you can see it's already turned on for us which is great i'm not going to click on the security center because it's a pain to exit out but let's look at dynamic data masking so here what it's doing is it's suggesting uh fields that should be mass and we go ahead and just click add and add and then we can mask these domains so i just click that there and so now these domains are masked and it shows you a bit of the mass function as to what it would do to to mask it so it's as easy as that um data discovery classification this will be useful it's in preview right now it's not showing any information as of yet um but the great thing about this is that it could discover some information that you might care about that needs to be that's considered sensitive right so you might say this is confidential gdpr address here um information types let's see what we can do here so we'll say customer and we'll say credit card and we'll say highly confidential and i'll say add classification i haven't used the service before because i had never seen it before but um it looks like that's a great way to um you know keep track of that kind of stuff so that's cool uh auditing i don't think i ever do auditing but um you can turn azure sql auditing it will track logs and stuff like that so the thing is is that if someone were to get into your database and try to make changes or to corrupt your data stuff like that you'd want to see who was actually accessing those queries so definitely if you care about security you'd absolutely want to turn that on and then we'll just scroll back to the top here back go to our overview i want to show you that uh through connect we can connect to data studio or visual studio if you click this it's going to tell you you need to install it i don't have azure data yet so i'm going to go ahead and install it on this computer and we'll go down below and i'm on windows machine today so this is for windows mac and linux and so i will go ahead and use the user installer i guess and i will download that it's 100 megabytes as that's going we'll make our way back to our main page here and i also want you to notice there's power bi so we will go and download this uh we can go download actually right now so if we were to click on um uh is it just get started yeah it's gonna download this my azure pb ids and we'll be able to open that file in power bi and you know we did power bi earlier in earlier follow along so it'll be easy to open i want you to know if you don't have time to do all the stuff because we're going to have this database up and running for a little a little longer than you expect so if you if you don't feel like you can do this all in one setting you can delete this database and always go ahead and spin it up again when you're ready to continue but on the left hand side they have a query editor here and we're just going to go log in so i said my password was testing with capital t one two three we'll hit okay and now that we're in i can see my tables on the lefthand side here uh we got views and stored procedures but let's say we wanted to execute a query so i'll just expand this i've never seen this database before so i'm just going to go with it but we'll type in select um oops and it's going to be very aggressive about autocompletes you got to be really careful here and i'm just going to say select asterisk which means all fields and we want to select it from sales lt dot customer and notice when it auto completes it has square back brackets around it that is just the thing that tsql does let's say you had a field called first name and you wanted to have it with a space and that's what brackets that you do so they're just filling that in case you have reserved words or um you know spaces but we don't actually have any spaces here so we just remove it like that sales lte is kind of like the container for all these um all these tables it's just a naming convention they have for their table names and so we'll go here select all and hit run and we should get some data back and so let's just perform a very simple join so a join will happen when you actually have a foreign key you can join on so here you can see this is the primary key customer doesn't seem to have a whole lot of information so let's go into products and this should have yeah here you see it says product category that is a foreign key so we can use that so i'm gonna go here and type in sales lt and we'll do product okay and then we'll do where or sorry we'll do a left join so we'll say left join and i want to left join on this product id so it would be left join with sales lt dot product uh category and we would say on and so we would say sales lt dot product and it's going to be uh what was it called it is product category okay so we say product category id equals and we'd say sales lt dot product category category dot id so we're saying we want to join where these match on here assuming joins working like every other language which it should and so i'm going to be very particular and just pull out some particular keys that i want so here i'm going to say i want the sales lt.product oops and i wonder if i can do this as on this like as prod or for p here because i'm going to go crazy if i keep on typing that a so we'll just try that um i think it should work um if we do that because then it'll just be easy to read here it's like a way of aliasing it sometimes you can don't even need to put the word as you just put p like this and for sales product we could do that as well so i took out the on so i'll just say like pc and we'll see if it lets us do that okay and so here we have product id and i want to get the product name looks like it's working and we want the category name i think so we'll drop this down i assume it must have a category it's taking it's time to load here but we'll just say pc.name as product name as name as id and we'll see if that lets us run that it's currently not available if the problem persists contact customer supports we'll hit run again i'm not sure why our server's having problems i didn't i didn't turn it off did i let's go back here and take a look it's online yeah it's in good shape here so we should be able to query it maybe just lost our connection so what i'm going to do is just click back here i'll type in testing123 and i'll go ahead and close that there we'll make our way back go back in the query editor there we go i'm not sure i was giving us trouble i threw an error here um invalid id oh yeah it's not called that it's called uh product id there we go we'll run that invalid column name product id must be naming these wrong here capital d here uh is it the same thing down below here normally you know like when i name these i would have them all lowercase underscore but you know in the azure world they they like to do this quite a bit so we will try this um product id so it doesn't oh right it wouldn't be uh this would be product category sorry so this is product category id and yeah that should match there we go so we just did a join between the two tables so we have the name and the product name which is uh pretty good there our azure data studio looks like it's done downloading so i'm going to go ahead and install that so what i'm all i'm doing here is i'm just double clicking the file to open it up and we'll just give that there a second oh it was here the entire time we'll say i accept next next next uh we'll get a desktop icon sometimes i can't find them later registered data suit is an editor for sporto files new i like other other programs for that and we'll go ahead and install that doesn't take too long and we will launch the data studio and yeah if you've ever used visual studio code it's gonna look very familiar to you see um so don't like the light mode rather have it dark appearance can we change that to dark zen mode oh no oh no what did i do uh i don't know how to get out of that okay great maybe not do zen mode i just wanted to make it a dark mode uh not sure how to change that i guess it's not a big deal but what we'll do is we'll go over here the top and we can go ahead and add a new connection and so that's an mssql server what we'll need to do is i want to keep this open because um it's very hard to find the tables here but what we'll go here is go back and we'll get the server name so we'll copy that and then within visual azure data studio we'll paste in our server we have three options windows authentication sql login azure active directory i'm going to use sql login the password is azure user username is azure user the password is capital t testing 123. what i like will tell to remember is that if you drop it down it'll see if the database is there and there it is so my azure sql that's what we called it i'm going to go ahead and connect and if i expand it i should be able to see tables good and stuff like that very good a lot easier to work with than that preview editor there so what we can do i'm just going to go over here and grab our query copy this and what we'll do up here is we'll make a new query so i think you uh it's over here new query maybe it's over here as well let me see now i guess we just do it from there i'm just trying to figure out what's easiest way to go up there and do a new query i'm going to go ahead and paste that in notice we can change our connection i don't think a database is selected right now so we'll go ahead and connect to this database here we'll go ahead and run that and it's the same data okay uh there's the thing called notebooks which is kind of cool so i can go ahead and make a new notebook i think it's using jupyter notebooks underneath um but what we'll do can we save that query we sure can i'm not going to save that query today because i do not care but yeah it is jupiter notebooks great so what we'll do is go file new notebook and here i can write some text so this will return back both products and product categories okay and then what i can do here is add another piece of code we'll paste that in oops and i can even say like limit so i could say like limit to 20 or maybe 10. let me go ahead and run that and we'll just connect on the server here just needs to connect okay and we spelt limit wrong so we'll just spell that again there limit i don't think i'm spelling it wrong but i'll just take it out because i don't feel like goofing around with that today and so here we have our records okay it should just be limit right lim maybe it's not limited tsql give me two seconds tsql limit um oh they don't have a limit function i did not know that okay well that's kind of interesting they have things like where row is equal to like i'm just looking at it over here right so c areas use the top record so select top five you know top ten let me try that there we go so i guess it's just not limit so we have that in there which is kind of nice um and so let's say we wanted to narrow that down to something in particular uh if there's a way we could browse the data oops i mean that by accident but what we could do here is just take a look i wanted to see the um i'll do it over here i want to see the product category id because then maybe we could just do a like a where statement really quickly and so these are all mountain bikes when it's five so what we'll do is we'll make a where statement and we will say when pc product category id is equal to five uh and then we'll hit run okay and then up here we'll just say that you know show us oops uh can i edit this yup show us the top 10 mountain bikes it's not really saying like it's not this is not very useful information because it's not saying uh why it's the top ten it's just listing it there's no ordering here so maybe that would be also a good idea here so let's just take a look at our data structure here i mean we could do it in in here as well if we want to uh explore that way but i think it's a bit easier to do it here so let's look at the product um and see if there's like any definitive information that makes it interesting like list price so maybe that's something we could return here so we'd say um p dot list price as price and what we could do is order that so we'd say order by p dot list price descending there we go when you have to do that tutorial it's it's easier to forget so now it's listing it from descending prices uh that amount isn't very human readable i don't know if it's like 3000 whatever i wonder if we could use a function to do that if we're using ms or sorry like postgres it would just be like floor so maybe i could like floor the result here or round it let's just see if i can do that sometimes it's like round yeah there it is those are like builtin functions to most sql languages here and uh it needs two arguments what's the second oh probably what to round it to so we'll probably put a zero here because it's gonna be like rounded to what maybe it needs to be a one other languages you just put round in so i don't know what that is so what we'll do is look that up so we'll say tsql round okay just doing that off screen here and so it says number expression is an expression of the exact numeric approximation is the precision in which you want it to be if i can get a nice example so uh round's a number i just don't want it with zeros um so that will round it to two decimal points and then we have negative two so we'll go up the top here is the precision to numeric to be rounded length must be an expression of tiny intent when the length is a positive number is rounded to the number of decimal points specified with the length with a number uh it's on the left side of the decimal point oh okay that's kind of interesting so what we want to do is probably round the left or right side so either give it two there can't convert data type v and char oh its name you know what you're probably watching me be like andrew what the heck are you doing so it's supposed to be up here so we do round okay and i'll just say two and that should round it to two decimal points which is fine but i don't i really don't want anything there so i'll say zero and you know i probably would want that just as an integer there's probably a way to cast it so maybe there's like a cast function or two int so i don't see that so cast uh float to ant uh tsql yeah it's called cast that's what i thought and so it's as simple as that so what we can do and we probably don't need to round it because the cast will probably uh round it itself so we can say cast and say as int very good that looks really nice let's say we wanted to like concat something onto that uh again i don't know how to do concat and tsql so we'll look it up so we'll say tsql concat strings in postgres it's just a pipe oh it's a plus wow we get a plus i love that you don't usually get a plus so what i'll do here is i'll cast this as an end and then i'll cast this as i don't know what it is in this as varchar bar chart yes can i do that first it's still a number great and now that it is a a string we can do maybe string dollar sign like that we'll say run we'll let me do that yes very good and we'll give it also the usd there he goes that's kind of nice so now we kind of have a human readable human readable thing there okay so what i'll do is i'll just copy that over put it in my notebook and we'll just have that there i'll move that off screen and you know that's pretty much how you would create data with sql now now that we have a mysql database we can actually consume it with uh with a lot of other services like power bi or maybe some other services that we can think of but yeah how about we do that next let's try to load this data into power bi all right so now let's take a look at using power bi with our ms sql server here so in the bottom left corner you saw that i downloaded that file earlier so i'm going to go ahead and open that up if you don't have it just make your way over to power bi and download it again and all it's going to do is establish a connection to the data source now we didn't really need to download this file to do it it's not that hard to establish our own connection uh but it does save us one little step here okay so it's gonna load here just give it a moment it's gonna pop this up if you don't see this what you can do i'm just gonna go ahead and close this to show you how to make a manual connection so i'm going to go over to server name copy that go back and if we go into now you have this button here but we'll just go get data more so you have a more predictable way of loading data in and we'll go to database here we can go to azure and so we have azure sql database um which i think that's what we're using and so we'll go ahead and hit connect and so we'll put the server name in there and the server name was my azure sql we'll hit ok we'll give it a moment to connect and we'll get back to the same screen so here we can import the data that we want we'll say product product category um customer right customer address address the more we bring in the more it's going to be so you know what i'm just going to narrow it down to just product product category and customer you can transfer the data which is really nice i'm not going to get into that because that's a whole other thing but we'll go ahead and hit load and what this is going to do is it's going to start to import this data and we'll give it a moment so it's going to load the data in and then it's also going to detect relationships and do our data modeling so the data is now in if we go to here we'll be able to see these tables and explore that data in its raw format if we go over to this tab here we can see that it's detected a relationship the more tables the more relationships it will auto detect so we didn't want to make that too hard i want to go back over here to our report and i just want to show you how you can visualize some information so one of the easiest ones to do off the bat is card so i'm looking for card it's this one nope slicer card okay and so i'm just dragging this one out just there we go i just clicked it whatever you have to do to get there and this is great if you have a single field so i'm going to go drop this down and i'm going to find product by list price so i'm going to drag that onto the field here notice it went into there it only takes one input for the fields and there's my price if i want to change the look of it and go down here and there's all sorts of things we can change here like shadow you know other things that you can do for fun but you know there's a little bit of styling there and let's say i wanted to see that as a table so what i could do is just change that to table and now it's a table uh table display but this information is not very useful so what i want to do is drop this down and then drag out the name of the product category remember that these have a relationship it's established here so it's going to know how to slice that information to make it useful and so now we have a breakdown of price and name based on category right and i mean that's category which was a i don't know how to rename that very well yeah for this visual here we'll say uh you know product or so this would be category and for here we'll just say we'll just rename that to make it a bit easier we'll just say price there we go now let's say we want that as a graph so we just change that over to a graph so we can kind of see the difference in prices all sorts of things we just click around here to have some fun okay bar chart is going to be the most useful one i believe so yeah i don't know if there's anything else that's really fun here we have another breakdown kind of see like uh the cost there of all the stuff that they're selling okay i think that has the highest price is the bikes right because so the volume of sales is just the cost of each item right that they're selling uh you know so that's how you'd use power bi desktop with um uh with your sql server so yeah there we go so another thing that we can do since we have power bi and we connected it to our mysql server or our sql server no problem is we probably should try to publish it to the power bi service so that we can see how to make dashboards so what i'm going to do is go up the top here and type in app.powerbi.com if this is the first time you've ever gone here it's going to ask you to make an account even even if you have a microsoft account it's just another way of authorizing that there and so here you can see uh we can explore all sorts of data sets if we click into here these are just dashboards right and we'll go in here we can see all this kind of information which is uh really nice and we can go here and just select some information you can see it's very interactive which is really nice um and you know we can go ahead and create our own uh kind of things here if we were to publish a data center or do whatever but uh you know what i want to do is uh to connect some more data sources download power bi desktop we already have that installed so what i want to do is just publish and get something into here so i'm going to reopen up our file we had there earlier and establish a connection to our ms sql server we'll make a little port and we'll publish it and see how we can access it through the power bi service here okay so we'll give it a moment and all i want is product and product category once it decides to load here so we want product and product category and i will say load we'll give it a moment and now that is loaded what we'll do is again make ourselves a visual so i will drag out a card like we did last time and we'll go down to product here i will try price uh it's better probably better if it's a table it's a little bit more useful i think and we'll go here to product category and drag out the name again we'll rename these we'll just say rename price rename category and what we'll do there's a publish bubble go ahead and hit our publish button so do you want to save changes yes uh and we'll just save this whatever we want to call we'll just say my power bi report okay and uh so we will just have to put our email in here okay oops it's thinking we'll give it a second there we go so it's asking me to log into my account and we'll say my workspace sounds great to me and we'll give it a moment to publish that report great that report has now been published we'll make our way back to power bi and so it should be under our workspaces right all right so what we'll do is make our way over to our workspace on the lefthand side and you'll have to click on here it's not very clear and we have a data center report if we click into a report we have this error about missing credentials so what we'll do is make our way over to our data set and we'll just go to uh settings here and we might need to provide some credentials here again so let's edit the credentials and we'll just say azure user capital t testing one two three uh privacy level settings is private we'll sign in and you know what the pro i wonder if it'll give us access to user updates etc because it's from a a uh you know like remember how we had that firewall rule so it could be the firewall rules that is preventing it so by clicking on my report yeah it's still a problem so what i'll do is go over to my azure server set the firewall or rules here allow azure services to access this resource yes um so it's denying the rules so we'll have to figure that out there okay so it looks like actually let's go in the dataset and updating the credentials did work that message was just a bit of a false flag there i mean like it did need to reestablish connection but uh uh you know i thought i thought maybe i had to go digging around in our firewall settings but it since we have this turned on you know powerbi should be able to access it and it can so we have uh we're looking at a report right now within power bi but let's say we want to make a dashboard well how would you do that well you go ahead and you just go and hit the pin we'll say my dashboard we'll go ahead and pin that and uh i mean if we wanted to create a mobile layout we could do that okay so there it is for our mobile layout and we can go back to our web layout right and from there you know now that we have our dashboard we can go ahead and just you know share that with our team or put it in chat you know whatever it is but this is where the point where you would then hit the power bi pro where you have to upgrade but uh yeah it's as simple as that so yeah there you go so we saw azure data studio as a means to connector database but there's another tool which is um at sql server management studio i figured we should give that a go so what i'll do is go ahead and download that and it's 635 megabytes so you have to decide whether you want to download that once that's downloaded what we'll do is go ahead and install and give it uh give it a look okay all right so after a very long wait sss ms is done downloading and even in here it says that it downloads azure data studio with it so they really really want to use azure data studio which we've already used but you know it doesn't hurt to open up ssmss and give it a go if you can use it and so i'm just going to go ahead and install that there okay and it sounds great it's going to go there we go couldn't tell i hit install or not all right so waiting a little bit of time here that this finally installs we'll go ahead and close that and now what we'll do is go actually ahead and open that program ms i'm not sure where it installs into so i'll see you back here in a moment all right so i'm back and i found it it's under c program files microsoft sql server management studio 18 common seven id now uh you know your start menu you should be able to type in ssms and find it for whatever reason on my computer nothing ever shows up under the start menu and i can't be bothered to fix it but it's good to know where it actually installs to so now we know where it is i just double clicked it and now it is open and so here we can see a bunch of stuff and this will be a way for us to load this so what we'll need is our server name as per usual so i'm going to go back to azure here and i'm going to grab the server name we'll go back paste that in we'll choose sql server authentication we'll type in azure user we'll type in testing with a capital t one two three connect honestly i like this a lot better as your data studio i don't know like real apps just feel feel like they work a lot better you know and so we can go into our database here look at some tables okay give it a moment to load and we can go into like customer or maybe product okay uh there should be a way to visualize whoops like that so maybe go design mode here give it a second to open up yeah so we can kind of see like what columns there are and stuff in here which is really great but what i want to do is create a new query and i just want to show you that you can query it in here too just query it everywhere right and so uh from our azure data studio that i still have open from before what we can do is grab ourselves uh this code here and paste it on in and go ahead and execute that and we can see we get the results so you know this is a very very very very powerful tool um you know and it's just i could do spend all day trying to show you how to use this uh tool here but um yeah i mean this is all you need to know so yeah there you go so that is ssms and i'll just go ahead and close that and there we go hey it's andrew brown from exam pro and what we're going to look at here is storage accounts and specifically blob storage tables and maybe azure files and so what i want you to do is go all the way to the top here and by the way i'm just carrying this over from my mysql or sql tutorial so something to do while i'm waiting for this thing to install but what we'll do is type in uh account or storage account so we'll go here and oh yeah just hit cancel there that's for my other tutorial we'll hit add and we'll create a storage account okay and what we'll do is i'll say dp 900 storage it will say ok and we'll name this storage account we'll say we'll call it my storage account notice we have the option between oops one two three four five six there we go oh somebody really really wants uh to have the same name as me nine eight seven six five four three two one because those are unique names right um oh it doesn't like that uh can only be contains must be three characters is now too long okay there we go and so they're uniquely identifiable across all accounts that kind of gets annoying between standard and premium if you choose it you get a few different options so with premium you get block blobs file shares page blob so we're going to stick with standard to save some money we change the redundancy we'll stick with local redundant zones because that's the most cost effective go over to advance see what else we want here um here we have the option to turn on uh uh higher or higher cool name space that's if we're doing data like gen storage 2 which we're not doing at this point in time so i'll do is go ahead and hit review and create and we'll scroll down and say create okay and we'll just wait for that to create the storage account i'll see you back here in a moment all right so we waited a little bit of time here and our storage account is ready so i'll go in here and what we'll do is go to the lefthand side we can see we have some containers we wanted to make a file share it's really easy just hit the file share button and we'll name it so say my azure file share okay and we can pick how the tiers we want um transaction optimize seems okay to me we have to set a quota uh we'll say i don't know one gigabyte we're not really going to do anything real here so i'm just gonna hit create okay and so now we have our nice file share there if we click into it we can uh upload files so i need to go grab some kind of file off the internet and we'll just type in star trek here anything images as long as it's appropriate and yeah here's a photo so go ahead and save that save image as and we'll go to our downloads here and we'll just say kirk and spock okay and that is now downloaded we'll make our way back here i'll say upload what we can do is go to our downloads again kirk and spock we'll say open upload and there's the file right uh not super complicated so that's that um let's go back over to here into our storage accounts uh back here and i want you to show you uh storage explorer so you can go ahead and download search explorer if you haven't so you go here you download it hit download now i think i already have it installed so all i'll do is just click open and say open takes like two minutes to install yep i do and this is just an easier way like if you don't want to have to open up the portal and you want to be able to easily work with files it's another way you can do stuff so we'll give it a moment to open and so here we have our storage account we can drop it down and see what's under there because we have a bunch of disks that i need to go ahead and delete but if we go here to file share we have our file share here and you can just see it's just an alternative interface we can upload and do stuff there if i wanted to download that i could do that okay but let's go take a look at blob storage okay so go over to containers we'll make ourselves a new container we'll say my blob storage uh yeah it can be private that's fine we'll say create we'll click into our blob storage and we need to upload a file so again we'll upload the same file so we'll go here and we'll upload our kirk and spock file here and we'll upload it and there it is again and if we go over to our storage explorer and we go under blob containers and there you can see we have the data there so that's really really nice um so there's that we should probably take a look at tables so go back to my storage account left hand side we'll make ourselves a new table so we'll say my azure table say okay we click into this actually we can't this is kind of a pain you'd have to use the api stuff but this is where the explorer comes in handy so on the lefthand side we will look for tables and we'll say my azure table and uh we'll say add okay we have to set a partition key and a row key so our partition key would be something like we could say wharf and our category could be lieutenant commander okay insert and uh if we wanted to edit this we could add more property such as like um you know planet kronos whatever you want you got all sorts of types here but it's pretty darn simple as you can see not super complicated there um but yeah maybe we should learn how to use the azure tables via the cli so i'll be back here in a second and show you how to do that okay all right okay so i have the commands that we can do to uh do that for the cli so what i want you to do is click up here in the top right corner that is the azure cloud shell there and i want you to start in bash if it asks you to create a storage account go ahead and do that because it needs a volume so under here we'll actually make one this is the one that is for my developer environment so just hit yes and just wait for this to load sometimes it takes a little bit of time and if it's giving you a lot of trouble i'm going to hit restart doesn't usually give me that much trouble but there we go okay great it's i'm so used to waiting around in azure it's crazy but uh so what we can do is type in a z that is the cli command for az if we hit enter it should spit out some information yeah so it tells us all the things we can do there so what we want to do i'm typing clear here az storage identity insert hyphen t we want to put the name of our table table is called my azure table uh we need the container uh account name we'll need we don't need the container name so the account name is up here so i'm going to try to type it in here i'm going to go ahead and paste that in and now we can go ahead and insert so yeah yeah we'll go here to the hyphen e so e is going to be what we insert so we have partition key and this will be uh we'll say uh beverly and we'll put the row key as a commander and then we could say planet equals earth okay and if we hit enter i think we got it all right there give it a second uh it doesn't know what account name is let me just see if i spelt that wrong maybe it just doesn't like where it is so i'm going to just copy it out of here trying to make a bit clean by putting it there but um i will put it on the end here oops go ahead and paste that in there still doesn't like it um if it doesn't like that we could try giving it the container name it might just pick up the account name which is the account we're in um but i'm going to go here and find my container that's where i had this like working like a second ago and just decides not to work okay okay so that's fine so i already have a working version here on the side so what i'm going to do i'm just going to change some of the values here okay and so i have this here and i know this 100 works right so we have um a z a z sword storage identity identity insert insert um hyphen t for the table so maybe that was my problem is that i just didn't do hyphen t there so i'll just paste that in there i don't think container is used anymore but we'll hit enter so it doesn't like the container name so we'll just erase that out there and so there it has inserted the data so there we go so yeah i'll just show you like azure is a bit painful sometimes even when you have perfect instructions it still doesn't work properly we'll go ahead and hit refresh and there is the record in our table database so there you go uh and i might just leave these accounts open here because they might want to adjust them in another tutorial but of course we will clean them up at some point under the end of the course okay hey it's andrew brown from exam pro and in this uh follow along what we're going to do is take a look at cosmodb so what i want you to do is go to the top here and type in cosmodb and we'll go here and add a new cosmodb account and notice that we have some options we have sql mongodb cassandra gremlin azure table so what i want to do i'm going to be pretty crazy here i'm going to make a new core sql one and so we'll just say under here i'll say cosmo dp900 cosmodb okay and i'm just gonna call this one cosmo db coresql and just to be more you can say dp900 here i don't know if this one conflicts with other ones so we have provision throughput and serverless i'm going to choose serverless because um i don't need provision throughput and there is a free tier here but i'm going to go with serverless okay just so we don't have to worry about it we've got a lot of options here like global distribution et cetera like that i don't care about any of that stuff and so what i'm going to do is go ahead and hit review and create okay and we're going to repeat this process again and again so what i'm going to do is go back to cosmodb create a new account and we'll create a mongodb one now and we'll drop this down and we'll choose cosmodb i think i called it dp900 cosmodb so this would be dp900 cosmo db db the name is not available you just have to change until you get it i'm going to make this one serverless as well and we'll hit review and create so i just want to show you the variance of these ones okay so this is a little bit of annoying but what we have to do we'll add another one here this one will be azure table we'll drop it down we'll choose uh dp900 cosmodb we'll say cosmodb uh we'll say dp900 cosmo db um azure table serverless go ahead review and create and we'll create that and we'll go one more time we'll do a graph database okay oops doesn't want us to go back and we'll say gremlin and we'll choose our dp900 dp900 cosmo db gremlin okay serverless review and create and we'll go back to here and i'm just going to wait until these are all done so i'll see you back when we see all of our i think we made four four of our cosmo dbs okay all right so after waiting a little while here actually my gremlin database didn't create or my account didn't create so i had to make that twice but here they are and you can see this one is still creating but we can go and do some things while we're waiting first let's go check check out the core sql and so on the right hand side you'll see there's this quick start it's always a great way to get started so if we wanted to uh you know in azure we could start inserting data so if i was to go over to node.js and we say create items container this is what i'm the most familiar with now a lot of people go over to the explore data but i'm just curious as to what this stuff looks like so because i've never used the quick starts before so i'm just curious to see the quality of it so let's go ahead and create that items container right now i think it's just waiting to create so if i go over to that data explorer tab uh it's probably just going through that process of creating so it actually created us a todo list which is nice okay and it set up a partition key and oh looks like it's done so now we can go ahead and download that application says npm install npm start it's kind of cool and i have it over here so i just got to unzip it okay so i'm just unzipping that here choosing winrar that's what i got installed and we'll just drag that on over here and i wonder we'll go up here now you may or may not be able to easily open this depending on how your setup of your computer is so if you do you can't just you can just follow along here i'm opening up visual studio code here you just can't see it's off screen but i just want to close some things in here okay and i'm just going to go open go here file open a folder and this is in my downloads so i'm going to navigate all the way to my downloads here so we'll say um this is users andrews if you want to see me i'm just writing this over here downloads sql and we'll say okay and what we'll do is open up a new terminal here and the instructions said that we should be able to do npm install npm starts we'll give that a go we'll say npm install i might as well take a look here at the code just curious what's going on here so it's actually using the azure's cosm yeah cosmodb client it has a config file over here which contains uh our configuration information looks like it has some initial items that it's going to insert so there's some initial data which looks kind of cool it sets up a partition key here we have an endpoint so what it does create the database if it does not exist read the database definition create the container if it does not exist read the container scale the container create family item query the container so there's a lot of examples here so here it just gives you some stuff so you know if you know you're coding and you want to go through this it's a great way to get started it looks like finished installing we'll do an npm start i don't know if i'll be able to actually view this because it's running through a machine oh so it just runs it okay i thought i was going to like uh output a um like a localhost 3000 you could view it so it actually just ran that stuff so create the todo list reading created etc so yeah all of it's there so if you're familiar with that you can take a look but what we'll do now is make our way over to day explorer we can also go up to cosmos uh was it cosmos.azure.com is that what it is there we go this has a little bit more room i'm going to click on sign in it's the same thing as the date explorer just a little bit easier to work with subscription one and we are in sql here we'll drop it down now we actually have some data we can take a look at some good items check out items here and it should show us some right hmm go back here it says first name etc so you can see that it created the todo list reading the database create the items reading oh completed with an error entity with the specified id does not exist in the system okay so it's expecting something to already exist there and that's why it couldn't run so maybe what we'll do is just go ahead and create what it wants okay so while we go ahead and create a new item so the partition key is called partition key that's a very uncreative name but when you create these records you always have to have the id and the partition key name so i'll go here and we'll type in partition key and i guess it could just be whatever we want um i'm just looking at the record here to see what they wrote so we just scroll up here we are looking for create item just scroll up here see if we can find it this is more like reading an item here i'm not necessarily creating partition key at sign country usa so um i guess we just say usa here say name oops andrew go see if he'll let us create that uh we'll go ahead and just copy this oops place with id so we'll just say one we'll say save okay and it added some additional data underscores ts is for uh for a time stamp okay um and now what we can do is go and look at that data so we'll hit more oh i guess we're exploring it right there okay so let's say we wanted to query that data we could create a few other records because that's not a lot of information right so what i'll do is just copy this we'll create a new item here uh i'm technically not the usa i'm in canada so we'll put in vaco here just copy that and we will save we'll create a new item uh they should have been different ids but whatever we'll say brazil put in roger and we will go ahead and save okay and so now let's say we wanted to query that data uh it should be these icons are very cryptic let's store procedure whoops um yeah i guess we could do it right here no usquery that's what i want okay so uh if we hit run execute query to return us all the data but let's say we wanted to only select a subset of data so we just do where um name equals andrew let's give that a go see if that works oops uh what if we do c dot name there we go so yeah it's as simple as writing queries not like any other kind of sql so that's pretty easy let's take a look at another one here so that was that let's close that off and we'll take a look at mongodb next all right let's take a look at mongodb so we'll go ahead and click into here make our way to quickstart and we have some options we have node.js so if you wrote javascript and use the connection girl that's one way of doing it i think mongodb might be the shell might be a fun way to do it so what i'll do so go over here i don't think i have this installed but it comes part of the server installation for mongodb so i'll go ahead if you would like to download the sell shepardly from server yeah that sounds better to me i don't think i need everything and so select the zip download which includes the okay and i guess we'll have to click on the computer community edition here oh windows we can do the zip or msi yeah i guess we'll just install the full thing it's not a big deal maybe i'll use it for something else but we'll go there it's only 200 megabytes so we'll just wait for that to download shouldn't take too long here there we go and i just got my downloads over here we'll open that up sure we'll go ahead and hit next i agree next i guess we can customize it so maybe if we only wanted the client i'm going to just take everything um we'll say next that seems all fine to me we'll hit next install this process is on a mac or linux it's just going to be different process but yeah we'll go ahead and see this here all right so after waiting a very long time uh on that compass step it finally uh finished installing i guess it was doing a visual installer there it looks like it is opened and uh i guess we installed compass and it makes it really easy cool uh sure we'll say yes to everything there there's a little bit too much information but i guess what we can do is establish a connection and i guess i was thinking we just use the shell but this looks better so what we'll do is go ahead and take a look here so we will go grab our connection string and hopefully you'll just take it as that it looks like it's hold on here i think what we really want is this this is the string here connection string uh i guess we can grab it from here it's a bit easier we'll go back to our connection service here we'll hit connect fingers crossed it works first time yeah there we are we're in okay cool so uh i it's a new data like it's completely new so i guess we wouldn't have anything in it like a database or anything like that so we can do at the same time as we're doing it let's make our way over to cosmodb um oh i guess i closed it cosmos azure.com and we'll go down and flip over to our mongodb database and we don't we haven't created anything yet so we have to make a new database so i'll just say my database or my mongodb my collection can you think of a collection as like a table right uh shared key whatever you want uh my shared key again i'm just showing you how to insert stuff uh we don't need this sharded so we'll just say unsharded okay makes it a bit easier on us here i just want to see if that actually reflects over here in our manga db compass because it probably should we'll give it a moment there to create the database if it doesn't what we can do because there's a reload here right so let's hit reload until the data loads and yeah looks like we have it now so that's all good and i guess we have to click on this maybe click on this one again here to reconnect great now we have a database so we can go in here click on my collection we can add data let's hoping to execute a command oops and i'm just trying to see if it allows us to do that here in any way again i'm not that familiar with this tool but yeah i guess we can go create a document here we can make it through here so let's go to documents we'll add a new document and say one and i guess we should probably put our shirt key so my shared key say blue we'll just say name andrew and we will go ahead and save that document we'll go back here uh refresh we can see our document there we can edit it in here as well a lot nicer in this say we can add data we'll say document it's a little bit more cryptic i think i prefer to do it in there i'm again i'm not that familiar with this tool but what i really wanted to show you was the command shell so what we'll do oh let's just do shell up here oh that's nice okay i thought we were going to have to uh you know i thought we're going to have to paste this in so maybe we didn't have to install that all along but hey this looks pretty good to me right so what i have pulled up here is the api i had here a second ago at least say mongodb shell i definitely had it open here for us um because i wanted to show you the api uh so there's like right one okay here we go so i found the mongodb api here and if i just go back it's shell method so if you typed in mongodb.com for slash mail reference methods you can see all the kind of stuff that we can do and so let's just go ahead and see if we can insert something we only need to insert a single document and so here we kind of see the uh the stuff here yeah underscore id is the the main field there and what we'll do is scroll on down and we'll just go ahead and copy this command and see if we can get that to work okay so we'll go back over here give that a paste hit enter and we'll go back to our document and see if it's there there it is we will go back and check in our mongodb cluster here give it a refresh i want to see uh well it shows two records so what i'll do yeah it's not what i want so we'll try this again yeah i just started typing it six i already have an error and we want the my shared keys we'll say my shared key okay double quotations we'll say green and we'll just put in here name baco we'll say insert okay that record is there uh we'll give this a refresh here oops there it is there's the record as well it didn't insert the other one i wanted so we'll go back into our shell here we'll say db.products it's probably because i didn't i you know if you don't have the um partition keys just not going to insert it right i thought it would error out or say something that's what i was looking for so i'm going to say id we'll say 5 i'm going to put my shared key we'll say red and we will put name here and we'll say roger we'll give that a go and if you don't type it right it will not work so we'll say insert one there we go we'll go back to our document here we'll give it a refresh here i'm having like no luck doing inserts today but you know you get the general idea right so we're not here to really teach you a mongodb tutorial but i just want to show you that you know you use the shell to insert data there's this tool here and what a document looks like so that pretty much covers the document mongodb so what we'll do let's go back here and let's take a look at azure tables alright so let's take a look at cosmo db azure table so click into it we'll make our way over to the data explorer we will say new table and i'll just say my new table say okay and we'll give it a moment to create this new table here as you can see we didn't have to make it a database or anything it was just a lot more straightforward here but it does seem to take a long time to make a table to be fair it is serverless so it could be uh having to provision something before it do something we'll go here and look at entities and we can go ahead and add ourselves uh a new entity here we go up the top here so we'll just say worf lieutenant commander planet kronos add entity and there it is okay go maybe over to the query builder and if we wanted to filter this out we could say partition key i guess we need another record for this to make sense so we'll say um data uh commander um okay he's not from earth but we'll just put earth we'll add that entity we'll add another one might as well we'll say um crusher commander earth and we can go here we just filter it out so we'll say crusher uh partition key yep that's right so it should be able to run that so run query okay gets that exact record so pretty simple not too complicated and so now we'll move on to gremlin all right so let's take a look at cosmodb gremlins so click into it and what i want you to do is go to quick starts we'll go to guided gremlin tour and we'll create ourselves a sales graph collection so we get a little bit more information that we normally would have give it a moment here and as there we go yep it's ready so we created the sample this downloaded a package that contains a console app as dot net the console app needs to be executed first to upload the sample data and the asp.net web app will allow you to visualize your data okay so i guess we have to go ahead and do that so we'll go ahead and download that and there it is i'm just going to unzip it here and we'll double click and do it and it doesn't look like much but i'll drag it out into my folder here whoops and we will go and click into this and i want to know how to use this so i guess we'll open this up in vs code okay let's give this a read sample application shows to interact with the cosmic db gremlin etc etc visual studio code 2015 sample data has two projects uploading the sample data is done via the console application project included in the quick start run the console app all right so it's saying that we need to go to tools and we're going to double click this and luckily i have visual studio installed if you don't have install you don't have to go do this but we just want to see a nice looking gremlin graph in our cosmodb and i will go ahead and sign in and i'll use my example account and i guess it really wants me to choose some stuff but i'm fine let's just go ahead and start it uh the c sharp project is targeting.net which is not installed in this machine to proceed select the option below change the target you can change it sure as long as it runs you know i didn't feel like i was going to be doing.net today but we'll go ahead and open this up all that really matters is that we can run it projects loaded ready to use in the background and so we need to run this so it says over here so verify the settings if you download it etc okay if the console is run upload etc i just want to run it run the application f5 so i think it's time to hit f5 on my keyboard and so i have a mac keyboard so i'm not sure if that's going to work but what i'll do is i'll go to the top here oops i might need to install extra components so we'll go ahead and install whatever components it wants just click that over here in the right hand corner there it's probably because it needs a very particular version of net yeah it needs the.net desktop development here even though i'm not a dotnet developer i still know my way around ids pretty darn well so we'll go ahead and give that an install and it looks like it's at 848 megabytes i'll see you back here in a moment okay all right so after waiting a little while here it looks like it's installed and restarted and i think i can actually run the application now so we'll go at the top here and we will run let's say should be like a project run i guess we just hit the start button that works too right you.net fans that are watching you can make as much fun as you want to be okay uh and so you know we just want this to run so that it loads the data right that's all this thing does so it's going to build the project and it's complaining about virus and threat protection not really worried about that and it said it succeeded verified the database exists your collection etc uploading the graph now excellent there we go and we're just going to wait for it to upload those nodes and we're going to have a really really good example here right i'm not sure how many nodes there are in this could be a lot there we go oh now it's starting up edges great this is 469 nodes i wonder if there's going to be as many edges or less we'll see there we go so now it says graph uploaded you can now show this application okay sounds great so what we'll do and i'll just go ahead and close visual studio because i don't need it open anymore and what we'll do is open up the data explorer actually kind of prefer cosmodb so we'll type in cosmodb again and we will switch over to our gremlin database here and go into persons graph database you should be able to see a visualization without have well i guess we can just hit execute search query hmm maybe insert it in this one we'll go ahead and execute it over here there we go so now we have some records so cool all right we got that working and um so yeah this is like that gremlin language up here um i'm not that familiar with it so you know what i'm going to do i'm going to be right back and uh and learn it in two seconds okay all right so i'm back and i learned a little bit here and what i did was i went to the apache gremlin website for tinker pop or gremlin in particular so g dot v parentheses is when we want to get all records back if we put in a particular id here it should give us a particular result i also figured out as we move around here if you scroll into things like this and you click on there it will expand the next area it should there it goes it's a bit slow right but you know we have an id here so let's give it a go and see if it actually works so we'll do g v and let's see if we put that in there like this we'll give it uh maybe quotations they just have the number one so i'm not sure to tell you there and yeah so it grabbed that particular point there um it looks like we can grab different values so i'm assuming that these are values on it here so maybe we'll just do dot values here values uh well if we give a label here there we go service plan so yeah it's not that hard go back here execute the plan and then there's like commands like out e i assume that's the way of like you would select things in the region so um i don't know if like that is a definitive one but we'll just go ahead and put that in there and see what happens and we got nothing because you know we have relationships between other sources like edges and so maybe it's based on the label here so let's try this execute the plan yeah i'm not sure what it is but my point is is that we populated ourselves a graph you can tell that that's how you use the language if you really want to learn more you can go through the tutorial uh on how to create that information but for our purposes we just wanted to show that we could popular graph and play around with one there a bit okay so that is gremlin we did we did azure so that's all of them so we're all done here again i'm going to keep these around just in case i want to pull them into other services and at the end we will destroy all these resources okay hey this is andrew brown from exam pro we're going to learn how to use data factories to run a transformation job maybe between our sql and our blob storage because we do still have the setup from previous tutorials so i'll make my way over to factories you just type in data factory at the top here and you can see i have one there it's right now deleting because i'm not happy with it but what we'll do is create ourselves a new one so i'm going to make a new group here dp900 data factory 2 okay and here i'll have to name it so we'll say dp900 data factory we'll go with version two it doesn't like it so i'm gonna put a two on the end there and we'll go to next i do not want to use git at all that was the reason i uh i restarted this was because it was complaining about that for networking it's going to have a public endpoint for we have the ability to encrypt our data which we do not care today i'll go ahead and hit review and create we'll go ahead and create arc data factory and we'll give it a moment here it doesn't take too long to set up a data factory so there it is so we'll go ahead and go to resource and the way we're going to actually access it i don't know why they have the button here but this is where it is it's author and monitor it's going to open up takes about a second and here we are so what we'll do is we want to create ourselves a new pipeline we'll make it easier on ourselves go to the left hand side hit the plus create a pipeline and we'll say sql to um sql to what it's a bit glitchy there sql to blob okay and we'll go ahead and save that um i think it uh does it auto save yeah it auto saves so what we'll do is go to the left hand side here go to link services click the plus we need sql we have an sql database so we'll go here we'll say continue we'll have a connection string we'll select our subscription our server name our database name our type is sql authentication azure user capital t testing one two three okay you can have always encrypted on there i'll go ahead and create that you can hit test connection if you're concerned that it's not working i think we can click into it hit text test connection there it's got a nice little green so that's all good we want to put this in our blob storage so i will add another one here we'll type in blob we will create that hit continue i don't care about the name but what we'll do is use a connection string to select it as well so we'll go down here subscription one it's my storage account and we will test the connection and it's all good we'll hit create cool so we'll now go back to our pipeline or our authoring here we need a data set um can we get that from sql we sure can we'll click on that we'll hit continue we'll select our linked service which is here we'll select the table we want so let's say we want to translate over the products and we will go hit ok all right and so now we have our data set okay and so we'll move on to the next step all right so let's go set up a transformation so what we'll do is drag out copy data and we will choose our source and we'll say our sql table and we're going to want to put that into a csv oops i did not create our data set here so we'll create a new data set go over to blob storage here hit continue choose csv format so you can do excel json all fun things we'll choose our link service which is blob storage uh this has to be the name of our blob storage i already forgot what it's called so we'll go up here make our way over to storage accounts on the lefthand side go into storage accounts we need to find that container name there it is go all the way back put that in there call that transform and we'll say products dot csv first row is header sure but we don't have any import scheme because like you could have one in there and say okay this is what the schema should be like have the headings in there so it knows what it needs to translate over to but uh it's going to be a onetoone mapping so it's going to be something really simple here uh so this seems all fine to me notice we don't have any schema we could import one but there's nothing to import so go back to our copy data we'll choose delimited file and we'll take a look at mappings we'll import schemas it's going to import the schema from the table here are the column names here so as you can see there are no column names which is totally fine and uh what we can do is go ahead and let's go hit uh debug okay and so it's queued up and now what we'll do is go ahead and validate okay and it says it's been validated no issues as of yet we'll go hit publish um yep go ahead and publish it and so now it has actually ran the pipeline so what i want you to do is go over to your storage accounts give it a refresh go into my blob storage now we have a folder called transform we have a product called uh csv called products we'll go ahead and download that okay we can go ahead and open that up in excel and there you go so we just did a transformation it's not the most beautiful transformation but you can see that it's a very powerful tool we didn't have to write any any code whatsoever which is really nice so there you go hey it's andrew brown from exam pro and in this follow along we're going to take a look at a data bricks and sparks so what i want you to do is go to the top and type in azure data bricks we'll go ahead and hit add we'll create an azure databrick service and what we'll do is hit create dp900 azure databricks okay within our workspace here what we'll do is choose dp900 azure databricks very uncreative of me but it works out fine and we have some options standard premium trial i'm going to stick with standard i do not want this to cost much whatsoever go ahead and create that environment there and i'll see you back here in a moment when it's deployed all right so um our databricks environment is ready so if you click launch workspace it does take a little bit time to launch so i'm not pressing it but we'll have our environment here and so uh you know there's stuff you can do here like create clusters and run jobs and notebooks but uh just so you don't spend any money i'm going to show you another way you can run this service which is totally uh totally won't cost you anything so we'll go to databricks and type in community edition if we go down below below here i already have a log if you sign up here it's exact same thing pretty much but there's no chance of you having unexpected spend so i prefer to do it this way even the cluster is shut off over a certain amount of time and so we'll click on the explore quick start tutorial here and i think it's shift enter if you want to the commands it's in the top right corner this little keyboard thing it tells you what you can press so run command and move to next cell that sounds like what we want to do so i'm going to hit shift enter shift so it says in the sidebar create a cluster etc but if we just keep on going here it should automatically create this one so automatically launch one without to clusters without prompting so we'll just hit attach and run okay and that's going to start up a cluster but as it's running we can just kind of take a look here and so you can see that we're running sql commands um and so it looks like we're it has we're loading a csv and then we're treating the csv with sql which is really nice so that's the thing where it's like you have data that's not necessarily in a database but you're able to run sql commands on csv files and things like that and then down below you can see uh it's using python to use spark to load and format the file um and uh and you can even visualize information in here so it automatically let us plot it and change some of the options here so we go here make a histogram quantile map that's useless area bars so it's very very useful tool but wait till that cluster spins up here takes a little bit of time if you want to monitor a cluster we can go on the lefthand side and just go to clusters here okay and you could leave this and and and totally not be worried about it you don't have to worry about it like spinning up and costing you money okay so i'd go here and give it a refresh and i don't see the cluster so i think it's just because i was terminating an old one and now it's it's waiting until it can determine uh start up a new one so it might not spin up but it's not really that oh there we go it's terminating great uh is it going now you can never tell these things is the free edition so but if i just go back here we can even look at the commands here cyborg create cluster quick start and database runtime drop down to l3 create a cluster um so is it going to create cluster now nope we'll cancel there yes and let's just follow it manually so i go to clusters create the cluster here it's saying l3 lts kind of gives you an idea how old their tutorial is lts but that's what it wants we should match it exactly sparking scala i doubt that it's for that one there and so we will just name our cluster quick start oops right after the name of what it asked for oops and we'll go ahead and create and we'll i guess wait till that starts up it could take about a few minutes to start up so what i'll do is i'll see you back here in a moment when it started okay all right so we had to wait there a little bit but uh now it looks like our cluster is running so what i want you to do is go back to that notebook we had and this time it should possibly we'll close the other one because that is our the one we don't want to use but we'll do shift enter shift enter shift enter um it's not would you like to launch a new cluster no i want to use the existing one maybe i can drop down and choose the cluster oh here we go um cancel hold on here quick start confirm there we go okay and we'll see if that executes the command now it's a bit slow but again this is a community edition and also imagine if you're dealing with massive amounts of data so it's not really surprised that it's not like super fast um but like you have to think of it at scale right and so it completed the job so that was pretty good we can go ahead and run this line again for fun okay so you get the idea here right so that's all i really wanted to show you here um just kind of a bit of exposure to data bricks there but you'd have to learn all about apache spark to really understand the stuff behind it um but uh yeah so what we'll do and again you don't have to shut it off it doesn't matter but what we'll do is just terminate and be be good about that also we'll delete this at the end all these follow alongs for the dp900 we'll definitely be sure to delete all of them but just for this one in particular i just want to get rid of it so i don't forget about it um yeah there you go that is databricks hey this is andrew brown from exam pro we're going to be looking at azure snaps analytics today in this follow along so what we'll do is go to the type here and type in synapse make our way over to analytics we'll go ahead and add ourselves a new workspace and i'll create a new one here this is the dp900 so we'll say dp900 um synapse workspace okay and we need to give it a managed resource group uh so i guess same thing dp900 synapse workspace i'm just going to keep on naming it the same as long as i can have it for our subscription we'll have to create a new account because we do not have a gen 2 storage account i wonder if i could just make it in my other one nope so we'll have to just because i already have a storage account right on there so we'll say data lake storage okay probably wants it all lower case data lake storage dp900 okay and uh we don't have a file system so dp900 data lake file system okay and notice here it says assign myself storage blob data contributor role for the data lake storage too if you remember uh from our lecture content about very important roles this is one of them and it's being auto assigned to us go here to security here it has this but i'm going to just say azure user and capital t testing one two three capital t testing one two three just so i can remember between our other ones enable double encryption that sounds good but not something we're doing today um we'll go to network this seems okay to me next review and create and just notice up here it says azure synapse workspace is 6.40 so it's not something we want to uh have lying around um this is per terabyte right but we're not going to be working with a lot of data here so it shouldn't be a big deal for us we'll go ahead and create that and uh yeah i'll see you back when this is all done okay all right so after waiting a little bit of time there our synapse analytics environment is ready what i want to show you on the left hand side is if you scroll on we'll actually got to go to the workspace first scrolling down you have your analytics pool so you have sql pools where you can run sql operations so you can go there and create those clusters these get pretty darn expensive so um it's not even showing the estimation there but um the thing is is that you know if you needed sql you could do that because remember there's two engines that you can use you can use the sql engine or the apache spark engine right so you create those pools sql pools first you can also create them within um the workspace but we'll go up to overview the workspace is the studio so we're going to open that up and once we get in here we can ingest data and do all sorts of things so it makes it very easy but let's actually just go back you can't exit out of that once you open that up so i'll go and reopen this up because they do have some test data we can use which is uh should not cost us much but if we go all the way over to learn here and we go to sample sample data we have some options here if you choose this one it'll create a pool but what i'm going to do is go to query with data with sql and it will give us some sample data so we click on this and what you can see here and this is just me trying to run it before but what you'll see here is we could use the synapse sql right and we're actually able to import files so here it's importing a parquet file from a um azure opens like it's a storage account by azure and it's just an open data set so here you can just kind of see that there's ways of querying it and what we would do is choose where we want to do it so we go built in we can publish that before we uh do that there and we'll go ahead and hit run and so you know it's going to pull that data it's going to take a little bit time to get it loaded in and before it runs it but it doesn't usually take too long and there is the data so yeah that gives you kind of an idea how azure synapse uh works um but yeah so if it was an sql here you just write your apache spark usual stuff there uh yeah and hopefully that gives you a good idea how that works i don't like to keep the service running it is can get very expensive very quickly so me being paranoid i'm going to make sure i terminate this one definitely early even right now um but yeah that is azure synapse analytics okay all right so we are at the end of our follow alongs and you could tell that they all kind of tied in with one another so what we'll do is we do some cleanup so from the lefthand side go to resource groups and and it's dp900 you can go ahead and delete so what i'm going to do is just click on each one here it could be from other ones but i'll go ahead and just type in like cosmodb again it's based on what you name them but this is the fastest way i find to clean up all my stuff and i'll go here and just keep on going down the list you know because mostly the things we used they're not going to have ongoing cost the one that will will definitely be like the sql server so that's something that we definitely need to terminate okay some of these might give us some trouble so i'm again i'm just running down the list here deleting as much as i can and then i'm going to see if there's any errors here but i think we already deleted this one but we'll give it another go here and yeah that should be all of them here if you get a failure you might have to investigate it so here it says failed to resource the client uh does not have permission yeah i do however that's tonight against the deny assignment so the idea is just you know wait for this to finish uh it could take a little bit of time and sometimes like it's deleted but it just takes time for this to vanish and another thing you can do is go to your resources and just make sure the resources aren't running anymore so i'm looking here and what's going to cost me money is this sql database that's running so you know i want to make sure that stuff is deleted so say yes you could also do it from here as well you should let the resource groups delete it because sometimes it gets confused but uh you know i just want it all gone i've got an ip here data factory storage account all these things i want them gone just say yes uh you know it doesn't show up here is disks i know i have a bunch of disks let's go over here no there's nothing okay good i thought i might have had like a bunch of disks you know why because i was looking at my explorer and probably just showing the old ones so yeah it takes a while for this to to happen but um you know i'll come back here and show you that i've deleted them all and that's what you should do too okay so i'll see you back here in one more wrapup video and that's it all right so after waiting uh quite a little while here i just wanted to see if it was all cleaned up so i'm going to give it a nice refresh here looks like my resource groups are cleaned up if i make my way over to my resources here it doesn't look like there's much uh here remaining so and just check our notifications make sure everything is deleted so that's how we make sure that everything is cleaned up in our azure account and that's it for our follow alongs okay
