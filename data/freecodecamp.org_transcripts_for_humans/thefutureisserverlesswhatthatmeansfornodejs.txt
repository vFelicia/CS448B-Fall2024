With timestamps:

00:00 - i'm chris anderson i'm a program manager
00:01 - on the azure functions team um i
00:03 - actually
00:04 - started the functions project about two
00:06 - years ago and it was mainly kind of the
00:09 - background was as i was working on a
00:11 - product that was called web jobs which
00:12 - was for kind of doing background jobs in
00:14 - the cloud and it was mostly just an sdk
00:17 - framework you deployed onto your own
00:19 - you know instances your own vms you had
00:21 - to manage everything yourself
00:22 - and
00:23 - one day we tried to add node.js into it
00:26 - and ended up making it really really
00:27 - hard to manage it actually made it worse
00:29 - but the end product was really really
00:31 - cool in terms of what you wrote and so
00:32 - we discovered if well if we just run
00:34 - this as a service we'll end up with a
00:36 - better product and i'll kind of talk
00:37 - about why we ended up going with node.js
00:39 - as far as how that goes
00:41 - so
00:42 - serverless node.js
00:44 - i choose you
00:46 - there's really i think two kind of big
00:48 - things it also had to do a lot with
00:50 - javascript and then we'll also talk
00:52 - about node.js
00:53 - javascript i like to say is kind of the
00:55 - english of programming languages a lot
00:58 - of people can speak at least a little
00:59 - bit of it even if it's bad right it's
01:01 - not necessarily the perfect language but
01:03 - it's a language which is relatively easy
01:04 - to learn for a lot of people um it might
01:07 - not even be the most widely used
01:08 - language but it's used by a good section
01:10 - of the developer community out there
01:11 - especially in the kind of web
01:14 - application space so it's a very very
01:16 - useful language for us to choose it
01:18 - doesn't require any compilation which
01:20 - means that a lot of the goal that we had
01:22 - was for people to go ahead and get cloud
01:24 - running in the cloud as fast as possible
01:26 - our goal was that you were basically in
01:28 - the editor doing something in like 30
01:30 - seconds and that you were actually
01:31 - walking away within five minutes having
01:33 - done something and have a service up and
01:34 - running in the cloud that meant that we
01:36 - having compilation having build even
01:38 - having an ide meant slow down to that
01:40 - time and so we really focused on that
01:42 - quick path first since then we've
01:45 - kind of gone back and said okay for
01:47 - people who do want to work in an ide for
01:48 - people who do want to have build and
01:50 - compilation they still can but we
01:52 - started that initial goal of cl of code
01:55 - in the cloud as fast as possible
01:57 - um
01:57 - and then yeah there's just a giant great
01:59 - community of which you guys are all a
02:01 - part of of course which made it easy for
02:03 - everything that we needed to kind of
02:04 - work out of the box or we have available
02:06 - to us
02:07 - and then node.js
02:09 - node.js is a great platform in the case
02:11 - of serverless because it's a very very
02:13 - lightweight framework
02:15 - when you go and just measure how fast it
02:17 - is for node.js to come up and start
02:18 - running for the first time versus how
02:20 - fast it is for say like java or even you
02:22 - know c-sharp in some cases to come up
02:23 - and run for the first time it's much
02:25 - faster and for the general case and
02:28 - that's really useful because in
02:29 - serverless
02:30 - it's actually not serverless there's
02:32 - servers back there but those servers
02:34 - have to come up basically when your
02:35 - request comes in or when the event shows
02:37 - up you know for asynchronous messages
02:39 - it's not the worst thing in the world if
02:40 - there's like a a second or two or you
02:42 - know a couple seconds of you know
02:44 - latency coming up but a lot of the times
02:46 - people are using these functions in http
02:48 - scenarios in which case any latency of
02:50 - bringing the instance up is impacted by
02:53 - the user that's trying to use that
02:54 - function and we know that in the case of
02:56 - like websites latency as far as like
02:59 - responding to the request can mean lost
03:01 - users and so we know that it's critical
03:02 - for things to come up as fast as
03:03 - possible and node.js is already kind of
03:05 - designed to come up nice and quickly so
03:07 - it was a great platform out of the box
03:08 - there and the other nice thing was that
03:10 - it was very
03:12 - widely used at microsoft we make sure
03:14 - that we kind of reach all developers
03:16 - developers running on windows max linux
03:18 - and linux and node.js was already really
03:20 - really good at running in all those
03:22 - places and so we know that we'd have the
03:23 - best support across all those various
03:25 - places we needed to run
03:29 - so what can serverless do for node.js
03:31 - developers you know we kind of chose
03:34 - node.js to go out with in the first
03:36 - place
03:37 - um
03:38 - because it was very easy for us to get
03:39 - going and we wanted to kind of get
03:40 - started we wanted people to be able to
03:42 - use it easily but for people who are
03:44 - just kind of already using node.js
03:46 - already used to using it why is
03:48 - serverless a useful thing
03:49 - well one of the things that i came
03:51 - across frequently when i was building
03:53 - node.js applications was
03:55 - while going ahead and you know building
03:56 - my server is really easy kind of doing
03:58 - background process of messages can be a
04:00 - little bit less easy to manage and
04:02 - maintain there's a lot less tools out
04:04 - there which understand frameworks for
04:05 - doing background processing and i also
04:07 - have to kind of manage the scale for
04:09 - those background processing it's really
04:10 - easy to scale on like http request
04:12 - messages in a lot of platforms
04:14 - but you know cues and things like that
04:16 - that can be a little bit trickier to get
04:17 - right and so serverless makes it so i
04:19 - can basically go ahead and have a
04:20 - function up and running and it scales
04:22 - dynamically to
04:24 - whatever i need to and since you pay for
04:26 - execution and most serverless frameworks
04:27 - out there
04:28 - you don't really have to ever overpay
04:29 - for your system
04:31 - it also means that things are much
04:32 - lighter
04:34 - end-to-end frameworks become a lot less
04:36 - necessary since all i'm really
04:37 - necessarily having to go ahead and push
04:39 - is just a function
04:41 - i'm not having to worry about like
04:43 - having that full stack of things that's
04:44 - going to manage like the various http
04:46 - entry points i'm not having to go
04:48 - through there and worry about a lot of
04:49 - extra code
04:50 - a lot of the times especially if you
04:52 - take advantage and for instance at
04:53 - microsoft we have azure functions where
04:56 - we have actually bindings to various
04:57 - services you never actually have to
04:58 - bring in the sdks to talk to for
05:01 - instance like cosmos db
05:03 - which is a giant you know
05:05 - document based database that we have you
05:07 - can actually use our bindings and just
05:09 - return a json object from the function
05:10 - and it'll get stored to the database for
05:12 - you so you never actually have to deal
05:14 - with working with those sdks it makes it
05:15 - very quick
05:16 - and of course horizontal scaling high
05:18 - scale is easier than ever basically with
05:20 - functions we just keep on throwing new
05:22 - instances at you and you never actually
05:23 - have to see the instance account worry
05:25 - about managing the vms worry about
05:26 - patching things it just keeps on scaling
05:28 - and then as soon as you have no more
05:29 - traffic on your service the vms
05:31 - disappear and you don't really worry
05:33 - about when they're there and when
05:34 - they're not
05:35 - you'll just kind of scale out when you
05:36 - actually have requests when you actually
05:38 - have usage this has turned out to be
05:40 - really useful in a lot of cases one of
05:41 - the first places that someone came up to
05:43 - me and it's like oh yeah azure functions
05:44 - it was really useful because of that
05:45 - whole kind of scaling thing was actually
05:47 - at a conference someone had built the
05:48 - conference app using functions and it
05:51 - was really great because they got a lot
05:52 - of like random requests especially like
05:53 - the week leading up to the conference
05:55 - from the various project managers trying
05:56 - to do the conference and he would just
05:58 - go through there and add a random
05:59 - function to serve that purpose and then
06:00 - after the conference was done he'd have
06:01 - to worry about scaling down the vms or
06:03 - anything like that he just kind of
06:04 - walked away and he left it there in case
06:06 - people wanted to use it randomly and
06:07 - then he didn't get charged anything for
06:09 - those functions running in the meantime
06:10 - so it's very useful for those kinds of
06:11 - use cases
06:12 - um and just to give you a sense before i
06:14 - kind of move on to
06:15 - what can node.js do for serverless i
06:17 - just wanted to give you a simple sense
06:18 - of how this can work
06:20 - here i actually have the functions
06:21 - runtime running locally on my machine
06:24 - this is the simplest kind of demo of
06:27 - hello world so i've got the very classic
06:29 - case here your function in this case let
06:32 - me pull up this one
06:33 - your function in this case is just going
06:35 - to be exporting a function from the
06:36 - module so i've got a context object
06:39 - coming in a request object coming in i
06:41 - can go ahead and do some log statements
06:43 - i'm able to access it we actually
06:46 - purposely made the request object look
06:48 - like a connect or an express object so
06:49 - that way if you wanted to use connector
06:51 - express middleware those things can
06:53 - still work with inside of your function
06:54 - without having to do too many tweaks
06:56 - and then my response here i'm just able
06:58 - to add headers straight to the message
07:00 - do my body here and it just kind of
07:02 - works like normal and so here let's just
07:05 - kind of reset the demo
07:07 - you'll see here in the bottom right that
07:08 - i'll see various um kind of things
07:10 - scroll by here the way that i got this
07:12 - running
07:13 - is i just run funko start you can
07:15 - actually do npm install
07:18 - azure dash functions dash core dash
07:20 - tools dash d and then you'll have
07:23 - functions running on your local machine
07:25 - and you can run funk knit funk new
07:27 - function and then do funko start once
07:28 - you have your project going so that
07:30 - spins up it's listening on 7071 we can
07:33 - see our api hello function that's been
07:35 - hung out there because this function is
07:37 - called hello based off of the folder
07:39 - path
07:41 - and i can go ahead and run and it sees
07:43 - says please pass the name on the
07:44 - correction on the request body if we
07:45 - want to inspect why we got that logic
07:47 - back we can actually go ahead and just
07:48 - hit attach in vs code when you use
07:50 - functions we go ahead and actually drop
07:52 - a vs code file there with the
07:53 - launch.json which is pre-configured to
07:55 - talk to your local functions runtime so
07:57 - there's no setup there to get debugging
07:59 - working it just automatically drops that
08:01 - launch.json file there so now that i'm
08:03 - connected i can go ahead and hit enter
08:05 - again and you see that i've got this
08:06 - breakpoint set up here i can now go
08:08 - ahead and inspect the request object
08:09 - look at the query object and i see that
08:11 - it's actually empty there's nothing on
08:13 - there a query object which is why it's
08:15 - falling into this else statement which
08:16 - is saying please pass the name on the
08:18 - query string or the request body
08:20 - so we can fix that real quick
08:25 - and now when i do that i can go ahead
08:27 - and see the query object now has chris
08:28 - on it and name is now equal to chris and
08:30 - so if i go ahead and continue we now see
08:32 - hello chris with a question mark at the
08:34 - end of it
08:35 - and it's still really fast to develop
08:36 - this i don't need to go ahead and hit
08:38 - funko start every single time that i've
08:39 - got to change to my function i can go
08:41 - ahead and let's get excited rather than
08:43 - kind of
08:44 - asking questions about it hit enter
08:47 - and you can see there i go i got my
08:48 - exclamation mark so the host is sitting
08:50 - there constantly listening to file
08:51 - changes so it's very easy to just write
08:53 - some code change write some code change
08:55 - um the only nice thing with this is it's
08:56 - not just http though i'm using http
08:58 - right now because it works entirely
08:59 - locally and the internet's been kind of
09:01 - like in and out throughout the
09:02 - conference but it can also still listen
09:04 - to all your q services it'll listen to
09:05 - event hubs in fact if you configure it
09:07 - to listen to your production services
09:09 - you'll actually kind of end up with n
09:11 - plus one things because it'll actually
09:13 - collaborate in real time with the
09:14 - cloud-based instances the same way that
09:16 - all the cloud instances themselves
09:17 - collaborate with each other using a
09:19 - storage account with the covers
09:21 - so now i just kind of understand what
09:23 - this you know this is kind of what led
09:24 - us to you see how easy it is to go ahead
09:26 - and write this code if this was you know
09:28 - a language that required building for
09:30 - instance like you know java this would
09:32 - still be relatively easy to be a build
09:33 - step and then i'd have to go and hit run
09:35 - but because i can script it and i can
09:36 - live edit things we get a really nice
09:38 - kind of fast inner loop in terms of
09:39 - developing goes and then i can just
09:42 - deploy it up to the cloud or even do
09:43 - this from the portal itself if the
09:45 - internet was working a bit better
09:48 - so from there let's kind of kind of talk
09:50 - to about the next phase of things in
09:51 - terms of what
09:52 - has been weird about running node.js and
09:54 - serverless together and what can the
09:56 - node.js ecosystem do to think about
09:58 - serverless when
10:00 - you know we're doing the things that our
10:01 - community does um
10:04 - one of the things that i really liked
10:06 - about node.js and javascript was how
10:08 - easy it was for you know new developers
10:10 - kind of walk up and get started with it
10:11 - one of the nice things about node.js and
10:13 - javascript is since there's a lot of
10:14 - client developer client-side developers
10:17 - kind of already knowing javascript
10:18 - already used to how that language works
10:19 - it's very easy for them to walk up to
10:21 - node.js to have them start building
10:23 - their own backends if they needed to do
10:24 - that
10:25 - serverless in some ways kind of has a
10:28 - similar kind of thing about it where
10:29 - serverless is very easy to walk up to
10:31 - and start building a cloud service and
10:33 - get up and running you don't need to
10:34 - worry about setting up a vm and managing
10:35 - it you don't need to worry about
10:36 - anti-virus software you don't need to
10:37 - worry about firewalls all that's done
10:38 - for you by the managed service so now
10:40 - those same cloud you know those same
10:42 - developers who might be able to get up
10:43 - that single vm but couldn't really reach
10:45 - scalable systems can now use serverless
10:47 - technology to deploy systems at scale
10:50 - for both you know front-end facing http
10:52 - applications but also back-end systems
10:54 - that might have even been farther away
10:56 - from their area of expertise it's a very
10:57 - accessible thing to get up to so because
10:59 - node.js is already kind of the star
11:01 - language of serverless we're going to
11:03 - see serverless as it's becoming more
11:04 - popular bring even more people into the
11:07 - server into the node.js ecosystem
11:10 - but there's a couple of gotchas that
11:12 - those new people coming into the node.js
11:13 - ecosystem are going to have to deal with
11:15 - and some of them is that
11:17 - node.js
11:19 - kind of started ignoring some of the
11:20 - lessons that we learned from the web
11:22 - space and that we got kind of very
11:24 - comfortable with the fact that since
11:25 - these things are going to be long
11:26 - running on servers it doesn't matter if
11:28 - our packages are huge and we load
11:29 - everything in the world and through that
11:31 - process and you know as long as it's not
11:33 - taking up too huge of a memory footprint
11:34 - like who cares how many tiny little
11:36 - files that we have and things along
11:37 - those lines
11:38 - well as i kind of mentioned before about
11:40 - node.js is actually great because for
11:41 - the simple cases it comes up very
11:43 - quickly
11:44 - for the cases where you decide to go
11:46 - ahead and you know import low dash and
11:48 - import every single giant
11:50 - framework that you can think of into
11:52 - memory it takes actually a while to even
11:53 - just read those things from disk and the
11:55 - memory footprint then means that people
11:57 - who are using those libraries inside of
11:59 - functions have to pay more because they
12:01 - have to pay for that memory footprint to
12:03 - actually exist so if you load giant
12:05 - libraries into memory
12:07 - more than ever you're not having to pay
12:08 - for that if i can choose a library which
12:10 - is you know one megabyte versus a
12:12 - library which is you know maybe 30
12:13 - megabytes that could mean big
12:15 - differences for my end of month bill in
12:17 - the case of serverless and so now as you
12:19 - know any package authors who are
12:21 - listening to this talk or here in the
12:22 - audience it really pays to be thinking
12:24 - about how can we go ahead and reduce
12:27 - what actually gets loaded there's lots
12:28 - of things that can help with this you
12:30 - know if we're now using you know the new
12:31 - kind of uh you know common gs stuff for
12:34 - it it'll only load the parts that it
12:35 - needs to in memory those are really good
12:37 - things i actually wrote a tool that's
12:39 - called azure functions pack which i
12:41 - actually go through there and web pack
12:42 - your code all together it doesn't
12:44 - necessarily do as much to reduce memory
12:46 - footprint for that you'd want to go
12:47 - ahead and run uglify on top of that the
12:49 - result of that web package
12:51 - but it does reduce the load time i
12:53 - actually did a lot of kind of testing of
12:56 - various impacts of i took the four
12:58 - largest node modules i could find i put
13:00 - them into a function and measured it on
13:02 - an i7 with an ssd so very very fast
13:04 - system it still took about two seconds
13:07 - for that function to come up for the
13:08 - first time because i had to start
13:09 - node.js and node.js had to read all that
13:10 - stuff into memory
13:11 - when i web packed it it actually
13:14 - took about 100 milliseconds so it's a
13:16 - giant improvement and when you're
13:18 - running inside a serverless on maybe a
13:19 - little bit more of commodity hardware
13:21 - you're going to be running it's going to
13:23 - take even longer to load those modules
13:24 - into memory so
13:26 - and a lot of the sales providers
13:28 - actually have limits on the amount of
13:29 - content you can actually put up there in
13:30 - the first place
13:32 - in the case of functions we don't
13:33 - actually have any file in the sizes we
13:34 - haven't hit those issues but they have
13:35 - existed in other platforms and so it's
13:37 - important to think as we're writing
13:39 - these packages especially if we want
13:40 - these packages to work instead of
13:41 - serverless um
13:43 - we need to be thinking about what those
13:45 - things are going to do there's actually
13:46 - a couple of examples of this out there
13:47 - in the wild already beyond just like
13:49 - things that i've seen as i've tried to
13:50 - work with customers using functions um
13:52 - like on postgres one of the bigger
13:55 - angular threads there for the the pg
13:57 - module is someone talking about hey this
13:59 - thing
14:00 - started consuming a lot of sockets
14:02 - without me realizing it because my
14:03 - function was kind of coming in there and
14:04 - using it and using it using it and
14:05 - opening up sockets and not closing them
14:07 - and to be fair they were misusing the
14:09 - library they didn't read the
14:10 - documentation properly they weren't kind
14:12 - of using it the way they thought it to
14:13 - but they used it the way that it felt
14:15 - natural to use it inside of their
14:18 - function and so we might want to think
14:20 - of especially for writing a package
14:22 - which we know will be harmful and to use
14:24 - wrong in serverless to include a section
14:26 - about hey if you're trying to use this
14:27 - inside of one of the service platforms
14:29 - like you know here's instructions on how
14:31 - to create a singleton instance of it
14:32 - outside the function so it stays in
14:34 - their memory here's how to write yours
14:35 - through the code for it to go ahead and
14:36 - close its connection before the
14:38 - function's completed and you know some
14:40 - examples can really help a long way with
14:42 - making sure that anyone news coming to
14:43 - this ecosystem from serverless into
14:45 - node.js has a good time
14:47 - because it'll pay to make sure that
14:48 - people are you know successful in that
14:50 - way
14:52 - and then there's one other thing that we
14:53 - should be kind of mindful of is that you
14:55 - know node.js and serverless and
14:57 - javascript has really gotten very far
14:59 - and the community's been doing really
15:00 - well because i think of the strong open
15:02 - source nature of things and how easy it
15:05 - is to get access to the parts the system
15:07 - runs on you know the even the core
15:09 - brains of like you know
15:11 - you know chromium and things like that
15:12 - or open source it's really really great
15:14 - just how fast and how much all of us are
15:16 - empowered to contribute to the platforms
15:19 - that we run on um and since a lot of the
15:21 - service implementations today are very
15:22 - vendor specific there is some risk there
15:24 - so it's something that's worth keeping
15:25 - in mind there's you know people out
15:26 - there that are thinking about problem uh
15:29 - ways to address some of these problems
15:31 - um like service framework is one of
15:33 - those cases there's you know apex which
15:35 - is more of a go-based one from tj
15:38 - a couple different examples um so one of
15:40 - the things that we're trying to do from
15:42 - the function side of things is even
15:43 - though we're a vendor and we are
15:44 - building a specific implementation
15:46 - that's mainly designed to run on azure
15:48 - it is all open source and we are working
15:50 - really hard to try to build it so it is
15:52 - a bit more of a portable system and so
15:54 - we really do want to make sure that you
15:55 - don't really feel like you're
15:56 - necessarily locked in even though it's
15:58 - built to run really well on azure we are
16:00 - trying to make sure that we at least
16:01 - build it in the open source and
16:02 - eventually we can work towards a way
16:03 - where it's even open platform
16:05 - so
16:06 - um with that in mind the future is now i
16:08 - encourage all of you guys who are
16:10 - thinking about trying to deploy services
16:11 - out into the cloud to consider
16:13 - serverless from you know whatever vendor
16:14 - that you feel is good it's all about
16:16 - trying to improve the agility with which
16:18 - we can deploy services and build
16:20 - services which we can trust will operate
16:22 - well at scale
16:23 - um node.js can build more with
16:26 - serverless since you're already in good
16:28 - shape because you know the language of
16:29 - choice on this platform so you're
16:30 - already kind of ahead of the game
16:32 - and
16:33 - i think
16:34 - serverless has taken advantage of the
16:36 - fact that node.js and javascript already
16:38 - had this great big open community and we
16:41 - should try to do our best to encourage
16:43 - that community as node.js you know
16:45 - community members to stay open so
16:48 - um thank you everyone for listening
16:50 - thank you for microsoft for paying for
16:52 - me to come here um and if you ever want
16:54 - to learn more about what i do for my day
16:56 - job check out functionsasher.com or
16:58 - tweet at me with candycodes thanks
17:00 - everyone
17:05 - you

Cleaned transcript:

i'm chris anderson i'm a program manager on the azure functions team um i actually started the functions project about two years ago and it was mainly kind of the background was as i was working on a product that was called web jobs which was for kind of doing background jobs in the cloud and it was mostly just an sdk framework you deployed onto your own you know instances your own vms you had to manage everything yourself and one day we tried to add node.js into it and ended up making it really really hard to manage it actually made it worse but the end product was really really cool in terms of what you wrote and so we discovered if well if we just run this as a service we'll end up with a better product and i'll kind of talk about why we ended up going with node.js as far as how that goes so serverless node.js i choose you there's really i think two kind of big things it also had to do a lot with javascript and then we'll also talk about node.js javascript i like to say is kind of the english of programming languages a lot of people can speak at least a little bit of it even if it's bad right it's not necessarily the perfect language but it's a language which is relatively easy to learn for a lot of people um it might not even be the most widely used language but it's used by a good section of the developer community out there especially in the kind of web application space so it's a very very useful language for us to choose it doesn't require any compilation which means that a lot of the goal that we had was for people to go ahead and get cloud running in the cloud as fast as possible our goal was that you were basically in the editor doing something in like 30 seconds and that you were actually walking away within five minutes having done something and have a service up and running in the cloud that meant that we having compilation having build even having an ide meant slow down to that time and so we really focused on that quick path first since then we've kind of gone back and said okay for people who do want to work in an ide for people who do want to have build and compilation they still can but we started that initial goal of cl of code in the cloud as fast as possible um and then yeah there's just a giant great community of which you guys are all a part of of course which made it easy for everything that we needed to kind of work out of the box or we have available to us and then node.js node.js is a great platform in the case of serverless because it's a very very lightweight framework when you go and just measure how fast it is for node.js to come up and start running for the first time versus how fast it is for say like java or even you know csharp in some cases to come up and run for the first time it's much faster and for the general case and that's really useful because in serverless it's actually not serverless there's servers back there but those servers have to come up basically when your request comes in or when the event shows up you know for asynchronous messages it's not the worst thing in the world if there's like a a second or two or you know a couple seconds of you know latency coming up but a lot of the times people are using these functions in http scenarios in which case any latency of bringing the instance up is impacted by the user that's trying to use that function and we know that in the case of like websites latency as far as like responding to the request can mean lost users and so we know that it's critical for things to come up as fast as possible and node.js is already kind of designed to come up nice and quickly so it was a great platform out of the box there and the other nice thing was that it was very widely used at microsoft we make sure that we kind of reach all developers developers running on windows max linux and linux and node.js was already really really good at running in all those places and so we know that we'd have the best support across all those various places we needed to run so what can serverless do for node.js developers you know we kind of chose node.js to go out with in the first place um because it was very easy for us to get going and we wanted to kind of get started we wanted people to be able to use it easily but for people who are just kind of already using node.js already used to using it why is serverless a useful thing well one of the things that i came across frequently when i was building node.js applications was while going ahead and you know building my server is really easy kind of doing background process of messages can be a little bit less easy to manage and maintain there's a lot less tools out there which understand frameworks for doing background processing and i also have to kind of manage the scale for those background processing it's really easy to scale on like http request messages in a lot of platforms but you know cues and things like that that can be a little bit trickier to get right and so serverless makes it so i can basically go ahead and have a function up and running and it scales dynamically to whatever i need to and since you pay for execution and most serverless frameworks out there you don't really have to ever overpay for your system it also means that things are much lighter endtoend frameworks become a lot less necessary since all i'm really necessarily having to go ahead and push is just a function i'm not having to worry about like having that full stack of things that's going to manage like the various http entry points i'm not having to go through there and worry about a lot of extra code a lot of the times especially if you take advantage and for instance at microsoft we have azure functions where we have actually bindings to various services you never actually have to bring in the sdks to talk to for instance like cosmos db which is a giant you know document based database that we have you can actually use our bindings and just return a json object from the function and it'll get stored to the database for you so you never actually have to deal with working with those sdks it makes it very quick and of course horizontal scaling high scale is easier than ever basically with functions we just keep on throwing new instances at you and you never actually have to see the instance account worry about managing the vms worry about patching things it just keeps on scaling and then as soon as you have no more traffic on your service the vms disappear and you don't really worry about when they're there and when they're not you'll just kind of scale out when you actually have requests when you actually have usage this has turned out to be really useful in a lot of cases one of the first places that someone came up to me and it's like oh yeah azure functions it was really useful because of that whole kind of scaling thing was actually at a conference someone had built the conference app using functions and it was really great because they got a lot of like random requests especially like the week leading up to the conference from the various project managers trying to do the conference and he would just go through there and add a random function to serve that purpose and then after the conference was done he'd have to worry about scaling down the vms or anything like that he just kind of walked away and he left it there in case people wanted to use it randomly and then he didn't get charged anything for those functions running in the meantime so it's very useful for those kinds of use cases um and just to give you a sense before i kind of move on to what can node.js do for serverless i just wanted to give you a simple sense of how this can work here i actually have the functions runtime running locally on my machine this is the simplest kind of demo of hello world so i've got the very classic case here your function in this case let me pull up this one your function in this case is just going to be exporting a function from the module so i've got a context object coming in a request object coming in i can go ahead and do some log statements i'm able to access it we actually purposely made the request object look like a connect or an express object so that way if you wanted to use connector express middleware those things can still work with inside of your function without having to do too many tweaks and then my response here i'm just able to add headers straight to the message do my body here and it just kind of works like normal and so here let's just kind of reset the demo you'll see here in the bottom right that i'll see various um kind of things scroll by here the way that i got this running is i just run funko start you can actually do npm install azure dash functions dash core dash tools dash d and then you'll have functions running on your local machine and you can run funk knit funk new function and then do funko start once you have your project going so that spins up it's listening on 7071 we can see our api hello function that's been hung out there because this function is called hello based off of the folder path and i can go ahead and run and it sees says please pass the name on the correction on the request body if we want to inspect why we got that logic back we can actually go ahead and just hit attach in vs code when you use functions we go ahead and actually drop a vs code file there with the launch.json which is preconfigured to talk to your local functions runtime so there's no setup there to get debugging working it just automatically drops that launch.json file there so now that i'm connected i can go ahead and hit enter again and you see that i've got this breakpoint set up here i can now go ahead and inspect the request object look at the query object and i see that it's actually empty there's nothing on there a query object which is why it's falling into this else statement which is saying please pass the name on the query string or the request body so we can fix that real quick and now when i do that i can go ahead and see the query object now has chris on it and name is now equal to chris and so if i go ahead and continue we now see hello chris with a question mark at the end of it and it's still really fast to develop this i don't need to go ahead and hit funko start every single time that i've got to change to my function i can go ahead and let's get excited rather than kind of asking questions about it hit enter and you can see there i go i got my exclamation mark so the host is sitting there constantly listening to file changes so it's very easy to just write some code change write some code change um the only nice thing with this is it's not just http though i'm using http right now because it works entirely locally and the internet's been kind of like in and out throughout the conference but it can also still listen to all your q services it'll listen to event hubs in fact if you configure it to listen to your production services you'll actually kind of end up with n plus one things because it'll actually collaborate in real time with the cloudbased instances the same way that all the cloud instances themselves collaborate with each other using a storage account with the covers so now i just kind of understand what this you know this is kind of what led us to you see how easy it is to go ahead and write this code if this was you know a language that required building for instance like you know java this would still be relatively easy to be a build step and then i'd have to go and hit run but because i can script it and i can live edit things we get a really nice kind of fast inner loop in terms of developing goes and then i can just deploy it up to the cloud or even do this from the portal itself if the internet was working a bit better so from there let's kind of kind of talk to about the next phase of things in terms of what has been weird about running node.js and serverless together and what can the node.js ecosystem do to think about serverless when you know we're doing the things that our community does um one of the things that i really liked about node.js and javascript was how easy it was for you know new developers kind of walk up and get started with it one of the nice things about node.js and javascript is since there's a lot of client developer clientside developers kind of already knowing javascript already used to how that language works it's very easy for them to walk up to node.js to have them start building their own backends if they needed to do that serverless in some ways kind of has a similar kind of thing about it where serverless is very easy to walk up to and start building a cloud service and get up and running you don't need to worry about setting up a vm and managing it you don't need to worry about antivirus software you don't need to worry about firewalls all that's done for you by the managed service so now those same cloud you know those same developers who might be able to get up that single vm but couldn't really reach scalable systems can now use serverless technology to deploy systems at scale for both you know frontend facing http applications but also backend systems that might have even been farther away from their area of expertise it's a very accessible thing to get up to so because node.js is already kind of the star language of serverless we're going to see serverless as it's becoming more popular bring even more people into the server into the node.js ecosystem but there's a couple of gotchas that those new people coming into the node.js ecosystem are going to have to deal with and some of them is that node.js kind of started ignoring some of the lessons that we learned from the web space and that we got kind of very comfortable with the fact that since these things are going to be long running on servers it doesn't matter if our packages are huge and we load everything in the world and through that process and you know as long as it's not taking up too huge of a memory footprint like who cares how many tiny little files that we have and things along those lines well as i kind of mentioned before about node.js is actually great because for the simple cases it comes up very quickly for the cases where you decide to go ahead and you know import low dash and import every single giant framework that you can think of into memory it takes actually a while to even just read those things from disk and the memory footprint then means that people who are using those libraries inside of functions have to pay more because they have to pay for that memory footprint to actually exist so if you load giant libraries into memory more than ever you're not having to pay for that if i can choose a library which is you know one megabyte versus a library which is you know maybe 30 megabytes that could mean big differences for my end of month bill in the case of serverless and so now as you know any package authors who are listening to this talk or here in the audience it really pays to be thinking about how can we go ahead and reduce what actually gets loaded there's lots of things that can help with this you know if we're now using you know the new kind of uh you know common gs stuff for it it'll only load the parts that it needs to in memory those are really good things i actually wrote a tool that's called azure functions pack which i actually go through there and web pack your code all together it doesn't necessarily do as much to reduce memory footprint for that you'd want to go ahead and run uglify on top of that the result of that web package but it does reduce the load time i actually did a lot of kind of testing of various impacts of i took the four largest node modules i could find i put them into a function and measured it on an i7 with an ssd so very very fast system it still took about two seconds for that function to come up for the first time because i had to start node.js and node.js had to read all that stuff into memory when i web packed it it actually took about 100 milliseconds so it's a giant improvement and when you're running inside a serverless on maybe a little bit more of commodity hardware you're going to be running it's going to take even longer to load those modules into memory so and a lot of the sales providers actually have limits on the amount of content you can actually put up there in the first place in the case of functions we don't actually have any file in the sizes we haven't hit those issues but they have existed in other platforms and so it's important to think as we're writing these packages especially if we want these packages to work instead of serverless um we need to be thinking about what those things are going to do there's actually a couple of examples of this out there in the wild already beyond just like things that i've seen as i've tried to work with customers using functions um like on postgres one of the bigger angular threads there for the the pg module is someone talking about hey this thing started consuming a lot of sockets without me realizing it because my function was kind of coming in there and using it and using it using it and opening up sockets and not closing them and to be fair they were misusing the library they didn't read the documentation properly they weren't kind of using it the way they thought it to but they used it the way that it felt natural to use it inside of their function and so we might want to think of especially for writing a package which we know will be harmful and to use wrong in serverless to include a section about hey if you're trying to use this inside of one of the service platforms like you know here's instructions on how to create a singleton instance of it outside the function so it stays in their memory here's how to write yours through the code for it to go ahead and close its connection before the function's completed and you know some examples can really help a long way with making sure that anyone news coming to this ecosystem from serverless into node.js has a good time because it'll pay to make sure that people are you know successful in that way and then there's one other thing that we should be kind of mindful of is that you know node.js and serverless and javascript has really gotten very far and the community's been doing really well because i think of the strong open source nature of things and how easy it is to get access to the parts the system runs on you know the even the core brains of like you know you know chromium and things like that or open source it's really really great just how fast and how much all of us are empowered to contribute to the platforms that we run on um and since a lot of the service implementations today are very vendor specific there is some risk there so it's something that's worth keeping in mind there's you know people out there that are thinking about problem uh ways to address some of these problems um like service framework is one of those cases there's you know apex which is more of a gobased one from tj a couple different examples um so one of the things that we're trying to do from the function side of things is even though we're a vendor and we are building a specific implementation that's mainly designed to run on azure it is all open source and we are working really hard to try to build it so it is a bit more of a portable system and so we really do want to make sure that you don't really feel like you're necessarily locked in even though it's built to run really well on azure we are trying to make sure that we at least build it in the open source and eventually we can work towards a way where it's even open platform so um with that in mind the future is now i encourage all of you guys who are thinking about trying to deploy services out into the cloud to consider serverless from you know whatever vendor that you feel is good it's all about trying to improve the agility with which we can deploy services and build services which we can trust will operate well at scale um node.js can build more with serverless since you're already in good shape because you know the language of choice on this platform so you're already kind of ahead of the game and i think serverless has taken advantage of the fact that node.js and javascript already had this great big open community and we should try to do our best to encourage that community as node.js you know community members to stay open so um thank you everyone for listening thank you for microsoft for paying for me to come here um and if you ever want to learn more about what i do for my day job check out functionsasher.com or tweet at me with candycodes thanks everyone you
