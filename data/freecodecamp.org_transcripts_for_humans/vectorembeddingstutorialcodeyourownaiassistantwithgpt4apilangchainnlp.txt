With timestamps:

00:00 - learn about Vector embeddings which
00:02 - transform Rich data like words or images
00:04 - into numerical vectors that capture
00:07 - their Essence this course from anikubo
00:09 - will help you understand the
00:10 - significance of text embeddings showcase
00:13 - their diverse applications guide you
00:15 - through generating your own with open Ai
00:17 - and even delve into integrating vectors
00:19 - with databases by the end you'll be
00:21 - equipped to build an AI assistant using
00:23 - these powerful representations so let's
00:26 - begin hi everyone and welcome to this
00:29 - course all about vector embeddings by
00:31 - the end of this course you will be able
00:33 - to understand what Vector embeddings are
00:35 - how they are generated as well as
00:37 - understand why we even care about them
00:39 - in the first place we are going to do
00:40 - this thanks to visual explainers as well
00:42 - as some hands-on experience by building
00:44 - out a project that uses Vector
00:46 - embeddings to submit your understanding
00:47 - of them by the end
00:49 - my name is Anya Kubo and I'm a software
00:51 - developer and course creator on YouTube
00:53 - as well as on codewithanyu.com and I'm
00:56 - going to be your guide to this hot but
00:58 - slightly complex topic so before we get
01:01 - going let's just have a quick look at
01:03 - what this course will cover so first off
01:05 - we're going to learn what Vector
01:06 - embeddings are in the first place and
01:08 - what they're used for after we
01:10 - understand that I will show you what a
01:12 - real Vector embedding looks like and
01:14 - show you how to make one yourself and
01:17 - after that I will delve into why
01:18 - companies might want to store Vector
01:20 - embeddings in a database as well as show
01:22 - you how to store Vector embeddings in
01:24 - your own database just as a company
01:26 - focused on AI word next we will take a
01:29 - quick look at a popular package called
01:30 - Lang Chen that will help us with the
01:32 - next part of making an AI assistant in
01:34 - Python and if you don't know any python
01:37 - don't worry I'm going to talk you
01:38 - through it step by step okay so a lot to
01:42 - learn but by the end you should be an
01:43 - expert in this aspect of AI development
01:46 - so what are we waiting for let's do it
01:49 - what are vector embeddings in computer
01:52 - science particularly in the realm of
01:54 - machine learning and natural language
01:55 - processing or NLP for short Vector
01:59 - embedding is a popular technique to
02:00 - represent information in a format that
02:02 - can be easily processed by algorithms
02:05 - especially deep learning models this
02:08 - information can be text pictures video
02:10 - and audio and much more let's look at
02:13 - text embeddings first so in terms of
02:16 - text we can create a text embedding that
02:18 - will give us more information about our
02:20 - word such as its meaning that computer
02:23 - can understand a word will go from
02:25 - looking like this for us humans to this
02:28 - for computers so essentially the word
02:30 - food is represented by an array of lots
02:33 - and lots of numbers but why do this well
02:37 - think about it this way say we have this
02:39 - text right here diary went to town on
02:42 - foot she set off early in the morning to
02:44 - beat the rush to the shop she wanted to
02:46 - be sure to get the best lettuce and
02:48 - tomatoes for her grandfather's recipe
02:50 - now say you want a computer to scan this
02:53 - for words with the closest meaning if
02:56 - you ask a computer to come back with a
02:57 - word similar to food for example you
03:00 - wouldn't really expect it to come back
03:01 - with letters or tomatoes right that's
03:04 - what a human might do when thinking of
03:06 - similar words to food a computer is much
03:09 - more likely to look at the words in the
03:10 - text lexicographically kind of when you
03:13 - scroll through a dictionary and come
03:15 - back with foot for example this is kind
03:18 - of useless to us we want to capture
03:20 - words semantic meaning so the meaning
03:22 - behind the word Texas embeddings
03:25 - essentially represent that thanks to the
03:27 - data captured in the super long array by
03:30 - creating a text embedding of each word I
03:33 - can now find words that are similar to
03:35 - food in a large Corpus of texts by
03:37 - comparing text embedding to text
03:39 - embedding and returning the most similar
03:42 - ones so words such as lettuce instead of
03:45 - foot will be more similar now you might
03:48 - still be wondering what even are these
03:50 - numbers what does each one represent
03:52 - well that actually depends on the
03:54 - machine learning model that generated
03:56 - them to understand how these numbers can
03:58 - help us find words that are similar
04:00 - however let's look at this fantastic
04:02 - visual explainer from Jay Alamar I
04:04 - absolutely love this explainer so full
04:06 - credit to him for these next few
04:08 - illustrations it really is great imagine
04:11 - you are also conduct a personality test
04:13 - similar to that of the Big Five
04:15 - personality traits test that rates your
04:17 - openness agreeableness conscientiousness
04:20 - negative emotionality and extroversion
04:22 - the test requires a score from 0 to 100
04:25 - on each of the five traits in order to
04:27 - get a good understanding of a person's
04:29 - personality let's start by looking at
04:31 - the extroversion trait first
04:33 - imagine Jay gives himself a 38 out of
04:36 - 100 as its introversion extroversion
04:38 - score let's show this in one dimension
04:41 - like so on the left
04:43 - now let's switch this out to be a score
04:45 - of minus one to one now it is hard to
04:49 - know a person from just one personality
04:51 - trait right so let's add another one and
04:53 - then turn another dimension the aguria
04:56 - winners score of a person for example or
04:58 - any of the other five traits great and
05:01 - already you can start to get a better
05:02 - understanding of Jay's personality now
05:05 - say we have three people here are their
05:07 - personalities plotted out based on two
05:09 - personality traits so we can see them on
05:12 - a two-dimensional graph and we can also
05:14 - see them on the right in numeric
05:16 - representations from minus one to one
05:18 - now CJ got hit by a bus and we miss our
05:22 - friend I want to replace them with a
05:23 - person with a similar personality dark I
05:26 - know but you get the idea
05:28 - when dealing with these numerical values
05:30 - or vectors a common way to calculate a
05:33 - similarity score is using cosine
05:35 - similarity this is the formula for
05:37 - getting cosine similarity
05:39 - using cosine similarity you will see
05:41 - person one is more similar in
05:43 - personality to J than person two but
05:47 - still two personality traits probably
05:49 - aren't enough let's use all five trait
05:52 - scores so we can use five dimensions for
05:54 - our comparison
05:55 - now the problem is this is kind of hard
05:58 - to draw let alone think about on a graph
06:00 - this is a common challenge in machine
06:02 - learning where we often have to think in
06:04 - higher dimensional space however cosine
06:07 - similarity still works so we can get a
06:09 - numerical value by passing through the
06:12 - vectors for each person we want to
06:13 - compare to each other into the formula
06:16 - to get one numeric value which
06:18 - represents similarity
06:20 - so now by comparing J to person 1 J to
06:23 - person 2 and J2 person 3 we can see
06:26 - which person is most similar to him
06:28 - great now that we understand this
06:31 - concept let's look at the actual text
06:33 - embedding
06:34 - so for example this is the word food
06:36 - generated by open AIS create embedding
06:40 - so as you can see it's an array of lots
06:42 - and lots of numbers from -1 to 1. the
06:45 - meaning behind each numeric
06:46 - representation varies based on which
06:48 - model generates them here are some of
06:51 - the other models you can use to create
06:53 - text embeddings so you've got open AI as
06:55 - we've seen huertevac and glove so as we
06:59 - now know we can use these text
07:00 - embeddings to compare them to other text
07:02 - embeddings just like we did with the
07:04 - example of comparing personality trait
07:06 - to personality trait apart from instead
07:09 - of capturing a personality the meaning
07:11 - of a word is captured instead
07:13 - there is another cool benefit to Turning
07:16 - words into numeric representation one of
07:19 - them being that we can now apply math to
07:21 - them take for instance this now
07:23 - well-renowned example so King minus man
07:26 - plus woman equals Queen
07:28 - so for example here you can see how you
07:31 - can take the word King and minus the
07:34 - word man and replace it with the word
07:35 - woman and you get Queen this is truly
07:39 - incredible and it's all thanks to text
07:41 - embeddings
07:42 - we can use code in order to pass through
07:45 - the words King and woman and subtract
07:47 - man and then we get a bunch of words
07:49 - returned to us each with a similarities
07:51 - score
07:52 - Queen is the most similar and hence it
07:54 - has the highest score pretty cool right
07:57 - let's move on by actually talking about
07:59 - what Vector embeddings can be used for
08:02 - so far we have looked at Texan beddings
08:04 - but Vector embeddings actually cover a
08:07 - lot more text is just one of the things
08:09 - that we can vectorize we can vectorize
08:11 - sentences documents notes and graphs
08:13 - images and even our faces we have word
08:16 - embeddings like we just seen this is one
08:19 - of the most popular applications why are
08:21 - the baddings like what back or glove
08:23 - convert words into dense vectors where
08:26 - semantically similar words are closer in
08:28 - the vector space for instance we saw
08:31 - king and queen would have vectors that
08:33 - are closer than King and paper
08:36 - next we also have document and sentence
08:38 - embeddings methods like Dr vac but and
08:42 - sentence but can represent whole
08:43 - documents or sentences as vectors this
08:47 - can be used in document classification
08:48 - semantic search and more next we also
08:52 - have graph embeddings nodes in a graph
08:55 - can be represented as vectors
08:57 - applications include recommendation
08:59 - systems social network analysis and more
09:02 - here are some of the primary
09:03 - applications of vector embeddings we
09:06 - have recommendation systems embeddings
09:08 - can be used to represent users and items
09:10 - like movies books or products the
09:13 - similarity between user and item
09:14 - embeddings can help in making
09:16 - personalized recommendations
09:18 - we also have anomaly detection if you
09:21 - can represent data as vectors you can
09:23 - measure instances or similarities to
09:25 - detect outliers or anomalies in data we
09:28 - also have transfer learning pre-trained
09:31 - embeddings especially in the context of
09:32 - deep learning models can be transferred
09:35 - to another task to Kickstart learning
09:37 - especially when the target task has
09:39 - limited data and also amazingly we have
09:42 - visualizations high dimensional data can
09:45 - be converted into 2D or 3D embeddings
09:48 - using techniques like tsne or PCA to
09:51 - visualize clusters or relationships in
09:53 - the data we also use it for information
09:56 - retrieval by embedding both queries and
09:58 - documents in a shared space one can find
10:00 - documents that semantically match the
10:02 - query even if they don't share exact
10:04 - keywords and of course we can use them
10:07 - for natural language processing tasks
10:09 - tasks like text classification sentiment
10:11 - analysis named entity recognition and
10:14 - machine translation benefit from
10:16 - embeddings as they capture semantic
10:18 - information and relationship between
10:20 - words we also have audio and speech
10:22 - processing audio clips can be converted
10:24 - to embeddings for tasks like speaker
10:27 - identification speech recognition or
10:29 - emotion detection and finally we can use
10:32 - it for facial recognition face
10:34 - embeddings can represent a face as a
10:37 - vector making it easier to compare faces
10:39 - and recognize identities so a lot of
10:42 - things that Vector embeddings can be
10:44 - used for we're going to create a few of
10:46 - Our Own in the lesson coming up but the
10:48 - main takeaway here is that the core
10:50 - advantage of vector embeddings is that
10:52 - they provide a way to transform complex
10:54 - multi-dimensional and often discrete
10:57 - data into a lower dimensional continuous
10:59 - space that captures semantic or
11:01 - structural relationships within the
11:03 - original data
11:05 - next up how do we generate Vector
11:07 - embeddings I'm going to show you how
11:09 - using openai's Create embedding okay so
11:13 - here we are on open AI so please go
11:15 - ahead and log in or sign up if you
11:17 - haven't before and you'll be taken to
11:19 - this landing page now once here what we
11:23 - are going to do is interact with the API
11:26 - so just go ahead and click that and the
11:29 - first thing you will need to do is just
11:31 - make sure you have an API key so under
11:34 - your username here just view your API
11:37 - keys and go ahead and create a new
11:39 - secret key I'm gonna call this demo key
11:44 - like so so that is the name of my key
11:46 - and I'm going to create a secret key and
11:48 - save this somewhere safe so please go
11:51 - ahead and do the same just save your API
11:53 - key and once you're done with that click
11:55 - done
11:56 - you can of course delete previous API
11:59 - keys to revoke access to them so that's
12:01 - what I'm going to be doing with this one
12:03 - so that you can't use it in the future
12:05 - it will be deleted
12:08 - now once you have that let's go back to
12:10 - our API reference and what we're going
12:12 - to do is create embedding so let's click
12:14 - on embeddings here here's the URL if you
12:17 - are lost just copy that into your
12:19 - browser and we are going to be using the
12:23 - embedding object so essentially here is
12:27 - the code that we're going to use so it's
12:30 - right here we have in node.js you can
12:32 - have it in python or you can also have
12:34 - it in curl it is up to you whichever one
12:37 - you would prefer to do so let's just go
12:40 - ahead and use this version first so this
12:44 - is what we're going to be using this is
12:46 - the request we're going to write and
12:48 - this is the response that we are going
12:50 - to get based on the input we passed
12:54 - through okay so essentially the food was
12:56 - delicious and the waiter dot dot dot is
12:59 - now represented by this embedding this
13:01 - array of numbers right here
13:04 - so let's go ahead and do it I'm going to
13:06 - copy this just make sure that it's
13:08 - copied let's get up our terminals I'm
13:10 - just going to make this a little bit
13:11 - bigger for you let's paste that in and
13:14 - now here we need to replace the open AI
13:17 - API key with our own
13:19 - so let's just go ahead and do that
13:24 - it's going to navigate
13:28 - to that piece of text
13:30 - delete it all just like so and paste it
13:34 - and let's just use the same input for
13:37 - now so hit enter and amazing so there is
13:41 - the array of essentially numbers from -1
13:44 - to 1 that make up the food was delicious
13:47 - and the waiter dot dot so there we go
13:50 - there is that whole object the full
13:53 - thing right here
13:54 - it is also telling us how many tokens we
13:57 - have used in order to create that so
14:00 - great I'm just going to clear that out
14:03 - so let's paste that in again in fact
14:05 - what we can do is just press up and now
14:08 - let's change the input to something else
14:11 - so for example let's just use an example
14:13 - from the beginning of this tutorial
14:16 - let's just go with food
14:21 - so third hit enter and that is the text
14:25 - embedding for food okay it's this whole
14:28 - array and we have used exactly one token
14:31 - to create that amazing
14:34 - now it's time to look at vectors and
14:36 - databases with the rapid adoption of AI
14:39 - and The Innovation that is happening
14:41 - around large language models we need at
14:44 - the center of it all the ability to take
14:46 - large amounts of data contextualize it
14:49 - process it and enable it to be searched
14:52 - with meaning
14:53 - generative AI processes and applications
14:55 - that are being built to natively
14:57 - incorporate generative AI functionality
14:59 - or rely on the ability to access Vector
15:02 - embeddings a data type that provides a
15:05 - semantics necessary for AI to have a
15:08 - similar long-term memory processing to
15:10 - what we have allowing it to draw on and
15:12 - record information for complex task
15:14 - execution
15:16 - as we now know Vector embeddings are the
15:18 - data representation that AI models such
15:21 - as large language models use and
15:23 - generate to make complex decisions like
15:26 - Memories in the human brain there is
15:28 - complexity Dimension pattern and
15:30 - relationships that all need to be stored
15:32 - and represented as part of the
15:34 - underlying structures which make all of
15:36 - this difficult to manage
15:38 - that is why for AI workloads we need a
15:41 - purpose-built database or brain designed
15:44 - for highly scalable access and
15:46 - specifically built for storing and
15:48 - accessing these vector embeddings Vector
15:51 - databases like data Stacks astrodb built
15:54 - on Apache Cassandra are designed to
15:56 - provide optimized storage and data
15:58 - access capabilities specifically for
16:00 - embeddings
16:02 - now that we understand how important it
16:04 - is to store these vectors in the right
16:06 - type a database let's get to setting one
16:09 - up ourselves in preparation for creating
16:12 - our AI assistant so let's do it so fast
16:15 - off I'm just going to navigate to data
16:18 - stacks and log in please go ahead and
16:21 - sign up if you haven't already this is
16:24 - what your screen should look like once
16:27 - you are signed in and you can see all
16:29 - your options here as well along with
16:32 - your username and so on so as you will
16:35 - see I've previously created a bunch of
16:38 - databases on here already but don't
16:40 - worry I'm going to show you how to get
16:41 - started completely from scratch
16:44 - so all you're going to do is Click
16:46 - create database here and make sure you
16:49 - have Vector database selected and once
16:52 - you have that it's super simple just go
16:54 - ahead and name your database making sure
16:57 - to use the correct characters so for
16:59 - example it won't let you use certain
17:01 - ones you have to use ones that are
17:04 - allowed you will get a little prompt
17:06 - message if you do use an incorrect one
17:08 - I'm just going to go ahead and call my
17:11 - database Vector database
17:13 - now we have to create a key space name
17:17 - that will go inside our database and
17:20 - once again just make sure to name it
17:23 - with the correct conventions so I'm just
17:25 - going to call it search as that was what
17:27 - we are creating we are creating a vector
17:30 - search database so I'm just being super
17:32 - literal with my naming conventions and
17:35 - now I'm going to pick a region that is
17:38 - closest to me so I'm going to go ahead
17:40 - and select Us East one and then just
17:43 - click create database and that's it
17:45 - that's really all there is you will see
17:47 - my database is pending right here we
17:50 - have done it we created our database I'm
17:53 - going to leave that running and come
17:55 - back to it when it's time to use it and
17:57 - pending has gone from pending to active
18:01 - for now let's carry on with a little bit
18:03 - of learning
18:04 - before we dive into creating an AI
18:07 - project I want to talk to you a little
18:09 - bit about line chain
18:10 - Lang chain is an open source framework
18:12 - that allows AI developers to have better
18:15 - interactions with several large language
18:16 - models or llms like open ai's gbt4 for
18:21 - example we can use the M python or
18:24 - JavaScript which is great news for us
18:25 - Developers what do I mean by better
18:28 - interactions however well for one it
18:30 - allows developers to create chains which
18:33 - are logical links between one or more
18:35 - llm
18:37 - you can even use it to load documents
18:39 - such as PDFs or csvs for example to
18:42 - change to each other or an llm heck you
18:46 - can even use it to split up documents
18:47 - and much more you can have basic chains
18:51 - or more advanced chains we'll be
18:53 - creating our own chain soon enough in
18:55 - this course
18:56 - for now just know that langtune's
18:58 - superpower lies in allowing you the
19:00 - developer to chain together different AI
19:02 - large language models external data and
19:05 - prompts in a structured way in order to
19:07 - create cool and Powerful AI applications
19:10 - such as an AIS system for example that
19:13 - not only uses data from the internet but
19:15 - perhaps an essay that you wrote as well
19:17 - that we can then feed into it in order
19:20 - for the AI assistant to answer questions
19:22 - about it too
19:23 - okay so we finally gathered enough
19:26 - knowledge in order to proceed in
19:28 - building an AI assistant built in Python
19:31 - so let's go ahead and do it just to
19:34 - recap this AI assistant is going to
19:36 - essentially be an AI assistant that will
19:39 - help us search for similar text in a
19:42 - data set okay so once again we are going
19:46 - to be able to get some data break it up
19:50 - into little chunks save it in a database
19:53 - in order for us to essentially perform
19:56 - Vector search on it thanks to packages
19:59 - such as Lang chain don't worry if that's
20:01 - a lot I'm going to be explaining
20:02 - everything step by step as we do it so
20:06 - first off let's go back to our database
20:08 - in order to continue with this tutorial
20:12 - so we've already created a serverless
20:14 - database the next thing we're going to
20:17 - do is actually learn to connect with it
20:19 - from an external source and in order to
20:22 - do that we need to essentially get our
20:24 - token so please go ahead and get that
20:27 - token we need to go to the connect Tab
20:30 - and we're simply going to get an
20:32 - application token using the generate
20:34 - token button in the quick start section
20:36 - so you can save this in any way you want
20:39 - just make sure it's saved somewhere safe
20:42 - now once we have got that token saved we
20:46 - need to get a secure connect bundle okay
20:49 - so just go ahead and do that get your
20:52 - bundle and once again save this
20:55 - somewhere safe this time however we're
20:57 - going to down with the secure bundle
20:59 - because we're going to point to this
21:02 - somewhere on our computer so just
21:04 - download the whole thing onto your
21:06 - computer into your downloads wherever
21:08 - you want
21:09 - great now that we have that once again
21:12 - we are going to have to get our API key
21:15 - so as a refresher all you're going to do
21:18 - is head over to the openai.com page
21:23 - and then once you have signed in you
21:26 - will see this platform and you're going
21:27 - to go to API and then once again we are
21:31 - going to be working with embeddings
21:33 - however we are not going to be doing a
21:37 - call request from here we are going to
21:40 - Simply feed in our API token in order
21:43 - for Lang chain to do its thing instead
21:45 - so just go ahead and navigate to your
21:47 - username view your API Keys create a new
21:50 - one so this time I'm just going to call
21:52 - it demo copy this key okay and keep it
21:56 - somewhere safe and then let's go back to
21:58 - data Stacks once more
22:00 - okay
22:02 - so great we have done everything that is
22:05 - to be done here now let's create a
22:08 - python script using Lang chain and
22:10 - Castor IO
22:12 - so I'm just going to go ahead and get up
22:15 - my terminal once more and navigate to a
22:18 - directory where I want to store this
22:20 - this time I'm going to store it in a
22:23 - another directory which is going to be
22:25 - webstorm projects and I'm going to
22:27 - create a
22:29 - directory I'm going to call it search
22:31 - python so I'm going to use the mukter
22:33 - command and then I'm going to go into
22:36 - that project so using the CD command I'm
22:38 - just going into search python or
22:40 - whatever you called your project and
22:42 - then I'm just going to open it up using
22:44 - Code dot which is the shortcut to
22:46 - opening this up in vs code
22:48 - and great
22:50 - so once we are in vs code I'm just going
22:54 - to make sure that this is enabled for
22:56 - python
22:58 - so in order to work with python files
23:00 - let's just go ahead and create a python
23:02 - file first I'm going to call it index.py
23:05 - giving it the py extension so that our
23:07 - code editor knows to treat this as a
23:09 - python file
23:10 - next I'm just going to go here and click
23:15 - on the prompt and this has prompted me
23:17 - it's saying that you know it's
23:19 - recognizing we're working with python
23:21 - and it's asking us to install the
23:23 - recommended python extension so I'm just
23:25 - going to go ahead and install that and
23:27 - that is installing for me right now
23:30 - great once we have done that we are also
23:33 - prompted with a little checklist so I'm
23:35 - going to go ahead and just run through
23:37 - this it's telling me to create a python
23:39 - file which we've already done so the
23:41 - next thing we're going to do is actually
23:43 - add a python environment so let's go
23:47 - ahead and do that I'm just going to go
23:49 - with the first one
23:54 - so great once we have made our
23:57 - environment we can essentially do stuff
23:59 - like this so I'm going to write print
24:01 - hello so just go ahead and do the same
24:04 - as me this is a python script a very
24:06 - simple one and now if we run this by
24:09 - essentially pressing this little plus
24:11 - sign right here that will run the script
24:13 - and you will see Hello being printed in
24:15 - our terminal okay so that's really it
24:17 - everything's now ready to go we've been
24:19 - set up correctly great
24:22 - so now let's get to the media stuff now
24:25 - in order to install packages you can't
24:28 - write it in the script here okay if I go
24:31 - ahead and write pip install and all the
24:33 - packages packages python packages that
24:36 - we need and hit plus that will not work
24:39 - we need to do this in the terminal so
24:41 - just go ahead and paste that in like so
24:44 - just copy it out and hit enter and that
24:47 - will do its thing and install all the
24:49 - packages that we need the packages once
24:51 - again are Cas IO Data sets Lang chain
24:55 - open Ai and tick token so go ahead and
24:59 - wait for that to do its thing it will
25:01 - take some time and once it's ready we
25:03 - should be able to continue with our
25:05 - tutorial
25:07 - great now I'm just going to go ahead and
25:10 - rename this file for readability okay
25:12 - I'm going to rename this to mini.qa you
25:16 - will see there's another directory above
25:18 - me that has been generated with
25:20 - everything that we've done so just go
25:21 - ahead and rename your file if you want
25:23 - to continue having everything the same
25:26 - as me you don't have to though that is
25:28 - completely up to you
25:29 - great
25:31 - the next thing I'm going to do is just
25:33 - ask you to copy this code so here are
25:36 - some variables that we're going to need
25:38 - in order to continue with this tutorial
25:41 - so we have the Astro DB secure bundle
25:45 - part we've already downloaded this onto
25:48 - our computers so that is something that
25:51 - we're going to have to point to and the
25:54 - next thing we're going to have is our
25:55 - astrodb application token as well as our
25:57 - Azure DB client ID okay so these are
26:01 - things we're going to fill out from the
26:02 - stuff that we saw earlier from our
26:04 - secret things that we downloaded some
26:06 - other things that we're gonna have to
26:07 - add are our astrodb key space name as
26:11 - well as our open AI API key which we
26:13 - just recently saved as well and the
26:16 - final thing of our astrodb client secret
26:18 - to so now let's go ahead and fill all
26:21 - these out the open AI API key should be
26:24 - easy it starts with SK like so we
26:27 - previously saw this on the API keys on
26:30 - open AI next I'm going to just put my
26:33 - astrodb keyspace name which we also did
26:36 - together I named the keyspace name
26:38 - search so if you do the same as me just
26:41 - go ahead and write the string search
26:42 - there too of course this is all very
26:46 - down to you if you're copying along with
26:48 - me please feel free to copy these as
26:50 - they are apart from the open AI API key
26:53 - that would be unique to you it should
26:55 - start with SK and if you try use mine
26:57 - mine will now be deactivated or revoked
27:00 - as I showed you how to do earlier as
27:02 - well
27:03 - next we're going to have our Astro DB
27:05 - client secret so again this would be
27:07 - unique to you here is mine if you try to
27:10 - use mine it will not work for you but
27:11 - I'm pasting in here like so so you can
27:14 - kind of see the format that yours should
27:16 - look like as well next we're going to
27:18 - have our astrodb client ID which again
27:20 - will be unique to you but it should be
27:22 - kind of similar in length and kind of
27:24 - similar in terms of the characters that
27:26 - are used
27:27 - and once again we have our token which
27:30 - again is unique it should start with
27:32 - Astra CS like so so make sure yours does
27:36 - two but the characters after it will be
27:38 - unique to you and finally we have the
27:40 - path to our secure bundle so for this
27:44 - just uh go ahead and go to your
27:46 - downloads and find the ZIP file okay
27:49 - that we downloaded when it came to
27:52 - downloading a secure bundle and just
27:54 - drag it into the project and once that's
27:57 - dragged into the project I'm going to
27:58 - ask you to get the path to it okay so
28:01 - get the part like I am here and paste it
28:03 - in like so as mine's in my project now
28:06 - you will see the path to it in my
28:08 - project which is stored in webstorm
28:10 - project in the project called search
28:12 - python so that's all I've done make sure
28:15 - it is still a zip file so it will have
28:17 - the dot zip extension at the end
28:19 - and great so we are now done with all
28:22 - those variables let's continue next
28:25 - we're going to have to get some stuff
28:26 - from the Lang chain package that we
28:29 - installed so here's all the things that
28:32 - we're going to have to get from Lang
28:33 - chain so just paste that out like so
28:36 - we're going to be getting Cassandra from
28:39 - there we're going to be getting Vex the
28:40 - store index wrapper we're going to be
28:42 - also getting the open AI we're also
28:45 - going to be importing openai as the
28:47 - large language model and also importing
28:50 - open AI embeddings from Lang chain
28:52 - embedding
28:53 - great as well as using the Lang chain
28:56 - package that we installed we're also
28:58 - going to get stuff from Cassandra so
29:01 - what are we going to be getting from
29:02 - Cassandra where we're going to be
29:04 - importing the cluster and the plain text
29:06 - auth provider and finally from datasets
29:09 - we're going to import the load data set
29:11 - from the package data sets that we
29:14 - imported earlier
29:15 - great
29:16 - now we are going to essentially have to
29:20 - write some configuration in order for us
29:22 - to connect two data Stacks Astra and
29:26 - create an Astra session and to do this
29:28 - we are going to create a cluster so
29:31 - using the cluster import I'm just going
29:33 - to pass through two things I'm going to
29:36 - pass through the secure connect bundle
29:39 - which we can pass through as the
29:40 - variable we defined above so that is
29:43 - going to be passed through and another
29:45 - thing we're going to have to pass
29:46 - through is an auth provider okay and to
29:49 - create an auth provider we're going to
29:51 - use the plain text or provider that we
29:53 - imported from Cassandra and pass through
29:56 - our Azure DB client ID and astrodb
29:59 - client secret so this is essentially
30:02 - just configuration like I said in order
30:04 - to be able to communicate with our
30:06 - database that we created using
30:08 - datastacks Astra okay so again we're
30:12 - just using all these variables and
30:14 - passing them through in order to connect
30:15 - this to our Astra database
30:19 - great
30:20 - so once we've done that we have
30:22 - essentially created an asterisk session
30:24 - that we're going to be using later now
30:27 - we're going to have to essentially
30:29 - connect to open AI using our open AI API
30:33 - key so we're going to get the openai
30:35 - import and just pass through our API key
30:38 - like so so we're getting the varial so
30:40 - we're getting the variable that we
30:42 - defined above and passing it through and
30:44 - saving it under the variable llm
30:48 - the next thing we're going to do is do
30:50 - the same pool using the open AI
30:52 - embedding so once again we're just
30:54 - passing through our open AI API key and
30:57 - we're storing all of this under the
30:59 - variable my embedding great next we're
31:02 - going to essentially create a table so
31:06 - we're going to actually name our table
31:07 - in here we are going to pass through our
31:11 - astrodb key space which we know is the
31:13 - string search and we are now going to
31:16 - create a table and we're going to do
31:18 - that my table is going to be called QA
31:20 - mini demo so I'm passing through that
31:22 - string that is being created and being
31:24 - saved under the key space name of search
31:27 - in my Cassandra database
31:30 - great
31:31 - so let's just check this is working
31:35 - I'm just going to print loading data
31:37 - from hugging face and then I'm actually
31:40 - going to get some data from hugging face
31:42 - okay this is just one that exists on the
31:45 - Internet it's going to be some onion
31:47 - news so I'm just going to load that data
31:49 - set from hugging face and I'm just going
31:52 - to pass that through and I'm going to
31:54 - save it as my data set and then I'm
31:57 - going to get that data and I'm going to
31:59 - just get the headlines and in order to
32:02 - check that has worked first off I'm just
32:04 - going to print some texts okay so I'm
32:06 - just going to print some text generating
32:08 - embeddings and storing an astro DB and
32:10 - then I'm going to add those headlines to
32:14 - my Cassandra store so I'm essentially
32:17 - passing that through to my database to
32:20 - my astrodb database which is the
32:22 - Cassandra database and then once that is
32:25 - done I'm going to print those headlines
32:27 - so let's run that code okay that's all
32:31 - I've done I'm just printing stuff out to
32:32 - appear in our console log so you can
32:34 - kind of have this ability about what's
32:36 - going on in the back end so once that's
32:38 - done doing its thing we should get that
32:40 - being printed so we're going to print
32:42 - that text loading data from hugging face
32:44 - so we can kind of see where we are in
32:46 - the script essentially then we're just
32:48 - going to print generating embeddings and
32:50 - storing an astro DB and then it's going
32:53 - to tell us exactly the amount of
32:55 - headlines that we've inserted into
32:57 - astrodb is going to be 50 headlines
32:59 - great that is looking wonderful so we
33:03 - can now continue now I'm just going to
33:06 - essentially pass my store my Cassandra
33:10 - store into the vexer store index wrapper
33:12 - and save it under Vector index and now
33:15 - I'm going to paste in some code
33:17 - it's going to prompt us to enter a
33:20 - question and once we have entered that
33:22 - question it's going to do an if else
33:25 - statement in order to print out the
33:27 - question and then the answer as well as
33:29 - the documents by relevance so
33:31 - essentially we are going to write some
33:34 - text it's going to search the hugging
33:37 - face data so the onion news in order to
33:40 - bring back any similar text it's going
33:42 - to do a vector search on our database to
33:45 - find similar text to it based on the
33:48 - vector search so let's go ahead and do
33:51 - that that is the code for doing so
33:53 - please copy it and once we have finished
33:55 - copying it just run the script and
33:58 - you'll be prompted to enter a question
34:01 - so now we can enter a question let's go
34:05 - ahead and paste one that is what are the
34:08 - biggest questions in science the answer
34:10 - is I don't know so not a great answer
34:13 - really but it's going to return some
34:16 - documents by relevance in our database
34:19 - so you will have here the relevant
34:22 - document along with a temperature to
34:24 - show you how relevant that document is
34:26 - we have biologist torture amoeba for
34:29 - information on where life comes from so
34:31 - that's kind of similar to the question
34:33 - that we propose and then we also have
34:35 - study shows humans still have genes to
34:39 - grow full coat of something so
34:42 - essentially we are getting documents by
34:44 - relevance that exists in our database
34:47 - based on the question that we asked
34:49 - let's try another one so I'm just going
34:51 - to ask another question I'm going to ask
34:54 - what should I know about Silicon Valley
34:57 - Banks again it says I'm sorry I don't
35:00 - know but then it returns some documents
35:01 - by relevance again we get a similarity
35:04 - score and then we also get the document
35:08 - that is most relevant to the question
35:10 - I'm going to ask one more let's ask one
35:12 - about amoebas because I saw it being
35:14 - returned in the first document so our
35:17 - amiibos really are overlords and then
35:19 - the answer is no amoebas are not our
35:22 - overlords and again it returns documents
35:24 - by relevance so we see that same
35:27 - document being returned with a
35:29 - similarity score that is higher based on
35:32 - the question we can see that should be
35:33 - higher because we do have the word
35:34 - amoebas in there so there is no surprise
35:37 - that that document is coming back with a
35:39 - higher similarity score and then we also
35:41 - have some other documents being returned
35:44 - back with the lower similarity score
35:46 - so great we have managed to build an AI
35:50 - assistant that will essentially look in
35:52 - a database look for similar documents
35:55 - based on our question and if we want to
35:58 - have a look at what this database looks
36:00 - like under the hood so essentially has
36:02 - been vectorized right we have vectorized
36:05 - all the documents and then we are doing
36:07 - a vector search on it to bring back
36:09 - similar vexes to us that is exactly what
36:12 - we discussed in the beginning of this
36:13 - tutorial I hope you can now see how
36:16 - Vector search works behind the hood as
36:18 - well as on the front end to

Cleaned transcript:

learn about Vector embeddings which transform Rich data like words or images into numerical vectors that capture their Essence this course from anikubo will help you understand the significance of text embeddings showcase their diverse applications guide you through generating your own with open Ai and even delve into integrating vectors with databases by the end you'll be equipped to build an AI assistant using these powerful representations so let's begin hi everyone and welcome to this course all about vector embeddings by the end of this course you will be able to understand what Vector embeddings are how they are generated as well as understand why we even care about them in the first place we are going to do this thanks to visual explainers as well as some handson experience by building out a project that uses Vector embeddings to submit your understanding of them by the end my name is Anya Kubo and I'm a software developer and course creator on YouTube as well as on codewithanyu.com and I'm going to be your guide to this hot but slightly complex topic so before we get going let's just have a quick look at what this course will cover so first off we're going to learn what Vector embeddings are in the first place and what they're used for after we understand that I will show you what a real Vector embedding looks like and show you how to make one yourself and after that I will delve into why companies might want to store Vector embeddings in a database as well as show you how to store Vector embeddings in your own database just as a company focused on AI word next we will take a quick look at a popular package called Lang Chen that will help us with the next part of making an AI assistant in Python and if you don't know any python don't worry I'm going to talk you through it step by step okay so a lot to learn but by the end you should be an expert in this aspect of AI development so what are we waiting for let's do it what are vector embeddings in computer science particularly in the realm of machine learning and natural language processing or NLP for short Vector embedding is a popular technique to represent information in a format that can be easily processed by algorithms especially deep learning models this information can be text pictures video and audio and much more let's look at text embeddings first so in terms of text we can create a text embedding that will give us more information about our word such as its meaning that computer can understand a word will go from looking like this for us humans to this for computers so essentially the word food is represented by an array of lots and lots of numbers but why do this well think about it this way say we have this text right here diary went to town on foot she set off early in the morning to beat the rush to the shop she wanted to be sure to get the best lettuce and tomatoes for her grandfather's recipe now say you want a computer to scan this for words with the closest meaning if you ask a computer to come back with a word similar to food for example you wouldn't really expect it to come back with letters or tomatoes right that's what a human might do when thinking of similar words to food a computer is much more likely to look at the words in the text lexicographically kind of when you scroll through a dictionary and come back with foot for example this is kind of useless to us we want to capture words semantic meaning so the meaning behind the word Texas embeddings essentially represent that thanks to the data captured in the super long array by creating a text embedding of each word I can now find words that are similar to food in a large Corpus of texts by comparing text embedding to text embedding and returning the most similar ones so words such as lettuce instead of foot will be more similar now you might still be wondering what even are these numbers what does each one represent well that actually depends on the machine learning model that generated them to understand how these numbers can help us find words that are similar however let's look at this fantastic visual explainer from Jay Alamar I absolutely love this explainer so full credit to him for these next few illustrations it really is great imagine you are also conduct a personality test similar to that of the Big Five personality traits test that rates your openness agreeableness conscientiousness negative emotionality and extroversion the test requires a score from 0 to 100 on each of the five traits in order to get a good understanding of a person's personality let's start by looking at the extroversion trait first imagine Jay gives himself a 38 out of 100 as its introversion extroversion score let's show this in one dimension like so on the left now let's switch this out to be a score of minus one to one now it is hard to know a person from just one personality trait right so let's add another one and then turn another dimension the aguria winners score of a person for example or any of the other five traits great and already you can start to get a better understanding of Jay's personality now say we have three people here are their personalities plotted out based on two personality traits so we can see them on a twodimensional graph and we can also see them on the right in numeric representations from minus one to one now CJ got hit by a bus and we miss our friend I want to replace them with a person with a similar personality dark I know but you get the idea when dealing with these numerical values or vectors a common way to calculate a similarity score is using cosine similarity this is the formula for getting cosine similarity using cosine similarity you will see person one is more similar in personality to J than person two but still two personality traits probably aren't enough let's use all five trait scores so we can use five dimensions for our comparison now the problem is this is kind of hard to draw let alone think about on a graph this is a common challenge in machine learning where we often have to think in higher dimensional space however cosine similarity still works so we can get a numerical value by passing through the vectors for each person we want to compare to each other into the formula to get one numeric value which represents similarity so now by comparing J to person 1 J to person 2 and J2 person 3 we can see which person is most similar to him great now that we understand this concept let's look at the actual text embedding so for example this is the word food generated by open AIS create embedding so as you can see it's an array of lots and lots of numbers from 1 to 1. the meaning behind each numeric representation varies based on which model generates them here are some of the other models you can use to create text embeddings so you've got open AI as we've seen huertevac and glove so as we now know we can use these text embeddings to compare them to other text embeddings just like we did with the example of comparing personality trait to personality trait apart from instead of capturing a personality the meaning of a word is captured instead there is another cool benefit to Turning words into numeric representation one of them being that we can now apply math to them take for instance this now wellrenowned example so King minus man plus woman equals Queen so for example here you can see how you can take the word King and minus the word man and replace it with the word woman and you get Queen this is truly incredible and it's all thanks to text embeddings we can use code in order to pass through the words King and woman and subtract man and then we get a bunch of words returned to us each with a similarities score Queen is the most similar and hence it has the highest score pretty cool right let's move on by actually talking about what Vector embeddings can be used for so far we have looked at Texan beddings but Vector embeddings actually cover a lot more text is just one of the things that we can vectorize we can vectorize sentences documents notes and graphs images and even our faces we have word embeddings like we just seen this is one of the most popular applications why are the baddings like what back or glove convert words into dense vectors where semantically similar words are closer in the vector space for instance we saw king and queen would have vectors that are closer than King and paper next we also have document and sentence embeddings methods like Dr vac but and sentence but can represent whole documents or sentences as vectors this can be used in document classification semantic search and more next we also have graph embeddings nodes in a graph can be represented as vectors applications include recommendation systems social network analysis and more here are some of the primary applications of vector embeddings we have recommendation systems embeddings can be used to represent users and items like movies books or products the similarity between user and item embeddings can help in making personalized recommendations we also have anomaly detection if you can represent data as vectors you can measure instances or similarities to detect outliers or anomalies in data we also have transfer learning pretrained embeddings especially in the context of deep learning models can be transferred to another task to Kickstart learning especially when the target task has limited data and also amazingly we have visualizations high dimensional data can be converted into 2D or 3D embeddings using techniques like tsne or PCA to visualize clusters or relationships in the data we also use it for information retrieval by embedding both queries and documents in a shared space one can find documents that semantically match the query even if they don't share exact keywords and of course we can use them for natural language processing tasks tasks like text classification sentiment analysis named entity recognition and machine translation benefit from embeddings as they capture semantic information and relationship between words we also have audio and speech processing audio clips can be converted to embeddings for tasks like speaker identification speech recognition or emotion detection and finally we can use it for facial recognition face embeddings can represent a face as a vector making it easier to compare faces and recognize identities so a lot of things that Vector embeddings can be used for we're going to create a few of Our Own in the lesson coming up but the main takeaway here is that the core advantage of vector embeddings is that they provide a way to transform complex multidimensional and often discrete data into a lower dimensional continuous space that captures semantic or structural relationships within the original data next up how do we generate Vector embeddings I'm going to show you how using openai's Create embedding okay so here we are on open AI so please go ahead and log in or sign up if you haven't before and you'll be taken to this landing page now once here what we are going to do is interact with the API so just go ahead and click that and the first thing you will need to do is just make sure you have an API key so under your username here just view your API keys and go ahead and create a new secret key I'm gonna call this demo key like so so that is the name of my key and I'm going to create a secret key and save this somewhere safe so please go ahead and do the same just save your API key and once you're done with that click done you can of course delete previous API keys to revoke access to them so that's what I'm going to be doing with this one so that you can't use it in the future it will be deleted now once you have that let's go back to our API reference and what we're going to do is create embedding so let's click on embeddings here here's the URL if you are lost just copy that into your browser and we are going to be using the embedding object so essentially here is the code that we're going to use so it's right here we have in node.js you can have it in python or you can also have it in curl it is up to you whichever one you would prefer to do so let's just go ahead and use this version first so this is what we're going to be using this is the request we're going to write and this is the response that we are going to get based on the input we passed through okay so essentially the food was delicious and the waiter dot dot dot is now represented by this embedding this array of numbers right here so let's go ahead and do it I'm going to copy this just make sure that it's copied let's get up our terminals I'm just going to make this a little bit bigger for you let's paste that in and now here we need to replace the open AI API key with our own so let's just go ahead and do that it's going to navigate to that piece of text delete it all just like so and paste it and let's just use the same input for now so hit enter and amazing so there is the array of essentially numbers from 1 to 1 that make up the food was delicious and the waiter dot dot so there we go there is that whole object the full thing right here it is also telling us how many tokens we have used in order to create that so great I'm just going to clear that out so let's paste that in again in fact what we can do is just press up and now let's change the input to something else so for example let's just use an example from the beginning of this tutorial let's just go with food so third hit enter and that is the text embedding for food okay it's this whole array and we have used exactly one token to create that amazing now it's time to look at vectors and databases with the rapid adoption of AI and The Innovation that is happening around large language models we need at the center of it all the ability to take large amounts of data contextualize it process it and enable it to be searched with meaning generative AI processes and applications that are being built to natively incorporate generative AI functionality or rely on the ability to access Vector embeddings a data type that provides a semantics necessary for AI to have a similar longterm memory processing to what we have allowing it to draw on and record information for complex task execution as we now know Vector embeddings are the data representation that AI models such as large language models use and generate to make complex decisions like Memories in the human brain there is complexity Dimension pattern and relationships that all need to be stored and represented as part of the underlying structures which make all of this difficult to manage that is why for AI workloads we need a purposebuilt database or brain designed for highly scalable access and specifically built for storing and accessing these vector embeddings Vector databases like data Stacks astrodb built on Apache Cassandra are designed to provide optimized storage and data access capabilities specifically for embeddings now that we understand how important it is to store these vectors in the right type a database let's get to setting one up ourselves in preparation for creating our AI assistant so let's do it so fast off I'm just going to navigate to data stacks and log in please go ahead and sign up if you haven't already this is what your screen should look like once you are signed in and you can see all your options here as well along with your username and so on so as you will see I've previously created a bunch of databases on here already but don't worry I'm going to show you how to get started completely from scratch so all you're going to do is Click create database here and make sure you have Vector database selected and once you have that it's super simple just go ahead and name your database making sure to use the correct characters so for example it won't let you use certain ones you have to use ones that are allowed you will get a little prompt message if you do use an incorrect one I'm just going to go ahead and call my database Vector database now we have to create a key space name that will go inside our database and once again just make sure to name it with the correct conventions so I'm just going to call it search as that was what we are creating we are creating a vector search database so I'm just being super literal with my naming conventions and now I'm going to pick a region that is closest to me so I'm going to go ahead and select Us East one and then just click create database and that's it that's really all there is you will see my database is pending right here we have done it we created our database I'm going to leave that running and come back to it when it's time to use it and pending has gone from pending to active for now let's carry on with a little bit of learning before we dive into creating an AI project I want to talk to you a little bit about line chain Lang chain is an open source framework that allows AI developers to have better interactions with several large language models or llms like open ai's gbt4 for example we can use the M python or JavaScript which is great news for us Developers what do I mean by better interactions however well for one it allows developers to create chains which are logical links between one or more llm you can even use it to load documents such as PDFs or csvs for example to change to each other or an llm heck you can even use it to split up documents and much more you can have basic chains or more advanced chains we'll be creating our own chain soon enough in this course for now just know that langtune's superpower lies in allowing you the developer to chain together different AI large language models external data and prompts in a structured way in order to create cool and Powerful AI applications such as an AIS system for example that not only uses data from the internet but perhaps an essay that you wrote as well that we can then feed into it in order for the AI assistant to answer questions about it too okay so we finally gathered enough knowledge in order to proceed in building an AI assistant built in Python so let's go ahead and do it just to recap this AI assistant is going to essentially be an AI assistant that will help us search for similar text in a data set okay so once again we are going to be able to get some data break it up into little chunks save it in a database in order for us to essentially perform Vector search on it thanks to packages such as Lang chain don't worry if that's a lot I'm going to be explaining everything step by step as we do it so first off let's go back to our database in order to continue with this tutorial so we've already created a serverless database the next thing we're going to do is actually learn to connect with it from an external source and in order to do that we need to essentially get our token so please go ahead and get that token we need to go to the connect Tab and we're simply going to get an application token using the generate token button in the quick start section so you can save this in any way you want just make sure it's saved somewhere safe now once we have got that token saved we need to get a secure connect bundle okay so just go ahead and do that get your bundle and once again save this somewhere safe this time however we're going to down with the secure bundle because we're going to point to this somewhere on our computer so just download the whole thing onto your computer into your downloads wherever you want great now that we have that once again we are going to have to get our API key so as a refresher all you're going to do is head over to the openai.com page and then once you have signed in you will see this platform and you're going to go to API and then once again we are going to be working with embeddings however we are not going to be doing a call request from here we are going to Simply feed in our API token in order for Lang chain to do its thing instead so just go ahead and navigate to your username view your API Keys create a new one so this time I'm just going to call it demo copy this key okay and keep it somewhere safe and then let's go back to data Stacks once more okay so great we have done everything that is to be done here now let's create a python script using Lang chain and Castor IO so I'm just going to go ahead and get up my terminal once more and navigate to a directory where I want to store this this time I'm going to store it in a another directory which is going to be webstorm projects and I'm going to create a directory I'm going to call it search python so I'm going to use the mukter command and then I'm going to go into that project so using the CD command I'm just going into search python or whatever you called your project and then I'm just going to open it up using Code dot which is the shortcut to opening this up in vs code and great so once we are in vs code I'm just going to make sure that this is enabled for python so in order to work with python files let's just go ahead and create a python file first I'm going to call it index.py giving it the py extension so that our code editor knows to treat this as a python file next I'm just going to go here and click on the prompt and this has prompted me it's saying that you know it's recognizing we're working with python and it's asking us to install the recommended python extension so I'm just going to go ahead and install that and that is installing for me right now great once we have done that we are also prompted with a little checklist so I'm going to go ahead and just run through this it's telling me to create a python file which we've already done so the next thing we're going to do is actually add a python environment so let's go ahead and do that I'm just going to go with the first one so great once we have made our environment we can essentially do stuff like this so I'm going to write print hello so just go ahead and do the same as me this is a python script a very simple one and now if we run this by essentially pressing this little plus sign right here that will run the script and you will see Hello being printed in our terminal okay so that's really it everything's now ready to go we've been set up correctly great so now let's get to the media stuff now in order to install packages you can't write it in the script here okay if I go ahead and write pip install and all the packages packages python packages that we need and hit plus that will not work we need to do this in the terminal so just go ahead and paste that in like so just copy it out and hit enter and that will do its thing and install all the packages that we need the packages once again are Cas IO Data sets Lang chain open Ai and tick token so go ahead and wait for that to do its thing it will take some time and once it's ready we should be able to continue with our tutorial great now I'm just going to go ahead and rename this file for readability okay I'm going to rename this to mini.qa you will see there's another directory above me that has been generated with everything that we've done so just go ahead and rename your file if you want to continue having everything the same as me you don't have to though that is completely up to you great the next thing I'm going to do is just ask you to copy this code so here are some variables that we're going to need in order to continue with this tutorial so we have the Astro DB secure bundle part we've already downloaded this onto our computers so that is something that we're going to have to point to and the next thing we're going to have is our astrodb application token as well as our Azure DB client ID okay so these are things we're going to fill out from the stuff that we saw earlier from our secret things that we downloaded some other things that we're gonna have to add are our astrodb key space name as well as our open AI API key which we just recently saved as well and the final thing of our astrodb client secret to so now let's go ahead and fill all these out the open AI API key should be easy it starts with SK like so we previously saw this on the API keys on open AI next I'm going to just put my astrodb keyspace name which we also did together I named the keyspace name search so if you do the same as me just go ahead and write the string search there too of course this is all very down to you if you're copying along with me please feel free to copy these as they are apart from the open AI API key that would be unique to you it should start with SK and if you try use mine mine will now be deactivated or revoked as I showed you how to do earlier as well next we're going to have our Astro DB client secret so again this would be unique to you here is mine if you try to use mine it will not work for you but I'm pasting in here like so so you can kind of see the format that yours should look like as well next we're going to have our astrodb client ID which again will be unique to you but it should be kind of similar in length and kind of similar in terms of the characters that are used and once again we have our token which again is unique it should start with Astra CS like so so make sure yours does two but the characters after it will be unique to you and finally we have the path to our secure bundle so for this just uh go ahead and go to your downloads and find the ZIP file okay that we downloaded when it came to downloading a secure bundle and just drag it into the project and once that's dragged into the project I'm going to ask you to get the path to it okay so get the part like I am here and paste it in like so as mine's in my project now you will see the path to it in my project which is stored in webstorm project in the project called search python so that's all I've done make sure it is still a zip file so it will have the dot zip extension at the end and great so we are now done with all those variables let's continue next we're going to have to get some stuff from the Lang chain package that we installed so here's all the things that we're going to have to get from Lang chain so just paste that out like so we're going to be getting Cassandra from there we're going to be getting Vex the store index wrapper we're going to be also getting the open AI we're also going to be importing openai as the large language model and also importing open AI embeddings from Lang chain embedding great as well as using the Lang chain package that we installed we're also going to get stuff from Cassandra so what are we going to be getting from Cassandra where we're going to be importing the cluster and the plain text auth provider and finally from datasets we're going to import the load data set from the package data sets that we imported earlier great now we are going to essentially have to write some configuration in order for us to connect two data Stacks Astra and create an Astra session and to do this we are going to create a cluster so using the cluster import I'm just going to pass through two things I'm going to pass through the secure connect bundle which we can pass through as the variable we defined above so that is going to be passed through and another thing we're going to have to pass through is an auth provider okay and to create an auth provider we're going to use the plain text or provider that we imported from Cassandra and pass through our Azure DB client ID and astrodb client secret so this is essentially just configuration like I said in order to be able to communicate with our database that we created using datastacks Astra okay so again we're just using all these variables and passing them through in order to connect this to our Astra database great so once we've done that we have essentially created an asterisk session that we're going to be using later now we're going to have to essentially connect to open AI using our open AI API key so we're going to get the openai import and just pass through our API key like so so we're getting the varial so we're getting the variable that we defined above and passing it through and saving it under the variable llm the next thing we're going to do is do the same pool using the open AI embedding so once again we're just passing through our open AI API key and we're storing all of this under the variable my embedding great next we're going to essentially create a table so we're going to actually name our table in here we are going to pass through our astrodb key space which we know is the string search and we are now going to create a table and we're going to do that my table is going to be called QA mini demo so I'm passing through that string that is being created and being saved under the key space name of search in my Cassandra database great so let's just check this is working I'm just going to print loading data from hugging face and then I'm actually going to get some data from hugging face okay this is just one that exists on the Internet it's going to be some onion news so I'm just going to load that data set from hugging face and I'm just going to pass that through and I'm going to save it as my data set and then I'm going to get that data and I'm going to just get the headlines and in order to check that has worked first off I'm just going to print some texts okay so I'm just going to print some text generating embeddings and storing an astro DB and then I'm going to add those headlines to my Cassandra store so I'm essentially passing that through to my database to my astrodb database which is the Cassandra database and then once that is done I'm going to print those headlines so let's run that code okay that's all I've done I'm just printing stuff out to appear in our console log so you can kind of have this ability about what's going on in the back end so once that's done doing its thing we should get that being printed so we're going to print that text loading data from hugging face so we can kind of see where we are in the script essentially then we're just going to print generating embeddings and storing an astro DB and then it's going to tell us exactly the amount of headlines that we've inserted into astrodb is going to be 50 headlines great that is looking wonderful so we can now continue now I'm just going to essentially pass my store my Cassandra store into the vexer store index wrapper and save it under Vector index and now I'm going to paste in some code it's going to prompt us to enter a question and once we have entered that question it's going to do an if else statement in order to print out the question and then the answer as well as the documents by relevance so essentially we are going to write some text it's going to search the hugging face data so the onion news in order to bring back any similar text it's going to do a vector search on our database to find similar text to it based on the vector search so let's go ahead and do that that is the code for doing so please copy it and once we have finished copying it just run the script and you'll be prompted to enter a question so now we can enter a question let's go ahead and paste one that is what are the biggest questions in science the answer is I don't know so not a great answer really but it's going to return some documents by relevance in our database so you will have here the relevant document along with a temperature to show you how relevant that document is we have biologist torture amoeba for information on where life comes from so that's kind of similar to the question that we propose and then we also have study shows humans still have genes to grow full coat of something so essentially we are getting documents by relevance that exists in our database based on the question that we asked let's try another one so I'm just going to ask another question I'm going to ask what should I know about Silicon Valley Banks again it says I'm sorry I don't know but then it returns some documents by relevance again we get a similarity score and then we also get the document that is most relevant to the question I'm going to ask one more let's ask one about amoebas because I saw it being returned in the first document so our amiibos really are overlords and then the answer is no amoebas are not our overlords and again it returns documents by relevance so we see that same document being returned with a similarity score that is higher based on the question we can see that should be higher because we do have the word amoebas in there so there is no surprise that that document is coming back with a higher similarity score and then we also have some other documents being returned back with the lower similarity score so great we have managed to build an AI assistant that will essentially look in a database look for similar documents based on our question and if we want to have a look at what this database looks like under the hood so essentially has been vectorized right we have vectorized all the documents and then we are doing a vector search on it to bring back similar vexes to us that is exactly what we discussed in the beginning of this tutorial I hope you can now see how Vector search works behind the hood as well as on the front end to
